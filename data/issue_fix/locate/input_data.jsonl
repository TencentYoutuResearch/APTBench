{"uuid": "91c796b1-9a7c-4dcd-bff2-3d1d4b53e354", "issue_statement": "Possible bug in io.fits related to D exponents\nI came across the following code in ``fitsrec.py``:\r\n\r\n```python\r\n        # Replace exponent separator in floating point numbers\r\n        if 'D' in format:\r\n            output_field.replace(encode_ascii('E'), encode_ascii('D'))\r\n```\r\n\r\nI think this may be incorrect because as far as I can tell ``replace`` is not an in-place operation for ``chararray`` (it returns a copy). Commenting out this code doesn't cause any tests to fail so I think this code isn't being tested anyway.", "choices": "(A) \n        # Replace exponent separator in floating point numbers\n        if 'D' in format:\n            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n\n\ndef _get_recarray_field(array, key):\n(B) \n\ndef _get_recarray_field(array, key):\n    \"\"\"\n    Compatibility function for using the recarray base class's field method.\n    This incorporates the legacy functionality of returning string arrays as\n    Numeric-style chararray objects.\n(C) \n            output_field[jdx] = value\n\n        # Replace exponent separator in floating point numbers\n        if 'D' in format:\n            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n\n(D)         if 'D' in format:\n            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n\n\ndef _get_recarray_field(array, key):\n    \"\"\"\n    Compatibility function for using the recarray base class's field method.", "answer": "A"}
{"uuid": "4b88fa67-2f61-4da7-829f-a1bd07e99e51", "issue_statement": "Issue when passing empty lists/arrays to WCS transformations\nThe following should not fail but instead should return empty lists/arrays:\r\n\r\n```\r\nIn [1]: from astropy.wcs import WCS\r\n\r\nIn [2]: wcs = WCS('2MASS_h.fits')\r\n\r\nIn [3]: wcs.wcs_pix2world([], [], 0)\r\n---------------------------------------------------------------------------\r\nInconsistentAxisTypesError                Traceback (most recent call last)\r\n<ipython-input-3-e2cc0e97941a> in <module>()\r\n----> 1 wcs.wcs_pix2world([], [], 0)\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in wcs_pix2world(self, *args, **kwargs)\r\n   1352         return self._array_converter(\r\n   1353             lambda xy, o: self.wcs.p2s(xy, o)['world'],\r\n-> 1354             'output', *args, **kwargs)\r\n   1355     wcs_pix2world.__doc__ = \"\"\"\r\n   1356         Transforms pixel coordinates to world coordinates by doing\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in _array_converter(self, func, sky, ra_dec_order, *args)\r\n   1267                     \"a 1-D array for each axis, followed by an origin.\")\r\n   1268 \r\n-> 1269             return _return_list_of_arrays(axes, origin)\r\n   1270 \r\n   1271         raise TypeError(\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in _return_list_of_arrays(axes, origin)\r\n   1223             if ra_dec_order and sky == 'input':\r\n   1224                 xy = self._denormalize_sky(xy)\r\n-> 1225             output = func(xy, origin)\r\n   1226             if ra_dec_order and sky == 'output':\r\n   1227                 output = self._normalize_sky(output)\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in <lambda>(xy, o)\r\n   1351             raise ValueError(\"No basic WCS settings were created.\")\r\n   1352         return self._array_converter(\r\n-> 1353             lambda xy, o: self.wcs.p2s(xy, o)['world'],\r\n   1354             'output', *args, **kwargs)\r\n   1355     wcs_pix2world.__doc__ = \"\"\"\r\n\r\nInconsistentAxisTypesError: ERROR 4 in wcsp2s() at line 2646 of file cextern/wcslib/C/wcs.c:\r\nncoord and/or nelem inconsistent with the wcsprm.\r\n```", "choices": "(A)             out = np.empty((sky.shape[0], 2))\n            out[:, 0] = sky[:, self.wcs.lng]\n            out[:, 1] = sky[:, self.wcs.lat]\n            return out\n\n    def _array_converter(self, func, sky, *args, ra_dec_order=False):\n        \"\"\"\n        A helper function to support reading either a pair of arrays\n        or a single Nx2 array.\n        \"\"\"\n\n        def _return_list_of_arrays(axes, origin):\n            try:\n                axes = np.broadcast_arrays(*axes)\n            except ValueError:\n                raise ValueError(\n                    \"Coordinate arrays are not broadcastable to each other\")\n\n            xy = np.hstack([x.reshape((x.size, 1)) for x in axes])\n\n            if ra_dec_order and sky == 'input':\n                xy = self._denormalize_sky(xy)\n            output = func(xy, origin)\n            if ra_dec_order and sky == 'output':\n                output = self._normalize_sky(output)\n                return (output[:, 0].reshape(axes[0].shape),\n                        output[:, 1].reshape(axes[0].shape))\n            return [output[:, i].reshape(axes[0].shape)\n                    for i in range(output.shape[1])]\n\n        def _return_single_array(xy, origin):\n            if xy.shape[-1] != self.naxis:\n                raise ValueError(\n                    \"When providing two arguments, the array must be \"\n(B)             xy = np.hstack([x.reshape((x.size, 1)) for x in axes])\n\n            if ra_dec_order and sky == 'input':\n                xy = self._denormalize_sky(xy)\n            output = func(xy, origin)\n            if ra_dec_order and sky == 'output':\n                output = self._normalize_sky(output)\n                return (output[:, 0].reshape(axes[0].shape),\n                        output[:, 1].reshape(axes[0].shape))\n            return [output[:, i].reshape(axes[0].shape)\n                    for i in range(output.shape[1])]\n\n        def _return_single_array(xy, origin):\n            if xy.shape[-1] != self.naxis:\n                raise ValueError(\n                    \"When providing two arguments, the array must be \"\n                    \"of shape (N, {0})\".format(self.naxis))\n            if ra_dec_order and sky == 'input':\n                xy = self._denormalize_sky(xy)\n            result = func(xy, origin)\n            if ra_dec_order and sky == 'output':\n                result = self._normalize_sky(result)\n            return result\n\n        if len(args) == 2:\n            try:\n                xy, origin = args\n                xy = np.asarray(xy)\n                origin = int(origin)\n            except Exception:\n                raise TypeError(\n                    \"When providing two arguments, they must be \"\n                    \"(coords[N][{0}], origin)\".format(self.naxis))\n            if self.naxis == 1 and len(xy.shape) == 1:\n(C)                         output[:, 1].reshape(axes[0].shape))\n            return [output[:, i].reshape(axes[0].shape)\n                    for i in range(output.shape[1])]\n\n        def _return_single_array(xy, origin):\n            if xy.shape[-1] != self.naxis:\n                raise ValueError(\n                    \"When providing two arguments, the array must be \"\n                    \"of shape (N, {0})\".format(self.naxis))\n            if ra_dec_order and sky == 'input':\n                xy = self._denormalize_sky(xy)\n            result = func(xy, origin)\n            if ra_dec_order and sky == 'output':\n                result = self._normalize_sky(result)\n            return result\n\n        if len(args) == 2:\n            try:\n                xy, origin = args\n                xy = np.asarray(xy)\n                origin = int(origin)\n            except Exception:\n                raise TypeError(\n                    \"When providing two arguments, they must be \"\n                    \"(coords[N][{0}], origin)\".format(self.naxis))\n            if self.naxis == 1 and len(xy.shape) == 1:\n                return _return_list_of_arrays([xy], origin)\n            return _return_single_array(xy, origin)\n\n        elif len(args) == self.naxis + 1:\n            axes = args[:-1]\n            origin = args[-1]\n            try:\n                axes = [np.asarray(x) for x in axes]\n(D)         \"\"\"\n\n        def _return_list_of_arrays(axes, origin):\n            try:\n                axes = np.broadcast_arrays(*axes)\n            except ValueError:\n                raise ValueError(\n                    \"Coordinate arrays are not broadcastable to each other\")\n\n            xy = np.hstack([x.reshape((x.size, 1)) for x in axes])\n\n            if ra_dec_order and sky == 'input':\n                xy = self._denormalize_sky(xy)\n            output = func(xy, origin)\n            if ra_dec_order and sky == 'output':\n                output = self._normalize_sky(output)\n                return (output[:, 0].reshape(axes[0].shape),\n                        output[:, 1].reshape(axes[0].shape))\n            return [output[:, i].reshape(axes[0].shape)\n                    for i in range(output.shape[1])]\n\n        def _return_single_array(xy, origin):\n            if xy.shape[-1] != self.naxis:\n                raise ValueError(\n                    \"When providing two arguments, the array must be \"\n                    \"of shape (N, {0})\".format(self.naxis))\n            if ra_dec_order and sky == 'input':\n                xy = self._denormalize_sky(xy)\n            result = func(xy, origin)\n            if ra_dec_order and sky == 'output':\n                result = self._normalize_sky(result)\n            return result\n\n        if len(args) == 2:", "answer": "D"}
{"uuid": "21284f95-97a4-42dd-8f76-d1a3d540a63f", "issue_statement": "Set default FILE_UPLOAD_PERMISSION to 0o644.\nDescription\n\t\nHello,\nAs far as I can see, the \u200bFile Uploads documentation page does not mention any permission issues.\nWhat I would like to see is a warning that in absence of explicitly configured FILE_UPLOAD_PERMISSIONS, the permissions for a file uploaded to FileSystemStorage might not be consistent depending on whether a MemoryUploadedFile or a TemporaryUploadedFile was used for temporary storage of the uploaded data (which, with the default FILE_UPLOAD_HANDLERS, in turn depends on the uploaded data size).\nThe tempfile.NamedTemporaryFile + os.rename sequence causes the resulting file permissions to be 0o0600 on some systems (I experience it here on CentOS 7.4.1708 and Python 3.6.5). In all probability, the implementation of Python's built-in tempfile module explicitly sets such permissions for temporary files due to security considerations.\nI found mentions of this issue \u200bon GitHub, but did not manage to find any existing bug report in Django's bug tracker.", "choices": "(A) # you'd pass directly to os.chmod; see https://docs.python.org/library/os.html#files-and-directories.\nFILE_UPLOAD_PERMISSIONS = None\n\n# The numeric mode to assign to newly-created directories, when uploading files.\n# The value should be a mode as you'd pass to os.chmod;\n# see https://docs.python.org/library/os.html#files-and-directories.\nFILE_UPLOAD_DIRECTORY_PERMISSIONS = None\n(B) \n# The numeric mode to set newly-uploaded files to. The value should be a mode\n# you'd pass directly to os.chmod; see https://docs.python.org/library/os.html#files-and-directories.\nFILE_UPLOAD_PERMISSIONS = None\n\n# The numeric mode to assign to newly-created directories, when uploading files.\n# The value should be a mode as you'd pass to os.chmod;\n(C) # (i.e. \"/tmp\" on *nix systems).\nFILE_UPLOAD_TEMP_DIR = None\n\n# The numeric mode to set newly-uploaded files to. The value should be a mode\n# you'd pass directly to os.chmod; see https://docs.python.org/library/os.html#files-and-directories.\nFILE_UPLOAD_PERMISSIONS = None\n\n(D) \n# The numeric mode to assign to newly-created directories, when uploading files.\n# The value should be a mode as you'd pass to os.chmod;\n# see https://docs.python.org/library/os.html#files-and-directories.\nFILE_UPLOAD_DIRECTORY_PERMISSIONS = None\n\n# Python module path where user will place custom format definition.", "answer": "B"}
{"uuid": "f8861452-bb4d-462b-a2b8-38ec372d5c85", "issue_statement": "Allow FilePathField path to accept a callable.\nDescription\n\t\nI have a special case where I want to create a model containing the path to some local files on the server/dev machine. Seeing as the place where these files are stored is different on different machines I have the following:\nimport os\nfrom django.conf import settings\nfrom django.db import models\nclass LocalFiles(models.Model):\n\tname = models.CharField(max_length=255)\n\tfile = models.FilePathField(path=os.path.join(settings.LOCAL_FILE_DIR, 'example_dir'))\nNow when running manage.py makemigrations it will resolve the path based on the machine it is being run on. Eg: /home/<username>/server_files/example_dir\nI had to manually change the migration to include the os.path.join() part to not break this when running the migration on production/other machine.", "choices": "(A)             return None\n        return str(value)\n\n    def formfield(self, **kwargs):\n        return super().formfield(**{\n            'path': self.path,\n            'match': self.match,\n(B)         return super().formfield(**{\n            'path': self.path,\n            'match': self.match,\n            'recursive': self.recursive,\n            'form_class': forms.FilePathField,\n            'allow_files': self.allow_files,\n            'allow_folders': self.allow_folders,\n(C)             'match': self.match,\n            'recursive': self.recursive,\n            'form_class': forms.FilePathField,\n            'allow_files': self.allow_files,\n            'allow_folders': self.allow_folders,\n            **kwargs,\n        })\n(D) \n    def formfield(self, **kwargs):\n        return super().formfield(**{\n            'path': self.path,\n            'match': self.match,\n            'recursive': self.recursive,\n            'form_class': forms.FilePathField,", "answer": "D"}
{"uuid": "030a6fac-b4f3-43e8-bf40-55be389d4efe", "issue_statement": "Incorrect removal of order_by clause created as multiline RawSQL\nDescription\n\t\nHi.\nThe SQLCompiler is ripping off one of my \"order by\" clause, because he \"thinks\" the clause was already \"seen\" (in SQLCompiler.get_order_by()). I'm using expressions written as multiline RawSQLs, which are similar but not the same. \nThe bug is located in SQLCompiler.get_order_by(), somewhere around line computing part of SQL query without ordering:\nwithout_ordering = self.ordering_parts.search(sql).group(1)\nThe sql variable contains multiline sql. As a result, the self.ordering_parts regular expression is returning just a line containing ASC or DESC words. This line is added to seen set, and because my raw queries have identical last lines, only the first clasue is returing from SQLCompiler.get_order_by().\nAs a quick/temporal fix I can suggest making sql variable clean of newline characters, like this:\nsql_oneline = ' '.join(sql.split('\\n'))\nwithout_ordering = self.ordering_parts.search(sql_oneline).group(1)\nNote: beware of unicode (Py2.x u'') and EOL dragons (\\r).\nExample of my query:\n\treturn MyModel.objects.all().order_by(\n\t\tRawSQL('''\n\t\t\tcase when status in ('accepted', 'verification')\n\t\t\t\t then 2 else 1 end''', []).desc(),\n\t\tRawSQL('''\n\t\t\tcase when status in ('accepted', 'verification')\n\t\t\t\t then (accepted_datetime, preferred_datetime)\n\t\t\t\t else null end''', []).asc(),\n\t\tRawSQL('''\n\t\t\tcase when status not in ('accepted', 'verification')\n\t\t\t\t then (accepted_datetime, preferred_datetime, created_at)\n\t\t\t\t else null end''', []).desc())\nThe ordering_parts.search is returing accordingly:\n'\t\t\t\t then 2 else 1 end)'\n'\t\t\t\t else null end'\n'\t\t\t\t else null end'\nSecond RawSQL with a\t\t\t\t else null end part is removed from query.\nThe fun thing is that the issue can be solved by workaround by adding a space or any other char to the last line. \nSo in case of RawSQL I can just say, that current implementation of avoiding duplicates in order by clause works only for special/rare cases (or does not work in all cases). \nThe bug filed here is about wrong identification of duplicates (because it compares only last line of SQL passed to order by clause).\nHope my notes will help you fixing the issue. Sorry for my english.", "choices": "(A)         self.select = None\n        self.annotation_col_map = None\n        self.klass_info = None\n        self.ordering_parts = re.compile(r'(.*)\\s(ASC|DESC)(.*)')\n        self._meta_ordering = None\n\n    def setup_query(self):\n        if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):\n(B)         # separately a list of extra select columns needed for grammatical correctness\n        # of the query, but these columns are not included in self.select.\n        self.select = None\n        self.annotation_col_map = None\n        self.klass_info = None\n        self.ordering_parts = re.compile(r'(.*)\\s(ASC|DESC)(.*)')\n        self._meta_ordering = None\n\n(C)         self.klass_info = None\n        self.ordering_parts = re.compile(r'(.*)\\s(ASC|DESC)(.*)')\n        self._meta_ordering = None\n\n    def setup_query(self):\n        if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):\n            self.query.get_initial_alias()\n        self.select, self.klass_info, self.annotation_col_map = self.get_select()\n(D)         self._meta_ordering = None\n\n    def setup_query(self):\n        if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):\n            self.query.get_initial_alias()\n        self.select, self.klass_info, self.annotation_col_map = self.get_select()\n        self.col_count = len(self.select)\n", "answer": "A"}
{"uuid": "7b6977d6-7330-46b8-923f-4053fe9e42dd", "issue_statement": "Merging 3 or more media objects can throw unnecessary MediaOrderConflictWarnings\nDescription\n\t\nConsider the following form definition, where text-editor-extras.js depends on text-editor.js but all other JS files are independent:\nfrom django import forms\nclass ColorPicker(forms.Widget):\n\tclass Media:\n\t\tjs = ['color-picker.js']\nclass SimpleTextWidget(forms.Widget):\n\tclass Media:\n\t\tjs = ['text-editor.js']\nclass FancyTextWidget(forms.Widget):\n\tclass Media:\n\t\tjs = ['text-editor.js', 'text-editor-extras.js', 'color-picker.js']\nclass MyForm(forms.Form):\n\tbackground_color = forms.CharField(widget=ColorPicker())\n\tintro = forms.CharField(widget=SimpleTextWidget())\n\tbody = forms.CharField(widget=FancyTextWidget())\nDjango should be able to resolve the JS files for the final form into the order text-editor.js, text-editor-extras.js, color-picker.js. However, accessing MyForm().media results in:\n/projects/django/django/forms/widgets.py:145: MediaOrderConflictWarning: Detected duplicate Media files in an opposite order:\ntext-editor-extras.js\ntext-editor.js\n MediaOrderConflictWarning,\nMedia(css={}, js=['text-editor-extras.js', 'color-picker.js', 'text-editor.js'])\nThe MediaOrderConflictWarning is a result of the order that the additions happen in: ColorPicker().media + SimpleTextWidget().media produces Media(css={}, js=['color-picker.js', 'text-editor.js']), which (wrongly) imposes the constraint that color-picker.js must appear before text-editor.js.\nThe final result is particularly unintuitive here, as it's worse than the \"na\u00efve\" result produced by Django 1.11 before order-checking was added (color-picker.js, text-editor.js, text-editor-extras.js), and the pair of files reported in the warning message seems wrong too (aren't color-picker.js and text-editor.js the wrong-ordered ones?)", "choices": "(A)             return Media(**{str(name): getattr(self, '_' + name)})\n        raise KeyError('Unknown media type \"%s\"' % name)\n\n    @staticmethod\n    def merge(list_1, list_2):\n        \"\"\"\n        Merge two lists while trying to keep the relative order of the elements.\n        Warn if the lists have the same two elements in a different relative\n        order.\n\n        For static assets it can be important to have them included in the DOM\n        in a certain order. In JavaScript you may not be able to reference a\n        global or in CSS you might want to override a style.\n        \"\"\"\n        # Start with a copy of list_1.\n        combined_list = list(list_1)\n        last_insert_index = len(list_1)\n        # Walk list_2 in reverse, inserting each element into combined_list if\n        # it doesn't already exist.\n        for path in reversed(list_2):\n            try:\n                # Does path already exist in the list?\n                index = combined_list.index(path)\n            except ValueError:\n                # Add path to combined_list since it doesn't exist.\n                combined_list.insert(last_insert_index, path)\n            else:\n                if index > last_insert_index:\n                    warnings.warn(\n                        'Detected duplicate Media files in an opposite order:\\n'\n                        '%s\\n%s' % (combined_list[last_insert_index], combined_list[index]),\n                        MediaOrderConflictWarning,\n                    )\n                # path already exists in the list. Update last_insert_index so\n                # that the following elements are inserted in front of this one.\n                last_insert_index = index\n        return combined_list\n\n    def __add__(self, other):\n        combined = Media()\n        combined._css_lists = self._css_lists + other._css_lists\n        combined._js_lists = self._js_lists + other._js_lists\n        return combined\n\n\ndef media_property(cls):\n    def _media(self):\n        # Get the media property of the superclass, if it exists\n        sup_cls = super(cls, self)\n        try:\n            base = sup_cls.media\n        except AttributeError:\n            base = Media()\n\n        # Get the media definition for this class\n        definition = getattr(cls, 'Media', None)\n        if definition:\n            extend = getattr(definition, 'extend', True)\n            if extend:\n                if extend is True:\n                    m = base\n                else:\n                    m = Media()\n                    for medium in extend:\n                        m = m + base[medium]\n                return m + Media(definition)\n            return Media(definition)\n        return base\n    return property(_media)\n\n\nclass MediaDefiningClass(type):\n    \"\"\"\n    Metaclass for classes that can have media definitions.\n    \"\"\"\n    def __new__(mcs, name, bases, attrs):\n        new_class = super(MediaDefiningClass, mcs).__new__(mcs, name, bases, attrs)\n\n        if 'media' not in attrs:\n            new_class.media = media_property(new_class)\n\n        return new_class\n\n\nclass Widget(metaclass=MediaDefiningClass):\n    needs_multipart_form = False  # Determines does this widget need multipart form\n    is_localized = False\n    is_required = False\n    supports_microseconds = True\n\n    def __init__(self, attrs=None):\n        self.attrs = {} if attrs is None else attrs.copy()\n\n    def __deepcopy__(self, memo):\n        obj = copy.copy(self)\n        obj.attrs = self.attrs.copy()\n        memo[id(self)] = obj\n        return obj\n\n    @property\n    def is_hidden(self):\n        return self.input_type == 'hidden' if hasattr(self, 'input_type') else False\n\n    def subwidgets(self, name, value, attrs=None):\n        context = self.get_context(name, value, attrs)\n        yield context['widget']\n\n    def format_value(self, value):\n        \"\"\"\n        Return a value as it should appear when rendered in a template.\n        \"\"\"\n        if value == '' or value is None:\n            return None\n        if self.is_localized:\n            return formats.localize_input(value)\n        return str(value)\n\n    def get_context(self, name, value, attrs):\n        context = {}\n        context['widget'] = {\n            'name': name,\n            'is_hidden': self.is_hidden,\n            'required': self.is_required,\n            'value': self.format_value(value),\n            'attrs': self.build_attrs(self.attrs, attrs),\n            'template_name': self.template_name,\n        }\n        return context\n\n    def render(self, name, value, attrs=None, renderer=None):\n        \"\"\"Render the widget as an HTML string.\"\"\"\n        context = self.get_context(name, value, attrs)\n        return self._render(self.template_name, context, renderer)\n\n    def _render(self, template_name, context, renderer=None):\n        if renderer is None:\n            renderer = get_default_renderer()\n        return mark_safe(renderer.render(template_name, context))\n\n    def build_attrs(self, base_attrs, extra_attrs=None):\n        \"\"\"Build an attribute dictionary.\"\"\"\n        return {**base_attrs, **(extra_attrs or {})}\n\n    def value_from_datadict(self, data, files, name):\n(B)     def __init__(self, media=None, css=None, js=None):\n        if media is not None:\n            css = getattr(media, 'css', {})\n            js = getattr(media, 'js', [])\n        else:\n            if css is None:\n                css = {}\n            if js is None:\n                js = []\n        self._css_lists = [css]\n        self._js_lists = [js]\n\n    def __repr__(self):\n        return 'Media(css=%r, js=%r)' % (self._css, self._js)\n\n    def __str__(self):\n        return self.render()\n\n    @property\n    def _css(self):\n        css = self._css_lists[0]\n        # filter(None, ...) avoids calling merge with empty dicts.\n        for obj in filter(None, self._css_lists[1:]):\n            css = {\n                medium: self.merge(css.get(medium, []), obj.get(medium, []))\n                for medium in css.keys() | obj.keys()\n            }\n        return css\n\n    @property\n    def _js(self):\n        js = self._js_lists[0]\n        # filter(None, ...) avoids calling merge() with empty lists.\n        for obj in filter(None, self._js_lists[1:]):\n            js = self.merge(js, obj)\n        return js\n\n    def render(self):\n        return mark_safe('\\n'.join(chain.from_iterable(getattr(self, 'render_' + name)() for name in MEDIA_TYPES)))\n\n    def render_js(self):\n        return [\n            format_html(\n                '<script type=\"text/javascript\" src=\"{}\"></script>',\n                self.absolute_path(path)\n            ) for path in self._js\n        ]\n\n    def render_css(self):\n        # To keep rendering order consistent, we can't just iterate over items().\n        # We need to sort the keys, and iterate over the sorted list.\n        media = sorted(self._css)\n        return chain.from_iterable([\n            format_html(\n                '<link href=\"{}\" type=\"text/css\" media=\"{}\" rel=\"stylesheet\">',\n                self.absolute_path(path), medium\n            ) for path in self._css[medium]\n        ] for medium in media)\n\n    def absolute_path(self, path):\n        \"\"\"\n        Given a relative or absolute path to a static asset, return an absolute\n        path. An absolute path will be returned unchanged while a relative path\n        will be passed to django.templatetags.static.static().\n        \"\"\"\n        if path.startswith(('http://', 'https://', '/')):\n            return path\n        return static(path)\n\n    def __getitem__(self, name):\n        \"\"\"Return a Media object that only contains media of the given type.\"\"\"\n        if name in MEDIA_TYPES:\n            return Media(**{str(name): getattr(self, '_' + name)})\n        raise KeyError('Unknown media type \"%s\"' % name)\n\n    @staticmethod\n    def merge(list_1, list_2):\n        \"\"\"\n        Merge two lists while trying to keep the relative order of the elements.\n        Warn if the lists have the same two elements in a different relative\n        order.\n\n        For static assets it can be important to have them included in the DOM\n        in a certain order. In JavaScript you may not be able to reference a\n        global or in CSS you might want to override a style.\n        \"\"\"\n        # Start with a copy of list_1.\n        combined_list = list(list_1)\n        last_insert_index = len(list_1)\n        # Walk list_2 in reverse, inserting each element into combined_list if\n        # it doesn't already exist.\n        for path in reversed(list_2):\n            try:\n                # Does path already exist in the list?\n                index = combined_list.index(path)\n            except ValueError:\n                # Add path to combined_list since it doesn't exist.\n                combined_list.insert(last_insert_index, path)\n            else:\n                if index > last_insert_index:\n                    warnings.warn(\n                        'Detected duplicate Media files in an opposite order:\\n'\n                        '%s\\n%s' % (combined_list[last_insert_index], combined_list[index]),\n                        MediaOrderConflictWarning,\n                    )\n                # path already exists in the list. Update last_insert_index so\n                # that the following elements are inserted in front of this one.\n                last_insert_index = index\n        return combined_list\n\n    def __add__(self, other):\n        combined = Media()\n        combined._css_lists = self._css_lists + other._css_lists\n        combined._js_lists = self._js_lists + other._js_lists\n        return combined\n\n\ndef media_property(cls):\n    def _media(self):\n        # Get the media property of the superclass, if it exists\n        sup_cls = super(cls, self)\n        try:\n            base = sup_cls.media\n        except AttributeError:\n            base = Media()\n\n        # Get the media definition for this class\n        definition = getattr(cls, 'Media', None)\n        if definition:\n            extend = getattr(definition, 'extend', True)\n            if extend:\n                if extend is True:\n                    m = base\n                else:\n                    m = Media()\n                    for medium in extend:\n                        m = m + base[medium]\n                return m + Media(definition)\n            return Media(definition)\n        return base\n    return property(_media)\n\n\nclass MediaDefiningClass(type):\n(C) \n    def render(self):\n        return mark_safe('\\n'.join(chain.from_iterable(getattr(self, 'render_' + name)() for name in MEDIA_TYPES)))\n\n    def render_js(self):\n        return [\n            format_html(\n                '<script type=\"text/javascript\" src=\"{}\"></script>',\n                self.absolute_path(path)\n            ) for path in self._js\n        ]\n\n    def render_css(self):\n        # To keep rendering order consistent, we can't just iterate over items().\n        # We need to sort the keys, and iterate over the sorted list.\n        media = sorted(self._css)\n        return chain.from_iterable([\n            format_html(\n                '<link href=\"{}\" type=\"text/css\" media=\"{}\" rel=\"stylesheet\">',\n                self.absolute_path(path), medium\n            ) for path in self._css[medium]\n        ] for medium in media)\n\n    def absolute_path(self, path):\n        \"\"\"\n        Given a relative or absolute path to a static asset, return an absolute\n        path. An absolute path will be returned unchanged while a relative path\n        will be passed to django.templatetags.static.static().\n        \"\"\"\n        if path.startswith(('http://', 'https://', '/')):\n            return path\n        return static(path)\n\n    def __getitem__(self, name):\n        \"\"\"Return a Media object that only contains media of the given type.\"\"\"\n        if name in MEDIA_TYPES:\n            return Media(**{str(name): getattr(self, '_' + name)})\n        raise KeyError('Unknown media type \"%s\"' % name)\n\n    @staticmethod\n    def merge(list_1, list_2):\n        \"\"\"\n        Merge two lists while trying to keep the relative order of the elements.\n        Warn if the lists have the same two elements in a different relative\n        order.\n\n        For static assets it can be important to have them included in the DOM\n        in a certain order. In JavaScript you may not be able to reference a\n        global or in CSS you might want to override a style.\n        \"\"\"\n        # Start with a copy of list_1.\n        combined_list = list(list_1)\n        last_insert_index = len(list_1)\n        # Walk list_2 in reverse, inserting each element into combined_list if\n        # it doesn't already exist.\n        for path in reversed(list_2):\n            try:\n                # Does path already exist in the list?\n                index = combined_list.index(path)\n            except ValueError:\n                # Add path to combined_list since it doesn't exist.\n                combined_list.insert(last_insert_index, path)\n            else:\n                if index > last_insert_index:\n                    warnings.warn(\n                        'Detected duplicate Media files in an opposite order:\\n'\n                        '%s\\n%s' % (combined_list[last_insert_index], combined_list[index]),\n                        MediaOrderConflictWarning,\n                    )\n                # path already exists in the list. Update last_insert_index so\n                # that the following elements are inserted in front of this one.\n                last_insert_index = index\n        return combined_list\n\n    def __add__(self, other):\n        combined = Media()\n        combined._css_lists = self._css_lists + other._css_lists\n        combined._js_lists = self._js_lists + other._js_lists\n        return combined\n\n\ndef media_property(cls):\n    def _media(self):\n        # Get the media property of the superclass, if it exists\n        sup_cls = super(cls, self)\n        try:\n            base = sup_cls.media\n        except AttributeError:\n            base = Media()\n\n        # Get the media definition for this class\n        definition = getattr(cls, 'Media', None)\n        if definition:\n            extend = getattr(definition, 'extend', True)\n            if extend:\n                if extend is True:\n                    m = base\n                else:\n                    m = Media()\n                    for medium in extend:\n                        m = m + base[medium]\n                return m + Media(definition)\n            return Media(definition)\n        return base\n    return property(_media)\n\n\nclass MediaDefiningClass(type):\n    \"\"\"\n    Metaclass for classes that can have media definitions.\n    \"\"\"\n    def __new__(mcs, name, bases, attrs):\n        new_class = super(MediaDefiningClass, mcs).__new__(mcs, name, bases, attrs)\n\n        if 'media' not in attrs:\n            new_class.media = media_property(new_class)\n\n        return new_class\n\n\nclass Widget(metaclass=MediaDefiningClass):\n    needs_multipart_form = False  # Determines does this widget need multipart form\n    is_localized = False\n    is_required = False\n    supports_microseconds = True\n\n    def __init__(self, attrs=None):\n        self.attrs = {} if attrs is None else attrs.copy()\n\n    def __deepcopy__(self, memo):\n        obj = copy.copy(self)\n        obj.attrs = self.attrs.copy()\n        memo[id(self)] = obj\n        return obj\n\n    @property\n    def is_hidden(self):\n        return self.input_type == 'hidden' if hasattr(self, 'input_type') else False\n\n    def subwidgets(self, name, value, attrs=None):\n        context = self.get_context(name, value, attrs)\n        yield context['widget']\n\n    def format_value(self, value):\n(D) import datetime\nimport re\nimport warnings\nfrom itertools import chain\n\nfrom django.conf import settings\nfrom django.forms.utils import to_current_timezone\nfrom django.templatetags.static import static\nfrom django.utils import datetime_safe, formats\nfrom django.utils.dates import MONTHS\nfrom django.utils.formats import get_format\nfrom django.utils.html import format_html, html_safe\nfrom django.utils.safestring import mark_safe\nfrom django.utils.translation import gettext_lazy as _\n\nfrom .renderers import get_default_renderer\n\n__all__ = (\n    'Media', 'MediaDefiningClass', 'Widget', 'TextInput', 'NumberInput',\n    'EmailInput', 'URLInput', 'PasswordInput', 'HiddenInput',\n    'MultipleHiddenInput', 'FileInput', 'ClearableFileInput', 'Textarea',\n    'DateInput', 'DateTimeInput', 'TimeInput', 'CheckboxInput', 'Select',\n    'NullBooleanSelect', 'SelectMultiple', 'RadioSelect',\n    'CheckboxSelectMultiple', 'MultiWidget', 'SplitDateTimeWidget',\n    'SplitHiddenDateTimeWidget', 'SelectDateWidget',\n)\n\nMEDIA_TYPES = ('css', 'js')\n\n\nclass MediaOrderConflictWarning(RuntimeWarning):\n    pass\n\n\n@html_safe\nclass Media:\n    def __init__(self, media=None, css=None, js=None):\n        if media is not None:\n            css = getattr(media, 'css', {})\n            js = getattr(media, 'js', [])\n        else:\n            if css is None:\n                css = {}\n            if js is None:\n                js = []\n        self._css_lists = [css]\n        self._js_lists = [js]\n\n    def __repr__(self):\n        return 'Media(css=%r, js=%r)' % (self._css, self._js)\n\n    def __str__(self):\n        return self.render()\n\n    @property\n    def _css(self):\n        css = self._css_lists[0]\n        # filter(None, ...) avoids calling merge with empty dicts.\n        for obj in filter(None, self._css_lists[1:]):\n            css = {\n                medium: self.merge(css.get(medium, []), obj.get(medium, []))\n                for medium in css.keys() | obj.keys()\n            }\n        return css\n\n    @property\n    def _js(self):\n        js = self._js_lists[0]\n        # filter(None, ...) avoids calling merge() with empty lists.\n        for obj in filter(None, self._js_lists[1:]):\n            js = self.merge(js, obj)\n        return js\n\n    def render(self):\n        return mark_safe('\\n'.join(chain.from_iterable(getattr(self, 'render_' + name)() for name in MEDIA_TYPES)))\n\n    def render_js(self):\n        return [\n            format_html(\n                '<script type=\"text/javascript\" src=\"{}\"></script>',\n                self.absolute_path(path)\n            ) for path in self._js\n        ]\n\n    def render_css(self):\n        # To keep rendering order consistent, we can't just iterate over items().\n        # We need to sort the keys, and iterate over the sorted list.\n        media = sorted(self._css)\n        return chain.from_iterable([\n            format_html(\n                '<link href=\"{}\" type=\"text/css\" media=\"{}\" rel=\"stylesheet\">',\n                self.absolute_path(path), medium\n            ) for path in self._css[medium]\n        ] for medium in media)\n\n    def absolute_path(self, path):\n        \"\"\"\n        Given a relative or absolute path to a static asset, return an absolute\n        path. An absolute path will be returned unchanged while a relative path\n        will be passed to django.templatetags.static.static().\n        \"\"\"\n        if path.startswith(('http://', 'https://', '/')):\n            return path\n        return static(path)\n\n    def __getitem__(self, name):\n        \"\"\"Return a Media object that only contains media of the given type.\"\"\"\n        if name in MEDIA_TYPES:\n            return Media(**{str(name): getattr(self, '_' + name)})\n        raise KeyError('Unknown media type \"%s\"' % name)\n\n    @staticmethod\n    def merge(list_1, list_2):\n        \"\"\"\n        Merge two lists while trying to keep the relative order of the elements.\n        Warn if the lists have the same two elements in a different relative\n        order.\n\n        For static assets it can be important to have them included in the DOM\n        in a certain order. In JavaScript you may not be able to reference a\n        global or in CSS you might want to override a style.\n        \"\"\"\n        # Start with a copy of list_1.\n        combined_list = list(list_1)\n        last_insert_index = len(list_1)\n        # Walk list_2 in reverse, inserting each element into combined_list if\n        # it doesn't already exist.\n        for path in reversed(list_2):\n            try:\n                # Does path already exist in the list?\n                index = combined_list.index(path)\n            except ValueError:\n                # Add path to combined_list since it doesn't exist.\n                combined_list.insert(last_insert_index, path)\n            else:\n                if index > last_insert_index:\n                    warnings.warn(\n                        'Detected duplicate Media files in an opposite order:\\n'\n                        '%s\\n%s' % (combined_list[last_insert_index], combined_list[index]),\n                        MediaOrderConflictWarning,\n                    )\n                # path already exists in the list. Update last_insert_index so\n                # that the following elements are inserted in front of this one.\n                last_insert_index = index", "answer": "D"}
{"uuid": "6fa7f05e-18fa-4c0f-8239-90d6012a458c", "issue_statement": "sqlmigrate wraps it's outpout in BEGIN/COMMIT even if the database doesn't support transactional DDL\nDescription\n\t \n\t\t(last modified by Simon Charette)\n\t \nThe migration executor only adds the outer BEGIN/COMMIT \u200bif the migration is atomic and \u200bthe schema editor can rollback DDL but the current sqlmigrate logic only takes migration.atomic into consideration.\nThe issue can be addressed by\nChanging sqlmigrate \u200bassignment of self.output_transaction to consider connection.features.can_rollback_ddl as well.\nAdding a test in tests/migrations/test_commands.py based on \u200ban existing test for non-atomic migrations that mocks connection.features.can_rollback_ddl to False instead of overdidding MIGRATION_MODULES to point to a non-atomic migration.\nI marked the ticket as easy picking because I included the above guidelines but feel free to uncheck it if you deem it inappropriate.", "choices": "(A)         try:\n            migration = executor.loader.get_migration_by_prefix(app_label, migration_name)\n        except AmbiguityError:\n            raise CommandError(\"More than one migration matches '%s' in app '%s'. Please be more specific.\" % (\n                migration_name, app_label))\n        except KeyError:\n            raise CommandError(\"Cannot find a migration matching '%s' from app '%s'. Is it in INSTALLED_APPS?\" % (\n                migration_name, app_label))\n        targets = [(app_label, migration.name)]\n(B)                 migration_name, app_label))\n        targets = [(app_label, migration.name)]\n\n        # Show begin/end around output only for atomic migrations\n        self.output_transaction = migration.atomic\n\n        # Make a plan that represents just the requested migrations and show SQL\n        # for it\n        plan = [(executor.loader.graph.nodes[targets[0]], options['backwards'])]\n(C)                 migration_name, app_label))\n        except KeyError:\n            raise CommandError(\"Cannot find a migration matching '%s' from app '%s'. Is it in INSTALLED_APPS?\" % (\n                migration_name, app_label))\n        targets = [(app_label, migration.name)]\n\n        # Show begin/end around output only for atomic migrations\n        self.output_transaction = migration.atomic\n\n(D)         except AmbiguityError:\n            raise CommandError(\"More than one migration matches '%s' in app '%s'. Please be more specific.\" % (\n                migration_name, app_label))\n        except KeyError:\n            raise CommandError(\"Cannot find a migration matching '%s' from app '%s'. Is it in INSTALLED_APPS?\" % (\n                migration_name, app_label))\n        targets = [(app_label, migration.name)]\n\n        # Show begin/end around output only for atomic migrations", "answer": "B"}
{"uuid": "bcfcbbae-8c81-4aaf-ab8d-9e77f5e08e7a", "issue_statement": "Correct expected format in invalid DurationField error message\nDescription\n\t\nIf you enter a duration \"14:00\" into a duration field, it translates to \"00:14:00\" which is 14 minutes.\nThe current error message for invalid DurationField says that this should be the format of durations: \"[DD] [HH:[MM:]]ss[.uuuuuu]\". But according to the actual behaviour, it should be: \"[DD] [[HH:]MM:]ss[.uuuuuu]\", because seconds are mandatory, minutes are optional, and hours are optional if minutes are provided.\nThis seems to be a mistake in all Django versions that support the DurationField.\nAlso the duration fields could have a default help_text with the requested format, because the syntax is not self-explanatory.", "choices": "(A)     empty_strings_allowed = False\n    default_error_messages = {\n        'invalid': _(\"'%(value)s' value has an invalid format. It must be in \"\n                     \"[DD] [HH:[MM:]]ss[.uuuuuu] format.\")\n    }\n    description = _(\"Duration\")\n\n(B)     of microseconds on other databases.\n    \"\"\"\n    empty_strings_allowed = False\n    default_error_messages = {\n        'invalid': _(\"'%(value)s' value has an invalid format. It must be in \"\n                     \"[DD] [HH:[MM:]]ss[.uuuuuu] format.\")\n    }\n(C)     }\n    description = _(\"Duration\")\n\n    def get_internal_type(self):\n        return \"DurationField\"\n\n    def to_python(self, value):\n(D)         'invalid': _(\"'%(value)s' value has an invalid format. It must be in \"\n                     \"[DD] [HH:[MM:]]ss[.uuuuuu] format.\")\n    }\n    description = _(\"Duration\")\n\n    def get_internal_type(self):\n        return \"DurationField\"", "answer": "A"}
{"uuid": "727e36d3-6bfb-46e7-904d-6456ff9e3153", "issue_statement": "UsernameValidator allows trailing newline in usernames\nDescription\n\t\nASCIIUsernameValidator and UnicodeUsernameValidator use the regex \nr'^[\\w.@+-]+$'\nThe intent is to only allow alphanumeric characters as well as ., @, +, and -. However, a little known quirk of Python regexes is that $ will also match a trailing newline. Therefore, the user name validators will accept usernames which end with a newline. You can avoid this behavior by instead using \\A and \\Z to terminate regexes. For example, the validator regex could be changed to\nr'\\A[\\w.@+-]+\\Z'\nin order to reject usernames that end with a newline.\nI am not sure how to officially post a patch, but the required change is trivial - using the regex above in the two validators in contrib.auth.validators.", "choices": "(A) \nfrom django.core import validators\nfrom django.utils.deconstruct import deconstructible\nfrom django.utils.translation import gettext_lazy as _\n\n\n@deconstructible\nclass ASCIIUsernameValidator(validators.RegexValidator):\n    regex = r'^[\\w.@+-]+$'\n    message = _(\n        'Enter a valid username. This value may contain only English letters, '\n        'numbers, and @/./+/-/_ characters.'\n    )\n    flags = re.ASCII\n\n\n@deconstructible\n(B) \n@deconstructible\nclass ASCIIUsernameValidator(validators.RegexValidator):\n    regex = r'^[\\w.@+-]+$'\n    message = _(\n        'Enter a valid username. This value may contain only English letters, '\n        'numbers, and @/./+/-/_ characters.'\n    )\n    flags = re.ASCII\n\n\n@deconstructible\nclass UnicodeUsernameValidator(validators.RegexValidator):\n    regex = r'^[\\w.@+-]+$'\n    message = _(\n        'Enter a valid username. This value may contain only letters, '\n        'numbers, and @/./+/-/_ characters.'", "answer": "D"}
{"uuid": "0e7b71a0-5ef3-446b-b612-b6bd0de9c5cc", "issue_statement": "HttpResponse doesn't handle memoryview objects\nDescription\n\t\nI am trying to write a BinaryField retrieved from the database into a HttpResponse. When the database is Sqlite this works correctly, but Postgresql returns the contents of the field as a memoryview object and it seems like current Django doesn't like this combination:\nfrom django.http import HttpResponse\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \n# String content\nresponse = HttpResponse(\"My Content\")\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\nresponse.content\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \n# Out: b'My Content'\n# This is correct\n# Bytes content\nresponse = HttpResponse(b\"My Content\")\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \nresponse.content\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \n# Out: b'My Content'\n# This is also correct\n# memoryview content\nresponse = HttpResponse(memoryview(b\"My Content\"))\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \nresponse.content\n# Out: b'<memory at 0x7fcc47ab2648>'\n# This is not correct, I am expecting b'My Content'", "choices": "(A)         # Handle string types -- we can't rely on force_bytes here because:\n        # - Python attempts str conversion first\n        # - when self._charset != 'utf-8' it re-encodes the content\n        if isinstance(value, bytes):\n            return bytes(value)\n        if isinstance(value, str):\n            return bytes(value.encode(self.charset))\n(B)             return bytes(value)\n        if isinstance(value, str):\n            return bytes(value.encode(self.charset))\n        # Handle non-string types.\n        return str(value).encode(self.charset)\n\n    # These methods partially implement the file-like object interface.\n(C)         # - when self._charset != 'utf-8' it re-encodes the content\n        if isinstance(value, bytes):\n            return bytes(value)\n        if isinstance(value, str):\n            return bytes(value.encode(self.charset))\n        # Handle non-string types.\n        return str(value).encode(self.charset)\n(D)         # This doesn't make a copy when `value` already contains bytes.\n\n        # Handle string types -- we can't rely on force_bytes here because:\n        # - Python attempts str conversion first\n        # - when self._charset != 'utf-8' it re-encodes the content\n        if isinstance(value, bytes):\n            return bytes(value)", "answer": "A"}
{"uuid": "43742602-8427-418a-9001-c30621659bda", "issue_statement": "delete() on instances of models without any dependencies doesn't clear PKs.\nDescription\n\t\nDeleting any model with no dependencies not updates the PK on the model. It should be set to None after .delete() call.\nSee Django.db.models.deletion:276-281. Should update the model line 280.", "choices": "(A) \n        with transaction.atomic(using=self.using, savepoint=False):\n            # send pre_delete signals\n            for model, obj in self.instances_with_model():\n                if not model._meta.auto_created:\n                    signals.pre_delete.send(\n                        sender=model, instance=obj, using=self.using\n(B)         if len(self.data) == 1 and len(instances) == 1:\n            instance = list(instances)[0]\n            if self.can_fast_delete(instance):\n                with transaction.mark_for_rollback_on_error():\n                    count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n                return count, {model._meta.label: count}\n\n(C)             if self.can_fast_delete(instance):\n                with transaction.mark_for_rollback_on_error():\n                    count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n                return count, {model._meta.label: count}\n\n        with transaction.atomic(using=self.using, savepoint=False):\n            # send pre_delete signals\n(D)                     count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n                return count, {model._meta.label: count}\n\n        with transaction.atomic(using=self.using, savepoint=False):\n            # send pre_delete signals\n            for model, obj in self.instances_with_model():\n                if not model._meta.auto_created:", "answer": "C"}
{"uuid": "f6664cdb-9f76-46a3-973d-a98b33b6adbb", "issue_statement": "Autoreloader with StatReloader doesn't track changes in manage.py.\nDescription\n\t \n\t\t(last modified by Mariusz Felisiak)\n\t \nThis is a bit convoluted, but here we go.\nEnvironment (OSX 10.11):\n$ python -V\nPython 3.6.2\n$ pip -V\npip 19.1.1\n$ pip install Django==2.2.1\nSteps to reproduce:\nRun a server python manage.py runserver\nEdit the manage.py file, e.g. add print(): \ndef main():\n\tprint('sth')\n\tos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ticket_30479.settings')\n\t...\nUnder 2.1.8 (and prior), this will trigger the auto-reloading mechanism. Under 2.2.1, it won't. As far as I can tell from the django.utils.autoreload log lines, it never sees the manage.py itself.", "choices": "(A) def iter_modules_and_files(modules, extra_files):\n    \"\"\"Iterate through all modules needed to be watched.\"\"\"\n    sys_file_paths = []\n    for module in modules:\n        # During debugging (with PyDev) the 'typing.io' and 'typing.re' objects\n        # are added to sys.modules, however they are types not modules and so\n        # cause issues here.\n        if not isinstance(module, ModuleType) or getattr(module, '__spec__', None) is None:\n            continue\n        spec = module.__spec__\n        # Modules could be loaded from places without a concrete location. If\n        # this is the case, skip them.\n        if spec.has_location:\n            origin = spec.loader.archive if isinstance(spec.loader, zipimporter) else spec.origin\n            sys_file_paths.append(origin)\n(B)         # During debugging (with PyDev) the 'typing.io' and 'typing.re' objects\n        # are added to sys.modules, however they are types not modules and so\n        # cause issues here.\n        if not isinstance(module, ModuleType) or getattr(module, '__spec__', None) is None:\n            continue\n        spec = module.__spec__\n        # Modules could be loaded from places without a concrete location. If\n        # this is the case, skip them.\n        if spec.has_location:\n            origin = spec.loader.archive if isinstance(spec.loader, zipimporter) else spec.origin\n            sys_file_paths.append(origin)\n\n    results = set()\n    for filename in itertools.chain(sys_file_paths, extra_files):\n        if not filename:\n(C)             continue\n        spec = module.__spec__\n        # Modules could be loaded from places without a concrete location. If\n        # this is the case, skip them.\n        if spec.has_location:\n            origin = spec.loader.archive if isinstance(spec.loader, zipimporter) else spec.origin\n            sys_file_paths.append(origin)\n\n    results = set()\n    for filename in itertools.chain(sys_file_paths, extra_files):\n        if not filename:\n            continue\n        path = pathlib.Path(filename)\n        if not path.exists():\n            # The module could have been removed, don't fail loudly if this\n(D)         if spec.has_location:\n            origin = spec.loader.archive if isinstance(spec.loader, zipimporter) else spec.origin\n            sys_file_paths.append(origin)\n\n    results = set()\n    for filename in itertools.chain(sys_file_paths, extra_files):\n        if not filename:\n            continue\n        path = pathlib.Path(filename)\n        if not path.exists():\n            # The module could have been removed, don't fail loudly if this\n            # is the case.\n            continue\n        results.add(path.resolve().absolute())\n    return frozenset(results)", "answer": "B"}
{"uuid": "a53140ab-3976-41be-ac58-7c962975ba6f", "issue_statement": "Add support for SCRIPT_NAME in STATIC_URL and MEDIA_URL\nDescription\n\t \n\t\t(last modified by Rostyslav Bryzgunov)\n\t \nBy default, {% static '...' %} tag just appends STATIC_URL in the path. When running on sub-path, using SCRIPT_NAME WSGI param, it results in incorrect static URL - it doesn't prepend SCRIPT_NAME prefix.\nThis problem can be solved with prepending SCRIPT_NAME to STATIC_URL in settings.py but that doesn't work when SCRIPT_NAME is a dynamic value.\nThis can be easily added into default Django static tag and django.contrib.staticfiles tag as following:\ndef render(self, context):\n\turl = self.url(context)\n\t# Updating url here with request.META['SCRIPT_NAME'] \n\tif self.varname is None:\n\t\treturn url\n\tcontext[self.varname] = url\n\t\treturn ''\nOn more research I found that FileSystemStorage and StaticFilesStorage ignores SCRIPT_NAME as well. \nWe might have to do a lot of changes but I think it's worth the efforts.", "choices": "(A) \nimport django\nfrom django.conf import global_settings\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.utils.deprecation import RemovedInDjango40Warning\nfrom django.utils.functional import LazyObject, empty\n\nENVIRONMENT_VARIABLE = \"DJANGO_SETTINGS_MODULE\"\n\nPASSWORD_RESET_TIMEOUT_DAYS_DEPRECATED_MSG = (\n    'The PASSWORD_RESET_TIMEOUT_DAYS setting is deprecated. Use '\n    'PASSWORD_RESET_TIMEOUT instead.'\n)\n\n\nclass SettingsReference(str):\n    \"\"\"\n    String subclass which references a current settings value. It's treated as\n    the value in memory but serializes to a settings.NAME attribute reference.\n    \"\"\"\n    def __new__(self, value, setting_name):\n        return str.__new__(self, value)\n\n    def __init__(self, value, setting_name):\n        self.setting_name = setting_name\n\n\nclass LazySettings(LazyObject):\n    \"\"\"\n    A lazy proxy for either global Django settings or a custom settings object.\n    The user can manually configure settings prior to using them. Otherwise,\n    Django uses the settings module pointed to by DJANGO_SETTINGS_MODULE.\n    \"\"\"\n    def _setup(self, name=None):\n        \"\"\"\n        Load the settings module pointed to by the environment variable. This\n        is used the first time settings are needed, if the user hasn't\n        configured settings manually.\n        \"\"\"\n        settings_module = os.environ.get(ENVIRONMENT_VARIABLE)\n        if not settings_module:\n            desc = (\"setting %s\" % name) if name else \"settings\"\n            raise ImproperlyConfigured(\n                \"Requested %s, but settings are not configured. \"\n                \"You must either define the environment variable %s \"\n                \"or call settings.configure() before accessing settings.\"\n                % (desc, ENVIRONMENT_VARIABLE))\n\n        self._wrapped = Settings(settings_module)\n\n    def __repr__(self):\n        # Hardcode the class name as otherwise it yields 'Settings'.\n        if self._wrapped is empty:\n            return '<LazySettings [Unevaluated]>'\n        return '<LazySettings \"%(settings_module)s\">' % {\n            'settings_module': self._wrapped.SETTINGS_MODULE,\n        }\n\n    def __getattr__(self, name):\n        \"\"\"Return the value of a setting and cache it in self.__dict__.\"\"\"\n        if self._wrapped is empty:\n            self._setup(name)\n        val = getattr(self._wrapped, name)\n        self.__dict__[name] = val\n        return val\n\n    def __setattr__(self, name, value):\n        \"\"\"\n        Set the value of setting. Clear all cached values if _wrapped changes\n        (@override_settings does this) or clear single values when set.\n        \"\"\"\n        if name == '_wrapped':\n            self.__dict__.clear()\n        else:\n            self.__dict__.pop(name, None)\n        super().__setattr__(name, value)\n\n    def __delattr__(self, name):\n        \"\"\"Delete a setting and clear it from cache if needed.\"\"\"\n        super().__delattr__(name)\n        self.__dict__.pop(name, None)\n\n    def configure(self, default_settings=global_settings, **options):\n        \"\"\"\n        Called to manually configure the settings. The 'default_settings'\n        parameter sets where to retrieve any unspecified values from (its\n        argument must support attribute access (__getattr__)).\n        \"\"\"\n        if self._wrapped is not empty:\n            raise RuntimeError('Settings already configured.')\n        holder = UserSettingsHolder(default_settings)\n        for name, value in options.items():\n            if not name.isupper():\n                raise TypeError('Setting %r must be uppercase.' % name)\n            setattr(holder, name, value)\n        self._wrapped = holder\n\n    @property\n    def configured(self):\n        \"\"\"Return True if the settings have already been configured.\"\"\"\n        return self._wrapped is not empty\n\n    @property\n    def PASSWORD_RESET_TIMEOUT_DAYS(self):\n        stack = traceback.extract_stack()\n        # Show a warning if the setting is used outside of Django.\n        # Stack index: -1 this line, -2 the caller.\n        filename, _, _, _ = stack[-2]\n        if not filename.startswith(os.path.dirname(django.__file__)):\n            warnings.warn(\n                PASSWORD_RESET_TIMEOUT_DAYS_DEPRECATED_MSG,\n                RemovedInDjango40Warning,\n                stacklevel=2,\n            )\n        return self.__getattr__('PASSWORD_RESET_TIMEOUT_DAYS')\n\n\nclass Settings:\n    def __init__(self, settings_module):\n        # update this dict from global settings (but only for ALL_CAPS settings)\n        for setting in dir(global_settings):\n            if setting.isupper():\n                setattr(self, setting, getattr(global_settings, setting))\n\n        # store the settings module in case someone later cares\n        self.SETTINGS_MODULE = settings_module\n\n        mod = importlib.import_module(self.SETTINGS_MODULE)\n\n        tuple_settings = (\n            \"INSTALLED_APPS\",\n            \"TEMPLATE_DIRS\",\n            \"LOCALE_PATHS\",\n        )\n        self._explicit_settings = set()\n        for setting in dir(mod):\n            if setting.isupper():\n                setting_value = getattr(mod, setting)\n\n                if (setting in tuple_settings and\n                        not isinstance(setting_value, (list, tuple))):\n                    raise ImproperlyConfigured(\"The %s setting must be a list or a tuple. \" % setting)\n                setattr(self, setting, setting_value)\n                self._explicit_settings.add(setting)\n\n        if not self.SECRET_KEY:\n            raise ImproperlyConfigured(\"The SECRET_KEY setting must not be empty.\")\n\n(B)         configured settings manually.\n        \"\"\"\n        settings_module = os.environ.get(ENVIRONMENT_VARIABLE)\n        if not settings_module:\n            desc = (\"setting %s\" % name) if name else \"settings\"\n            raise ImproperlyConfigured(\n                \"Requested %s, but settings are not configured. \"\n                \"You must either define the environment variable %s \"\n                \"or call settings.configure() before accessing settings.\"\n                % (desc, ENVIRONMENT_VARIABLE))\n\n        self._wrapped = Settings(settings_module)\n\n    def __repr__(self):\n        # Hardcode the class name as otherwise it yields 'Settings'.\n        if self._wrapped is empty:\n            return '<LazySettings [Unevaluated]>'\n        return '<LazySettings \"%(settings_module)s\">' % {\n            'settings_module': self._wrapped.SETTINGS_MODULE,\n        }\n\n    def __getattr__(self, name):\n        \"\"\"Return the value of a setting and cache it in self.__dict__.\"\"\"\n        if self._wrapped is empty:\n            self._setup(name)\n        val = getattr(self._wrapped, name)\n        self.__dict__[name] = val\n        return val\n\n    def __setattr__(self, name, value):\n        \"\"\"\n        Set the value of setting. Clear all cached values if _wrapped changes\n        (@override_settings does this) or clear single values when set.\n        \"\"\"\n        if name == '_wrapped':\n            self.__dict__.clear()\n        else:\n            self.__dict__.pop(name, None)\n        super().__setattr__(name, value)\n\n    def __delattr__(self, name):\n        \"\"\"Delete a setting and clear it from cache if needed.\"\"\"\n        super().__delattr__(name)\n        self.__dict__.pop(name, None)\n\n    def configure(self, default_settings=global_settings, **options):\n        \"\"\"\n        Called to manually configure the settings. The 'default_settings'\n        parameter sets where to retrieve any unspecified values from (its\n        argument must support attribute access (__getattr__)).\n        \"\"\"\n        if self._wrapped is not empty:\n            raise RuntimeError('Settings already configured.')\n        holder = UserSettingsHolder(default_settings)\n        for name, value in options.items():\n            if not name.isupper():\n                raise TypeError('Setting %r must be uppercase.' % name)\n            setattr(holder, name, value)\n        self._wrapped = holder\n\n    @property\n    def configured(self):\n        \"\"\"Return True if the settings have already been configured.\"\"\"\n        return self._wrapped is not empty\n\n    @property\n    def PASSWORD_RESET_TIMEOUT_DAYS(self):\n        stack = traceback.extract_stack()\n        # Show a warning if the setting is used outside of Django.\n        # Stack index: -1 this line, -2 the caller.\n        filename, _, _, _ = stack[-2]\n        if not filename.startswith(os.path.dirname(django.__file__)):\n            warnings.warn(\n                PASSWORD_RESET_TIMEOUT_DAYS_DEPRECATED_MSG,\n                RemovedInDjango40Warning,\n                stacklevel=2,\n            )\n        return self.__getattr__('PASSWORD_RESET_TIMEOUT_DAYS')\n\n\nclass Settings:\n    def __init__(self, settings_module):\n        # update this dict from global settings (but only for ALL_CAPS settings)\n        for setting in dir(global_settings):\n            if setting.isupper():\n                setattr(self, setting, getattr(global_settings, setting))\n\n        # store the settings module in case someone later cares\n        self.SETTINGS_MODULE = settings_module\n\n        mod = importlib.import_module(self.SETTINGS_MODULE)\n\n        tuple_settings = (\n            \"INSTALLED_APPS\",\n            \"TEMPLATE_DIRS\",\n            \"LOCALE_PATHS\",\n        )\n        self._explicit_settings = set()\n        for setting in dir(mod):\n            if setting.isupper():\n                setting_value = getattr(mod, setting)\n\n                if (setting in tuple_settings and\n                        not isinstance(setting_value, (list, tuple))):\n                    raise ImproperlyConfigured(\"The %s setting must be a list or a tuple. \" % setting)\n                setattr(self, setting, setting_value)\n                self._explicit_settings.add(setting)\n\n        if not self.SECRET_KEY:\n            raise ImproperlyConfigured(\"The SECRET_KEY setting must not be empty.\")\n\n        if self.is_overridden('PASSWORD_RESET_TIMEOUT_DAYS'):\n            if self.is_overridden('PASSWORD_RESET_TIMEOUT'):\n                raise ImproperlyConfigured(\n                    'PASSWORD_RESET_TIMEOUT_DAYS/PASSWORD_RESET_TIMEOUT are '\n                    'mutually exclusive.'\n                )\n            setattr(self, 'PASSWORD_RESET_TIMEOUT', self.PASSWORD_RESET_TIMEOUT_DAYS * 60 * 60 * 24)\n            warnings.warn(PASSWORD_RESET_TIMEOUT_DAYS_DEPRECATED_MSG, RemovedInDjango40Warning)\n\n        if hasattr(time, 'tzset') and self.TIME_ZONE:\n            # When we can, attempt to validate the timezone. If we can't find\n            # this file, no check happens and it's harmless.\n            zoneinfo_root = Path('/usr/share/zoneinfo')\n            zone_info_file = zoneinfo_root.joinpath(*self.TIME_ZONE.split('/'))\n            if zoneinfo_root.exists() and not zone_info_file.exists():\n                raise ValueError(\"Incorrect timezone setting: %s\" % self.TIME_ZONE)\n            # Move the time zone info into os.environ. See ticket #2315 for why\n            # we don't do this unconditionally (breaks Windows).\n            os.environ['TZ'] = self.TIME_ZONE\n            time.tzset()\n\n    def is_overridden(self, setting):\n        return setting in self._explicit_settings\n\n    def __repr__(self):\n        return '<%(cls)s \"%(settings_module)s\">' % {\n            'cls': self.__class__.__name__,\n            'settings_module': self.SETTINGS_MODULE,\n        }\n\n\nclass UserSettingsHolder:\n    \"\"\"Holder for user configured settings.\"\"\"\n    # SETTINGS_MODULE doesn't make much sense in the manually configured\n    # (standalone) case.\n    SETTINGS_MODULE = None\n\n(C)             self.__dict__.pop(name, None)\n        super().__setattr__(name, value)\n\n    def __delattr__(self, name):\n        \"\"\"Delete a setting and clear it from cache if needed.\"\"\"\n        super().__delattr__(name)\n        self.__dict__.pop(name, None)\n\n    def configure(self, default_settings=global_settings, **options):\n        \"\"\"\n        Called to manually configure the settings. The 'default_settings'\n        parameter sets where to retrieve any unspecified values from (its\n        argument must support attribute access (__getattr__)).\n        \"\"\"\n        if self._wrapped is not empty:\n            raise RuntimeError('Settings already configured.')\n        holder = UserSettingsHolder(default_settings)\n        for name, value in options.items():\n            if not name.isupper():\n                raise TypeError('Setting %r must be uppercase.' % name)\n            setattr(holder, name, value)\n        self._wrapped = holder\n\n    @property\n    def configured(self):\n        \"\"\"Return True if the settings have already been configured.\"\"\"\n        return self._wrapped is not empty\n\n    @property\n    def PASSWORD_RESET_TIMEOUT_DAYS(self):\n        stack = traceback.extract_stack()\n        # Show a warning if the setting is used outside of Django.\n        # Stack index: -1 this line, -2 the caller.\n        filename, _, _, _ = stack[-2]\n        if not filename.startswith(os.path.dirname(django.__file__)):\n            warnings.warn(\n                PASSWORD_RESET_TIMEOUT_DAYS_DEPRECATED_MSG,\n                RemovedInDjango40Warning,\n                stacklevel=2,\n            )\n        return self.__getattr__('PASSWORD_RESET_TIMEOUT_DAYS')\n\n\nclass Settings:\n    def __init__(self, settings_module):\n        # update this dict from global settings (but only for ALL_CAPS settings)\n        for setting in dir(global_settings):\n            if setting.isupper():\n                setattr(self, setting, getattr(global_settings, setting))\n\n        # store the settings module in case someone later cares\n        self.SETTINGS_MODULE = settings_module\n\n        mod = importlib.import_module(self.SETTINGS_MODULE)\n\n        tuple_settings = (\n            \"INSTALLED_APPS\",\n            \"TEMPLATE_DIRS\",\n            \"LOCALE_PATHS\",\n        )\n        self._explicit_settings = set()\n        for setting in dir(mod):\n            if setting.isupper():\n                setting_value = getattr(mod, setting)\n\n                if (setting in tuple_settings and\n                        not isinstance(setting_value, (list, tuple))):\n                    raise ImproperlyConfigured(\"The %s setting must be a list or a tuple. \" % setting)\n                setattr(self, setting, setting_value)\n                self._explicit_settings.add(setting)\n\n        if not self.SECRET_KEY:\n            raise ImproperlyConfigured(\"The SECRET_KEY setting must not be empty.\")\n\n        if self.is_overridden('PASSWORD_RESET_TIMEOUT_DAYS'):\n            if self.is_overridden('PASSWORD_RESET_TIMEOUT'):\n                raise ImproperlyConfigured(\n                    'PASSWORD_RESET_TIMEOUT_DAYS/PASSWORD_RESET_TIMEOUT are '\n                    'mutually exclusive.'\n                )\n            setattr(self, 'PASSWORD_RESET_TIMEOUT', self.PASSWORD_RESET_TIMEOUT_DAYS * 60 * 60 * 24)\n            warnings.warn(PASSWORD_RESET_TIMEOUT_DAYS_DEPRECATED_MSG, RemovedInDjango40Warning)\n\n        if hasattr(time, 'tzset') and self.TIME_ZONE:\n            # When we can, attempt to validate the timezone. If we can't find\n            # this file, no check happens and it's harmless.\n            zoneinfo_root = Path('/usr/share/zoneinfo')\n            zone_info_file = zoneinfo_root.joinpath(*self.TIME_ZONE.split('/'))\n            if zoneinfo_root.exists() and not zone_info_file.exists():\n                raise ValueError(\"Incorrect timezone setting: %s\" % self.TIME_ZONE)\n            # Move the time zone info into os.environ. See ticket #2315 for why\n            # we don't do this unconditionally (breaks Windows).\n            os.environ['TZ'] = self.TIME_ZONE\n            time.tzset()\n\n    def is_overridden(self, setting):\n        return setting in self._explicit_settings\n\n    def __repr__(self):\n        return '<%(cls)s \"%(settings_module)s\">' % {\n            'cls': self.__class__.__name__,\n            'settings_module': self.SETTINGS_MODULE,\n        }\n\n\nclass UserSettingsHolder:\n    \"\"\"Holder for user configured settings.\"\"\"\n    # SETTINGS_MODULE doesn't make much sense in the manually configured\n    # (standalone) case.\n    SETTINGS_MODULE = None\n\n    def __init__(self, default_settings):\n        \"\"\"\n        Requests for configuration variables not in this class are satisfied\n        from the module specified in default_settings (if possible).\n        \"\"\"\n        self.__dict__['_deleted'] = set()\n        self.default_settings = default_settings\n\n    def __getattr__(self, name):\n        if not name.isupper() or name in self._deleted:\n            raise AttributeError\n        return getattr(self.default_settings, name)\n\n    def __setattr__(self, name, value):\n        self._deleted.discard(name)\n        if name == 'PASSWORD_RESET_TIMEOUT_DAYS':\n            setattr(self, 'PASSWORD_RESET_TIMEOUT', value * 60 * 60 * 24)\n            warnings.warn(PASSWORD_RESET_TIMEOUT_DAYS_DEPRECATED_MSG, RemovedInDjango40Warning)\n        super().__setattr__(name, value)\n\n    def __delattr__(self, name):\n        self._deleted.add(name)\n        if hasattr(self, name):\n            super().__delattr__(name)\n\n    def __dir__(self):\n        return sorted(\n            s for s in [*self.__dict__, *dir(self.default_settings)]\n            if s not in self._deleted\n        )\n\n    def is_overridden(self, setting):\n        deleted = (setting in self._deleted)\n        set_locally = (setting in self.__dict__)\n        set_on_default = getattr(self.default_settings, 'is_overridden', lambda s: False)(setting)\n        return deleted or set_locally or set_on_default\n", "answer": "A"}
{"uuid": "120d5d47-7110-4f82-9aed-f55d96e27e16", "issue_statement": "Auto-reloading with StatReloader very intermittently throws \"ValueError: embedded null byte\".\nDescription\n\t\nRaising this mainly so that it's tracked, as I have no idea how to reproduce it, nor why it's happening. It ultimately looks like a problem with Pathlib, which wasn't used prior to 2.2.\nStacktrace:\nTraceback (most recent call last):\n File \"manage.py\" ...\n\texecute_from_command_line(sys.argv)\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/core/management/__init__.py\", line 381, in execute_from_command_line\n\tutility.execute()\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/core/management/__init__.py\", line 375, in execute\n\tself.fetch_command(subcommand).run_from_argv(self.argv)\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/core/management/base.py\", line 323, in run_from_argv\n\tself.execute(*args, **cmd_options)\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/core/management/commands/runserver.py\", line 60, in execute\n\tsuper().execute(*args, **options)\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/core/management/base.py\", line 364, in execute\n\toutput = self.handle(*args, **options)\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/core/management/commands/runserver.py\", line 95, in handle\n\tself.run(**options)\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/core/management/commands/runserver.py\", line 102, in run\n\tautoreload.run_with_reloader(self.inner_run, **options)\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/utils/autoreload.py\", line 577, in run_with_reloader\n\tstart_django(reloader, main_func, *args, **kwargs)\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/utils/autoreload.py\", line 562, in start_django\n\treloader.run(django_main_thread)\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/utils/autoreload.py\", line 280, in run\n\tself.run_loop()\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/utils/autoreload.py\", line 286, in run_loop\n\tnext(ticker)\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/utils/autoreload.py\", line 326, in tick\n\tfor filepath, mtime in self.snapshot_files():\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/utils/autoreload.py\", line 342, in snapshot_files\n\tfor file in self.watched_files():\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/utils/autoreload.py\", line 241, in watched_files\n\tyield from iter_all_python_module_files()\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/utils/autoreload.py\", line 103, in iter_all_python_module_files\n\treturn iter_modules_and_files(modules, frozenset(_error_files))\n File \"/Userz/kez/path/to/venv/lib/python3.6/site-packages/django/utils/autoreload.py\", line 132, in iter_modules_and_files\n\tresults.add(path.resolve().absolute())\n File \"/Users/kez/.pyenv/versions/3.6.2/lib/python3.6/pathlib.py\", line 1120, in resolve\n\ts = self._flavour.resolve(self, strict=strict)\n File \"/Users/kez/.pyenv/versions/3.6.2/lib/python3.6/pathlib.py\", line 346, in resolve\n\treturn _resolve(base, str(path)) or sep\n File \"/Users/kez/.pyenv/versions/3.6.2/lib/python3.6/pathlib.py\", line 330, in _resolve\n\ttarget = accessor.readlink(newpath)\n File \"/Users/kez/.pyenv/versions/3.6.2/lib/python3.6/pathlib.py\", line 441, in readlink\n\treturn os.readlink(path)\nValueError: embedded null byte\nI did print(path) before os.readlink(path) in pathlib and ended up with:\n/Users/kez\n/Users/kez/.pyenv\n/Users/kez/.pyenv/versions\n/Users/kez/.pyenv/versions/3.6.2\n/Users/kez/.pyenv/versions/3.6.2/lib\n/Users/kez/.pyenv/versions/3.6.2/lib/python3.6\n/Users/kez/.pyenv/versions/3.6.2/lib/python3.6/asyncio\n/Users/kez/.pyenv/versions/3.6.2/lib/python3.6/asyncio/selector_events.py\n/Users\nIt always seems to be /Users which is last\nIt may have already printed /Users as part of another .resolve() multiple times (that is, the order is not deterministic, and it may have traversed beyond /Users successfully many times during startup.\nI don't know where to begin looking for the rogue null byte, nor why it only exists sometimes.\nBest guess I have is that there's a mountpoint in /Users to a samba share which may not have been connected to yet? I dunno.\nI have no idea if it's fixable without removing the use of pathlib (which tbh I think should happen anyway, because it's slow) and reverting to using os.path.join and friends. \nI have no idea if it's fixed in a later Python version, but with no easy way to reproduce ... dunno how I'd check.\nI have no idea if it's something specific to my system (pyenv, OSX 10.11, etc)", "choices": "(A)             # The module could have been removed, don't fail loudly if this\n            # is the case.\n            continue\n        results.add(resolved_path)\n    return frozenset(results)\n\n\n@functools.lru_cache(maxsize=1)\ndef common_roots(paths):\n    \"\"\"\n(B) \n\n@functools.lru_cache(maxsize=1)\ndef common_roots(paths):\n    \"\"\"\n    Return a tuple of common roots that are shared between the given paths.\n    File system watchers operate on directories and aren't cheap to create.\n    Try to find the minimum set of directories to watch that encompass all of\n    the files that need to be watched.\n    \"\"\"\n(C)         try:\n            resolved_path = path.resolve(strict=True).absolute()\n        except FileNotFoundError:\n            # The module could have been removed, don't fail loudly if this\n            # is the case.\n            continue\n        results.add(resolved_path)\n    return frozenset(results)\n\n\n(D)         results.add(resolved_path)\n    return frozenset(results)\n\n\n@functools.lru_cache(maxsize=1)\ndef common_roots(paths):\n    \"\"\"\n    Return a tuple of common roots that are shared between the given paths.\n    File system watchers operate on directories and aren't cheap to create.\n    Try to find the minimum set of directories to watch that encompass all of", "answer": "A"}
{"uuid": "f21d817b-41e4-41b6-ae70-3bcf75e5e3f6", "issue_statement": "Django throws error when different apps with different models have the same name table name.\nDescription\n\t\nError message:\ntable_name: (models.E028) db_table 'table_name' is used by multiple models: base.ModelName, app2.ModelName.\nWe have a Base app that points to a central database and that has its own tables. We then have multiple Apps that talk to their own databases. Some share the same table names.\nWe have used this setup for a while, but after upgrading to Django 2.2 we're getting an error saying we're not allowed 2 apps, with 2 different models to have the same table names. \nIs this correct behavior? We've had to roll back to Django 2.0 for now.", "choices": "(A)             )\n    for index_name, model_labels in indexes.items():\n        if len(model_labels) > 1:\n            model_labels = set(model_labels)\n            errors.append(\n                Error(\n                    \"index name '%s' is not unique %s %s.\" % (\n                        index_name,\n                        'for model' if len(model_labels) == 1 else 'amongst models:',\n                        ', '.join(sorted(model_labels)),\n                    ),\n                    id='models.E029' if len(model_labels) == 1 else 'models.E030',\n                ),\n            )\n    for constraint_name, model_labels in constraints.items():\n        if len(model_labels) > 1:\n            model_labels = set(model_labels)\n            errors.append(\n                Error(\n                    \"constraint name '%s' is not unique %s %s.\" % (\n                        constraint_name,\n                        'for model' if len(model_labels) == 1 else 'amongst models:',\n                        ', '.join(sorted(model_labels)),\n                    ),\n                    id='models.E031' if len(model_labels) == 1 else 'models.E032',\n                ),\n            )\n    return errors\n\n\ndef _check_lazy_references(apps, ignore=None):\n    \"\"\"\n    Ensure all lazy (i.e. string) model references have been resolved.\n\n    Lazy references are used in various places throughout Django, primarily in\n    related fields and model signals. Identify those common cases and provide\n    more helpful error messages for them.\n\n    The ignore parameter is used by StateApps to exclude swappable models from\n    this check.\n    \"\"\"\n    pending_models = set(apps._pending_operations) - (ignore or set())\n\n    # Short circuit if there aren't any errors.\n    if not pending_models:\n        return []\n\n    from django.db.models import signals\n    model_signals = {\n        signal: name for name, signal in vars(signals).items()\n        if isinstance(signal, signals.ModelSignal)\n    }\n\n    def extract_operation(obj):\n        \"\"\"\n        Take a callable found in Apps._pending_operations and identify the\n        original callable passed to Apps.lazy_model_operation(). If that\n(B)             errors.extend(model.check(**kwargs))\n        for model_index in model._meta.indexes:\n            indexes[model_index.name].append(model._meta.label)\n        for model_constraint in model._meta.constraints:\n            constraints[model_constraint.name].append(model._meta.label)\n    for db_table, model_labels in db_table_models.items():\n        if len(model_labels) != 1:\n            errors.append(\n                Error(\n                    \"db_table '%s' is used by multiple models: %s.\"\n                    % (db_table, ', '.join(db_table_models[db_table])),\n                    obj=db_table,\n                    id='models.E028',\n                )\n            )\n    for index_name, model_labels in indexes.items():\n        if len(model_labels) > 1:\n            model_labels = set(model_labels)\n            errors.append(\n                Error(\n                    \"index name '%s' is not unique %s %s.\" % (\n                        index_name,\n                        'for model' if len(model_labels) == 1 else 'amongst models:',\n                        ', '.join(sorted(model_labels)),\n                    ),\n                    id='models.E029' if len(model_labels) == 1 else 'models.E030',\n                ),\n            )\n    for constraint_name, model_labels in constraints.items():\n        if len(model_labels) > 1:\n            model_labels = set(model_labels)\n            errors.append(\n                Error(\n                    \"constraint name '%s' is not unique %s %s.\" % (\n                        constraint_name,\n                        'for model' if len(model_labels) == 1 else 'amongst models:',\n                        ', '.join(sorted(model_labels)),\n                    ),\n                    id='models.E031' if len(model_labels) == 1 else 'models.E032',\n                ),\n            )\n    return errors\n\n\ndef _check_lazy_references(apps, ignore=None):\n    \"\"\"\n    Ensure all lazy (i.e. string) model references have been resolved.\n\n    Lazy references are used in various places throughout Django, primarily in\n    related fields and model signals. Identify those common cases and provide\n    more helpful error messages for them.\n\n    The ignore parameter is used by StateApps to exclude swappable models from\n    this check.\n    \"\"\"\n    pending_models = set(apps._pending_operations) - (ignore or set())\n\n(C) from itertools import chain\n\nfrom django.apps import apps\nfrom django.core.checks import Error, Tags, register\n\n\n@register(Tags.models)\ndef check_all_models(app_configs=None, **kwargs):\n    db_table_models = defaultdict(list)\n    indexes = defaultdict(list)\n    constraints = defaultdict(list)\n    errors = []\n    if app_configs is None:\n        models = apps.get_models()\n    else:\n        models = chain.from_iterable(app_config.get_models() for app_config in app_configs)\n    for model in models:\n        if model._meta.managed and not model._meta.proxy:\n            db_table_models[model._meta.db_table].append(model._meta.label)\n        if not inspect.ismethod(model.check):\n            errors.append(\n                Error(\n                    \"The '%s.check()' class method is currently overridden by %r.\"\n                    % (model.__name__, model.check),\n                    obj=model,\n                    id='models.E020'\n                )\n            )\n        else:\n            errors.extend(model.check(**kwargs))\n        for model_index in model._meta.indexes:\n            indexes[model_index.name].append(model._meta.label)\n        for model_constraint in model._meta.constraints:\n            constraints[model_constraint.name].append(model._meta.label)\n    for db_table, model_labels in db_table_models.items():\n        if len(model_labels) != 1:\n            errors.append(\n                Error(\n                    \"db_table '%s' is used by multiple models: %s.\"\n                    % (db_table, ', '.join(db_table_models[db_table])),\n                    obj=db_table,\n                    id='models.E028',\n                )\n            )\n    for index_name, model_labels in indexes.items():\n        if len(model_labels) > 1:\n            model_labels = set(model_labels)\n            errors.append(\n                Error(\n                    \"index name '%s' is not unique %s %s.\" % (\n                        index_name,\n                        'for model' if len(model_labels) == 1 else 'amongst models:',\n                        ', '.join(sorted(model_labels)),\n                    ),\n                    id='models.E029' if len(model_labels) == 1 else 'models.E030',\n                ),\n            )\n(D)         models = chain.from_iterable(app_config.get_models() for app_config in app_configs)\n    for model in models:\n        if model._meta.managed and not model._meta.proxy:\n            db_table_models[model._meta.db_table].append(model._meta.label)\n        if not inspect.ismethod(model.check):\n            errors.append(\n                Error(\n                    \"The '%s.check()' class method is currently overridden by %r.\"\n                    % (model.__name__, model.check),\n                    obj=model,\n                    id='models.E020'\n                )\n            )\n        else:\n            errors.extend(model.check(**kwargs))\n        for model_index in model._meta.indexes:\n            indexes[model_index.name].append(model._meta.label)\n        for model_constraint in model._meta.constraints:\n            constraints[model_constraint.name].append(model._meta.label)\n    for db_table, model_labels in db_table_models.items():\n        if len(model_labels) != 1:\n            errors.append(\n                Error(\n                    \"db_table '%s' is used by multiple models: %s.\"\n                    % (db_table, ', '.join(db_table_models[db_table])),\n                    obj=db_table,\n                    id='models.E028',\n                )\n            )\n    for index_name, model_labels in indexes.items():\n        if len(model_labels) > 1:\n            model_labels = set(model_labels)\n            errors.append(\n                Error(\n                    \"index name '%s' is not unique %s %s.\" % (\n                        index_name,\n                        'for model' if len(model_labels) == 1 else 'amongst models:',\n                        ', '.join(sorted(model_labels)),\n                    ),\n                    id='models.E029' if len(model_labels) == 1 else 'models.E030',\n                ),\n            )\n    for constraint_name, model_labels in constraints.items():\n        if len(model_labels) > 1:\n            model_labels = set(model_labels)\n            errors.append(\n                Error(\n                    \"constraint name '%s' is not unique %s %s.\" % (\n                        constraint_name,\n                        'for model' if len(model_labels) == 1 else 'amongst models:',\n                        ', '.join(sorted(model_labels)),\n                    ),\n                    id='models.E031' if len(model_labels) == 1 else 'models.E032',\n                ),\n            )\n    return errors\n", "answer": "C"}
{"uuid": "0fdfec5a-eea3-4faa-ae1b-1f8e2d320b5c", "issue_statement": "Add check to ensure max_length fits longest choice.\nDescription\n\t\nThere is currently no check to ensure that Field.max_length is large enough to fit the longest value in Field.choices.\nThis would be very helpful as often this mistake is not noticed until an attempt is made to save a record with those values that are too long.", "choices": "(A)                     is_value(value) and is_value(human_name)\n                    for value, human_name in group_choices\n                ):\n                    break\n            except (TypeError, ValueError):\n                # No groups, choices in the form [value, display]\n                value, human_name = group_name, group_choices\n                if not is_value(value) or not is_value(human_name):\n                    break\n\n            # Special case: choices=['ab']\n            if isinstance(choices_group, str):\n                break\n        else:\n            return []\n\n        return [\n            checks.Error(\n                \"'choices' must be an iterable containing \"\n                \"(actual value, human readable name) tuples.\",\n                obj=self,\n                id='fields.E005',\n            )\n        ]\n\n    def _check_db_index(self):\n        if self.db_index not in (None, True, False):\n            return [\n                checks.Error(\n                    \"'db_index' must be None, True or False.\",\n                    obj=self,\n                    id='fields.E006',\n                )\n            ]\n        else:\n            return []\n\n    def _check_null_allowed_for_primary_keys(self):\n        if (self.primary_key and self.null and\n                not connection.features.interprets_empty_strings_as_nulls):\n            # We cannot reliably check this for backends like Oracle which\n            # consider NULL and '' to be equal (and thus set up\n            # character-based fields a little differently).\n            return [\n                checks.Error(\n                    'Primary keys must not have null=True.',\n(B)             if isinstance(choices_group, str):\n                break\n        else:\n            return []\n\n        return [\n            checks.Error(\n                \"'choices' must be an iterable containing \"\n                \"(actual value, human readable name) tuples.\",\n                obj=self,\n                id='fields.E005',\n            )\n        ]\n\n    def _check_db_index(self):\n        if self.db_index not in (None, True, False):\n            return [\n                checks.Error(\n                    \"'db_index' must be None, True or False.\",\n                    obj=self,\n                    id='fields.E006',\n                )\n            ]\n        else:\n            return []\n\n    def _check_null_allowed_for_primary_keys(self):\n        if (self.primary_key and self.null and\n                not connection.features.interprets_empty_strings_as_nulls):\n            # We cannot reliably check this for backends like Oracle which\n            # consider NULL and '' to be equal (and thus set up\n            # character-based fields a little differently).\n            return [\n                checks.Error(\n                    'Primary keys must not have null=True.',\n                    hint=('Set null=False on the field, or '\n                          'remove primary_key=True argument.'),\n                    obj=self,\n                    id='fields.E007',\n                )\n            ]\n        else:\n            return []\n\n    def _check_backend_specific_checks(self, **kwargs):\n        app_label = self.model._meta.app_label\n(C)                 )\n            ]\n\n        # Expect [group_name, [value, display]]\n        for choices_group in self.choices:\n            try:\n                group_name, group_choices = choices_group\n            except (TypeError, ValueError):\n                # Containing non-pairs\n                break\n            try:\n                if not all(\n                    is_value(value) and is_value(human_name)\n                    for value, human_name in group_choices\n                ):\n                    break\n            except (TypeError, ValueError):\n                # No groups, choices in the form [value, display]\n                value, human_name = group_name, group_choices\n                if not is_value(value) or not is_value(human_name):\n                    break\n\n            # Special case: choices=['ab']\n            if isinstance(choices_group, str):\n                break\n        else:\n            return []\n\n        return [\n            checks.Error(\n                \"'choices' must be an iterable containing \"\n                \"(actual value, human readable name) tuples.\",\n                obj=self,\n                id='fields.E005',\n            )\n        ]\n\n    def _check_db_index(self):\n        if self.db_index not in (None, True, False):\n            return [\n                checks.Error(\n                    \"'db_index' must be None, True or False.\",\n                    obj=self,\n                    id='fields.E006',\n                )\n            ]\n(D)         if not self.choices:\n            return []\n\n        def is_value(value, accept_promise=True):\n            return isinstance(value, (str, Promise) if accept_promise else str) or not is_iterable(value)\n\n        if is_value(self.choices, accept_promise=False):\n            return [\n                checks.Error(\n                    \"'choices' must be an iterable (e.g., a list or tuple).\",\n                    obj=self,\n                    id='fields.E004',\n                )\n            ]\n\n        # Expect [group_name, [value, display]]\n        for choices_group in self.choices:\n            try:\n                group_name, group_choices = choices_group\n            except (TypeError, ValueError):\n                # Containing non-pairs\n                break\n            try:\n                if not all(\n                    is_value(value) and is_value(human_name)\n                    for value, human_name in group_choices\n                ):\n                    break\n            except (TypeError, ValueError):\n                # No groups, choices in the form [value, display]\n                value, human_name = group_name, group_choices\n                if not is_value(value) or not is_value(human_name):\n                    break\n\n            # Special case: choices=['ab']\n            if isinstance(choices_group, str):\n                break\n        else:\n            return []\n\n        return [\n            checks.Error(\n                \"'choices' must be an iterable containing \"\n                \"(actual value, human readable name) tuples.\",\n                obj=self,\n                id='fields.E005',", "answer": "C"}
{"uuid": "684c4604-448c-427f-bc97-5d518b5662cf", "issue_statement": "Filtering on query result overrides GROUP BY of internal query\nDescription\n\t\nfrom django.contrib.auth import models\na = models.User.objects.filter(email__isnull=True).values('email').annotate(m=Max('id')).values('m')\nprint(a.query) # good\n# SELECT MAX(\"auth_user\".\"id\") AS \"m\" FROM \"auth_user\" WHERE \"auth_user\".\"email\" IS NULL GROUP BY \"auth_user\".\"email\"\nprint(a[:1].query) # good\n# SELECT MAX(\"auth_user\".\"id\") AS \"m\" FROM \"auth_user\" WHERE \"auth_user\".\"email\" IS NULL GROUP BY \"auth_user\".\"email\" LIMIT 1\nb = models.User.objects.filter(id=a[:1])\nprint(b.query) # GROUP BY U0.\"id\" should be GROUP BY U0.\"email\"\n# SELECT ... FROM \"auth_user\" WHERE \"auth_user\".\"id\" = (SELECT U0.\"id\" FROM \"auth_user\" U0 WHERE U0.\"email\" IS NULL GROUP BY U0.\"id\" LIMIT 1)", "choices": "(A)     lookup_name = 'exact'\n\n    def process_rhs(self, compiler, connection):\n        from django.db.models.sql.query import Query\n        if isinstance(self.rhs, Query):\n            if self.rhs.has_limit_one():\n                # The subquery must select only the pk.\n                self.rhs.clear_select_clause()\n                self.rhs.add_fields(['pk'])\n(B)         from django.db.models.sql.query import Query\n        if isinstance(self.rhs, Query):\n            if self.rhs.has_limit_one():\n                # The subquery must select only the pk.\n                self.rhs.clear_select_clause()\n                self.rhs.add_fields(['pk'])\n            else:\n                raise ValueError(\n                    'The QuerySet value for an exact lookup must be limited to '\n(C)                 self.rhs.add_fields(['pk'])\n            else:\n                raise ValueError(\n                    'The QuerySet value for an exact lookup must be limited to '\n                    'one result using slicing.'\n                )\n        return super().process_rhs(compiler, connection)\n\n\n(D)                 # The subquery must select only the pk.\n                self.rhs.clear_select_clause()\n                self.rhs.add_fields(['pk'])\n            else:\n                raise ValueError(\n                    'The QuerySet value for an exact lookup must be limited to '\n                    'one result using slicing.'\n                )\n        return super().process_rhs(compiler, connection)", "answer": "B"}
{"uuid": "8da10ebd-5c26-4766-aac4-fb2023ffd73b", "issue_statement": "Migrations uses value of enum object instead of its name.\nDescription\n\t \n\t\t(last modified by oasl)\n\t \nWhen using Enum object as a default value for a CharField, the generated migration file uses the value of the Enum object instead of the its name. This causes a problem when using Django translation on the value of the Enum object. \nThe problem is that, when the Enum object value get translated to the users language, the old migration files raise an error stating that the Enum does not have the corresponding value. (because the Enum value is translated to another language)\nExample:\nLet say we have this code in models.py:\nfrom enum import Enum\nfrom django.utils.translation import gettext_lazy as _\nfrom django.db import models\nclass Status(Enum):\n\tGOOD = _('Good') # 'Good' will be translated\n\tBAD = _('Bad') # 'Bad' will be translated\n\tdef __str__(self):\n\t\treturn self.name\nclass Item(models.Model):\n\tstatus = models.CharField(default=Status.GOOD, max_length=128)\nIn the generated migration file, the code will be:\n...\n('status', models.CharField(default=Status('Good'), max_length=128))\n...\nAfter the translation, 'Good' will be translated to another word and it will not be part of the Status Enum class any more, so the migration file will raise the error on the previous line:\nValueError: 'Good' is not a valid Status\nShouldn't the code generated by the migration uses the name of the Status Enum 'GOOD', not the value of it, since it is changeable?\nIt should be:\n('status', models.CharField(default=Status['GOOD'], max_length=128))\nThis will be correct regardless of the translated word", "choices": "(A)     def serialize(self):\n        enum_class = self.value.__class__\n        module = enum_class.__module__\n        v_string, v_imports = serializer_factory(self.value.value).serialize()\n        imports = {'import %s' % module, *v_imports}\n        return \"%s.%s(%s)\" % (module, enum_class.__name__, v_string), imports\n\n\nclass FloatSerializer(BaseSimpleSerializer):\n    def serialize(self):\n(B)         return \"%s.%s(%s)\" % (module, enum_class.__name__, v_string), imports\n\n\nclass FloatSerializer(BaseSimpleSerializer):\n    def serialize(self):\n        if math.isnan(self.value) or math.isinf(self.value):\n            return 'float(\"{}\")'.format(self.value), set()\n        return super().serialize()\n\n\n(C) \n\nclass EnumSerializer(BaseSerializer):\n    def serialize(self):\n        enum_class = self.value.__class__\n        module = enum_class.__module__\n        v_string, v_imports = serializer_factory(self.value.value).serialize()\n        imports = {'import %s' % module, *v_imports}\n        return \"%s.%s(%s)\" % (module, enum_class.__name__, v_string), imports\n\n(D)         v_string, v_imports = serializer_factory(self.value.value).serialize()\n        imports = {'import %s' % module, *v_imports}\n        return \"%s.%s(%s)\" % (module, enum_class.__name__, v_string), imports\n\n\nclass FloatSerializer(BaseSimpleSerializer):\n    def serialize(self):\n        if math.isnan(self.value) or math.isinf(self.value):\n            return 'float(\"{}\")'.format(self.value), set()\n        return super().serialize()", "answer": "A"}
{"uuid": "5feb08ea-fc9a-4221-bd5e-ad448e0e06ac", "issue_statement": "django.utils.http.parse_http_date two digit year check is incorrect\nDescription\n\t \n\t\t(last modified by Ad Timmering)\n\t \nRFC 850 does not mention this, but in RFC 7231 (and there's something similar in RFC 2822), there's the following quote:\nRecipients of a timestamp value in rfc850-date format, which uses a\ntwo-digit year, MUST interpret a timestamp that appears to be more\nthan 50 years in the future as representing the most recent year in\nthe past that had the same last two digits.\nCurrent logic is hard coded to consider 0-69 to be in 2000-2069, and 70-99 to be 1970-1999, instead of comparing versus the current year.", "choices": "(A)                 year += 2000\n            else:\n                year += 1900\n        month = MONTHS.index(m.group('mon').lower()) + 1\n        day = int(m.group('day'))\n        hour = int(m.group('hour'))\n        min = int(m.group('min'))\n        sec = int(m.group('sec'))\n        result = datetime.datetime(year, month, day, hour, min, sec)\n        return calendar.timegm(result.utctimetuple())\n    except Exception as exc:\n        raise ValueError(\"%r is not a valid date\" % date) from exc\n\n\n(B)         month = MONTHS.index(m.group('mon').lower()) + 1\n        day = int(m.group('day'))\n        hour = int(m.group('hour'))\n        min = int(m.group('min'))\n        sec = int(m.group('sec'))\n        result = datetime.datetime(year, month, day, hour, min, sec)\n        return calendar.timegm(result.utctimetuple())\n    except Exception as exc:\n        raise ValueError(\"%r is not a valid date\" % date) from exc\n\n\ndef parse_http_date_safe(date):\n    \"\"\"\n    Same as parse_http_date, but return None if the input is invalid.\n(C)         if m is not None:\n            break\n    else:\n        raise ValueError(\"%r is not in a valid HTTP date format\" % date)\n    try:\n        year = int(m.group('year'))\n        if year < 100:\n            if year < 70:\n                year += 2000\n            else:\n                year += 1900\n        month = MONTHS.index(m.group('mon').lower()) + 1\n        day = int(m.group('day'))\n        hour = int(m.group('hour'))\n(D)     try:\n        year = int(m.group('year'))\n        if year < 100:\n            if year < 70:\n                year += 2000\n            else:\n                year += 1900\n        month = MONTHS.index(m.group('mon').lower()) + 1\n        day = int(m.group('day'))\n        hour = int(m.group('hour'))\n        min = int(m.group('min'))\n        sec = int(m.group('sec'))\n        result = datetime.datetime(year, month, day, hour, min, sec)\n        return calendar.timegm(result.utctimetuple())", "answer": "D"}
{"uuid": "2069ecce-77be-4387-9bbe-7947b3090dc2", "issue_statement": "ForeignKey's to_field parameter gets the old field's name when renaming a PrimaryKey.\nDescription\n\t\nHaving these two models \nclass ModelA(models.Model):\n\tfield_wrong = models.CharField('field1', max_length=50, primary_key=True) # I'm a Primary key.\nclass ModelB(models.Model):\n\tfield_fk = models.ForeignKey(ModelA, blank=True, null=True, on_delete=models.CASCADE) \n... migrations applyed ...\nthe ModelA.field_wrong field has been renamed ... and Django recognizes the \"renaming\"\n# Primary key renamed\nclass ModelA(models.Model):\n\tfield_fixed = models.CharField('field1', max_length=50, primary_key=True) # I'm a Primary key.\nAttempts to to_field parameter. \nThe to_field points to the old_name (field_typo) and not to the new one (\"field_fixed\")\nclass Migration(migrations.Migration):\n\tdependencies = [\n\t\t('app1', '0001_initial'),\n\t]\n\toperations = [\n\t\tmigrations.RenameField(\n\t\t\tmodel_name='modela',\n\t\t\told_name='field_wrong',\n\t\t\tnew_name='field_fixed',\n\t\t),\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='modelb',\n\t\t\tname='modela',\n\t\t\tfield=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, to='app1.ModelB', to_field='field_wrong'),\n\t\t),\n\t]", "choices": "(A)                 if remote_field_name:\n                    to_field_rename_key = rename_key + (remote_field_name,)\n                    if to_field_rename_key in self.renamed_fields:\n                        new_field.remote_field.field_name = old_field.remote_field.field_name\n                # Handle ForeignObjects which can have multiple from_fields/to_fields.\n                from_fields = getattr(new_field, 'from_fields', None)\n                if from_fields:\n                    from_rename_key = (app_label, model_name)\n                    new_field.from_fields = tuple([\n                        self.renamed_fields.get(from_rename_key + (from_field,), from_field)\n(B)                         new_field.remote_field.field_name = old_field.remote_field.field_name\n                # Handle ForeignObjects which can have multiple from_fields/to_fields.\n                from_fields = getattr(new_field, 'from_fields', None)\n                if from_fields:\n                    from_rename_key = (app_label, model_name)\n                    new_field.from_fields = tuple([\n                        self.renamed_fields.get(from_rename_key + (from_field,), from_field)\n                        for from_field in from_fields\n                    ])\n                    new_field.to_fields = tuple([\n(C)                 from_fields = getattr(new_field, 'from_fields', None)\n                if from_fields:\n                    from_rename_key = (app_label, model_name)\n                    new_field.from_fields = tuple([\n                        self.renamed_fields.get(from_rename_key + (from_field,), from_field)\n                        for from_field in from_fields\n                    ])\n                    new_field.to_fields = tuple([\n                        self.renamed_fields.get(rename_key + (to_field,), to_field)\n                        for to_field in new_field.to_fields\n(D)                     new_field.remote_field.model = old_field.remote_field.model\n                # Handle ForeignKey which can only have a single to_field.\n                remote_field_name = getattr(new_field.remote_field, 'field_name', None)\n                if remote_field_name:\n                    to_field_rename_key = rename_key + (remote_field_name,)\n                    if to_field_rename_key in self.renamed_fields:\n                        new_field.remote_field.field_name = old_field.remote_field.field_name\n                # Handle ForeignObjects which can have multiple from_fields/to_fields.\n                from_fields = getattr(new_field, 'from_fields', None)\n                if from_fields:", "answer": "A"}
{"uuid": "dd24774a-ad9a-4984-b0fd-b77cf6ac9a58", "issue_statement": "The value of a TextChoices/IntegerChoices field has a differing type\nDescription\n\t\nIf we create an instance of a model having a CharField or IntegerField with the keyword choices pointing to IntegerChoices or TextChoices, the value returned by the getter of the field will be of the same type as the one created by enum.Enum (enum value).\nFor example, this model:\nfrom django.db import models\nfrom django.utils.translation import gettext_lazy as _\nclass MyChoice(models.TextChoices):\n\tFIRST_CHOICE = \"first\", _(\"The first choice, it is\")\n\tSECOND_CHOICE = \"second\", _(\"The second choice, it is\")\nclass MyObject(models.Model):\n\tmy_str_value = models.CharField(max_length=10, choices=MyChoice.choices)\nThen this test:\nfrom django.test import TestCase\nfrom testing.pkg.models import MyObject, MyChoice\nclass EnumTest(TestCase):\n\tdef setUp(self) -> None:\n\t\tself.my_object = MyObject.objects.create(my_str_value=MyChoice.FIRST_CHOICE)\n\tdef test_created_object_is_str(self):\n\t\tmy_object = self.my_object\n\t\tself.assertIsInstance(my_object.my_str_value, str)\n\t\tself.assertEqual(str(my_object.my_str_value), \"first\")\n\tdef test_retrieved_object_is_str(self):\n\t\tmy_object = MyObject.objects.last()\n\t\tself.assertIsInstance(my_object.my_str_value, str)\n\t\tself.assertEqual(str(my_object.my_str_value), \"first\")\nAnd then the results:\n(django30-venv) \u279c django30 ./manage.py test\nCreating test database for alias 'default'...\nSystem check identified no issues (0 silenced).\nF.\n======================================================================\nFAIL: test_created_object_is_str (testing.tests.EnumTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n File \"/Users/mikailkocak/Development/django30/testing/tests.py\", line 14, in test_created_object_is_str\n\tself.assertEqual(str(my_object.my_str_value), \"first\")\nAssertionError: 'MyChoice.FIRST_CHOICE' != 'first'\n- MyChoice.FIRST_CHOICE\n+ first\n----------------------------------------------------------------------\nRan 2 tests in 0.002s\nFAILED (failures=1)\nWe notice when invoking __str__(...) we don't actually get the value property of the enum value which can lead to some unexpected issues, especially when communicating to an external API with a freshly created instance that will send MyEnum.MyValue, and the one that was retrieved would send my_value.", "choices": "(A)     def labels(cls):\n        return [label for _, label in cls.choices]\n\n    @property\n    def values(cls):\n        return [value for value, _ in cls.choices]\n\n\nclass Choices(enum.Enum, metaclass=ChoicesMeta):\n    \"\"\"Class for creating enumerated choices.\"\"\"\n    pass\n\n\n(B) \nclass Choices(enum.Enum, metaclass=ChoicesMeta):\n    \"\"\"Class for creating enumerated choices.\"\"\"\n    pass\n\n\nclass IntegerChoices(int, Choices):\n    \"\"\"Class for creating enumerated integer choices.\"\"\"\n    pass\n\n\nclass TextChoices(str, Choices):\n    \"\"\"Class for creating enumerated string choices.\"\"\"\n(C)     @property\n    def values(cls):\n        return [value for value, _ in cls.choices]\n\n\nclass Choices(enum.Enum, metaclass=ChoicesMeta):\n    \"\"\"Class for creating enumerated choices.\"\"\"\n    pass\n\n\nclass IntegerChoices(int, Choices):\n    \"\"\"Class for creating enumerated integer choices.\"\"\"\n    pass\n(D)         return empty + [(member.value, member.label) for member in cls]\n\n    @property\n    def labels(cls):\n        return [label for _, label in cls.choices]\n\n    @property\n    def values(cls):\n        return [value for value, _ in cls.choices]\n\n\nclass Choices(enum.Enum, metaclass=ChoicesMeta):\n    \"\"\"Class for creating enumerated choices.\"\"\"", "answer": "B"}
{"uuid": "73666ec0-7110-4ba5-9da6-518b04b76967", "issue_statement": "Cannot override get_FOO_display() in Django 2.2+.\nDescription\n\t\nI cannot override the get_FIELD_display function on models since version 2.2. It works in version 2.1.\nExample:\nclass FooBar(models.Model):\n\tfoo_bar = models.CharField(_(\"foo\"), choices=[(1, 'foo'), (2, 'bar')])\n\tdef __str__(self):\n\t\treturn self.get_foo_bar_display() # This returns 'foo' or 'bar' in 2.2, but 'something' in 2.1\n\tdef get_foo_bar_display(self):\n\t\treturn \"something\"\nWhat I expect is that I should be able to override this function.", "choices": "(A)             # Don't override classmethods with the descriptor. This means that\n            # if you have a classmethod and a field with the same name, then\n            # such fields can't be deferred (we don't have a check for this).\n            if not getattr(cls, self.attname, None):\n                setattr(cls, self.attname, self.descriptor_class(self))\n        if self.choices is not None:\n            setattr(cls, 'get_%s_display' % self.name,\n                    partialmethod(cls._get_FIELD_display, field=self))\n\n    def get_filter_kwargs_for_object(self, obj):\n        \"\"\"\n        Return a dict that when passed as kwargs to self.model.filter(), would\n(B)             setattr(cls, 'get_%s_display' % self.name,\n                    partialmethod(cls._get_FIELD_display, field=self))\n\n    def get_filter_kwargs_for_object(self, obj):\n        \"\"\"\n        Return a dict that when passed as kwargs to self.model.filter(), would\n        yield all instances having the same value for this field as obj has.\n        \"\"\"\n        return {self.name: getattr(obj, self.attname)}\n\n    def get_attname(self):\n        return self.name\n(C)     def get_filter_kwargs_for_object(self, obj):\n        \"\"\"\n        Return a dict that when passed as kwargs to self.model.filter(), would\n        yield all instances having the same value for this field as obj has.\n        \"\"\"\n        return {self.name: getattr(obj, self.attname)}\n\n    def get_attname(self):\n        return self.name\n\n    def get_attname_column(self):\n        attname = self.get_attname()\n(D)             if not getattr(cls, self.attname, None):\n                setattr(cls, self.attname, self.descriptor_class(self))\n        if self.choices is not None:\n            setattr(cls, 'get_%s_display' % self.name,\n                    partialmethod(cls._get_FIELD_display, field=self))\n\n    def get_filter_kwargs_for_object(self, obj):\n        \"\"\"\n        Return a dict that when passed as kwargs to self.model.filter(), would\n        yield all instances having the same value for this field as obj has.\n        \"\"\"\n        return {self.name: getattr(obj, self.attname)}", "answer": "D"}
{"uuid": "e4febfa7-e773-42c9-bbf6-30abc1c963cb", "issue_statement": "admin_views.test_multidb fails with persistent test SQLite database.\nDescription\n\t \n\t\t(last modified by Mariusz Felisiak)\n\t \nI've tried using persistent SQLite databases for the tests (to make use of\n--keepdb), but at least some test fails with:\nsqlite3.OperationalError: database is locked\nThis is not an issue when only using TEST[\"NAME\"] with \"default\" (which is good enough in terms of performance).\ndiff --git i/tests/test_sqlite.py w/tests/test_sqlite.py\nindex f1b65f7d01..9ce4e32e14 100644\n--- i/tests/test_sqlite.py\n+++ w/tests/test_sqlite.py\n@@ -15,9 +15,15 @@\n DATABASES = {\n\t 'default': {\n\t\t 'ENGINE': 'django.db.backends.sqlite3',\n+\t\t'TEST': {\n+\t\t\t'NAME': 'test_default.sqlite3'\n+\t\t},\n\t },\n\t 'other': {\n\t\t 'ENGINE': 'django.db.backends.sqlite3',\n+\t\t'TEST': {\n+\t\t\t'NAME': 'test_other.sqlite3'\n+\t\t},\n\t }\n }\n% tests/runtests.py admin_views.test_multidb -v 3 --keepdb --parallel 1\n\u2026\nOperations to perform:\n Synchronize unmigrated apps: admin_views, auth, contenttypes, messages, sessions, staticfiles\n Apply all migrations: admin, sites\nRunning pre-migrate handlers for application contenttypes\nRunning pre-migrate handlers for application auth\nRunning pre-migrate handlers for application sites\nRunning pre-migrate handlers for application sessions\nRunning pre-migrate handlers for application admin\nRunning pre-migrate handlers for application admin_views\nSynchronizing apps without migrations:\n Creating tables...\n\tRunning deferred SQL...\nRunning migrations:\n No migrations to apply.\nRunning post-migrate handlers for application contenttypes\nRunning post-migrate handlers for application auth\nRunning post-migrate handlers for application sites\nRunning post-migrate handlers for application sessions\nRunning post-migrate handlers for application admin\nRunning post-migrate handlers for application admin_views\nSystem check identified no issues (0 silenced).\nERROR\n======================================================================\nERROR: setUpClass (admin_views.test_multidb.MultiDatabaseTests)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n File \"\u2026/Vcs/django/django/db/backends/utils.py\", line 84, in _execute\n\treturn self.cursor.execute(sql, params)\n File \"\u2026/Vcs/django/django/db/backends/sqlite3/base.py\", line 391, in execute\n\treturn Database.Cursor.execute(self, query, params)\nsqlite3.OperationalError: database is locked\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n File \"\u2026/Vcs/django/django/test/testcases.py\", line 1137, in setUpClass\n\tcls.setUpTestData()\n File \"\u2026/Vcs/django/tests/admin_views/test_multidb.py\", line 40, in setUpTestData\n\tusername='admin', password='something', email='test@test.org',\n File \"\u2026/Vcs/django/django/contrib/auth/models.py\", line 158, in create_superuser\n\treturn self._create_user(username, email, password, **extra_fields)\n File \"\u2026/Vcs/django/django/contrib/auth/models.py\", line 141, in _create_user\n\tuser.save(using=self._db)\n File \"\u2026/Vcs/django/django/contrib/auth/base_user.py\", line 66, in save\n\tsuper().save(*args, **kwargs)\n File \"\u2026/Vcs/django/django/db/models/base.py\", line 741, in save\n\tforce_update=force_update, update_fields=update_fields)\n File \"\u2026/Vcs/django/django/db/models/base.py\", line 779, in save_base\n\tforce_update, using, update_fields,\n File \"\u2026/Vcs/django/django/db/models/base.py\", line 870, in _save_table\n\tresult = self._do_insert(cls._base_manager, using, fields, update_pk, raw)\n File \"\u2026/Vcs/django/django/db/models/base.py\", line 908, in _do_insert\n\tusing=using, raw=raw)\n File \"\u2026/Vcs/django/django/db/models/manager.py\", line 82, in manager_method\n\treturn getattr(self.get_queryset(), name)(*args, **kwargs)\n File \"\u2026/Vcs/django/django/db/models/query.py\", line 1175, in _insert\n\treturn query.get_compiler(using=using).execute_sql(return_id)\n File \"\u2026/Vcs/django/django/db/models/sql/compiler.py\", line 1321, in execute_sql\n\tcursor.execute(sql, params)\n File \"\u2026/Vcs/django/django/db/backends/utils.py\", line 67, in execute\n\treturn self._execute_with_wrappers(sql, params, many=False, executor=self._execute)\n File \"\u2026/Vcs/django/django/db/backends/utils.py\", line 76, in _execute_with_wrappers\n\treturn executor(sql, params, many, context)\n File \"\u2026/Vcs/django/django/db/backends/utils.py\", line 84, in _execute\n\treturn self.cursor.execute(sql, params)\n File \"\u2026/Vcs/django/django/db/utils.py\", line 89, in __exit__\n\traise dj_exc_value.with_traceback(traceback) from exc_value\n File \"\u2026/Vcs/django/django/db/backends/utils.py\", line 84, in _execute\n\treturn self.cursor.execute(sql, params)\n File \"\u2026/Vcs/django/django/db/backends/sqlite3/base.py\", line 391, in execute\n\treturn Database.Cursor.execute(self, query, params)\ndjango.db.utils.OperationalError: database is locked", "choices": "(A)         sig = [self.connection.settings_dict['NAME']]\n        if self.is_in_memory_db(test_database_name):\n            sig.append(self.connection.alias)\n        return tuple(sig)\n(B)         This takes into account the special cases of \":memory:\" and \"\" for\n        SQLite since the databases will be distinct despite having the same\n        TEST NAME. See https://www.sqlite.org/inmemorydb.html\n        \"\"\"\n        test_database_name = self._get_test_db_name()\n        sig = [self.connection.settings_dict['NAME']]\n(C)         TEST NAME. See https://www.sqlite.org/inmemorydb.html\n        \"\"\"\n        test_database_name = self._get_test_db_name()\n        sig = [self.connection.settings_dict['NAME']]\n        if self.is_in_memory_db(test_database_name):\n            sig.append(self.connection.alias)\n(D)         \"\"\"\n        test_database_name = self._get_test_db_name()\n        sig = [self.connection.settings_dict['NAME']]\n        if self.is_in_memory_db(test_database_name):\n            sig.append(self.connection.alias)\n        return tuple(sig)", "answer": "A"}
{"uuid": "478960e0-16c8-49a9-b959-23f9cabb6e73", "issue_statement": "makemigrations produces incorrect path for inner classes\nDescription\n\t\nWhen you define a subclass from django.db.models.Field as an inner class of some other class, and use this field inside a django.db.models.Model class, then when you run manage.py makemigrations, a migrations file is created which refers to the inner class as if it were a top-level class of the module it is in.\nTo reproduce, create the following as your model:\nclass Outer(object):\n\tclass Inner(models.CharField):\n\t\tpass\nclass A(models.Model):\n\tfield = Outer.Inner(max_length=20)\nAfter running manage.py makemigrations, the generated migrations file contains the following:\nmigrations.CreateModel(\n\tname='A',\n\tfields=[\n\t\t('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n\t\t('field', test1.models.Inner(max_length=20)),\n\t],\n),\nNote the test1.models.Inner, which should have been test1.models.Outer.Inner.\nThe real life case involved an EnumField from django-enumfields, defined as an inner class of a Django Model class, similar to this:\nimport enum\nfrom enumfields import Enum, EnumField\nclass Thing(models.Model):\n\t@enum.unique\n\tclass State(Enum):\n\t\ton = 'on'\n\t\toff = 'off'\n\tstate = EnumField(enum=State)\nThis results in the following migrations code:\nmigrations.CreateModel(\n\tname='Thing',\n\tfields=[\n\t\t('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n\t\t('state', enumfields.fields.EnumField(enum=test1.models.State, max_length=10)),\n\t],\n),\nThis refers to test1.models.State, instead of to test1.models.Thing.State.", "choices": "(A)             if module == builtins.__name__:\n                return self.value.__name__, set()\n            else:\n                return \"%s.%s\" % (module, self.value.__name__), {\"import %s\" % module}\n\n\nclass UUIDSerializer(BaseSerializer):\n(B) \n\nclass UUIDSerializer(BaseSerializer):\n    def serialize(self):\n        return \"uuid.%s\" % repr(self.value), {\"import uuid\"}\n\n\n(C)         if hasattr(self.value, \"__module__\"):\n            module = self.value.__module__\n            if module == builtins.__name__:\n                return self.value.__name__, set()\n            else:\n                return \"%s.%s\" % (module, self.value.__name__), {\"import %s\" % module}\n\n(D)             else:\n                return \"%s.%s\" % (module, self.value.__name__), {\"import %s\" % module}\n\n\nclass UUIDSerializer(BaseSerializer):\n    def serialize(self):\n        return \"uuid.%s\" % repr(self.value), {\"import uuid\"}", "answer": "A"}
{"uuid": "3fbd77ba-0072-4170-8f56-80a9535ae1b0", "issue_statement": "Optional URL params crash some view functions.\nDescription\n\t\nMy use case, running fine with Django until 2.2:\nURLConf:\nurlpatterns += [\n\t...\n\tre_path(r'^module/(?P<format>(html|json|xml))?/?$', views.modules, name='modules'),\n]\nView:\ndef modules(request, format='html'):\n\t...\n\treturn render(...)\nWith Django 3.0, this is now producing an error:\nTraceback (most recent call last):\n File \"/l10n/venv/lib/python3.6/site-packages/django/core/handlers/exception.py\", line 34, in inner\n\tresponse = get_response(request)\n File \"/l10n/venv/lib/python3.6/site-packages/django/core/handlers/base.py\", line 115, in _get_response\n\tresponse = self.process_exception_by_middleware(e, request)\n File \"/l10n/venv/lib/python3.6/site-packages/django/core/handlers/base.py\", line 113, in _get_response\n\tresponse = wrapped_callback(request, *callback_args, **callback_kwargs)\nException Type: TypeError at /module/\nException Value: modules() takes from 1 to 2 positional arguments but 3 were given", "choices": "(A)             kwargs = {k: v for k, v in match.groupdict().items() if v is not None}\n            args = () if kwargs else match.groups()\n            return path[match.end():], args, kwargs\n        return None\n\n    def check(self):\n        warnings = []\n        warnings.extend(self._check_pattern_startswith_slash())\n        if not self._is_endpoint:\n(B)             return path[match.end():], args, kwargs\n        return None\n\n    def check(self):\n        warnings = []\n        warnings.extend(self._check_pattern_startswith_slash())\n        if not self._is_endpoint:\n            warnings.extend(self._check_include_trailing_dollar())\n        return warnings\n(C)     def match(self, path):\n        match = self.regex.search(path)\n        if match:\n            # If there are any named groups, use those as kwargs, ignoring\n            # non-named groups. Otherwise, pass all non-named arguments as\n            # positional arguments.\n            kwargs = {k: v for k, v in match.groupdict().items() if v is not None}\n            args = () if kwargs else match.groups()\n            return path[match.end():], args, kwargs\n(D)             # If there are any named groups, use those as kwargs, ignoring\n            # non-named groups. Otherwise, pass all non-named arguments as\n            # positional arguments.\n            kwargs = {k: v for k, v in match.groupdict().items() if v is not None}\n            args = () if kwargs else match.groups()\n            return path[match.end():], args, kwargs\n        return None\n\n    def check(self):", "answer": "D"}
{"uuid": "7947292d-db59-4f4d-96b2-1340c5391610", "issue_statement": "Model.get_FOO_display() does not work correctly with inherited choices.\nDescription\n\t \n\t\t(last modified by Mariusz Felisiak)\n\t \nGiven a base model with choices A containing 3 tuples\nChild Model inherits the base model overrides the choices A and adds 2 more tuples\nget_foo_display does not work correctly for the new tuples added\nExample:\nclass A(models.Model):\n foo_choice = [(\"A\",\"output1\"),(\"B\",\"output2\")]\n field_foo = models.CharField(max_length=254,choices=foo_choice)\n class Meta:\n\t abstract = True\nclass B(A):\n foo_choice = [(\"A\",\"output1\"),(\"B\",\"output2\"),(\"C\",\"output3\")]\n field_foo = models.CharField(max_length=254,choices=foo_choice)\nUpon invoking get_field_foo_display() on instance of B , \nFor value \"A\" and \"B\" the output works correctly i.e. returns \"output1\" / \"output2\"\nbut for value \"C\" the method returns \"C\" and not \"output3\" which is the expected behaviour", "choices": "(A)             if not getattr(cls, self.attname, None):\n                setattr(cls, self.attname, self.descriptor_class(self))\n        if self.choices is not None:\n            if not hasattr(cls, 'get_%s_display' % self.name):\n                setattr(\n                    cls,\n                    'get_%s_display' % self.name,\n                    partialmethod(cls._get_FIELD_display, field=self),\n                )\n\n    def get_filter_kwargs_for_object(self, obj):\n(B)                     'get_%s_display' % self.name,\n                    partialmethod(cls._get_FIELD_display, field=self),\n                )\n\n    def get_filter_kwargs_for_object(self, obj):\n        \"\"\"\n        Return a dict that when passed as kwargs to self.model.filter(), would\n        yield all instances having the same value for this field as obj has.\n        \"\"\"\n        return {self.name: getattr(obj, self.attname)}\n\n(C)             if not hasattr(cls, 'get_%s_display' % self.name):\n                setattr(\n                    cls,\n                    'get_%s_display' % self.name,\n                    partialmethod(cls._get_FIELD_display, field=self),\n                )\n\n    def get_filter_kwargs_for_object(self, obj):\n        \"\"\"\n        Return a dict that when passed as kwargs to self.model.filter(), would\n        yield all instances having the same value for this field as obj has.\n(D)             # Don't override classmethods with the descriptor. This means that\n            # if you have a classmethod and a field with the same name, then\n            # such fields can't be deferred (we don't have a check for this).\n            if not getattr(cls, self.attname, None):\n                setattr(cls, self.attname, self.descriptor_class(self))\n        if self.choices is not None:\n            if not hasattr(cls, 'get_%s_display' % self.name):\n                setattr(\n                    cls,\n                    'get_%s_display' % self.name,\n                    partialmethod(cls._get_FIELD_display, field=self),", "answer": "A"}
{"uuid": "5c0a155f-9283-4ed5-825d-089b649d1588", "issue_statement": "JSONField are not properly displayed in admin when they are readonly.\nDescription\n\t\nJSONField values are displayed as dict when readonly in the admin.\nFor example, {\"foo\": \"bar\"} would be displayed as {'foo': 'bar'}, which is not valid JSON.\nI believe the fix would be to add a special case in django.contrib.admin.utils.display_for_field to call the prepare_value of the JSONField (not calling json.dumps directly to take care of the InvalidJSONInput case).", "choices": "(A)         return formats.number_format(value)\n    elif isinstance(field, models.FileField) and value:\n        return format_html('<a href=\"{}\">{}</a>', value.url, value)\n    else:\n        return display_for_value(value, empty_value_display)\n\n\ndef display_for_value(value, empty_value_display, boolean=False):\n    from django.contrib.admin.templatetags.admin_list import _boolean_icon\n\n    if boolean:\n(B)     else:\n        return display_for_value(value, empty_value_display)\n\n\ndef display_for_value(value, empty_value_display, boolean=False):\n    from django.contrib.admin.templatetags.admin_list import _boolean_icon\n\n    if boolean:\n        return _boolean_icon(value)\n    elif value is None:\n        return empty_value_display\n(C)     elif isinstance(field, models.DecimalField):\n        return formats.number_format(value, field.decimal_places)\n    elif isinstance(field, (models.IntegerField, models.FloatField)):\n        return formats.number_format(value)\n    elif isinstance(field, models.FileField) and value:\n        return format_html('<a href=\"{}\">{}</a>', value.url, value)\n    else:\n        return display_for_value(value, empty_value_display)\n\n\ndef display_for_value(value, empty_value_display, boolean=False):\n(D) \ndef display_for_value(value, empty_value_display, boolean=False):\n    from django.contrib.admin.templatetags.admin_list import _boolean_icon\n\n    if boolean:\n        return _boolean_icon(value)\n    elif value is None:\n        return empty_value_display\n    elif isinstance(value, bool):\n        return str(value)\n    elif isinstance(value, datetime.datetime):", "answer": "A"}
{"uuid": "24604a66-5a22-4080-b0d7-b08206a03782", "issue_statement": "`TransactionTestCase.serialized_rollback` fails to restore objects due to ordering constraints\nDescription\n\t\nI hit this problem in a fairly complex projet and haven't had the time to write a minimal reproduction case. I think it can be understood just by inspecting the code so I'm going to describe it while I have it in mind.\nSetting serialized_rollback = True on a TransactionTestCase triggers \u200brollback emulation. In practice, for each database:\nBaseDatabaseCreation.create_test_db calls connection._test_serialized_contents = connection.creation.serialize_db_to_string()\nTransactionTestCase._fixture_setup calls connection.creation.deserialize_db_from_string(connection._test_serialized_contents)\n(The actual code isn't written that way; it's equivalent but the symmetry is less visible.)\nserialize_db_to_string orders models with serializers.sort_dependencies and serializes them. The sorting algorithm only deals with natural keys. It doesn't do anything to order models referenced by foreign keys before models containing said foreign keys. That wouldn't be possible in general because circular foreign keys are allowed.\ndeserialize_db_from_string deserializes and saves models without wrapping in a transaction. This can result in integrity errors if an instance containing a foreign key is saved before the instance it references. I'm suggesting to fix it as follows:\ndiff --git a/django/db/backends/base/creation.py b/django/db/backends/base/creation.py\nindex bca8376..7bed2be 100644\n--- a/django/db/backends/base/creation.py\n+++ b/django/db/backends/base/creation.py\n@@ -4,7 +4,7 @@ import time\n from django.apps import apps\n from django.conf import settings\n from django.core import serializers\n-from django.db import router\n+from django.db import router, transaction\n from django.utils.six import StringIO\n from django.utils.six.moves import input\n \n@@ -128,8 +128,9 @@ class BaseDatabaseCreation(object):\n\t\t the serialize_db_to_string method.\n\t\t \"\"\"\n\t\t data = StringIO(data)\n-\t\tfor obj in serializers.deserialize(\"json\", data, using=self.connection.alias):\n-\t\t\tobj.save()\n+\t\twith transaction.atomic(using=self.connection.alias):\n+\t\t\tfor obj in serializers.deserialize(\"json\", data, using=self.connection.alias):\n+\t\t\t\tobj.save()\n \n\t def _get_database_display_str(self, verbosity, database_name):\n\t\t \"\"\"\nNote that loaddata doesn't have this problem because it wraps everything in a transaction:\n\tdef handle(self, *fixture_labels, **options):\n\t\t# ...\n\t\twith transaction.atomic(using=self.using):\n\t\t\tself.loaddata(fixture_labels)\n\t\t# ...\nThis suggest that the transaction was just forgotten in the implementation of deserialize_db_from_string.\nIt should be possible to write a deterministic test for this bug because the order in which serialize_db_to_string serializes models depends on the app registry, and the app registry uses OrderedDict to store apps and models in a deterministic order.", "choices": "(A)             if keepdb:\n                action = \"Using existing\"\n\n            self.log('%s test database for alias %s...' % (\n                action,\n                self._get_database_display_str(verbosity, test_database_name),\n            ))\n\n        # We could skip this call if keepdb is True, but we instead\n        # give it the keepdb param. This is to handle the case\n        # where the test DB doesn't exist, in which case we need to\n        # create it, then just not destroy it. If we instead skip\n        # this, we will get an exception.\n        self._create_test_db(verbosity, autoclobber, keepdb)\n\n        self.connection.close()\n        settings.DATABASES[self.connection.alias][\"NAME\"] = test_database_name\n        self.connection.settings_dict[\"NAME\"] = test_database_name\n\n        if self.connection.settings_dict['TEST']['MIGRATE']:\n            # We report migrate messages at one level lower than that\n            # requested. This ensures we don't get flooded with messages during\n            # testing (unless you really ask to be flooded).\n            call_command(\n                'migrate',\n                verbosity=max(verbosity - 1, 0),\n                interactive=False,\n                database=self.connection.alias,\n                run_syncdb=True,\n            )\n\n        # We then serialize the current state of the database into a string\n        # and store it on the connection. This slightly horrific process is so people\n        # who are testing on databases without transactions or who are using\n        # a TransactionTestCase still get a clean database on every test run.\n        if serialize:\n            self.connection._test_serialized_contents = self.serialize_db_to_string()\n\n        call_command('createcachetable', database=self.connection.alias)\n\n        # Ensure a connection for the side effect of initializing the test database.\n        self.connection.ensure_connection()\n\n        return test_database_name\n\n    def set_as_test_mirror(self, primary_settings_dict):\n        \"\"\"\n        Set this database up to be used in testing as a mirror of a primary\n        database whose settings are given.\n        \"\"\"\n        self.connection.settings_dict['NAME'] = primary_settings_dict['NAME']\n\n    def serialize_db_to_string(self):\n        \"\"\"\n        Serialize all data in the database into a JSON string.\n        Designed only for test runner usage; will not handle large\n        amounts of data.\n        \"\"\"\n        # Build list of all apps to serialize\n        from django.db.migrations.loader import MigrationLoader\n        loader = MigrationLoader(self.connection)\n        app_list = []\n        for app_config in apps.get_app_configs():\n            if (\n                app_config.models_module is not None and\n                app_config.label in loader.migrated_apps and\n                app_config.name not in settings.TEST_NON_SERIALIZED_APPS\n            ):\n                app_list.append((app_config, None))\n\n        # Make a function to iteratively return every object\n        def get_objects():\n            for model in serializers.sort_dependencies(app_list):\n                if (model._meta.can_migrate(self.connection) and\n                        router.allow_migrate_model(self.connection.alias, model)):\n                    queryset = model._default_manager.using(self.connection.alias).order_by(model._meta.pk.name)\n                    yield from queryset.iterator()\n        # Serialize to a string\n        out = StringIO()\n        serializers.serialize(\"json\", get_objects(), indent=None, stream=out)\n        return out.getvalue()\n\n    def deserialize_db_from_string(self, data):\n        \"\"\"\n        Reload the database with data from a string generated by\n        the serialize_db_to_string() method.\n        \"\"\"\n        data = StringIO(data)\n        for obj in serializers.deserialize(\"json\", data, using=self.connection.alias):\n            obj.save()\n\n    def _get_database_display_str(self, verbosity, database_name):\n        \"\"\"\n        Return display string for a database for use in various actions.\n        \"\"\"\n        return \"'%s'%s\" % (\n            self.connection.alias,\n            (\" ('%s')\" % database_name) if verbosity >= 2 else '',\n        )\n\n    def _get_test_db_name(self):\n        \"\"\"\n        Internal implementation - return the name of the test DB that will be\n        created. Only useful when called from create_test_db() and\n        _create_test_db() and when no external munging is done with the 'NAME'\n        settings.\n        \"\"\"\n        if self.connection.settings_dict['TEST']['NAME']:\n            return self.connection.settings_dict['TEST']['NAME']\n        return TEST_DATABASE_PREFIX + self.connection.settings_dict['NAME']\n\n    def _execute_create_test_db(self, cursor, parameters, keepdb=False):\n        cursor.execute('CREATE DATABASE %(dbname)s %(suffix)s' % parameters)\n\n    def _create_test_db(self, verbosity, autoclobber, keepdb=False):\n        \"\"\"\n        Internal implementation - create the test db tables.\n        \"\"\"\n        test_database_name = self._get_test_db_name()\n        test_db_params = {\n            'dbname': self.connection.ops.quote_name(test_database_name),\n            'suffix': self.sql_table_creation_suffix(),\n        }\n        # Create the test database and connect to it.\n        with self._nodb_cursor() as cursor:\n            try:\n                self._execute_create_test_db(cursor, test_db_params, keepdb)\n            except Exception as e:\n                # if we want to keep the db, then no need to do any of the below,\n                # just return and skip it all.\n                if keepdb:\n                    return test_database_name\n\n                self.log('Got an error creating the test database: %s' % e)\n                if not autoclobber:\n                    confirm = input(\n                        \"Type 'yes' if you would like to try deleting the test \"\n(B)         # a TransactionTestCase still get a clean database on every test run.\n        if serialize:\n            self.connection._test_serialized_contents = self.serialize_db_to_string()\n\n        call_command('createcachetable', database=self.connection.alias)\n\n        # Ensure a connection for the side effect of initializing the test database.\n        self.connection.ensure_connection()\n\n        return test_database_name\n\n    def set_as_test_mirror(self, primary_settings_dict):\n        \"\"\"\n        Set this database up to be used in testing as a mirror of a primary\n        database whose settings are given.\n        \"\"\"\n        self.connection.settings_dict['NAME'] = primary_settings_dict['NAME']\n\n    def serialize_db_to_string(self):\n        \"\"\"\n        Serialize all data in the database into a JSON string.\n        Designed only for test runner usage; will not handle large\n        amounts of data.\n        \"\"\"\n        # Build list of all apps to serialize\n        from django.db.migrations.loader import MigrationLoader\n        loader = MigrationLoader(self.connection)\n        app_list = []\n        for app_config in apps.get_app_configs():\n            if (\n                app_config.models_module is not None and\n                app_config.label in loader.migrated_apps and\n                app_config.name not in settings.TEST_NON_SERIALIZED_APPS\n            ):\n                app_list.append((app_config, None))\n\n        # Make a function to iteratively return every object\n        def get_objects():\n            for model in serializers.sort_dependencies(app_list):\n                if (model._meta.can_migrate(self.connection) and\n                        router.allow_migrate_model(self.connection.alias, model)):\n                    queryset = model._default_manager.using(self.connection.alias).order_by(model._meta.pk.name)\n                    yield from queryset.iterator()\n        # Serialize to a string\n        out = StringIO()\n        serializers.serialize(\"json\", get_objects(), indent=None, stream=out)\n        return out.getvalue()\n\n    def deserialize_db_from_string(self, data):\n        \"\"\"\n        Reload the database with data from a string generated by\n        the serialize_db_to_string() method.\n        \"\"\"\n        data = StringIO(data)\n        for obj in serializers.deserialize(\"json\", data, using=self.connection.alias):\n            obj.save()\n\n    def _get_database_display_str(self, verbosity, database_name):\n        \"\"\"\n        Return display string for a database for use in various actions.\n        \"\"\"\n        return \"'%s'%s\" % (\n            self.connection.alias,\n            (\" ('%s')\" % database_name) if verbosity >= 2 else '',\n        )\n\n    def _get_test_db_name(self):\n        \"\"\"\n        Internal implementation - return the name of the test DB that will be\n        created. Only useful when called from create_test_db() and\n        _create_test_db() and when no external munging is done with the 'NAME'\n        settings.\n        \"\"\"\n        if self.connection.settings_dict['TEST']['NAME']:\n            return self.connection.settings_dict['TEST']['NAME']\n        return TEST_DATABASE_PREFIX + self.connection.settings_dict['NAME']\n\n    def _execute_create_test_db(self, cursor, parameters, keepdb=False):\n        cursor.execute('CREATE DATABASE %(dbname)s %(suffix)s' % parameters)\n\n    def _create_test_db(self, verbosity, autoclobber, keepdb=False):\n        \"\"\"\n        Internal implementation - create the test db tables.\n        \"\"\"\n        test_database_name = self._get_test_db_name()\n        test_db_params = {\n            'dbname': self.connection.ops.quote_name(test_database_name),\n            'suffix': self.sql_table_creation_suffix(),\n        }\n        # Create the test database and connect to it.\n        with self._nodb_cursor() as cursor:\n            try:\n                self._execute_create_test_db(cursor, test_db_params, keepdb)\n            except Exception as e:\n                # if we want to keep the db, then no need to do any of the below,\n                # just return and skip it all.\n                if keepdb:\n                    return test_database_name\n\n                self.log('Got an error creating the test database: %s' % e)\n                if not autoclobber:\n                    confirm = input(\n                        \"Type 'yes' if you would like to try deleting the test \"\n                        \"database '%s', or 'no' to cancel: \" % test_database_name)\n                if autoclobber or confirm == 'yes':\n                    try:\n                        if verbosity >= 1:\n                            self.log('Destroying old test database for alias %s...' % (\n                                self._get_database_display_str(verbosity, test_database_name),\n                            ))\n                        cursor.execute('DROP DATABASE %(dbname)s' % test_db_params)\n                        self._execute_create_test_db(cursor, test_db_params, keepdb)\n                    except Exception as e:\n                        self.log('Got an error recreating the test database: %s' % e)\n                        sys.exit(2)\n                else:\n                    self.log('Tests cancelled.')\n                    sys.exit(1)\n\n        return test_database_name\n\n    def clone_test_db(self, suffix, verbosity=1, autoclobber=False, keepdb=False):\n        \"\"\"\n        Clone a test database.\n        \"\"\"\n        source_database_name = self.connection.settings_dict['NAME']\n\n        if verbosity >= 1:\n            action = 'Cloning test database'\n            if keepdb:\n                action = 'Using existing clone'\n            self.log('%s for alias %s...' % (\n                action,\n                self._get_database_display_str(verbosity, source_database_name),\n            ))\n\n        # We could skip this call if keepdb is True, but we instead\n(C)                 app_list.append((app_config, None))\n\n        # Make a function to iteratively return every object\n        def get_objects():\n            for model in serializers.sort_dependencies(app_list):\n                if (model._meta.can_migrate(self.connection) and\n                        router.allow_migrate_model(self.connection.alias, model)):\n                    queryset = model._default_manager.using(self.connection.alias).order_by(model._meta.pk.name)\n                    yield from queryset.iterator()\n        # Serialize to a string\n        out = StringIO()\n        serializers.serialize(\"json\", get_objects(), indent=None, stream=out)\n        return out.getvalue()\n\n    def deserialize_db_from_string(self, data):\n        \"\"\"\n        Reload the database with data from a string generated by\n        the serialize_db_to_string() method.\n        \"\"\"\n        data = StringIO(data)\n        for obj in serializers.deserialize(\"json\", data, using=self.connection.alias):\n            obj.save()\n\n    def _get_database_display_str(self, verbosity, database_name):\n        \"\"\"\n        Return display string for a database for use in various actions.\n        \"\"\"\n        return \"'%s'%s\" % (\n            self.connection.alias,\n            (\" ('%s')\" % database_name) if verbosity >= 2 else '',\n        )\n\n    def _get_test_db_name(self):\n        \"\"\"\n        Internal implementation - return the name of the test DB that will be\n        created. Only useful when called from create_test_db() and\n        _create_test_db() and when no external munging is done with the 'NAME'\n        settings.\n        \"\"\"\n        if self.connection.settings_dict['TEST']['NAME']:\n            return self.connection.settings_dict['TEST']['NAME']\n        return TEST_DATABASE_PREFIX + self.connection.settings_dict['NAME']\n\n    def _execute_create_test_db(self, cursor, parameters, keepdb=False):\n        cursor.execute('CREATE DATABASE %(dbname)s %(suffix)s' % parameters)\n\n    def _create_test_db(self, verbosity, autoclobber, keepdb=False):\n        \"\"\"\n        Internal implementation - create the test db tables.\n        \"\"\"\n        test_database_name = self._get_test_db_name()\n        test_db_params = {\n            'dbname': self.connection.ops.quote_name(test_database_name),\n            'suffix': self.sql_table_creation_suffix(),\n        }\n        # Create the test database and connect to it.\n        with self._nodb_cursor() as cursor:\n            try:\n                self._execute_create_test_db(cursor, test_db_params, keepdb)\n            except Exception as e:\n                # if we want to keep the db, then no need to do any of the below,\n                # just return and skip it all.\n                if keepdb:\n                    return test_database_name\n\n                self.log('Got an error creating the test database: %s' % e)\n                if not autoclobber:\n                    confirm = input(\n                        \"Type 'yes' if you would like to try deleting the test \"\n                        \"database '%s', or 'no' to cancel: \" % test_database_name)\n                if autoclobber or confirm == 'yes':\n                    try:\n                        if verbosity >= 1:\n                            self.log('Destroying old test database for alias %s...' % (\n                                self._get_database_display_str(verbosity, test_database_name),\n                            ))\n                        cursor.execute('DROP DATABASE %(dbname)s' % test_db_params)\n                        self._execute_create_test_db(cursor, test_db_params, keepdb)\n                    except Exception as e:\n                        self.log('Got an error recreating the test database: %s' % e)\n                        sys.exit(2)\n                else:\n                    self.log('Tests cancelled.')\n                    sys.exit(1)\n\n        return test_database_name\n\n    def clone_test_db(self, suffix, verbosity=1, autoclobber=False, keepdb=False):\n        \"\"\"\n        Clone a test database.\n        \"\"\"\n        source_database_name = self.connection.settings_dict['NAME']\n\n        if verbosity >= 1:\n            action = 'Cloning test database'\n            if keepdb:\n                action = 'Using existing clone'\n            self.log('%s for alias %s...' % (\n                action,\n                self._get_database_display_str(verbosity, source_database_name),\n            ))\n\n        # We could skip this call if keepdb is True, but we instead\n        # give it the keepdb param. See create_test_db for details.\n        self._clone_test_db(suffix, verbosity, keepdb)\n\n    def get_test_db_clone_settings(self, suffix):\n        \"\"\"\n        Return a modified connection settings dict for the n-th clone of a DB.\n        \"\"\"\n        # When this function is called, the test database has been created\n        # already and its name has been copied to settings_dict['NAME'] so\n        # we don't need to call _get_test_db_name.\n        orig_settings_dict = self.connection.settings_dict\n        return {**orig_settings_dict, 'NAME': '{}_{}'.format(orig_settings_dict['NAME'], suffix)}\n\n    def _clone_test_db(self, suffix, verbosity, keepdb=False):\n        \"\"\"\n        Internal implementation - duplicate the test db tables.\n        \"\"\"\n        raise NotImplementedError(\n            \"The database backend doesn't support cloning databases. \"\n            \"Disable the option to run tests in parallel processes.\")\n\n    def destroy_test_db(self, old_database_name=None, verbosity=1, keepdb=False, suffix=None):\n        \"\"\"\n        Destroy a test database, prompting the user for confirmation if the\n        database already exists.\n        \"\"\"\n        self.connection.close()\n        if suffix is None:\n            test_database_name = self.connection.settings_dict['NAME']\n        else:\n            test_database_name = self.get_test_db_clone_settings(suffix)['NAME']\n\n        if verbosity >= 1:\n            action = 'Destroying'\n(D) from django.conf import settings\nfrom django.core import serializers\nfrom django.db import router\n\n# The prefix to put on the default database name when creating\n# the test database.\nTEST_DATABASE_PREFIX = 'test_'\n\n\nclass BaseDatabaseCreation:\n    \"\"\"\n    Encapsulate backend-specific differences pertaining to creation and\n    destruction of the test database.\n    \"\"\"\n    def __init__(self, connection):\n        self.connection = connection\n\n    def _nodb_cursor(self):\n        return self.connection._nodb_cursor()\n\n    def log(self, msg):\n        sys.stderr.write(msg + os.linesep)\n\n    def create_test_db(self, verbosity=1, autoclobber=False, serialize=True, keepdb=False):\n        \"\"\"\n        Create a test database, prompting the user for confirmation if the\n        database already exists. Return the name of the test database created.\n        \"\"\"\n        # Don't import django.core.management if it isn't needed.\n        from django.core.management import call_command\n\n        test_database_name = self._get_test_db_name()\n\n        if verbosity >= 1:\n            action = 'Creating'\n            if keepdb:\n                action = \"Using existing\"\n\n            self.log('%s test database for alias %s...' % (\n                action,\n                self._get_database_display_str(verbosity, test_database_name),\n            ))\n\n        # We could skip this call if keepdb is True, but we instead\n        # give it the keepdb param. This is to handle the case\n        # where the test DB doesn't exist, in which case we need to\n        # create it, then just not destroy it. If we instead skip\n        # this, we will get an exception.\n        self._create_test_db(verbosity, autoclobber, keepdb)\n\n        self.connection.close()\n        settings.DATABASES[self.connection.alias][\"NAME\"] = test_database_name\n        self.connection.settings_dict[\"NAME\"] = test_database_name\n\n        if self.connection.settings_dict['TEST']['MIGRATE']:\n            # We report migrate messages at one level lower than that\n            # requested. This ensures we don't get flooded with messages during\n            # testing (unless you really ask to be flooded).\n            call_command(\n                'migrate',\n                verbosity=max(verbosity - 1, 0),\n                interactive=False,\n                database=self.connection.alias,\n                run_syncdb=True,\n            )\n\n        # We then serialize the current state of the database into a string\n        # and store it on the connection. This slightly horrific process is so people\n        # who are testing on databases without transactions or who are using\n        # a TransactionTestCase still get a clean database on every test run.\n        if serialize:\n            self.connection._test_serialized_contents = self.serialize_db_to_string()\n\n        call_command('createcachetable', database=self.connection.alias)\n\n        # Ensure a connection for the side effect of initializing the test database.\n        self.connection.ensure_connection()\n\n        return test_database_name\n\n    def set_as_test_mirror(self, primary_settings_dict):\n        \"\"\"\n        Set this database up to be used in testing as a mirror of a primary\n        database whose settings are given.\n        \"\"\"\n        self.connection.settings_dict['NAME'] = primary_settings_dict['NAME']\n\n    def serialize_db_to_string(self):\n        \"\"\"\n        Serialize all data in the database into a JSON string.\n        Designed only for test runner usage; will not handle large\n        amounts of data.\n        \"\"\"\n        # Build list of all apps to serialize\n        from django.db.migrations.loader import MigrationLoader\n        loader = MigrationLoader(self.connection)\n        app_list = []\n        for app_config in apps.get_app_configs():\n            if (\n                app_config.models_module is not None and\n                app_config.label in loader.migrated_apps and\n                app_config.name not in settings.TEST_NON_SERIALIZED_APPS\n            ):\n                app_list.append((app_config, None))\n\n        # Make a function to iteratively return every object\n        def get_objects():\n            for model in serializers.sort_dependencies(app_list):\n                if (model._meta.can_migrate(self.connection) and\n                        router.allow_migrate_model(self.connection.alias, model)):\n                    queryset = model._default_manager.using(self.connection.alias).order_by(model._meta.pk.name)\n                    yield from queryset.iterator()\n        # Serialize to a string\n        out = StringIO()\n        serializers.serialize(\"json\", get_objects(), indent=None, stream=out)\n        return out.getvalue()\n\n    def deserialize_db_from_string(self, data):\n        \"\"\"\n        Reload the database with data from a string generated by\n        the serialize_db_to_string() method.\n        \"\"\"\n        data = StringIO(data)\n        for obj in serializers.deserialize(\"json\", data, using=self.connection.alias):\n            obj.save()\n\n    def _get_database_display_str(self, verbosity, database_name):\n        \"\"\"\n        Return display string for a database for use in various actions.\n        \"\"\"\n        return \"'%s'%s\" % (\n            self.connection.alias,\n            (\" ('%s')\" % database_name) if verbosity >= 2 else '',\n        )\n\n    def _get_test_db_name(self):\n        \"\"\"", "answer": "D"}
{"uuid": "c2ea7723-4b21-4516-862a-c6b2656f7861", "issue_statement": "Inherited model doesn't correctly order by \"-pk\" when specified on Parent.Meta.ordering\nDescription\n\t\nGiven the following model definition:\nfrom django.db import models\nclass Parent(models.Model):\n\tclass Meta:\n\t\tordering = [\"-pk\"]\nclass Child(Parent):\n\tpass\nQuerying the Child class results in the following:\n>>> print(Child.objects.all().query)\nSELECT \"myapp_parent\".\"id\", \"myapp_child\".\"parent_ptr_id\" FROM \"myapp_child\" INNER JOIN \"myapp_parent\" ON (\"myapp_child\".\"parent_ptr_id\" = \"myapp_parent\".\"id\") ORDER BY \"myapp_parent\".\"id\" ASC\nThe query is ordered ASC but I expect the order to be DESC.", "choices": "(A)         if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:\n            # Firstly, avoid infinite loops.\n            already_seen = already_seen or set()\n            join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)\n            if join_tuple in already_seen:\n                raise FieldError('Infinite loop caused by ordering.')\n            already_seen.add(join_tuple)\n\n            results = []\n(B)         # append the default ordering for that model unless the attribute name\n        # of the field is specified.\n        if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:\n            # Firstly, avoid infinite loops.\n            already_seen = already_seen or set()\n            join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)\n            if join_tuple in already_seen:\n                raise FieldError('Infinite loop caused by ordering.')\n            already_seen.add(join_tuple)\n(C)         name, order = get_order_dir(name, default_order)\n        descending = order == 'DESC'\n        pieces = name.split(LOOKUP_SEP)\n        field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)\n\n        # If we get to this point and the field is a relation to another model,\n        # append the default ordering for that model unless the attribute name\n        # of the field is specified.\n        if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:\n(D)         field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)\n\n        # If we get to this point and the field is a relation to another model,\n        # append the default ordering for that model unless the attribute name\n        # of the field is specified.\n        if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:\n            # Firstly, avoid infinite loops.\n            already_seen = already_seen or set()\n            join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)", "answer": "D"}
{"uuid": "4a02f46d-87a2-483e-98c4-5a2efcf0e69c", "issue_statement": "Wrong hint about recursive relationship.\nDescription\n\t \n\t\t(last modified by Matheus Cunha Motta)\n\t \nWhen there's more than 2 ForeignKeys in an intermediary model of a m2m field and no through_fields have been set, Django will show an error with the following hint:\nhint=(\n\t'If you want to create a recursive relationship, '\n\t'use ForeignKey(\"%s\", symmetrical=False, through=\"%s\").'\nBut 'symmetrical' and 'through' are m2m keyword arguments, not ForeignKey.\nThis was probably a small mistake where the developer thought ManyToManyField but typed ForeignKey instead. And the symmetrical=False is an outdated requirement to recursive relationships with intermediary model to self, not required since 3.0. I'll provide a PR with a proposed correction shortly after.\nEdit: fixed description.", "choices": "(A)                     errors.append(\n                        checks.Error(\n                            \"The model is used as an intermediate model by \"\n                            \"'%s', but it has more than one foreign key \"\n                            \"to '%s', which is ambiguous. You must specify \"\n                            \"which foreign key Django should use via the \"\n                            \"through_fields keyword argument.\" % (self, to_model_name),\n                            hint=(\n                                'If you want to create a recursive relationship, '\n                                'use ForeignKey(\"%s\", symmetrical=False, through=\"%s\").'\n                            ) % (\n                                RECURSIVE_RELATIONSHIP_CONSTANT,\n                                relationship_model_name,\n                            ),\n                            obj=self,\n                            id='fields.E335',\n                        )\n                    )\n\n                if seen_from == 0 or seen_to == 0:\n                    errors.append(\n                        checks.Error(\n                            \"The model is used as an intermediate model by \"\n                            \"'%s', but it does not have a foreign key to '%s' or '%s'.\" % (\n                                self, from_model_name, to_model_name\n                            ),\n                            obj=self.remote_field.through,\n(B)                              \"through_fields keyword argument.\") % (self, from_model_name),\n                            hint=(\n                                'If you want to create a recursive relationship, '\n                                'use ForeignKey(\"%s\", symmetrical=False, through=\"%s\").'\n                            ) % (\n                                RECURSIVE_RELATIONSHIP_CONSTANT,\n                                relationship_model_name,\n                            ),\n                            obj=self,\n                            id='fields.E334',\n                        )\n                    )\n\n                if seen_to > 1 and not self.remote_field.through_fields:\n                    errors.append(\n                        checks.Error(\n                            \"The model is used as an intermediate model by \"\n                            \"'%s', but it has more than one foreign key \"\n                            \"to '%s', which is ambiguous. You must specify \"\n                            \"which foreign key Django should use via the \"\n                            \"through_fields keyword argument.\" % (self, to_model_name),\n                            hint=(\n                                'If you want to create a recursive relationship, '\n                                'use ForeignKey(\"%s\", symmetrical=False, through=\"%s\").'\n                            ) % (\n                                RECURSIVE_RELATIONSHIP_CONSTANT,\n                                relationship_model_name,\n(C)                 if seen_from > 1 and not self.remote_field.through_fields:\n                    errors.append(\n                        checks.Error(\n                            (\"The model is used as an intermediate model by \"\n                             \"'%s', but it has more than one foreign key \"\n                             \"from '%s', which is ambiguous. You must specify \"\n                             \"which foreign key Django should use via the \"\n                             \"through_fields keyword argument.\") % (self, from_model_name),\n                            hint=(\n                                'If you want to create a recursive relationship, '\n                                'use ForeignKey(\"%s\", symmetrical=False, through=\"%s\").'\n                            ) % (\n                                RECURSIVE_RELATIONSHIP_CONSTANT,\n                                relationship_model_name,\n                            ),\n                            obj=self,\n                            id='fields.E334',\n                        )\n                    )\n\n                if seen_to > 1 and not self.remote_field.through_fields:\n                    errors.append(\n                        checks.Error(\n                            \"The model is used as an intermediate model by \"\n                            \"'%s', but it has more than one foreign key \"\n                            \"to '%s', which is ambiguous. You must specify \"\n                            \"which foreign key Django should use via the \"\n(D)                             ),\n                            obj=self,\n                            id='fields.E334',\n                        )\n                    )\n\n                if seen_to > 1 and not self.remote_field.through_fields:\n                    errors.append(\n                        checks.Error(\n                            \"The model is used as an intermediate model by \"\n                            \"'%s', but it has more than one foreign key \"\n                            \"to '%s', which is ambiguous. You must specify \"\n                            \"which foreign key Django should use via the \"\n                            \"through_fields keyword argument.\" % (self, to_model_name),\n                            hint=(\n                                'If you want to create a recursive relationship, '\n                                'use ForeignKey(\"%s\", symmetrical=False, through=\"%s\").'\n                            ) % (\n                                RECURSIVE_RELATIONSHIP_CONSTANT,\n                                relationship_model_name,\n                            ),\n                            obj=self,\n                            id='fields.E335',\n                        )\n                    )\n\n                if seen_from == 0 or seen_to == 0:", "answer": "B"}
{"uuid": "c8620414-6784-4998-ad0d-31c34f6a0685", "issue_statement": "Django 3.0: \"GROUP BY\" clauses error with tricky field annotation\nDescription\n\t\nLet's pretend that we have next model structure with next model's relations:\nclass A(models.Model):\n\tbs = models.ManyToManyField('B',\n\t\t\t\t\t\t\t\trelated_name=\"a\",\n\t\t\t\t\t\t\t\tthrough=\"AB\")\nclass B(models.Model):\n\tpass\nclass AB(models.Model):\n\ta = models.ForeignKey(A, on_delete=models.CASCADE, related_name=\"ab_a\")\n\tb = models.ForeignKey(B, on_delete=models.CASCADE, related_name=\"ab_b\")\n\tstatus = models.IntegerField()\nclass C(models.Model):\n\ta = models.ForeignKey(\n\t\tA,\n\t\tnull=True,\n\t\tblank=True,\n\t\ton_delete=models.SET_NULL,\n\t\trelated_name=\"c\",\n\t\tverbose_name=_(\"a\")\n\t)\n\tstatus = models.IntegerField()\nLet's try to evaluate next query\nab_query = AB.objects.filter(a=OuterRef(\"pk\"), b=1)\nfilter_conditions = Q(pk=1) | Q(ab_a__b=1)\nquery = A.objects.\\\n\tfilter(filter_conditions).\\\n\tannotate(\n\t\tstatus=Subquery(ab_query.values(\"status\")),\n\t\tc_count=Count(\"c\"),\n)\nanswer = query.values(\"status\").annotate(total_count=Count(\"status\"))\nprint(answer.query)\nprint(answer)\nOn Django 3.0.4 we have an error\ndjango.db.utils.ProgrammingError: column reference \"status\" is ambiguous\nand query is next:\nSELECT (SELECT U0.\"status\" FROM \"test_app_ab\" U0 WHERE (U0.\"a_id\" = \"test_app_a\".\"id\" AND U0.\"b_id\" = 1)) AS \"status\", COUNT((SELECT U0.\"status\" FROM \"test_app_ab\" U0 WHERE (U0.\"a_id\" = \"test_app_a\".\"id\" AND U0.\"b_id\" = 1))) AS \"total_count\" FROM \"test_app_a\" LEFT OUTER JOIN \"test_app_ab\" ON (\"test_app_a\".\"id\" = \"test_app_ab\".\"a_id\") LEFT OUTER JOIN \"test_app_c\" ON (\"test_app_a\".\"id\" = \"test_app_c\".\"a_id\") WHERE (\"test_app_a\".\"id\" = 1 OR \"test_app_ab\".\"b_id\" = 1) GROUP BY \"status\"\nHowever, Django 2.2.11 processed this query properly with the next query:\nSELECT (SELECT U0.\"status\" FROM \"test_app_ab\" U0 WHERE (U0.\"a_id\" = (\"test_app_a\".\"id\") AND U0.\"b_id\" = 1)) AS \"status\", COUNT((SELECT U0.\"status\" FROM \"test_app_ab\" U0 WHERE (U0.\"a_id\" = (\"test_app_a\".\"id\") AND U0.\"b_id\" = 1))) AS \"total_count\" FROM \"test_app_a\" LEFT OUTER JOIN \"test_app_ab\" ON (\"test_app_a\".\"id\" = \"test_app_ab\".\"a_id\") LEFT OUTER JOIN \"test_app_c\" ON (\"test_app_a\".\"id\" = \"test_app_c\".\"a_id\") WHERE (\"test_app_a\".\"id\" = 1 OR \"test_app_ab\".\"b_id\" = 1) GROUP BY (SELECT U0.\"status\" FROM \"test_app_ab\" U0 WHERE (U0.\"a_id\" = (\"test_app_a\".\"id\") AND U0.\"b_id\" = 1))\nso, the difference in \"GROUP BY\" clauses\n(as DB provider uses \"django.db.backends.postgresql\", postgresql 11)", "choices": "(A)                         alias = None\n                    group_by_cols = annotation.get_group_by_cols(alias=alias)\n                group_by.extend(group_by_cols)\n        self.group_by = tuple(group_by)\n\n    def add_select_related(self, fields):\n        \"\"\"\n        Set up the select_related data structure so that we only select\n        certain related models (as opposed to all models, when\n        self.select_related=True).\n        \"\"\"\n        if isinstance(self.select_related, bool):\n            field_dict = {}\n        else:\n            field_dict = self.select_related\n        for field in fields:\n            d = field_dict\n            for part in field.split(LOOKUP_SEP):\n                d = d.setdefault(part, {})\n        self.select_related = field_dict\n\n    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n        \"\"\"\n        if select:\n            # We need to pair any placeholder markers in the 'select'\n            # dictionary with their parameters in 'select_params' so that\n            # subsequent updates to the select dictionary also adjust the\n            # parameters appropriately.\n            select_pairs = {}\n            if select_params:\n(B)                     msg = (\n                        '`alias=None` must be added to the signature of '\n                        '%s.%s.get_group_by_cols().'\n                    ) % (annotation_class.__module__, annotation_class.__qualname__)\n                    warnings.warn(msg, category=RemovedInDjango40Warning)\n                    group_by_cols = annotation.get_group_by_cols()\n                else:\n                    if not allow_aliases:\n                        alias = None\n                    group_by_cols = annotation.get_group_by_cols(alias=alias)\n                group_by.extend(group_by_cols)\n        self.group_by = tuple(group_by)\n\n    def add_select_related(self, fields):\n        \"\"\"\n        Set up the select_related data structure so that we only select\n        certain related models (as opposed to all models, when\n        self.select_related=True).\n        \"\"\"\n        if isinstance(self.select_related, bool):\n            field_dict = {}\n        else:\n            field_dict = self.select_related\n        for field in fields:\n            d = field_dict\n            for part in field.split(LOOKUP_SEP):\n                d = d.setdefault(part, {})\n        self.select_related = field_dict\n\n    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n(C)         if force_empty:\n            self.default_ordering = False\n\n    def set_group_by(self, allow_aliases=True):\n        \"\"\"\n        Expand the GROUP BY clause required by the query.\n\n        This will usually be the set of all non-aggregate fields in the\n        return data. If the database backend supports grouping by the\n        primary key, and the query would be equivalent, the optimization\n        will be made automatically.\n        \"\"\"\n        group_by = list(self.select)\n        if self.annotation_select:\n            for alias, annotation in self.annotation_select.items():\n                signature = inspect.signature(annotation.get_group_by_cols)\n                if 'alias' not in signature.parameters:\n                    annotation_class = annotation.__class__\n                    msg = (\n                        '`alias=None` must be added to the signature of '\n                        '%s.%s.get_group_by_cols().'\n                    ) % (annotation_class.__module__, annotation_class.__qualname__)\n                    warnings.warn(msg, category=RemovedInDjango40Warning)\n                    group_by_cols = annotation.get_group_by_cols()\n                else:\n                    if not allow_aliases:\n                        alias = None\n                    group_by_cols = annotation.get_group_by_cols(alias=alias)\n                group_by.extend(group_by_cols)\n        self.group_by = tuple(group_by)\n\n    def add_select_related(self, fields):\n        \"\"\"\n(D)         primary key, and the query would be equivalent, the optimization\n        will be made automatically.\n        \"\"\"\n        group_by = list(self.select)\n        if self.annotation_select:\n            for alias, annotation in self.annotation_select.items():\n                signature = inspect.signature(annotation.get_group_by_cols)\n                if 'alias' not in signature.parameters:\n                    annotation_class = annotation.__class__\n                    msg = (\n                        '`alias=None` must be added to the signature of '\n                        '%s.%s.get_group_by_cols().'\n                    ) % (annotation_class.__module__, annotation_class.__qualname__)\n                    warnings.warn(msg, category=RemovedInDjango40Warning)\n                    group_by_cols = annotation.get_group_by_cols()\n                else:\n                    if not allow_aliases:\n                        alias = None\n                    group_by_cols = annotation.get_group_by_cols(alias=alias)\n                group_by.extend(group_by_cols)\n        self.group_by = tuple(group_by)\n\n    def add_select_related(self, fields):\n        \"\"\"\n        Set up the select_related data structure so that we only select\n        certain related models (as opposed to all models, when\n        self.select_related=True).\n        \"\"\"\n        if isinstance(self.select_related, bool):\n            field_dict = {}\n        else:\n            field_dict = self.select_related\n        for field in fields:", "answer": "D"}
{"uuid": "a9e1f064-4faa-47ff-8d7c-d9252cc4095d", "issue_statement": "Settings are cleaned insufficiently.\nDescription\n\t\nPosting publicly after checking with the rest of the security team.\nI just ran into a case where django.views.debug.SafeExceptionReporterFilter.get_safe_settings() would return several un-cleansed values. Looking at cleanse_setting() I realized that we \u200bonly take care of `dict`s but don't take other types of iterables into account but \u200breturn them as-is.\nExample:\nIn my settings.py I have this:\nMY_SETTING = {\n\t\"foo\": \"value\",\n\t\"secret\": \"value\",\n\t\"token\": \"value\",\n\t\"something\": [\n\t\t{\"foo\": \"value\"},\n\t\t{\"secret\": \"value\"},\n\t\t{\"token\": \"value\"},\n\t],\n\t\"else\": [\n\t\t[\n\t\t\t{\"foo\": \"value\"},\n\t\t\t{\"secret\": \"value\"},\n\t\t\t{\"token\": \"value\"},\n\t\t],\n\t\t[\n\t\t\t{\"foo\": \"value\"},\n\t\t\t{\"secret\": \"value\"},\n\t\t\t{\"token\": \"value\"},\n\t\t],\n\t]\n}\nOn Django 3.0 and below:\n>>> import pprint\n>>> from django.views.debug import get_safe_settings\n>>> pprint.pprint(get_safe_settings()[\"MY_SETTING\"])\n{'else': [[{'foo': 'value'}, {'secret': 'value'}, {'token': 'value'}],\n\t\t [{'foo': 'value'}, {'secret': 'value'}, {'token': 'value'}]],\n 'foo': 'value',\n 'secret': '********************',\n 'something': [{'foo': 'value'}, {'secret': 'value'}, {'token': 'value'}],\n 'token': '********************'}\nOn Django 3.1 and up:\n>>> from django.views.debug import SafeExceptionReporterFilter\n>>> import pprint\n>>> pprint.pprint(SafeExceptionReporterFilter().get_safe_settings()[\"MY_SETTING\"])\n{'else': [[{'foo': 'value'}, {'secret': 'value'}, {'token': 'value'}],\n\t\t [{'foo': 'value'}, {'secret': 'value'}, {'token': 'value'}]],\n 'foo': 'value',\n 'secret': '********************',\n 'something': [{'foo': 'value'}, {'secret': 'value'}, {'token': 'value'}],\n 'token': '********************'}", "choices": "(A)                 cleansed = self.cleansed_substitute\n            elif isinstance(value, dict):\n                cleansed = {k: self.cleanse_setting(k, v) for k, v in value.items()}\n            else:\n                cleansed = value\n        except TypeError:\n            # If the key isn't regex-able, just return as-is.\n            cleansed = value\n\n        if callable(cleansed):\n(B)             else:\n                cleansed = value\n        except TypeError:\n            # If the key isn't regex-able, just return as-is.\n            cleansed = value\n\n        if callable(cleansed):\n            cleansed = CallableSettingWrapper(cleansed)\n\n        return cleansed\n(C)         \"\"\"\n        try:\n            if self.hidden_settings.search(key):\n                cleansed = self.cleansed_substitute\n            elif isinstance(value, dict):\n                cleansed = {k: self.cleanse_setting(k, v) for k, v in value.items()}\n            else:\n                cleansed = value\n        except TypeError:\n            # If the key isn't regex-able, just return as-is.\n(D)         except TypeError:\n            # If the key isn't regex-able, just return as-is.\n            cleansed = value\n\n        if callable(cleansed):\n            cleansed = CallableSettingWrapper(cleansed)\n\n        return cleansed\n\n    def get_safe_settings(self):", "answer": "A"}
{"uuid": "031453f6-19e1-4244-bb14-dce181bde7ad", "issue_statement": "Migration crashes deleting an index_together if there is a unique_together on the same fields\nDescription\n\t\nHappens with Django 1.11.10\nSteps to reproduce:\n1) Create models with 2 fields, add 2 same fields to unique_together and to index_together\n2) Delete index_together -> Fail\nIt will fail at django/db/backends/base/schema.py, line 378, in _delete_composed_index(), ValueError: Found wrong number (2) of constraints for as this one will find two constraints, the _uniq and the _idx one. No way to get out of this...\nThe worst in my case is that happened as I wanted to refactor my code to use the \"new\" (Dj 1.11) Options.indexes feature. I am actually not deleting the index, just the way it is declared in my code.\nI think there are 2 different points here:\n1) The deletion of index_together should be possible alone or made coherent (migrations side?) with unique_together\n2) Moving the declaration of an index should not result in an index re-creation", "choices": "(A)             fields = [model._meta.get_field(field) for field in field_names]\n            self.execute(self._create_index_sql(model, fields, suffix=\"_idx\"))\n\n    def _delete_composed_index(self, model, fields, constraint_kwargs, sql):\n        meta_constraint_names = {constraint.name for constraint in model._meta.constraints}\n        meta_index_names = {constraint.name for constraint in model._meta.indexes}\n        columns = [model._meta.get_field(field).column for field in fields]\n        constraint_names = self._constraint_names(\n            model, columns, exclude=meta_constraint_names | meta_index_names,\n            **constraint_kwargs\n        )\n        if len(constraint_names) != 1:\n(B)         news = {tuple(fields) for fields in new_index_together}\n        # Deleted indexes\n        for fields in olds.difference(news):\n            self._delete_composed_index(model, fields, {'index': True}, self.sql_delete_index)\n        # Created indexes\n        for field_names in news.difference(olds):\n            fields = [model._meta.get_field(field) for field in field_names]\n            self.execute(self._create_index_sql(model, fields, suffix=\"_idx\"))\n\n    def _delete_composed_index(self, model, fields, constraint_kwargs, sql):\n        meta_constraint_names = {constraint.name for constraint in model._meta.constraints}\n        meta_index_names = {constraint.name for constraint in model._meta.indexes}\n(C)             self._delete_composed_index(model, fields, {'index': True}, self.sql_delete_index)\n        # Created indexes\n        for field_names in news.difference(olds):\n            fields = [model._meta.get_field(field) for field in field_names]\n            self.execute(self._create_index_sql(model, fields, suffix=\"_idx\"))\n\n    def _delete_composed_index(self, model, fields, constraint_kwargs, sql):\n        meta_constraint_names = {constraint.name for constraint in model._meta.constraints}\n        meta_index_names = {constraint.name for constraint in model._meta.indexes}\n        columns = [model._meta.get_field(field).column for field in fields]\n        constraint_names = self._constraint_names(\n            model, columns, exclude=meta_constraint_names | meta_index_names,\n(D)         [\"foo\", \"bar\"] format.\n        \"\"\"\n        olds = {tuple(fields) for fields in old_index_together}\n        news = {tuple(fields) for fields in new_index_together}\n        # Deleted indexes\n        for fields in olds.difference(news):\n            self._delete_composed_index(model, fields, {'index': True}, self.sql_delete_index)\n        # Created indexes\n        for field_names in news.difference(olds):\n            fields = [model._meta.get_field(field) for field in field_names]\n            self.execute(self._create_index_sql(model, fields, suffix=\"_idx\"))\n", "answer": "B"}
{"uuid": "d3646d06-e317-45bf-9282-882f26ed9df7", "issue_statement": "QuerySet.Delete - inconsistent result when zero objects deleted\nDescription\n\t\nThe result format of the QuerySet.Delete method is a tuple: (X, Y) \nX - is the total amount of deleted objects (including foreign key deleted objects)\nY - is a dictionary specifying counters of deleted objects for each specific model (the key is the _meta.label of the model and the value is counter of deleted objects of this model).\nExample: <class 'tuple'>: (2, {'my_app.FileAccess': 1, 'my_app.File': 1})\nWhen there are zero objects to delete in total - the result is inconsistent:\nFor models with foreign keys - the result will be: <class 'tuple'>: (0, {})\nFor \"simple\" models without foreign key - the result will be: <class 'tuple'>: (0, {'my_app.BlockLibrary': 0})\nI would expect there will be no difference between the two cases: Either both will have the empty dictionary OR both will have dictionary with model-label keys and zero value.", "choices": "(A)             # send pre_delete signals\n            for model, obj in self.instances_with_model():\n                if not model._meta.auto_created:\n                    signals.pre_delete.send(\n                        sender=model, instance=obj, using=self.using\n                    )\n\n            # fast deletes\n            for qs in self.fast_deletes:\n                count = qs._raw_delete(using=self.using)\n                deleted_counter[qs.model._meta.label] += count\n\n            # update fields\n            for model, instances_for_fieldvalues in self.field_updates.items():\n                for (field, value), instances in instances_for_fieldvalues.items():\n                    query = sql.UpdateQuery(model)\n                    query.update_batch([obj.pk for obj in instances],\n                                       {field.name: value}, self.using)\n\n            # reverse instance collections\n            for instances in self.data.values():\n                instances.reverse()\n\n            # delete instances\n            for model, instances in self.data.items():\n                query = sql.DeleteQuery(model)\n                pk_list = [obj.pk for obj in instances]\n(B)             # fast deletes\n            for qs in self.fast_deletes:\n                count = qs._raw_delete(using=self.using)\n                deleted_counter[qs.model._meta.label] += count\n\n            # update fields\n            for model, instances_for_fieldvalues in self.field_updates.items():\n                for (field, value), instances in instances_for_fieldvalues.items():\n                    query = sql.UpdateQuery(model)\n                    query.update_batch([obj.pk for obj in instances],\n                                       {field.name: value}, self.using)\n\n            # reverse instance collections\n            for instances in self.data.values():\n                instances.reverse()\n\n            # delete instances\n            for model, instances in self.data.items():\n                query = sql.DeleteQuery(model)\n                pk_list = [obj.pk for obj in instances]\n                count = query.delete_batch(pk_list, self.using)\n                deleted_counter[model._meta.label] += count\n\n                if not model._meta.auto_created:\n                    for obj in instances:\n                        signals.post_delete.send(\n                            sender=model, instance=obj, using=self.using\n(C)                 for (field, value), instances in instances_for_fieldvalues.items():\n                    query = sql.UpdateQuery(model)\n                    query.update_batch([obj.pk for obj in instances],\n                                       {field.name: value}, self.using)\n\n            # reverse instance collections\n            for instances in self.data.values():\n                instances.reverse()\n\n            # delete instances\n            for model, instances in self.data.items():\n                query = sql.DeleteQuery(model)\n                pk_list = [obj.pk for obj in instances]\n                count = query.delete_batch(pk_list, self.using)\n                deleted_counter[model._meta.label] += count\n\n                if not model._meta.auto_created:\n                    for obj in instances:\n                        signals.post_delete.send(\n                            sender=model, instance=obj, using=self.using\n                        )\n\n        # update collected instances\n        for instances_for_fieldvalues in self.field_updates.values():\n            for (field, value), instances in instances_for_fieldvalues.items():\n                for obj in instances:\n                    setattr(obj, field.attname, value)\n(D)             if self.can_fast_delete(instance):\n                with transaction.mark_for_rollback_on_error():\n                    count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n                setattr(instance, model._meta.pk.attname, None)\n                return count, {model._meta.label: count}\n\n        with transaction.atomic(using=self.using, savepoint=False):\n            # send pre_delete signals\n            for model, obj in self.instances_with_model():\n                if not model._meta.auto_created:\n                    signals.pre_delete.send(\n                        sender=model, instance=obj, using=self.using\n                    )\n\n            # fast deletes\n            for qs in self.fast_deletes:\n                count = qs._raw_delete(using=self.using)\n                deleted_counter[qs.model._meta.label] += count\n\n            # update fields\n            for model, instances_for_fieldvalues in self.field_updates.items():\n                for (field, value), instances in instances_for_fieldvalues.items():\n                    query = sql.UpdateQuery(model)\n                    query.update_batch([obj.pk for obj in instances],\n                                       {field.name: value}, self.using)\n\n            # reverse instance collections", "answer": "B"}
{"uuid": "bcb6847d-2639-4147-949b-482d2f6dea25", "issue_statement": "Add check for fields of UniqueConstraints.\nDescription\n\t \n\t\t(last modified by Marnanel Thurman)\n\t \nWhen a model gains a UniqueConstraint, makemigrations doesn't check that the fields named therein actually exist.\nThis is in contrast to the older unique_together syntax, which raises models.E012 if the fields don't exist.\nIn the attached demonstration, you'll need to uncomment \"with_unique_together\" in settings.py in order to show that unique_together raises E012.", "choices": "(A) ############################################\n# HELPER FUNCTIONS (CURRIED MODEL METHODS) #\n############################################\n\n# ORDERING METHODS #########################\n\ndef method_set_order(self, ordered_obj, id_list, using=None):\n    if using is None:\n        using = DEFAULT_DB_ALIAS\n    order_wrt = ordered_obj._meta.order_with_respect_to\n    filter_args = order_wrt.get_forward_related_filter(self)\n    ordered_obj.objects.db_manager(using).filter(**filter_args).bulk_update([\n(B)         return errors\n\n\n############################################\n# HELPER FUNCTIONS (CURRIED MODEL METHODS) #\n############################################\n\n# ORDERING METHODS #########################\n\ndef method_set_order(self, ordered_obj, id_list, using=None):\n    if using is None:\n        using = DEFAULT_DB_ALIAS\n(C)                         id='models.W038',\n                    )\n                )\n        return errors\n\n\n############################################\n# HELPER FUNCTIONS (CURRIED MODEL METHODS) #\n############################################\n\n# ORDERING METHODS #########################\n\n(D)                             \"warning if you don't care about it.\"\n                        ),\n                        obj=cls,\n                        id='models.W038',\n                    )\n                )\n        return errors\n\n\n############################################\n# HELPER FUNCTIONS (CURRIED MODEL METHODS) #\n############################################", "answer": "C"}
{"uuid": "6a017b2f-8f14-404a-a01b-c1f77a6de9b8", "issue_statement": "Union queryset should raise on distinct().\nDescription\n\t \n\t\t(last modified by Sielc Technologies)\n\t \nAfter using\n.annotate() on 2 different querysets\nand then .union()\n.distinct() will not affect the queryset\n\tdef setUp(self) -> None:\n\t\tuser = self.get_or_create_admin_user()\n\t\tSample.h.create(user, name=\"Sam1\")\n\t\tSample.h.create(user, name=\"Sam2 acid\")\n\t\tSample.h.create(user, name=\"Sam3\")\n\t\tSample.h.create(user, name=\"Sam4 acid\")\n\t\tSample.h.create(user, name=\"Dub\")\n\t\tSample.h.create(user, name=\"Dub\")\n\t\tSample.h.create(user, name=\"Dub\")\n\t\tself.user = user\n\tdef test_union_annotated_diff_distinct(self):\n\t\tqs = Sample.objects.filter(user=self.user)\n\t\tqs1 = qs.filter(name='Dub').annotate(rank=Value(0, IntegerField()))\n\t\tqs2 = qs.filter(name='Sam1').annotate(rank=Value(1, IntegerField()))\n\t\tqs = qs1.union(qs2)\n\t\tqs = qs.order_by('name').distinct('name') # THIS DISTINCT DOESN'T WORK\n\t\tself.assertEqual(qs.count(), 2)\nexpected to get wrapped union\n\tSELECT DISTINCT ON (siebox_sample.name) * FROM (SELECT ... UNION SELECT ...) AS siebox_sample", "choices": "(A)         \"\"\"\n        Return a new QuerySet instance that will select only distinct results.\n        \"\"\"\n        assert not self.query.is_sliced, \\\n            \"Cannot create distinct fields once a slice has been taken.\"\n        obj = self._chain()\n        obj.query.add_distinct_fields(*field_names)\n(B) \n    def distinct(self, *field_names):\n        \"\"\"\n        Return a new QuerySet instance that will select only distinct results.\n        \"\"\"\n        assert not self.query.is_sliced, \\\n            \"Cannot create distinct fields once a slice has been taken.\"\n(C)         \"\"\"\n        assert not self.query.is_sliced, \\\n            \"Cannot create distinct fields once a slice has been taken.\"\n        obj = self._chain()\n        obj.query.add_distinct_fields(*field_names)\n        return obj\n\n(D)             \"Cannot create distinct fields once a slice has been taken.\"\n        obj = self._chain()\n        obj.query.add_distinct_fields(*field_names)\n        return obj\n\n    def extra(self, select=None, where=None, params=None, tables=None,\n              order_by=None, select_params=None):", "answer": "A"}
{"uuid": "9d3b9639-53ba-40ee-b363-fcef214c42e8", "issue_statement": "Add get_response_async for ASGIStaticFilesHandler\nDescription\n\t\nIt looks like the StaticFilesHandlerMixin is missing the the async response function.\nWithout this, when trying to use the ASGIStaticFilesHandler, this is the traceback:\nException inside application: 'NoneType' object is not callable\nTraceback (most recent call last):\n File \".../lib/python3.7/site-packages/daphne/cli.py\", line 30, in asgi\n\tawait self.app(scope, receive, send)\n File \".../src/django/django/contrib/staticfiles/handlers.py\", line 86, in __call__\n\treturn await super().__call__(scope, receive, send)\n File \".../src/django/django/core/handlers/asgi.py\", line 161, in __call__\n\tresponse = await self.get_response_async(request)\n File \".../src/django/django/core/handlers/base.py\", line 148, in get_response_async\n\tresponse = await self._middleware_chain(request)\nTypeError: 'NoneType' object is not callable", "choices": "(A)     # request_finished signal)\n    handles_files = True\n\n    def load_middleware(self):\n        # Middleware are already loaded for self.application; no need to reload\n        # them for self.\n        pass\n\n    def get_base_url(self):\n        utils.check_settings()\n        return settings.STATIC_URL\n\n    def _should_handle(self, path):\n        \"\"\"\n        Check if the path should be handled. Ignore the path if:\n        * the host is provided as part of the base_url\n        * the request's path isn't under the media path (or equal)\n        \"\"\"\n        return path.startswith(self.base_url[2]) and not self.base_url[1]\n\n    def file_path(self, url):\n        \"\"\"\n        Return the relative path to the media file on disk for the given URL.\n        \"\"\"\n        relative_url = url[len(self.base_url[2]):]\n        return url2pathname(relative_url)\n\n    def serve(self, request):\n        \"\"\"Serve the request path.\"\"\"\n        return serve(request, self.file_path(request.path), insecure=True)\n\n    def get_response(self, request):\n        try:\n            return self.serve(request)\n        except Http404 as e:\n            return response_for_exception(request, e)\n\n\nclass StaticFilesHandler(StaticFilesHandlerMixin, WSGIHandler):\n    \"\"\"\n    WSGI middleware that intercepts calls to the static files directory, as\n    defined by the STATIC_URL setting, and serves those files.\n    \"\"\"\n    def __init__(self, application):\n        self.application = application\n        self.base_url = urlparse(self.get_base_url())\n        super().__init__()\n\n    def __call__(self, environ, start_response):\n        if not self._should_handle(get_path_info(environ)):\n            return self.application(environ, start_response)\n        return super().__call__(environ, start_response)\n\n\nclass ASGIStaticFilesHandler(StaticFilesHandlerMixin, ASGIHandler):\n    \"\"\"\n    ASGI application which wraps another and intercepts requests for static\n    files, passing them off to Django's static file serving.\n    \"\"\"\n    def __init__(self, application):\n        self.application = application\n        self.base_url = urlparse(self.get_base_url())\n\n    async def __call__(self, scope, receive, send):\n        # Only even look at HTTP requests\n(B) from urllib.parse import urlparse\nfrom urllib.request import url2pathname\n\nfrom django.conf import settings\nfrom django.contrib.staticfiles import utils\nfrom django.contrib.staticfiles.views import serve\nfrom django.core.handlers.asgi import ASGIHandler\nfrom django.core.handlers.exception import response_for_exception\nfrom django.core.handlers.wsgi import WSGIHandler, get_path_info\nfrom django.http import Http404\n\n\nclass StaticFilesHandlerMixin:\n    \"\"\"\n    Common methods used by WSGI and ASGI handlers.\n    \"\"\"\n    # May be used to differentiate between handler types (e.g. in a\n    # request_finished signal)\n    handles_files = True\n\n    def load_middleware(self):\n        # Middleware are already loaded for self.application; no need to reload\n        # them for self.\n        pass\n\n    def get_base_url(self):\n        utils.check_settings()\n        return settings.STATIC_URL\n\n    def _should_handle(self, path):\n        \"\"\"\n        Check if the path should be handled. Ignore the path if:\n        * the host is provided as part of the base_url\n        * the request's path isn't under the media path (or equal)\n        \"\"\"\n        return path.startswith(self.base_url[2]) and not self.base_url[1]\n\n    def file_path(self, url):\n        \"\"\"\n        Return the relative path to the media file on disk for the given URL.\n        \"\"\"\n        relative_url = url[len(self.base_url[2]):]\n        return url2pathname(relative_url)\n\n    def serve(self, request):\n        \"\"\"Serve the request path.\"\"\"\n        return serve(request, self.file_path(request.path), insecure=True)\n\n    def get_response(self, request):\n        try:\n            return self.serve(request)\n        except Http404 as e:\n            return response_for_exception(request, e)\n\n\nclass StaticFilesHandler(StaticFilesHandlerMixin, WSGIHandler):\n    \"\"\"\n    WSGI middleware that intercepts calls to the static files directory, as\n    defined by the STATIC_URL setting, and serves those files.\n    \"\"\"\n    def __init__(self, application):\n        self.application = application\n        self.base_url = urlparse(self.get_base_url())\n        super().__init__()\n", "answer": "D"}
{"uuid": "94be4c40-420b-46c2-92c3-841b63f3afc7", "issue_statement": "Make django.utils.text.slugify() strip dashes and underscores\nDescription\n\t \n\t\t(last modified by Elinaldo do Nascimento Monteiro)\n\t \nBug generation slug\nExample:\nfrom django.utils import text\ntext.slugify(\"___This is a test ---\")\noutput: ___this-is-a-test-\nImprovement after correction\nfrom django.utils import text\ntext.slugify(\"___This is a test ---\")\noutput: this-is-a-test\n\u200bPR", "choices": "(A)         raise ValueError(\"Not a string literal: %r\" % s)\n    quote = s[0]\n    return s[1:-1].replace(r'\\%s' % quote, quote).replace(r'\\\\', '\\\\')\n\n\n@keep_lazy_text\ndef slugify(value, allow_unicode=False):\n    \"\"\"\n    Convert to ASCII if 'allow_unicode' is False. Convert spaces to hyphens.\n    Remove characters that aren't alphanumerics, underscores, or hyphens.\n    Convert to lowercase. Also strip leading and trailing whitespace.\n    \"\"\"\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value.lower()).strip()\n(B)         value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value.lower()).strip()\n    return re.sub(r'[-\\s]+', '-', value)\n\n\ndef camel_case_to_spaces(value):\n    \"\"\"\n    Split CamelCase and convert to lowercase. Strip surrounding whitespace.\n    \"\"\"\n    return re_camel_case.sub(r' \\1', value).strip().lower()\n\n\ndef _format_lazy(format_string, *args, **kwargs):\n    \"\"\"\n    Apply str.format() on 'format_string' where format_string, args,\n    and/or kwargs might be lazy.\n(C)     Convert to lowercase. Also strip leading and trailing whitespace.\n    \"\"\"\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value.lower()).strip()\n    return re.sub(r'[-\\s]+', '-', value)\n\n\ndef camel_case_to_spaces(value):\n    \"\"\"\n    Split CamelCase and convert to lowercase. Strip surrounding whitespace.\n    \"\"\"\n    return re_camel_case.sub(r' \\1', value).strip().lower()\n\n\n(D) @keep_lazy_text\ndef slugify(value, allow_unicode=False):\n    \"\"\"\n    Convert to ASCII if 'allow_unicode' is False. Convert spaces to hyphens.\n    Remove characters that aren't alphanumerics, underscores, or hyphens.\n    Convert to lowercase. Also strip leading and trailing whitespace.\n    \"\"\"\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value.lower()).strip()\n    return re.sub(r'[-\\s]+', '-', value)\n\n\ndef camel_case_to_spaces(value):\n    \"\"\"", "answer": "D"}
{"uuid": "9352889f-3f9a-4756-aab8-6a87dad88686", "issue_statement": "Queryset raises NotSupportedError when RHS has filterable=False attribute.\nDescription\n\t \n\t\t(last modified by Nicolas Baccelli)\n\t \nI'm migrating my app to django 3.0.7 and I hit a strange behavior using a model class with a field labeled filterable\nclass ProductMetaDataType(models.Model):\n\tlabel = models.CharField(max_length=255, unique=True, blank=False, null=False)\n\tfilterable = models.BooleanField(default=False, verbose_name=_(\"filterable\"))\n\tclass Meta:\n\t\tapp_label = \"adminpricing\"\n\t\tverbose_name = _(\"product meta data type\")\n\t\tverbose_name_plural = _(\"product meta data types\")\n\tdef __str__(self):\n\t\treturn self.label\nclass ProductMetaData(models.Model):\n\tid = models.BigAutoField(primary_key=True)\n\tproduct = models.ForeignKey(\n\t\tProduit, null=False, blank=False, on_delete=models.CASCADE\n\t)\n\tvalue = models.TextField(null=False, blank=False)\n\tmarketplace = models.ForeignKey(\n\t\tPlateforme, null=False, blank=False, on_delete=models.CASCADE\n\t)\n\tdate_created = models.DateTimeField(null=True, default=timezone.now)\n\tmetadata_type = models.ForeignKey(\n\t\tProductMetaDataType, null=False, blank=False, on_delete=models.CASCADE\n\t)\n\tclass Meta:\n\t\tapp_label = \"adminpricing\"\n\t\tverbose_name = _(\"product meta data\")\n\t\tverbose_name_plural = _(\"product meta datas\")\nError happened when filtering ProductMetaData with a metadata_type :\nProductMetaData.objects.filter(value=\"Dark Vador\", metadata_type=self.brand_metadata)\nError traceback :\nTraceback (most recent call last):\n File \"/backoffice/backoffice/adminpricing/tests/test_pw.py\", line 481, in test_checkpolicywarning_by_fields\n\tfor p in ProductMetaData.objects.filter(\n File \"/usr/local/lib/python3.8/site-packages/django/db/models/manager.py\", line 82, in manager_method\n\treturn getattr(self.get_queryset(), name)(*args, **kwargs)\n File \"/usr/local/lib/python3.8/site-packages/django/db/models/query.py\", line 904, in filter\n\treturn self._filter_or_exclude(False, *args, **kwargs)\n File \"/usr/local/lib/python3.8/site-packages/django/db/models/query.py\", line 923, in _filter_or_exclude\n\tclone.query.add_q(Q(*args, **kwargs))\n File \"/usr/local/lib/python3.8/site-packages/django/db/models/sql/query.py\", line 1351, in add_q\n\tclause, _ = self._add_q(q_object, self.used_aliases)\n File \"/usr/local/lib/python3.8/site-packages/django/db/models/sql/query.py\", line 1378, in _add_q\n\tchild_clause, needed_inner = self.build_filter(\n File \"/usr/local/lib/python3.8/site-packages/django/db/models/sql/query.py\", line 1264, in build_filter\n\tself.check_filterable(value)\n File \"/usr/local/lib/python3.8/site-packages/django/db/models/sql/query.py\", line 1131, in check_filterable\n\traise NotSupportedError(\ndjango.db.utils.NotSupportedError: ProductMetaDataType is disallowed in the filter clause.\nI changed label to filterable_test and it fixed this issue\nThis should be documented or fix.", "choices": "(A)             elif hasattr(value, '__iter__'):\n                for v in value:\n                    self.check_query_object_type(v, opts, field)\n\n    def check_filterable(self, expression):\n        \"\"\"Raise an error if expression cannot be used in a WHERE clause.\"\"\"\n        if not getattr(expression, 'filterable', True):\n            raise NotSupportedError(\n                expression.__class__.__name__ + ' is disallowed in the filter '\n                'clause.'\n(B)                 expression.__class__.__name__ + ' is disallowed in the filter '\n                'clause.'\n            )\n        if hasattr(expression, 'get_source_expressions'):\n            for expr in expression.get_source_expressions():\n                self.check_filterable(expr)\n\n    def build_lookup(self, lookups, lhs, rhs):\n        \"\"\"\n        Try to extract transforms and lookup from given lhs.\n(C) \n    def check_filterable(self, expression):\n        \"\"\"Raise an error if expression cannot be used in a WHERE clause.\"\"\"\n        if not getattr(expression, 'filterable', True):\n            raise NotSupportedError(\n                expression.__class__.__name__ + ' is disallowed in the filter '\n                'clause.'\n            )\n        if hasattr(expression, 'get_source_expressions'):\n            for expr in expression.get_source_expressions():\n(D)         if not getattr(expression, 'filterable', True):\n            raise NotSupportedError(\n                expression.__class__.__name__ + ' is disallowed in the filter '\n                'clause.'\n            )\n        if hasattr(expression, 'get_source_expressions'):\n            for expr in expression.get_source_expressions():\n                self.check_filterable(expr)\n\n    def build_lookup(self, lookups, lhs, rhs):", "answer": "C"}
{"uuid": "e98dcb4f-96d0-4e11-8a94-829815c26a6f", "issue_statement": "Self referencing foreign key doesn't correctly order by a relation \"_id\" field.\nDescription\n\t\nInitially discovered on 2.2.10 but verified still happens on 3.0.6. Given the following models:\nclass OneModel(models.Model):\n\tclass Meta:\n\t\tordering = (\"-id\",)\n\tid = models.BigAutoField(primary_key=True)\n\troot = models.ForeignKey(\"OneModel\", on_delete=models.CASCADE, null=True)\n\toneval = models.BigIntegerField(null=True)\nclass TwoModel(models.Model):\n\tid = models.BigAutoField(primary_key=True)\n\trecord = models.ForeignKey(OneModel, on_delete=models.CASCADE)\n\ttwoval = models.BigIntegerField(null=True)\nThe following queryset gives unexpected results and appears to be an incorrect SQL query:\nqs = TwoModel.objects.filter(record__oneval__in=[1,2,3])\nqs = qs.order_by(\"record__root_id\")\nprint(qs.query)\nSELECT \"orion_twomodel\".\"id\", \"orion_twomodel\".\"record_id\", \"orion_twomodel\".\"twoval\" FROM \"orion_twomodel\" INNER JOIN \"orion_onemodel\" ON (\"orion_twomodel\".\"record_id\" = \"orion_onemodel\".\"id\") LEFT OUTER JOIN \"orion_onemodel\" T3 ON (\"orion_onemodel\".\"root_id\" = T3.\"id\") WHERE \"orion_onemodel\".\"oneval\" IN (1, 2, 3) ORDER BY T3.\"id\" DESC\nThe query has an unexpected DESCENDING sort. That appears to come from the default sort order on the OneModel class, but I would expect the order_by() to take prececence. The the query has two JOINS, which is unnecessary. It appears that, since OneModel.root is a foreign key to itself, that is causing it to do the unnecessary extra join. In fact, testing a model where root is a foreign key to a third model doesn't show the problem behavior.\nNote also that the queryset with order_by(\"record__root\") gives the exact same SQL.\nThis queryset gives correct results and what looks like a pretty optimal SQL:\nqs = TwoModel.objects.filter(record__oneval__in=[1,2,3])\nqs = qs.order_by(\"record__root__id\")\nprint(qs.query)\nSELECT \"orion_twomodel\".\"id\", \"orion_twomodel\".\"record_id\", \"orion_twomodel\".\"twoval\" FROM \"orion_twomodel\" INNER JOIN \"orion_onemodel\" ON (\"orion_twomodel\".\"record_id\" = \"orion_onemodel\".\"id\") WHERE \"orion_onemodel\".\"oneval\" IN (1, 2, 3) ORDER BY \"orion_onemodel\".\"root_id\" ASC\nSo is this a potential bug or a misunderstanding on my part?\nAnother queryset that works around the issue and gives a reasonable SQL query and expected results:\nqs = TwoModel.objects.filter(record__oneval__in=[1,2,3])\nqs = qs.annotate(root_id=F(\"record__root_id\"))\nqs = qs.order_by(\"root_id\")\nprint(qs.query)\nSELECT \"orion_twomodel\".\"id\", \"orion_twomodel\".\"record_id\", \"orion_twomodel\".\"twoval\" FROM \"orion_twomodel\" INNER JOIN \"orion_onemodel\" ON (\"orion_twomodel\".\"record_id\" = \"orion_onemodel\".\"id\") WHERE \"orion_onemodel\".\"oneval\" IN (1, 2, 3) ORDER BY \"orion_onemodel\".\"zero_id\" ASC\nASCENDING sort, and a single INNER JOIN, as I'd expect. That actually works for my use because I need that output column anyway.\nOne final oddity; with the original queryset but the inverted sort order_by():\nqs = TwoModel.objects.filter(record__oneval__in=[1,2,3])\nqs = qs.order_by(\"-record__root_id\")\nprint(qs.query)\nSELECT \"orion_twomodel\".\"id\", \"orion_twomodel\".\"record_id\", \"orion_twomodel\".\"twoval\" FROM \"orion_twomodel\" INNER JOIN \"orion_onemodel\" ON (\"orion_twomodel\".\"record_id\" = \"orion_onemodel\".\"id\") LEFT OUTER JOIN \"orion_onemodel\" T3 ON (\"orion_onemodel\".\"root_id\" = T3.\"id\") WHERE \"orion_onemodel\".\"oneval\" IN (1, 2, 3) ORDER BY T3.\"id\" ASC\nOne gets the query with the two JOINs but an ASCENDING sort order. I was not under the impression that sort orders are somehow relative to the class level sort order, eg: does specifing order_by(\"-record__root_id\") invert the class sort order? Testing that on a simple case doesn't show that behavior at all.\nThanks for any assistance and clarification.", "choices": "(A)         pieces = name.split(LOOKUP_SEP)\n        field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)\n\n        # If we get to this point and the field is a relation to another model,\n        # append the default ordering for that model unless it is the pk\n        # shortcut or the attribute name of the field that is specified.\n        if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name and name != 'pk':\n            # Firstly, avoid infinite loops.\n            already_seen = already_seen or set()\n            join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)\n            if join_tuple in already_seen:\n                raise FieldError('Infinite loop caused by ordering.')\n(B)         if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name and name != 'pk':\n            # Firstly, avoid infinite loops.\n            already_seen = already_seen or set()\n            join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)\n            if join_tuple in already_seen:\n                raise FieldError('Infinite loop caused by ordering.')\n            already_seen.add(join_tuple)\n\n            results = []\n            for item in opts.ordering:\n                if hasattr(item, 'resolve_expression') and not isinstance(item, OrderBy):\n                    item = item.desc() if descending else item.asc()\n(C)         # If we get to this point and the field is a relation to another model,\n        # append the default ordering for that model unless it is the pk\n        # shortcut or the attribute name of the field that is specified.\n        if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name and name != 'pk':\n            # Firstly, avoid infinite loops.\n            already_seen = already_seen or set()\n            join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)\n            if join_tuple in already_seen:\n                raise FieldError('Infinite loop caused by ordering.')\n            already_seen.add(join_tuple)\n\n            results = []\n(D)             join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)\n            if join_tuple in already_seen:\n                raise FieldError('Infinite loop caused by ordering.')\n            already_seen.add(join_tuple)\n\n            results = []\n            for item in opts.ordering:\n                if hasattr(item, 'resolve_expression') and not isinstance(item, OrderBy):\n                    item = item.desc() if descending else item.asc()\n                if isinstance(item, OrderBy):\n                    results.append((item, False))\n                    continue", "answer": "C"}
{"uuid": "d68209ce-34f4-4569-bb27-63e571c4f950", "issue_statement": "QuerySet.none() on combined queries returns all results.\nDescription\n\t\nI came across this issue on Stack Overflow. I'm not 100% sure it's a bug, but it does seem strange. With this code (excuse the bizarre example filtering):\nclass Publication(models.Model):\n\tpass\nclass Article(models.Model):\n\tpublications = models.ManyToManyField(to=Publication, blank=True, null=True)\nclass ArticleForm(forms.ModelForm):\n\tpublications = forms.ModelMultipleChoiceField(\n\t\tPublication.objects.filter(id__lt=2) | Publication.objects.filter(id__gt=5),\n\t\trequired=False,\n\t)\n\tclass Meta:\n\t\tmodel = Article\n\t\tfields = [\"publications\"]\nclass ArticleAdmin(admin.ModelAdmin):\n\tform = ArticleForm\nThis works well. However, changing the ModelMultipleChoiceField queryset to use union() breaks things.\npublications = forms.ModelMultipleChoiceField(\n\tPublication.objects.filter(id__lt=2).union(\n\t\tPublication.objects.filter(id__gt=5)\n\t),\n\trequired=False,\n)\nThe form correctly shows only the matching objects. However, if you submit this form while empty (i.e. you didn't select any publications), ALL objects matching the queryset will be added. Using the OR query, NO objects are added, as I'd expect.", "choices": "(A)                 if is_reverse_o2o(source):\n                    cur_model = source.related_model\n                else:\n                    cur_model = source.remote_field.model\n                opts = cur_model._meta\n                # Even if we're \"just passing through\" this model, we must add\n                # both the current model's pk and the related reference field\n                # (if it's not a reverse relation) to the things we select.\n                if not is_reverse_o2o(source):\n                    must_include[old_model].add(source)\n                add_to_dict(must_include, cur_model, opts.pk)\n            field = opts.get_field(parts[-1])\n            is_reverse_object = field.auto_created and not field.concrete\n            model = field.related_model if is_reverse_object else field.model\n            model = model._meta.concrete_model\n            if model == opts.model:\n                model = cur_model\n            if not is_reverse_o2o(field):\n                add_to_dict(seen, model, field)\n\n        if defer:\n            # We need to load all fields for each model, except those that\n            # appear in \"seen\" (for all models that appear in \"seen\"). The only\n            # slight complexity here is handling fields that exist on parent\n            # models.\n            workset = {}\n            for model, values in seen.items():\n                for field in model._meta.local_fields:\n                    if field not in values:\n                        m = field.model._meta.concrete_model\n                        add_to_dict(workset, m, field)\n            for model, values in must_include.items():\n                # If we haven't included a model in workset, we don't add the\n                # corresponding must_include fields for that model, since an\n                # empty set means \"include all fields\". That's why there's no\n                # \"else\" branch here.\n                if model in workset:\n                    workset[model].update(values)\n            for model, values in workset.items():\n                callback(target, model, values)\n        else:\n            for model, values in must_include.items():\n                if model in seen:\n                    seen[model].update(values)\n                else:\n                    # As we've passed through this model, but not explicitly\n                    # included any fields, we have to make sure it's mentioned\n                    # so that only the \"must include\" fields are pulled in.\n                    seen[model] = values\n            # Now ensure that every model in the inheritance chain is mentioned\n            # in the parent list. Again, it must be mentioned to ensure that\n            # only \"must include\" fields are pulled in.\n            for model in orig_opts.get_parent_list():\n                seen.setdefault(model, set())\n            for model, values in seen.items():\n                callback(target, model, values)\n\n    def table_alias(self, table_name, create=False, filtered_relation=None):\n        \"\"\"\n        Return a table alias for the given table_name and whether this is a\n        new alias or not.\n\n        If 'create' is true, a new alias is always created. Otherwise, the\n        most recently created alias for the table (if one exists) is reused.\n        \"\"\"\n        alias_list = self.table_map.get(table_name)\n        if not create and alias_list:\n            alias = alias_list[0]\n            self.alias_refcount[alias] += 1\n            return alias, False\n\n        # Create a new alias for this table.\n        if alias_list:\n            alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)\n            alias_list.append(alias)\n        else:\n            # The first occurrence of a table uses the table name directly.\n            alias = filtered_relation.alias if filtered_relation is not None else table_name\n            self.table_map[table_name] = [alias]\n        self.alias_refcount[alias] = 1\n        return alias, True\n\n    def ref_alias(self, alias):\n        \"\"\"Increases the reference count for this alias.\"\"\"\n        self.alias_refcount[alias] += 1\n\n    def unref_alias(self, alias, amount=1):\n        \"\"\"Decreases the reference count for this alias.\"\"\"\n        self.alias_refcount[alias] -= amount\n\n    def promote_joins(self, aliases):\n        \"\"\"\n        Promote recursively the join type of given aliases and its children to\n        an outer join. If 'unconditional' is False, only promote the join if\n        it is nullable or the parent join is an outer join.\n\n        The children promotion is done to avoid join chains that contain a LOUTER\n        b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,\n        then we must also promote b->c automatically, or otherwise the promotion\n        of a->b doesn't actually change anything in the query results.\n        \"\"\"\n        aliases = list(aliases)\n        while aliases:\n            alias = aliases.pop(0)\n            if self.alias_map[alias].join_type is None:\n                # This is the base table (first FROM entry) - this table\n                # isn't really joined at all in the query, so we should not\n                # alter its join type.\n                continue\n            # Only the first alias (skipped above) should have None join_type\n            assert self.alias_map[alias].join_type is not None\n            parent_alias = self.alias_map[alias].parent_alias\n            parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER\n            already_louter = self.alias_map[alias].join_type == LOUTER\n            if ((self.alias_map[alias].nullable or parent_louter) and\n                    not already_louter):\n                self.alias_map[alias] = self.alias_map[alias].promote()\n                # Join type of 'alias' changed, so re-examine all aliases that\n                # refer to this one.\n                aliases.extend(\n                    join for join in self.alias_map\n                    if self.alias_map[join].parent_alias == alias and join not in aliases\n                )\n\n    def demote_joins(self, aliases):\n        \"\"\"\n        Change join type from LOUTER to INNER for all joins in aliases.\n\n        Similarly to promote_joins(), this method must ensure no join chains\n        containing first an outer, then an inner join are generated. If we\n        are demoting b->c join in chain a LOUTER b LOUTER c then we must\n        demote a->b automatically, or otherwise the demotion of b->c doesn't\n        actually change anything in the query results. .\n        \"\"\"\n        aliases = list(aliases)\n        while aliases:\n            alias = aliases.pop(0)\n            if self.alias_map[alias].join_type == LOUTER:\n                self.alias_map[alias] = self.alias_map[alias].demote()\n                parent_alias = self.alias_map[alias].parent_alias\n                if self.alias_map[parent_alias].join_type == INNER:\n                    aliases.append(parent_alias)\n\n    def reset_refcounts(self, to_counts):\n        \"\"\"\n        Reset reference counts for aliases so that they match the value passed\n        in `to_counts`.\n        \"\"\"\n        for alias, cur_refcount in self.alias_refcount.copy().items():\n            unref_amount = cur_refcount - to_counts.get(alias, 0)\n            self.unref_alias(alias, unref_amount)\n\n    def change_aliases(self, change_map):\n        \"\"\"\n        Change the aliases in change_map (which maps old-alias -> new-alias),\n        relabelling any references to them in select columns and the where\n        clause.\n        \"\"\"\n        assert set(change_map).isdisjoint(change_map.values())\n\n        # 1. Update references in \"select\" (normal columns plus aliases),\n        # \"group by\" and \"where\".\n        self.where.relabel_aliases(change_map)\n        if isinstance(self.group_by, tuple):\n            self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])\n        self.select = tuple([col.relabeled_clone(change_map) for col in self.select])\n        self.annotations = self.annotations and {\n            key: col.relabeled_clone(change_map) for key, col in self.annotations.items()\n        }\n\n        # 2. Rename the alias in the internal table/alias datastructures.\n        for old_alias, new_alias in change_map.items():\n            if old_alias not in self.alias_map:\n                continue\n            alias_data = self.alias_map[old_alias].relabeled_clone(change_map)\n            self.alias_map[new_alias] = alias_data\n            self.alias_refcount[new_alias] = self.alias_refcount[old_alias]\n            del self.alias_refcount[old_alias]\n            del self.alias_map[old_alias]\n\n            table_aliases = self.table_map[alias_data.table_name]\n            for pos, alias in enumerate(table_aliases):\n                if alias == old_alias:\n                    table_aliases[pos] = new_alias\n                    break\n        self.external_aliases = {\n            # Table is aliased or it's being changed and thus is aliased.\n            change_map.get(alias, alias): (aliased or alias in change_map)\n            for alias, aliased in self.external_aliases.items()\n        }\n\n    def bump_prefix(self, outer_query):\n        \"\"\"\n        Change the alias prefix to the next letter in the alphabet in a way\n        that the outer query's aliases and this query's aliases will not\n        conflict. Even tables that previously had no alias will get an alias\n        after this call.\n        \"\"\"\n        def prefix_gen():\n            \"\"\"\n            Generate a sequence of characters in alphabetical order:\n                -> 'A', 'B', 'C', ...\n\n            When the alphabet is finished, the sequence will continue with the\n            Cartesian product:\n                -> 'AA', 'AB', 'AC', ...\n            \"\"\"\n            alphabet = ascii_uppercase\n            prefix = chr(ord(self.alias_prefix) + 1)\n            yield prefix\n            for n in count(1):\n                seq = alphabet[alphabet.index(prefix):] if prefix else alphabet\n                for s in product(seq, repeat=n):\n                    yield ''.join(s)\n                prefix = None\n\n        if self.alias_prefix != outer_query.alias_prefix:\n            # No clashes between self and outer query should be possible.\n            return\n\n        # Explicitly avoid infinite loop. The constant divider is based on how\n        # much depth recursive subquery references add to the stack. This value\n        # might need to be adjusted when adding or removing function calls from\n        # the code path in charge of performing these operations.\n        local_recursion_limit = sys.getrecursionlimit() // 16\n        for pos, prefix in enumerate(prefix_gen()):\n            if prefix not in self.subq_aliases:\n                self.alias_prefix = prefix\n                break\n            if pos > local_recursion_limit:\n                raise RecursionError(\n                    'Maximum recursion depth exceeded: too many subqueries.'\n                )\n        self.subq_aliases = self.subq_aliases.union([self.alias_prefix])\n        outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)\n        self.change_aliases({\n            alias: '%s%d' % (self.alias_prefix, pos)\n            for pos, alias in enumerate(self.alias_map)\n        })\n\n    def get_initial_alias(self):\n        \"\"\"\n        Return the first alias for this query, after increasing its reference\n        count.\n        \"\"\"\n        if self.alias_map:\n            alias = self.base_table\n            self.ref_alias(alias)\n        else:\n            alias = self.join(BaseTable(self.get_meta().db_table, None))\n        return alias\n\n    def count_active_tables(self):\n        \"\"\"\n        Return the number of tables in this query with a non-zero reference\n        count. After execution, the reference counts are zeroed, so tables\n        added in compiler will not be seen by this method.\n        \"\"\"\n        return len([1 for count in self.alias_refcount.values() if count])\n\n    def join(self, join, reuse=None, reuse_with_filtered_relation=False):\n        \"\"\"\n        Return an alias for the 'join', either reusing an existing alias for\n        that join or creating a new one. 'join' is either a\n        sql.datastructures.BaseTable or Join.\n\n        The 'reuse' parameter can be either None which means all joins are\n        reusable, or it can be a set containing the aliases that can be reused.\n\n        The 'reuse_with_filtered_relation' parameter is used when computing\n        FilteredRelation instances.\n\n        A join is always created as LOUTER if the lhs alias is LOUTER to make\n        sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new\n        joins are created as LOUTER if the join is nullable.\n        \"\"\"\n        if reuse_with_filtered_relation and reuse:\n            reuse_aliases = [\n                a for a, j in self.alias_map.items()\n                if a in reuse and j.equals(join, with_filtered_relation=False)\n            ]\n        else:\n            reuse_aliases = [\n                a for a, j in self.alias_map.items()\n                if (reuse is None or a in reuse) and j == join\n            ]\n        if reuse_aliases:\n            if join.table_alias in reuse_aliases:\n                reuse_alias = join.table_alias\n            else:\n                # Reuse the most recent alias of the joined table\n                # (a many-to-many relation may be joined multiple times).\n                reuse_alias = reuse_aliases[-1]\n            self.ref_alias(reuse_alias)\n            return reuse_alias\n\n        # No reuse is possible, so we need a new alias.\n        alias, _ = self.table_alias(join.table_name, create=True, filtered_relation=join.filtered_relation)\n        if join.join_type:\n            if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:\n                join_type = LOUTER\n            else:\n                join_type = INNER\n            join.join_type = join_type\n        join.table_alias = alias\n        self.alias_map[alias] = join\n        return alias\n\n    def join_parent_model(self, opts, model, alias, seen):\n        \"\"\"\n        Make sure the given 'model' is joined in the query. If 'model' isn't\n        a parent of 'opts' or if it is None this method is a no-op.\n\n        The 'alias' is the root alias for starting the join, 'seen' is a dict\n        of model -> alias of existing joins. It must also contain a mapping\n        of None -> some alias. This will be returned in the no-op case.\n        \"\"\"\n        if model in seen:\n            return seen[model]\n        chain = opts.get_base_chain(model)\n        if not chain:\n            return alias\n        curr_opts = opts\n        for int_model in chain:\n            if int_model in seen:\n                curr_opts = int_model._meta\n                alias = seen[int_model]\n                continue\n            # Proxy model have elements in base chain\n            # with no parents, assign the new options\n            # object and skip to the next base in that\n            # case\n            if not curr_opts.parents[int_model]:\n                curr_opts = int_model._meta\n                continue\n            link_field = curr_opts.get_ancestor_link(int_model)\n            join_info = self.setup_joins([link_field.name], curr_opts, alias)\n            curr_opts = int_model._meta\n            alias = seen[int_model] = join_info.joins[-1]\n        return alias or seen[None]\n\n    def add_annotation(self, annotation, alias, is_summary=False):\n        \"\"\"Add a single annotation expression to the Query.\"\"\"\n        annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n                                                   summarize=is_summary)\n        self.append_annotation_mask([alias])\n        self.annotations[alias] = annotation\n\n    def resolve_expression(self, query, *args, **kwargs):\n        clone = self.clone()\n        # Subqueries need to use a different set of aliases than the outer query.\n        clone.bump_prefix(query)\n        clone.subquery = True\n        # It's safe to drop ordering if the queryset isn't using slicing,\n        # distinct(*fields) or select_for_update().\n        if (self.low_mark == 0 and self.high_mark is None and\n                not self.distinct_fields and\n                not self.select_for_update):\n            clone.clear_ordering(True)\n        clone.where.resolve_expression(query, *args, **kwargs)\n        for key, value in clone.annotations.items():\n            resolved = value.resolve_expression(query, *args, **kwargs)\n            if hasattr(resolved, 'external_aliases'):\n                resolved.external_aliases.update(clone.external_aliases)\n            clone.annotations[key] = resolved\n        # Outer query's aliases are considered external.\n        for alias, table in query.alias_map.items():\n            clone.external_aliases[alias] = (\n                (isinstance(table, Join) and table.join_field.related_model._meta.db_table != alias) or\n                (isinstance(table, BaseTable) and table.table_name != table.table_alias)\n            )\n        return clone\n\n    def get_external_cols(self):\n        exprs = chain(self.annotations.values(), self.where.children)\n        return [\n            col for col in self._gen_cols(exprs)\n            if col.alias in self.external_aliases\n        ]\n\n    def as_sql(self, compiler, connection):\n        sql, params = self.get_compiler(connection=connection).as_sql()\n        if self.subquery:\n            sql = '(%s)' % sql\n        return sql, params\n\n    def resolve_lookup_value(self, value, can_reuse, allow_joins):\n        if hasattr(value, 'resolve_expression'):\n            value = value.resolve_expression(\n                self, reuse=can_reuse, allow_joins=allow_joins,\n            )\n        elif isinstance(value, (list, tuple)):\n            # The items of the iterable may be expressions and therefore need\n            # to be resolved independently.\n            return type(value)(\n                self.resolve_lookup_value(sub_value, can_reuse, allow_joins)\n                for sub_value in value\n            )\n        return value\n\n    def solve_lookup_type(self, lookup):\n        \"\"\"\n        Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').\n        \"\"\"\n        lookup_splitted = lookup.split(LOOKUP_SEP)\n        if self.annotations:\n            expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)\n            if expression:\n                return expression_lookups, (), expression\n        _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())\n        field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]\n        if len(lookup_parts) > 1 and not field_parts:\n            raise FieldError(\n                'Invalid lookup \"%s\" for model %s\".' %\n                (lookup, self.get_meta().model.__name__)\n            )\n        return lookup_parts, field_parts, False\n\n    def check_query_object_type(self, value, opts, field):\n        \"\"\"\n        Check whether the object passed while querying is of the correct type.\n        If not, raise a ValueError specifying the wrong object.\n        \"\"\"\n        if hasattr(value, '_meta'):\n            if not check_rel_lookup_compatibility(value._meta.model, opts, field):\n                raise ValueError(\n                    'Cannot query \"%s\": Must be \"%s\" instance.' %\n                    (value, opts.object_name))\n\n    def check_related_objects(self, field, value, opts):\n        \"\"\"Check the type of object passed to query relations.\"\"\"\n        if field.is_relation:\n            # Check that the field and the queryset use the same model in a\n            # query like .filter(author=Author.objects.all()). For example, the\n            # opts would be Author's (from the author field) and value.model\n            # would be Author.objects.all() queryset's .model (Author also).\n            # The field is the related field on the lhs side.\n            if (isinstance(value, Query) and not value.has_select_fields and\n                    not check_rel_lookup_compatibility(value.model, opts, field)):\n                raise ValueError(\n                    'Cannot use QuerySet for \"%s\": Use a QuerySet for \"%s\".' %\n                    (value.model._meta.object_name, opts.object_name)\n                )\n            elif hasattr(value, '_meta'):\n                self.check_query_object_type(value, opts, field)\n            elif hasattr(value, '__iter__'):\n                for v in value:\n                    self.check_query_object_type(v, opts, field)\n\n    def check_filterable(self, expression):\n        \"\"\"Raise an error if expression cannot be used in a WHERE clause.\"\"\"\n        if (\n            hasattr(expression, 'resolve_expression') and\n            not getattr(expression, 'filterable', True)\n        ):\n            raise NotSupportedError(\n                expression.__class__.__name__ + ' is disallowed in the filter '\n                'clause.'\n            )\n        if hasattr(expression, 'get_source_expressions'):\n            for expr in expression.get_source_expressions():\n                self.check_filterable(expr)\n\n    def build_lookup(self, lookups, lhs, rhs):\n        \"\"\"\n        Try to extract transforms and lookup from given lhs.\n\n        The lhs value is something that works like SQLExpression.\n        The rhs value is what the lookup is going to compare against.\n        The lookups is a list of names to extract using get_lookup()\n        and get_transform().\n        \"\"\"\n        # __exact is the default lookup if one isn't given.\n        *transforms, lookup_name = lookups or ['exact']\n        for name in transforms:\n            lhs = self.try_transform(lhs, name)\n        # First try get_lookup() so that the lookup takes precedence if the lhs\n        # supports both transform and lookup for the name.\n        lookup_class = lhs.get_lookup(lookup_name)\n        if not lookup_class:\n            if lhs.field.is_relation:\n                raise FieldError('Related Field got invalid lookup: {}'.format(lookup_name))\n            # A lookup wasn't found. Try to interpret the name as a transform\n            # and do an Exact lookup against it.\n            lhs = self.try_transform(lhs, lookup_name)\n            lookup_name = 'exact'\n            lookup_class = lhs.get_lookup(lookup_name)\n            if not lookup_class:\n                return\n\n        lookup = lookup_class(lhs, rhs)\n        # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all\n        # uses of None as a query value unless the lookup supports it.\n        if lookup.rhs is None and not lookup.can_use_none_as_rhs:\n            if lookup_name not in ('exact', 'iexact'):\n                raise ValueError(\"Cannot use None as a query value\")\n            return lhs.get_lookup('isnull')(lhs, True)\n\n        # For Oracle '' is equivalent to null. The check must be done at this\n        # stage because join promotion can't be done in the compiler. Using\n        # DEFAULT_DB_ALIAS isn't nice but it's the best that can be done here.\n        # A similar thing is done in is_nullable(), too.\n        if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and\n                lookup_name == 'exact' and lookup.rhs == ''):\n            return lhs.get_lookup('isnull')(lhs, True)\n\n        return lookup\n\n    def try_transform(self, lhs, name):\n        \"\"\"\n        Helper method for build_lookup(). Try to fetch and initialize\n        a transform for name parameter from lhs.\n        \"\"\"\n        transform_class = lhs.get_transform(name)\n        if transform_class:\n            return transform_class(lhs)\n        else:\n            output_field = lhs.output_field.__class__\n            suggested_lookups = difflib.get_close_matches(name, output_field.get_lookups())\n            if suggested_lookups:\n                suggestion = ', perhaps you meant %s?' % ' or '.join(suggested_lookups)\n            else:\n                suggestion = '.'\n            raise FieldError(\n                \"Unsupported lookup '%s' for %s or join on the field not \"\n                \"permitted%s\" % (name, output_field.__name__, suggestion)\n            )\n\n    def build_filter(self, filter_expr, branch_negated=False, current_negated=False,\n                     can_reuse=None, allow_joins=True, split_subq=True,\n                     reuse_with_filtered_relation=False, check_filterable=True):\n        \"\"\"\n        Build a WhereNode for a single filter clause but don't add it\n        to this Query. Query.add_q() will then add this filter to the where\n        Node.\n\n        The 'branch_negated' tells us if the current branch contains any\n        negations. This will be used to determine if subqueries are needed.\n\n        The 'current_negated' is used to determine if the current filter is\n        negated or not and this will be used to determine if IS NULL filtering\n        is needed.\n\n        The difference between current_negated and branch_negated is that\n        branch_negated is set on first negation, but current_negated is\n        flipped for each negation.\n\n        Note that add_filter will not do any negating itself, that is done\n        upper in the code by add_q().\n\n        The 'can_reuse' is a set of reusable joins for multijoins.\n\n        If 'reuse_with_filtered_relation' is True, then only joins in can_reuse\n        will be reused.\n\n        The method will create a filter clause that can be added to the current\n        query. However, if the filter isn't added to the query then the caller\n        is responsible for unreffing the joins used.\n        \"\"\"\n        if isinstance(filter_expr, dict):\n            raise FieldError(\"Cannot parse keyword query as dict\")\n        if isinstance(filter_expr, Q):\n            return self._add_q(\n                filter_expr,\n                branch_negated=branch_negated,\n                current_negated=current_negated,\n                used_aliases=can_reuse,\n                allow_joins=allow_joins,\n                split_subq=split_subq,\n                check_filterable=check_filterable,\n            )\n        if hasattr(filter_expr, 'resolve_expression'):\n            if not getattr(filter_expr, 'conditional', False):\n                raise TypeError('Cannot filter against a non-conditional expression.')\n            condition = self.build_lookup(\n                ['exact'], filter_expr.resolve_expression(self, allow_joins=allow_joins), True\n            )\n            clause = self.where_class()\n            clause.add(condition, AND)\n            return clause, []\n        arg, value = filter_expr\n        if not arg:\n            raise FieldError(\"Cannot parse keyword query %r\" % arg)\n        lookups, parts, reffed_expression = self.solve_lookup_type(arg)\n\n        if check_filterable:\n            self.check_filterable(reffed_expression)\n\n        if not allow_joins and len(parts) > 1:\n            raise FieldError(\"Joined field references are not permitted in this query\")\n\n        pre_joins = self.alias_refcount.copy()\n        value = self.resolve_lookup_value(value, can_reuse, allow_joins)\n        used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}\n\n        if check_filterable:\n            self.check_filterable(value)\n\n        clause = self.where_class()\n        if reffed_expression:\n            condition = self.build_lookup(lookups, reffed_expression, value)\n            clause.add(condition, AND)\n            return clause, []\n\n        opts = self.get_meta()\n        alias = self.get_initial_alias()\n        allow_many = not branch_negated or not split_subq\n\n        try:\n            join_info = self.setup_joins(\n                parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,\n                reuse_with_filtered_relation=reuse_with_filtered_relation,\n            )\n\n            # Prevent iterator from being consumed by check_related_objects()\n            if isinstance(value, Iterator):\n                value = list(value)\n            self.check_related_objects(join_info.final_field, value, join_info.opts)\n\n            # split_exclude() needs to know which joins were generated for the\n            # lookup parts\n            self._lookup_joins = join_info.joins\n        except MultiJoin as e:\n            return self.split_exclude(filter_expr, can_reuse, e.names_with_path)\n\n        # Update used_joins before trimming since they are reused to determine\n        # which joins could be later promoted to INNER.\n        used_joins.update(join_info.joins)\n        targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)\n        if can_reuse is not None:\n            can_reuse.update(join_list)\n\n        if join_info.final_field.is_relation:\n            # No support for transforms for relational fields\n            num_lookups = len(lookups)\n            if num_lookups > 1:\n                raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))\n            if len(targets) == 1:\n                col = self._get_col(targets[0], join_info.final_field, alias)\n            else:\n                col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)\n        else:\n            col = self._get_col(targets[0], join_info.final_field, alias)\n\n        condition = self.build_lookup(lookups, col, value)\n        lookup_type = condition.lookup_name\n        clause.add(condition, AND)\n\n        require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated\n        if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:\n            require_outer = True\n            if lookup_type != 'isnull':\n                # The condition added here will be SQL like this:\n                # NOT (col IS NOT NULL), where the first NOT is added in\n                # upper layers of code. The reason for addition is that if col\n                # is null, then col != someval will result in SQL \"unknown\"\n                # which isn't the same as in Python. The Python None handling\n                # is wanted, and it can be gotten by\n                # (col IS NULL OR col != someval)\n                #   <=>\n                # NOT (col IS NOT NULL AND col = someval).\n                if (\n                    self.is_nullable(targets[0]) or\n                    self.alias_map[join_list[-1]].join_type == LOUTER\n                ):\n                    lookup_class = targets[0].get_lookup('isnull')\n                    col = self._get_col(targets[0], join_info.targets[0], alias)\n                    clause.add(lookup_class(col, False), AND)\n                # If someval is a nullable column, someval IS NOT NULL is\n                # added.\n                if isinstance(value, Col) and self.is_nullable(value.target):\n                    lookup_class = value.target.get_lookup('isnull')\n                    clause.add(lookup_class(value, False), AND)\n        return clause, used_joins if not require_outer else ()\n\n    def add_filter(self, filter_clause):\n        self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))\n\n    def add_q(self, q_object):\n        \"\"\"\n        A preprocessor for the internal _add_q(). Responsible for doing final\n        join promotion.\n        \"\"\"\n        # For join promotion this case is doing an AND for the added q_object\n        # and existing conditions. So, any existing inner join forces the join\n        # type to remain inner. Existing outer joins can however be demoted.\n        # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if\n        # rel_a doesn't produce any rows, then the whole condition must fail.\n        # So, demotion is OK.\n        existing_inner = {a for a in self.alias_map if self.alias_map[a].join_type == INNER}\n        clause, _ = self._add_q(q_object, self.used_aliases)\n        if clause:\n            self.where.add(clause, AND)\n        self.demote_joins(existing_inner)\n\n    def build_where(self, filter_expr):\n        return self.build_filter(filter_expr, allow_joins=False)[0]\n\n    def _add_q(self, q_object, used_aliases, branch_negated=False,\n               current_negated=False, allow_joins=True, split_subq=True,\n               check_filterable=True):\n        \"\"\"Add a Q-object to the current filter.\"\"\"\n        connector = q_object.connector\n        current_negated = current_negated ^ q_object.negated\n        branch_negated = branch_negated or q_object.negated\n        target_clause = self.where_class(connector=connector,\n                                         negated=q_object.negated)\n        joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)\n        for child in q_object.children:\n            child_clause, needed_inner = self.build_filter(\n                child, can_reuse=used_aliases, branch_negated=branch_negated,\n                current_negated=current_negated, allow_joins=allow_joins,\n                split_subq=split_subq, check_filterable=check_filterable,\n            )\n            joinpromoter.add_votes(needed_inner)\n            if child_clause:\n                target_clause.add(child_clause, connector)\n        needed_inner = joinpromoter.update_join_types(self)\n        return target_clause, needed_inner\n\n    def build_filtered_relation_q(self, q_object, reuse, branch_negated=False, current_negated=False):\n        \"\"\"Add a FilteredRelation object to the current filter.\"\"\"\n        connector = q_object.connector\n        current_negated ^= q_object.negated\n        branch_negated = branch_negated or q_object.negated\n        target_clause = self.where_class(connector=connector, negated=q_object.negated)\n        for child in q_object.children:\n            if isinstance(child, Node):\n                child_clause = self.build_filtered_relation_q(\n                    child, reuse=reuse, branch_negated=branch_negated,\n                    current_negated=current_negated,\n                )\n            else:\n                child_clause, _ = self.build_filter(\n                    child, can_reuse=reuse, branch_negated=branch_negated,\n                    current_negated=current_negated,\n                    allow_joins=True, split_subq=False,\n                    reuse_with_filtered_relation=True,\n                )\n            target_clause.add(child_clause, connector)\n        return target_clause\n\n    def add_filtered_relation(self, filtered_relation, alias):\n        filtered_relation.alias = alias\n        lookups = dict(get_children_from_q(filtered_relation.condition))\n        for lookup in chain((filtered_relation.relation_name,), lookups):\n            lookup_parts, field_parts, _ = self.solve_lookup_type(lookup)\n            shift = 2 if not lookup_parts else 1\n            if len(field_parts) > (shift + len(lookup_parts)):\n                raise ValueError(\n                    \"FilteredRelation's condition doesn't support nested \"\n                    \"relations (got %r).\" % lookup\n                )\n        self._filtered_relations[filtered_relation.alias] = filtered_relation\n\n    def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):\n        \"\"\"\n        Walk the list of names and turns them into PathInfo tuples. A single\n        name in 'names' can generate multiple PathInfos (m2m, for example).\n\n        'names' is the path of names to travel, 'opts' is the model Options we\n        start the name resolving from, 'allow_many' is as for setup_joins().\n        If fail_on_missing is set to True, then a name that can't be resolved\n        will generate a FieldError.\n\n        Return a list of PathInfo tuples. In addition return the final field\n        (the last used join field) and target (which is a field guaranteed to\n        contain the same value as the final field). Finally, return those names\n        that weren't found (which are likely transforms and the final lookup).\n        \"\"\"\n        path, names_with_path = [], []\n        for pos, name in enumerate(names):\n            cur_names_with_path = (name, [])\n            if name == 'pk':\n                name = opts.pk.name\n\n            field = None\n            filtered_relation = None\n            try:\n                field = opts.get_field(name)\n            except FieldDoesNotExist:\n                if name in self.annotation_select:\n                    field = self.annotation_select[name].output_field\n                elif name in self._filtered_relations and pos == 0:\n                    filtered_relation = self._filtered_relations[name]\n                    field = opts.get_field(filtered_relation.relation_name)\n            if field is not None:\n                # Fields that contain one-to-many relations with a generic\n                # model (like a GenericForeignKey) cannot generate reverse\n                # relations and therefore cannot be used for reverse querying.\n                if field.is_relation and not field.related_model:\n                    raise FieldError(\n                        \"Field %r does not generate an automatic reverse \"\n                        \"relation and therefore cannot be used for reverse \"\n                        \"querying. If it is a GenericForeignKey, consider \"\n                        \"adding a GenericRelation.\" % name\n                    )\n                try:\n                    model = field.model._meta.concrete_model\n                except AttributeError:\n                    # QuerySet.annotate() may introduce fields that aren't\n                    # attached to a model.\n                    model = None\n            else:\n                # We didn't find the current field, so move position back\n                # one step.\n                pos -= 1\n                if pos == -1 or fail_on_missing:\n                    available = sorted([\n                        *get_field_names_from_opts(opts),\n                        *self.annotation_select,\n                        *self._filtered_relations,\n                    ])\n                    raise FieldError(\"Cannot resolve keyword '%s' into field. \"\n                                     \"Choices are: %s\" % (name, \", \".join(available)))\n                break\n            # Check if we need any joins for concrete inheritance cases (the\n            # field lives in parent, but we are currently in one of its\n            # children)\n            if model is not opts.model:\n                path_to_parent = opts.get_path_to_parent(model)\n                if path_to_parent:\n                    path.extend(path_to_parent)\n                    cur_names_with_path[1].extend(path_to_parent)\n                    opts = path_to_parent[-1].to_opts\n            if hasattr(field, 'get_path_info'):\n                pathinfos = field.get_path_info(filtered_relation)\n                if not allow_many:\n                    for inner_pos, p in enumerate(pathinfos):\n                        if p.m2m:\n                            cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])\n                            names_with_path.append(cur_names_with_path)\n                            raise MultiJoin(pos + 1, names_with_path)\n                last = pathinfos[-1]\n                path.extend(pathinfos)\n                final_field = last.join_field\n                opts = last.to_opts\n                targets = last.target_fields\n                cur_names_with_path[1].extend(pathinfos)\n                names_with_path.append(cur_names_with_path)\n            else:\n                # Local non-relational field.\n                final_field = field\n                targets = (field,)\n                if fail_on_missing and pos + 1 != len(names):\n                    raise FieldError(\n                        \"Cannot resolve keyword %r into field. Join on '%s'\"\n                        \" not permitted.\" % (names[pos + 1], name))\n                break\n        return path, final_field, targets, names[pos + 1:]\n\n    def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True,\n                    reuse_with_filtered_relation=False):\n        \"\"\"\n        Compute the necessary table joins for the passage through the fields\n        given in 'names'. 'opts' is the Options class for the current model\n        (which gives the table we are starting from), 'alias' is the alias for\n        the table to start the joining from.\n\n        The 'can_reuse' defines the reverse foreign key joins we can reuse. It\n        can be None in which case all joins are reusable or a set of aliases\n        that can be reused. Note that non-reverse foreign keys are always\n        reusable when using setup_joins().\n\n        The 'reuse_with_filtered_relation' can be used to force 'can_reuse'\n        parameter and force the relation on the given connections.\n\n        If 'allow_many' is False, then any reverse foreign key seen will\n        generate a MultiJoin exception.\n\n        Return the final field involved in the joins, the target field (used\n        for any 'where' constraint), the final 'opts' value, the joins, the\n        field path traveled to generate the joins, and a transform function\n        that takes a field and alias and is equivalent to `field.get_col(alias)`\n        in the simple case but wraps field transforms if they were included in\n        names.\n\n        The target field is the field containing the concrete value. Final\n        field can be something different, for example foreign key pointing to\n        that value. Final field is needed for example in some value\n        conversions (convert 'obj' in fk__id=obj to pk val using the foreign\n        key field for example).\n        \"\"\"\n        joins = [alias]\n        # The transform can't be applied yet, as joins must be trimmed later.\n        # To avoid making every caller of this method look up transforms\n        # directly, compute transforms here and create a partial that converts\n        # fields to the appropriate wrapped version.\n\n        def final_transformer(field, alias):\n            return field.get_col(alias)\n\n        # Try resolving all the names as fields first. If there's an error,\n        # treat trailing names as lookups until a field can be resolved.\n        last_field_exception = None\n        for pivot in range(len(names), 0, -1):\n            try:\n                path, final_field, targets, rest = self.names_to_path(\n                    names[:pivot], opts, allow_many, fail_on_missing=True,\n                )\n            except FieldError as exc:\n                if pivot == 1:\n                    # The first item cannot be a lookup, so it's safe\n                    # to raise the field error here.\n                    raise\n                else:\n                    last_field_exception = exc\n            else:\n                # The transforms are the remaining items that couldn't be\n                # resolved into fields.\n                transforms = names[pivot:]\n                break\n        for name in transforms:\n            def transform(field, alias, *, name, previous):\n                try:\n                    wrapped = previous(field, alias)\n                    return self.try_transform(wrapped, name)\n                except FieldError:\n                    # FieldError is raised if the transform doesn't exist.\n                    if isinstance(final_field, Field) and last_field_exception:\n                        raise last_field_exception\n                    else:\n                        raise\n            final_transformer = functools.partial(transform, name=name, previous=final_transformer)\n        # Then, add the path to the query's joins. Note that we can't trim\n        # joins at this stage - we will need the information about join type\n        # of the trimmed joins.\n        for join in path:\n            if join.filtered_relation:\n                filtered_relation = join.filtered_relation.clone()\n                table_alias = filtered_relation.alias\n            else:\n                filtered_relation = None\n                table_alias = None\n            opts = join.to_opts\n            if join.direct:\n                nullable = self.is_nullable(join.join_field)\n            else:\n                nullable = True\n            connection = Join(\n                opts.db_table, alias, table_alias, INNER, join.join_field,\n                nullable, filtered_relation=filtered_relation,\n            )\n            reuse = can_reuse if join.m2m or reuse_with_filtered_relation else None\n            alias = self.join(\n                connection, reuse=reuse,\n                reuse_with_filtered_relation=reuse_with_filtered_relation,\n            )\n            joins.append(alias)\n            if filtered_relation:\n                filtered_relation.path = joins[:]\n        return JoinInfo(final_field, targets, opts, joins, path, final_transformer)\n\n    def trim_joins(self, targets, joins, path):\n        \"\"\"\n        The 'target' parameter is the final field being joined to, 'joins'\n        is the full list of join aliases. The 'path' contain the PathInfos\n        used to create the joins.\n\n        Return the final target field and table alias and the new active\n        joins.\n\n        Always trim any direct join if the target column is already in the\n        previous table. Can't trim reverse joins as it's unknown if there's\n        anything on the other side of the join.\n        \"\"\"\n        joins = joins[:]\n        for pos, info in enumerate(reversed(path)):\n            if len(joins) == 1 or not info.direct:\n                break\n            if info.filtered_relation:\n                break\n            join_targets = {t.column for t in info.join_field.foreign_related_fields}\n            cur_targets = {t.column for t in targets}\n            if not cur_targets.issubset(join_targets):\n                break\n            targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}\n            targets = tuple(targets_dict[t.column] for t in targets)\n            self.unref_alias(joins.pop())\n        return targets, joins[-1], joins\n\n    @classmethod\n    def _gen_cols(cls, exprs):\n        for expr in exprs:\n            if isinstance(expr, Col):\n                yield expr\n            else:\n                yield from cls._gen_cols(expr.get_source_expressions())\n\n    @classmethod\n    def _gen_col_aliases(cls, exprs):\n        yield from (expr.alias for expr in cls._gen_cols(exprs))\n\n    def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False):\n        if not allow_joins and LOOKUP_SEP in name:\n            raise FieldError(\"Joined field references are not permitted in this query\")\n        annotation = self.annotations.get(name)\n        if annotation is not None:\n            if not allow_joins:\n                for alias in self._gen_col_aliases([annotation]):\n                    if isinstance(self.alias_map[alias], Join):\n                        raise FieldError(\n                            'Joined field references are not permitted in '\n                            'this query'\n                        )\n            if summarize:\n                # Summarize currently means we are doing an aggregate() query\n                # which is executed as a wrapped subquery if any of the\n                # aggregate() elements reference an existing annotation. In\n                # that case we need to return a Ref to the subquery's annotation.\n                return Ref(name, self.annotation_select[name])\n            else:\n                return annotation\n        else:\n            field_list = name.split(LOOKUP_SEP)\n            join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse)\n            targets, final_alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)\n            if not allow_joins and len(join_list) > 1:\n                raise FieldError('Joined field references are not permitted in this query')\n            if len(targets) > 1:\n                raise FieldError(\"Referencing multicolumn fields with F() objects \"\n                                 \"isn't supported\")\n            # Verify that the last lookup in name is a field or a transform:\n            # transform_function() raises FieldError if not.\n            join_info.transform_function(targets[0], final_alias)\n            if reuse is not None:\n                reuse.update(join_list)\n            return self._get_col(targets[0], join_info.targets[0], join_list[-1])\n\n    def split_exclude(self, filter_expr, can_reuse, names_with_path):\n        \"\"\"\n        When doing an exclude against any kind of N-to-many relation, we need\n        to use a subquery. This method constructs the nested query, given the\n        original exclude filter (filter_expr) and the portion up to the first\n        N-to-many relation field.\n\n        For example, if the origin filter is ~Q(child__name='foo'), filter_expr\n        is ('child__name', 'foo') and can_reuse is a set of joins usable for\n        filters in the original query.\n\n        We will turn this into equivalent of:\n            WHERE NOT (pk IN (SELECT parent_id FROM thetable\n                              WHERE name = 'foo' AND parent_id IS NOT NULL))\n\n        It might be worth it to consider using WHERE NOT EXISTS as that has\n        saner null handling, and is easier for the backend's optimizer to\n        handle.\n        \"\"\"\n        filter_lhs, filter_rhs = filter_expr\n        if isinstance(filter_rhs, OuterRef):\n            filter_expr = (filter_lhs, OuterRef(filter_rhs))\n        elif isinstance(filter_rhs, F):\n            filter_expr = (filter_lhs, OuterRef(filter_rhs.name))\n        # Generate the inner query.\n        query = Query(self.model)\n        query._filtered_relations = self._filtered_relations\n        query.add_filter(filter_expr)\n        query.clear_ordering(True)\n        # Try to have as simple as possible subquery -> trim leading joins from\n        # the subquery.\n        trimmed_prefix, contains_louter = query.trim_start(names_with_path)\n\n        # Add extra check to make sure the selected field will not be null\n        # since we are adding an IN <subquery> clause. This prevents the\n        # database from tripping over IN (...,NULL,...) selects and returning\n        # nothing\n        col = query.select[0]\n        select_field = col.target\n        alias = col.alias\n        if self.is_nullable(select_field):\n            lookup_class = select_field.get_lookup('isnull')\n            lookup = lookup_class(select_field.get_col(alias), False)\n            query.where.add(lookup, AND)\n        if alias in can_reuse:\n            pk = select_field.model._meta.pk\n            # Need to add a restriction so that outer query's filters are in effect for\n            # the subquery, too.\n            query.bump_prefix(self)\n            lookup_class = select_field.get_lookup('exact')\n            # Note that the query.select[0].alias is different from alias\n            # due to bump_prefix above.\n            lookup = lookup_class(pk.get_col(query.select[0].alias),\n                                  pk.get_col(alias))\n            query.where.add(lookup, AND)\n            query.external_aliases[alias] = True\n\n        condition, needed_inner = self.build_filter(\n            ('%s__in' % trimmed_prefix, query),\n            current_negated=True, branch_negated=True, can_reuse=can_reuse)\n        if contains_louter:\n            or_null_condition, _ = self.build_filter(\n                ('%s__isnull' % trimmed_prefix, True),\n                current_negated=True, branch_negated=True, can_reuse=can_reuse)\n            condition.add(or_null_condition, OR)\n            # Note that the end result will be:\n            # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.\n            # This might look crazy but due to how IN works, this seems to be\n            # correct. If the IS NOT NULL check is removed then outercol NOT\n            # IN will return UNKNOWN. If the IS NULL check is removed, then if\n            # outercol IS NULL we will not match the row.\n        return condition, needed_inner\n\n    def set_empty(self):\n        self.where.add(NothingNode(), AND)\n\n    def is_empty(self):\n        return any(isinstance(c, NothingNode) for c in self.where.children)\n\n    def set_limits(self, low=None, high=None):\n        \"\"\"\n        Adjust the limits on the rows retrieved. Use low/high to set these,\n        as it makes it more Pythonic to read and write. When the SQL query is\n        created, convert them to the appropriate offset and limit values.\n\n        Apply any limits passed in here to the existing constraints. Add low\n        to the current low value and clamp both to any existing high value.\n        \"\"\"\n        if high is not None:\n            if self.high_mark is not None:\n                self.high_mark = min(self.high_mark, self.low_mark + high)\n            else:\n                self.high_mark = self.low_mark + high\n        if low is not None:\n            if self.high_mark is not None:\n                self.low_mark = min(self.high_mark, self.low_mark + low)\n            else:\n                self.low_mark = self.low_mark + low\n\n        if self.low_mark == self.high_mark:\n            self.set_empty()\n\n    def clear_limits(self):\n        \"\"\"Clear any existing limits.\"\"\"\n        self.low_mark, self.high_mark = 0, None\n\n    @property\n    def is_sliced(self):\n        return self.low_mark != 0 or self.high_mark is not None\n\n    def has_limit_one(self):\n        return self.high_mark is not None and (self.high_mark - self.low_mark) == 1\n\n    def can_filter(self):\n        \"\"\"\n        Return True if adding filters to this instance is still possible.\n\n        Typically, this means no limits or offsets have been put on the results.\n        \"\"\"\n        return not self.is_sliced\n\n    def clear_select_clause(self):\n        \"\"\"Remove all fields from SELECT clause.\"\"\"\n        self.select = ()\n        self.default_cols = False\n        self.select_related = False\n        self.set_extra_mask(())\n        self.set_annotation_mask(())\n\n    def clear_select_fields(self):\n        \"\"\"\n        Clear the list of fields to select (but not extra_select columns).\n        Some queryset types completely replace any existing list of select\n        columns.\n        \"\"\"\n        self.select = ()\n        self.values_select = ()\n\n    def add_select_col(self, col):\n        self.select += col,\n        self.values_select += col.output_field.name,\n\n    def set_select(self, cols):\n        self.default_cols = False\n        self.select = tuple(cols)\n\n    def add_distinct_fields(self, *field_names):\n        \"\"\"\n        Add and resolve the given fields to the query's \"distinct on\" clause.\n        \"\"\"\n        self.distinct_fields = field_names\n        self.distinct = True\n\n    def add_fields(self, field_names, allow_m2m=True):\n        \"\"\"\n        Add the given (model) fields to the select set. Add the field names in\n        the order specified.\n        \"\"\"\n        alias = self.get_initial_alias()\n        opts = self.get_meta()\n\n        try:\n            cols = []\n            for name in field_names:\n                # Join promotion note - we must not remove any rows here, so\n                # if there is no existing joins, use outer join.\n                join_info = self.setup_joins(name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)\n                targets, final_alias, joins = self.trim_joins(\n                    join_info.targets,\n                    join_info.joins,\n                    join_info.path,\n                )\n                for target in targets:\n                    cols.append(join_info.transform_function(target, final_alias))\n            if cols:\n                self.set_select(cols)\n        except MultiJoin:\n            raise FieldError(\"Invalid field name: '%s'\" % name)\n        except FieldError:\n            if LOOKUP_SEP in name:\n                # For lookups spanning over relationships, show the error\n                # from the model on which the lookup failed.\n                raise\n            else:\n                names = sorted([\n                    *get_field_names_from_opts(opts), *self.extra,\n                    *self.annotation_select, *self._filtered_relations\n                ])\n                raise FieldError(\"Cannot resolve keyword %r into field. \"\n                                 \"Choices are: %s\" % (name, \", \".join(names)))\n\n    def add_ordering(self, *ordering):\n        \"\"\"\n        Add items from the 'ordering' sequence to the query's \"order by\"\n        clause. These items are either field names (not column names) --\n        possibly with a direction prefix ('-' or '?') -- or OrderBy\n        expressions.\n\n        If 'ordering' is empty, clear all ordering from the query.\n        \"\"\"\n        errors = []\n        for item in ordering:\n            if isinstance(item, str):\n                if '.' in item:\n                    warnings.warn(\n                        'Passing column raw column aliases to order_by() is '\n                        'deprecated. Wrap %r in a RawSQL expression before '\n                        'passing it to order_by().' % item,\n                        category=RemovedInDjango40Warning,\n                        stacklevel=3,\n                    )\n                    continue\n                if item == '?':\n                    continue\n                if item.startswith('-'):\n                    item = item[1:]\n                if item in self.annotations:\n                    continue\n                if self.extra and item in self.extra:\n                    continue\n                # names_to_path() validates the lookup. A descriptive\n                # FieldError will be raise if it's not.\n                self.names_to_path(item.split(LOOKUP_SEP), self.model._meta)\n            elif not hasattr(item, 'resolve_expression'):\n                errors.append(item)\n            if getattr(item, 'contains_aggregate', False):\n                raise FieldError(\n                    'Using an aggregate in order_by() without also including '\n                    'it in annotate() is not allowed: %s' % item\n                )\n        if errors:\n            raise FieldError('Invalid order_by arguments: %s' % errors)\n        if ordering:\n            self.order_by += ordering\n        else:\n            self.default_ordering = False\n\n    def clear_ordering(self, force_empty):\n        \"\"\"\n        Remove any ordering settings. If 'force_empty' is True, there will be\n        no ordering in the resulting query (not even the model's default).\n        \"\"\"\n        self.order_by = ()\n        self.extra_order_by = ()\n        if force_empty:\n            self.default_ordering = False\n\n    def set_group_by(self, allow_aliases=True):\n        \"\"\"\n        Expand the GROUP BY clause required by the query.\n\n        This will usually be the set of all non-aggregate fields in the\n        return data. If the database backend supports grouping by the\n        primary key, and the query would be equivalent, the optimization\n        will be made automatically.\n        \"\"\"\n        # Column names from JOINs to check collisions with aliases.\n        if allow_aliases:\n            column_names = set()\n            seen_models = set()\n            for join in list(self.alias_map.values())[1:]:  # Skip base table.\n                model = join.join_field.related_model\n                if model not in seen_models:\n                    column_names.update({\n                        field.column\n                        for field in model._meta.local_concrete_fields\n                    })\n                    seen_models.add(model)\n\n        group_by = list(self.select)\n        if self.annotation_select:\n            for alias, annotation in self.annotation_select.items():\n                signature = inspect.signature(annotation.get_group_by_cols)\n                if 'alias' not in signature.parameters:\n                    annotation_class = annotation.__class__\n                    msg = (\n                        '`alias=None` must be added to the signature of '\n                        '%s.%s.get_group_by_cols().'\n                    ) % (annotation_class.__module__, annotation_class.__qualname__)\n                    warnings.warn(msg, category=RemovedInDjango40Warning)\n                    group_by_cols = annotation.get_group_by_cols()\n                else:\n                    if not allow_aliases or alias in column_names:\n                        alias = None\n                    group_by_cols = annotation.get_group_by_cols(alias=alias)\n                group_by.extend(group_by_cols)\n        self.group_by = tuple(group_by)\n\n    def add_select_related(self, fields):\n        \"\"\"\n        Set up the select_related data structure so that we only select\n        certain related models (as opposed to all models, when\n        self.select_related=True).\n        \"\"\"\n        if isinstance(self.select_related, bool):\n            field_dict = {}\n        else:\n            field_dict = self.select_related\n        for field in fields:\n            d = field_dict\n            for part in field.split(LOOKUP_SEP):\n                d = d.setdefault(part, {})\n        self.select_related = field_dict\n\n    def add_extra(self, select, select_params, where, params, tables, order_by):\n        \"\"\"\n        Add data to the various extra_* attributes for user-created additions\n        to the query.\n        \"\"\"\n        if select:\n            # We need to pair any placeholder markers in the 'select'\n            # dictionary with their parameters in 'select_params' so that\n            # subsequent updates to the select dictionary also adjust the\n            # parameters appropriately.\n            select_pairs = {}\n            if select_params:\n                param_iter = iter(select_params)\n            else:\n                param_iter = iter([])\n            for name, entry in select.items():\n                entry = str(entry)\n                entry_params = []\n                pos = entry.find(\"%s\")\n                while pos != -1:\n                    if pos == 0 or entry[pos - 1] != '%':\n                        entry_params.append(next(param_iter))\n                    pos = entry.find(\"%s\", pos + 2)\n                select_pairs[name] = (entry, entry_params)\n            self.extra.update(select_pairs)\n        if where or params:\n            self.where.add(ExtraWhere(where, params), AND)\n        if tables:\n            self.extra_tables += tuple(tables)\n        if order_by:\n            self.extra_order_by = order_by\n\n    def clear_deferred_loading(self):\n        \"\"\"Remove any fields from the deferred loading set.\"\"\"\n        self.deferred_loading = (frozenset(), True)\n\n    def add_deferred_loading(self, field_names):\n        \"\"\"\n        Add the given list of model field names to the set of fields to\n        exclude from loading from the database when automatic column selection\n        is done. Add the new field names to any existing field names that\n        are deferred (or removed from any existing field names that are marked\n        as the only ones for immediate loading).\n        \"\"\"\n        # Fields on related models are stored in the literal double-underscore\n        # format, so that we can use a set datastructure. We do the foo__bar\n        # splitting and handling when computing the SQL column names (as part of\n        # get_columns()).\n        existing, defer = self.deferred_loading\n        if defer:\n            # Add to existing deferred names.\n            self.deferred_loading = existing.union(field_names), True\n        else:\n            # Remove names from the set of any existing \"immediate load\" names.\n            self.deferred_loading = existing.difference(field_names), False\n\n    def add_immediate_loading(self, field_names):\n        \"\"\"\n        Add the given list of model field names to the set of fields to\n        retrieve when the SQL is executed (\"immediate loading\" fields). The\n        field names replace any existing immediate loading field names. If\n        there are field names already specified for deferred loading, remove\n        those names from the new field_names before storing the new names\n        for immediate loading. (That is, immediate loading overrides any\n        existing immediate values, but respects existing deferrals.)\n        \"\"\"\n        existing, defer = self.deferred_loading\n        field_names = set(field_names)\n        if 'pk' in field_names:\n            field_names.remove('pk')\n            field_names.add(self.get_meta().pk.name)\n\n        if defer:\n            # Remove any existing deferred names from the current set before\n            # setting the new names.\n            self.deferred_loading = field_names.difference(existing), False\n        else:\n            # Replace any existing \"immediate load\" field names.\n            self.deferred_loading = frozenset(field_names), False\n\n    def get_loaded_field_names(self):\n        \"\"\"\n        If any fields are marked to be deferred, return a dictionary mapping\n        models to a set of names in those fields that will be loaded. If a\n        model is not in the returned dictionary, none of its fields are\n        deferred.\n\n        If no fields are marked for deferral, return an empty dictionary.\n        \"\"\"\n        # We cache this because we call this function multiple times\n        # (compiler.fill_related_selections, query.iterator)\n        try:\n            return self._loaded_field_names_cache\n        except AttributeError:\n            collection = {}\n            self.deferred_to_data(collection, self.get_loaded_field_names_cb)\n            self._loaded_field_names_cache = collection\n            return collection\n\n    def get_loaded_field_names_cb(self, target, model, fields):\n        \"\"\"Callback used by get_deferred_field_names().\"\"\"\n        target[model] = {f.attname for f in fields}\n\n    def set_annotation_mask(self, names):\n        \"\"\"Set the mask of annotations that will be returned by the SELECT.\"\"\"\n        if names is None:\n            self.annotation_select_mask = None\n        else:\n            self.annotation_select_mask = set(names)\n        self._annotation_select_cache = None\n\n    def append_annotation_mask(self, names):\n        if self.annotation_select_mask is not None:\n            self.set_annotation_mask(self.annotation_select_mask.union(names))\n\n    def set_extra_mask(self, names):\n        \"\"\"\n        Set the mask of extra select items that will be returned by SELECT.\n        Don't remove them from the Query since they might be used later.\n        \"\"\"\n        if names is None:\n            self.extra_select_mask = None\n        else:\n            self.extra_select_mask = set(names)\n        self._extra_select_cache = None\n\n    def set_values(self, fields):\n        self.select_related = False\n        self.clear_deferred_loading()\n        self.clear_select_fields()\n\n        if fields:\n            field_names = []\n            extra_names = []\n            annotation_names = []\n            if not self.extra and not self.annotations:\n                # Shortcut - if there are no extra or annotations, then\n                # the values() clause must be just field names.\n                field_names = list(fields)\n            else:\n                self.default_cols = False\n                for f in fields:\n                    if f in self.extra_select:\n                        extra_names.append(f)\n                    elif f in self.annotation_select:\n                        annotation_names.append(f)\n                    else:\n                        field_names.append(f)\n(B)             obj.annotation_select_mask = None\n        else:\n            obj.annotation_select_mask = self.annotation_select_mask.copy()\n        # _annotation_select_cache cannot be copied, as doing so breaks the\n        # (necessary) state in which both annotations and\n        # _annotation_select_cache point to the same underlying objects.\n        # It will get re-populated in the cloned queryset the next time it's\n        # used.\n        obj._annotation_select_cache = None\n        obj.extra = self.extra.copy()\n        if self.extra_select_mask is None:\n            obj.extra_select_mask = None\n        else:\n            obj.extra_select_mask = self.extra_select_mask.copy()\n        if self._extra_select_cache is None:\n            obj._extra_select_cache = None\n        else:\n            obj._extra_select_cache = self._extra_select_cache.copy()\n        if self.select_related is not False:\n            # Use deepcopy because select_related stores fields in nested\n            # dicts.\n            obj.select_related = copy.deepcopy(obj.select_related)\n        if 'subq_aliases' in self.__dict__:\n            obj.subq_aliases = self.subq_aliases.copy()\n        obj.used_aliases = self.used_aliases.copy()\n        obj._filtered_relations = self._filtered_relations.copy()\n        # Clear the cached_property\n        try:\n            del obj.base_table\n        except AttributeError:\n            pass\n        return obj\n\n    def chain(self, klass=None):\n        \"\"\"\n        Return a copy of the current Query that's ready for another operation.\n        The klass argument changes the type of the Query, e.g. UpdateQuery.\n        \"\"\"\n        obj = self.clone()\n        if klass and obj.__class__ != klass:\n            obj.__class__ = klass\n        if not obj.filter_is_sticky:\n            obj.used_aliases = set()\n        obj.filter_is_sticky = False\n        if hasattr(obj, '_setup_query'):\n            obj._setup_query()\n        return obj\n\n    def relabeled_clone(self, change_map):\n        clone = self.clone()\n        clone.change_aliases(change_map)\n        return clone\n\n    def _get_col(self, target, field, alias):\n        if not self.alias_cols:\n            alias = None\n        return target.get_col(alias, field)\n\n    def rewrite_cols(self, annotation, col_cnt):\n        # We must make sure the inner query has the referred columns in it.\n        # If we are aggregating over an annotation, then Django uses Ref()\n        # instances to note this. However, if we are annotating over a column\n        # of a related model, then it might be that column isn't part of the\n        # SELECT clause of the inner query, and we must manually make sure\n        # the column is selected. An example case is:\n        #    .aggregate(Sum('author__awards'))\n        # Resolving this expression results in a join to author, but there\n        # is no guarantee the awards column of author is in the select clause\n        # of the query. Thus we must manually add the column to the inner\n        # query.\n        orig_exprs = annotation.get_source_expressions()\n        new_exprs = []\n        for expr in orig_exprs:\n            # FIXME: These conditions are fairly arbitrary. Identify a better\n            # method of having expressions decide which code path they should\n            # take.\n            if isinstance(expr, Ref):\n                # Its already a Ref to subquery (see resolve_ref() for\n                # details)\n                new_exprs.append(expr)\n            elif isinstance(expr, (WhereNode, Lookup)):\n                # Decompose the subexpressions further. The code here is\n                # copied from the else clause, but this condition must appear\n                # before the contains_aggregate/is_summary condition below.\n                new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)\n                new_exprs.append(new_expr)\n            else:\n                # Reuse aliases of expressions already selected in subquery.\n                for col_alias, selected_annotation in self.annotation_select.items():\n                    if selected_annotation is expr:\n                        new_expr = Ref(col_alias, expr)\n                        break\n                else:\n                    # An expression that is not selected the subquery.\n                    if isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):\n                        # Reference column or another aggregate. Select it\n                        # under a non-conflicting alias.\n                        col_cnt += 1\n                        col_alias = '__col%d' % col_cnt\n                        self.annotations[col_alias] = expr\n                        self.append_annotation_mask([col_alias])\n                        new_expr = Ref(col_alias, expr)\n                    else:\n                        # Some other expression not referencing database values\n                        # directly. Its subexpression might contain Cols.\n                        new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)\n                new_exprs.append(new_expr)\n        annotation.set_source_expressions(new_exprs)\n        return annotation, col_cnt\n\n    def get_aggregation(self, using, added_aggregate_names):\n        \"\"\"\n        Return the dictionary with the values of the existing aggregations.\n        \"\"\"\n        if not self.annotation_select:\n            return {}\n        existing_annotations = [\n            annotation for alias, annotation\n            in self.annotations.items()\n            if alias not in added_aggregate_names\n        ]\n        # Decide if we need to use a subquery.\n        #\n        # Existing annotations would cause incorrect results as get_aggregation()\n        # must produce just one result and thus must not use GROUP BY. But we\n        # aren't smart enough to remove the existing annotations from the\n        # query, so those would force us to use GROUP BY.\n        #\n        # If the query has limit or distinct, or uses set operations, then\n        # those operations must be done in a subquery so that the query\n        # aggregates on the limit and/or distinct results instead of applying\n        # the distinct and limit after the aggregation.\n        if (isinstance(self.group_by, tuple) or self.is_sliced or existing_annotations or\n                self.distinct or self.combinator):\n            from django.db.models.sql.subqueries import AggregateQuery\n            outer_query = AggregateQuery(self.model)\n            inner_query = self.clone()\n            inner_query.select_for_update = False\n            inner_query.select_related = False\n            inner_query.set_annotation_mask(self.annotation_select)\n            if not self.is_sliced and not self.distinct_fields:\n                # Queries with distinct_fields need ordering and when a limit\n                # is applied we must take the slice from the ordered query.\n                # Otherwise no need for ordering.\n                inner_query.clear_ordering(True)\n            if not inner_query.distinct:\n                # If the inner query uses default select and it has some\n                # aggregate annotations, then we must make sure the inner\n                # query is grouped by the main model's primary key. However,\n                # clearing the select clause can alter results if distinct is\n                # used.\n                has_existing_aggregate_annotations = any(\n                    annotation for annotation in existing_annotations\n                    if getattr(annotation, 'contains_aggregate', True)\n                )\n                if inner_query.default_cols and has_existing_aggregate_annotations:\n                    inner_query.group_by = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)\n                inner_query.default_cols = False\n\n            relabels = {t: 'subquery' for t in inner_query.alias_map}\n            relabels[None] = 'subquery'\n            # Remove any aggregates marked for reduction from the subquery\n            # and move them to the outer AggregateQuery.\n            col_cnt = 0\n            for alias, expression in list(inner_query.annotation_select.items()):\n                annotation_select_mask = inner_query.annotation_select_mask\n                if expression.is_summary:\n                    expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)\n                    outer_query.annotations[alias] = expression.relabeled_clone(relabels)\n                    del inner_query.annotations[alias]\n                    annotation_select_mask.remove(alias)\n                # Make sure the annotation_select wont use cached results.\n                inner_query.set_annotation_mask(inner_query.annotation_select_mask)\n            if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:\n                # In case of Model.objects[0:3].count(), there would be no\n                # field selected in the inner query, yet we must use a subquery.\n                # So, make sure at least one field is selected.\n                inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)\n            try:\n                outer_query.add_subquery(inner_query, using)\n            except EmptyResultSet:\n                return {\n                    alias: None\n                    for alias in outer_query.annotation_select\n                }\n        else:\n            outer_query = self\n            self.select = ()\n            self.default_cols = False\n            self.extra = {}\n\n        outer_query.clear_ordering(True)\n        outer_query.clear_limits()\n        outer_query.select_for_update = False\n        outer_query.select_related = False\n        compiler = outer_query.get_compiler(using)\n        result = compiler.execute_sql(SINGLE)\n        if result is None:\n            result = [None] * len(outer_query.annotation_select)\n\n        converters = compiler.get_converters(outer_query.annotation_select.values())\n        result = next(compiler.apply_converters((result,), converters))\n\n        return dict(zip(outer_query.annotation_select, result))\n\n    def get_count(self, using):\n        \"\"\"\n        Perform a COUNT() query using the current filter constraints.\n        \"\"\"\n        obj = self.clone()\n        obj.add_annotation(Count('*'), alias='__count', is_summary=True)\n        number = obj.get_aggregation(using, ['__count'])['__count']\n        if number is None:\n            number = 0\n        return number\n\n    def has_filters(self):\n        return self.where\n\n    def has_results(self, using):\n        q = self.clone()\n        if not q.distinct:\n            if q.group_by is True:\n                q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)\n                # Disable GROUP BY aliases to avoid orphaning references to the\n                # SELECT clause which is about to be cleared.\n                q.set_group_by(allow_aliases=False)\n            q.clear_select_clause()\n        q.clear_ordering(True)\n        q.set_limits(high=1)\n        compiler = q.get_compiler(using=using)\n        return compiler.has_results()\n\n    def explain(self, using, format=None, **options):\n        q = self.clone()\n        q.explain_query = True\n        q.explain_format = format\n        q.explain_options = options\n        compiler = q.get_compiler(using=using)\n        return '\\n'.join(compiler.explain_query())\n\n    def combine(self, rhs, connector):\n        \"\"\"\n        Merge the 'rhs' query into the current one (with any 'rhs' effects\n        being applied *after* (that is, \"to the right of\") anything in the\n        current query. 'rhs' is not modified during a call to this function.\n\n        The 'connector' parameter describes how to connect filters from the\n        'rhs' query.\n        \"\"\"\n        assert self.model == rhs.model, \\\n            \"Cannot combine queries on two different base models.\"\n        assert not self.is_sliced, \\\n            \"Cannot combine queries once a slice has been taken.\"\n        assert self.distinct == rhs.distinct, \\\n            \"Cannot combine a unique query with a non-unique query.\"\n        assert self.distinct_fields == rhs.distinct_fields, \\\n            \"Cannot combine queries with different distinct fields.\"\n\n        # Work out how to relabel the rhs aliases, if necessary.\n        change_map = {}\n        conjunction = (connector == AND)\n\n        # Determine which existing joins can be reused. When combining the\n        # query with AND we must recreate all joins for m2m filters. When\n        # combining with OR we can reuse joins. The reason is that in AND\n        # case a single row can't fulfill a condition like:\n        #     revrel__col=1 & revrel__col=2\n        # But, there might be two different related rows matching this\n        # condition. In OR case a single True is enough, so single row is\n        # enough, too.\n        #\n        # Note that we will be creating duplicate joins for non-m2m joins in\n        # the AND case. The results will be correct but this creates too many\n        # joins. This is something that could be fixed later on.\n        reuse = set() if conjunction else set(self.alias_map)\n        # Base table must be present in the query - this is the same\n        # table on both sides.\n        self.get_initial_alias()\n        joinpromoter = JoinPromoter(connector, 2, False)\n        joinpromoter.add_votes(\n            j for j in self.alias_map if self.alias_map[j].join_type == INNER)\n        rhs_votes = set()\n        # Now, add the joins from rhs query into the new query (skipping base\n        # table).\n        rhs_tables = list(rhs.alias_map)[1:]\n        for alias in rhs_tables:\n            join = rhs.alias_map[alias]\n            # If the left side of the join was already relabeled, use the\n            # updated alias.\n            join = join.relabeled_clone(change_map)\n            new_alias = self.join(join, reuse=reuse)\n            if join.join_type == INNER:\n                rhs_votes.add(new_alias)\n            # We can't reuse the same join again in the query. If we have two\n            # distinct joins for the same connection in rhs query, then the\n            # combined query must have two joins, too.\n            reuse.discard(new_alias)\n            if alias != new_alias:\n                change_map[alias] = new_alias\n            if not rhs.alias_refcount[alias]:\n                # The alias was unused in the rhs query. Unref it so that it\n                # will be unused in the new query, too. We have to add and\n                # unref the alias so that join promotion has information of\n                # the join type for the unused alias.\n                self.unref_alias(new_alias)\n        joinpromoter.add_votes(rhs_votes)\n        joinpromoter.update_join_types(self)\n\n        # Now relabel a copy of the rhs where-clause and add it to the current\n        # one.\n        w = rhs.where.clone()\n        w.relabel_aliases(change_map)\n        self.where.add(w, connector)\n\n        # Selection columns and extra extensions are those provided by 'rhs'.\n        if rhs.select:\n            self.set_select([col.relabeled_clone(change_map) for col in rhs.select])\n        else:\n            self.select = ()\n\n        if connector == OR:\n            # It would be nice to be able to handle this, but the queries don't\n            # really make sense (or return consistent value sets). Not worth\n            # the extra complexity when you can write a real query instead.\n            if self.extra and rhs.extra:\n                raise ValueError(\"When merging querysets using 'or', you cannot have extra(select=...) on both sides.\")\n        self.extra.update(rhs.extra)\n        extra_select_mask = set()\n        if self.extra_select_mask is not None:\n            extra_select_mask.update(self.extra_select_mask)\n        if rhs.extra_select_mask is not None:\n            extra_select_mask.update(rhs.extra_select_mask)\n        if extra_select_mask:\n            self.set_extra_mask(extra_select_mask)\n        self.extra_tables += rhs.extra_tables\n\n        # Ordering uses the 'rhs' ordering, unless it has none, in which case\n        # the current ordering is used.\n        self.order_by = rhs.order_by or self.order_by\n        self.extra_order_by = rhs.extra_order_by or self.extra_order_by\n\n    def deferred_to_data(self, target, callback):\n        \"\"\"\n        Convert the self.deferred_loading data structure to an alternate data\n        structure, describing the field that *will* be loaded. This is used to\n        compute the columns to select from the database and also by the\n        QuerySet class to work out which fields are being initialized on each\n        model. Models that have all their fields included aren't mentioned in\n        the result, only those that have field restrictions in place.\n\n        The \"target\" parameter is the instance that is populated (in place).\n        The \"callback\" is a function that is called whenever a (model, field)\n        pair need to be added to \"target\". It accepts three parameters:\n        \"target\", and the model and list of fields being added for that model.\n        \"\"\"\n        field_names, defer = self.deferred_loading\n        if not field_names:\n            return\n        orig_opts = self.get_meta()\n        seen = {}\n        must_include = {orig_opts.concrete_model: {orig_opts.pk}}\n        for field_name in field_names:\n            parts = field_name.split(LOOKUP_SEP)\n            cur_model = self.model._meta.concrete_model\n            opts = orig_opts\n            for name in parts[:-1]:\n                old_model = cur_model\n                if name in self._filtered_relations:\n                    name = self._filtered_relations[name].relation_name\n                source = opts.get_field(name)\n                if is_reverse_o2o(source):\n                    cur_model = source.related_model\n                else:\n                    cur_model = source.remote_field.model\n                opts = cur_model._meta\n                # Even if we're \"just passing through\" this model, we must add\n                # both the current model's pk and the related reference field\n                # (if it's not a reverse relation) to the things we select.\n                if not is_reverse_o2o(source):\n                    must_include[old_model].add(source)\n                add_to_dict(must_include, cur_model, opts.pk)\n            field = opts.get_field(parts[-1])\n            is_reverse_object = field.auto_created and not field.concrete\n            model = field.related_model if is_reverse_object else field.model\n            model = model._meta.concrete_model\n            if model == opts.model:\n                model = cur_model\n            if not is_reverse_o2o(field):\n                add_to_dict(seen, model, field)\n\n        if defer:\n            # We need to load all fields for each model, except those that\n            # appear in \"seen\" (for all models that appear in \"seen\"). The only\n            # slight complexity here is handling fields that exist on parent\n            # models.\n            workset = {}\n            for model, values in seen.items():\n                for field in model._meta.local_fields:\n                    if field not in values:\n                        m = field.model._meta.concrete_model\n                        add_to_dict(workset, m, field)\n            for model, values in must_include.items():\n                # If we haven't included a model in workset, we don't add the\n                # corresponding must_include fields for that model, since an\n                # empty set means \"include all fields\". That's why there's no\n                # \"else\" branch here.\n                if model in workset:\n                    workset[model].update(values)\n            for model, values in workset.items():\n                callback(target, model, values)\n        else:\n            for model, values in must_include.items():\n                if model in seen:\n                    seen[model].update(values)\n                else:\n                    # As we've passed through this model, but not explicitly\n                    # included any fields, we have to make sure it's mentioned\n                    # so that only the \"must include\" fields are pulled in.\n                    seen[model] = values\n            # Now ensure that every model in the inheritance chain is mentioned\n            # in the parent list. Again, it must be mentioned to ensure that\n            # only \"must include\" fields are pulled in.\n            for model in orig_opts.get_parent_list():\n                seen.setdefault(model, set())\n            for model, values in seen.items():\n                callback(target, model, values)\n\n    def table_alias(self, table_name, create=False, filtered_relation=None):\n        \"\"\"\n        Return a table alias for the given table_name and whether this is a\n        new alias or not.\n\n        If 'create' is true, a new alias is always created. Otherwise, the\n        most recently created alias for the table (if one exists) is reused.\n        \"\"\"\n        alias_list = self.table_map.get(table_name)\n        if not create and alias_list:\n            alias = alias_list[0]\n            self.alias_refcount[alias] += 1\n            return alias, False\n\n        # Create a new alias for this table.\n        if alias_list:\n            alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)\n            alias_list.append(alias)\n        else:\n            # The first occurrence of a table uses the table name directly.\n            alias = filtered_relation.alias if filtered_relation is not None else table_name\n            self.table_map[table_name] = [alias]\n        self.alias_refcount[alias] = 1\n        return alias, True\n\n    def ref_alias(self, alias):\n        \"\"\"Increases the reference count for this alias.\"\"\"\n        self.alias_refcount[alias] += 1\n\n    def unref_alias(self, alias, amount=1):\n        \"\"\"Decreases the reference count for this alias.\"\"\"\n        self.alias_refcount[alias] -= amount\n\n    def promote_joins(self, aliases):\n        \"\"\"\n        Promote recursively the join type of given aliases and its children to\n        an outer join. If 'unconditional' is False, only promote the join if\n        it is nullable or the parent join is an outer join.\n\n        The children promotion is done to avoid join chains that contain a LOUTER\n        b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,\n        then we must also promote b->c automatically, or otherwise the promotion\n        of a->b doesn't actually change anything in the query results.\n        \"\"\"\n        aliases = list(aliases)\n        while aliases:\n            alias = aliases.pop(0)\n            if self.alias_map[alias].join_type is None:\n                # This is the base table (first FROM entry) - this table\n                # isn't really joined at all in the query, so we should not\n                # alter its join type.\n                continue\n            # Only the first alias (skipped above) should have None join_type\n            assert self.alias_map[alias].join_type is not None\n            parent_alias = self.alias_map[alias].parent_alias\n            parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER\n            already_louter = self.alias_map[alias].join_type == LOUTER\n            if ((self.alias_map[alias].nullable or parent_louter) and\n                    not already_louter):\n                self.alias_map[alias] = self.alias_map[alias].promote()\n                # Join type of 'alias' changed, so re-examine all aliases that\n                # refer to this one.\n                aliases.extend(\n                    join for join in self.alias_map\n                    if self.alias_map[join].parent_alias == alias and join not in aliases\n                )\n\n    def demote_joins(self, aliases):\n        \"\"\"\n        Change join type from LOUTER to INNER for all joins in aliases.\n\n        Similarly to promote_joins(), this method must ensure no join chains\n        containing first an outer, then an inner join are generated. If we\n        are demoting b->c join in chain a LOUTER b LOUTER c then we must\n        demote a->b automatically, or otherwise the demotion of b->c doesn't\n        actually change anything in the query results. .\n        \"\"\"\n        aliases = list(aliases)\n        while aliases:\n            alias = aliases.pop(0)\n            if self.alias_map[alias].join_type == LOUTER:\n                self.alias_map[alias] = self.alias_map[alias].demote()\n                parent_alias = self.alias_map[alias].parent_alias\n                if self.alias_map[parent_alias].join_type == INNER:\n                    aliases.append(parent_alias)\n\n    def reset_refcounts(self, to_counts):\n        \"\"\"\n        Reset reference counts for aliases so that they match the value passed\n        in `to_counts`.\n        \"\"\"\n        for alias, cur_refcount in self.alias_refcount.copy().items():\n            unref_amount = cur_refcount - to_counts.get(alias, 0)\n            self.unref_alias(alias, unref_amount)\n\n    def change_aliases(self, change_map):\n        \"\"\"\n        Change the aliases in change_map (which maps old-alias -> new-alias),\n        relabelling any references to them in select columns and the where\n        clause.\n        \"\"\"\n        assert set(change_map).isdisjoint(change_map.values())\n\n        # 1. Update references in \"select\" (normal columns plus aliases),\n        # \"group by\" and \"where\".\n        self.where.relabel_aliases(change_map)\n        if isinstance(self.group_by, tuple):\n            self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])\n        self.select = tuple([col.relabeled_clone(change_map) for col in self.select])\n        self.annotations = self.annotations and {\n            key: col.relabeled_clone(change_map) for key, col in self.annotations.items()\n        }\n\n        # 2. Rename the alias in the internal table/alias datastructures.\n        for old_alias, new_alias in change_map.items():\n            if old_alias not in self.alias_map:\n                continue\n            alias_data = self.alias_map[old_alias].relabeled_clone(change_map)\n            self.alias_map[new_alias] = alias_data\n            self.alias_refcount[new_alias] = self.alias_refcount[old_alias]\n            del self.alias_refcount[old_alias]\n            del self.alias_map[old_alias]\n\n            table_aliases = self.table_map[alias_data.table_name]\n            for pos, alias in enumerate(table_aliases):\n                if alias == old_alias:\n                    table_aliases[pos] = new_alias\n                    break\n        self.external_aliases = {\n            # Table is aliased or it's being changed and thus is aliased.\n            change_map.get(alias, alias): (aliased or alias in change_map)\n            for alias, aliased in self.external_aliases.items()\n        }\n\n    def bump_prefix(self, outer_query):\n        \"\"\"\n        Change the alias prefix to the next letter in the alphabet in a way\n        that the outer query's aliases and this query's aliases will not\n        conflict. Even tables that previously had no alias will get an alias\n        after this call.\n        \"\"\"\n        def prefix_gen():\n            \"\"\"\n            Generate a sequence of characters in alphabetical order:\n                -> 'A', 'B', 'C', ...\n\n            When the alphabet is finished, the sequence will continue with the\n            Cartesian product:\n                -> 'AA', 'AB', 'AC', ...\n            \"\"\"\n            alphabet = ascii_uppercase\n            prefix = chr(ord(self.alias_prefix) + 1)\n            yield prefix\n            for n in count(1):\n                seq = alphabet[alphabet.index(prefix):] if prefix else alphabet\n                for s in product(seq, repeat=n):\n                    yield ''.join(s)\n                prefix = None\n\n        if self.alias_prefix != outer_query.alias_prefix:\n            # No clashes between self and outer query should be possible.\n            return\n\n        # Explicitly avoid infinite loop. The constant divider is based on how\n        # much depth recursive subquery references add to the stack. This value\n        # might need to be adjusted when adding or removing function calls from\n        # the code path in charge of performing these operations.\n        local_recursion_limit = sys.getrecursionlimit() // 16\n        for pos, prefix in enumerate(prefix_gen()):\n            if prefix not in self.subq_aliases:\n                self.alias_prefix = prefix\n                break\n            if pos > local_recursion_limit:\n                raise RecursionError(\n                    'Maximum recursion depth exceeded: too many subqueries.'\n                )\n        self.subq_aliases = self.subq_aliases.union([self.alias_prefix])\n        outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)\n        self.change_aliases({\n            alias: '%s%d' % (self.alias_prefix, pos)\n            for pos, alias in enumerate(self.alias_map)\n        })\n\n    def get_initial_alias(self):\n        \"\"\"\n        Return the first alias for this query, after increasing its reference\n        count.\n        \"\"\"\n        if self.alias_map:\n            alias = self.base_table\n            self.ref_alias(alias)\n        else:\n            alias = self.join(BaseTable(self.get_meta().db_table, None))\n        return alias\n\n    def count_active_tables(self):\n        \"\"\"\n        Return the number of tables in this query with a non-zero reference\n        count. After execution, the reference counts are zeroed, so tables\n        added in compiler will not be seen by this method.\n        \"\"\"\n        return len([1 for count in self.alias_refcount.values() if count])\n\n    def join(self, join, reuse=None, reuse_with_filtered_relation=False):\n        \"\"\"\n        Return an alias for the 'join', either reusing an existing alias for\n        that join or creating a new one. 'join' is either a\n        sql.datastructures.BaseTable or Join.\n\n        The 'reuse' parameter can be either None which means all joins are\n        reusable, or it can be a set containing the aliases that can be reused.\n\n        The 'reuse_with_filtered_relation' parameter is used when computing\n        FilteredRelation instances.\n\n        A join is always created as LOUTER if the lhs alias is LOUTER to make\n        sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new\n        joins are created as LOUTER if the join is nullable.\n        \"\"\"\n        if reuse_with_filtered_relation and reuse:\n            reuse_aliases = [\n                a for a, j in self.alias_map.items()\n                if a in reuse and j.equals(join, with_filtered_relation=False)\n            ]\n        else:\n            reuse_aliases = [\n                a for a, j in self.alias_map.items()\n                if (reuse is None or a in reuse) and j == join\n            ]\n        if reuse_aliases:\n            if join.table_alias in reuse_aliases:\n                reuse_alias = join.table_alias\n            else:\n                # Reuse the most recent alias of the joined table\n                # (a many-to-many relation may be joined multiple times).\n                reuse_alias = reuse_aliases[-1]\n            self.ref_alias(reuse_alias)\n            return reuse_alias\n\n        # No reuse is possible, so we need a new alias.\n        alias, _ = self.table_alias(join.table_name, create=True, filtered_relation=join.filtered_relation)\n        if join.join_type:\n            if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:\n                join_type = LOUTER\n            else:\n                join_type = INNER\n            join.join_type = join_type\n        join.table_alias = alias\n        self.alias_map[alias] = join\n        return alias\n\n    def join_parent_model(self, opts, model, alias, seen):\n        \"\"\"\n        Make sure the given 'model' is joined in the query. If 'model' isn't\n        a parent of 'opts' or if it is None this method is a no-op.\n\n        The 'alias' is the root alias for starting the join, 'seen' is a dict\n        of model -> alias of existing joins. It must also contain a mapping\n        of None -> some alias. This will be returned in the no-op case.\n        \"\"\"\n        if model in seen:\n            return seen[model]\n        chain = opts.get_base_chain(model)\n        if not chain:\n            return alias\n        curr_opts = opts\n        for int_model in chain:\n            if int_model in seen:\n                curr_opts = int_model._meta\n                alias = seen[int_model]\n                continue\n            # Proxy model have elements in base chain\n            # with no parents, assign the new options\n            # object and skip to the next base in that\n            # case\n            if not curr_opts.parents[int_model]:\n                curr_opts = int_model._meta\n                continue\n            link_field = curr_opts.get_ancestor_link(int_model)\n            join_info = self.setup_joins([link_field.name], curr_opts, alias)\n            curr_opts = int_model._meta\n            alias = seen[int_model] = join_info.joins[-1]\n        return alias or seen[None]\n\n    def add_annotation(self, annotation, alias, is_summary=False):\n        \"\"\"Add a single annotation expression to the Query.\"\"\"\n        annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n                                                   summarize=is_summary)\n        self.append_annotation_mask([alias])\n        self.annotations[alias] = annotation\n\n    def resolve_expression(self, query, *args, **kwargs):\n        clone = self.clone()\n        # Subqueries need to use a different set of aliases than the outer query.\n        clone.bump_prefix(query)\n        clone.subquery = True\n        # It's safe to drop ordering if the queryset isn't using slicing,\n        # distinct(*fields) or select_for_update().\n        if (self.low_mark == 0 and self.high_mark is None and\n                not self.distinct_fields and\n                not self.select_for_update):\n            clone.clear_ordering(True)\n        clone.where.resolve_expression(query, *args, **kwargs)\n        for key, value in clone.annotations.items():\n            resolved = value.resolve_expression(query, *args, **kwargs)\n            if hasattr(resolved, 'external_aliases'):\n                resolved.external_aliases.update(clone.external_aliases)\n            clone.annotations[key] = resolved\n        # Outer query's aliases are considered external.\n        for alias, table in query.alias_map.items():\n            clone.external_aliases[alias] = (\n                (isinstance(table, Join) and table.join_field.related_model._meta.db_table != alias) or\n                (isinstance(table, BaseTable) and table.table_name != table.table_alias)\n            )\n        return clone\n\n    def get_external_cols(self):\n        exprs = chain(self.annotations.values(), self.where.children)\n        return [\n            col for col in self._gen_cols(exprs)\n            if col.alias in self.external_aliases\n        ]\n\n    def as_sql(self, compiler, connection):\n        sql, params = self.get_compiler(connection=connection).as_sql()\n        if self.subquery:\n            sql = '(%s)' % sql\n        return sql, params\n\n    def resolve_lookup_value(self, value, can_reuse, allow_joins):\n        if hasattr(value, 'resolve_expression'):\n            value = value.resolve_expression(\n                self, reuse=can_reuse, allow_joins=allow_joins,\n            )\n        elif isinstance(value, (list, tuple)):\n            # The items of the iterable may be expressions and therefore need\n            # to be resolved independently.\n            return type(value)(\n                self.resolve_lookup_value(sub_value, can_reuse, allow_joins)\n                for sub_value in value\n            )\n        return value\n\n    def solve_lookup_type(self, lookup):\n        \"\"\"\n        Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').\n        \"\"\"\n        lookup_splitted = lookup.split(LOOKUP_SEP)\n        if self.annotations:\n            expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)\n            if expression:\n                return expression_lookups, (), expression\n        _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())\n        field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]\n        if len(lookup_parts) > 1 and not field_parts:\n            raise FieldError(\n                'Invalid lookup \"%s\" for model %s\".' %\n                (lookup, self.get_meta().model.__name__)\n            )\n        return lookup_parts, field_parts, False\n\n    def check_query_object_type(self, value, opts, field):\n        \"\"\"\n        Check whether the object passed while querying is of the correct type.\n        If not, raise a ValueError specifying the wrong object.\n        \"\"\"\n        if hasattr(value, '_meta'):\n            if not check_rel_lookup_compatibility(value._meta.model, opts, field):\n                raise ValueError(\n                    'Cannot query \"%s\": Must be \"%s\" instance.' %\n                    (value, opts.object_name))\n\n    def check_related_objects(self, field, value, opts):\n        \"\"\"Check the type of object passed to query relations.\"\"\"\n        if field.is_relation:\n            # Check that the field and the queryset use the same model in a\n            # query like .filter(author=Author.objects.all()). For example, the\n            # opts would be Author's (from the author field) and value.model\n            # would be Author.objects.all() queryset's .model (Author also).\n            # The field is the related field on the lhs side.\n            if (isinstance(value, Query) and not value.has_select_fields and\n                    not check_rel_lookup_compatibility(value.model, opts, field)):\n                raise ValueError(\n                    'Cannot use QuerySet for \"%s\": Use a QuerySet for \"%s\".' %\n                    (value.model._meta.object_name, opts.object_name)\n                )\n            elif hasattr(value, '_meta'):\n                self.check_query_object_type(value, opts, field)\n            elif hasattr(value, '__iter__'):\n                for v in value:\n                    self.check_query_object_type(v, opts, field)\n\n    def check_filterable(self, expression):\n        \"\"\"Raise an error if expression cannot be used in a WHERE clause.\"\"\"\n        if (\n            hasattr(expression, 'resolve_expression') and\n            not getattr(expression, 'filterable', True)\n        ):\n            raise NotSupportedError(\n                expression.__class__.__name__ + ' is disallowed in the filter '\n                'clause.'\n            )\n        if hasattr(expression, 'get_source_expressions'):\n            for expr in expression.get_source_expressions():\n                self.check_filterable(expr)\n\n    def build_lookup(self, lookups, lhs, rhs):\n        \"\"\"\n        Try to extract transforms and lookup from given lhs.\n\n        The lhs value is something that works like SQLExpression.\n        The rhs value is what the lookup is going to compare against.\n        The lookups is a list of names to extract using get_lookup()\n        and get_transform().\n        \"\"\"\n        # __exact is the default lookup if one isn't given.\n        *transforms, lookup_name = lookups or ['exact']\n        for name in transforms:\n            lhs = self.try_transform(lhs, name)\n        # First try get_lookup() so that the lookup takes precedence if the lhs\n        # supports both transform and lookup for the name.\n        lookup_class = lhs.get_lookup(lookup_name)\n        if not lookup_class:\n            if lhs.field.is_relation:\n                raise FieldError('Related Field got invalid lookup: {}'.format(lookup_name))\n            # A lookup wasn't found. Try to interpret the name as a transform\n            # and do an Exact lookup against it.\n            lhs = self.try_transform(lhs, lookup_name)\n            lookup_name = 'exact'\n            lookup_class = lhs.get_lookup(lookup_name)\n            if not lookup_class:\n                return\n\n        lookup = lookup_class(lhs, rhs)\n        # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all\n        # uses of None as a query value unless the lookup supports it.\n        if lookup.rhs is None and not lookup.can_use_none_as_rhs:\n            if lookup_name not in ('exact', 'iexact'):\n                raise ValueError(\"Cannot use None as a query value\")\n            return lhs.get_lookup('isnull')(lhs, True)\n\n        # For Oracle '' is equivalent to null. The check must be done at this\n        # stage because join promotion can't be done in the compiler. Using\n        # DEFAULT_DB_ALIAS isn't nice but it's the best that can be done here.\n        # A similar thing is done in is_nullable(), too.\n        if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and\n                lookup_name == 'exact' and lookup.rhs == ''):\n            return lhs.get_lookup('isnull')(lhs, True)\n\n        return lookup\n\n    def try_transform(self, lhs, name):\n        \"\"\"\n        Helper method for build_lookup(). Try to fetch and initialize\n        a transform for name parameter from lhs.\n        \"\"\"\n        transform_class = lhs.get_transform(name)\n        if transform_class:\n            return transform_class(lhs)\n        else:\n            output_field = lhs.output_field.__class__\n            suggested_lookups = difflib.get_close_matches(name, output_field.get_lookups())\n            if suggested_lookups:\n                suggestion = ', perhaps you meant %s?' % ' or '.join(suggested_lookups)\n            else:\n                suggestion = '.'\n            raise FieldError(\n                \"Unsupported lookup '%s' for %s or join on the field not \"\n                \"permitted%s\" % (name, output_field.__name__, suggestion)\n            )\n\n    def build_filter(self, filter_expr, branch_negated=False, current_negated=False,\n                     can_reuse=None, allow_joins=True, split_subq=True,\n                     reuse_with_filtered_relation=False, check_filterable=True):\n        \"\"\"\n        Build a WhereNode for a single filter clause but don't add it\n        to this Query. Query.add_q() will then add this filter to the where\n        Node.\n\n        The 'branch_negated' tells us if the current branch contains any\n        negations. This will be used to determine if subqueries are needed.\n\n        The 'current_negated' is used to determine if the current filter is\n        negated or not and this will be used to determine if IS NULL filtering\n        is needed.\n\n        The difference between current_negated and branch_negated is that\n        branch_negated is set on first negation, but current_negated is\n        flipped for each negation.\n\n        Note that add_filter will not do any negating itself, that is done\n        upper in the code by add_q().\n\n        The 'can_reuse' is a set of reusable joins for multijoins.\n\n        If 'reuse_with_filtered_relation' is True, then only joins in can_reuse\n        will be reused.\n\n        The method will create a filter clause that can be added to the current\n        query. However, if the filter isn't added to the query then the caller\n        is responsible for unreffing the joins used.\n        \"\"\"\n        if isinstance(filter_expr, dict):\n            raise FieldError(\"Cannot parse keyword query as dict\")\n        if isinstance(filter_expr, Q):\n            return self._add_q(\n                filter_expr,\n                branch_negated=branch_negated,\n                current_negated=current_negated,\n                used_aliases=can_reuse,\n                allow_joins=allow_joins,\n                split_subq=split_subq,\n                check_filterable=check_filterable,\n            )\n        if hasattr(filter_expr, 'resolve_expression'):\n            if not getattr(filter_expr, 'conditional', False):\n                raise TypeError('Cannot filter against a non-conditional expression.')\n            condition = self.build_lookup(\n                ['exact'], filter_expr.resolve_expression(self, allow_joins=allow_joins), True\n            )\n            clause = self.where_class()\n            clause.add(condition, AND)\n            return clause, []\n        arg, value = filter_expr\n        if not arg:\n            raise FieldError(\"Cannot parse keyword query %r\" % arg)\n        lookups, parts, reffed_expression = self.solve_lookup_type(arg)\n\n        if check_filterable:\n            self.check_filterable(reffed_expression)\n\n        if not allow_joins and len(parts) > 1:\n            raise FieldError(\"Joined field references are not permitted in this query\")\n\n        pre_joins = self.alias_refcount.copy()\n        value = self.resolve_lookup_value(value, can_reuse, allow_joins)\n        used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}\n\n        if check_filterable:\n            self.check_filterable(value)\n\n        clause = self.where_class()\n        if reffed_expression:\n            condition = self.build_lookup(lookups, reffed_expression, value)\n            clause.add(condition, AND)\n            return clause, []\n\n        opts = self.get_meta()\n        alias = self.get_initial_alias()\n        allow_many = not branch_negated or not split_subq\n\n        try:\n            join_info = self.setup_joins(\n                parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,\n                reuse_with_filtered_relation=reuse_with_filtered_relation,\n            )\n\n            # Prevent iterator from being consumed by check_related_objects()\n            if isinstance(value, Iterator):\n                value = list(value)\n            self.check_related_objects(join_info.final_field, value, join_info.opts)\n\n            # split_exclude() needs to know which joins were generated for the\n            # lookup parts\n            self._lookup_joins = join_info.joins\n        except MultiJoin as e:\n            return self.split_exclude(filter_expr, can_reuse, e.names_with_path)\n\n        # Update used_joins before trimming since they are reused to determine\n        # which joins could be later promoted to INNER.\n        used_joins.update(join_info.joins)\n        targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)\n        if can_reuse is not None:\n            can_reuse.update(join_list)\n\n        if join_info.final_field.is_relation:\n            # No support for transforms for relational fields\n            num_lookups = len(lookups)\n            if num_lookups > 1:\n                raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))\n            if len(targets) == 1:\n                col = self._get_col(targets[0], join_info.final_field, alias)\n            else:\n                col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)\n        else:\n            col = self._get_col(targets[0], join_info.final_field, alias)\n\n        condition = self.build_lookup(lookups, col, value)\n        lookup_type = condition.lookup_name\n        clause.add(condition, AND)\n\n        require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated\n        if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:\n            require_outer = True\n            if lookup_type != 'isnull':\n                # The condition added here will be SQL like this:\n                # NOT (col IS NOT NULL), where the first NOT is added in\n                # upper layers of code. The reason for addition is that if col\n                # is null, then col != someval will result in SQL \"unknown\"\n                # which isn't the same as in Python. The Python None handling\n                # is wanted, and it can be gotten by\n                # (col IS NULL OR col != someval)\n                #   <=>\n                # NOT (col IS NOT NULL AND col = someval).\n                if (\n                    self.is_nullable(targets[0]) or\n                    self.alias_map[join_list[-1]].join_type == LOUTER\n                ):\n                    lookup_class = targets[0].get_lookup('isnull')\n                    col = self._get_col(targets[0], join_info.targets[0], alias)\n                    clause.add(lookup_class(col, False), AND)\n                # If someval is a nullable column, someval IS NOT NULL is\n                # added.\n                if isinstance(value, Col) and self.is_nullable(value.target):\n                    lookup_class = value.target.get_lookup('isnull')\n                    clause.add(lookup_class(value, False), AND)\n        return clause, used_joins if not require_outer else ()\n\n    def add_filter(self, filter_clause):\n        self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))\n\n    def add_q(self, q_object):\n        \"\"\"\n        A preprocessor for the internal _add_q(). Responsible for doing final\n        join promotion.\n        \"\"\"\n        # For join promotion this case is doing an AND for the added q_object\n        # and existing conditions. So, any existing inner join forces the join\n        # type to remain inner. Existing outer joins can however be demoted.\n        # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if\n        # rel_a doesn't produce any rows, then the whole condition must fail.\n        # So, demotion is OK.\n        existing_inner = {a for a in self.alias_map if self.alias_map[a].join_type == INNER}\n        clause, _ = self._add_q(q_object, self.used_aliases)\n        if clause:\n            self.where.add(clause, AND)\n        self.demote_joins(existing_inner)\n\n    def build_where(self, filter_expr):\n        return self.build_filter(filter_expr, allow_joins=False)[0]\n\n    def _add_q(self, q_object, used_aliases, branch_negated=False,\n               current_negated=False, allow_joins=True, split_subq=True,\n               check_filterable=True):\n        \"\"\"Add a Q-object to the current filter.\"\"\"\n        connector = q_object.connector\n        current_negated = current_negated ^ q_object.negated\n        branch_negated = branch_negated or q_object.negated\n        target_clause = self.where_class(connector=connector,\n                                         negated=q_object.negated)\n        joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)\n        for child in q_object.children:\n            child_clause, needed_inner = self.build_filter(\n                child, can_reuse=used_aliases, branch_negated=branch_negated,\n                current_negated=current_negated, allow_joins=allow_joins,\n                split_subq=split_subq, check_filterable=check_filterable,\n            )\n            joinpromoter.add_votes(needed_inner)\n            if child_clause:\n                target_clause.add(child_clause, connector)\n        needed_inner = joinpromoter.update_join_types(self)\n        return target_clause, needed_inner\n\n    def build_filtered_relation_q(self, q_object, reuse, branch_negated=False, current_negated=False):\n        \"\"\"Add a FilteredRelation object to the current filter.\"\"\"\n        connector = q_object.connector\n        current_negated ^= q_object.negated\n        branch_negated = branch_negated or q_object.negated\n        target_clause = self.where_class(connector=connector, negated=q_object.negated)\n        for child in q_object.children:\n            if isinstance(child, Node):\n                child_clause = self.build_filtered_relation_q(\n                    child, reuse=reuse, branch_negated=branch_negated,\n                    current_negated=current_negated,\n                )\n            else:\n                child_clause, _ = self.build_filter(\n                    child, can_reuse=reuse, branch_negated=branch_negated,\n                    current_negated=current_negated,\n                    allow_joins=True, split_subq=False,\n                    reuse_with_filtered_relation=True,\n                )\n            target_clause.add(child_clause, connector)\n        return target_clause\n\n    def add_filtered_relation(self, filtered_relation, alias):\n        filtered_relation.alias = alias\n        lookups = dict(get_children_from_q(filtered_relation.condition))\n        for lookup in chain((filtered_relation.relation_name,), lookups):\n            lookup_parts, field_parts, _ = self.solve_lookup_type(lookup)\n            shift = 2 if not lookup_parts else 1\n            if len(field_parts) > (shift + len(lookup_parts)):\n                raise ValueError(\n                    \"FilteredRelation's condition doesn't support nested \"\n                    \"relations (got %r).\" % lookup\n                )\n        self._filtered_relations[filtered_relation.alias] = filtered_relation\n\n    def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):\n        \"\"\"\n        Walk the list of names and turns them into PathInfo tuples. A single\n        name in 'names' can generate multiple PathInfos (m2m, for example).\n\n        'names' is the path of names to travel, 'opts' is the model Options we\n        start the name resolving from, 'allow_many' is as for setup_joins().\n        If fail_on_missing is set to True, then a name that can't be resolved\n        will generate a FieldError.\n\n        Return a list of PathInfo tuples. In addition return the final field\n        (the last used join field) and target (which is a field guaranteed to\n        contain the same value as the final field). Finally, return those names\n        that weren't found (which are likely transforms and the final lookup).\n        \"\"\"\n        path, names_with_path = [], []\n        for pos, name in enumerate(names):\n            cur_names_with_path = (name, [])\n            if name == 'pk':\n                name = opts.pk.name\n\n            field = None\n            filtered_relation = None\n            try:\n                field = opts.get_field(name)\n            except FieldDoesNotExist:\n                if name in self.annotation_select:\n                    field = self.annotation_select[name].output_field\n                elif name in self._filtered_relations and pos == 0:\n                    filtered_relation = self._filtered_relations[name]\n                    field = opts.get_field(filtered_relation.relation_name)\n            if field is not None:\n                # Fields that contain one-to-many relations with a generic\n                # model (like a GenericForeignKey) cannot generate reverse\n                # relations and therefore cannot be used for reverse querying.\n                if field.is_relation and not field.related_model:\n                    raise FieldError(\n                        \"Field %r does not generate an automatic reverse \"\n                        \"relation and therefore cannot be used for reverse \"\n                        \"querying. If it is a GenericForeignKey, consider \"\n                        \"adding a GenericRelation.\" % name\n                    )\n                try:\n                    model = field.model._meta.concrete_model\n                except AttributeError:\n                    # QuerySet.annotate() may introduce fields that aren't\n                    # attached to a model.\n                    model = None\n            else:\n                # We didn't find the current field, so move position back\n                # one step.\n                pos -= 1\n                if pos == -1 or fail_on_missing:\n                    available = sorted([\n                        *get_field_names_from_opts(opts),\n                        *self.annotation_select,\n                        *self._filtered_relations,\n                    ])\n                    raise FieldError(\"Cannot resolve keyword '%s' into field. \"\n                                     \"Choices are: %s\" % (name, \", \".join(available)))\n                break\n            # Check if we need any joins for concrete inheritance cases (the\n            # field lives in parent, but we are currently in one of its\n            # children)\n            if model is not opts.model:\n                path_to_parent = opts.get_path_to_parent(model)\n                if path_to_parent:\n                    path.extend(path_to_parent)\n                    cur_names_with_path[1].extend(path_to_parent)\n                    opts = path_to_parent[-1].to_opts\n            if hasattr(field, 'get_path_info'):\n                pathinfos = field.get_path_info(filtered_relation)\n                if not allow_many:\n                    for inner_pos, p in enumerate(pathinfos):\n                        if p.m2m:\n                            cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])\n                            names_with_path.append(cur_names_with_path)\n                            raise MultiJoin(pos + 1, names_with_path)\n                last = pathinfos[-1]\n                path.extend(pathinfos)\n                final_field = last.join_field\n                opts = last.to_opts\n                targets = last.target_fields\n                cur_names_with_path[1].extend(pathinfos)\n                names_with_path.append(cur_names_with_path)\n            else:\n                # Local non-relational field.\n                final_field = field\n                targets = (field,)\n                if fail_on_missing and pos + 1 != len(names):\n                    raise FieldError(\n                        \"Cannot resolve keyword %r into field. Join on '%s'\"\n                        \" not permitted.\" % (names[pos + 1], name))\n                break\n        return path, final_field, targets, names[pos + 1:]\n\n    def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True,\n                    reuse_with_filtered_relation=False):\n        \"\"\"\n        Compute the necessary table joins for the passage through the fields\n        given in 'names'. 'opts' is the Options class for the current model\n        (which gives the table we are starting from), 'alias' is the alias for\n        the table to start the joining from.\n\n        The 'can_reuse' defines the reverse foreign key joins we can reuse. It\n        can be None in which case all joins are reusable or a set of aliases\n        that can be reused. Note that non-reverse foreign keys are always\n        reusable when using setup_joins().\n\n        The 'reuse_with_filtered_relation' can be used to force 'can_reuse'\n        parameter and force the relation on the given connections.\n\n        If 'allow_many' is False, then any reverse foreign key seen will\n        generate a MultiJoin exception.\n\n        Return the final field involved in the joins, the target field (used\n        for any 'where' constraint), the final 'opts' value, the joins, the\n        field path traveled to generate the joins, and a transform function\n        that takes a field and alias and is equivalent to `field.get_col(alias)`\n        in the simple case but wraps field transforms if they were included in\n        names.\n\n        The target field is the field containing the concrete value. Final\n        field can be something different, for example foreign key pointing to\n        that value. Final field is needed for example in some value\n        conversions (convert 'obj' in fk__id=obj to pk val using the foreign\n        key field for example).\n        \"\"\"\n        joins = [alias]\n        # The transform can't be applied yet, as joins must be trimmed later.\n        # To avoid making every caller of this method look up transforms\n        # directly, compute transforms here and create a partial that converts\n        # fields to the appropriate wrapped version.\n\n        def final_transformer(field, alias):\n            return field.get_col(alias)\n\n        # Try resolving all the names as fields first. If there's an error,\n        # treat trailing names as lookups until a field can be resolved.\n        last_field_exception = None\n        for pivot in range(len(names), 0, -1):\n            try:\n                path, final_field, targets, rest = self.names_to_path(\n                    names[:pivot], opts, allow_many, fail_on_missing=True,\n                )\n            except FieldError as exc:\n                if pivot == 1:\n                    # The first item cannot be a lookup, so it's safe\n                    # to raise the field error here.\n                    raise\n                else:\n                    last_field_exception = exc\n            else:\n                # The transforms are the remaining items that couldn't be\n                # resolved into fields.\n                transforms = names[pivot:]\n                break\n        for name in transforms:\n            def transform(field, alias, *, name, previous):\n                try:\n                    wrapped = previous(field, alias)\n                    return self.try_transform(wrapped, name)\n                except FieldError:\n                    # FieldError is raised if the transform doesn't exist.\n                    if isinstance(final_field, Field) and last_field_exception:\n                        raise last_field_exception\n                    else:\n                        raise\n            final_transformer = functools.partial(transform, name=name, previous=final_transformer)\n        # Then, add the path to the query's joins. Note that we can't trim\n        # joins at this stage - we will need the information about join type\n        # of the trimmed joins.\n        for join in path:\n            if join.filtered_relation:\n                filtered_relation = join.filtered_relation.clone()\n                table_alias = filtered_relation.alias\n            else:\n                filtered_relation = None\n                table_alias = None\n            opts = join.to_opts\n            if join.direct:\n                nullable = self.is_nullable(join.join_field)\n            else:\n                nullable = True\n            connection = Join(\n                opts.db_table, alias, table_alias, INNER, join.join_field,\n                nullable, filtered_relation=filtered_relation,\n            )\n            reuse = can_reuse if join.m2m or reuse_with_filtered_relation else None\n            alias = self.join(\n                connection, reuse=reuse,\n                reuse_with_filtered_relation=reuse_with_filtered_relation,\n            )\n            joins.append(alias)\n            if filtered_relation:\n                filtered_relation.path = joins[:]\n        return JoinInfo(final_field, targets, opts, joins, path, final_transformer)\n\n    def trim_joins(self, targets, joins, path):\n        \"\"\"\n        The 'target' parameter is the final field being joined to, 'joins'\n        is the full list of join aliases. The 'path' contain the PathInfos\n        used to create the joins.\n\n        Return the final target field and table alias and the new active\n        joins.\n\n        Always trim any direct join if the target column is already in the\n        previous table. Can't trim reverse joins as it's unknown if there's\n        anything on the other side of the join.\n        \"\"\"\n        joins = joins[:]\n        for pos, info in enumerate(reversed(path)):\n            if len(joins) == 1 or not info.direct:\n                break\n            if info.filtered_relation:\n                break\n            join_targets = {t.column for t in info.join_field.foreign_related_fields}\n            cur_targets = {t.column for t in targets}\n            if not cur_targets.issubset(join_targets):\n                break\n            targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}\n            targets = tuple(targets_dict[t.column] for t in targets)\n            self.unref_alias(joins.pop())\n        return targets, joins[-1], joins\n\n    @classmethod\n    def _gen_cols(cls, exprs):\n        for expr in exprs:\n            if isinstance(expr, Col):\n                yield expr\n            else:\n                yield from cls._gen_cols(expr.get_source_expressions())\n\n    @classmethod\n    def _gen_col_aliases(cls, exprs):\n        yield from (expr.alias for expr in cls._gen_cols(exprs))\n\n    def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False):\n        if not allow_joins and LOOKUP_SEP in name:\n            raise FieldError(\"Joined field references are not permitted in this query\")\n        annotation = self.annotations.get(name)\n        if annotation is not None:\n            if not allow_joins:\n                for alias in self._gen_col_aliases([annotation]):\n                    if isinstance(self.alias_map[alias], Join):\n                        raise FieldError(\n                            'Joined field references are not permitted in '\n                            'this query'\n                        )\n            if summarize:\n                # Summarize currently means we are doing an aggregate() query\n                # which is executed as a wrapped subquery if any of the\n                # aggregate() elements reference an existing annotation. In\n                # that case we need to return a Ref to the subquery's annotation.\n                return Ref(name, self.annotation_select[name])\n            else:\n                return annotation\n        else:\n            field_list = name.split(LOOKUP_SEP)\n            join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse)\n            targets, final_alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)\n            if not allow_joins and len(join_list) > 1:\n                raise FieldError('Joined field references are not permitted in this query')\n            if len(targets) > 1:\n                raise FieldError(\"Referencing multicolumn fields with F() objects \"\n                                 \"isn't supported\")\n            # Verify that the last lookup in name is a field or a transform:\n            # transform_function() raises FieldError if not.\n            join_info.transform_function(targets[0], final_alias)\n            if reuse is not None:\n                reuse.update(join_list)\n            return self._get_col(targets[0], join_info.targets[0], join_list[-1])\n\n    def split_exclude(self, filter_expr, can_reuse, names_with_path):\n        \"\"\"\n        When doing an exclude against any kind of N-to-many relation, we need\n        to use a subquery. This method constructs the nested query, given the\n        original exclude filter (filter_expr) and the portion up to the first\n        N-to-many relation field.\n\n        For example, if the origin filter is ~Q(child__name='foo'), filter_expr\n        is ('child__name', 'foo') and can_reuse is a set of joins usable for\n        filters in the original query.\n\n        We will turn this into equivalent of:\n            WHERE NOT (pk IN (SELECT parent_id FROM thetable\n                              WHERE name = 'foo' AND parent_id IS NOT NULL))\n\n        It might be worth it to consider using WHERE NOT EXISTS as that has\n        saner null handling, and is easier for the backend's optimizer to\n        handle.\n        \"\"\"\n        filter_lhs, filter_rhs = filter_expr\n        if isinstance(filter_rhs, OuterRef):\n            filter_expr = (filter_lhs, OuterRef(filter_rhs))\n        elif isinstance(filter_rhs, F):\n            filter_expr = (filter_lhs, OuterRef(filter_rhs.name))\n        # Generate the inner query.\n        query = Query(self.model)\n        query._filtered_relations = self._filtered_relations\n        query.add_filter(filter_expr)\n        query.clear_ordering(True)\n        # Try to have as simple as possible subquery -> trim leading joins from\n        # the subquery.\n        trimmed_prefix, contains_louter = query.trim_start(names_with_path)\n\n        # Add extra check to make sure the selected field will not be null\n        # since we are adding an IN <subquery> clause. This prevents the\n        # database from tripping over IN (...,NULL,...) selects and returning\n        # nothing\n        col = query.select[0]\n        select_field = col.target\n        alias = col.alias\n        if self.is_nullable(select_field):\n            lookup_class = select_field.get_lookup('isnull')\n            lookup = lookup_class(select_field.get_col(alias), False)\n            query.where.add(lookup, AND)\n        if alias in can_reuse:\n            pk = select_field.model._meta.pk\n            # Need to add a restriction so that outer query's filters are in effect for\n            # the subquery, too.\n            query.bump_prefix(self)\n            lookup_class = select_field.get_lookup('exact')\n            # Note that the query.select[0].alias is different from alias\n            # due to bump_prefix above.\n            lookup = lookup_class(pk.get_col(query.select[0].alias),\n                                  pk.get_col(alias))\n            query.where.add(lookup, AND)\n            query.external_aliases[alias] = True\n\n        condition, needed_inner = self.build_filter(\n            ('%s__in' % trimmed_prefix, query),\n            current_negated=True, branch_negated=True, can_reuse=can_reuse)\n        if contains_louter:\n            or_null_condition, _ = self.build_filter(\n                ('%s__isnull' % trimmed_prefix, True),\n                current_negated=True, branch_negated=True, can_reuse=can_reuse)\n            condition.add(or_null_condition, OR)\n            # Note that the end result will be:\n            # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.\n            # This might look crazy but due to how IN works, this seems to be\n            # correct. If the IS NOT NULL check is removed then outercol NOT\n            # IN will return UNKNOWN. If the IS NULL check is removed, then if\n            # outercol IS NULL we will not match the row.\n        return condition, needed_inner\n\n    def set_empty(self):\n        self.where.add(NothingNode(), AND)\n\n    def is_empty(self):\n        return any(isinstance(c, NothingNode) for c in self.where.children)\n\n    def set_limits(self, low=None, high=None):\n        \"\"\"", "answer": "C"}
{"uuid": "0b3b8d0d-9278-4e36-930d-c728e349ed1c", "issue_statement": "Add support for item_comments to syndication framework\nDescription\n\t\nAdd comments argument to feed.add_item() in syndication.views so that item_comments can be defined directly without having to take the detour via item_extra_kwargs .\nAdditionally, comments is already explicitly mentioned in the feedparser, but not implemented in the view.", "choices": "(A)                 author_name=author_name,\n                author_email=author_email,\n                author_link=author_link,\n                categories=self._get_dynamic_attr('item_categories', item),\n                item_copyright=self._get_dynamic_attr('item_copyright', item),\n                **self.item_extra_kwargs(item)\n            )\n(B)                     'item_guid_is_permalink', item),\n                enclosures=enclosures,\n                pubdate=pubdate,\n                updateddate=updateddate,\n                author_name=author_name,\n                author_email=author_email,\n                author_link=author_link,\n(C)                 unique_id=self._get_dynamic_attr('item_guid', item, link),\n                unique_id_is_permalink=self._get_dynamic_attr(\n                    'item_guid_is_permalink', item),\n                enclosures=enclosures,\n                pubdate=pubdate,\n                updateddate=updateddate,\n                author_name=author_name,\n(D)                 pubdate=pubdate,\n                updateddate=updateddate,\n                author_name=author_name,\n                author_email=author_email,\n                author_link=author_link,\n                categories=self._get_dynamic_attr('item_categories', item),\n                item_copyright=self._get_dynamic_attr('item_copyright', item),", "answer": "A"}
{"uuid": "063add4a-5db8-47cb-8042-ff200ed851d6", "issue_statement": "AlterOrderWithRespectTo() with ForeignKey crash when _order is included in Index().\nDescription\n\t\n\tclass Meta:\n\t\tdb_table = 'look_image'\n\t\torder_with_respect_to = 'look'\n\t\tindexes = [\n\t\t\tmodels.Index(fields=['look', '_order']),\n\t\t\tmodels.Index(fields=['created_at']),\n\t\t\tmodels.Index(fields=['updated_at']),\n\t\t]\nmigrations.CreateModel(\n\t\t\tname='LookImage',\n\t\t\tfields=[\n\t\t\t\t('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n\t\t\t\t('look', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='images', to='posts.Look', verbose_name='LOOK')),\n\t\t\t\t('image_url', models.URLField(blank=True, max_length=10000, null=True)),\n\t\t\t\t('image', models.ImageField(max_length=2000, upload_to='')),\n\t\t\t\t('deleted', models.DateTimeField(editable=False, null=True)),\n\t\t\t\t('created_at', models.DateTimeField(auto_now_add=True)),\n\t\t\t\t('updated_at', models.DateTimeField(auto_now=True)),\n\t\t\t],\n\t\t),\n\t\tmigrations.AddIndex(\n\t\t\tmodel_name='lookimage',\n\t\t\tindex=models.Index(fields=['look', '_order'], name='look_image_look_id_eaff30_idx'),\n\t\t),\n\t\tmigrations.AddIndex(\n\t\t\tmodel_name='lookimage',\n\t\t\tindex=models.Index(fields=['created_at'], name='look_image_created_f746cf_idx'),\n\t\t),\n\t\tmigrations.AddIndex(\n\t\t\tmodel_name='lookimage',\n\t\t\tindex=models.Index(fields=['updated_at'], name='look_image_updated_aceaf9_idx'),\n\t\t),\n\t\tmigrations.AlterOrderWithRespectTo(\n\t\t\tname='lookimage',\n\t\t\torder_with_respect_to='look',\n\t\t),\nI added orders_with_respect_to in new model class's Meta class and also made index for '_order' field by combining with other field. And a new migration file based on the model looks like the code above.\nThe problem is operation AlterOrderWithRespectTo after AddIndex of '_order' raising error because '_order' field had not been created yet.\nIt seems to be AlterOrderWithRespectTo has to proceed before AddIndex of '_order'.", "choices": "(A)         self.generate_removed_fields()\n        self.generate_added_fields()\n        self.generate_altered_fields()\n        self.generate_altered_unique_together()\n        self.generate_altered_index_together()\n        self.generate_added_indexes()\n        self.generate_added_constraints()\n        self.generate_altered_db_table()\n        self.generate_altered_order_with_respect_to()\n\n        self._sort_migrations()\n        self._build_migration_list(graph)\n        self._optimize_migrations()\n\n        return self.migrations\n\n    def _prepare_field_lists(self):\n        \"\"\"\n        Prepare field lists and a list of the fields that used through models\n        in the old state so dependencies can be made from the through model\n        deletion to the field that uses it.\n        \"\"\"\n        self.kept_model_keys = self.old_model_keys & self.new_model_keys\n        self.kept_proxy_keys = self.old_proxy_keys & self.new_proxy_keys\n        self.kept_unmanaged_keys = self.old_unmanaged_keys & self.new_unmanaged_keys\n        self.through_users = {}\n        self.old_field_keys = {\n            (app_label, model_name, field_name)\n            for app_label, model_name in self.kept_model_keys\n            for field_name in self.from_state.models[\n                app_label,\n                self.renamed_models.get((app_label, model_name), model_name)\n            ].fields\n        }\n        self.new_field_keys = {\n            (app_label, model_name, field_name)\n            for app_label, model_name in self.kept_model_keys\n            for field_name in self.to_state.models[app_label, model_name].fields\n        }\n\n    def _generate_through_model_map(self):\n        \"\"\"Through model map generation.\"\"\"\n        for app_label, model_name in sorted(self.old_model_keys):\n            old_model_name = self.renamed_models.get((app_label, model_name), model_name)\n            old_model_state = self.from_state.models[app_label, old_model_name]\n            for field_name in old_model_state.fields:\n                old_field = self.old_apps.get_model(app_label, old_model_name)._meta.get_field(field_name)\n                if (hasattr(old_field, \"remote_field\") and getattr(old_field.remote_field, \"through\", None) and\n                        not old_field.remote_field.through._meta.auto_created):\n                    through_key = (\n                        old_field.remote_field.through._meta.app_label,\n                        old_field.remote_field.through._meta.model_name,\n                    )\n                    self.through_users[through_key] = (app_label, old_model_name, field_name)\n\n    @staticmethod\n    def _resolve_dependency(dependency):\n        \"\"\"\n        Return the resolved dependency and a boolean denoting whether or not\n        it was swappable.\n        \"\"\"\n        if dependency[0] != '__setting__':\n            return dependency, False\n        resolved_app_label, resolved_object_name = getattr(settings, dependency[1]).split('.')\n        return (resolved_app_label, resolved_object_name.lower()) + dependency[2:], True\n\n    def _build_migration_list(self, graph=None):\n        \"\"\"\n        Chop the lists of operations up into migrations with dependencies on\n        each other. Do this by going through an app's list of operations until\n        one is found that has an outgoing dependency that isn't in another\n        app's migration yet (hasn't been chopped off its list). Then chop off\n        the operations before it into a migration and move onto the next app.\n        If the loops completes without doing anything, there's a circular\n        dependency (which _should_ be impossible as the operations are\n        all split at this point so they can't depend and be depended on).\n        \"\"\"\n        self.migrations = {}\n        num_ops = sum(len(x) for x in self.generated_operations.values())\n        chop_mode = False\n        while num_ops:\n            # On every iteration, we step through all the apps and see if there\n            # is a completed set of operations.\n            # If we find that a subset of the operations are complete we can\n            # try to chop it off from the rest and continue, but we only\n            # do this if we've already been through the list once before\n            # without any chopping and nothing has changed.\n            for app_label in sorted(self.generated_operations):\n                chopped = []\n                dependencies = set()\n                for operation in list(self.generated_operations[app_label]):\n                    deps_satisfied = True\n                    operation_dependencies = set()\n                    for dep in operation._auto_deps:\n                        # Temporarily resolve the swappable dependency to\n                        # prevent circular references. While keeping the\n                        # dependency checks on the resolved model, add the\n                        # swappable dependencies.\n                        original_dep = dep\n                        dep, is_swappable_dep = self._resolve_dependency(dep)\n                        if dep[0] != app_label:\n                            # External app dependency. See if it's not yet\n                            # satisfied.\n                            for other_operation in self.generated_operations.get(dep[0], []):\n                                if self.check_dependency(other_operation, dep):\n                                    deps_satisfied = False\n                                    break\n                            if not deps_satisfied:\n                                break\n                            else:\n                                if is_swappable_dep:\n                                    operation_dependencies.add((original_dep[0], original_dep[1]))\n                                elif dep[0] in self.migrations:\n                                    operation_dependencies.add((dep[0], self.migrations[dep[0]][-1].name))\n                                else:\n                                    # If we can't find the other app, we add a first/last dependency,\n                                    # but only if we've already been through once and checked everything\n                                    if chop_mode:\n                                        # If the app already exists, we add a dependency on the last migration,\n                                        # as we don't know which migration contains the target field.\n                                        # If it's not yet migrated or has no migrations, we use __first__\n                                        if graph and graph.leaf_nodes(dep[0]):\n                                            operation_dependencies.add(graph.leaf_nodes(dep[0])[0])\n                                        else:\n                                            operation_dependencies.add((dep[0], \"__first__\"))\n                                    else:\n                                        deps_satisfied = False\n                    if deps_satisfied:\n                        chopped.append(operation)\n                        dependencies.update(operation_dependencies)\n                        del self.generated_operations[app_label][0]\n                    else:\n                        break\n                # Make a migration! Well, only if there's stuff to put in it\n                if dependencies or chopped:\n                    if not self.generated_operations[app_label] or chop_mode:\n                        subclass = type(\"Migration\", (Migration,), {\"operations\": [], \"dependencies\": []})\n                        instance = subclass(\"auto_%i\" % (len(self.migrations.get(app_label, [])) + 1), app_label)\n                        instance.dependencies = list(dependencies)\n                        instance.operations = chopped\n                        instance.initial = app_label not in self.existing_apps\n                        self.migrations.setdefault(app_label, []).append(instance)\n                        chop_mode = False\n                    else:\n                        self.generated_operations[app_label] = chopped + self.generated_operations[app_label]\n            new_num_ops = sum(len(x) for x in self.generated_operations.values())\n            if new_num_ops == num_ops:\n                if not chop_mode:\n                    chop_mode = True\n                else:\n                    raise ValueError(\"Cannot resolve operation dependencies: %r\" % self.generated_operations)\n            num_ops = new_num_ops\n\n    def _sort_migrations(self):\n        \"\"\"\n        Reorder to make things possible. Reordering may be needed so FKs work\n        nicely inside the same app.\n        \"\"\"\n        for app_label, ops in sorted(self.generated_operations.items()):\n            # construct a dependency graph for intra-app dependencies\n            dependency_graph = {op: set() for op in ops}\n            for op in ops:\n                for dep in op._auto_deps:\n                    # Resolve intra-app dependencies to handle circular\n                    # references involving a swappable model.\n                    dep = self._resolve_dependency(dep)[0]\n                    if dep[0] == app_label:\n                        for op2 in ops:\n                            if self.check_dependency(op2, dep):\n                                dependency_graph[op].add(op2)\n\n            # we use a stable sort for deterministic tests & general behavior\n            self.generated_operations[app_label] = stable_topological_sort(ops, dependency_graph)\n\n    def _optimize_migrations(self):\n        # Add in internal dependencies among the migrations\n        for app_label, migrations in self.migrations.items():\n            for m1, m2 in zip(migrations, migrations[1:]):\n                m2.dependencies.append((app_label, m1.name))\n\n        # De-dupe dependencies\n        for migrations in self.migrations.values():\n            for migration in migrations:\n                migration.dependencies = list(set(migration.dependencies))\n\n        # Optimize migrations\n        for app_label, migrations in self.migrations.items():\n            for migration in migrations:\n                migration.operations = MigrationOptimizer().optimize(migration.operations, app_label)\n\n    def check_dependency(self, operation, dependency):\n        \"\"\"\n        Return True if the given operation depends on the given dependency,\n        False otherwise.\n        \"\"\"\n        # Created model\n        if dependency[2] is None and dependency[3] is True:\n            return (\n                isinstance(operation, operations.CreateModel) and\n                operation.name_lower == dependency[1].lower()\n            )\n        # Created field\n        elif dependency[2] is not None and dependency[3] is True:\n            return (\n                (\n                    isinstance(operation, operations.CreateModel) and\n                    operation.name_lower == dependency[1].lower() and\n                    any(dependency[2] == x for x, y in operation.fields)\n                ) or\n                (\n                    isinstance(operation, operations.AddField) and\n                    operation.model_name_lower == dependency[1].lower() and\n                    operation.name_lower == dependency[2].lower()\n                )\n            )\n        # Removed field\n        elif dependency[2] is not None and dependency[3] is False:\n            return (\n                isinstance(operation, operations.RemoveField) and\n                operation.model_name_lower == dependency[1].lower() and\n                operation.name_lower == dependency[2].lower()\n            )\n        # Removed model\n        elif dependency[2] is None and dependency[3] is False:\n            return (\n                isinstance(operation, operations.DeleteModel) and\n                operation.name_lower == dependency[1].lower()\n            )\n        # Field being altered\n        elif dependency[2] is not None and dependency[3] == \"alter\":\n            return (\n                isinstance(operation, operations.AlterField) and\n                operation.model_name_lower == dependency[1].lower() and\n                operation.name_lower == dependency[2].lower()\n            )\n        # order_with_respect_to being unset for a field\n        elif dependency[2] is not None and dependency[3] == \"order_wrt_unset\":\n            return (\n                isinstance(operation, operations.AlterOrderWithRespectTo) and\n                operation.name_lower == dependency[1].lower() and\n                (operation.order_with_respect_to or \"\").lower() != dependency[2].lower()\n            )\n        # Field is removed and part of an index/unique_together\n        elif dependency[2] is not None and dependency[3] == \"foo_together_change\":\n            return (\n                isinstance(operation, (operations.AlterUniqueTogether,\n                                       operations.AlterIndexTogether)) and\n                operation.name_lower == dependency[1].lower()\n            )\n        # Unknown dependency. Raise an error.\n        else:\n            raise ValueError(\"Can't handle dependency %r\" % (dependency,))\n\n    def add_operation(self, app_label, operation, dependencies=None, beginning=False):\n        # Dependencies are (app_label, model_name, field_name, create/delete as True/False)\n        operation._auto_deps = dependencies or []\n        if beginning:\n            self.generated_operations.setdefault(app_label, []).insert(0, operation)\n        else:\n            self.generated_operations.setdefault(app_label, []).append(operation)\n\n    def swappable_first_key(self, item):\n        \"\"\"\n        Place potential swappable models first in lists of created models (only\n        real way to solve #22783).\n        \"\"\"\n        try:\n            model = self.new_apps.get_model(item[0], item[1])\n            base_names = [base.__name__ for base in model.__bases__]\n            string_version = \"%s.%s\" % (item[0], item[1])\n            if (\n                model._meta.swappable or\n                \"AbstractUser\" in base_names or\n                \"AbstractBaseUser\" in base_names or\n                settings.AUTH_USER_MODEL.lower() == string_version.lower()\n            ):\n                return (\"___\" + item[0], \"___\" + item[1])\n        except LookupError:\n            pass\n        return item\n\n    def generate_renamed_models(self):\n        \"\"\"\n        Find any renamed models, generate the operations for them, and remove\n        the old entry from the model lists. Must be run before other\n        model-level generation.\n        \"\"\"\n        self.renamed_models = {}\n        self.renamed_models_rel = {}\n        added_models = self.new_model_keys - self.old_model_keys\n        for app_label, model_name in sorted(added_models):\n            model_state = self.to_state.models[app_label, model_name]\n            model_fields_def = self.only_relation_agnostic_fields(model_state.fields)\n\n            removed_models = self.old_model_keys - self.new_model_keys\n            for rem_app_label, rem_model_name in removed_models:\n                if rem_app_label == app_label:\n                    rem_model_state = self.from_state.models[rem_app_label, rem_model_name]\n                    rem_model_fields_def = self.only_relation_agnostic_fields(rem_model_state.fields)\n                    if model_fields_def == rem_model_fields_def:\n                        if self.questioner.ask_rename_model(rem_model_state, model_state):\n                            model_opts = self.new_apps.get_model(app_label, model_name)._meta\n                            dependencies = []\n                            for field in model_opts.get_fields():\n                                if field.is_relation:\n                                    dependencies.extend(self._get_dependencies_for_foreign_key(field))\n                            self.add_operation(\n                                app_label,\n                                operations.RenameModel(\n                                    old_name=rem_model_state.name,\n                                    new_name=model_state.name,\n                                ),\n                                dependencies=dependencies,\n                            )\n                            self.renamed_models[app_label, model_name] = rem_model_name\n                            renamed_models_rel_key = '%s.%s' % (\n                                rem_model_state.app_label,\n                                rem_model_state.name_lower,\n                            )\n                            self.renamed_models_rel[renamed_models_rel_key] = '%s.%s' % (\n                                model_state.app_label,\n                                model_state.name_lower,\n                            )\n                            self.old_model_keys.remove((rem_app_label, rem_model_name))\n                            self.old_model_keys.add((app_label, model_name))\n                            break\n\n    def generate_created_models(self):\n        \"\"\"\n        Find all new models (both managed and unmanaged) and make create\n        operations for them as well as separate operations to create any\n        foreign key or M2M relationships (these are optimized later, if\n        possible).\n\n        Defer any model options that refer to collections of fields that might\n        be deferred (e.g. unique_together, index_together).\n        \"\"\"\n        old_keys = self.old_model_keys | self.old_unmanaged_keys\n        added_models = self.new_model_keys - old_keys\n        added_unmanaged_models = self.new_unmanaged_keys - old_keys\n        all_added_models = chain(\n            sorted(added_models, key=self.swappable_first_key, reverse=True),\n            sorted(added_unmanaged_models, key=self.swappable_first_key, reverse=True)\n        )\n        for app_label, model_name in all_added_models:\n            model_state = self.to_state.models[app_label, model_name]\n            model_opts = self.new_apps.get_model(app_label, model_name)._meta\n            # Gather related fields\n            related_fields = {}\n            primary_key_rel = None\n            for field in model_opts.local_fields:\n                if field.remote_field:\n                    if field.remote_field.model:\n                        if field.primary_key:\n                            primary_key_rel = field.remote_field.model\n                        elif not field.remote_field.parent_link:\n                            related_fields[field.name] = field\n                    # through will be none on M2Ms on swapped-out models;\n                    # we can treat lack of through as auto_created=True, though.\n                    if (getattr(field.remote_field, \"through\", None) and\n                            not field.remote_field.through._meta.auto_created):\n                        related_fields[field.name] = field\n            for field in model_opts.local_many_to_many:\n                if field.remote_field.model:\n                    related_fields[field.name] = field\n                if getattr(field.remote_field, \"through\", None) and not field.remote_field.through._meta.auto_created:\n                    related_fields[field.name] = field\n            # Are there indexes/unique|index_together to defer?\n            indexes = model_state.options.pop('indexes')\n            constraints = model_state.options.pop('constraints')\n            unique_together = model_state.options.pop('unique_together', None)\n            index_together = model_state.options.pop('index_together', None)\n            order_with_respect_to = model_state.options.pop('order_with_respect_to', None)\n            # Depend on the deletion of any possible proxy version of us\n            dependencies = [\n                (app_label, model_name, None, False),\n            ]\n            # Depend on all bases\n            for base in model_state.bases:\n                if isinstance(base, str) and \".\" in base:\n                    base_app_label, base_name = base.split(\".\", 1)\n                    dependencies.append((base_app_label, base_name, None, True))\n                    # Depend on the removal of base fields if the new model has\n                    # a field with the same name.\n                    old_base_model_state = self.from_state.models.get((base_app_label, base_name))\n                    new_base_model_state = self.to_state.models.get((base_app_label, base_name))\n                    if old_base_model_state and new_base_model_state:\n                        removed_base_fields = set(old_base_model_state.fields).difference(\n                            new_base_model_state.fields,\n                        ).intersection(model_state.fields)\n                        for removed_base_field in removed_base_fields:\n                            dependencies.append((base_app_label, base_name, removed_base_field, False))\n            # Depend on the other end of the primary key if it's a relation\n            if primary_key_rel:\n                dependencies.append((\n                    primary_key_rel._meta.app_label,\n                    primary_key_rel._meta.object_name,\n                    None,\n                    True\n                ))\n            # Generate creation operation\n            self.add_operation(\n                app_label,\n                operations.CreateModel(\n                    name=model_state.name,\n                    fields=[d for d in model_state.fields.items() if d[0] not in related_fields],\n                    options=model_state.options,\n                    bases=model_state.bases,\n                    managers=model_state.managers,\n                ),\n                dependencies=dependencies,\n                beginning=True,\n            )\n\n            # Don't add operations which modify the database for unmanaged models\n            if not model_opts.managed:\n                continue\n\n            # Generate operations for each related field\n            for name, field in sorted(related_fields.items()):\n                dependencies = self._get_dependencies_for_foreign_key(field)\n                # Depend on our own model being created\n                dependencies.append((app_label, model_name, None, True))\n                # Make operation\n                self.add_operation(\n                    app_label,\n                    operations.AddField(\n                        model_name=model_name,\n                        name=name,\n                        field=field,\n                    ),\n                    dependencies=list(set(dependencies)),\n                )\n            # Generate other opns\n            related_dependencies = [\n                (app_label, model_name, name, True)\n                for name in sorted(related_fields)\n            ]\n            related_dependencies.append((app_label, model_name, None, True))\n            for index in indexes:\n                self.add_operation(\n                    app_label,\n                    operations.AddIndex(\n                        model_name=model_name,\n                        index=index,\n                    ),\n                    dependencies=related_dependencies,\n                )\n            for constraint in constraints:\n                self.add_operation(\n                    app_label,\n                    operations.AddConstraint(\n                        model_name=model_name,\n                        constraint=constraint,\n                    ),\n                    dependencies=related_dependencies,\n                )\n            if unique_together:\n                self.add_operation(\n                    app_label,\n                    operations.AlterUniqueTogether(\n                        name=model_name,\n                        unique_together=unique_together,\n                    ),\n                    dependencies=related_dependencies\n                )\n            if index_together:\n                self.add_operation(\n                    app_label,\n                    operations.AlterIndexTogether(\n                        name=model_name,\n                        index_together=index_together,\n                    ),\n                    dependencies=related_dependencies\n                )\n            if order_with_respect_to:\n                self.add_operation(\n                    app_label,\n                    operations.AlterOrderWithRespectTo(\n                        name=model_name,\n                        order_with_respect_to=order_with_respect_to,\n                    ),\n                    dependencies=[\n                        (app_label, model_name, order_with_respect_to, True),\n                        (app_label, model_name, None, True),\n                    ]\n                )\n\n            # Fix relationships if the model changed from a proxy model to a\n            # concrete model.\n(B)                 isinstance(operation, (operations.AlterUniqueTogether,\n                                       operations.AlterIndexTogether)) and\n                operation.name_lower == dependency[1].lower()\n            )\n        # Unknown dependency. Raise an error.\n        else:\n            raise ValueError(\"Can't handle dependency %r\" % (dependency,))\n\n    def add_operation(self, app_label, operation, dependencies=None, beginning=False):\n        # Dependencies are (app_label, model_name, field_name, create/delete as True/False)\n        operation._auto_deps = dependencies or []\n        if beginning:\n            self.generated_operations.setdefault(app_label, []).insert(0, operation)\n        else:\n            self.generated_operations.setdefault(app_label, []).append(operation)\n\n    def swappable_first_key(self, item):\n        \"\"\"\n        Place potential swappable models first in lists of created models (only\n        real way to solve #22783).\n        \"\"\"\n        try:\n            model = self.new_apps.get_model(item[0], item[1])\n            base_names = [base.__name__ for base in model.__bases__]\n            string_version = \"%s.%s\" % (item[0], item[1])\n            if (\n                model._meta.swappable or\n                \"AbstractUser\" in base_names or\n                \"AbstractBaseUser\" in base_names or\n                settings.AUTH_USER_MODEL.lower() == string_version.lower()\n            ):\n                return (\"___\" + item[0], \"___\" + item[1])\n        except LookupError:\n            pass\n        return item\n\n    def generate_renamed_models(self):\n        \"\"\"\n        Find any renamed models, generate the operations for them, and remove\n        the old entry from the model lists. Must be run before other\n        model-level generation.\n        \"\"\"\n        self.renamed_models = {}\n        self.renamed_models_rel = {}\n        added_models = self.new_model_keys - self.old_model_keys\n        for app_label, model_name in sorted(added_models):\n            model_state = self.to_state.models[app_label, model_name]\n            model_fields_def = self.only_relation_agnostic_fields(model_state.fields)\n\n            removed_models = self.old_model_keys - self.new_model_keys\n            for rem_app_label, rem_model_name in removed_models:\n                if rem_app_label == app_label:\n                    rem_model_state = self.from_state.models[rem_app_label, rem_model_name]\n                    rem_model_fields_def = self.only_relation_agnostic_fields(rem_model_state.fields)\n                    if model_fields_def == rem_model_fields_def:\n                        if self.questioner.ask_rename_model(rem_model_state, model_state):\n                            model_opts = self.new_apps.get_model(app_label, model_name)._meta\n                            dependencies = []\n                            for field in model_opts.get_fields():\n                                if field.is_relation:\n                                    dependencies.extend(self._get_dependencies_for_foreign_key(field))\n                            self.add_operation(\n                                app_label,\n                                operations.RenameModel(\n                                    old_name=rem_model_state.name,\n                                    new_name=model_state.name,\n                                ),\n                                dependencies=dependencies,\n                            )\n                            self.renamed_models[app_label, model_name] = rem_model_name\n                            renamed_models_rel_key = '%s.%s' % (\n                                rem_model_state.app_label,\n                                rem_model_state.name_lower,\n                            )\n                            self.renamed_models_rel[renamed_models_rel_key] = '%s.%s' % (\n                                model_state.app_label,\n                                model_state.name_lower,\n                            )\n                            self.old_model_keys.remove((rem_app_label, rem_model_name))\n                            self.old_model_keys.add((app_label, model_name))\n                            break\n\n    def generate_created_models(self):\n        \"\"\"\n        Find all new models (both managed and unmanaged) and make create\n        operations for them as well as separate operations to create any\n        foreign key or M2M relationships (these are optimized later, if\n        possible).\n\n        Defer any model options that refer to collections of fields that might\n        be deferred (e.g. unique_together, index_together).\n        \"\"\"\n        old_keys = self.old_model_keys | self.old_unmanaged_keys\n        added_models = self.new_model_keys - old_keys\n        added_unmanaged_models = self.new_unmanaged_keys - old_keys\n        all_added_models = chain(\n            sorted(added_models, key=self.swappable_first_key, reverse=True),\n            sorted(added_unmanaged_models, key=self.swappable_first_key, reverse=True)\n        )\n        for app_label, model_name in all_added_models:\n            model_state = self.to_state.models[app_label, model_name]\n            model_opts = self.new_apps.get_model(app_label, model_name)._meta\n            # Gather related fields\n            related_fields = {}\n            primary_key_rel = None\n            for field in model_opts.local_fields:\n                if field.remote_field:\n                    if field.remote_field.model:\n                        if field.primary_key:\n                            primary_key_rel = field.remote_field.model\n                        elif not field.remote_field.parent_link:\n                            related_fields[field.name] = field\n                    # through will be none on M2Ms on swapped-out models;\n                    # we can treat lack of through as auto_created=True, though.\n                    if (getattr(field.remote_field, \"through\", None) and\n                            not field.remote_field.through._meta.auto_created):\n                        related_fields[field.name] = field\n            for field in model_opts.local_many_to_many:\n                if field.remote_field.model:\n                    related_fields[field.name] = field\n                if getattr(field.remote_field, \"through\", None) and not field.remote_field.through._meta.auto_created:\n                    related_fields[field.name] = field\n            # Are there indexes/unique|index_together to defer?\n            indexes = model_state.options.pop('indexes')\n            constraints = model_state.options.pop('constraints')\n            unique_together = model_state.options.pop('unique_together', None)\n            index_together = model_state.options.pop('index_together', None)\n            order_with_respect_to = model_state.options.pop('order_with_respect_to', None)\n            # Depend on the deletion of any possible proxy version of us\n            dependencies = [\n                (app_label, model_name, None, False),\n            ]\n            # Depend on all bases\n            for base in model_state.bases:\n                if isinstance(base, str) and \".\" in base:\n                    base_app_label, base_name = base.split(\".\", 1)\n                    dependencies.append((base_app_label, base_name, None, True))\n                    # Depend on the removal of base fields if the new model has\n                    # a field with the same name.\n                    old_base_model_state = self.from_state.models.get((base_app_label, base_name))\n                    new_base_model_state = self.to_state.models.get((base_app_label, base_name))\n                    if old_base_model_state and new_base_model_state:\n                        removed_base_fields = set(old_base_model_state.fields).difference(\n                            new_base_model_state.fields,\n                        ).intersection(model_state.fields)\n                        for removed_base_field in removed_base_fields:\n                            dependencies.append((base_app_label, base_name, removed_base_field, False))\n            # Depend on the other end of the primary key if it's a relation\n            if primary_key_rel:\n                dependencies.append((\n                    primary_key_rel._meta.app_label,\n                    primary_key_rel._meta.object_name,\n                    None,\n                    True\n                ))\n            # Generate creation operation\n            self.add_operation(\n                app_label,\n                operations.CreateModel(\n                    name=model_state.name,\n                    fields=[d for d in model_state.fields.items() if d[0] not in related_fields],\n                    options=model_state.options,\n                    bases=model_state.bases,\n                    managers=model_state.managers,\n                ),\n                dependencies=dependencies,\n                beginning=True,\n            )\n\n            # Don't add operations which modify the database for unmanaged models\n            if not model_opts.managed:\n                continue\n\n            # Generate operations for each related field\n            for name, field in sorted(related_fields.items()):\n                dependencies = self._get_dependencies_for_foreign_key(field)\n                # Depend on our own model being created\n                dependencies.append((app_label, model_name, None, True))\n                # Make operation\n                self.add_operation(\n                    app_label,\n                    operations.AddField(\n                        model_name=model_name,\n                        name=name,\n                        field=field,\n                    ),\n                    dependencies=list(set(dependencies)),\n                )\n            # Generate other opns\n            related_dependencies = [\n                (app_label, model_name, name, True)\n                for name in sorted(related_fields)\n            ]\n            related_dependencies.append((app_label, model_name, None, True))\n            for index in indexes:\n                self.add_operation(\n                    app_label,\n                    operations.AddIndex(\n                        model_name=model_name,\n                        index=index,\n                    ),\n                    dependencies=related_dependencies,\n                )\n            for constraint in constraints:\n                self.add_operation(\n                    app_label,\n                    operations.AddConstraint(\n                        model_name=model_name,\n                        constraint=constraint,\n                    ),\n                    dependencies=related_dependencies,\n                )\n            if unique_together:\n                self.add_operation(\n                    app_label,\n                    operations.AlterUniqueTogether(\n                        name=model_name,\n                        unique_together=unique_together,\n                    ),\n                    dependencies=related_dependencies\n                )\n            if index_together:\n                self.add_operation(\n                    app_label,\n                    operations.AlterIndexTogether(\n                        name=model_name,\n                        index_together=index_together,\n                    ),\n                    dependencies=related_dependencies\n                )\n            if order_with_respect_to:\n                self.add_operation(\n                    app_label,\n                    operations.AlterOrderWithRespectTo(\n                        name=model_name,\n                        order_with_respect_to=order_with_respect_to,\n                    ),\n                    dependencies=[\n                        (app_label, model_name, order_with_respect_to, True),\n                        (app_label, model_name, None, True),\n                    ]\n                )\n\n            # Fix relationships if the model changed from a proxy model to a\n            # concrete model.\n            if (app_label, model_name) in self.old_proxy_keys:\n                for related_object in model_opts.related_objects:\n                    self.add_operation(\n                        related_object.related_model._meta.app_label,\n                        operations.AlterField(\n                            model_name=related_object.related_model._meta.object_name,\n                            name=related_object.field.name,\n                            field=related_object.field,\n                        ),\n                        dependencies=[(app_label, model_name, None, True)],\n                    )\n\n    def generate_created_proxies(self):\n        \"\"\"\n        Make CreateModel statements for proxy models. Use the same statements\n        as that way there's less code duplication, but for proxy models it's\n        safe to skip all the pointless field stuff and chuck out an operation.\n        \"\"\"\n        added = self.new_proxy_keys - self.old_proxy_keys\n        for app_label, model_name in sorted(added):\n            model_state = self.to_state.models[app_label, model_name]\n            assert model_state.options.get(\"proxy\")\n            # Depend on the deletion of any possible non-proxy version of us\n            dependencies = [\n                (app_label, model_name, None, False),\n            ]\n            # Depend on all bases\n            for base in model_state.bases:\n                if isinstance(base, str) and \".\" in base:\n                    base_app_label, base_name = base.split(\".\", 1)\n                    dependencies.append((base_app_label, base_name, None, True))\n            # Generate creation operation\n            self.add_operation(\n                app_label,\n                operations.CreateModel(\n                    name=model_state.name,\n                    fields=[],\n                    options=model_state.options,\n                    bases=model_state.bases,\n                    managers=model_state.managers,\n                ),\n                # Depend on the deletion of any possible non-proxy version of us\n                dependencies=dependencies,\n            )\n\n    def generate_deleted_models(self):\n        \"\"\"\n        Find all deleted models (managed and unmanaged) and make delete\n        operations for them as well as separate operations to delete any\n        foreign key or M2M relationships (these are optimized later, if\n        possible).\n\n        Also bring forward removal of any model options that refer to\n        collections of fields - the inverse of generate_created_models().\n        \"\"\"\n        new_keys = self.new_model_keys | self.new_unmanaged_keys\n        deleted_models = self.old_model_keys - new_keys\n        deleted_unmanaged_models = self.old_unmanaged_keys - new_keys\n        all_deleted_models = chain(sorted(deleted_models), sorted(deleted_unmanaged_models))\n        for app_label, model_name in all_deleted_models:\n            model_state = self.from_state.models[app_label, model_name]\n            model = self.old_apps.get_model(app_label, model_name)\n            # Gather related fields\n            related_fields = {}\n            for field in model._meta.local_fields:\n                if field.remote_field:\n                    if field.remote_field.model:\n                        related_fields[field.name] = field\n                    # through will be none on M2Ms on swapped-out models;\n                    # we can treat lack of through as auto_created=True, though.\n                    if (getattr(field.remote_field, \"through\", None) and\n                            not field.remote_field.through._meta.auto_created):\n                        related_fields[field.name] = field\n            for field in model._meta.local_many_to_many:\n                if field.remote_field.model:\n                    related_fields[field.name] = field\n                if getattr(field.remote_field, \"through\", None) and not field.remote_field.through._meta.auto_created:\n                    related_fields[field.name] = field\n            # Generate option removal first\n            unique_together = model_state.options.pop('unique_together', None)\n            index_together = model_state.options.pop('index_together', None)\n            if unique_together:\n                self.add_operation(\n                    app_label,\n                    operations.AlterUniqueTogether(\n                        name=model_name,\n                        unique_together=None,\n                    )\n                )\n            if index_together:\n                self.add_operation(\n                    app_label,\n                    operations.AlterIndexTogether(\n                        name=model_name,\n                        index_together=None,\n                    )\n                )\n            # Then remove each related field\n            for name in sorted(related_fields):\n                self.add_operation(\n                    app_label,\n                    operations.RemoveField(\n                        model_name=model_name,\n                        name=name,\n                    )\n                )\n            # Finally, remove the model.\n            # This depends on both the removal/alteration of all incoming fields\n            # and the removal of all its own related fields, and if it's\n            # a through model the field that references it.\n            dependencies = []\n            for related_object in model._meta.related_objects:\n                related_object_app_label = related_object.related_model._meta.app_label\n                object_name = related_object.related_model._meta.object_name\n                field_name = related_object.field.name\n                dependencies.append((related_object_app_label, object_name, field_name, False))\n                if not related_object.many_to_many:\n                    dependencies.append((related_object_app_label, object_name, field_name, \"alter\"))\n\n            for name in sorted(related_fields):\n                dependencies.append((app_label, model_name, name, False))\n            # We're referenced in another field's through=\n            through_user = self.through_users.get((app_label, model_state.name_lower))\n            if through_user:\n                dependencies.append((through_user[0], through_user[1], through_user[2], False))\n            # Finally, make the operation, deduping any dependencies\n            self.add_operation(\n                app_label,\n                operations.DeleteModel(\n                    name=model_state.name,\n                ),\n                dependencies=list(set(dependencies)),\n            )\n\n    def generate_deleted_proxies(self):\n        \"\"\"Make DeleteModel options for proxy models.\"\"\"\n        deleted = self.old_proxy_keys - self.new_proxy_keys\n        for app_label, model_name in sorted(deleted):\n            model_state = self.from_state.models[app_label, model_name]\n            assert model_state.options.get(\"proxy\")\n            self.add_operation(\n                app_label,\n                operations.DeleteModel(\n                    name=model_state.name,\n                ),\n            )\n\n    def generate_renamed_fields(self):\n        \"\"\"Work out renamed fields.\"\"\"\n        self.renamed_fields = {}\n        for app_label, model_name, field_name in sorted(self.new_field_keys - self.old_field_keys):\n            old_model_name = self.renamed_models.get((app_label, model_name), model_name)\n            old_model_state = self.from_state.models[app_label, old_model_name]\n            field = self.new_apps.get_model(app_label, model_name)._meta.get_field(field_name)\n            # Scan to see if this is actually a rename!\n            field_dec = self.deep_deconstruct(field)\n            for rem_app_label, rem_model_name, rem_field_name in sorted(self.old_field_keys - self.new_field_keys):\n                if rem_app_label == app_label and rem_model_name == model_name:\n                    old_field = old_model_state.fields[rem_field_name]\n                    old_field_dec = self.deep_deconstruct(old_field)\n                    if field.remote_field and field.remote_field.model and 'to' in old_field_dec[2]:\n                        old_rel_to = old_field_dec[2]['to']\n                        if old_rel_to in self.renamed_models_rel:\n                            old_field_dec[2]['to'] = self.renamed_models_rel[old_rel_to]\n                    old_field.set_attributes_from_name(rem_field_name)\n                    old_db_column = old_field.get_attname_column()[1]\n                    if (old_field_dec == field_dec or (\n                            # Was the field renamed and db_column equal to the\n                            # old field's column added?\n                            old_field_dec[0:2] == field_dec[0:2] and\n                            dict(old_field_dec[2], db_column=old_db_column) == field_dec[2])):\n                        if self.questioner.ask_rename(model_name, rem_field_name, field_name, field):\n                            self.add_operation(\n                                app_label,\n                                operations.RenameField(\n                                    model_name=model_name,\n                                    old_name=rem_field_name,\n                                    new_name=field_name,\n                                )\n                            )\n                            self.old_field_keys.remove((rem_app_label, rem_model_name, rem_field_name))\n                            self.old_field_keys.add((app_label, model_name, field_name))\n                            self.renamed_fields[app_label, model_name, field_name] = rem_field_name\n                            break\n\n    def generate_added_fields(self):\n        \"\"\"Make AddField operations.\"\"\"\n        for app_label, model_name, field_name in sorted(self.new_field_keys - self.old_field_keys):\n            self._generate_added_field(app_label, model_name, field_name)\n\n    def _generate_added_field(self, app_label, model_name, field_name):\n        field = self.new_apps.get_model(app_label, model_name)._meta.get_field(field_name)\n        # Fields that are foreignkeys/m2ms depend on stuff\n        dependencies = []\n        if field.remote_field and field.remote_field.model:\n            dependencies.extend(self._get_dependencies_for_foreign_key(field))\n        # You can't just add NOT NULL fields with no default or fields\n        # which don't allow empty strings as default.\n        time_fields = (models.DateField, models.DateTimeField, models.TimeField)\n        preserve_default = (\n            field.null or field.has_default() or field.many_to_many or\n            (field.blank and field.empty_strings_allowed) or\n            (isinstance(field, time_fields) and field.auto_now)\n        )\n        if not preserve_default:\n            field = field.clone()\n            if isinstance(field, time_fields) and field.auto_now_add:\n                field.default = self.questioner.ask_auto_now_add_addition(field_name, model_name)\n            else:\n                field.default = self.questioner.ask_not_null_addition(field_name, model_name)\n        self.add_operation(\n            app_label,\n            operations.AddField(\n                model_name=model_name,\n                name=field_name,\n                field=field,\n                preserve_default=preserve_default,\n            ),\n            dependencies=dependencies,\n        )\n\n    def generate_removed_fields(self):\n        \"\"\"Make RemoveField operations.\"\"\"\n        for app_label, model_name, field_name in sorted(self.old_field_keys - self.new_field_keys):\n            self._generate_removed_field(app_label, model_name, field_name)\n\n    def _generate_removed_field(self, app_label, model_name, field_name):\n        self.add_operation(\n            app_label,\n            operations.RemoveField(\n                model_name=model_name,\n                name=field_name,\n            ),\n            # We might need to depend on the removal of an\n            # order_with_respect_to or index/unique_together operation;\n            # this is safely ignored if there isn't one\n            dependencies=[\n                (app_label, model_name, field_name, \"order_wrt_unset\"),\n                (app_label, model_name, field_name, \"foo_together_change\"),\n            ],\n        )\n\n    def generate_altered_fields(self):\n        \"\"\"\n        Make AlterField operations, or possibly RemovedField/AddField if alter\n(C)                 key: self.deep_deconstruct(value)\n                for key, value in obj.items()\n            }\n        elif isinstance(obj, functools.partial):\n            return (obj.func, self.deep_deconstruct(obj.args), self.deep_deconstruct(obj.keywords))\n        elif isinstance(obj, COMPILED_REGEX_TYPE):\n            return RegexObject(obj)\n        elif isinstance(obj, type):\n            # If this is a type that implements 'deconstruct' as an instance method,\n            # avoid treating this as being deconstructible itself - see #22951\n            return obj\n        elif hasattr(obj, 'deconstruct'):\n            deconstructed = obj.deconstruct()\n            if isinstance(obj, models.Field):\n                # we have a field which also returns a name\n                deconstructed = deconstructed[1:]\n            path, args, kwargs = deconstructed\n            return (\n                path,\n                [self.deep_deconstruct(value) for value in args],\n                {\n                    key: self.deep_deconstruct(value)\n                    for key, value in kwargs.items()\n                },\n            )\n        else:\n            return obj\n\n    def only_relation_agnostic_fields(self, fields):\n        \"\"\"\n        Return a definition of the fields that ignores field names and\n        what related fields actually relate to. Used for detecting renames (as\n        the related fields change during renames).\n        \"\"\"\n        fields_def = []\n        for name, field in sorted(fields.items()):\n            deconstruction = self.deep_deconstruct(field)\n            if field.remote_field and field.remote_field.model:\n                del deconstruction[2]['to']\n            fields_def.append(deconstruction)\n        return fields_def\n\n    def _detect_changes(self, convert_apps=None, graph=None):\n        \"\"\"\n        Return a dict of migration plans which will achieve the\n        change from from_state to to_state. The dict has app labels\n        as keys and a list of migrations as values.\n\n        The resulting migrations aren't specially named, but the names\n        do matter for dependencies inside the set.\n\n        convert_apps is the list of apps to convert to use migrations\n        (i.e. to make initial migrations for, in the usual case)\n\n        graph is an optional argument that, if provided, can help improve\n        dependency generation and avoid potential circular dependencies.\n        \"\"\"\n        # The first phase is generating all the operations for each app\n        # and gathering them into a big per-app list.\n        # Then go through that list, order it, and split into migrations to\n        # resolve dependencies caused by M2Ms and FKs.\n        self.generated_operations = {}\n        self.altered_indexes = {}\n        self.altered_constraints = {}\n\n        # Prepare some old/new state and model lists, separating\n        # proxy models and ignoring unmigrated apps.\n        self.old_apps = self.from_state.concrete_apps\n        self.new_apps = self.to_state.apps\n        self.old_model_keys = set()\n        self.old_proxy_keys = set()\n        self.old_unmanaged_keys = set()\n        self.new_model_keys = set()\n        self.new_proxy_keys = set()\n        self.new_unmanaged_keys = set()\n        for al, mn in self.from_state.models:\n            model = self.old_apps.get_model(al, mn)\n            if not model._meta.managed:\n                self.old_unmanaged_keys.add((al, mn))\n            elif al not in self.from_state.real_apps:\n                if model._meta.proxy:\n                    self.old_proxy_keys.add((al, mn))\n                else:\n                    self.old_model_keys.add((al, mn))\n\n        for al, mn in self.to_state.models:\n            model = self.new_apps.get_model(al, mn)\n            if not model._meta.managed:\n                self.new_unmanaged_keys.add((al, mn))\n            elif (\n                al not in self.from_state.real_apps or\n                (convert_apps and al in convert_apps)\n            ):\n                if model._meta.proxy:\n                    self.new_proxy_keys.add((al, mn))\n                else:\n                    self.new_model_keys.add((al, mn))\n\n        # Renames have to come first\n        self.generate_renamed_models()\n\n        # Prepare lists of fields and generate through model map\n        self._prepare_field_lists()\n        self._generate_through_model_map()\n\n        # Generate non-rename model operations\n        self.generate_deleted_models()\n        self.generate_created_models()\n        self.generate_deleted_proxies()\n        self.generate_created_proxies()\n        self.generate_altered_options()\n        self.generate_altered_managers()\n\n        # Create the altered indexes and store them in self.altered_indexes.\n        # This avoids the same computation in generate_removed_indexes()\n        # and generate_added_indexes().\n        self.create_altered_indexes()\n        self.create_altered_constraints()\n        # Generate index removal operations before field is removed\n        self.generate_removed_constraints()\n        self.generate_removed_indexes()\n        # Generate field operations\n        self.generate_renamed_fields()\n        self.generate_removed_fields()\n        self.generate_added_fields()\n        self.generate_altered_fields()\n        self.generate_altered_unique_together()\n        self.generate_altered_index_together()\n        self.generate_added_indexes()\n        self.generate_added_constraints()\n        self.generate_altered_db_table()\n        self.generate_altered_order_with_respect_to()\n\n        self._sort_migrations()\n        self._build_migration_list(graph)\n        self._optimize_migrations()\n\n        return self.migrations\n\n    def _prepare_field_lists(self):\n        \"\"\"\n        Prepare field lists and a list of the fields that used through models\n        in the old state so dependencies can be made from the through model\n        deletion to the field that uses it.\n        \"\"\"\n        self.kept_model_keys = self.old_model_keys & self.new_model_keys\n        self.kept_proxy_keys = self.old_proxy_keys & self.new_proxy_keys\n        self.kept_unmanaged_keys = self.old_unmanaged_keys & self.new_unmanaged_keys\n        self.through_users = {}\n        self.old_field_keys = {\n            (app_label, model_name, field_name)\n            for app_label, model_name in self.kept_model_keys\n            for field_name in self.from_state.models[\n                app_label,\n                self.renamed_models.get((app_label, model_name), model_name)\n            ].fields\n        }\n        self.new_field_keys = {\n            (app_label, model_name, field_name)\n            for app_label, model_name in self.kept_model_keys\n            for field_name in self.to_state.models[app_label, model_name].fields\n        }\n\n    def _generate_through_model_map(self):\n        \"\"\"Through model map generation.\"\"\"\n        for app_label, model_name in sorted(self.old_model_keys):\n            old_model_name = self.renamed_models.get((app_label, model_name), model_name)\n            old_model_state = self.from_state.models[app_label, old_model_name]\n            for field_name in old_model_state.fields:\n                old_field = self.old_apps.get_model(app_label, old_model_name)._meta.get_field(field_name)\n                if (hasattr(old_field, \"remote_field\") and getattr(old_field.remote_field, \"through\", None) and\n                        not old_field.remote_field.through._meta.auto_created):\n                    through_key = (\n                        old_field.remote_field.through._meta.app_label,\n                        old_field.remote_field.through._meta.model_name,\n                    )\n                    self.through_users[through_key] = (app_label, old_model_name, field_name)\n\n    @staticmethod\n    def _resolve_dependency(dependency):\n        \"\"\"\n        Return the resolved dependency and a boolean denoting whether or not\n        it was swappable.\n        \"\"\"\n        if dependency[0] != '__setting__':\n            return dependency, False\n        resolved_app_label, resolved_object_name = getattr(settings, dependency[1]).split('.')\n        return (resolved_app_label, resolved_object_name.lower()) + dependency[2:], True\n\n    def _build_migration_list(self, graph=None):\n        \"\"\"\n        Chop the lists of operations up into migrations with dependencies on\n        each other. Do this by going through an app's list of operations until\n        one is found that has an outgoing dependency that isn't in another\n        app's migration yet (hasn't been chopped off its list). Then chop off\n        the operations before it into a migration and move onto the next app.\n        If the loops completes without doing anything, there's a circular\n        dependency (which _should_ be impossible as the operations are\n        all split at this point so they can't depend and be depended on).\n        \"\"\"\n        self.migrations = {}\n        num_ops = sum(len(x) for x in self.generated_operations.values())\n        chop_mode = False\n        while num_ops:\n            # On every iteration, we step through all the apps and see if there\n            # is a completed set of operations.\n            # If we find that a subset of the operations are complete we can\n            # try to chop it off from the rest and continue, but we only\n            # do this if we've already been through the list once before\n            # without any chopping and nothing has changed.\n            for app_label in sorted(self.generated_operations):\n                chopped = []\n                dependencies = set()\n                for operation in list(self.generated_operations[app_label]):\n                    deps_satisfied = True\n                    operation_dependencies = set()\n                    for dep in operation._auto_deps:\n                        # Temporarily resolve the swappable dependency to\n                        # prevent circular references. While keeping the\n                        # dependency checks on the resolved model, add the\n                        # swappable dependencies.\n                        original_dep = dep\n                        dep, is_swappable_dep = self._resolve_dependency(dep)\n                        if dep[0] != app_label:\n                            # External app dependency. See if it's not yet\n                            # satisfied.\n                            for other_operation in self.generated_operations.get(dep[0], []):\n                                if self.check_dependency(other_operation, dep):\n                                    deps_satisfied = False\n                                    break\n                            if not deps_satisfied:\n                                break\n                            else:\n                                if is_swappable_dep:\n                                    operation_dependencies.add((original_dep[0], original_dep[1]))\n                                elif dep[0] in self.migrations:\n                                    operation_dependencies.add((dep[0], self.migrations[dep[0]][-1].name))\n                                else:\n                                    # If we can't find the other app, we add a first/last dependency,\n                                    # but only if we've already been through once and checked everything\n                                    if chop_mode:\n                                        # If the app already exists, we add a dependency on the last migration,\n                                        # as we don't know which migration contains the target field.\n                                        # If it's not yet migrated or has no migrations, we use __first__\n                                        if graph and graph.leaf_nodes(dep[0]):\n                                            operation_dependencies.add(graph.leaf_nodes(dep[0])[0])\n                                        else:\n                                            operation_dependencies.add((dep[0], \"__first__\"))\n                                    else:\n                                        deps_satisfied = False\n                    if deps_satisfied:\n                        chopped.append(operation)\n                        dependencies.update(operation_dependencies)\n                        del self.generated_operations[app_label][0]\n                    else:\n                        break\n                # Make a migration! Well, only if there's stuff to put in it\n                if dependencies or chopped:\n                    if not self.generated_operations[app_label] or chop_mode:\n                        subclass = type(\"Migration\", (Migration,), {\"operations\": [], \"dependencies\": []})\n                        instance = subclass(\"auto_%i\" % (len(self.migrations.get(app_label, [])) + 1), app_label)\n                        instance.dependencies = list(dependencies)\n                        instance.operations = chopped\n                        instance.initial = app_label not in self.existing_apps\n                        self.migrations.setdefault(app_label, []).append(instance)\n                        chop_mode = False\n                    else:\n                        self.generated_operations[app_label] = chopped + self.generated_operations[app_label]\n            new_num_ops = sum(len(x) for x in self.generated_operations.values())\n            if new_num_ops == num_ops:\n                if not chop_mode:\n                    chop_mode = True\n                else:\n                    raise ValueError(\"Cannot resolve operation dependencies: %r\" % self.generated_operations)\n            num_ops = new_num_ops\n\n    def _sort_migrations(self):\n        \"\"\"\n        Reorder to make things possible. Reordering may be needed so FKs work\n        nicely inside the same app.\n        \"\"\"\n        for app_label, ops in sorted(self.generated_operations.items()):\n            # construct a dependency graph for intra-app dependencies\n            dependency_graph = {op: set() for op in ops}\n            for op in ops:\n                for dep in op._auto_deps:\n                    # Resolve intra-app dependencies to handle circular\n                    # references involving a swappable model.\n                    dep = self._resolve_dependency(dep)[0]\n                    if dep[0] == app_label:\n                        for op2 in ops:\n                            if self.check_dependency(op2, dep):\n                                dependency_graph[op].add(op2)\n\n            # we use a stable sort for deterministic tests & general behavior\n            self.generated_operations[app_label] = stable_topological_sort(ops, dependency_graph)\n\n    def _optimize_migrations(self):\n        # Add in internal dependencies among the migrations\n        for app_label, migrations in self.migrations.items():\n            for m1, m2 in zip(migrations, migrations[1:]):\n                m2.dependencies.append((app_label, m1.name))\n\n        # De-dupe dependencies\n        for migrations in self.migrations.values():\n            for migration in migrations:\n                migration.dependencies = list(set(migration.dependencies))\n\n        # Optimize migrations\n        for app_label, migrations in self.migrations.items():\n            for migration in migrations:\n                migration.operations = MigrationOptimizer().optimize(migration.operations, app_label)\n\n    def check_dependency(self, operation, dependency):\n        \"\"\"\n        Return True if the given operation depends on the given dependency,\n        False otherwise.\n        \"\"\"\n        # Created model\n        if dependency[2] is None and dependency[3] is True:\n            return (\n                isinstance(operation, operations.CreateModel) and\n                operation.name_lower == dependency[1].lower()\n            )\n        # Created field\n        elif dependency[2] is not None and dependency[3] is True:\n            return (\n                (\n                    isinstance(operation, operations.CreateModel) and\n                    operation.name_lower == dependency[1].lower() and\n                    any(dependency[2] == x for x, y in operation.fields)\n                ) or\n                (\n                    isinstance(operation, operations.AddField) and\n                    operation.model_name_lower == dependency[1].lower() and\n                    operation.name_lower == dependency[2].lower()\n                )\n            )\n        # Removed field\n        elif dependency[2] is not None and dependency[3] is False:\n            return (\n                isinstance(operation, operations.RemoveField) and\n                operation.model_name_lower == dependency[1].lower() and\n                operation.name_lower == dependency[2].lower()\n            )\n        # Removed model\n        elif dependency[2] is None and dependency[3] is False:\n            return (\n                isinstance(operation, operations.DeleteModel) and\n                operation.name_lower == dependency[1].lower()\n            )\n        # Field being altered\n        elif dependency[2] is not None and dependency[3] == \"alter\":\n            return (\n                isinstance(operation, operations.AlterField) and\n                operation.model_name_lower == dependency[1].lower() and\n                operation.name_lower == dependency[2].lower()\n            )\n        # order_with_respect_to being unset for a field\n        elif dependency[2] is not None and dependency[3] == \"order_wrt_unset\":\n            return (\n                isinstance(operation, operations.AlterOrderWithRespectTo) and\n                operation.name_lower == dependency[1].lower() and\n                (operation.order_with_respect_to or \"\").lower() != dependency[2].lower()\n            )\n        # Field is removed and part of an index/unique_together\n        elif dependency[2] is not None and dependency[3] == \"foo_together_change\":\n            return (\n                isinstance(operation, (operations.AlterUniqueTogether,\n                                       operations.AlterIndexTogether)) and\n                operation.name_lower == dependency[1].lower()\n            )\n        # Unknown dependency. Raise an error.\n        else:\n            raise ValueError(\"Can't handle dependency %r\" % (dependency,))\n\n    def add_operation(self, app_label, operation, dependencies=None, beginning=False):\n        # Dependencies are (app_label, model_name, field_name, create/delete as True/False)\n        operation._auto_deps = dependencies or []\n        if beginning:\n            self.generated_operations.setdefault(app_label, []).insert(0, operation)\n        else:\n            self.generated_operations.setdefault(app_label, []).append(operation)\n\n    def swappable_first_key(self, item):\n        \"\"\"\n        Place potential swappable models first in lists of created models (only\n        real way to solve #22783).\n        \"\"\"\n        try:\n            model = self.new_apps.get_model(item[0], item[1])\n            base_names = [base.__name__ for base in model.__bases__]\n            string_version = \"%s.%s\" % (item[0], item[1])\n            if (\n                model._meta.swappable or\n                \"AbstractUser\" in base_names or\n                \"AbstractBaseUser\" in base_names or\n                settings.AUTH_USER_MODEL.lower() == string_version.lower()\n            ):\n                return (\"___\" + item[0], \"___\" + item[1])\n        except LookupError:\n            pass\n        return item\n\n    def generate_renamed_models(self):\n        \"\"\"\n        Find any renamed models, generate the operations for them, and remove\n        the old entry from the model lists. Must be run before other\n        model-level generation.\n        \"\"\"\n        self.renamed_models = {}\n        self.renamed_models_rel = {}\n        added_models = self.new_model_keys - self.old_model_keys\n        for app_label, model_name in sorted(added_models):\n            model_state = self.to_state.models[app_label, model_name]\n            model_fields_def = self.only_relation_agnostic_fields(model_state.fields)\n\n            removed_models = self.old_model_keys - self.new_model_keys\n            for rem_app_label, rem_model_name in removed_models:\n                if rem_app_label == app_label:\n                    rem_model_state = self.from_state.models[rem_app_label, rem_model_name]\n                    rem_model_fields_def = self.only_relation_agnostic_fields(rem_model_state.fields)\n                    if model_fields_def == rem_model_fields_def:\n                        if self.questioner.ask_rename_model(rem_model_state, model_state):\n                            model_opts = self.new_apps.get_model(app_label, model_name)._meta\n                            dependencies = []\n                            for field in model_opts.get_fields():\n                                if field.is_relation:\n                                    dependencies.extend(self._get_dependencies_for_foreign_key(field))\n                            self.add_operation(\n                                app_label,\n                                operations.RenameModel(\n                                    old_name=rem_model_state.name,\n                                    new_name=model_state.name,\n                                ),\n                                dependencies=dependencies,\n                            )\n                            self.renamed_models[app_label, model_name] = rem_model_name\n                            renamed_models_rel_key = '%s.%s' % (\n                                rem_model_state.app_label,\n                                rem_model_state.name_lower,\n                            )\n                            self.renamed_models_rel[renamed_models_rel_key] = '%s.%s' % (\n                                model_state.app_label,\n                                model_state.name_lower,\n                            )\n                            self.old_model_keys.remove((rem_app_label, rem_model_name))\n                            self.old_model_keys.add((app_label, model_name))\n                            break\n\n    def generate_created_models(self):\n        \"\"\"\n        Find all new models (both managed and unmanaged) and make create\n        operations for them as well as separate operations to create any\n        foreign key or M2M relationships (these are optimized later, if\n        possible).\n\n        Defer any model options that refer to collections of fields that might\n        be deferred (e.g. unique_together, index_together).\n        \"\"\"\n        old_keys = self.old_model_keys | self.old_unmanaged_keys\n        added_models = self.new_model_keys - old_keys\n        added_unmanaged_models = self.new_unmanaged_keys - old_keys\n        all_added_models = chain(\n            sorted(added_models, key=self.swappable_first_key, reverse=True),\n            sorted(added_unmanaged_models, key=self.swappable_first_key, reverse=True)\n        )\n        for app_label, model_name in all_added_models:\n            model_state = self.to_state.models[app_label, model_name]\n            model_opts = self.new_apps.get_model(app_label, model_name)._meta\n            # Gather related fields\n            related_fields = {}\n            primary_key_rel = None\n            for field in model_opts.local_fields:\n                if field.remote_field:\n                    if field.remote_field.model:\n                        if field.primary_key:\n                            primary_key_rel = field.remote_field.model\n                        elif not field.remote_field.parent_link:\n                            related_fields[field.name] = field\n                    # through will be none on M2Ms on swapped-out models;\n                    # we can treat lack of through as auto_created=True, though.\n                    if (getattr(field.remote_field, \"through\", None) and\n                            not field.remote_field.through._meta.auto_created):\n                        related_fields[field.name] = field\n            for field in model_opts.local_many_to_many:\n                if field.remote_field.model:\n                    related_fields[field.name] = field\n                if getattr(field.remote_field, \"through\", None) and not field.remote_field.through._meta.auto_created:\n                    related_fields[field.name] = field\n(D)                                         else:\n                                            operation_dependencies.add((dep[0], \"__first__\"))\n                                    else:\n                                        deps_satisfied = False\n                    if deps_satisfied:\n                        chopped.append(operation)\n                        dependencies.update(operation_dependencies)\n                        del self.generated_operations[app_label][0]\n                    else:\n                        break\n                # Make a migration! Well, only if there's stuff to put in it\n                if dependencies or chopped:\n                    if not self.generated_operations[app_label] or chop_mode:\n                        subclass = type(\"Migration\", (Migration,), {\"operations\": [], \"dependencies\": []})\n                        instance = subclass(\"auto_%i\" % (len(self.migrations.get(app_label, [])) + 1), app_label)\n                        instance.dependencies = list(dependencies)\n                        instance.operations = chopped\n                        instance.initial = app_label not in self.existing_apps\n                        self.migrations.setdefault(app_label, []).append(instance)\n                        chop_mode = False\n                    else:\n                        self.generated_operations[app_label] = chopped + self.generated_operations[app_label]\n            new_num_ops = sum(len(x) for x in self.generated_operations.values())\n            if new_num_ops == num_ops:\n                if not chop_mode:\n                    chop_mode = True\n                else:\n                    raise ValueError(\"Cannot resolve operation dependencies: %r\" % self.generated_operations)\n            num_ops = new_num_ops\n\n    def _sort_migrations(self):\n        \"\"\"\n        Reorder to make things possible. Reordering may be needed so FKs work\n        nicely inside the same app.\n        \"\"\"\n        for app_label, ops in sorted(self.generated_operations.items()):\n            # construct a dependency graph for intra-app dependencies\n            dependency_graph = {op: set() for op in ops}\n            for op in ops:\n                for dep in op._auto_deps:\n                    # Resolve intra-app dependencies to handle circular\n                    # references involving a swappable model.\n                    dep = self._resolve_dependency(dep)[0]\n                    if dep[0] == app_label:\n                        for op2 in ops:\n                            if self.check_dependency(op2, dep):\n                                dependency_graph[op].add(op2)\n\n            # we use a stable sort for deterministic tests & general behavior\n            self.generated_operations[app_label] = stable_topological_sort(ops, dependency_graph)\n\n    def _optimize_migrations(self):\n        # Add in internal dependencies among the migrations\n        for app_label, migrations in self.migrations.items():\n            for m1, m2 in zip(migrations, migrations[1:]):\n                m2.dependencies.append((app_label, m1.name))\n\n        # De-dupe dependencies\n        for migrations in self.migrations.values():\n            for migration in migrations:\n                migration.dependencies = list(set(migration.dependencies))\n\n        # Optimize migrations\n        for app_label, migrations in self.migrations.items():\n            for migration in migrations:\n                migration.operations = MigrationOptimizer().optimize(migration.operations, app_label)\n\n    def check_dependency(self, operation, dependency):\n        \"\"\"\n        Return True if the given operation depends on the given dependency,\n        False otherwise.\n        \"\"\"\n        # Created model\n        if dependency[2] is None and dependency[3] is True:\n            return (\n                isinstance(operation, operations.CreateModel) and\n                operation.name_lower == dependency[1].lower()\n            )\n        # Created field\n        elif dependency[2] is not None and dependency[3] is True:\n            return (\n                (\n                    isinstance(operation, operations.CreateModel) and\n                    operation.name_lower == dependency[1].lower() and\n                    any(dependency[2] == x for x, y in operation.fields)\n                ) or\n                (\n                    isinstance(operation, operations.AddField) and\n                    operation.model_name_lower == dependency[1].lower() and\n                    operation.name_lower == dependency[2].lower()\n                )\n            )\n        # Removed field\n        elif dependency[2] is not None and dependency[3] is False:\n            return (\n                isinstance(operation, operations.RemoveField) and\n                operation.model_name_lower == dependency[1].lower() and\n                operation.name_lower == dependency[2].lower()\n            )\n        # Removed model\n        elif dependency[2] is None and dependency[3] is False:\n            return (\n                isinstance(operation, operations.DeleteModel) and\n                operation.name_lower == dependency[1].lower()\n            )\n        # Field being altered\n        elif dependency[2] is not None and dependency[3] == \"alter\":\n            return (\n                isinstance(operation, operations.AlterField) and\n                operation.model_name_lower == dependency[1].lower() and\n                operation.name_lower == dependency[2].lower()\n            )\n        # order_with_respect_to being unset for a field\n        elif dependency[2] is not None and dependency[3] == \"order_wrt_unset\":\n            return (\n                isinstance(operation, operations.AlterOrderWithRespectTo) and\n                operation.name_lower == dependency[1].lower() and\n                (operation.order_with_respect_to or \"\").lower() != dependency[2].lower()\n            )\n        # Field is removed and part of an index/unique_together\n        elif dependency[2] is not None and dependency[3] == \"foo_together_change\":\n            return (\n                isinstance(operation, (operations.AlterUniqueTogether,\n                                       operations.AlterIndexTogether)) and\n                operation.name_lower == dependency[1].lower()\n            )\n        # Unknown dependency. Raise an error.\n        else:\n            raise ValueError(\"Can't handle dependency %r\" % (dependency,))\n\n    def add_operation(self, app_label, operation, dependencies=None, beginning=False):\n        # Dependencies are (app_label, model_name, field_name, create/delete as True/False)\n        operation._auto_deps = dependencies or []\n        if beginning:\n            self.generated_operations.setdefault(app_label, []).insert(0, operation)\n        else:\n            self.generated_operations.setdefault(app_label, []).append(operation)\n\n    def swappable_first_key(self, item):\n        \"\"\"\n        Place potential swappable models first in lists of created models (only\n        real way to solve #22783).\n        \"\"\"\n        try:\n            model = self.new_apps.get_model(item[0], item[1])\n            base_names = [base.__name__ for base in model.__bases__]\n            string_version = \"%s.%s\" % (item[0], item[1])\n            if (\n                model._meta.swappable or\n                \"AbstractUser\" in base_names or\n                \"AbstractBaseUser\" in base_names or\n                settings.AUTH_USER_MODEL.lower() == string_version.lower()\n            ):\n                return (\"___\" + item[0], \"___\" + item[1])\n        except LookupError:\n            pass\n        return item\n\n    def generate_renamed_models(self):\n        \"\"\"\n        Find any renamed models, generate the operations for them, and remove\n        the old entry from the model lists. Must be run before other\n        model-level generation.\n        \"\"\"\n        self.renamed_models = {}\n        self.renamed_models_rel = {}\n        added_models = self.new_model_keys - self.old_model_keys\n        for app_label, model_name in sorted(added_models):\n            model_state = self.to_state.models[app_label, model_name]\n            model_fields_def = self.only_relation_agnostic_fields(model_state.fields)\n\n            removed_models = self.old_model_keys - self.new_model_keys\n            for rem_app_label, rem_model_name in removed_models:\n                if rem_app_label == app_label:\n                    rem_model_state = self.from_state.models[rem_app_label, rem_model_name]\n                    rem_model_fields_def = self.only_relation_agnostic_fields(rem_model_state.fields)\n                    if model_fields_def == rem_model_fields_def:\n                        if self.questioner.ask_rename_model(rem_model_state, model_state):\n                            model_opts = self.new_apps.get_model(app_label, model_name)._meta\n                            dependencies = []\n                            for field in model_opts.get_fields():\n                                if field.is_relation:\n                                    dependencies.extend(self._get_dependencies_for_foreign_key(field))\n                            self.add_operation(\n                                app_label,\n                                operations.RenameModel(\n                                    old_name=rem_model_state.name,\n                                    new_name=model_state.name,\n                                ),\n                                dependencies=dependencies,\n                            )\n                            self.renamed_models[app_label, model_name] = rem_model_name\n                            renamed_models_rel_key = '%s.%s' % (\n                                rem_model_state.app_label,\n                                rem_model_state.name_lower,\n                            )\n                            self.renamed_models_rel[renamed_models_rel_key] = '%s.%s' % (\n                                model_state.app_label,\n                                model_state.name_lower,\n                            )\n                            self.old_model_keys.remove((rem_app_label, rem_model_name))\n                            self.old_model_keys.add((app_label, model_name))\n                            break\n\n    def generate_created_models(self):\n        \"\"\"\n        Find all new models (both managed and unmanaged) and make create\n        operations for them as well as separate operations to create any\n        foreign key or M2M relationships (these are optimized later, if\n        possible).\n\n        Defer any model options that refer to collections of fields that might\n        be deferred (e.g. unique_together, index_together).\n        \"\"\"\n        old_keys = self.old_model_keys | self.old_unmanaged_keys\n        added_models = self.new_model_keys - old_keys\n        added_unmanaged_models = self.new_unmanaged_keys - old_keys\n        all_added_models = chain(\n            sorted(added_models, key=self.swappable_first_key, reverse=True),\n            sorted(added_unmanaged_models, key=self.swappable_first_key, reverse=True)\n        )\n        for app_label, model_name in all_added_models:\n            model_state = self.to_state.models[app_label, model_name]\n            model_opts = self.new_apps.get_model(app_label, model_name)._meta\n            # Gather related fields\n            related_fields = {}\n            primary_key_rel = None\n            for field in model_opts.local_fields:\n                if field.remote_field:\n                    if field.remote_field.model:\n                        if field.primary_key:\n                            primary_key_rel = field.remote_field.model\n                        elif not field.remote_field.parent_link:\n                            related_fields[field.name] = field\n                    # through will be none on M2Ms on swapped-out models;\n                    # we can treat lack of through as auto_created=True, though.\n                    if (getattr(field.remote_field, \"through\", None) and\n                            not field.remote_field.through._meta.auto_created):\n                        related_fields[field.name] = field\n            for field in model_opts.local_many_to_many:\n                if field.remote_field.model:\n                    related_fields[field.name] = field\n                if getattr(field.remote_field, \"through\", None) and not field.remote_field.through._meta.auto_created:\n                    related_fields[field.name] = field\n            # Are there indexes/unique|index_together to defer?\n            indexes = model_state.options.pop('indexes')\n            constraints = model_state.options.pop('constraints')\n            unique_together = model_state.options.pop('unique_together', None)\n            index_together = model_state.options.pop('index_together', None)\n            order_with_respect_to = model_state.options.pop('order_with_respect_to', None)\n            # Depend on the deletion of any possible proxy version of us\n            dependencies = [\n                (app_label, model_name, None, False),\n            ]\n            # Depend on all bases\n            for base in model_state.bases:\n                if isinstance(base, str) and \".\" in base:\n                    base_app_label, base_name = base.split(\".\", 1)\n                    dependencies.append((base_app_label, base_name, None, True))\n                    # Depend on the removal of base fields if the new model has\n                    # a field with the same name.\n                    old_base_model_state = self.from_state.models.get((base_app_label, base_name))\n                    new_base_model_state = self.to_state.models.get((base_app_label, base_name))\n                    if old_base_model_state and new_base_model_state:\n                        removed_base_fields = set(old_base_model_state.fields).difference(\n                            new_base_model_state.fields,\n                        ).intersection(model_state.fields)\n                        for removed_base_field in removed_base_fields:\n                            dependencies.append((base_app_label, base_name, removed_base_field, False))\n            # Depend on the other end of the primary key if it's a relation\n            if primary_key_rel:\n                dependencies.append((\n                    primary_key_rel._meta.app_label,\n                    primary_key_rel._meta.object_name,\n                    None,\n                    True\n                ))\n            # Generate creation operation\n            self.add_operation(\n                app_label,\n                operations.CreateModel(\n                    name=model_state.name,\n                    fields=[d for d in model_state.fields.items() if d[0] not in related_fields],\n                    options=model_state.options,\n                    bases=model_state.bases,\n                    managers=model_state.managers,\n                ),\n                dependencies=dependencies,\n                beginning=True,\n            )\n\n            # Don't add operations which modify the database for unmanaged models\n            if not model_opts.managed:\n                continue\n\n            # Generate operations for each related field\n            for name, field in sorted(related_fields.items()):\n                dependencies = self._get_dependencies_for_foreign_key(field)\n                # Depend on our own model being created\n                dependencies.append((app_label, model_name, None, True))\n                # Make operation\n                self.add_operation(\n                    app_label,\n                    operations.AddField(\n                        model_name=model_name,\n                        name=name,\n                        field=field,\n                    ),\n                    dependencies=list(set(dependencies)),\n                )\n            # Generate other opns\n            related_dependencies = [\n                (app_label, model_name, name, True)\n                for name in sorted(related_fields)\n            ]\n            related_dependencies.append((app_label, model_name, None, True))\n            for index in indexes:\n                self.add_operation(\n                    app_label,\n                    operations.AddIndex(\n                        model_name=model_name,\n                        index=index,\n                    ),\n                    dependencies=related_dependencies,\n                )\n            for constraint in constraints:\n                self.add_operation(\n                    app_label,\n                    operations.AddConstraint(\n                        model_name=model_name,\n                        constraint=constraint,\n                    ),\n                    dependencies=related_dependencies,\n                )\n            if unique_together:\n                self.add_operation(\n                    app_label,\n                    operations.AlterUniqueTogether(\n                        name=model_name,\n                        unique_together=unique_together,\n                    ),\n                    dependencies=related_dependencies\n                )\n            if index_together:\n                self.add_operation(\n                    app_label,\n                    operations.AlterIndexTogether(\n                        name=model_name,\n                        index_together=index_together,\n                    ),\n                    dependencies=related_dependencies\n                )\n            if order_with_respect_to:\n                self.add_operation(\n                    app_label,\n                    operations.AlterOrderWithRespectTo(\n                        name=model_name,\n                        order_with_respect_to=order_with_respect_to,\n                    ),\n                    dependencies=[\n                        (app_label, model_name, order_with_respect_to, True),\n                        (app_label, model_name, None, True),\n                    ]\n                )\n\n            # Fix relationships if the model changed from a proxy model to a\n            # concrete model.\n            if (app_label, model_name) in self.old_proxy_keys:\n                for related_object in model_opts.related_objects:\n                    self.add_operation(\n                        related_object.related_model._meta.app_label,\n                        operations.AlterField(\n                            model_name=related_object.related_model._meta.object_name,\n                            name=related_object.field.name,\n                            field=related_object.field,\n                        ),\n                        dependencies=[(app_label, model_name, None, True)],\n                    )\n\n    def generate_created_proxies(self):\n        \"\"\"\n        Make CreateModel statements for proxy models. Use the same statements\n        as that way there's less code duplication, but for proxy models it's\n        safe to skip all the pointless field stuff and chuck out an operation.\n        \"\"\"\n        added = self.new_proxy_keys - self.old_proxy_keys\n        for app_label, model_name in sorted(added):\n            model_state = self.to_state.models[app_label, model_name]\n            assert model_state.options.get(\"proxy\")\n            # Depend on the deletion of any possible non-proxy version of us\n            dependencies = [\n                (app_label, model_name, None, False),\n            ]\n            # Depend on all bases\n            for base in model_state.bases:\n                if isinstance(base, str) and \".\" in base:\n                    base_app_label, base_name = base.split(\".\", 1)\n                    dependencies.append((base_app_label, base_name, None, True))\n            # Generate creation operation\n            self.add_operation(\n                app_label,\n                operations.CreateModel(\n                    name=model_state.name,\n                    fields=[],\n                    options=model_state.options,\n                    bases=model_state.bases,\n                    managers=model_state.managers,\n                ),\n                # Depend on the deletion of any possible non-proxy version of us\n                dependencies=dependencies,\n            )\n\n    def generate_deleted_models(self):\n        \"\"\"\n        Find all deleted models (managed and unmanaged) and make delete\n        operations for them as well as separate operations to delete any\n        foreign key or M2M relationships (these are optimized later, if\n        possible).\n\n        Also bring forward removal of any model options that refer to\n        collections of fields - the inverse of generate_created_models().\n        \"\"\"\n        new_keys = self.new_model_keys | self.new_unmanaged_keys\n        deleted_models = self.old_model_keys - new_keys\n        deleted_unmanaged_models = self.old_unmanaged_keys - new_keys\n        all_deleted_models = chain(sorted(deleted_models), sorted(deleted_unmanaged_models))\n        for app_label, model_name in all_deleted_models:\n            model_state = self.from_state.models[app_label, model_name]\n            model = self.old_apps.get_model(app_label, model_name)\n            # Gather related fields\n            related_fields = {}\n            for field in model._meta.local_fields:\n                if field.remote_field:\n                    if field.remote_field.model:\n                        related_fields[field.name] = field\n                    # through will be none on M2Ms on swapped-out models;\n                    # we can treat lack of through as auto_created=True, though.\n                    if (getattr(field.remote_field, \"through\", None) and\n                            not field.remote_field.through._meta.auto_created):\n                        related_fields[field.name] = field\n            for field in model._meta.local_many_to_many:\n                if field.remote_field.model:\n                    related_fields[field.name] = field\n                if getattr(field.remote_field, \"through\", None) and not field.remote_field.through._meta.auto_created:\n                    related_fields[field.name] = field\n            # Generate option removal first\n            unique_together = model_state.options.pop('unique_together', None)\n            index_together = model_state.options.pop('index_together', None)\n            if unique_together:\n                self.add_operation(\n                    app_label,\n                    operations.AlterUniqueTogether(\n                        name=model_name,\n                        unique_together=None,\n                    )\n                )\n            if index_together:\n                self.add_operation(\n                    app_label,\n                    operations.AlterIndexTogether(\n                        name=model_name,\n                        index_together=None,\n                    )\n                )\n            # Then remove each related field\n            for name in sorted(related_fields):\n                self.add_operation(\n                    app_label,\n                    operations.RemoveField(\n                        model_name=model_name,\n                        name=name,\n                    )\n                )\n            # Finally, remove the model.\n            # This depends on both the removal/alteration of all incoming fields\n            # and the removal of all its own related fields, and if it's\n            # a through model the field that references it.\n            dependencies = []\n            for related_object in model._meta.related_objects:\n                related_object_app_label = related_object.related_model._meta.app_label\n                object_name = related_object.related_model._meta.object_name\n                field_name = related_object.field.name\n                dependencies.append((related_object_app_label, object_name, field_name, False))\n                if not related_object.many_to_many:\n                    dependencies.append((related_object_app_label, object_name, field_name, \"alter\"))\n\n            for name in sorted(related_fields):\n                dependencies.append((app_label, model_name, name, False))\n            # We're referenced in another field's through=\n            through_user = self.through_users.get((app_label, model_state.name_lower))", "answer": "A"}
{"uuid": "68f6601a-94c6-4cb7-9f7f-c6823fb79695", "issue_statement": "limit_choices_to on a ForeignKey can render duplicate options in formfield\nDescription\n\t\nIf you pass a Q object as limit_choices_to on a ForeignKey field involving a join, you may end up with duplicate options in your form.\nSee regressiontest in patch for a clear view on the problem.", "choices": "(A) def fields_for_model(model, fields=None, exclude=None, widgets=None,\n                     formfield_callback=None, localized_fields=None,\n                     labels=None, help_texts=None, error_messages=None,\n                     field_classes=None, *, apply_limit_choices_to=True):\n    \"\"\"\n    Return a dictionary containing form fields for the given model.\n\n    ``fields`` is an optional list of field names. If provided, return only the\n    named fields.\n\n    ``exclude`` is an optional list of field names. If provided, exclude the\n    named fields from the returned fields, even if they are listed in the\n    ``fields`` argument.\n\n    ``widgets`` is a dictionary of model field names mapped to a widget.\n\n    ``formfield_callback`` is a callable that takes a model field and returns\n    a form field.\n(B) \ndef apply_limit_choices_to_to_formfield(formfield):\n    \"\"\"Apply limit_choices_to to the formfield's queryset if needed.\"\"\"\n    if hasattr(formfield, 'queryset') and hasattr(formfield, 'get_limit_choices_to'):\n        limit_choices_to = formfield.get_limit_choices_to()\n        if limit_choices_to is not None:\n            formfield.queryset = formfield.queryset.complex_filter(limit_choices_to)\n\n\ndef fields_for_model(model, fields=None, exclude=None, widgets=None,\n                     formfield_callback=None, localized_fields=None,\n                     labels=None, help_texts=None, error_messages=None,\n                     field_classes=None, *, apply_limit_choices_to=True):\n    \"\"\"\n    Return a dictionary containing form fields for the given model.\n\n    ``fields`` is an optional list of field names. If provided, return only the\n    named fields.\n(C)         if limit_choices_to is not None:\n            formfield.queryset = formfield.queryset.complex_filter(limit_choices_to)\n\n\ndef fields_for_model(model, fields=None, exclude=None, widgets=None,\n                     formfield_callback=None, localized_fields=None,\n                     labels=None, help_texts=None, error_messages=None,\n                     field_classes=None, *, apply_limit_choices_to=True):\n    \"\"\"\n    Return a dictionary containing form fields for the given model.\n\n    ``fields`` is an optional list of field names. If provided, return only the\n    named fields.\n\n    ``exclude`` is an optional list of field names. If provided, exclude the\n    named fields from the returned fields, even if they are listed in the\n    ``fields`` argument.\n\n(D)         if exclude and f.name in exclude:\n            continue\n        data[f.name] = f.value_from_object(instance)\n    return data\n\n\ndef apply_limit_choices_to_to_formfield(formfield):\n    \"\"\"Apply limit_choices_to to the formfield's queryset if needed.\"\"\"\n    if hasattr(formfield, 'queryset') and hasattr(formfield, 'get_limit_choices_to'):\n        limit_choices_to = formfield.get_limit_choices_to()\n        if limit_choices_to is not None:\n            formfield.queryset = formfield.queryset.complex_filter(limit_choices_to)\n\n\ndef fields_for_model(model, fields=None, exclude=None, widgets=None,\n                     formfield_callback=None, localized_fields=None,\n                     labels=None, help_texts=None, error_messages=None,\n                     field_classes=None, *, apply_limit_choices_to=True):", "answer": "B"}
{"uuid": "c4644346-e220-4b7c-b414-9eac318a897e", "issue_statement": "Decoding an invalid session data crashes.\nDescription\n\t \n\t\t(last modified by Matt Hegarty)\n\t \nHi\nI recently upgraded my staging server to 3.1. I think that there was an old session which was still active.\nOn browsing to any URL, I get the crash below. It looks similar to \u200bthis issue.\nI cannot login at all with Chrome - each attempt to access the site results in a crash. Login with Firefox works fine.\nThis is only happening on my Staging site, which is running Gunicorn behind nginx proxy.\nInternal Server Error: /overview/\nTraceback (most recent call last):\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/sessions/backends/base.py\", line 215, in _get_session\nreturn self._session_cache\nAttributeError: 'SessionStore' object has no attribute '_session_cache'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/sessions/backends/base.py\", line 118, in decode\nreturn signing.loads(session_data, salt=self.key_salt, serializer=self.serializer)\nFile \"/usr/local/lib/python3.8/site-packages/django/core/signing.py\", line 135, in loads\nbase64d = TimestampSigner(key, salt=salt).unsign(s, max_age=max_age).encode()\nFile \"/usr/local/lib/python3.8/site-packages/django/core/signing.py\", line 201, in unsign\nresult = super().unsign(value)\nFile \"/usr/local/lib/python3.8/site-packages/django/core/signing.py\", line 184, in unsign\nraise BadSignature('Signature \"%s\" does not match' % sig)\ndjango.core.signing.BadSignature: Signature \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" does not match\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \"/usr/local/lib/python3.8/site-packages/django/core/handlers/exception.py\", line 47, in inner\nresponse = get_response(request)\nFile \"/usr/local/lib/python3.8/site-packages/django/core/handlers/base.py\", line 179, in _get_response\nresponse = wrapped_callback(request, *callback_args, **callback_kwargs)\nFile \"/usr/local/lib/python3.8/site-packages/django/views/generic/base.py\", line 73, in view\nreturn self.dispatch(request, *args, **kwargs)\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/auth/mixins.py\", line 50, in dispatch\nif not request.user.is_authenticated:\nFile \"/usr/local/lib/python3.8/site-packages/django/utils/functional.py\", line 240, in inner\nself._setup()\nFile \"/usr/local/lib/python3.8/site-packages/django/utils/functional.py\", line 376, in _setup\nself._wrapped = self._setupfunc()\nFile \"/usr/local/lib/python3.8/site-packages/django_otp/middleware.py\", line 38, in _verify_user\nuser.otp_device = None\nFile \"/usr/local/lib/python3.8/site-packages/django/utils/functional.py\", line 270, in __setattr__\nself._setup()\nFile \"/usr/local/lib/python3.8/site-packages/django/utils/functional.py\", line 376, in _setup\nself._wrapped = self._setupfunc()\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/auth/middleware.py\", line 23, in <lambda>\nrequest.user = SimpleLazyObject(lambda: get_user(request))\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/auth/middleware.py\", line 11, in get_user\nrequest._cached_user = auth.get_user(request)\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/auth/__init__.py\", line 174, in get_user\nuser_id = _get_user_session_key(request)\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/auth/__init__.py\", line 58, in _get_user_session_key\nreturn get_user_model()._meta.pk.to_python(request.session[SESSION_KEY])\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/sessions/backends/base.py\", line 65, in __getitem__\nreturn self._session[key]\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/sessions/backends/base.py\", line 220, in _get_session\nself._session_cache = self.load()\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/sessions/backends/db.py\", line 44, in load\nreturn self.decode(s.session_data) if s else {}\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/sessions/backends/base.py\", line 122, in decode\nreturn self._legacy_decode(session_data)\nFile \"/usr/local/lib/python3.8/site-packages/django/contrib/sessions/backends/base.py\", line 126, in _legacy_decode\nencoded_data = base64.b64decode(session_data.encode('ascii'))\nFile \"/usr/local/lib/python3.8/base64.py\", line 87, in b64decode\nreturn binascii.a2b_base64(s)\nbinascii.Error: Incorrect padding", "choices": "(A)         )\n\n    def decode(self, session_data):\n        try:\n            return signing.loads(session_data, salt=self.key_salt, serializer=self.serializer)\n        # RemovedInDjango40Warning: when the deprecation ends, handle here\n        # exceptions similar to what _legacy_decode() does now.\n        except Exception:\n            return self._legacy_decode(session_data)\n\n    def _legacy_encode(self, session_dict):\n        # RemovedInDjango40Warning.\n        serialized = self.serializer().dumps(session_dict)\n        hash = self._hash(serialized)\n        return base64.b64encode(hash.encode() + b':' + serialized).decode('ascii')\n(B)             return signing.loads(session_data, salt=self.key_salt, serializer=self.serializer)\n        # RemovedInDjango40Warning: when the deprecation ends, handle here\n        # exceptions similar to what _legacy_decode() does now.\n        except Exception:\n            return self._legacy_decode(session_data)\n\n    def _legacy_encode(self, session_dict):\n        # RemovedInDjango40Warning.\n        serialized = self.serializer().dumps(session_dict)\n        hash = self._hash(serialized)\n        return base64.b64encode(hash.encode() + b':' + serialized).decode('ascii')\n\n    def _legacy_decode(self, session_data):\n        # RemovedInDjango40Warning: pre-Django 3.1 format will be invalid.\n        encoded_data = base64.b64decode(session_data.encode('ascii'))\n(C)         serialized = self.serializer().dumps(session_dict)\n        hash = self._hash(serialized)\n        return base64.b64encode(hash.encode() + b':' + serialized).decode('ascii')\n\n    def _legacy_decode(self, session_data):\n        # RemovedInDjango40Warning: pre-Django 3.1 format will be invalid.\n        encoded_data = base64.b64decode(session_data.encode('ascii'))\n        try:\n            # could produce ValueError if there is no ':'\n            hash, serialized = encoded_data.split(b':', 1)\n            expected_hash = self._hash(serialized)\n            if not constant_time_compare(hash.decode(), expected_hash):\n                raise SuspiciousSession(\"Session data corrupted\")\n            else:\n                return self.serializer().loads(serialized)\n(D)             return self._legacy_decode(session_data)\n\n    def _legacy_encode(self, session_dict):\n        # RemovedInDjango40Warning.\n        serialized = self.serializer().dumps(session_dict)\n        hash = self._hash(serialized)\n        return base64.b64encode(hash.encode() + b':' + serialized).decode('ascii')\n\n    def _legacy_decode(self, session_data):\n        # RemovedInDjango40Warning: pre-Django 3.1 format will be invalid.\n        encoded_data = base64.b64decode(session_data.encode('ascii'))\n        try:\n            # could produce ValueError if there is no ':'\n            hash, serialized = encoded_data.split(b':', 1)\n            expected_hash = self._hash(serialized)", "answer": "B"}
{"uuid": "877891aa-c39a-4380-b95e-33eea61d31e8", "issue_statement": "Abstract model field should not be equal across models\nDescription\n\t\nConsider the following models:\nclass A(models.Model):\n\tclass Meta:\n\t\tabstract = True\n\tmyfield = IntegerField()\nclass B(A):\n\tpass\nclass C(A):\n\tpass\nIf I pull the fields of B and C into a shared set, one will be de-duplicated away, because they compare as equal. I found this surprising, though in practice using a list was sufficient for my need. The root of the issue is that they compare equal, as fields only consider self.creation_counter when comparing for equality.\nlen({B._meta.get_field('myfield'), C._meta.get_field('myfield')}) == 1\nB._meta.get_field('myfield') == C._meta.get_field('myfield')\nWe should adjust __eq__ so that if the field.model is different, they will compare unequal. Similarly, it is probably wise to adjust __hash__ and __lt__ to match.\nWhen adjusting __lt__, it may be wise to order first by self.creation_counter so that cases not affected by this equality collision won't be re-ordered. In my experimental branch, there was one test that broke if I ordered them by model first.\nI brought this up on IRC django-dev to check my intuitions, and those conversing with me there seemed to agree that the current behavior is not intuitive.", "choices": "(A)     def __eq__(self, other):\n        # Needed for @total_ordering\n        if isinstance(other, Field):\n            return self.creation_counter == other.creation_counter\n        return NotImplemented\n\n    def __lt__(self, other):\n        # This is needed because bisect does not take a comparison function.\n        if isinstance(other, Field):\n            return self.creation_counter < other.creation_counter\n        return NotImplemented\n\n    def __hash__(self):\n        return hash(self.creation_counter)\n\n    def __deepcopy__(self, memodict):\n        # We don't have to deepcopy very much here, since most things are not\n        # intended to be altered after initial creation.\n        obj = copy.copy(self)\n        if self.remote_field:\n            obj.remote_field = copy.copy(self.remote_field)\n            if hasattr(self.remote_field, 'field') and self.remote_field.field is self:\n                obj.remote_field.field = obj\n        memodict[id(self)] = obj\n        return obj\n\n    def __copy__(self):\n        # We need to avoid hitting __reduce__, so define this\n        # slightly weird copy construct.\n        obj = Empty()\n        obj.__class__ = self.__class__\n        obj.__dict__ = self.__dict__.copy()\n        return obj\n\n    def __reduce__(self):\n        \"\"\"\n        Pickling should return the model._meta.fields instance of the field,\n(B)         return NotImplemented\n\n    def __hash__(self):\n        return hash(self.creation_counter)\n\n    def __deepcopy__(self, memodict):\n        # We don't have to deepcopy very much here, since most things are not\n        # intended to be altered after initial creation.\n        obj = copy.copy(self)\n        if self.remote_field:\n            obj.remote_field = copy.copy(self.remote_field)\n            if hasattr(self.remote_field, 'field') and self.remote_field.field is self:\n                obj.remote_field.field = obj\n        memodict[id(self)] = obj\n        return obj\n\n    def __copy__(self):\n        # We need to avoid hitting __reduce__, so define this\n        # slightly weird copy construct.\n        obj = Empty()\n        obj.__class__ = self.__class__\n        obj.__dict__ = self.__dict__.copy()\n        return obj\n\n    def __reduce__(self):\n        \"\"\"\n        Pickling should return the model._meta.fields instance of the field,\n        not a new copy of that field. So, use the app registry to load the\n        model and then the field back.\n        \"\"\"\n        if not hasattr(self, 'model'):\n            # Fields are sometimes used without attaching them to models (for\n            # example in aggregation). In this case give back a plain field\n            # instance. The code below will create a new empty instance of\n            # class self.__class__, then update its dict with self.__dict__\n            # values - so, this is very close to normal pickle.\n            state = self.__dict__.copy()\n(C)         return (self.name, path, [], keywords)\n\n    def clone(self):\n        \"\"\"\n        Uses deconstruct() to clone a new copy of this Field.\n        Will not preserve any class attachments/attribute names.\n        \"\"\"\n        name, path, args, kwargs = self.deconstruct()\n        return self.__class__(*args, **kwargs)\n\n    def __eq__(self, other):\n        # Needed for @total_ordering\n        if isinstance(other, Field):\n            return self.creation_counter == other.creation_counter\n        return NotImplemented\n\n    def __lt__(self, other):\n        # This is needed because bisect does not take a comparison function.\n        if isinstance(other, Field):\n            return self.creation_counter < other.creation_counter\n        return NotImplemented\n\n    def __hash__(self):\n        return hash(self.creation_counter)\n\n    def __deepcopy__(self, memodict):\n        # We don't have to deepcopy very much here, since most things are not\n        # intended to be altered after initial creation.\n        obj = copy.copy(self)\n        if self.remote_field:\n            obj.remote_field = copy.copy(self.remote_field)\n            if hasattr(self.remote_field, 'field') and self.remote_field.field is self:\n                obj.remote_field.field = obj\n        memodict[id(self)] = obj\n        return obj\n\n    def __copy__(self):\n(D)         if self.remote_field:\n            obj.remote_field = copy.copy(self.remote_field)\n            if hasattr(self.remote_field, 'field') and self.remote_field.field is self:\n                obj.remote_field.field = obj\n        memodict[id(self)] = obj\n        return obj\n\n    def __copy__(self):\n        # We need to avoid hitting __reduce__, so define this\n        # slightly weird copy construct.\n        obj = Empty()\n        obj.__class__ = self.__class__\n        obj.__dict__ = self.__dict__.copy()\n        return obj\n\n    def __reduce__(self):\n        \"\"\"\n        Pickling should return the model._meta.fields instance of the field,\n        not a new copy of that field. So, use the app registry to load the\n        model and then the field back.\n        \"\"\"\n        if not hasattr(self, 'model'):\n            # Fields are sometimes used without attaching them to models (for\n            # example in aggregation). In this case give back a plain field\n            # instance. The code below will create a new empty instance of\n            # class self.__class__, then update its dict with self.__dict__\n            # values - so, this is very close to normal pickle.\n            state = self.__dict__.copy()\n            # The _get_default cached_property can't be pickled due to lambda\n            # usage.\n            state.pop('_get_default', None)\n            return _empty, (self.__class__,), state\n        return _load_field, (self.model._meta.app_label, self.model._meta.object_name,\n                             self.name)\n\n    def get_pk_value_on_save(self, instance):\n        \"\"\"", "answer": "A"}
{"uuid": "1b1aa54b-e3de-4905-90b4-4ae7e09c182b", "issue_statement": "Added model class to app_list context\nDescription\n\t \n\t\t(last modified by Raffaele Salmaso)\n\t \nI need to manipulate the app_list in my custom admin view, and the easiest way to get the result is to have access to the model class (currently the dictionary is a serialized model).\nIn addition I would make the _build_app_dict method public, as it is used by the two views index and app_index.", "choices": "(A)             if True not in perms.values():\n                continue\n\n            info = (app_label, model._meta.model_name)\n            model_dict = {\n                'name': capfirst(model._meta.verbose_name_plural),\n                'object_name': model._meta.object_name,\n(B)             model_dict = {\n                'name': capfirst(model._meta.verbose_name_plural),\n                'object_name': model._meta.object_name,\n                'perms': perms,\n                'admin_url': None,\n                'add_url': None,\n            }\n(C) \n            info = (app_label, model._meta.model_name)\n            model_dict = {\n                'name': capfirst(model._meta.verbose_name_plural),\n                'object_name': model._meta.object_name,\n                'perms': perms,\n                'admin_url': None,\n(D)                 'object_name': model._meta.object_name,\n                'perms': perms,\n                'admin_url': None,\n                'add_url': None,\n            }\n            if perms.get('change') or perms.get('view'):\n                model_dict['view_only'] = not perms.get('change')", "answer": "C"}
{"uuid": "b18a02f7-61ed-4458-96b5-6d13e71516e7", "issue_statement": "Test runner setup_databases crashes with \"TEST\": {\"MIGRATE\": False}.\nDescription\n\t\nI'm trying to upgrade a project from Django 3.0 to Django 3.1 and wanted to try out the new \"TEST\": {\"MIGRATE\": False} database setting.\nSadly I'm running into an issue immediately when running ./manage.py test.\nRemoving the \"TEST\": {\"MIGRATE\": False} line allows the tests to run. So this is not blocking the upgrade for us, but it would be nice if we were able to use the new feature to skip migrations during testing.\nFor reference, this project was recently upgraded from Django 1.4 all the way to 3.0 so there might be some legacy cruft somewhere that triggers this.\nHere's the trackeback. I'll try to debug this some more.\nTraceback (most recent call last):\n File \"/usr/local/lib/python3.6/site-packages/django/db/backends/utils.py\", line 84, in _execute\n\treturn self.cursor.execute(sql, params)\npsycopg2.errors.UndefinedTable: relation \"django_admin_log\" does not exist\nLINE 1: ...n_flag\", \"django_admin_log\".\"change_message\" FROM \"django_ad...\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t ^\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n File \"/usr/local/lib/python3.6/site-packages/django/db/models/sql/compiler.py\", line 1156, in execute_sql\n\tcursor.execute(sql, params)\n File \"/usr/local/lib/python3.6/site-packages/django/db/backends/utils.py\", line 66, in execute\n\treturn self._execute_with_wrappers(sql, params, many=False, executor=self._execute)\n File \"/usr/local/lib/python3.6/site-packages/django/db/backends/utils.py\", line 75, in _execute_with_wrappers\n\treturn executor(sql, params, many, context)\n File \"/usr/local/lib/python3.6/site-packages/django/db/backends/utils.py\", line 84, in _execute\n\treturn self.cursor.execute(sql, params)\n File \"/usr/local/lib/python3.6/site-packages/django/db/utils.py\", line 90, in __exit__\n\traise dj_exc_value.with_traceback(traceback) from exc_value\n File \"/usr/local/lib/python3.6/site-packages/django/db/backends/utils.py\", line 84, in _execute\n\treturn self.cursor.execute(sql, params)\ndjango.db.utils.ProgrammingError: relation \"django_admin_log\" does not exist\nLINE 1: ...n_flag\", \"django_admin_log\".\"change_message\" FROM \"django_ad...\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t ^\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n File \"./manage.py\", line 15, in <module>\n\tmain()\n File \"./manage.py\", line 11, in main\n\texecute_from_command_line(sys.argv)\n File \"/usr/local/lib/python3.6/site-packages/django/core/management/__init__.py\", line 401, in execute_from_command_line\n\tutility.execute()\n File \"/usr/local/lib/python3.6/site-packages/django/core/management/__init__.py\", line 395, in execute\n\tself.fetch_command(subcommand).run_from_argv(self.argv)\n File \"/usr/local/lib/python3.6/site-packages/django/core/management/commands/test.py\", line 23, in run_from_argv\n\tsuper().run_from_argv(argv)\n File \"/usr/local/lib/python3.6/site-packages/django/core/management/base.py\", line 330, in run_from_argv\n\tself.execute(*args, **cmd_options)\n File \"/usr/local/lib/python3.6/site-packages/django/core/management/base.py\", line 371, in execute\n\toutput = self.handle(*args, **options)\n File \"/usr/local/lib/python3.6/site-packages/django/core/management/commands/test.py\", line 53, in handle\n\tfailures = test_runner.run_tests(test_labels)\n File \"/usr/local/lib/python3.6/site-packages/django/test/runner.py\", line 695, in run_tests\n\told_config = self.setup_databases(aliases=databases)\n File \"/usr/local/lib/python3.6/site-packages/django/test/runner.py\", line 616, in setup_databases\n\tself.parallel, **kwargs\n File \"/usr/local/lib/python3.6/site-packages/django/test/utils.py\", line 174, in setup_databases\n\tserialize=connection.settings_dict['TEST'].get('SERIALIZE', True),\n File \"/usr/local/lib/python3.6/site-packages/django/db/backends/base/creation.py\", line 78, in create_test_db\n\tself.connection._test_serialized_contents = self.serialize_db_to_string()\n File \"/usr/local/lib/python3.6/site-packages/django/db/backends/base/creation.py\", line 121, in serialize_db_to_string\n\tserializers.serialize(\"json\", get_objects(), indent=None, stream=out)\n File \"/usr/local/lib/python3.6/site-packages/django/core/serializers/__init__.py\", line 128, in serialize\n\ts.serialize(queryset, **options)\n File \"/usr/local/lib/python3.6/site-packages/django/core/serializers/base.py\", line 90, in serialize\n\tfor count, obj in enumerate(queryset, start=1):\n File \"/usr/local/lib/python3.6/site-packages/django/db/backends/base/creation.py\", line 118, in get_objects\n\tyield from queryset.iterator()\n File \"/usr/local/lib/python3.6/site-packages/django/db/models/query.py\", line 360, in _iterator\n\tyield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)\n File \"/usr/local/lib/python3.6/site-packages/django/db/models/query.py\", line 53, in __iter__\n\tresults = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)\n File \"/usr/local/lib/python3.6/site-packages/django/db/models/sql/compiler.py\", line 1159, in execute_sql\n\tcursor.close()\npsycopg2.errors.InvalidCursorName: cursor \"_django_curs_139860821038912_sync_1\" does not exist", "choices": "(A)         settings.DATABASES[self.connection.alias][\"NAME\"] = test_database_name\n        self.connection.settings_dict[\"NAME\"] = test_database_name\n\n        if self.connection.settings_dict['TEST']['MIGRATE']:\n            # We report migrate messages at one level lower than that\n            # requested. This ensures we don't get flooded with messages during\n            # testing (unless you really ask to be flooded).\n            call_command(\n                'migrate',\n                verbosity=max(verbosity - 1, 0),\n                interactive=False,\n                database=self.connection.alias,\n                run_syncdb=True,\n            )\n\n        # We then serialize the current state of the database into a string\n        # and store it on the connection. This slightly horrific process is so people\n        # who are testing on databases without transactions or who are using\n        # a TransactionTestCase still get a clean database on every test run.\n        if serialize:\n            self.connection._test_serialized_contents = self.serialize_db_to_string()\n\n        call_command('createcachetable', database=self.connection.alias)\n\n        # Ensure a connection for the side effect of initializing the test database.\n        self.connection.ensure_connection()\n\n(B)         # give it the keepdb param. This is to handle the case\n        # where the test DB doesn't exist, in which case we need to\n        # create it, then just not destroy it. If we instead skip\n        # this, we will get an exception.\n        self._create_test_db(verbosity, autoclobber, keepdb)\n\n        self.connection.close()\n        settings.DATABASES[self.connection.alias][\"NAME\"] = test_database_name\n        self.connection.settings_dict[\"NAME\"] = test_database_name\n\n        if self.connection.settings_dict['TEST']['MIGRATE']:\n            # We report migrate messages at one level lower than that\n            # requested. This ensures we don't get flooded with messages during\n            # testing (unless you really ask to be flooded).\n            call_command(\n                'migrate',\n                verbosity=max(verbosity - 1, 0),\n                interactive=False,\n                database=self.connection.alias,\n                run_syncdb=True,\n            )\n\n        # We then serialize the current state of the database into a string\n        # and store it on the connection. This slightly horrific process is so people\n        # who are testing on databases without transactions or who are using\n        # a TransactionTestCase still get a clean database on every test run.\n        if serialize:\n(C) \n        # We then serialize the current state of the database into a string\n        # and store it on the connection. This slightly horrific process is so people\n        # who are testing on databases without transactions or who are using\n        # a TransactionTestCase still get a clean database on every test run.\n        if serialize:\n            self.connection._test_serialized_contents = self.serialize_db_to_string()\n\n        call_command('createcachetable', database=self.connection.alias)\n\n        # Ensure a connection for the side effect of initializing the test database.\n        self.connection.ensure_connection()\n\n        return test_database_name\n\n    def set_as_test_mirror(self, primary_settings_dict):\n        \"\"\"\n        Set this database up to be used in testing as a mirror of a primary\n        database whose settings are given.\n        \"\"\"\n        self.connection.settings_dict['NAME'] = primary_settings_dict['NAME']\n\n    def serialize_db_to_string(self):\n        \"\"\"\n        Serialize all data in the database into a JSON string.\n        Designed only for test runner usage; will not handle large\n        amounts of data.\n(D)             call_command(\n                'migrate',\n                verbosity=max(verbosity - 1, 0),\n                interactive=False,\n                database=self.connection.alias,\n                run_syncdb=True,\n            )\n\n        # We then serialize the current state of the database into a string\n        # and store it on the connection. This slightly horrific process is so people\n        # who are testing on databases without transactions or who are using\n        # a TransactionTestCase still get a clean database on every test run.\n        if serialize:\n            self.connection._test_serialized_contents = self.serialize_db_to_string()\n\n        call_command('createcachetable', database=self.connection.alias)\n\n        # Ensure a connection for the side effect of initializing the test database.\n        self.connection.ensure_connection()\n\n        return test_database_name\n\n    def set_as_test_mirror(self, primary_settings_dict):\n        \"\"\"\n        Set this database up to be used in testing as a mirror of a primary\n        database whose settings are given.\n        \"\"\"", "answer": "A"}
{"uuid": "6cf6ac45-cbb2-4292-b4a9-bd8f525726f6", "issue_statement": "Changing user's email could invalidate password reset tokens\nDescription\n\t\nSequence:\nHave account with email address foo@\u2026\nPassword reset request for that email (unused)\nfoo@\u2026 account changes their email address\nPassword reset email is used\nThe password reset email's token should be rejected at that point, but in fact it is allowed.\nThe fix is to add the user's email address into \u200bPasswordResetTokenGenerator._make_hash_value()\nNothing forces a user to even have an email as per AbstractBaseUser. Perhaps the token generation method could be factored out onto the model, ala get_session_auth_hash().", "choices": "(A)         if (self._num_seconds(self._now()) - ts) > settings.PASSWORD_RESET_TIMEOUT:\n            return False\n\n        return True\n\n    def _make_token_with_timestamp(self, user, timestamp, legacy=False):\n        # timestamp is number of seconds since 2001-1-1. Converted to base 36,\n        # this gives us a 6 digit string until about 2069.\n        ts_b36 = int_to_base36(timestamp)\n        hash_string = salted_hmac(\n            self.key_salt,\n            self._make_hash_value(user, timestamp),\n            secret=self.secret,\n            # RemovedInDjango40Warning: when the deprecation ends, remove the\n            # legacy argument and replace with:\n            #   algorithm=self.algorithm,\n            algorithm='sha1' if legacy else self.algorithm,\n        ).hexdigest()[::2]  # Limit to shorten the URL.\n        return \"%s-%s\" % (ts_b36, hash_string)\n\n    def _make_hash_value(self, user, timestamp):\n        \"\"\"\n        Hash the user's primary key and some user state that's sure to change\n        after a password reset to produce a token that invalidated when it's\n        used:\n(B)         # timestamp is number of seconds since 2001-1-1. Converted to base 36,\n        # this gives us a 6 digit string until about 2069.\n        ts_b36 = int_to_base36(timestamp)\n        hash_string = salted_hmac(\n            self.key_salt,\n            self._make_hash_value(user, timestamp),\n            secret=self.secret,\n            # RemovedInDjango40Warning: when the deprecation ends, remove the\n            # legacy argument and replace with:\n            #   algorithm=self.algorithm,\n            algorithm='sha1' if legacy else self.algorithm,\n        ).hexdigest()[::2]  # Limit to shorten the URL.\n        return \"%s-%s\" % (ts_b36, hash_string)\n\n    def _make_hash_value(self, user, timestamp):\n        \"\"\"\n        Hash the user's primary key and some user state that's sure to change\n        after a password reset to produce a token that invalidated when it's\n        used:\n        1. The password field will change upon a password reset (even if the\n           same password is chosen, due to password salting).\n        2. The last_login field will usually be updated very shortly after\n           a password reset.\n        Failing those things, settings.PASSWORD_RESET_TIMEOUT eventually\n        invalidates the token.\n(C)             secret=self.secret,\n            # RemovedInDjango40Warning: when the deprecation ends, remove the\n            # legacy argument and replace with:\n            #   algorithm=self.algorithm,\n            algorithm='sha1' if legacy else self.algorithm,\n        ).hexdigest()[::2]  # Limit to shorten the URL.\n        return \"%s-%s\" % (ts_b36, hash_string)\n\n    def _make_hash_value(self, user, timestamp):\n        \"\"\"\n        Hash the user's primary key and some user state that's sure to change\n        after a password reset to produce a token that invalidated when it's\n        used:\n        1. The password field will change upon a password reset (even if the\n           same password is chosen, due to password salting).\n        2. The last_login field will usually be updated very shortly after\n           a password reset.\n        Failing those things, settings.PASSWORD_RESET_TIMEOUT eventually\n        invalidates the token.\n\n        Running this data through salted_hmac() prevents password cracking\n        attempts using the reset token, provided the secret isn't compromised.\n        \"\"\"\n        # Truncate microseconds so that tokens are consistent even if the\n        # database doesn't support microseconds.\n(D) \n    def _make_hash_value(self, user, timestamp):\n        \"\"\"\n        Hash the user's primary key and some user state that's sure to change\n        after a password reset to produce a token that invalidated when it's\n        used:\n        1. The password field will change upon a password reset (even if the\n           same password is chosen, due to password salting).\n        2. The last_login field will usually be updated very shortly after\n           a password reset.\n        Failing those things, settings.PASSWORD_RESET_TIMEOUT eventually\n        invalidates the token.\n\n        Running this data through salted_hmac() prevents password cracking\n        attempts using the reset token, provided the secret isn't compromised.\n        \"\"\"\n        # Truncate microseconds so that tokens are consistent even if the\n        # database doesn't support microseconds.\n        login_timestamp = '' if user.last_login is None else user.last_login.replace(microsecond=0, tzinfo=None)\n        return str(user.pk) + user.password + str(login_timestamp) + str(timestamp)\n\n    def _num_seconds(self, dt):\n        return int((dt - datetime(2001, 1, 1)).total_seconds())\n\n    def _now(self):", "answer": "D"}
{"uuid": "8d844a62-5b0a-402f-aebd-dd6a3cec2004", "issue_statement": "Upgrading 2.2>3.0 causes named tuples used as arguments to __range to error.\nDescription\n\t\nI noticed this while upgrading a project from 2.2 to 3.0.\nThis project passes named 2-tuples as arguments to range queryset filters. This works fine on 2.2. On 3.0 it causes the following error: TypeError: __new__() missing 1 required positional argument: 'far'.\nThis happens because django.db.models.sql.query.Query.resolve_lookup_value goes into the tuple elements to resolve lookups and then attempts to reconstitute the tuple with the resolved elements.\nWhen it attempts to construct the new tuple it preserves the type (the named tuple) but it passes a iterator to it's constructor.\nNamedTuples don't have the code path for copying an iterator, and so it errors on insufficient arguments.\nThe fix is to * expand the contents of the iterator into the constructor.", "choices": "(A)         elif isinstance(value, (list, tuple)):\n            # The items of the iterable may be expressions and therefore need\n            # to be resolved independently.\n            return type(value)(\n                self.resolve_lookup_value(sub_value, can_reuse, allow_joins)\n                for sub_value in value\n            )\n        return value\n\n    def solve_lookup_type(self, lookup):\n        \"\"\"\n        Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').\n        \"\"\"\n        lookup_splitted = lookup.split(LOOKUP_SEP)\n(B)         return value\n\n    def solve_lookup_type(self, lookup):\n        \"\"\"\n        Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').\n        \"\"\"\n        lookup_splitted = lookup.split(LOOKUP_SEP)\n        if self.annotations:\n            expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)\n            if expression:\n                return expression_lookups, (), expression\n        _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())\n        field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]\n        if len(lookup_parts) > 1 and not field_parts:\n(C)         if hasattr(value, 'resolve_expression'):\n            value = value.resolve_expression(\n                self, reuse=can_reuse, allow_joins=allow_joins,\n            )\n        elif isinstance(value, (list, tuple)):\n            # The items of the iterable may be expressions and therefore need\n            # to be resolved independently.\n            return type(value)(\n                self.resolve_lookup_value(sub_value, can_reuse, allow_joins)\n                for sub_value in value\n            )\n        return value\n\n    def solve_lookup_type(self, lookup):\n(D)                 self.resolve_lookup_value(sub_value, can_reuse, allow_joins)\n                for sub_value in value\n            )\n        return value\n\n    def solve_lookup_type(self, lookup):\n        \"\"\"\n        Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').\n        \"\"\"\n        lookup_splitted = lookup.split(LOOKUP_SEP)\n        if self.annotations:\n            expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)\n            if expression:\n                return expression_lookups, (), expression", "answer": "A"}
{"uuid": "292f9acf-23e7-435a-ac29-0667df909be8", "issue_statement": "ManagementUtility instantiates CommandParser without passing already-computed prog argument\nDescription\n\t\nManagementUtility \u200bgoes to the trouble to parse the program name from the argv it's passed rather than from sys.argv: \n\tdef __init__(self, argv=None):\n\t\tself.argv = argv or sys.argv[:]\n\t\tself.prog_name = os.path.basename(self.argv[0])\n\t\tif self.prog_name == '__main__.py':\n\t\t\tself.prog_name = 'python -m django'\nBut then when it needs to parse --pythonpath and --settings, it \u200buses the program name from sys.argv: \n\t\tparser = CommandParser(usage='%(prog)s subcommand [options] [args]', add_help=False, allow_abbrev=False)\nAbove \"%(prog)s\" \u200brefers to sys.argv[0]. Instead, it should refer to self.prog_name. This can fixed as follows:\n\t\tparser = CommandParser(\n\t\t\tprog=self.prog_name,\n\t\t\tusage='%(prog)s subcommand [options] [args]',\n\t\t\tadd_help=False,\n\t\t\tallow_abbrev=False)\nI'm aware that execute_from_command_line is a private API, but it'd be really convenient for me if it worked properly in my weird embedded environment where sys.argv[0] is \u200bincorrectly None. If passing my own argv to execute_from_command_line avoided all the ensuing exceptions, I wouldn't have to modify sys.argv[0] globally as I'm doing in the meantime.", "choices": "(A)         parser.add_argument('args', nargs='*')  # catch-all\n        try:\n            options, args = parser.parse_known_args(self.argv[2:])\n            handle_default_options(options)\n        except CommandError:\n            pass  # Ignore any option errors at this point.\n\n        try:\n            settings.INSTALLED_APPS\n        except ImproperlyConfigured as exc:\n            self.settings_exception = exc\n        except ImportError as exc:\n(B)         # Preprocess options to extract --settings and --pythonpath.\n        # These options could affect the commands that are available, so they\n        # must be processed early.\n        parser = CommandParser(usage='%(prog)s subcommand [options] [args]', add_help=False, allow_abbrev=False)\n        parser.add_argument('--settings')\n        parser.add_argument('--pythonpath')\n        parser.add_argument('args', nargs='*')  # catch-all\n        try:\n            options, args = parser.parse_known_args(self.argv[2:])\n            handle_default_options(options)\n        except CommandError:\n            pass  # Ignore any option errors at this point.\n(C)         except IndexError:\n            subcommand = 'help'  # Display help if no arguments were given.\n\n        # Preprocess options to extract --settings and --pythonpath.\n        # These options could affect the commands that are available, so they\n        # must be processed early.\n        parser = CommandParser(usage='%(prog)s subcommand [options] [args]', add_help=False, allow_abbrev=False)\n        parser.add_argument('--settings')\n        parser.add_argument('--pythonpath')\n        parser.add_argument('args', nargs='*')  # catch-all\n        try:\n            options, args = parser.parse_known_args(self.argv[2:])\n(D)         parser = CommandParser(usage='%(prog)s subcommand [options] [args]', add_help=False, allow_abbrev=False)\n        parser.add_argument('--settings')\n        parser.add_argument('--pythonpath')\n        parser.add_argument('args', nargs='*')  # catch-all\n        try:\n            options, args = parser.parse_known_args(self.argv[2:])\n            handle_default_options(options)\n        except CommandError:\n            pass  # Ignore any option errors at this point.\n\n        try:\n            settings.INSTALLED_APPS", "answer": "B"}
{"uuid": "a4823785-e6d3-4d40-be8b-6caeeb305c3b", "issue_statement": "shell command crashes when passing (with -c) the python code with functions.\nDescription\n\t\nThe examples below use Python 3.7 and Django 2.2.16, but I checked that the code is the same on master and works the same in Python 3.8.\nHere's how \u200bpython -c works:\n$ python -c <<EOF \" \nimport django\ndef f():\n\t\tprint(django.__version__)\nf()\"\nEOF\n2.2.16\nHere's how \u200bpython -m django shell -c works (paths shortened for clarify):\n$ python -m django shell -c <<EOF \"\nimport django\ndef f():\n\t\tprint(django.__version__)\nf()\"\nEOF\nTraceback (most recent call last):\n File \"{sys.base_prefix}/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n\t\"__main__\", mod_spec)\n File \"{sys.base_prefix}/lib/python3.7/runpy.py\", line 85, in _run_code\n\texec(code, run_globals)\n File \"{sys.prefix}/lib/python3.7/site-packages/django/__main__.py\", line 9, in <module>\n\tmanagement.execute_from_command_line()\n File \"{sys.prefix}/lib/python3.7/site-packages/django/core/management/__init__.py\", line 381, in execute_from_command_line\n\tutility.execute()\n File \"{sys.prefix}/lib/python3.7/site-packages/django/core/management/__init__.py\", line 375, in execute\n\tself.fetch_command(subcommand).run_from_argv(self.argv)\n File \"{sys.prefix}/lib/python3.7/site-packages/django/core/management/base.py\", line 323, in run_from_argv\n\tself.execute(*args, **cmd_options)\n File \"{sys.prefix}/lib/python3.7/site-packages/django/core/management/base.py\", line 364, in execute\n\toutput = self.handle(*args, **options)\n File \"{sys.prefix}/lib/python3.7/site-packages/django/core/management/commands/shell.py\", line 86, in handle\n\texec(options['command'])\n File \"<string>\", line 5, in <module>\n File \"<string>\", line 4, in f\nNameError: name 'django' is not defined\nThe problem is in the \u200busage of \u200bexec:\n\tdef handle(self, **options):\n\t\t# Execute the command and exit.\n\t\tif options['command']:\n\t\t\texec(options['command'])\n\t\t\treturn\n\t\t# Execute stdin if it has anything to read and exit.\n\t\t# Not supported on Windows due to select.select() limitations.\n\t\tif sys.platform != 'win32' and not sys.stdin.isatty() and select.select([sys.stdin], [], [], 0)[0]:\n\t\t\texec(sys.stdin.read())\n\t\t\treturn\nexec should be passed a dictionary containing a minimal set of globals. This can be done by just passing a new, empty dictionary as the second argument of exec.", "choices": "(A)             return\n\n        # Execute stdin if it has anything to read and exit.\n        # Not supported on Windows due to select.select() limitations.\n        if sys.platform != 'win32' and not sys.stdin.isatty() and select.select([sys.stdin], [], [], 0)[0]:\n            exec(sys.stdin.read())\n            return\n\n        available_shells = [options['interface']] if options['interface'] else self.shells\n\n        for shell in available_shells:\n            try:\n                return getattr(self, shell)(options)\n(B)     def handle(self, **options):\n        # Execute the command and exit.\n        if options['command']:\n            exec(options['command'])\n            return\n\n        # Execute stdin if it has anything to read and exit.\n        # Not supported on Windows due to select.select() limitations.\n        if sys.platform != 'win32' and not sys.stdin.isatty() and select.select([sys.stdin], [], [], 0)[0]:\n            exec(sys.stdin.read())\n            return\n\n        available_shells = [options['interface']] if options['interface'] else self.shells\n(C)                     traceback.print_exc()\n\n        code.interact(local=imported_objects)\n\n    def handle(self, **options):\n        # Execute the command and exit.\n        if options['command']:\n            exec(options['command'])\n            return\n\n        # Execute stdin if it has anything to read and exit.\n        # Not supported on Windows due to select.select() limitations.\n        if sys.platform != 'win32' and not sys.stdin.isatty() and select.select([sys.stdin], [], [], 0)[0]:\n(D)         # Not supported on Windows due to select.select() limitations.\n        if sys.platform != 'win32' and not sys.stdin.isatty() and select.select([sys.stdin], [], [], 0)[0]:\n            exec(sys.stdin.read())\n            return\n\n        available_shells = [options['interface']] if options['interface'] else self.shells\n\n        for shell in available_shells:\n            try:\n                return getattr(self, shell)(options)\n            except ImportError:\n                pass\n        raise CommandError(\"Couldn't import {} interface.\".format(shell))", "answer": "B"}
{"uuid": "8c1742c1-b72c-4820-9ef9-7f6bd04c4b8d", "issue_statement": "Use Admin Inline verbose_name as default for Inline verbose_name_plural\nDescription\n\t\nDjango allows specification of a verbose_name and a verbose_name_plural for Inline classes in admin views. However, verbose_name_plural for an Inline is not currently based on a specified verbose_name. Instead, it continues to be based on the model name, or an a verbose_name specified in the model's Meta class. This was confusing to me initially (I didn't understand why I had to specify both name forms for an Inline if I wanted to overrule the default name), and seems inconsistent with the approach for a model's Meta class (which does automatically base the plural form on a specified verbose_name). I propose that verbose_name_plural for an Inline class should by default be based on the verbose_name for an Inline if that is specified.\nI have written a patch to implement this, including tests. Would be happy to submit that.", "choices": "(A) \n    def __init__(self, parent_model, admin_site):\n        self.admin_site = admin_site\n        self.parent_model = parent_model\n        self.opts = self.model._meta\n        self.has_registered_model = admin_site.is_registered(self.model)\n        super().__init__()\n        if self.verbose_name is None:\n            self.verbose_name = self.model._meta.verbose_name\n        if self.verbose_name_plural is None:\n            self.verbose_name_plural = self.model._meta.verbose_name_plural\n\n    @property\n(B)         self.opts = self.model._meta\n        self.has_registered_model = admin_site.is_registered(self.model)\n        super().__init__()\n        if self.verbose_name is None:\n            self.verbose_name = self.model._meta.verbose_name\n        if self.verbose_name_plural is None:\n            self.verbose_name_plural = self.model._meta.verbose_name_plural\n\n    @property\n    def media(self):\n        extra = '' if settings.DEBUG else '.min'\n        js = ['vendor/jquery/jquery%s.js' % extra, 'jquery.init.js', 'inlines.js']\n        if self.filter_vertical or self.filter_horizontal:\n(C) \n    @property\n    def media(self):\n        extra = '' if settings.DEBUG else '.min'\n        js = ['vendor/jquery/jquery%s.js' % extra, 'jquery.init.js', 'inlines.js']\n        if self.filter_vertical or self.filter_horizontal:\n            js.extend(['SelectBox.js', 'SelectFilter2.js'])\n        if self.classes and 'collapse' in self.classes:\n            js.append('collapse.js')\n        return forms.Media(js=['admin/js/%s' % url for url in js])\n\n    def get_extra(self, request, obj=None, **kwargs):\n        \"\"\"Hook for customizing the number of extra inline forms.\"\"\"\n(D)             self.verbose_name = self.model._meta.verbose_name\n        if self.verbose_name_plural is None:\n            self.verbose_name_plural = self.model._meta.verbose_name_plural\n\n    @property\n    def media(self):\n        extra = '' if settings.DEBUG else '.min'\n        js = ['vendor/jquery/jquery%s.js' % extra, 'jquery.init.js', 'inlines.js']\n        if self.filter_vertical or self.filter_horizontal:\n            js.extend(['SelectBox.js', 'SelectFilter2.js'])\n        if self.classes and 'collapse' in self.classes:\n            js.append('collapse.js')\n        return forms.Media(js=['admin/js/%s' % url for url in js])", "answer": "B"}
{"uuid": "a433272b-e0fc-4f88-ac68-414d7fe8fc1b", "issue_statement": "Using __isnull=True on a KeyTransform should not match JSON null on SQLite and Oracle\nDescription\n\t\nThe KeyTransformIsNull lookup borrows the logic from HasKey for isnull=False, which is correct. If isnull=True, the query should only match objects that do not have the key. The query is correct for MariaDB, MySQL, and PostgreSQL. However, on SQLite and Oracle, the query also matches objects that have the key with the value null, which is incorrect.\nTo confirm, edit tests.model_fields.test_jsonfield.TestQuerying.test_isnull_key. For the first assertion, change\n\t\tself.assertSequenceEqual(\n\t\t\tNullableJSONModel.objects.filter(value__a__isnull=True),\n\t\t\tself.objs[:3] + self.objs[5:],\n\t\t)\nto\n\t\tself.assertSequenceEqual(\n\t\t\tNullableJSONModel.objects.filter(value__j__isnull=True),\n\t\t\tself.objs[:4] + self.objs[5:],\n\t\t)\nThe test previously only checks with value__a which could not catch this behavior because the value is not JSON null.", "choices": "(A)     def process_rhs(self, compiler, connection):\n        rhs, rhs_params = super().process_rhs(compiler, connection)\n        if connection.vendor == 'mysql':\n            return 'LOWER(%s)' % rhs, rhs_params\n        return rhs, rhs_params\n\n\nclass KeyTransformIsNull(lookups.IsNull):\n    # key__isnull=False is the same as has_key='key'\n    def as_oracle(self, compiler, connection):\n        if not self.rhs:\n            return HasKey(self.lhs.lhs, self.lhs.key_name).as_oracle(compiler, connection)\n        return super().as_sql(compiler, connection)\n\n    def as_sqlite(self, compiler, connection):\n        if not self.rhs:\n            return HasKey(self.lhs.lhs, self.lhs.key_name).as_sqlite(compiler, connection)\n        return super().as_sql(compiler, connection)\n\n\nclass KeyTransformIn(lookups.In):\n    def resolve_expression_parameter(self, compiler, connection, sql, param):\n        sql, params = super().resolve_expression_parameter(\n            compiler, connection, sql, param,\n        )\n(B) class KeyTransformIsNull(lookups.IsNull):\n    # key__isnull=False is the same as has_key='key'\n    def as_oracle(self, compiler, connection):\n        if not self.rhs:\n            return HasKey(self.lhs.lhs, self.lhs.key_name).as_oracle(compiler, connection)\n        return super().as_sql(compiler, connection)\n\n    def as_sqlite(self, compiler, connection):\n        if not self.rhs:\n            return HasKey(self.lhs.lhs, self.lhs.key_name).as_sqlite(compiler, connection)\n        return super().as_sql(compiler, connection)\n\n\nclass KeyTransformIn(lookups.In):\n    def resolve_expression_parameter(self, compiler, connection, sql, param):\n        sql, params = super().resolve_expression_parameter(\n            compiler, connection, sql, param,\n        )\n        if (\n            not hasattr(param, 'as_sql') and\n            not connection.features.has_native_json_field\n        ):\n            if connection.vendor == 'oracle':\n                value = json.loads(param)\n                sql = \"%s(JSON_OBJECT('value' VALUE %%s FORMAT JSON), '$.value')\"\n(C) class KeyTransformIn(lookups.In):\n    def resolve_expression_parameter(self, compiler, connection, sql, param):\n        sql, params = super().resolve_expression_parameter(\n            compiler, connection, sql, param,\n        )\n        if (\n            not hasattr(param, 'as_sql') and\n            not connection.features.has_native_json_field\n        ):\n            if connection.vendor == 'oracle':\n                value = json.loads(param)\n                sql = \"%s(JSON_OBJECT('value' VALUE %%s FORMAT JSON), '$.value')\"\n                if isinstance(value, (list, dict)):\n                    sql = sql % 'JSON_QUERY'\n                else:\n                    sql = sql % 'JSON_VALUE'\n            elif connection.vendor in {'sqlite', 'mysql'}:\n                sql = \"JSON_EXTRACT(%s, '$')\"\n        if connection.vendor == 'mysql' and connection.mysql_is_mariadb:\n            sql = 'JSON_UNQUOTE(%s)' % sql\n        return sql, params\n\n\nclass KeyTransformExact(JSONExact):\n    def process_lhs(self, compiler, connection):\n(D)     def as_sqlite(self, compiler, connection):\n        if not self.rhs:\n            return HasKey(self.lhs.lhs, self.lhs.key_name).as_sqlite(compiler, connection)\n        return super().as_sql(compiler, connection)\n\n\nclass KeyTransformIn(lookups.In):\n    def resolve_expression_parameter(self, compiler, connection, sql, param):\n        sql, params = super().resolve_expression_parameter(\n            compiler, connection, sql, param,\n        )\n        if (\n            not hasattr(param, 'as_sql') and\n            not connection.features.has_native_json_field\n        ):\n            if connection.vendor == 'oracle':\n                value = json.loads(param)\n                sql = \"%s(JSON_OBJECT('value' VALUE %%s FORMAT JSON), '$.value')\"\n                if isinstance(value, (list, dict)):\n                    sql = sql % 'JSON_QUERY'\n                else:\n                    sql = sql % 'JSON_VALUE'\n            elif connection.vendor in {'sqlite', 'mysql'}:\n                sql = \"JSON_EXTRACT(%s, '$')\"\n        if connection.vendor == 'mysql' and connection.mysql_is_mariadb:", "answer": "B"}
{"uuid": "80d1e5d1-4ac3-48c5-b499-5eb77b6b8c52", "issue_statement": "Log exceptions handled in Signal.send_robust()\nDescription\n\t\nAs pointed out by \u200bHaki Benita on Twitter, by default Signal.send_robust() doesn't have any log messages for exceptions raised in receivers. Since Django logs exceptions in other similar situations, such as missing template variables, I think it would be worth adding a logger.exception() call in the except clause of send_robust() . Users would then see such exceptions in their error handling tools, e.g. Sentry, and be able to figure out what action to take from there. Ultimately any *expected* exception should be caught with a try in the receiver function.", "choices": "(A)         Arguments:\n\n            receiver\n                A function or an instance method which is to receive signals.\n                Receivers must be hashable objects.\n\n                If weak is True, then receiver must be weak referenceable.\n\n                Receivers must be able to accept keyword arguments.\n\n                If a receiver is connected with a dispatch_uid argument, it\n                will not be added if another receiver was already connected\n                with that dispatch_uid.\n\n            sender\n                The sender to which the receiver should respond. Must either be\n                a Python object, or None to receive events from any sender.\n\n            weak\n                Whether to use weak references to the receiver. By default, the\n                module will attempt to use weak references to the receiver\n                objects. If this parameter is false, then strong references will\n                be used.\n\n            dispatch_uid\n                An identifier used to uniquely identify a particular instance of\n                a receiver. This will usually be a string, though it may be\n                anything hashable.\n        \"\"\"\n        from django.conf import settings\n\n        # If DEBUG is on, check that we got a good receiver\n        if settings.configured and settings.DEBUG:\n            assert callable(receiver), \"Signal receivers must be callable.\"\n\n            # Check for **kwargs\n            if not func_accepts_kwargs(receiver):\n                raise ValueError(\"Signal receivers must accept keyword arguments (**kwargs).\")\n\n        if dispatch_uid:\n            lookup_key = (dispatch_uid, _make_id(sender))\n        else:\n            lookup_key = (_make_id(receiver), _make_id(sender))\n\n        if weak:\n            ref = weakref.ref\n            receiver_object = receiver\n            # Check for bound methods\n            if hasattr(receiver, '__self__') and hasattr(receiver, '__func__'):\n                ref = weakref.WeakMethod\n                receiver_object = receiver.__self__\n            receiver = ref(receiver)\n            weakref.finalize(receiver_object, self._remove_receiver)\n\n        with self.lock:\n            self._clear_dead_receivers()\n            if not any(r_key == lookup_key for r_key, _ in self.receivers):\n                self.receivers.append((lookup_key, receiver))\n            self.sender_receivers_cache.clear()\n\n    def disconnect(self, receiver=None, sender=None, dispatch_uid=None):\n        \"\"\"\n        Disconnect receiver from sender for signal.\n\n        If weak references are used, disconnect need not be called. The receiver\n        will be removed from dispatch automatically.\n\n        Arguments:\n\n            receiver\n                The registered receiver to disconnect. May be none if\n                dispatch_uid is specified.\n\n            sender\n                The registered sender to disconnect\n\n            dispatch_uid\n                the unique identifier of the receiver to disconnect\n        \"\"\"\n        if dispatch_uid:\n            lookup_key = (dispatch_uid, _make_id(sender))\n        else:\n            lookup_key = (_make_id(receiver), _make_id(sender))\n\n        disconnected = False\n        with self.lock:\n            self._clear_dead_receivers()\n            for index in range(len(self.receivers)):\n                (r_key, _) = self.receivers[index]\n                if r_key == lookup_key:\n                    disconnected = True\n                    del self.receivers[index]\n                    break\n            self.sender_receivers_cache.clear()\n        return disconnected\n\n    def has_listeners(self, sender=None):\n        return bool(self._live_receivers(sender))\n\n    def send(self, sender, **named):\n        \"\"\"\n        Send signal from sender to all connected receivers.\n\n        If any receiver raises an error, the error propagates back through send,\n        terminating the dispatch loop. So it's possible that all receivers\n        won't be called if an error is raised.\n\n        Arguments:\n\n            sender\n                The sender of the signal. Either a specific object or None.\n\n            named\n                Named arguments which will be passed to receivers.\n\n        Return a list of tuple pairs [(receiver, response), ... ].\n        \"\"\"\n        if not self.receivers or self.sender_receivers_cache.get(sender) is NO_RECEIVERS:\n            return []\n\n        return [\n            (receiver, receiver(signal=self, sender=sender, **named))\n            for receiver in self._live_receivers(sender)\n        ]\n\n    def send_robust(self, sender, **named):\n        \"\"\"\n        Send signal from sender to all connected receivers catching errors.\n\n        Arguments:\n\n            sender\n                The sender of the signal. Can be any Python object (normally one\n                registered with a connect if you actually want something to\n                occur).\n\n            named\n                Named arguments which will be passed to receivers.\n\n        Return a list of tuple pairs [(receiver, response), ... ].\n\n        If any receiver raises an error (specifically any subclass of\n        Exception), return the error instance as the result for that receiver.\n        \"\"\"\n        if not self.receivers or self.sender_receivers_cache.get(sender) is NO_RECEIVERS:\n            return []\n\n        # Call each receiver with whatever arguments it can accept.\n        # Return a list of tuple pairs [(receiver, response), ... ].\n        responses = []\n        for receiver in self._live_receivers(sender):\n            try:\n                response = receiver(signal=self, sender=sender, **named)\n            except Exception as err:\n                responses.append((receiver, err))\n            else:\n                responses.append((receiver, response))\n        return responses\n\n    def _clear_dead_receivers(self):\n        # Note: caller is assumed to hold self.lock.\n        if self._dead_receivers:\n            self._dead_receivers = False\n            self.receivers = [\n                r for r in self.receivers\n                if not(isinstance(r[1], weakref.ReferenceType) and r[1]() is None)\n            ]\n\n    def _live_receivers(self, sender):\n        \"\"\"\n        Filter sequence of receivers to get resolved, live receivers.\n\n        This checks for weak references and resolves them, then returning only\n        live receivers.\n        \"\"\"\n        receivers = None\n        if self.use_caching and not self._dead_receivers:\n            receivers = self.sender_receivers_cache.get(sender)\n            # We could end up here with NO_RECEIVERS even if we do check this case in\n            # .send() prior to calling _live_receivers() due to concurrent .send() call.\n            if receivers is NO_RECEIVERS:\n                return []\n        if receivers is None:\n            with self.lock:\n                self._clear_dead_receivers()\n                senderkey = _make_id(sender)\n                receivers = []\n                for (receiverkey, r_senderkey), receiver in self.receivers:\n                    if r_senderkey == NONE_ID or r_senderkey == senderkey:\n                        receivers.append(receiver)\n                if self.use_caching:\n                    if not receivers:\n                        self.sender_receivers_cache[sender] = NO_RECEIVERS\n                    else:\n                        # Note, we must cache the weakref versions.\n                        self.sender_receivers_cache[sender] = receivers\n        non_weak_receivers = []\n        for receiver in receivers:\n            if isinstance(receiver, weakref.ReferenceType):\n                # Dereference the weak reference.\n                receiver = receiver()\n                if receiver is not None:\n                    non_weak_receivers.append(receiver)\n            else:\n                non_weak_receivers.append(receiver)\n        return non_weak_receivers\n\n    def _remove_receiver(self, receiver=None):\n        # Mark that the self.receivers list has dead weakrefs. If so, we will\n        # clean those up in connect, disconnect and _live_receivers while\n        # holding self.lock. Note that doing the cleanup here isn't a good\n        # idea, _remove_receiver() will be called as side effect of garbage\n        # collection, and so the call can happen while we are already holding\n        # self.lock.\n        self._dead_receivers = True\n\n\ndef receiver(signal, **kwargs):\n    \"\"\"\n    A decorator for connecting receivers to signals. Used by passing in the\n    signal (or list of signals) and keyword arguments to connect::\n\n(B) import threading\nimport warnings\nimport weakref\n\nfrom django.utils.deprecation import RemovedInDjango40Warning\nfrom django.utils.inspect import func_accepts_kwargs\n\n\ndef _make_id(target):\n    if hasattr(target, '__func__'):\n        return (id(target.__self__), id(target.__func__))\n    return id(target)\n\n\nNONE_ID = _make_id(None)\n\n# A marker for caching\nNO_RECEIVERS = object()\n\n\nclass Signal:\n    \"\"\"\n    Base class for all signals\n\n    Internal attributes:\n\n        receivers\n            { receiverkey (id) : weakref(receiver) }\n    \"\"\"\n    def __init__(self, providing_args=None, use_caching=False):\n        \"\"\"\n        Create a new signal.\n        \"\"\"\n        self.receivers = []\n        if providing_args is not None:\n            warnings.warn(\n                'The providing_args argument is deprecated. As it is purely '\n                'documentational, it has no replacement. If you rely on this '\n                'argument as documentation, you can move the text to a code '\n                'comment or docstring.',\n                RemovedInDjango40Warning, stacklevel=2,\n            )\n        self.lock = threading.Lock()\n        self.use_caching = use_caching\n        # For convenience we create empty caches even if they are not used.\n        # A note about caching: if use_caching is defined, then for each\n        # distinct sender we cache the receivers that sender has in\n        # 'sender_receivers_cache'. The cache is cleaned when .connect() or\n        # .disconnect() is called and populated on send().\n        self.sender_receivers_cache = weakref.WeakKeyDictionary() if use_caching else {}\n        self._dead_receivers = False\n\n    def connect(self, receiver, sender=None, weak=True, dispatch_uid=None):\n        \"\"\"\n        Connect receiver to sender for signal.\n\n        Arguments:\n\n            receiver\n                A function or an instance method which is to receive signals.\n                Receivers must be hashable objects.\n\n                If weak is True, then receiver must be weak referenceable.\n\n                Receivers must be able to accept keyword arguments.\n\n                If a receiver is connected with a dispatch_uid argument, it\n                will not be added if another receiver was already connected\n                with that dispatch_uid.\n\n            sender\n                The sender to which the receiver should respond. Must either be\n                a Python object, or None to receive events from any sender.\n\n            weak\n                Whether to use weak references to the receiver. By default, the\n                module will attempt to use weak references to the receiver\n                objects. If this parameter is false, then strong references will\n                be used.\n\n            dispatch_uid\n                An identifier used to uniquely identify a particular instance of\n                a receiver. This will usually be a string, though it may be\n                anything hashable.\n        \"\"\"\n        from django.conf import settings\n\n        # If DEBUG is on, check that we got a good receiver\n        if settings.configured and settings.DEBUG:\n            assert callable(receiver), \"Signal receivers must be callable.\"\n\n            # Check for **kwargs\n            if not func_accepts_kwargs(receiver):\n                raise ValueError(\"Signal receivers must accept keyword arguments (**kwargs).\")\n\n        if dispatch_uid:\n            lookup_key = (dispatch_uid, _make_id(sender))\n        else:\n            lookup_key = (_make_id(receiver), _make_id(sender))\n\n        if weak:\n            ref = weakref.ref\n            receiver_object = receiver\n            # Check for bound methods\n            if hasattr(receiver, '__self__') and hasattr(receiver, '__func__'):\n                ref = weakref.WeakMethod\n                receiver_object = receiver.__self__\n            receiver = ref(receiver)\n            weakref.finalize(receiver_object, self._remove_receiver)\n\n        with self.lock:\n            self._clear_dead_receivers()\n            if not any(r_key == lookup_key for r_key, _ in self.receivers):\n                self.receivers.append((lookup_key, receiver))\n            self.sender_receivers_cache.clear()\n\n    def disconnect(self, receiver=None, sender=None, dispatch_uid=None):\n        \"\"\"\n        Disconnect receiver from sender for signal.\n\n        If weak references are used, disconnect need not be called. The receiver\n        will be removed from dispatch automatically.\n\n        Arguments:\n\n            receiver\n                The registered receiver to disconnect. May be none if\n                dispatch_uid is specified.\n\n            sender\n                The registered sender to disconnect\n\n            dispatch_uid\n                the unique identifier of the receiver to disconnect\n        \"\"\"\n        if dispatch_uid:\n            lookup_key = (dispatch_uid, _make_id(sender))\n        else:\n            lookup_key = (_make_id(receiver), _make_id(sender))\n\n        disconnected = False\n        with self.lock:\n            self._clear_dead_receivers()\n            for index in range(len(self.receivers)):\n                (r_key, _) = self.receivers[index]\n                if r_key == lookup_key:\n                    disconnected = True\n                    del self.receivers[index]\n                    break\n            self.sender_receivers_cache.clear()\n        return disconnected\n\n    def has_listeners(self, sender=None):\n        return bool(self._live_receivers(sender))\n\n    def send(self, sender, **named):\n        \"\"\"\n        Send signal from sender to all connected receivers.\n\n        If any receiver raises an error, the error propagates back through send,\n        terminating the dispatch loop. So it's possible that all receivers\n        won't be called if an error is raised.\n\n        Arguments:\n\n            sender\n                The sender of the signal. Either a specific object or None.\n\n            named\n                Named arguments which will be passed to receivers.\n\n        Return a list of tuple pairs [(receiver, response), ... ].\n        \"\"\"\n        if not self.receivers or self.sender_receivers_cache.get(sender) is NO_RECEIVERS:\n            return []\n\n        return [\n            (receiver, receiver(signal=self, sender=sender, **named))\n            for receiver in self._live_receivers(sender)\n        ]\n\n    def send_robust(self, sender, **named):\n        \"\"\"\n        Send signal from sender to all connected receivers catching errors.\n\n        Arguments:\n\n            sender\n                The sender of the signal. Can be any Python object (normally one\n                registered with a connect if you actually want something to\n                occur).\n\n            named\n                Named arguments which will be passed to receivers.\n\n        Return a list of tuple pairs [(receiver, response), ... ].\n\n        If any receiver raises an error (specifically any subclass of\n        Exception), return the error instance as the result for that receiver.\n        \"\"\"\n        if not self.receivers or self.sender_receivers_cache.get(sender) is NO_RECEIVERS:\n            return []\n\n        # Call each receiver with whatever arguments it can accept.\n        # Return a list of tuple pairs [(receiver, response), ... ].\n        responses = []\n        for receiver in self._live_receivers(sender):\n            try:\n                response = receiver(signal=self, sender=sender, **named)\n            except Exception as err:\n                responses.append((receiver, err))\n            else:\n                responses.append((receiver, response))\n        return responses\n\n    def _clear_dead_receivers(self):\n        # Note: caller is assumed to hold self.lock.\n        if self._dead_receivers:\n            self._dead_receivers = False\n            self.receivers = [\n                r for r in self.receivers\n                if not(isinstance(r[1], weakref.ReferenceType) and r[1]() is None)", "answer": "D"}
{"uuid": "c11814d2-6a61-4505-807e-c87f357f9886", "issue_statement": "models.W042 is raised on inherited manually specified primary key.\nDescription\n\t\nI have models which inherit from other models, and they should inherit the primary key. This works fine with Django 3.1. However, if I install Django 3.2 alpha, when I run make_migrations I get the following error messages:\nSystem check identified some issues:\nWARNINGS:\naccounts.ReservedUsername: (models.W042) Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\n\t\tHINT: Configure the DEFAULT_AUTO_FIELD setting or the SpeedyCoreAccountsConfig.default_auto_field attribute to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\naccounts.User: (models.W042) Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\n\t\tHINT: Configure the DEFAULT_AUTO_FIELD setting or the SpeedyCoreAccountsConfig.default_auto_field attribute to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\nblocks.Block: (models.W042) Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\n\t\tHINT: Configure the DEFAULT_AUTO_FIELD setting or the AppConfig.default_auto_field attribute to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\ncontact_by_form.Feedback: (models.W042) Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\n\t\tHINT: Configure the DEFAULT_AUTO_FIELD setting or the SpeedyCoreContactByFormConfig.default_auto_field attribute to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\ncore_messages.ReadMark: (models.W042) Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\n\t\tHINT: Configure the DEFAULT_AUTO_FIELD setting or the SpeedyCoreMessagesConfig.default_auto_field attribute to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\nfriendship.Block: (models.W042) Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\n\t\tHINT: Configure the DEFAULT_AUTO_FIELD setting or the AppConfig.default_auto_field attribute to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\nfriendship.Follow: (models.W042) Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\n\t\tHINT: Configure the DEFAULT_AUTO_FIELD setting or the AppConfig.default_auto_field attribute to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\nfriendship.Friend: (models.W042) Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\n\t\tHINT: Configure the DEFAULT_AUTO_FIELD setting or the AppConfig.default_auto_field attribute to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\nfriendship.FriendshipRequest: (models.W042) Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\n\t\tHINT: Configure the DEFAULT_AUTO_FIELD setting or the AppConfig.default_auto_field attribute to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\nlikes.UserLike: (models.W042) Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\n\t\tHINT: Configure the DEFAULT_AUTO_FIELD setting or the AppConfig.default_auto_field attribute to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\nuploads.Image: (models.W042) Auto-created primary key used when not defining a primary key type, by default 'django.db.models.AutoField'.\n\t\tHINT: Configure the DEFAULT_AUTO_FIELD setting or the AppConfig.default_auto_field attribute to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\nThese models should not use auto-created primary keys! I already defined the primary key in the ancestor of the model. For example class Entity which class User inherits from. It looks to me like a bug in Django 3.2 alpha.", "choices": "(A)             return [\n                checks.Warning(\n                    f\"Auto-created primary key used when not defining a \"\n                    f\"primary key type, by default \"\n                    f\"'{settings.DEFAULT_AUTO_FIELD}'.\",\n                    hint=(\n                        f\"Configure the DEFAULT_AUTO_FIELD setting or the \"\n                        f\"{cls._meta.app_config.__class__.__qualname__}.\"\n                        f\"default_auto_field attribute to point to a subclass \"\n                        f\"of AutoField, e.g. 'django.db.models.BigAutoField'.\"\n                    ),\n(B)             not settings.is_overridden('DEFAULT_AUTO_FIELD') and\n            not cls._meta.app_config._is_default_auto_field_overridden\n        ):\n            return [\n                checks.Warning(\n                    f\"Auto-created primary key used when not defining a \"\n                    f\"primary key type, by default \"\n                    f\"'{settings.DEFAULT_AUTO_FIELD}'.\",\n                    hint=(\n                        f\"Configure the DEFAULT_AUTO_FIELD setting or the \"\n                        f\"{cls._meta.app_config.__class__.__qualname__}.\"\n(C)         return errors\n\n    @classmethod\n    def _check_default_pk(cls):\n        if (\n            cls._meta.pk.auto_created and\n            not settings.is_overridden('DEFAULT_AUTO_FIELD') and\n            not cls._meta.app_config._is_default_auto_field_overridden\n        ):\n            return [\n                checks.Warning(\n(D)     def _check_default_pk(cls):\n        if (\n            cls._meta.pk.auto_created and\n            not settings.is_overridden('DEFAULT_AUTO_FIELD') and\n            not cls._meta.app_config._is_default_auto_field_overridden\n        ):\n            return [\n                checks.Warning(\n                    f\"Auto-created primary key used when not defining a \"\n                    f\"primary key type, by default \"\n                    f\"'{settings.DEFAULT_AUTO_FIELD}'.\",", "answer": "D"}
{"uuid": "6ba576a7-8853-4645-9061-3313dc8b96bc", "issue_statement": "ModelChoiceField does not provide value of invalid choice when raising ValidationError\nDescription\n\t \n\t\t(last modified by Aaron Wiegel)\n\t \nCompared with ChoiceField and others, ModelChoiceField does not show the value of the invalid choice when raising a validation error. Passing in parameters with the invalid value and modifying the default error message for the code invalid_choice should fix this.\nFrom source code:\nclass ModelMultipleChoiceField(ModelChoiceField):\n\t\"\"\"A MultipleChoiceField whose choices are a model QuerySet.\"\"\"\n\twidget = SelectMultiple\n\thidden_widget = MultipleHiddenInput\n\tdefault_error_messages = {\n\t\t'invalid_list': _('Enter a list of values.'),\n\t\t'invalid_choice': _('Select a valid choice. %(value)s is not one of the'\n\t\t\t\t\t\t\t' available choices.'),\n\t\t'invalid_pk_value': _('\u201c%(pk)s\u201d is not a valid value.')\n\t}\n\t...\nclass ModelChoiceField(ChoiceField):\n\t\"\"\"A ChoiceField whose choices are a model QuerySet.\"\"\"\n\t# This class is a subclass of ChoiceField for purity, but it doesn't\n\t# actually use any of ChoiceField's implementation.\n\tdefault_error_messages = {\n\t\t'invalid_choice': _('Select a valid choice. That choice is not one of'\n\t\t\t\t\t\t\t' the available choices.'),\n\t}\n\t...", "choices": "(A)         try:\n            key = self.to_field_name or 'pk'\n            if isinstance(value, self.queryset.model):\n                value = getattr(value, key)\n            value = self.queryset.get(**{key: value})\n        except (ValueError, TypeError, self.queryset.model.DoesNotExist):\n            raise ValidationError(self.error_messages['invalid_choice'], code='invalid_choice')\n        return value\n\n    def validate(self, value):\n        return Field.validate(self, value)\n(B)                 value = getattr(value, key)\n            value = self.queryset.get(**{key: value})\n        except (ValueError, TypeError, self.queryset.model.DoesNotExist):\n            raise ValidationError(self.error_messages['invalid_choice'], code='invalid_choice')\n        return value\n\n    def validate(self, value):\n        return Field.validate(self, value)\n\n    def has_changed(self, initial, data):\n        if self.disabled:\n(C)             raise ValidationError(self.error_messages['invalid_choice'], code='invalid_choice')\n        return value\n\n    def validate(self, value):\n        return Field.validate(self, value)\n\n    def has_changed(self, initial, data):\n        if self.disabled:\n            return False\n        initial_value = initial if initial is not None else ''\n        data_value = data if data is not None else ''\n(D)     def validate(self, value):\n        return Field.validate(self, value)\n\n    def has_changed(self, initial, data):\n        if self.disabled:\n            return False\n        initial_value = initial if initial is not None else ''\n        data_value = data if data is not None else ''\n        return str(self.prepare_value(initial_value)) != str(data_value)\n\n", "answer": "B"}
{"uuid": "c0d3be5b-4834-45d6-9f76-ca8de8bf2719", "issue_statement": "Saving parent object after setting on child leads to data loss for parents with non-numeric primary key.\nDescription\n\t \n\t\t(last modified by Charlie DeTar)\n\t \nGiven a model with a foreign key relation to another model that has a non-auto CharField as its primary key:\nclass Product(models.Model):\n\tsku = models.CharField(primary_key=True, max_length=50)\nclass Order(models.Model):\n\tproduct = models.ForeignKey(Product, on_delete=models.CASCADE)\nIf the relation is initialized on the parent with an empty instance that does not yet specify its primary key, and the primary key is subsequently defined, the parent does not \"see\" the primary key's change:\nwith transaction.atomic():\n\torder = Order()\n\torder.product = Product()\n\torder.product.sku = \"foo\"\n\torder.product.save()\n\torder.save()\n\tassert Order.objects.filter(product_id=\"\").exists() # Succeeds, but shouldn't\n\tassert Order.objects.filter(product=order.product).exists() # Fails\nInstead of product_id being populated with product.sku, it is set to emptystring. The foreign key constraint which would enforce the existence of a product with sku=\"\" is deferred until the transaction commits. The transaction does correctly fail on commit with a ForeignKeyViolation due to the non-existence of a product with emptystring as its primary key.\nOn the other hand, if the related unsaved instance is initialized with its primary key before assignment to the parent, it is persisted correctly:\nwith transaction.atomic():\n\torder = Order()\n\torder.product = Product(sku=\"foo\")\n\torder.product.save()\n\torder.save()\n\tassert Order.objects.filter(product=order.product).exists() # succeeds\nCommitting the transaction also succeeds.\nThis may have something to do with how the Order.product_id field is handled at assignment, together with something about handling fetching of auto vs non-auto primary keys from the related instance.", "choices": "(A)                     # Use pk from related object if it has been saved after\n                    # an assignment.\n                    setattr(self, field.attname, obj.pk)\n                # If the relationship's pk/to_field was changed, clear the\n                # cached relationship.\n                if getattr(obj, field.target_field.attname) != getattr(self, field.attname):\n                    field.delete_cached_value(self)\n(B)                         field.remote_field.delete_cached_value(obj)\n                    raise ValueError(\n                        \"%s() prohibited to prevent data loss due to unsaved \"\n                        \"related object '%s'.\" % (operation_name, field.name)\n                    )\n                elif getattr(self, field.attname) is None:\n                    # Use pk from related object if it has been saved after\n(C)                         \"%s() prohibited to prevent data loss due to unsaved \"\n                        \"related object '%s'.\" % (operation_name, field.name)\n                    )\n                elif getattr(self, field.attname) is None:\n                    # Use pk from related object if it has been saved after\n                    # an assignment.\n                    setattr(self, field.attname, obj.pk)\n(D)                     )\n                elif getattr(self, field.attname) is None:\n                    # Use pk from related object if it has been saved after\n                    # an assignment.\n                    setattr(self, field.attname, obj.pk)\n                # If the relationship's pk/to_field was changed, clear the\n                # cached relationship.", "answer": "C"}
{"uuid": "c69d3aea-eb27-4fa5-a8e9-66090e938877", "issue_statement": "\"TypeError: cannot pickle\" when applying | operator to a Q object\nDescription\n\t \n\t\t(last modified by Daniel Izquierdo)\n\t \nUsing a reference to a non-pickleable type of object such as dict_keys in a Q object makes the | operator fail:\n>>> from django.db.models import Q\n>>> Q(x__in={}.keys())\n<Q: (AND: ('x__in', dict_keys([])))>\n>>> Q() | Q(x__in={}.keys())\nTraceback (most recent call last):\n...\nTypeError: cannot pickle 'dict_keys' object\nEven though this particular example could be solved by doing Q() | Q(x__in={}) it still feels like using .keys() should work.\nI can work on a patch if there's agreement that this should not crash.", "choices": "(A) # describe the relation in Model terms (model Options and Fields for both\n# sides of the relation. The join_field is the field backing the relation.\nPathInfo = namedtuple('PathInfo', 'from_opts to_opts target_fields join_field m2m direct filtered_relation')\n\n\ndef subclasses(cls):\n    yield cls\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n\n\nclass Q(tree.Node):\n    \"\"\"\n    Encapsulate filters as objects that can then be combined logically (using\n    `&` and `|`).\n    \"\"\"\n    # Connection types\n    AND = 'AND'\n    OR = 'OR'\n    default = AND\n    conditional = True\n\n    def __init__(self, *args, _connector=None, _negated=False, **kwargs):\n        super().__init__(children=[*args, *sorted(kwargs.items())], connector=_connector, negated=_negated)\n\n    def _combine(self, other, conn):\n        if not isinstance(other, Q):\n            raise TypeError(other)\n\n        # If the other Q() is empty, ignore it and just use `self`.\n        if not other:\n            return copy.deepcopy(self)\n        # Or if this Q is empty, ignore it and just use `other`.\n        elif not self:\n            return copy.deepcopy(other)\n\n        obj = type(self)()\n        obj.connector = conn\n        obj.add(self, conn)\n        obj.add(other, conn)\n        return obj\n\n    def __or__(self, other):\n        return self._combine(other, self.OR)\n\n    def __and__(self, other):\n        return self._combine(other, self.AND)\n\n    def __invert__(self):\n        obj = type(self)()\n        obj.add(self, self.AND)\n        obj.negate()\n(B)     Encapsulate filters as objects that can then be combined logically (using\n    `&` and `|`).\n    \"\"\"\n    # Connection types\n    AND = 'AND'\n    OR = 'OR'\n    default = AND\n    conditional = True\n\n    def __init__(self, *args, _connector=None, _negated=False, **kwargs):\n        super().__init__(children=[*args, *sorted(kwargs.items())], connector=_connector, negated=_negated)\n\n    def _combine(self, other, conn):\n        if not isinstance(other, Q):\n            raise TypeError(other)\n\n        # If the other Q() is empty, ignore it and just use `self`.\n        if not other:\n            return copy.deepcopy(self)\n        # Or if this Q is empty, ignore it and just use `other`.\n        elif not self:\n            return copy.deepcopy(other)\n\n        obj = type(self)()\n        obj.connector = conn\n        obj.add(self, conn)\n        obj.add(other, conn)\n        return obj\n\n    def __or__(self, other):\n        return self._combine(other, self.OR)\n\n    def __and__(self, other):\n        return self._combine(other, self.AND)\n\n    def __invert__(self):\n        obj = type(self)()\n        obj.add(self, self.AND)\n        obj.negate()\n        return obj\n\n    def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):\n        # We must promote any new joins to left outer joins so that when Q is\n        # used as an expression, rows aren't filtered due to joins.\n        clause, joins = query._add_q(\n            self, reuse, allow_joins=allow_joins, split_subq=False,\n            check_filterable=False,\n        )\n        query.promote_joins(joins)\n        return clause\n\n    def deconstruct(self):\n(C)         if not isinstance(other, Q):\n            raise TypeError(other)\n\n        # If the other Q() is empty, ignore it and just use `self`.\n        if not other:\n            return copy.deepcopy(self)\n        # Or if this Q is empty, ignore it and just use `other`.\n        elif not self:\n            return copy.deepcopy(other)\n\n        obj = type(self)()\n        obj.connector = conn\n        obj.add(self, conn)\n        obj.add(other, conn)\n        return obj\n\n    def __or__(self, other):\n        return self._combine(other, self.OR)\n\n    def __and__(self, other):\n        return self._combine(other, self.AND)\n\n    def __invert__(self):\n        obj = type(self)()\n        obj.add(self, self.AND)\n        obj.negate()\n        return obj\n\n    def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):\n        # We must promote any new joins to left outer joins so that when Q is\n        # used as an expression, rows aren't filtered due to joins.\n        clause, joins = query._add_q(\n            self, reuse, allow_joins=allow_joins, split_subq=False,\n            check_filterable=False,\n        )\n        query.promote_joins(joins)\n        return clause\n\n    def deconstruct(self):\n        path = '%s.%s' % (self.__class__.__module__, self.__class__.__name__)\n        if path.startswith('django.db.models.query_utils'):\n            path = path.replace('django.db.models.query_utils', 'django.db.models')\n        args, kwargs = (), {}\n        if len(self.children) == 1 and not isinstance(self.children[0], Q):\n            child = self.children[0]\n            kwargs = {child[0]: child[1]}\n        else:\n            args = tuple(self.children)\n            if self.connector != self.default:\n                kwargs = {'_connector': self.connector}\n        if self.negated:\n            kwargs['_negated'] = True\n(D) large and/or so that they can be used by other modules without getting into\ncircular import difficulties.\n\"\"\"\nimport copy\nimport functools\nimport inspect\nfrom collections import namedtuple\n\nfrom django.core.exceptions import FieldError\nfrom django.db.models.constants import LOOKUP_SEP\nfrom django.utils import tree\n\n# PathInfo is used when converting lookups (fk__somecol). The contents\n# describe the relation in Model terms (model Options and Fields for both\n# sides of the relation. The join_field is the field backing the relation.\nPathInfo = namedtuple('PathInfo', 'from_opts to_opts target_fields join_field m2m direct filtered_relation')\n\n\ndef subclasses(cls):\n    yield cls\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n\n\nclass Q(tree.Node):\n    \"\"\"\n    Encapsulate filters as objects that can then be combined logically (using\n    `&` and `|`).\n    \"\"\"\n    # Connection types\n    AND = 'AND'\n    OR = 'OR'\n    default = AND\n    conditional = True\n\n    def __init__(self, *args, _connector=None, _negated=False, **kwargs):\n        super().__init__(children=[*args, *sorted(kwargs.items())], connector=_connector, negated=_negated)\n\n    def _combine(self, other, conn):\n        if not isinstance(other, Q):\n            raise TypeError(other)\n\n        # If the other Q() is empty, ignore it and just use `self`.\n        if not other:\n            return copy.deepcopy(self)\n        # Or if this Q is empty, ignore it and just use `other`.\n        elif not self:\n            return copy.deepcopy(other)\n\n        obj = type(self)()\n        obj.connector = conn\n        obj.add(self, conn)", "answer": "D"}
{"uuid": "bf7eb21c-22c1-4176-8f4c-673ba7977c51", "issue_statement": "Q(...) & Exists(...) raises a TypeError\nDescription\n\t\nExists(...) & Q(...) works, but Q(...) & Exists(...) raise a TypeError\nHere's a minimal example:\nIn [3]: Exists(Product.objects.all()) & Q()\nOut[3]: <Q: (AND: <django.db.models.expressions.Exists object at 0x7fc18dd0ed90>, (AND: ))>\nIn [4]: Q() & Exists(Product.objects.all())\n---------------------------------------------------------------------------\nTypeError\t\t\t\t\t\t\t\t Traceback (most recent call last)\n<ipython-input-4-21d3dea0fcb9> in <module>\n----> 1 Q() & Exists(Product.objects.all())\n~/Code/venv/ecom/lib/python3.8/site-packages/django/db/models/query_utils.py in __and__(self, other)\n\t 90 \n\t 91\t def __and__(self, other):\n---> 92\t\t return self._combine(other, self.AND)\n\t 93 \n\t 94\t def __invert__(self):\n~/Code/venv/ecom/lib/python3.8/site-packages/django/db/models/query_utils.py in _combine(self, other, conn)\n\t 71\t def _combine(self, other, conn):\n\t 72\t\t if not isinstance(other, Q):\n---> 73\t\t\t raise TypeError(other)\n\t 74 \n\t 75\t\t # If the other Q() is empty, ignore it and just use `self`.\nTypeError: <django.db.models.expressions.Exists object at 0x7fc18dd21400>\nThe & (and |) operators should be commutative on Q-Exists pairs, but it's not\nI think there's a missing definition of __rand__ somewhere.", "choices": "(A)     def _combine(self, other, conn):\n        if not isinstance(other, Q):\n            raise TypeError(other)\n\n        # If the other Q() is empty, ignore it and just use `self`.\n        if not other:\n            _, args, kwargs = self.deconstruct()\n(B)             raise TypeError(other)\n\n        # If the other Q() is empty, ignore it and just use `self`.\n        if not other:\n            _, args, kwargs = self.deconstruct()\n            return type(self)(*args, **kwargs)\n        # Or if this Q is empty, ignore it and just use `other`.\n(C) \n    def __init__(self, *args, _connector=None, _negated=False, **kwargs):\n        super().__init__(children=[*args, *sorted(kwargs.items())], connector=_connector, negated=_negated)\n\n    def _combine(self, other, conn):\n        if not isinstance(other, Q):\n            raise TypeError(other)\n(D)         super().__init__(children=[*args, *sorted(kwargs.items())], connector=_connector, negated=_negated)\n\n    def _combine(self, other, conn):\n        if not isinstance(other, Q):\n            raise TypeError(other)\n\n        # If the other Q() is empty, ignore it and just use `self`.", "answer": "D"}
{"uuid": "a00fb977-705a-48f2-a1f7-c1fe21aaa338", "issue_statement": "ResolverMatch.__repr__() doesn't handle functools.partial() nicely.\nDescription\n\t \n\t\t(last modified by Nick Pope)\n\t \nWhen a partial function is passed as the view, the __repr__ shows the func argument as functools.partial which isn't very helpful, especially as it doesn't reveal the underlying function or arguments provided.\nBecause a partial function also has arguments provided up front, we need to handle those specially so that they are accessible in __repr__.\nISTM that we can simply unwrap functools.partial objects in ResolverMatch.__init__().", "choices": "(A)         return (self.func, self.args, self.kwargs)[index]\n\n    def __repr__(self):\n        return \"ResolverMatch(func=%s, args=%s, kwargs=%s, url_name=%s, app_names=%s, namespaces=%s, route=%s)\" % (\n            self._func_path, self.args, self.kwargs, self.url_name,\n            self.app_names, self.namespaces, self.route,\n        )\n\n\ndef get_resolver(urlconf=None):\n    if urlconf is None:\n        urlconf = settings.ROOT_URLCONF\n    return _get_cached_resolver(urlconf)\n\n\n@functools.lru_cache(maxsize=None)\n(B)         view_path = url_name or self._func_path\n        self.view_name = ':'.join(self.namespaces + [view_path])\n\n    def __getitem__(self, index):\n        return (self.func, self.args, self.kwargs)[index]\n\n    def __repr__(self):\n        return \"ResolverMatch(func=%s, args=%s, kwargs=%s, url_name=%s, app_names=%s, namespaces=%s, route=%s)\" % (\n            self._func_path, self.args, self.kwargs, self.url_name,\n            self.app_names, self.namespaces, self.route,\n        )\n\n\ndef get_resolver(urlconf=None):\n    if urlconf is None:\n        urlconf = settings.ROOT_URLCONF\n(C) \ndef get_resolver(urlconf=None):\n    if urlconf is None:\n        urlconf = settings.ROOT_URLCONF\n    return _get_cached_resolver(urlconf)\n\n\n@functools.lru_cache(maxsize=None)\ndef _get_cached_resolver(urlconf=None):\n    return URLResolver(RegexPattern(r'^/'), urlconf)\n\n\n@functools.lru_cache(maxsize=None)\ndef get_ns_resolver(ns_pattern, resolver, converters):\n    # Build a namespaced resolver for the given parent URLconf pattern.\n    # This makes it possible to have captured parameters in the parent\n(D)             self._func_path, self.args, self.kwargs, self.url_name,\n            self.app_names, self.namespaces, self.route,\n        )\n\n\ndef get_resolver(urlconf=None):\n    if urlconf is None:\n        urlconf = settings.ROOT_URLCONF\n    return _get_cached_resolver(urlconf)\n\n\n@functools.lru_cache(maxsize=None)\ndef _get_cached_resolver(urlconf=None):\n    return URLResolver(RegexPattern(r'^/'), urlconf)\n\n", "answer": "A"}
{"uuid": "3beefcf1-7e7a-4d9a-b254-e3d231f1716f", "issue_statement": "DEFAULT_AUTO_FIELD subclass check fails for subclasses of BigAutoField and SmallAutoField.\nDescription\n\t\nSet DEFAULT_AUTO_FIELD = \"example.core.models.MyBigAutoField\" , with contents of example.core.models:\nfrom django.db import models\nclass MyBigAutoField(models.BigAutoField):\n\tpass\nclass MyModel(models.Model):\n\tpass\nDjango then crashes with:\nTraceback (most recent call last):\n File \"/..././manage.py\", line 21, in <module>\n\tmain()\n File \"/..././manage.py\", line 17, in main\n\texecute_from_command_line(sys.argv)\n File \"/.../venv/lib/python3.9/site-packages/django/core/management/__init__.py\", line 419, in execute_from_command_line\n\tutility.execute()\n File \"/.../venv/lib/python3.9/site-packages/django/core/management/__init__.py\", line 395, in execute\n\tdjango.setup()\n File \"/.../venv/lib/python3.9/site-packages/django/__init__.py\", line 24, in setup\n\tapps.populate(settings.INSTALLED_APPS)\n File \"/.../venv/lib/python3.9/site-packages/django/apps/registry.py\", line 114, in populate\n\tapp_config.import_models()\n File \"/.../venv/lib/python3.9/site-packages/django/apps/config.py\", line 301, in import_models\n\tself.models_module = import_module(models_module_name)\n File \"/Users/chainz/.pyenv/versions/3.9.1/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n\treturn _bootstrap._gcd_import(name[level:], package, level)\n File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n File \"<frozen importlib._bootstrap_external>\", line 790, in exec_module\n File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n File \"/.../example/core/models.py\", line 8, in <module>\n\tclass MyModel(models.Model):\n File \"/.../venv/lib/python3.9/site-packages/django/db/models/base.py\", line 320, in __new__\n\tnew_class._prepare()\n File \"/.../venv/lib/python3.9/site-packages/django/db/models/base.py\", line 333, in _prepare\n\topts._prepare(cls)\n File \"/.../venv/lib/python3.9/site-packages/django/db/models/options.py\", line 285, in _prepare\n\tpk_class = self._get_default_pk_class()\n File \"/.../venv/lib/python3.9/site-packages/django/db/models/options.py\", line 246, in _get_default_pk_class\n\traise ValueError(\nValueError: Primary key 'example.core.models.MyBigAutoField' referred by DEFAULT_AUTO_FIELD must subclass AutoField.\nThis can be fixed in AutoFieldMeta.__subclasscheck__ by allowing subclasses of those classes in the _subclasses property.", "choices": "(A) \n\nclass AutoField(AutoFieldMixin, IntegerField, metaclass=AutoFieldMeta):\n\n    def get_internal_type(self):\n        return 'AutoField'\n\n(B) \n    def __instancecheck__(self, instance):\n        return isinstance(instance, self._subclasses) or super().__instancecheck__(instance)\n\n    def __subclasscheck__(self, subclass):\n        return subclass in self._subclasses or super().__subclasscheck__(subclass)\n\n(C)     def __subclasscheck__(self, subclass):\n        return subclass in self._subclasses or super().__subclasscheck__(subclass)\n\n\nclass AutoField(AutoFieldMixin, IntegerField, metaclass=AutoFieldMeta):\n\n    def get_internal_type(self):\n(D)         return isinstance(instance, self._subclasses) or super().__instancecheck__(instance)\n\n    def __subclasscheck__(self, subclass):\n        return subclass in self._subclasses or super().__subclasscheck__(subclass)\n\n\nclass AutoField(AutoFieldMixin, IntegerField, metaclass=AutoFieldMeta):", "answer": "D"}
{"uuid": "1c5ba05c-4229-4da7-af6c-b3cf8772191f", "issue_statement": "django-admin startapp with trailing slash in directory name results in error\nDescription\n\t\nBash tab-completion appends trailing slashes to directory names. django-admin startapp name directory/ results in the error:\nCommandError: '' is not a valid app directory. Please make sure the directory is a valid identifier.\nThe error is caused by \u200bline 77 of django/core/management/templates.py by calling basename() on the path with no consideration for a trailing slash:\nself.validate_name(os.path.basename(target), 'directory')\nRemoving potential trailing slashes would solve the problem:\nself.validate_name(os.path.basename(target.rstrip(os.sep)), 'directory')", "choices": "(A)             top_dir = os.path.abspath(os.path.expanduser(target))\n            if not os.path.exists(top_dir):\n                raise CommandError(\"Destination directory '%s' does not \"\n                                   \"exist, please create it first.\" % top_dir)\n\n        extensions = tuple(handle_extensions(options['extensions']))\n        extra_files = []\n        for file in options['files']:\n            extra_files.extend(map(lambda x: x.strip(), file.split(',')))\n(B)             except OSError as e:\n                raise CommandError(e)\n        else:\n            if app_or_project == 'app':\n                self.validate_name(os.path.basename(target), 'directory')\n            top_dir = os.path.abspath(os.path.expanduser(target))\n            if not os.path.exists(top_dir):\n                raise CommandError(\"Destination directory '%s' does not \"\n                                   \"exist, please create it first.\" % top_dir)\n(C)             if app_or_project == 'app':\n                self.validate_name(os.path.basename(target), 'directory')\n            top_dir = os.path.abspath(os.path.expanduser(target))\n            if not os.path.exists(top_dir):\n                raise CommandError(\"Destination directory '%s' does not \"\n                                   \"exist, please create it first.\" % top_dir)\n\n        extensions = tuple(handle_extensions(options['extensions']))\n        extra_files = []\n(D)                 os.makedirs(top_dir)\n            except FileExistsError:\n                raise CommandError(\"'%s' already exists\" % top_dir)\n            except OSError as e:\n                raise CommandError(e)\n        else:\n            if app_or_project == 'app':\n                self.validate_name(os.path.basename(target), 'directory')\n            top_dir = os.path.abspath(os.path.expanduser(target))", "answer": "B"}
{"uuid": "fd526ba4-7431-44c0-98de-fa9776984dc3", "issue_statement": "Label for ReadOnlyPasswordHashWidget points to non-labelable element.\nDescription\n\t \n\t\t(last modified by David Sanders)\n\t \nIn the admin, the label element for the ReadOnlyPasswordHashWidget widget has a 'for' attribute which points to a non-labelable element, since the widget just renders text, not an input. There's no labelable element for the widget, so the label shouldn't have a 'for' attribute.", "choices": "(A)     widget = ReadOnlyPasswordHashWidget\n\n    def __init__(self, *args, **kwargs):\n        kwargs.setdefault(\"required\", False)\n        kwargs.setdefault('disabled', True)\n        super().__init__(*args, **kwargs)\n\n\nclass UsernameField(forms.CharField):\n(B)         context['summary'] = summary\n        return context\n\n\nclass ReadOnlyPasswordHashField(forms.Field):\n    widget = ReadOnlyPasswordHashWidget\n\n    def __init__(self, *args, **kwargs):\n        kwargs.setdefault(\"required\", False)\n(C)             else:\n                for key, value_ in hasher.safe_summary(value).items():\n                    summary.append({'label': gettext(key), 'value': value_})\n        context['summary'] = summary\n        return context\n\n\nclass ReadOnlyPasswordHashField(forms.Field):\n    widget = ReadOnlyPasswordHashWidget\n(D) \nclass ReadOnlyPasswordHashField(forms.Field):\n    widget = ReadOnlyPasswordHashWidget\n\n    def __init__(self, *args, **kwargs):\n        kwargs.setdefault(\"required\", False)\n        kwargs.setdefault('disabled', True)\n        super().__init__(*args, **kwargs)\n", "answer": "B"}
{"uuid": "4bff1ab9-a02f-4b7a-84df-ac4d312f8c10", "issue_statement": "BoundWidget.id_for_label ignores id set by ChoiceWidget.options\nDescription\n\t\nIf you look at the implementation of BoundField.subwidgets\nclass BoundField:\n\t...\n\tdef subwidgets(self):\n\t\tid_ = self.field.widget.attrs.get('id') or self.auto_id\n\t\tattrs = {'id': id_} if id_ else {}\n\t\tattrs = self.build_widget_attrs(attrs)\n\t\treturn [\n\t\t\tBoundWidget(self.field.widget, widget, self.form.renderer)\n\t\t\tfor widget in self.field.widget.subwidgets(self.html_name, self.value(), attrs=attrs)\n\t\t]\none sees that self.field.widget.subwidgets(self.html_name, self.value(), attrs=attrs) returns a dict and assigns it to widget. Now widget['attrs']['id'] contains the \"id\" we would like to use when rendering the label of our CheckboxSelectMultiple.\nHowever BoundWidget.id_for_label() is implemented as\nclass BoundWidget:\n\t...\n\tdef id_for_label(self):\n\t\treturn 'id_%s_%s' % (self.data['name'], self.data['index'])\nignoring the id available through self.data['attrs']['id']. This re-implementation for rendering the \"id\" is confusing and presumably not intended. Nobody has probably realized that so far, because rarely the auto_id-argument is overridden when initializing a form. If however we do, one would assume that the method BoundWidget.id_for_label renders that string as specified through the auto_id format-string.\nBy changing the code from above to\nclass BoundWidget:\n\t...\n\tdef id_for_label(self):\n\t\treturn self.data['attrs']['id']\nthat function behaves as expected.\nPlease note that this error only occurs when rendering the subwidgets of a widget of type CheckboxSelectMultiple. This has nothing to do with the method BoundField.id_for_label().", "choices": "(A)     def template_name(self):\n        if 'template_name' in self.data:\n            return self.data['template_name']\n        return self.parent_widget.template_name\n\n    @property\n    def id_for_label(self):\n(B) \n    @property\n    def id_for_label(self):\n        return 'id_%s_%s' % (self.data['name'], self.data['index'])\n\n    @property\n    def choice_label(self):\n(C)             return self.data['template_name']\n        return self.parent_widget.template_name\n\n    @property\n    def id_for_label(self):\n        return 'id_%s_%s' % (self.data['name'], self.data['index'])\n\n(D) \n    @property\n    def template_name(self):\n        if 'template_name' in self.data:\n            return self.data['template_name']\n        return self.parent_widget.template_name\n", "answer": "B"}
{"uuid": "bb397d44-3a60-4aeb-a179-cd39b6c14927", "issue_statement": "Missing import statement in generated migration (NameError: name 'models' is not defined)\nDescription\n\t\nI found a bug in Django's latest release: 3.2.4. \nGiven the following contents of models.py:\nfrom django.db import models\nclass MyField(models.TextField):\n\tpass\nclass MyBaseModel(models.Model):\n\tclass Meta:\n\t\tabstract = True\nclass MyMixin:\n\tpass\nclass MyModel(MyMixin, MyBaseModel):\n\tname = MyField(primary_key=True)\nThe makemigrations command will generate the following migration file:\n# Generated by Django 3.2.4 on 2021-06-30 19:13\nimport app.models\nfrom django.db import migrations\nclass Migration(migrations.Migration):\n\tinitial = True\n\tdependencies = [\n\t]\n\toperations = [\n\t\tmigrations.CreateModel(\n\t\t\tname='MyModel',\n\t\t\tfields=[\n\t\t\t\t('name', app.models.MyField(primary_key=True, serialize=False)),\n\t\t\t],\n\t\t\toptions={\n\t\t\t\t'abstract': False,\n\t\t\t},\n\t\t\tbases=(app.models.MyMixin, models.Model),\n\t\t),\n\t]\nWhich will then fail with the following error:\n File \"/home/jj/django_example/app/migrations/0001_initial.py\", line 7, in <module>\n\tclass Migration(migrations.Migration):\n File \"/home/jj/django_example/app/migrations/0001_initial.py\", line 23, in Migration\n\tbases=(app.models.MyMixin, models.Model),\nNameError: name 'models' is not defined\nExpected behavior: Django generates a migration file that is valid Python.\nActual behavior: Django generates a migration file that is missing an import statement.\nI think this is a bug of the module django.db.migrations.writer, but I'm not sure. I will be happy to assist with debugging.\nThanks for your attention,\nJaap Joris", "choices": "(A) class TypeSerializer(BaseSerializer):\n    def serialize(self):\n        special_cases = [\n            (models.Model, \"models.Model\", []),\n            (type(None), 'type(None)', []),\n        ]\n        for case, string, imports in special_cases:\n(B)         special_cases = [\n            (models.Model, \"models.Model\", []),\n            (type(None), 'type(None)', []),\n        ]\n        for case, string, imports in special_cases:\n            if case is self.value:\n                return string, set(imports)\n(C) \n\nclass TypeSerializer(BaseSerializer):\n    def serialize(self):\n        special_cases = [\n            (models.Model, \"models.Model\", []),\n            (type(None), 'type(None)', []),\n(D)             (type(None), 'type(None)', []),\n        ]\n        for case, string, imports in special_cases:\n            if case is self.value:\n                return string, set(imports)\n        if hasattr(self.value, \"__module__\"):\n            module = self.value.__module__", "answer": "A"}
{"uuid": "934528c0-128c-45a2-b74b-2d1ca7d13dbd", "issue_statement": "Add `nonform` CSS class for non form errors in FormSets\nDescription\n\t \n\t\t(last modified by Ties Jan Hefting)\n\t \nForms add the nonfield CSS class for non field errors in ErrorList instances. This is documented in a section on \u200brendering form error messages. Similarly, in FormSets I'd expect to see the nonform CSS class added for non form errors. This would allow a custom ErrorList to make a distinction in form field errors, non field errors (forms) and non form errors (FormSets) when rendering error messages. Therefore I'd suggest to add this nonform CSS class and document it for developers to use.", "choices": "(A)         if not self.is_bound:\n            return False\n        # Accessing errors triggers a full clean the first time only.\n        self.errors\n        # List comprehension ensures is_valid() is called for all forms.\n        # Forms due to be deleted shouldn't cause the formset to be invalid.\n        forms_valid = all([\n            form.is_valid() for form in self.forms\n            if not (self.can_delete and self._should_delete_form(form))\n        ])\n        return forms_valid and not self.non_form_errors()\n\n    def full_clean(self):\n        \"\"\"\n        Clean all of self.data and populate self._errors and\n        self._non_form_errors.\n        \"\"\"\n        self._errors = []\n        self._non_form_errors = self.error_class()\n        empty_forms_count = 0\n\n        if not self.is_bound:  # Stop further processing.\n            return\n\n        if not self.management_form.is_valid():\n            error = ValidationError(\n                self.error_messages['missing_management_form'],\n                params={\n                    'field_names': ', '.join(\n                        self.management_form.add_prefix(field_name)\n                        for field_name in self.management_form.errors\n                    ),\n                },\n                code='missing_management_form',\n            )\n            self._non_form_errors.append(error)\n\n        for i, form in enumerate(self.forms):\n            # Empty forms are unchanged forms beyond those with initial data.\n            if not form.has_changed() and i >= self.initial_form_count():\n                empty_forms_count += 1\n            # Accessing errors calls full_clean() if necessary.\n            # _should_delete_form() requires cleaned_data.\n            form_errors = form.errors\n            if self.can_delete and self._should_delete_form(form):\n                continue\n            self._errors.append(form_errors)\n        try:\n            if (self.validate_max and\n                    self.total_form_count() - len(self.deleted_forms) > self.max_num) or \\\n                    self.management_form.cleaned_data[TOTAL_FORM_COUNT] > self.absolute_max:\n                raise ValidationError(ngettext(\n                    \"Please submit at most %d form.\",\n                    \"Please submit at most %d forms.\", self.max_num) % self.max_num,\n                    code='too_many_forms',\n                )\n            if (self.validate_min and\n(B)                         for field_name in self.management_form.errors\n                    ),\n                },\n                code='missing_management_form',\n            )\n            self._non_form_errors.append(error)\n\n        for i, form in enumerate(self.forms):\n            # Empty forms are unchanged forms beyond those with initial data.\n            if not form.has_changed() and i >= self.initial_form_count():\n                empty_forms_count += 1\n            # Accessing errors calls full_clean() if necessary.\n            # _should_delete_form() requires cleaned_data.\n            form_errors = form.errors\n            if self.can_delete and self._should_delete_form(form):\n                continue\n            self._errors.append(form_errors)\n        try:\n            if (self.validate_max and\n                    self.total_form_count() - len(self.deleted_forms) > self.max_num) or \\\n                    self.management_form.cleaned_data[TOTAL_FORM_COUNT] > self.absolute_max:\n                raise ValidationError(ngettext(\n                    \"Please submit at most %d form.\",\n                    \"Please submit at most %d forms.\", self.max_num) % self.max_num,\n                    code='too_many_forms',\n                )\n            if (self.validate_min and\n                    self.total_form_count() - len(self.deleted_forms) - empty_forms_count < self.min_num):\n                raise ValidationError(ngettext(\n                    \"Please submit at least %d form.\",\n                    \"Please submit at least %d forms.\", self.min_num) % self.min_num,\n                    code='too_few_forms')\n            # Give self.clean() a chance to do cross-form validation.\n            self.clean()\n        except ValidationError as e:\n            self._non_form_errors = self.error_class(e.error_list)\n\n    def clean(self):\n        \"\"\"\n        Hook for doing any extra formset-wide cleaning after Form.clean() has\n        been called on every form. Any ValidationError raised by this method\n        will not be associated with a particular form; it will be accessible\n        via formset.non_form_errors()\n        \"\"\"\n        pass\n\n    def has_changed(self):\n        \"\"\"Return True if data in any form differs from initial.\"\"\"\n        return any(form.has_changed() for form in self)\n\n    def add_fields(self, form, index):\n        \"\"\"A hook for adding extra fields on to each form instance.\"\"\"\n        initial_form_count = self.initial_form_count()\n        if self.can_order:\n            # Only pre-fill the ordering field for initial forms.\n            if index is not None and index < initial_form_count:\n                form.fields[ORDERING_FIELD_NAME] = IntegerField(\n(C)         self._non_form_errors.\n        \"\"\"\n        self._errors = []\n        self._non_form_errors = self.error_class()\n        empty_forms_count = 0\n\n        if not self.is_bound:  # Stop further processing.\n            return\n\n        if not self.management_form.is_valid():\n            error = ValidationError(\n                self.error_messages['missing_management_form'],\n                params={\n                    'field_names': ', '.join(\n                        self.management_form.add_prefix(field_name)\n                        for field_name in self.management_form.errors\n                    ),\n                },\n                code='missing_management_form',\n            )\n            self._non_form_errors.append(error)\n\n        for i, form in enumerate(self.forms):\n            # Empty forms are unchanged forms beyond those with initial data.\n            if not form.has_changed() and i >= self.initial_form_count():\n                empty_forms_count += 1\n            # Accessing errors calls full_clean() if necessary.\n            # _should_delete_form() requires cleaned_data.\n            form_errors = form.errors\n            if self.can_delete and self._should_delete_form(form):\n                continue\n            self._errors.append(form_errors)\n        try:\n            if (self.validate_max and\n                    self.total_form_count() - len(self.deleted_forms) > self.max_num) or \\\n                    self.management_form.cleaned_data[TOTAL_FORM_COUNT] > self.absolute_max:\n                raise ValidationError(ngettext(\n                    \"Please submit at most %d form.\",\n                    \"Please submit at most %d forms.\", self.max_num) % self.max_num,\n                    code='too_many_forms',\n                )\n            if (self.validate_min and\n                    self.total_form_count() - len(self.deleted_forms) - empty_forms_count < self.min_num):\n                raise ValidationError(ngettext(\n                    \"Please submit at least %d form.\",\n                    \"Please submit at least %d forms.\", self.min_num) % self.min_num,\n                    code='too_few_forms')\n            # Give self.clean() a chance to do cross-form validation.\n            self.clean()\n        except ValidationError as e:\n            self._non_form_errors = self.error_class(e.error_list)\n\n    def clean(self):\n        \"\"\"\n        Hook for doing any extra formset-wide cleaning after Form.clean() has\n        been called on every form. Any ValidationError raised by this method\n        will not be associated with a particular form; it will be accessible\n(D)             if self.can_delete and self._should_delete_form(form):\n                continue\n            self._errors.append(form_errors)\n        try:\n            if (self.validate_max and\n                    self.total_form_count() - len(self.deleted_forms) > self.max_num) or \\\n                    self.management_form.cleaned_data[TOTAL_FORM_COUNT] > self.absolute_max:\n                raise ValidationError(ngettext(\n                    \"Please submit at most %d form.\",\n                    \"Please submit at most %d forms.\", self.max_num) % self.max_num,\n                    code='too_many_forms',\n                )\n            if (self.validate_min and\n                    self.total_form_count() - len(self.deleted_forms) - empty_forms_count < self.min_num):\n                raise ValidationError(ngettext(\n                    \"Please submit at least %d form.\",\n                    \"Please submit at least %d forms.\", self.min_num) % self.min_num,\n                    code='too_few_forms')\n            # Give self.clean() a chance to do cross-form validation.\n            self.clean()\n        except ValidationError as e:\n            self._non_form_errors = self.error_class(e.error_list)\n\n    def clean(self):\n        \"\"\"\n        Hook for doing any extra formset-wide cleaning after Form.clean() has\n        been called on every form. Any ValidationError raised by this method\n        will not be associated with a particular form; it will be accessible\n        via formset.non_form_errors()\n        \"\"\"\n        pass\n\n    def has_changed(self):\n        \"\"\"Return True if data in any form differs from initial.\"\"\"\n        return any(form.has_changed() for form in self)\n\n    def add_fields(self, form, index):\n        \"\"\"A hook for adding extra fields on to each form instance.\"\"\"\n        initial_form_count = self.initial_form_count()\n        if self.can_order:\n            # Only pre-fill the ordering field for initial forms.\n            if index is not None and index < initial_form_count:\n                form.fields[ORDERING_FIELD_NAME] = IntegerField(\n                    label=_('Order'),\n                    initial=index + 1,\n                    required=False,\n                    widget=self.get_ordering_widget(),\n                )\n            else:\n                form.fields[ORDERING_FIELD_NAME] = IntegerField(\n                    label=_('Order'),\n                    required=False,\n                    widget=self.get_ordering_widget(),\n                )\n        if self.can_delete and (self.can_delete_extra or index < initial_form_count):\n            form.fields[DELETION_FIELD_NAME] = BooleanField(label=_('Delete'), required=False)\n", "answer": "C"}
{"uuid": "6eb8cd7c-d460-4d13-9b02-29622919fb1c", "issue_statement": "QuerySet.defer() doesn't clear deferred field when chaining with only().\nDescription\n\t\nConsidering a simple Company model with four fields: id, name, trade_number and country. If we evaluate a queryset containing a .defer() following a .only(), the generated sql query selects unexpected fields. For example: \nCompany.objects.only(\"name\").defer(\"name\")\nloads all the fields with the following query:\nSELECT \"company\".\"id\", \"company\".\"name\", \"company\".\"trade_number\", \"company\".\"country\" FROM \"company\"\nand \nCompany.objects.only(\"name\").defer(\"name\").defer(\"country\")\nalso loads all the fields with the same query:\nSELECT \"company\".\"id\", \"company\".\"name\", \"company\".\"trade_number\", \"company\".\"country\" FROM \"company\"\nIn those two cases, i would expect the sql query to be:\nSELECT \"company\".\"id\" FROM \"company\"\nIn the following example, we get the expected behavior:\nCompany.objects.only(\"name\", \"country\").defer(\"name\")\nonly loads \"id\" and \"country\" fields with the following query:\nSELECT \"company\".\"id\", \"company\".\"country\" FROM \"company\"", "choices": "(A)         \"\"\"\n        Add the given list of model field names to the set of fields to\n        retrieve when the SQL is executed (\"immediate loading\" fields). The\n        field names replace any existing immediate loading field names. If\n        there are field names already specified for deferred loading, remove\n        those names from the new field_names before storing the new names\n        for immediate loading. (That is, immediate loading overrides any\n        existing immediate values, but respects existing deferrals.)\n        \"\"\"\n        existing, defer = self.deferred_loading\n        field_names = set(field_names)\n        if 'pk' in field_names:\n(B)         existing, defer = self.deferred_loading\n        if defer:\n            # Add to existing deferred names.\n            self.deferred_loading = existing.union(field_names), True\n        else:\n            # Remove names from the set of any existing \"immediate load\" names.\n            self.deferred_loading = existing.difference(field_names), False\n\n    def add_immediate_loading(self, field_names):\n        \"\"\"\n        Add the given list of model field names to the set of fields to\n        retrieve when the SQL is executed (\"immediate loading\" fields). The\n(C)             self.deferred_loading = existing.union(field_names), True\n        else:\n            # Remove names from the set of any existing \"immediate load\" names.\n            self.deferred_loading = existing.difference(field_names), False\n\n    def add_immediate_loading(self, field_names):\n        \"\"\"\n        Add the given list of model field names to the set of fields to\n        retrieve when the SQL is executed (\"immediate loading\" fields). The\n        field names replace any existing immediate loading field names. If\n        there are field names already specified for deferred loading, remove\n        those names from the new field_names before storing the new names\n(D)             self.deferred_loading = existing.difference(field_names), False\n\n    def add_immediate_loading(self, field_names):\n        \"\"\"\n        Add the given list of model field names to the set of fields to\n        retrieve when the SQL is executed (\"immediate loading\" fields). The\n        field names replace any existing immediate loading field names. If\n        there are field names already specified for deferred loading, remove\n        those names from the new field_names before storing the new names\n        for immediate loading. (That is, immediate loading overrides any\n        existing immediate values, but respects existing deferrals.)\n        \"\"\"", "answer": "C"}
{"uuid": "56de014d-f137-494f-a8a3-a0c062eb51cb", "issue_statement": "Missing call `make_hashable` on `through_fields` in `ManyToManyRel`\nDescription\n\t\nIn 3.2 identity property has been added to all ForeignObjectRel to make it possible to compare them. A hash is derived from said identity and it's possible because identity is a tuple. To make limit_choices_to hashable (one of this tuple elements), \u200bthere's a call to make_hashable.\nIt happens that through_fields can be a list. In such case, this make_hashable call is missing in \u200bManyToManyRel.\nFor some reason it only fails on checking proxy model. I think proxy models have 29 checks and normal ones 24, hence the issue, but that's just a guess.\nMinimal repro:\nclass Parent(models.Model):\n\tname = models.CharField(max_length=256)\nclass ProxyParent(Parent):\n\tclass Meta:\n\t\tproxy = True\nclass Child(models.Model):\n\tparent = models.ForeignKey(Parent, on_delete=models.CASCADE)\n\tmany_to_many_field = models.ManyToManyField(\n\t\tto=Parent,\n\t\tthrough=\"ManyToManyModel\",\n\t\tthrough_fields=['child', 'parent'],\n\t\trelated_name=\"something\"\n\t)\nclass ManyToManyModel(models.Model):\n\tparent = models.ForeignKey(Parent, on_delete=models.CASCADE, related_name='+')\n\tchild = models.ForeignKey(Child, on_delete=models.CASCADE, related_name='+')\n\tsecond_child = models.ForeignKey(Child, on_delete=models.CASCADE, null=True, default=None)\nWhich will result in \n File \"manage.py\", line 23, in <module>\n\tmain()\n File \"manage.py\", line 19, in main\n\texecute_from_command_line(sys.argv)\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/management/__init__.py\", line 419, in execute_from_command_line\n\tutility.execute()\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/management/__init__.py\", line 413, in execute\n\tself.fetch_command(subcommand).run_from_argv(self.argv)\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/management/base.py\", line 354, in run_from_argv\n\tself.execute(*args, **cmd_options)\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/management/base.py\", line 393, in execute\n\tself.check()\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/management/base.py\", line 419, in check\n\tall_issues = checks.run_checks(\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/checks/registry.py\", line 76, in run_checks\n\tnew_errors = check(app_configs=app_configs, databases=databases)\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/checks/model_checks.py\", line 34, in check_all_models\n\terrors.extend(model.check(**kwargs))\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/db/models/base.py\", line 1277, in check\n\t*cls._check_field_name_clashes(),\n File \"/home/tom/PycharmProjects/djangbroken_m2m_projectProject/venv/lib/python3.8/site-packages/django/db/models/base.py\", line 1465, in _check_field_name_clashes\n\tif f not in used_fields:\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/db/models/fields/reverse_related.py\", line 140, in __hash__\n\treturn hash(self.identity)\nTypeError: unhashable type: 'list'\nSolution: Add missing make_hashable call on self.through_fields in ManyToManyRel.\nMissing call `make_hashable` on `through_fields` in `ManyToManyRel`\nDescription\n\t\nIn 3.2 identity property has been added to all ForeignObjectRel to make it possible to compare them. A hash is derived from said identity and it's possible because identity is a tuple. To make limit_choices_to hashable (one of this tuple elements), \u200bthere's a call to make_hashable.\nIt happens that through_fields can be a list. In such case, this make_hashable call is missing in \u200bManyToManyRel.\nFor some reason it only fails on checking proxy model. I think proxy models have 29 checks and normal ones 24, hence the issue, but that's just a guess.\nMinimal repro:\nclass Parent(models.Model):\n\tname = models.CharField(max_length=256)\nclass ProxyParent(Parent):\n\tclass Meta:\n\t\tproxy = True\nclass Child(models.Model):\n\tparent = models.ForeignKey(Parent, on_delete=models.CASCADE)\n\tmany_to_many_field = models.ManyToManyField(\n\t\tto=Parent,\n\t\tthrough=\"ManyToManyModel\",\n\t\tthrough_fields=['child', 'parent'],\n\t\trelated_name=\"something\"\n\t)\nclass ManyToManyModel(models.Model):\n\tparent = models.ForeignKey(Parent, on_delete=models.CASCADE, related_name='+')\n\tchild = models.ForeignKey(Child, on_delete=models.CASCADE, related_name='+')\n\tsecond_child = models.ForeignKey(Child, on_delete=models.CASCADE, null=True, default=None)\nWhich will result in \n File \"manage.py\", line 23, in <module>\n\tmain()\n File \"manage.py\", line 19, in main\n\texecute_from_command_line(sys.argv)\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/management/__init__.py\", line 419, in execute_from_command_line\n\tutility.execute()\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/management/__init__.py\", line 413, in execute\n\tself.fetch_command(subcommand).run_from_argv(self.argv)\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/management/base.py\", line 354, in run_from_argv\n\tself.execute(*args, **cmd_options)\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/management/base.py\", line 393, in execute\n\tself.check()\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/management/base.py\", line 419, in check\n\tall_issues = checks.run_checks(\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/checks/registry.py\", line 76, in run_checks\n\tnew_errors = check(app_configs=app_configs, databases=databases)\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/core/checks/model_checks.py\", line 34, in check_all_models\n\terrors.extend(model.check(**kwargs))\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/db/models/base.py\", line 1277, in check\n\t*cls._check_field_name_clashes(),\n File \"/home/tom/PycharmProjects/djangbroken_m2m_projectProject/venv/lib/python3.8/site-packages/django/db/models/base.py\", line 1465, in _check_field_name_clashes\n\tif f not in used_fields:\n File \"/home/tom/PycharmProjects/broken_m2m_project/venv/lib/python3.8/site-packages/django/db/models/fields/reverse_related.py\", line 140, in __hash__\n\treturn hash(self.identity)\nTypeError: unhashable type: 'list'\nSolution: Add missing make_hashable call on self.through_fields in ManyToManyRel.", "choices": "(A)             self.db_constraint,\n        )\n\n    def get_related_field(self):\n        \"\"\"\n        Return the field in the 'to' object to which this relationship is tied.\n        Provided for symmetry with ManyToOneRel.\n(B)             self.through,\n            self.through_fields,\n            self.db_constraint,\n        )\n\n    def get_related_field(self):\n        \"\"\"\n(C) \n    @property\n    def identity(self):\n        return super().identity + (\n            self.through,\n            self.through_fields,\n            self.db_constraint,\n(D)     def identity(self):\n        return super().identity + (\n            self.through,\n            self.through_fields,\n            self.db_constraint,\n        )\n", "answer": "D"}
{"uuid": "5cdd7a0d-fe3b-4776-8288-d805b1eebea7", "issue_statement": "Prevent developers from defining a related_name on symmetrical ManyToManyFields\nDescription\n\t\nIn ManyToManyField, if the symmetrical argument is passed, or if it's a self-referential ManyToMany relationship, the related field on the target model is not created. However, if a developer passes in the related_name not understanding this fact, they may be confused until they find the information about symmetrical relationship. Thus, it is proposed to raise an error when the user defines a ManyToManyField in this condition.", "choices": "(A) \n    def _check_relationship_model(self, from_model=None, **kwargs):\n        if hasattr(self.remote_field.through, '_meta'):\n            qualified_model_name = \"%s.%s\" % (\n                self.remote_field.through._meta.app_label, self.remote_field.through.__name__)\n        else:\n            qualified_model_name = self.remote_field.through\n\n        errors = []\n\n        if self.remote_field.through not in self.opts.apps.get_models(include_auto_created=True):\n            # The relationship model is not installed.\n            errors.append(\n                checks.Error(\n                    \"Field specifies a many-to-many relation through model \"\n                    \"'%s', which has not been installed.\" % qualified_model_name,\n(B)                     'limit_choices_to has no effect on ManyToManyField '\n                    'with a through model.',\n                    obj=self,\n                    id='fields.W343',\n                )\n            )\n\n        return warnings\n\n    def _check_relationship_model(self, from_model=None, **kwargs):\n        if hasattr(self.remote_field.through, '_meta'):\n            qualified_model_name = \"%s.%s\" % (\n                self.remote_field.through._meta.app_label, self.remote_field.through.__name__)\n        else:\n            qualified_model_name = self.remote_field.through\n\n(C)                 )\n            )\n\n        return warnings\n\n    def _check_relationship_model(self, from_model=None, **kwargs):\n        if hasattr(self.remote_field.through, '_meta'):\n            qualified_model_name = \"%s.%s\" % (\n                self.remote_field.through._meta.app_label, self.remote_field.through.__name__)\n        else:\n            qualified_model_name = self.remote_field.through\n\n        errors = []\n\n        if self.remote_field.through not in self.opts.apps.get_models(include_auto_created=True):\n            # The relationship model is not installed.\n(D)                 self.remote_field.through._meta.app_label, self.remote_field.through.__name__)\n        else:\n            qualified_model_name = self.remote_field.through\n\n        errors = []\n\n        if self.remote_field.through not in self.opts.apps.get_models(include_auto_created=True):\n            # The relationship model is not installed.\n            errors.append(\n                checks.Error(\n                    \"Field specifies a many-to-many relation through model \"\n                    \"'%s', which has not been installed.\" % qualified_model_name,\n                    obj=self,\n                    id='fields.E331',\n                )\n            )", "answer": "C"}
{"uuid": "5f900177-b665-4ae9-91b4-bbf092770f79", "issue_statement": "Refactor AutocompleteJsonView to support extra fields in autocomplete response\nDescription\n\t \n\t\t(last modified by mrts)\n\t \nAdding data attributes to items in ordinary non-autocomplete foreign key fields that use forms.widgets.Select-based widgets is relatively easy. This enables powerful and dynamic admin site customizations where fields from related models are updated immediately when users change the selected item.\nHowever, adding new attributes to autocomplete field results currently requires extending contrib.admin.views.autocomplete.AutocompleteJsonView and fully overriding the AutocompleteJsonView.get() method. Here's an example:\nclass MyModelAdmin(admin.ModelAdmin):\n\tdef get_urls(self):\n\t\treturn [\n\t\t\tpath('autocomplete/', CustomAutocompleteJsonView.as_view(admin_site=self.admin_site))\n\t\t\tif url.pattern.match('autocomplete/')\n\t\t\telse url for url in super().get_urls()\n\t\t]\nclass CustomAutocompleteJsonView(AutocompleteJsonView):\n\tdef get(self, request, *args, **kwargs):\n\t\tself.term, self.model_admin, self.source_field, to_field_name = self.process_request(request)\n\t\tif not self.has_perm(request):\n\t\t\traise PermissionDenied\n\t\tself.object_list = self.get_queryset()\n\t\tcontext = self.get_context_data()\n\t\treturn JsonResponse({\n\t\t\t'results': [\n\t\t\t\t{'id': str(getattr(obj, to_field_name)), 'text': str(obj), 'notes': obj.notes} # <-- customization here\n\t\t\t\tfor obj in context['object_list']\n\t\t\t],\n\t\t\t'pagination': {'more': context['page_obj'].has_next()},\n\t\t})\nThe problem with this is that as AutocompleteJsonView.get() keeps evolving, there's quite a lot of maintenance overhead required to catch up.\nThe solutions is simple, side-effect- and risk-free: adding a result customization extension point to get() by moving the lines that construct the results inside JsonResponse constructor to a separate method. So instead of\n\t\treturn JsonResponse({\n\t\t\t'results': [\n\t\t\t\t{'id': str(getattr(obj, to_field_name)), 'text': str(obj)}\n\t\t\t\tfor obj in context['object_list']\n\t\t\t],\n\t\t\t'pagination': {'more': context['page_obj'].has_next()},\n\t\t})\nthere would be\n\t\treturn JsonResponse({\n\t\t\t'results': [\n\t\t\t\tself.serialize_result(obj, to_field_name) for obj in context['object_list']\n\t\t\t],\n\t\t\t'pagination': {'more': context['page_obj'].has_next()},\n\t\t})\nwhere serialize_result() contains the original object to dictionary conversion code that would be now easy to override:\ndef serialize_result(self, obj, to_field_name):\n\treturn {'id': str(getattr(obj, to_field_name)), 'text': str(obj)}\nThe example CustomAutocompleteJsonView from above would now become succinct and maintainable:\nclass CustomAutocompleteJsonView(AutocompleteJsonView):\n\tdef serialize_result(self, obj, to_field_name):\n\t\treturn super.serialize_result(obj, to_field_name) | {'notes': obj.notes}\nWhat do you think, is this acceptable? I'm more than happy to provide the patch.", "choices": "(A)                 {'id': str(getattr(obj, to_field_name)), 'text': str(obj)}\n                for obj in context['object_list']\n            ],\n            'pagination': {'more': context['page_obj'].has_next()},\n        })\n\n    def get_paginator(self, *args, **kwargs):\n        \"\"\"Use the ModelAdmin's paginator.\"\"\"\n        return self.model_admin.get_paginator(self.request, *args, **kwargs)\n\n    def get_queryset(self):\n        \"\"\"Return queryset based on ModelAdmin.get_search_results().\"\"\"\n        qs = self.model_admin.get_queryset(self.request)\n        qs = qs.complex_filter(self.source_field.get_limit_choices_to())\n        qs, search_use_distinct = self.model_admin.get_search_results(self.request, qs, self.term)\n        if search_use_distinct:\n            qs = qs.distinct()\n        return qs\n\n    def process_request(self, request):\n        \"\"\"\n        Validate request integrity, extract and return request parameters.\n\n        Since the subsequent view permission check requires the target model\n        admin, which is determined here, raise PermissionDenied if the\n        requested app, model or field are malformed.\n\n        Raise Http404 if the target model admin is not configured properly with\n        search_fields.\n        \"\"\"\n        term = request.GET.get('term', '')\n        try:\n            app_label = request.GET['app_label']\n            model_name = request.GET['model_name']\n            field_name = request.GET['field_name']\n(B) from django.core.exceptions import FieldDoesNotExist, PermissionDenied\nfrom django.http import Http404, JsonResponse\nfrom django.views.generic.list import BaseListView\n\n\nclass AutocompleteJsonView(BaseListView):\n    \"\"\"Handle AutocompleteWidget's AJAX requests for data.\"\"\"\n    paginate_by = 20\n    admin_site = None\n\n    def get(self, request, *args, **kwargs):\n        \"\"\"\n        Return a JsonResponse with search results of the form:\n        {\n            results: [{id: \"123\" text: \"foo\"}],\n            pagination: {more: true}\n        }\n        \"\"\"\n        self.term, self.model_admin, self.source_field, to_field_name = self.process_request(request)\n\n        if not self.has_perm(request):\n            raise PermissionDenied\n\n        self.object_list = self.get_queryset()\n        context = self.get_context_data()\n        return JsonResponse({\n            'results': [\n                {'id': str(getattr(obj, to_field_name)), 'text': str(obj)}\n                for obj in context['object_list']\n            ],\n            'pagination': {'more': context['page_obj'].has_next()},\n        })\n\n    def get_paginator(self, *args, **kwargs):\n        \"\"\"Use the ModelAdmin's paginator.\"\"\"\n(C) \n    def get(self, request, *args, **kwargs):\n        \"\"\"\n        Return a JsonResponse with search results of the form:\n        {\n            results: [{id: \"123\" text: \"foo\"}],\n            pagination: {more: true}\n        }\n        \"\"\"\n        self.term, self.model_admin, self.source_field, to_field_name = self.process_request(request)\n\n        if not self.has_perm(request):\n            raise PermissionDenied\n\n        self.object_list = self.get_queryset()\n        context = self.get_context_data()\n        return JsonResponse({\n            'results': [\n                {'id': str(getattr(obj, to_field_name)), 'text': str(obj)}\n                for obj in context['object_list']\n            ],\n            'pagination': {'more': context['page_obj'].has_next()},\n        })\n\n    def get_paginator(self, *args, **kwargs):\n        \"\"\"Use the ModelAdmin's paginator.\"\"\"\n        return self.model_admin.get_paginator(self.request, *args, **kwargs)\n\n    def get_queryset(self):\n        \"\"\"Return queryset based on ModelAdmin.get_search_results().\"\"\"\n        qs = self.model_admin.get_queryset(self.request)\n        qs = qs.complex_filter(self.source_field.get_limit_choices_to())\n        qs, search_use_distinct = self.model_admin.get_search_results(self.request, qs, self.term)\n        if search_use_distinct:\n            qs = qs.distinct()\n(D)         self.term, self.model_admin, self.source_field, to_field_name = self.process_request(request)\n\n        if not self.has_perm(request):\n            raise PermissionDenied\n\n        self.object_list = self.get_queryset()\n        context = self.get_context_data()\n        return JsonResponse({\n            'results': [\n                {'id': str(getattr(obj, to_field_name)), 'text': str(obj)}\n                for obj in context['object_list']\n            ],\n            'pagination': {'more': context['page_obj'].has_next()},\n        })\n\n    def get_paginator(self, *args, **kwargs):\n        \"\"\"Use the ModelAdmin's paginator.\"\"\"\n        return self.model_admin.get_paginator(self.request, *args, **kwargs)\n\n    def get_queryset(self):\n        \"\"\"Return queryset based on ModelAdmin.get_search_results().\"\"\"\n        qs = self.model_admin.get_queryset(self.request)\n        qs = qs.complex_filter(self.source_field.get_limit_choices_to())\n        qs, search_use_distinct = self.model_admin.get_search_results(self.request, qs, self.term)\n        if search_use_distinct:\n            qs = qs.distinct()\n        return qs\n\n    def process_request(self, request):\n        \"\"\"\n        Validate request integrity, extract and return request parameters.\n\n        Since the subsequent view permission check requires the target model\n        admin, which is determined here, raise PermissionDenied if the\n        requested app, model or field are malformed.", "answer": "C"}
{"uuid": "e4564fd9-4d6c-4bc7-ac2b-78e824ecf62a", "issue_statement": "method_decorator() should preserve wrapper assignments\nDescription\n\t\nthe function that is passed to the decorator is a partial object and does not have any of the attributes expected from a function i.e. __name__, __module__ etc...\nconsider the following case\ndef logger(func):\n\t@wraps(func)\n\tdef inner(*args, **kwargs):\n\t\ttry:\n\t\t\tresult = func(*args, **kwargs)\n\t\texcept Exception as e:\n\t\t\tresult = str(e)\n\t\tfinally:\n\t\t\tlogger.debug(f\"{func.__name__} called with args: {args} and kwargs: {kwargs} resulting: {result}\")\n\treturn inner\nclass Test:\n\t@method_decorator(logger)\n\tdef hello_world(self):\n\t\treturn \"hello\"\nTest().test_method()\nThis results in the following exception\nAttributeError: 'functools.partial' object has no attribute '__name__'", "choices": "(A)         # attributes can't be set on bound method objects, only on functions.\n        bound_method = partial(method.__get__(self, type(self)))\n        for dec in decorators:\n            bound_method = dec(bound_method)\n        return bound_method(*args, **kwargs)\n\n    # Copy any attributes that a decorator adds to the function it decorates.\n(B)     def _wrapper(self, *args, **kwargs):\n        # bound_method has the signature that 'decorator' expects i.e. no\n        # 'self' argument, but it's a closure over self so it can call\n        # 'func'. Also, wrap method.__get__() in a function because new\n        # attributes can't be set on bound method objects, only on functions.\n        bound_method = partial(method.__get__(self, type(self)))\n        for dec in decorators:\n(C)         # 'self' argument, but it's a closure over self so it can call\n        # 'func'. Also, wrap method.__get__() in a function because new\n        # attributes can't be set on bound method objects, only on functions.\n        bound_method = partial(method.__get__(self, type(self)))\n        for dec in decorators:\n            bound_method = dec(bound_method)\n        return bound_method(*args, **kwargs)\n(D)         for dec in decorators:\n            bound_method = dec(bound_method)\n        return bound_method(*args, **kwargs)\n\n    # Copy any attributes that a decorator adds to the function it decorates.\n    for dec in decorators:\n        _update_method_wrapper(_wrapper, dec)", "answer": "C"}
{"uuid": "53e0a185-724f-48b7-9622-e6a98f51b0f2", "issue_statement": "Wrong URL generated by get_admin_url for readonly field in custom Admin Site\nDescription\n\t\nWhen a model containing a ForeignKey field is viewed (or edited) in a custom Admin Site, and that ForeignKey field is listed in readonly_fields, the url generated for the link is /admin/... instead of /custom-admin/....\nThis appears to be caused by the following line in django.contrib.admin.helpers get_admin_url:\nurl = reverse(url_name, args=[quote(remote_obj.pk)])\nOther parts of the admin use the current_app keyword parameter to identify the correct current name of the Admin Site. (See django.contrib.admin.options.ModelAdmin response_add as just one example)\nI have been able to correct this specific issue by replacing the above line with:\nurl = reverse(\n\turl_name,\n\targs=[quote(remote_obj.pk)],\n\tcurrent_app=self.model_admin.admin_site.name\n)\nHowever, I don't know if there are any side effects and I have not yet run the full suite of tests on this. Mostly looking for feedback whether I'm on the right track.", "choices": "(A)             return str(remote_obj)\n\n    def contents(self):\n        from django.contrib.admin.templatetags.admin_list import _boolean_icon\n        field, obj, model_admin = self.field['field'], self.form.instance, self.model_admin\n        try:\n            f, attr, value = lookup_field(field, obj, model_admin)\n        except (AttributeError, ValueError, ObjectDoesNotExist):\n            result_repr = self.empty_value_display\n        else:\n            if field in self.form.fields:\n(B)             url = reverse(url_name, args=[quote(remote_obj.pk)])\n            return format_html('<a href=\"{}\">{}</a>', url, remote_obj)\n        except NoReverseMatch:\n            return str(remote_obj)\n\n    def contents(self):\n        from django.contrib.admin.templatetags.admin_list import _boolean_icon\n        field, obj, model_admin = self.field['field'], self.form.instance, self.model_admin\n        try:\n            f, attr, value = lookup_field(field, obj, model_admin)\n        except (AttributeError, ValueError, ObjectDoesNotExist):\n(C)     def get_admin_url(self, remote_field, remote_obj):\n        url_name = 'admin:%s_%s_change' % (\n            remote_field.model._meta.app_label,\n            remote_field.model._meta.model_name,\n        )\n        try:\n            url = reverse(url_name, args=[quote(remote_obj.pk)])\n            return format_html('<a href=\"{}\">{}</a>', url, remote_obj)\n        except NoReverseMatch:\n            return str(remote_obj)\n\n(D)             remote_field.model._meta.model_name,\n        )\n        try:\n            url = reverse(url_name, args=[quote(remote_obj.pk)])\n            return format_html('<a href=\"{}\">{}</a>', url, remote_obj)\n        except NoReverseMatch:\n            return str(remote_obj)\n\n    def contents(self):\n        from django.contrib.admin.templatetags.admin_list import _boolean_icon\n        field, obj, model_admin = self.field['field'], self.form.instance, self.model_admin", "answer": "D"}
{"uuid": "06123ea4-7927-411c-9915-e58c5039c852", "issue_statement": "ModelChoiceIteratorValue is not hashable.\nDescription\n\t\nRecently I migrated from Django 3.0 to Django 3.1. In my code, I add custom data-* attributes to the select widget options. After the upgrade some of those options broke. Error is {TypeError}unhashable type: 'ModelChoiceIteratorValue'.\nExample (this one breaks):\n\tdef create_option(self, name, value, label, selected, index, subindex=None, attrs=None):\n\t\tcontext = super().create_option(name, value, label, selected, index, subindex, attrs)\n\t\tif not value:\n\t\t\treturn context\n\t\tif value in self.show_fields: # This is a dict {1: ['first_name', 'last_name']}\n\t\t\tcontext['attrs']['data-fields'] = json.dumps(self.show_fields[value])\nHowever, working with arrays is not an issue:\n\tdef create_option(self, name, value, label, selected, index, subindex=None, attrs=None):\n\t\tcontext = super().create_option(name, value, label, selected, index, subindex, attrs)\n\t\tif not value:\n\t\t\treturn context\n\t\tif value in allowed_values: # This is an array [1, 2]\n\t\t\t...", "choices": "(A)     def __eq__(self, other):\n        if isinstance(other, ModelChoiceIteratorValue):\n            other = other.value\n        return self.value == other\n\n\nclass ModelChoiceIterator:\n    def __init__(self, field):\n        self.field = field\n(B)     def __str__(self):\n        return str(self.value)\n\n    def __eq__(self, other):\n        if isinstance(other, ModelChoiceIteratorValue):\n            other = other.value\n        return self.value == other\n\n\n(C)             other = other.value\n        return self.value == other\n\n\nclass ModelChoiceIterator:\n    def __init__(self, field):\n        self.field = field\n        self.queryset = field.queryset\n\n(D)         self.value = value\n        self.instance = instance\n\n    def __str__(self):\n        return str(self.value)\n\n    def __eq__(self, other):\n        if isinstance(other, ModelChoiceIteratorValue):\n            other = other.value", "answer": "B"}
{"uuid": "81e1026d-c0c7-42ac-8722-af942e7fbd31", "issue_statement": "Remaking table with unique constraint crashes on SQLite.\nDescription\n\t\nIn Django 4.0a1, this model:\nclass Tag(models.Model):\n\tname = models.SlugField(help_text=\"The tag key.\")\n\tvalue = models.CharField(max_length=150, help_text=\"The tag value.\")\n\tclass Meta:\n\t\tordering = [\"name\", \"value\"]\n\t\tconstraints = [\n\t\t\tmodels.UniqueConstraint(\n\t\t\t\t\"name\",\n\t\t\t\t\"value\",\n\t\t\t\tname=\"unique_name_value\",\n\t\t\t)\n\t\t]\n\tdef __str__(self):\n\t\treturn f\"{self.name}={self.value}\"\nwith these migrations, using sqlite:\nclass Migration(migrations.Migration):\n\tinitial = True\n\tdependencies = [\n\t]\n\toperations = [\n\t\tmigrations.CreateModel(\n\t\t\tname='Tag',\n\t\t\tfields=[\n\t\t\t\t('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n\t\t\t\t('name', models.SlugField(help_text='The tag key.')),\n\t\t\t\t('value', models.CharField(help_text='The tag value.', max_length=200)),\n\t\t\t],\n\t\t\toptions={\n\t\t\t\t'ordering': ['name', 'value'],\n\t\t\t},\n\t\t),\n\t\tmigrations.AddConstraint(\n\t\t\tmodel_name='tag',\n\t\t\tconstraint=models.UniqueConstraint(django.db.models.expressions.F('name'), django.db.models.expressions.F('value'), name='unique_name_value'),\n\t\t),\n\t]\nclass Migration(migrations.Migration):\n\tdependencies = [\n\t\t('myapp', '0001_initial'),\n\t]\n\toperations = [\n\t\tmigrations.AlterField(\n\t\t\tmodel_name='tag',\n\t\t\tname='value',\n\t\t\tfield=models.CharField(help_text='The tag value.', max_length=150),\n\t\t),\n\t]\nraises this error:\nmanage.py migrate\nOperations to perform:\n Apply all migrations: admin, auth, contenttypes, myapp, sessions\nRunning migrations:\n Applying myapp.0002_alter_tag_value...python-BaseException\nTraceback (most recent call last):\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\utils.py\", line 84, in _execute\n\treturn self.cursor.execute(sql, params)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\sqlite3\\base.py\", line 416, in execute\n\treturn Database.Cursor.execute(self, query, params)\nsqlite3.OperationalError: the \".\" operator prohibited in index expressions\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\core\\management\\base.py\", line 373, in run_from_argv\n\tself.execute(*args, **cmd_options)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\core\\management\\base.py\", line 417, in execute\n\toutput = self.handle(*args, **options)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\core\\management\\base.py\", line 90, in wrapped\n\tres = handle_func(*args, **kwargs)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\core\\management\\commands\\migrate.py\", line 253, in handle\n\tpost_migrate_state = executor.migrate(\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\migrations\\executor.py\", line 126, in migrate\n\tstate = self._migrate_all_forwards(state, plan, full_plan, fake=fake, fake_initial=fake_initial)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\migrations\\executor.py\", line 156, in _migrate_all_forwards\n\tstate = self.apply_migration(state, migration, fake=fake, fake_initial=fake_initial)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\migrations\\executor.py\", line 236, in apply_migration\n\tstate = migration.apply(state, schema_editor)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\migrations\\migration.py\", line 125, in apply\n\toperation.database_forwards(self.app_label, schema_editor, old_state, project_state)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\migrations\\operations\\fields.py\", line 225, in database_forwards\n\tschema_editor.alter_field(from_model, from_field, to_field)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\sqlite3\\schema.py\", line 140, in alter_field\n\tsuper().alter_field(model, old_field, new_field, strict=strict)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\base\\schema.py\", line 618, in alter_field\n\tself._alter_field(model, old_field, new_field, old_type, new_type,\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\sqlite3\\schema.py\", line 362, in _alter_field\n\tself._remake_table(model, alter_field=(old_field, new_field))\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\sqlite3\\schema.py\", line 303, in _remake_table\n\tself.execute(sql)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\base\\schema.py\", line 151, in execute\n\tcursor.execute(sql, params)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\utils.py\", line 98, in execute\n\treturn super().execute(sql, params)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\utils.py\", line 66, in execute\n\treturn self._execute_with_wrappers(sql, params, many=False, executor=self._execute)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\utils.py\", line 75, in _execute_with_wrappers\n\treturn executor(sql, params, many, context)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\utils.py\", line 84, in _execute\n\treturn self.cursor.execute(sql, params)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\utils.py\", line 90, in __exit__\n\traise dj_exc_value.with_traceback(traceback) from exc_value\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\utils.py\", line 84, in _execute\n\treturn self.cursor.execute(sql, params)\n File \"D:\\Projects\\Development\\sqliteerror\\.venv\\lib\\site-packages\\django\\db\\backends\\sqlite3\\base.py\", line 416, in execute\n\treturn Database.Cursor.execute(self, query, params)\ndjango.db.utils.OperationalError: the \".\" operator prohibited in index expressions", "choices": "(A)         self.columns = []\n        for col in self.compiler.query._gen_cols([expressions]):\n            col.alias = new_table\n        self.expressions = expressions\n        super().rename_table_references(old_table, new_table)\n\n    def rename_column_references(self, table, old_column, new_column):\n(B)     def rename_table_references(self, old_table, new_table):\n        if self.table != old_table:\n            return\n        expressions = deepcopy(self.expressions)\n        self.columns = []\n        for col in self.compiler.query._gen_cols([expressions]):\n            col.alias = new_table\n(C)         super().__init__(table, columns)\n\n    def rename_table_references(self, old_table, new_table):\n        if self.table != old_table:\n            return\n        expressions = deepcopy(self.expressions)\n        self.columns = []\n(D)             return\n        expressions = deepcopy(self.expressions)\n        self.columns = []\n        for col in self.compiler.query._gen_cols([expressions]):\n            col.alias = new_table\n        self.expressions = expressions\n        super().rename_table_references(old_table, new_table)", "answer": "B"}
{"uuid": "364dbc88-f804-491d-a3ca-35806919e769", "issue_statement": "RenameModel with db_table should be a noop.\nDescription\n\t\nA RenameModel operation that already has db_table defined must be a noop.\nIn Postgres, it drops and recreates foreign key constraints. In sqlite it recreates the table (as expected for a table renaming).", "choices": "(A)     def state_forwards(self, app_label, state):\n        state.rename_model(app_label, self.old_name, self.new_name)\n\n    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n        new_model = to_state.apps.get_model(app_label, self.new_name)\n        if self.allow_migrate_model(schema_editor.connection.alias, new_model):\n            old_model = from_state.apps.get_model(app_label, self.old_name)\n            # Move the main table\n            schema_editor.alter_db_table(\n                new_model,\n                old_model._meta.db_table,\n                new_model._meta.db_table,\n            )\n(B)         new_model = to_state.apps.get_model(app_label, self.new_name)\n        if self.allow_migrate_model(schema_editor.connection.alias, new_model):\n            old_model = from_state.apps.get_model(app_label, self.old_name)\n            # Move the main table\n            schema_editor.alter_db_table(\n                new_model,\n                old_model._meta.db_table,\n                new_model._meta.db_table,\n            )\n            # Alter the fields pointing to us\n            for related_object in old_model._meta.related_objects:\n                if related_object.related_model == old_model:\n                    model = new_model\n(C)             schema_editor.alter_db_table(\n                new_model,\n                old_model._meta.db_table,\n                new_model._meta.db_table,\n            )\n            # Alter the fields pointing to us\n            for related_object in old_model._meta.related_objects:\n                if related_object.related_model == old_model:\n                    model = new_model\n                    related_key = (app_label, self.new_name_lower)\n                else:\n                    model = related_object.related_model\n                    related_key = (\n(D)                 new_model._meta.db_table,\n            )\n            # Alter the fields pointing to us\n            for related_object in old_model._meta.related_objects:\n                if related_object.related_model == old_model:\n                    model = new_model\n                    related_key = (app_label, self.new_name_lower)\n                else:\n                    model = related_object.related_model\n                    related_key = (\n                        related_object.related_model._meta.app_label,\n                        related_object.related_model._meta.model_name,\n                    )", "answer": "B"}
{"uuid": "a1db1a19-aa2f-48f6-8806-d00af6087e96", "issue_statement": "Remove \"for = ...\" from MultiWidget's <label>.\nDescription\n\t\nThe instance from Raw MultiWidget class generate id_for_label like f'{id_}0'\nIt has not sense.\nFor example ChoiceWidget has self.add_id_index and I can decide it myself, how I will see label_id - with or without index.\nI think, it is better to remove completely id_for_label method from MultiWidget Class.", "choices": "(A)             subwidgets.append(widget.get_context(widget_name, widget_value, widget_attrs)['widget'])\n        context['widget']['subwidgets'] = subwidgets\n        return context\n\n    def id_for_label(self, id_):\n        if id_:\n            id_ += '_0'\n(B)             id_ += '_0'\n        return id_\n\n    def value_from_datadict(self, data, files, name):\n        return [\n            widget.value_from_datadict(data, files, name + widget_name)\n            for widget_name, widget in zip(self.widgets_names, self.widgets)\n(C)         return context\n\n    def id_for_label(self, id_):\n        if id_:\n            id_ += '_0'\n        return id_\n\n(D)     def id_for_label(self, id_):\n        if id_:\n            id_ += '_0'\n        return id_\n\n    def value_from_datadict(self, data, files, name):\n        return [", "answer": "C"}
{"uuid": "4d9cde94-fa5e-4602-8d51-b274d4e2d94d", "issue_statement": "URLField throws ValueError instead of ValidationError on clean\nDescription\n\t\nforms.URLField( ).clean('////]@N.AN')\nresults in:\n\tValueError: Invalid IPv6 URL\n\tTraceback (most recent call last):\n\t File \"basic_fuzzer.py\", line 22, in TestOneInput\n\t File \"fuzzers.py\", line 350, in test_forms_URLField\n\t File \"django/forms/fields.py\", line 151, in clean\n\t File \"django/forms/fields.py\", line 136, in run_validators\n\t File \"django/core/validators.py\", line 130, in __call__\n\t File \"urllib/parse.py\", line 440, in urlsplit", "choices": "(A)             raise ValidationError(self.message, code=self.code, params={'value': value})\n\n        # Then check full URL\n        try:\n            super().__call__(value)\n        except ValidationError as e:\n            # Trivial case failed. Try for possible IDN domain\n            if value:\n                try:\n                    scheme, netloc, path, query, fragment = urlsplit(value)\n                except ValueError:  # for example, \"Invalid IPv6 URL\"\n                    raise ValidationError(self.message, code=self.code, params={'value': value})\n                try:\n                    netloc = punycode(netloc)  # IDN -> ACE\n                except UnicodeError:  # invalid domain part\n                    raise e\n                url = urlunsplit((scheme, netloc, path, query, fragment))\n                super().__call__(url)\n            else:\n                raise\n        else:\n            # Now verify IPv6 in the netloc part\n            host_match = re.search(r'^\\[(.+)\\](?::\\d{1,5})?$', urlsplit(value).netloc)\n            if host_match:\n                potential_ip = host_match[1]\n                try:\n                    validate_ipv6_address(potential_ip)\n                except ValidationError:\n                    raise ValidationError(self.message, code=self.code, params={'value': value})\n\n        # The maximum length of a full host name is 253 characters per RFC 1034\n        # section 3.1. It's defined to be 255 bytes or less, but this includes\n        # one byte for the length of the name and one byte for the trailing dot\n        # that's used to indicate absolute names in DNS.\n        if len(urlsplit(value).hostname) > 253:\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n\n\ninteger_validator = RegexValidator(\n(B)         else:\n            # Now verify IPv6 in the netloc part\n            host_match = re.search(r'^\\[(.+)\\](?::\\d{1,5})?$', urlsplit(value).netloc)\n            if host_match:\n                potential_ip = host_match[1]\n                try:\n                    validate_ipv6_address(potential_ip)\n                except ValidationError:\n                    raise ValidationError(self.message, code=self.code, params={'value': value})\n\n        # The maximum length of a full host name is 253 characters per RFC 1034\n        # section 3.1. It's defined to be 255 bytes or less, but this includes\n        # one byte for the length of the name and one byte for the trailing dot\n        # that's used to indicate absolute names in DNS.\n        if len(urlsplit(value).hostname) > 253:\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n\n\ninteger_validator = RegexValidator(\n    _lazy_re_compile(r'^-?\\d+\\Z'),\n    message=_('Enter a valid integer.'),\n    code='invalid',\n)\n\n\ndef validate_integer(value):\n    return integer_validator(value)\n\n\n@deconstructible\nclass EmailValidator:\n    message = _('Enter a valid email address.')\n    code = 'invalid'\n    user_regex = _lazy_re_compile(\n        r\"(^[-!#$%&'*+/=?^_`{}|~0-9A-Z]+(\\.[-!#$%&'*+/=?^_`{}|~0-9A-Z]+)*\\Z\"  # dot-atom\n        r'|^\"([\\001-\\010\\013\\014\\016-\\037!#-\\[\\]-\\177]|\\\\[\\001-\\011\\013\\014\\016-\\177])*\"\\Z)',  # quoted-string\n        re.IGNORECASE)\n    domain_regex = _lazy_re_compile(\n        # max length for domain name labels is 63 characters per RFC 1034\n(C)                 except ValueError:  # for example, \"Invalid IPv6 URL\"\n                    raise ValidationError(self.message, code=self.code, params={'value': value})\n                try:\n                    netloc = punycode(netloc)  # IDN -> ACE\n                except UnicodeError:  # invalid domain part\n                    raise e\n                url = urlunsplit((scheme, netloc, path, query, fragment))\n                super().__call__(url)\n            else:\n                raise\n        else:\n            # Now verify IPv6 in the netloc part\n            host_match = re.search(r'^\\[(.+)\\](?::\\d{1,5})?$', urlsplit(value).netloc)\n            if host_match:\n                potential_ip = host_match[1]\n                try:\n                    validate_ipv6_address(potential_ip)\n                except ValidationError:\n                    raise ValidationError(self.message, code=self.code, params={'value': value})\n\n        # The maximum length of a full host name is 253 characters per RFC 1034\n        # section 3.1. It's defined to be 255 bytes or less, but this includes\n        # one byte for the length of the name and one byte for the trailing dot\n        # that's used to indicate absolute names in DNS.\n        if len(urlsplit(value).hostname) > 253:\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n\n\ninteger_validator = RegexValidator(\n    _lazy_re_compile(r'^-?\\d+\\Z'),\n    message=_('Enter a valid integer.'),\n    code='invalid',\n)\n\n\ndef validate_integer(value):\n    return integer_validator(value)\n\n\n(D)             self.schemes = schemes\n\n    def __call__(self, value):\n        if not isinstance(value, str):\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n        if self.unsafe_chars.intersection(value):\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n        # Check if the scheme is valid.\n        scheme = value.split('://')[0].lower()\n        if scheme not in self.schemes:\n            raise ValidationError(self.message, code=self.code, params={'value': value})\n\n        # Then check full URL\n        try:\n            super().__call__(value)\n        except ValidationError as e:\n            # Trivial case failed. Try for possible IDN domain\n            if value:\n                try:\n                    scheme, netloc, path, query, fragment = urlsplit(value)\n                except ValueError:  # for example, \"Invalid IPv6 URL\"\n                    raise ValidationError(self.message, code=self.code, params={'value': value})\n                try:\n                    netloc = punycode(netloc)  # IDN -> ACE\n                except UnicodeError:  # invalid domain part\n                    raise e\n                url = urlunsplit((scheme, netloc, path, query, fragment))\n                super().__call__(url)\n            else:\n                raise\n        else:\n            # Now verify IPv6 in the netloc part\n            host_match = re.search(r'^\\[(.+)\\](?::\\d{1,5})?$', urlsplit(value).netloc)\n            if host_match:\n                potential_ip = host_match[1]\n                try:\n                    validate_ipv6_address(potential_ip)\n                except ValidationError:\n                    raise ValidationError(self.message, code=self.code, params={'value': value})", "answer": "A"}
{"uuid": "58ab0376-f63a-41f9-bcd4-eee82c1d64c0", "issue_statement": "ExpressionWrapper for ~Q(pk__in=[]) crashes.\nDescription\n\t \n\t\t(last modified by Stefan Brand)\n\t \nProblem Description\nI'm reducing some Q objects (similar to what is described in ticket:32554. Everything is fine for the case where the result is ExpressionWrapper(Q(pk__in=[])). However, when I reduce to ExpressionWrapper(~Q(pk__in=[])) the query breaks.\nSymptoms\nWorking for ExpressionWrapper(Q(pk__in=[]))\nprint(queryset.annotate(foo=ExpressionWrapper(Q(pk__in=[]), output_field=BooleanField())).values(\"foo\").query)\nSELECT 0 AS \"foo\" FROM \"table\"\nNot working for ExpressionWrapper(~Q(pk__in=[]))\nprint(queryset.annotate(foo=ExpressionWrapper(~Q(pk__in=[]), output_field=BooleanField())).values(\"foo\").query)\nSELECT AS \"foo\" FROM \"table\"", "choices": "(A) class CharField(Field):\n    description = _(\"String (up to %(max_length)s)\")\n\n    def __init__(self, *args, db_collation=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.db_collation = db_collation\n        self.validators.append(validators.MaxLengthValidator(self.max_length))\n\n    def check(self, **kwargs):\n        databases = kwargs.get('databases') or []\n        return [\n            *super().check(**kwargs),\n            *self._check_db_collation(databases),\n            *self._check_max_length_attribute(**kwargs),\n        ]\n(B)         super().__init__(*args, **kwargs)\n        self.db_collation = db_collation\n        self.validators.append(validators.MaxLengthValidator(self.max_length))\n\n    def check(self, **kwargs):\n        databases = kwargs.get('databases') or []\n        return [\n            *super().check(**kwargs),\n            *self._check_db_collation(databases),\n            *self._check_max_length_attribute(**kwargs),\n        ]\n\n    def _check_max_length_attribute(self, **kwargs):\n        if self.max_length is None:\n            return [\n(C)             defaults = {'form_class': form_class, 'required': False}\n        return super().formfield(**{**defaults, **kwargs})\n\n\nclass CharField(Field):\n    description = _(\"String (up to %(max_length)s)\")\n\n    def __init__(self, *args, db_collation=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.db_collation = db_collation\n        self.validators.append(validators.MaxLengthValidator(self.max_length))\n\n    def check(self, **kwargs):\n        databases = kwargs.get('databases') or []\n        return [\n(D)             form_class = forms.NullBooleanField if self.null else forms.BooleanField\n            # In HTML checkboxes, 'required' means \"must be checked\" which is\n            # different from the choices case (\"must select some value\").\n            # required=False allows unchecked checkboxes.\n            defaults = {'form_class': form_class, 'required': False}\n        return super().formfield(**{**defaults, **kwargs})\n\n\nclass CharField(Field):\n    description = _(\"String (up to %(max_length)s)\")\n\n    def __init__(self, *args, db_collation=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.db_collation = db_collation\n        self.validators.append(validators.MaxLengthValidator(self.max_length))", "answer": "C"}
{"uuid": "f0db7927-49b9-47ad-ad62-4f77fe7ac70f", "issue_statement": "MigrationRecorder does not obey db_router allow_migrate rules\nDescription\n\t\nHi,\nWe have a multi-db setup. We have one connection that is for the django project, and several connections that talk to other dbs for information (ie models with managed = False). Django should only create tables in the first connection, never in any of the other connections. We have a simple router that does the following: \nclass Router(object):\n\tdef allow_migrate(self, db, model):\n\t\tif db == 'default':\n\t\t\treturn True\n\t\treturn False\nCurrent Behaviour\nWe run our functional tests and the migrate command is called against each connection when the test databases are created (see django/test/runner.py, setup_databases, line 300-ish, which calls django/db/backends/creation.py, create_test_db, line 377-ish)\nWhen this migrate runs, it tries to apply our migrations, which tries to record that a migration has been applied (see django/db/migrations/executor.py, apply_migration, which has several calls to self.recorder.record_applied). \nThe first thing that record_applied does is a call to self.ensure_schema() (see django/db/migrations/recorder.py, record_applied, lien 66-ish). \nensure_schema checks to see if the Migration model is in the tables in the connection. If it does not find the table then it tries to create the table. \nI believe that this is incorrect behaviour when a db_router has been provided. If using the router above, my expectation would be that the table is not created on any connection other than the 'default' connection. Looking at the other methods on the MigrationRecorder, I would expect that there will be similar issues with applied_migrations and record_unapplied.", "choices": "(A)         Migrate the database up to the given targets.\n\n        Django first needs to create all project states before a migration is\n        (un)applied and in a second step run all the database operations.\n        \"\"\"\n        # The django_migrations table must be present to record applied\n        # migrations.\n        self.recorder.ensure_schema()\n\n        if plan is None:\n            plan = self.migration_plan(targets)\n        # Create the forwards plan Django would follow on an empty database\n(B)         # migrations.\n        self.recorder.ensure_schema()\n\n        if plan is None:\n            plan = self.migration_plan(targets)\n        # Create the forwards plan Django would follow on an empty database\n        full_plan = self.migration_plan(self.loader.graph.leaf_nodes(), clean_start=True)\n\n        all_forwards = all(not backwards for mig, backwards in plan)\n        all_backwards = all(backwards for mig, backwards in plan)\n\n        if not plan:\n(C)         (un)applied and in a second step run all the database operations.\n        \"\"\"\n        # The django_migrations table must be present to record applied\n        # migrations.\n        self.recorder.ensure_schema()\n\n        if plan is None:\n            plan = self.migration_plan(targets)\n        # Create the forwards plan Django would follow on an empty database\n        full_plan = self.migration_plan(self.loader.graph.leaf_nodes(), clean_start=True)\n\n        all_forwards = all(not backwards for mig, backwards in plan)\n(D)         if plan is None:\n            plan = self.migration_plan(targets)\n        # Create the forwards plan Django would follow on an empty database\n        full_plan = self.migration_plan(self.loader.graph.leaf_nodes(), clean_start=True)\n\n        all_forwards = all(not backwards for mig, backwards in plan)\n        all_backwards = all(backwards for mig, backwards in plan)\n\n        if not plan:\n            if state is None:\n                # The resulting state should include applied migrations.\n                state = self._create_project_state(with_applied_migrations=True)", "answer": "C"}
{"uuid": "34322b70-fb22-4155-906a-d6ea8300d210", "issue_statement": "Subquery.as_sql() generates invalid SQL.\nDescription\n\t \n\t\t(last modified by M1ha Shvn)\n\t \nSince \u200bthis commit Subquery.as_sql(...) method returns incorrect SQL removing first and last symbols instead of absent breakets. Adding Subquery().query.subquery = True attribute fixes the problem. From my point of view, it should be set in Subquery constructor.\nfrom django.db import connection\nfrom apps.models import App\nq = Subquery(App.objects.all())\nprint(str(q.query))\n# Output SQL is valid:\n# 'SELECT \"apps_app\".\"id\", \"apps_app\".\"name\" FROM \"apps_app\"'\nprint(q.as_sql(q.query.get_compiler('default'), connection))\n# Outptut SQL is invalid (no S letter at the beggining and \" symbol at the end):\n# ('(ELECT \"apps_app\".\"id\", \"apps_app\".\"name\" FROM \"apps_app)', ())\nq.query.subquery = True\nprint(q.as_sql(q.query.get_compiler('default'), connection))\n# Outputs correct result\n('(SELECT \"apps_app\".\"id\", \"apps_app\".\"name\" FROM \"apps_app\")', ())", "choices": "(A) \n    def __init__(self, queryset, output_field=None, **extra):\n        # Allow the usage of both QuerySet and sql.Query objects.\n        self.query = getattr(queryset, 'query', queryset)\n        self.extra = extra\n        super().__init__(output_field)\n\n    def get_source_expressions(self):\n(B)         # Allow the usage of both QuerySet and sql.Query objects.\n        self.query = getattr(queryset, 'query', queryset)\n        self.extra = extra\n        super().__init__(output_field)\n\n    def get_source_expressions(self):\n        return [self.query]\n\n(C)     contains_aggregate = False\n    empty_result_set_value = None\n\n    def __init__(self, queryset, output_field=None, **extra):\n        # Allow the usage of both QuerySet and sql.Query objects.\n        self.query = getattr(queryset, 'query', queryset)\n        self.extra = extra\n        super().__init__(output_field)\n(D)         self.extra = extra\n        super().__init__(output_field)\n\n    def get_source_expressions(self):\n        return [self.query]\n\n    def set_source_expressions(self, exprs):\n        self.query = exprs[0]", "answer": "A"}
{"uuid": "be8f023d-bba4-480e-8c7b-ca4e9f666c81", "issue_statement": "Messages framework incorrectly serializes/deserializes extra_tags when it's an empty string\nDescription\n\t\nWhen a message is serialised and then deserialised with any of the built in storage backends, then extra_tags==\"\" is converted to extra_tags==None. This is because MessageEncoder checks for the truthyness of extra_tags rather than checking it is not None.\nTo replicate this bug\n>>> from django.conf import settings\n>>> settings.configure() # Just to allow the following import\n>>> from django.contrib.messages.storage.base import Message\n>>> from django.contrib.messages.storage.cookie import MessageEncoder, MessageDecoder\n>>> original_message = Message(10, \"Here is a message\", extra_tags=\"\")\n>>> encoded_message = MessageEncoder().encode(original_message)\n>>> decoded_message = MessageDecoder().decode(encoded_message)\n>>> original_message.extra_tags == \"\"\nTrue\n>>> decoded_message.extra_tags is None\nTrue\nEffect of the bug in application behaviour\nThis error occurred in the wild with a template tag similar to the following:\n{% if x not in message.extra_tags %}\nWhen the message was displayed as part of a redirect, it had been serialised and deserialized which meant that extra_tags was None instead of the empty string. This caused an error.\nIt's important to note that this bug affects all of the standard API (messages.debug, messages.info etc. all have a default value of extra_tags equal to \"\").", "choices": "(A)     def default(self, obj):\n        if isinstance(obj, Message):\n            # Using 0/1 here instead of False/True to produce more compact json\n            is_safedata = 1 if isinstance(obj.message, SafeData) else 0\n            message = [self.message_key, is_safedata, obj.level, obj.message]\n            if obj.extra_tags:\n                message.append(obj.extra_tags)\n(B)                 message.append(obj.extra_tags)\n            return message\n        return super().default(obj)\n\n\nclass MessageDecoder(json.JSONDecoder):\n    \"\"\"\n(C)             # Using 0/1 here instead of False/True to produce more compact json\n            is_safedata = 1 if isinstance(obj.message, SafeData) else 0\n            message = [self.message_key, is_safedata, obj.level, obj.message]\n            if obj.extra_tags:\n                message.append(obj.extra_tags)\n            return message\n        return super().default(obj)\n(D)             message = [self.message_key, is_safedata, obj.level, obj.message]\n            if obj.extra_tags:\n                message.append(obj.extra_tags)\n            return message\n        return super().default(obj)\n\n", "answer": "C"}
{"uuid": "51011e06-10dc-4c94-8f83-eaa44b8699c7", "issue_statement": "Dev Server fails to restart after adding BASE_DIR to TEMPLATES[0]['DIRS'] in settings\nDescription\n\t\nRepro steps:\n$ pip install -U django\n$ django-admin startproject <name>\nOpen settings.py, copy the BASE_DIR variable from line 16 and paste it into the empty DIRS list on line 57\n$ ./manage.py runserver\nBack in your IDE, save a file and watch the dev server *NOT* restart.\nBack in settings.py, remove BASE_DIR from the templates DIRS list. Manually CTRL-C your dev server (as it won't restart on its own when you save), restart the dev server. Now return to your settings.py file, re-save it, and notice the development server once again detects changes and restarts.\nThis bug prevents the dev server from restarting no matter where you make changes - it is not just scoped to edits to settings.py.", "choices": "(A)         sender.watch_dir(directory, '**/*')\n\n\n@receiver(file_changed, dispatch_uid='template_loaders_file_changed')\ndef template_changed(sender, file_path, **kwargs):\n    for template_dir in get_template_directories():\n        if template_dir in file_path.parents:\n            reset_loaders()\n(B) \n@receiver(file_changed, dispatch_uid='template_loaders_file_changed')\ndef template_changed(sender, file_path, **kwargs):\n    for template_dir in get_template_directories():\n        if template_dir in file_path.parents:\n            reset_loaders()\n            return True\n(C) def watch_for_template_changes(sender, **kwargs):\n    for directory in get_template_directories():\n        sender.watch_dir(directory, '**/*')\n\n\n@receiver(file_changed, dispatch_uid='template_loaders_file_changed')\ndef template_changed(sender, file_path, **kwargs):\n    for template_dir in get_template_directories():\n(D) \n@receiver(autoreload_started, dispatch_uid='template_loaders_watch_changes')\ndef watch_for_template_changes(sender, **kwargs):\n    for directory in get_template_directories():\n        sender.watch_dir(directory, '**/*')\n\n\n@receiver(file_changed, dispatch_uid='template_loaders_file_changed')", "answer": "B"}
{"uuid": "2ead66e1-4832-41ea-81dd-86091fdf335d", "issue_statement": "SimpleLazyObject doesn't implement __radd__\nDescription\n\t\nTechnically, there's a whole bunch of magic methods it doesn't implement, compared to a complete proxy implementation, like that of wrapt.ObjectProxy, but __radd__ being missing is the one that's biting me at the moment.\nAs far as I can tell, the implementation can't just be\n__radd__ = new_method_proxy(operator.radd)\nbecause that doesn't exist, which is rubbish.\n__radd__ = new_method_proxy(operator.attrgetter(\"__radd__\"))\nalso won't work because types may not have that attr, and attrgetter doesn't supress the exception (correctly)\nThe minimal implementation I've found that works for me is:\n\tdef __radd__(self, other):\n\t\tif self._wrapped is empty:\n\t\t\tself._setup()\n\t\treturn other + self._wrapped", "choices": "(A)             # latter is proxied.\n            result = SimpleLazyObject(self._setupfunc)\n            memo[id(self)] = result\n            return result\n        return copy.deepcopy(self._wrapped, memo)\n\n\ndef partition(predicate, values):\n    \"\"\"\n    Split the values into two sets, based on the return value of the function\n    (True/False). e.g.:\n\n(B)     def __deepcopy__(self, memo):\n        if self._wrapped is empty:\n            # We have to use SimpleLazyObject, not self.__class__, because the\n            # latter is proxied.\n            result = SimpleLazyObject(self._setupfunc)\n            memo[id(self)] = result\n            return result\n        return copy.deepcopy(self._wrapped, memo)\n\n\ndef partition(predicate, values):\n    \"\"\"\n(C) \ndef partition(predicate, values):\n    \"\"\"\n    Split the values into two sets, based on the return value of the function\n    (True/False). e.g.:\n\n        >>> partition(lambda x: x > 3, range(5))\n        [0, 1, 2, 3], [4]\n    \"\"\"\n    results = ([], [])\n    for item in values:\n        results[predicate(item)].append(item)\n(D)             return result\n        return copy.deepcopy(self._wrapped, memo)\n\n\ndef partition(predicate, values):\n    \"\"\"\n    Split the values into two sets, based on the return value of the function\n    (True/False). e.g.:\n\n        >>> partition(lambda x: x > 3, range(5))\n        [0, 1, 2, 3], [4]\n    \"\"\"", "answer": "D"}
{"uuid": "c9ac2de3-0d07-4b18-ad15-61cad37d309b", "issue_statement": "Fix handling empty string for If-Modified-Since header\nDescription\n\t\nEmpty string used to be ignored for If-Modified-Since header, but now raises exception since d6aff369ad3.\nFix handling empty string for If-Modified-Since header\nDescription\n\t\nEmpty string used to be ignored for If-Modified-Since header, but now raises exception since d6aff369ad3.", "choices": "(A)     header\n      This is the value of the If-Modified-Since header.  If this is None,\n      I'll just return True.\n\n    mtime\n      This is the modification time of the item we're talking about.\n\n    size\n      This is the size of the item we're talking about.\n    \"\"\"\n    try:\n        if header is None:\n            raise ValueError\n        matches = re.match(r\"^([^;]+)(; length=([0-9]+))?$\", header, re.IGNORECASE)\n(B)     mtime\n      This is the modification time of the item we're talking about.\n\n    size\n      This is the size of the item we're talking about.\n    \"\"\"\n    try:\n        if header is None:\n            raise ValueError\n        matches = re.match(r\"^([^;]+)(; length=([0-9]+))?$\", header, re.IGNORECASE)\n        header_mtime = parse_http_date(matches[1])\n        header_len = matches[3]\n        if header_len and int(header_len) != size:\n            raise ValueError\n(C)     size\n      This is the size of the item we're talking about.\n    \"\"\"\n    try:\n        if header is None:\n            raise ValueError\n        matches = re.match(r\"^([^;]+)(; length=([0-9]+))?$\", header, re.IGNORECASE)\n        header_mtime = parse_http_date(matches[1])\n        header_len = matches[3]\n        if header_len and int(header_len) != size:\n            raise ValueError\n        if int(mtime) > header_mtime:\n            raise ValueError\n    except (AttributeError, ValueError, OverflowError):\n(D)         if header is None:\n            raise ValueError\n        matches = re.match(r\"^([^;]+)(; length=([0-9]+))?$\", header, re.IGNORECASE)\n        header_mtime = parse_http_date(matches[1])\n        header_len = matches[3]\n        if header_len and int(header_len) != size:\n            raise ValueError\n        if int(mtime) > header_mtime:\n            raise ValueError\n    except (AttributeError, ValueError, OverflowError):\n        return True\n    return False", "answer": "D"}
{"uuid": "6a1a3d36-b7f7-4371-807d-79df1af5f26b", "issue_statement": "RenameIndex() crashes when unnamed index is moving backward and forward.\nDescription\n\t\nRenameIndex() should restore the old auto-generated name when an unnamed index for unique_together is moving backward. Now re-applying RenameIndex() crashes. For example:\ntests/migrations/test_operations.py\ndiff --git a/tests/migrations/test_operations.py b/tests/migrations/test_operations.py\nindex cfd28b1b39..c0a55023bb 100644\n\t\t\t\t\t\n\t\t\t\t\t a\n\t\t\t\t \n\t\t\t\t\t\n\t\t\t\t\t b\n\t\t\t\t \n class OperationTests(OperationTestBase):\u00a0\n29882988\u00a0 \u00a0 \u00a0 \u00a0 with connection.schema_editor() as editor, self.assertNumQueries(0):\n29892989\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operation.database_backwards(app_label, editor, new_state, project_state)\n29902990\u00a0 \u00a0 \u00a0 \u00a0 self.assertIndexNameExists(table_name, \"new_pony_test_idx\")\n\u00a02991\u00a0 \u00a0 \u00a0 \u00a0 # Re-apply renaming.\n\u00a02992\u00a0 \u00a0 \u00a0 \u00a0 with connection.schema_editor() as editor:\n\u00a02993\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operation.database_forwards(app_label, editor, project_state, new_state)\n\u00a02994\u00a0 \u00a0 \u00a0 \u00a0 self.assertIndexNameExists(table_name, \"new_pony_test_idx\")\n29912995\u00a0 \u00a0 \u00a0 \u00a0 # Deconstruction.\n29922996\u00a0 \u00a0 \u00a0 \u00a0 definition = operation.deconstruct()\n29932997\u00a0 \u00a0 \u00a0 \u00a0 self.assertEqual(definition[0], \"RenameIndex\")\ncrashes on PostgreSQL:\ndjango.db.utils.ProgrammingError: relation \"new_pony_test_idx\" already exists", "choices": "(A)         new_index = to_model_state.get_index_by_name(self.new_name)\n        schema_editor.rename_index(model, old_index, new_index)\n\n    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n        if self.old_fields:\n            # Backward operation with unnamed index is a no-op.\n            return\n\n        self.new_name_lower, self.old_name_lower = (\n(B)                 fields=self.old_fields,\n                name=matching_index_name[0],\n            )\n        else:\n            from_model_state = from_state.models[app_label, self.model_name_lower]\n            old_index = from_model_state.get_index_by_name(self.old_name)\n\n        to_model_state = to_state.models[app_label, self.model_name_lower]\n        new_index = to_model_state.get_index_by_name(self.new_name)\n(C)         else:\n            from_model_state = from_state.models[app_label, self.model_name_lower]\n            old_index = from_model_state.get_index_by_name(self.old_name)\n\n        to_model_state = to_state.models[app_label, self.model_name_lower]\n        new_index = to_model_state.get_index_by_name(self.new_name)\n        schema_editor.rename_index(model, old_index, new_index)\n\n    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n(D) \n        to_model_state = to_state.models[app_label, self.model_name_lower]\n        new_index = to_model_state.get_index_by_name(self.new_name)\n        schema_editor.rename_index(model, old_index, new_index)\n\n    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n        if self.old_fields:\n            # Backward operation with unnamed index is a no-op.\n            return", "answer": "C"}
{"uuid": "e14ce578-9c95-4e44-becf-3ff66a8988e5", "issue_statement": "Models migration with change field foreign to many and deleting unique together.\nDescription\n\t \n\t\t(last modified by Simon Charette)\n\t \nI have models like\nclass Authors(models.Model):\n\tproject_data_set = models.ForeignKey(\n\t\tProjectDataSet,\n\t\ton_delete=models.PROTECT\n\t)\n\tstate = models.IntegerField()\n\tstart_date = models.DateField()\n\tclass Meta:\n\t\t unique_together = (('project_data_set', 'state', 'start_date'),)\nand\nclass DataSet(models.Model):\n\tname = models.TextField(max_length=50)\nclass Project(models.Model):\n\tdata_sets = models.ManyToManyField(\n\t\tDataSet,\n\t\tthrough='ProjectDataSet',\n\t)\n\tname = models.TextField(max_length=50)\nclass ProjectDataSet(models.Model):\n\t\"\"\"\n\tCross table of data set and project\n\t\"\"\"\n\tdata_set = models.ForeignKey(DataSet, on_delete=models.PROTECT)\n\tproject = models.ForeignKey(Project, on_delete=models.PROTECT)\n\tclass Meta:\n\t\tunique_together = (('data_set', 'project'),)\nwhen i want to change field project_data_set in Authors model from foreign key field to many to many field I must delete a unique_together, cause it can't be on many to many field.\nThen my model should be like:\nclass Authors(models.Model):\n\tproject_data_set = models.ManyToManyField(\n\t\tProjectDataSet,\n\t)\n\tstate = models.IntegerField()\n\tstart_date = models.DateField()\nBut when I want to do a migrations.\npython3 manage.py makemigrations\npython3 manage.py migrate\nI have error:\nValueError: Found wrong number (0) of constraints for app_authors(project_data_set, state, start_date)\nThe database is on production, so I can't delete previous initial migrations, and this error isn't depending on database, cause I delete it and error is still the same.\nMy solve is to first delete unique_together, then do a makemigrations and then migrate. After that change the field from foreign key to many to many field, then do a makemigrations and then migrate.\nBut in this way I have 2 migrations instead of one.\nI added attachment with this project, download it and then do makemigrations and then migrate to see this error.", "choices": "(A)         if field.remote_field and field.remote_field.model:\n            dependencies.extend(\n                self._get_dependencies_for_foreign_key(\n                    app_label,\n                    model_name,\n                    field,\n                    self.to_state,\n                )\n            )\n(B)         # Fields that are foreignkeys/m2ms depend on stuff\n        dependencies = []\n        if field.remote_field and field.remote_field.model:\n            dependencies.extend(\n                self._get_dependencies_for_foreign_key(\n                    app_label,\n                    model_name,\n                    field,\n                    self.to_state,\n(C) \n    def _generate_added_field(self, app_label, model_name, field_name):\n        field = self.to_state.models[app_label, model_name].get_field(field_name)\n        # Fields that are foreignkeys/m2ms depend on stuff\n        dependencies = []\n        if field.remote_field and field.remote_field.model:\n            dependencies.extend(\n                self._get_dependencies_for_foreign_key(\n                    app_label,\n(D)             self.new_field_keys - self.old_field_keys\n        ):\n            self._generate_added_field(app_label, model_name, field_name)\n\n    def _generate_added_field(self, app_label, model_name, field_name):\n        field = self.to_state.models[app_label, model_name].get_field(field_name)\n        # Fields that are foreignkeys/m2ms depend on stuff\n        dependencies = []\n        if field.remote_field and field.remote_field.model:", "answer": "C"}
{"uuid": "208bc86f-abe9-46c0-b4be-5a2a1b3cd8ad", "issue_statement": "Customizable management command formatters.\nDescription\n\t\nWith code like:\nclass Command(BaseCommand):\n\thelp = '''\n\tImport a contract from tzkt.\n\tExample usage:\n\t\t./manage.py tzkt_import 'Tezos Mainnet' KT1HTDtMBRCKoNHjfWEEvXneGQpCfPAt6BRe\n\t'''\nHelp output is:\n$ ./manage.py help tzkt_import\nusage: manage.py tzkt_import [-h] [--api API] [--version] [-v {0,1,2,3}] [--settings SETTINGS]\n\t\t\t\t\t\t\t [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color]\n\t\t\t\t\t\t\t [--skip-checks]\n\t\t\t\t\t\t\t blockchain target\nImport a contract from tzkt Example usage: ./manage.py tzkt_import 'Tezos Mainnet'\nKT1HTDtMBRCKoNHjfWEEvXneGQpCfPAt6BRe\npositional arguments:\n blockchain\t\t\tName of the blockchain to import into\n target\t\t\t\tId of the contract to import\nWhen that was expected:\n$ ./manage.py help tzkt_import\nusage: manage.py tzkt_import [-h] [--api API] [--version] [-v {0,1,2,3}] [--settings SETTINGS]\n\t\t\t\t\t\t\t [--pythonpath PYTHONPATH] [--traceback] [--no-color] [--force-color]\n\t\t\t\t\t\t\t [--skip-checks]\n\t\t\t\t\t\t\t blockchain target\nImport a contract from tzkt \nExample usage: \n\t./manage.py tzkt_import 'Tezos Mainnet' KT1HTDtMBRCKoNHjfWEEvXneGQpCfPAt6BRe\npositional arguments:\n blockchain\t\t\tName of the blockchain to import into\n target\t\t\t\tId of the contract to import", "choices": "(A) \n    def create_parser(self, prog_name, subcommand, **kwargs):\n        \"\"\"\n        Create and return the ``ArgumentParser`` which will be used to\n        parse the arguments to this command.\n        \"\"\"\n        parser = CommandParser(\n            prog=\"%s %s\" % (os.path.basename(prog_name), subcommand),\n            description=self.help or None,\n            formatter_class=DjangoHelpFormatter,\n(B)         parser = CommandParser(\n            prog=\"%s %s\" % (os.path.basename(prog_name), subcommand),\n            description=self.help or None,\n            formatter_class=DjangoHelpFormatter,\n            missing_args_message=getattr(self, \"missing_args_message\", None),\n            called_from_command_line=getattr(self, \"_called_from_command_line\", None),\n            **kwargs,\n        )\n        self.add_base_argument(\n            parser,\n(C)         Create and return the ``ArgumentParser`` which will be used to\n        parse the arguments to this command.\n        \"\"\"\n        parser = CommandParser(\n            prog=\"%s %s\" % (os.path.basename(prog_name), subcommand),\n            description=self.help or None,\n            formatter_class=DjangoHelpFormatter,\n            missing_args_message=getattr(self, \"missing_args_message\", None),\n            called_from_command_line=getattr(self, \"_called_from_command_line\", None),\n            **kwargs,\n(D)             description=self.help or None,\n            formatter_class=DjangoHelpFormatter,\n            missing_args_message=getattr(self, \"missing_args_message\", None),\n            called_from_command_line=getattr(self, \"_called_from_command_line\", None),\n            **kwargs,\n        )\n        self.add_base_argument(\n            parser,\n            \"--version\",\n            action=\"version\",", "answer": "C"}
{"uuid": "a8240782-3333-4435-8170-b8e334810ccb", "issue_statement": "Add an encoder parameter to django.utils.html.json_script().\nDescription\n\t\nI have a use case where I want to customize the JSON encoding of some values to output to the template layer. It looks like django.utils.html.json_script is a good utility for that, however the JSON encoder is hardcoded to DjangoJSONEncoder. I think it would be nice to be able to pass a custom encoder class.\nBy the way, django.utils.html.json_script is not documented (only its template filter counterpart is), would it be a good thing to add to the docs?", "choices": "(A)     from django.core.serializers.json import DjangoJSONEncoder\n\n    json_str = json.dumps(value, cls=DjangoJSONEncoder).translate(_json_script_escapes)\n    if element_id:\n        template = '<script id=\"{}\" type=\"application/json\">{}</script>'\n        args = (element_id, mark_safe(json_str))\n    else:\n        template = '<script type=\"application/json\">{}</script>'\n        args = (mark_safe(json_str),)\n    return format_html(template, *args)\n\n\ndef conditional_escape(text):\n    \"\"\"\n    Similar to escape(), except that it doesn't operate on pre-escaped strings.\n\n    This function relies on the __html__ convention used both by Django's\n(B)     Escape all the HTML/XML special characters with their unicode escapes, so\n    value is safe to be output anywhere except for inside a tag attribute. Wrap\n    the escaped JSON in a script tag.\n    \"\"\"\n    from django.core.serializers.json import DjangoJSONEncoder\n\n    json_str = json.dumps(value, cls=DjangoJSONEncoder).translate(_json_script_escapes)\n    if element_id:\n        template = '<script id=\"{}\" type=\"application/json\">{}</script>'\n        args = (element_id, mark_safe(json_str))\n    else:\n        template = '<script type=\"application/json\">{}</script>'\n        args = (mark_safe(json_str),)\n    return format_html(template, *args)\n\n\ndef conditional_escape(text):\n(C) }\n\n\ndef json_script(value, element_id=None):\n    \"\"\"\n    Escape all the HTML/XML special characters with their unicode escapes, so\n    value is safe to be output anywhere except for inside a tag attribute. Wrap\n    the escaped JSON in a script tag.\n    \"\"\"\n    from django.core.serializers.json import DjangoJSONEncoder\n\n    json_str = json.dumps(value, cls=DjangoJSONEncoder).translate(_json_script_escapes)\n    if element_id:\n        template = '<script id=\"{}\" type=\"application/json\">{}</script>'\n        args = (element_id, mark_safe(json_str))\n    else:\n        template = '<script type=\"application/json\">{}</script>'\n(D) \n_json_script_escapes = {\n    ord(\">\"): \"\\\\u003E\",\n    ord(\"<\"): \"\\\\u003C\",\n    ord(\"&\"): \"\\\\u0026\",\n}\n\n\ndef json_script(value, element_id=None):\n    \"\"\"\n    Escape all the HTML/XML special characters with their unicode escapes, so\n    value is safe to be output anywhere except for inside a tag attribute. Wrap\n    the escaped JSON in a script tag.\n    \"\"\"\n    from django.core.serializers.json import DjangoJSONEncoder\n\n    json_str = json.dumps(value, cls=DjangoJSONEncoder).translate(_json_script_escapes)", "answer": "C"}
{"uuid": "71e595da-40bf-491b-b2f4-c63068f11961", "issue_statement": "check_for_template_tags_with_the_same_name with libraries in TEMPLATES\nDescription\n\t\nI didn't explore this thoroughly, but I think there might be an issue with the check_for_template_tags_with_the_same_name when you add a template tag library into TEMPLATES['OPTIONS']['librairies'].\nI'm getting an error like: \n(templates.E003) 'my_tags' is used for multiple template tag modules: 'someapp.templatetags.my_tags', 'someapp.templatetags.my_tags'", "choices": "(A)     errors = []\n    for conf in settings.TEMPLATES:\n        string_if_invalid = conf.get(\"OPTIONS\", {}).get(\"string_if_invalid\", \"\")\n        if not isinstance(string_if_invalid, str):\n            error = copy.copy(E002)\n            error.msg = error.msg.format(\n                string_if_invalid, type(string_if_invalid).__name__\n            )\n            errors.append(error)\n    return errors\n\n\n@register(Tags.templates)\ndef check_for_template_tags_with_the_same_name(app_configs, **kwargs):\n    errors = []\n    libraries = defaultdict(list)\n\n    for conf in settings.TEMPLATES:\n        custom_libraries = conf.get(\"OPTIONS\", {}).get(\"libraries\", {})\n        for module_name, module_path in custom_libraries.items():\n            libraries[module_name].append(module_path)\n\n    for module_name, module_path in get_template_tag_modules():\n(B)                 string_if_invalid, type(string_if_invalid).__name__\n            )\n            errors.append(error)\n    return errors\n\n\n@register(Tags.templates)\ndef check_for_template_tags_with_the_same_name(app_configs, **kwargs):\n    errors = []\n    libraries = defaultdict(list)\n\n    for conf in settings.TEMPLATES:\n        custom_libraries = conf.get(\"OPTIONS\", {}).get(\"libraries\", {})\n        for module_name, module_path in custom_libraries.items():\n            libraries[module_name].append(module_path)\n\n    for module_name, module_path in get_template_tag_modules():\n        libraries[module_name].append(module_path)\n\n    for library_name, items in libraries.items():\n        if len(items) > 1:\n            errors.append(\n                Error(\n(C)         else []\n    )\n\n\n@register(Tags.templates)\ndef check_string_if_invalid_is_string(app_configs, **kwargs):\n    errors = []\n    for conf in settings.TEMPLATES:\n        string_if_invalid = conf.get(\"OPTIONS\", {}).get(\"string_if_invalid\", \"\")\n        if not isinstance(string_if_invalid, str):\n            error = copy.copy(E002)\n            error.msg = error.msg.format(\n                string_if_invalid, type(string_if_invalid).__name__\n            )\n            errors.append(error)\n    return errors\n\n\n@register(Tags.templates)\ndef check_for_template_tags_with_the_same_name(app_configs, **kwargs):\n    errors = []\n    libraries = defaultdict(list)\n\n(D) @register(Tags.templates)\ndef check_for_template_tags_with_the_same_name(app_configs, **kwargs):\n    errors = []\n    libraries = defaultdict(list)\n\n    for conf in settings.TEMPLATES:\n        custom_libraries = conf.get(\"OPTIONS\", {}).get(\"libraries\", {})\n        for module_name, module_path in custom_libraries.items():\n            libraries[module_name].append(module_path)\n\n    for module_name, module_path in get_template_tag_modules():\n        libraries[module_name].append(module_path)\n\n    for library_name, items in libraries.items():\n        if len(items) > 1:\n            errors.append(\n                Error(\n                    E003.msg.format(\n                        repr(library_name),\n                        \", \".join(repr(item) for item in items),\n                    ),\n                    id=E003.id,\n                )", "answer": "D"}
{"uuid": "78310885-160d-4652-86a3-dd868409e59f", "issue_statement": "QuerySet.only() after select_related() crash on proxy models.\nDescription\n\t\nWhen I optimize a query using select_related() and only() methods from the proxy model I encounter an error:\nWindows 10; Python 3.10; Django 4.0.5\nTraceback (most recent call last):\n File \"D:\\study\\django_college\\manage.py\", line 22, in <module>\n\tmain()\n File \"D:\\study\\django_college\\manage.py\", line 18, in main\n\texecute_from_command_line(sys.argv)\n File \"D:\\Anaconda3\\envs\\django\\lib\\site-packages\\django\\core\\management\\__init__.py\", line 446, in execute_from_command_line\n\tutility.execute()\n File \"D:\\Anaconda3\\envs\\django\\lib\\site-packages\\django\\core\\management\\__init__.py\", line 440, in execute\n\tself.fetch_command(subcommand).run_from_argv(self.argv)\n File \"D:\\Anaconda3\\envs\\django\\lib\\site-packages\\django\\core\\management\\base.py\", line 414, in run_from_argv\n\tself.execute(*args, **cmd_options)\n File \"D:\\Anaconda3\\envs\\django\\lib\\site-packages\\django\\core\\management\\base.py\", line 460, in execute\n\toutput = self.handle(*args, **options)\n File \"D:\\study\\django_college\\project\\users\\management\\commands\\test_proxy.py\", line 9, in handle\n\tobjs = list(AnotherModel.objects.select_related(\"custom\").only(\"custom__name\").all())\n File \"D:\\Anaconda3\\envs\\django\\lib\\site-packages\\django\\db\\models\\query.py\", line 302, in __len__\n\tself._fetch_all()\n File \"D:\\Anaconda3\\envs\\django\\lib\\site-packages\\django\\db\\models\\query.py\", line 1507, in _fetch_all\n\tself._result_cache = list(self._iterable_class(self))\n File \"D:\\Anaconda3\\envs\\django\\lib\\site-packages\\django\\db\\models\\query.py\", line 71, in __iter__\n\trelated_populators = get_related_populators(klass_info, select, db)\n File \"D:\\Anaconda3\\envs\\django\\lib\\site-packages\\django\\db\\models\\query.py\", line 2268, in get_related_populators\n\trel_cls = RelatedPopulator(rel_klass_info, select, db)\n File \"D:\\Anaconda3\\envs\\django\\lib\\site-packages\\django\\db\\models\\query.py\", line 2243, in __init__\n\tself.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)\nValueError: 'id' is not in list\nModels:\nclass CustomModel(models.Model):\n\tname = models.CharField(max_length=16)\nclass ProxyCustomModel(CustomModel):\n\tclass Meta:\n\t\tproxy = True\nclass AnotherModel(models.Model):\n\tcustom = models.ForeignKey(\n\t\tProxyCustomModel,\n\t\ton_delete=models.SET_NULL,\n\t\tnull=True,\n\t\tblank=True,\n\t)\nCommand:\nclass Command(BaseCommand):\n\tdef handle(self, *args, **options):\n\t\tlist(AnotherModel.objects.select_related(\"custom\").only(\"custom__name\").all())\nAt django/db/models/sql/query.py in 745 line there is snippet:\nopts = cur_model._meta\nIf I replace it by \nopts = cur_model._meta.concrete_model._meta\nall works as expected.", "choices": "(A)                 source = opts.get_field(name)\n                if is_reverse_o2o(source):\n                    cur_model = source.related_model\n                else:\n                    cur_model = source.remote_field.model\n                opts = cur_model._meta\n                # Even if we're \"just passing through\" this model, we must add\n(B)                     cur_model = source.related_model\n                else:\n                    cur_model = source.remote_field.model\n                opts = cur_model._meta\n                # Even if we're \"just passing through\" this model, we must add\n                # both the current model's pk and the related reference field\n                # (if it's not a reverse relation) to the things we select.\n(C)                 # Even if we're \"just passing through\" this model, we must add\n                # both the current model's pk and the related reference field\n                # (if it's not a reverse relation) to the things we select.\n                if not is_reverse_o2o(source):\n                    must_include[old_model].add(source)\n                add_to_dict(must_include, cur_model, opts.pk)\n            field = opts.get_field(parts[-1])\n(D)                     cur_model = source.remote_field.model\n                opts = cur_model._meta\n                # Even if we're \"just passing through\" this model, we must add\n                # both the current model's pk and the related reference field\n                # (if it's not a reverse relation) to the things we select.\n                if not is_reverse_o2o(source):\n                    must_include[old_model].add(source)", "answer": "B"}
{"uuid": "aa31107b-fa7e-4b88-a204-e9571c6e891a", "issue_statement": "inspectdb should generate related_name on same relation links.\nDescription\n\t\nHi!\nAfter models generation with inspectdb command we have issue with relations to same enities\nmodule.Model.field1: (fields.E304) Reverse accessor for 'module.Model.field1' clashes with reverse accessor for 'module.Model.field2'.\nHINT: Add or change a related_name argument to the definition for 'module.Model.field1' or 'module.Model.field2'.\n*\nMaybe we can autogenerate\nrelated_name='attribute_name'\nto all fields in model if related Model was used for this table", "choices": "(A)                         constraints = {}\n                    primary_key_columns = (\n                        connection.introspection.get_primary_key_columns(\n                            cursor, table_name\n                        )\n                    )\n                    primary_key_column = (\n                        primary_key_columns[0] if primary_key_columns else None\n                    )\n                    unique_columns = [\n                        c[\"columns\"][0]\n                        for c in constraints.values()\n                        if c[\"unique\"] and len(c[\"columns\"]) == 1\n                    ]\n                    table_description = connection.introspection.get_table_description(\n                        cursor, table_name\n                    )\n                except Exception as e:\n                    yield \"# Unable to inspect table '%s'\" % table_name\n                    yield \"# The error was: %s\" % e\n                    continue\n\n                yield \"\"\n                yield \"\"\n                yield \"class %s(models.Model):\" % table2model(table_name)\n                known_models.append(table2model(table_name))\n                used_column_names = []  # Holds column names used in the table so far\n                column_to_field_name = {}  # Maps column names to names of model fields\n                for row in table_description:\n                    comment_notes = (\n                        []\n                    )  # Holds Field notes, to be displayed in a Python comment.\n                    extra_params = {}  # Holds Field parameters such as 'db_column'.\n                    column_name = row.name\n                    is_relation = column_name in relations\n\n                    att_name, params, notes = self.normalize_col_name(\n                        column_name, used_column_names, is_relation\n                    )\n                    extra_params.update(params)\n                    comment_notes.extend(notes)\n\n                    used_column_names.append(att_name)\n                    column_to_field_name[column_name] = att_name\n\n                    # Add primary_key and unique, if necessary.\n                    if column_name == primary_key_column:\n                        extra_params[\"primary_key\"] = True\n                        if len(primary_key_columns) > 1:\n                            comment_notes.append(\n                                \"The composite primary key (%s) found, that is not \"\n                                \"supported. The first column is selected.\"\n                                % \", \".join(primary_key_columns)\n                            )\n                    elif column_name in unique_columns:\n                        extra_params[\"unique\"] = True\n\n                    if is_relation:\n                        ref_db_column, ref_db_table = relations[column_name]\n                        if extra_params.pop(\"unique\", False) or extra_params.get(\n                            \"primary_key\"\n                        ):\n                            rel_type = \"OneToOneField\"\n                        else:\n                            rel_type = \"ForeignKey\"\n                            ref_pk_column = (\n                                connection.introspection.get_primary_key_column(\n                                    cursor, ref_db_table\n                                )\n                            )\n                            if ref_pk_column and ref_pk_column != ref_db_column:\n                                extra_params[\"to_field\"] = ref_db_column\n                        rel_to = (\n(B)                     yield \"# The error was: %s\" % e\n                    continue\n\n                yield \"\"\n                yield \"\"\n                yield \"class %s(models.Model):\" % table2model(table_name)\n                known_models.append(table2model(table_name))\n                used_column_names = []  # Holds column names used in the table so far\n                column_to_field_name = {}  # Maps column names to names of model fields\n                for row in table_description:\n                    comment_notes = (\n                        []\n                    )  # Holds Field notes, to be displayed in a Python comment.\n                    extra_params = {}  # Holds Field parameters such as 'db_column'.\n                    column_name = row.name\n                    is_relation = column_name in relations\n\n                    att_name, params, notes = self.normalize_col_name(\n                        column_name, used_column_names, is_relation\n                    )\n                    extra_params.update(params)\n                    comment_notes.extend(notes)\n\n                    used_column_names.append(att_name)\n                    column_to_field_name[column_name] = att_name\n\n                    # Add primary_key and unique, if necessary.\n                    if column_name == primary_key_column:\n                        extra_params[\"primary_key\"] = True\n                        if len(primary_key_columns) > 1:\n                            comment_notes.append(\n                                \"The composite primary key (%s) found, that is not \"\n                                \"supported. The first column is selected.\"\n                                % \", \".join(primary_key_columns)\n                            )\n                    elif column_name in unique_columns:\n                        extra_params[\"unique\"] = True\n\n                    if is_relation:\n                        ref_db_column, ref_db_table = relations[column_name]\n                        if extra_params.pop(\"unique\", False) or extra_params.get(\n                            \"primary_key\"\n                        ):\n                            rel_type = \"OneToOneField\"\n                        else:\n                            rel_type = \"ForeignKey\"\n                            ref_pk_column = (\n                                connection.introspection.get_primary_key_column(\n                                    cursor, ref_db_table\n                                )\n                            )\n                            if ref_pk_column and ref_pk_column != ref_db_column:\n                                extra_params[\"to_field\"] = ref_db_column\n                        rel_to = (\n                            \"self\"\n                            if ref_db_table == table_name\n                            else table2model(ref_db_table)\n                        )\n                        if rel_to in known_models:\n                            field_type = \"%s(%s\" % (rel_type, rel_to)\n                        else:\n                            field_type = \"%s('%s'\" % (rel_type, rel_to)\n                    else:\n                        # Calling `get_field_type` to get the field type string and any\n                        # additional parameters and notes.\n                        field_type, field_params, field_notes = self.get_field_type(\n                            connection, table_name, row\n                        )\n                        extra_params.update(field_params)\n                        comment_notes.extend(field_notes)\n\n                        field_type += \"(\"\n\n(C)                     )\n                    extra_params.update(params)\n                    comment_notes.extend(notes)\n\n                    used_column_names.append(att_name)\n                    column_to_field_name[column_name] = att_name\n\n                    # Add primary_key and unique, if necessary.\n                    if column_name == primary_key_column:\n                        extra_params[\"primary_key\"] = True\n                        if len(primary_key_columns) > 1:\n                            comment_notes.append(\n                                \"The composite primary key (%s) found, that is not \"\n                                \"supported. The first column is selected.\"\n                                % \", \".join(primary_key_columns)\n                            )\n                    elif column_name in unique_columns:\n                        extra_params[\"unique\"] = True\n\n                    if is_relation:\n                        ref_db_column, ref_db_table = relations[column_name]\n                        if extra_params.pop(\"unique\", False) or extra_params.get(\n                            \"primary_key\"\n                        ):\n                            rel_type = \"OneToOneField\"\n                        else:\n                            rel_type = \"ForeignKey\"\n                            ref_pk_column = (\n                                connection.introspection.get_primary_key_column(\n                                    cursor, ref_db_table\n                                )\n                            )\n                            if ref_pk_column and ref_pk_column != ref_db_column:\n                                extra_params[\"to_field\"] = ref_db_column\n                        rel_to = (\n                            \"self\"\n                            if ref_db_table == table_name\n                            else table2model(ref_db_table)\n                        )\n                        if rel_to in known_models:\n                            field_type = \"%s(%s\" % (rel_type, rel_to)\n                        else:\n                            field_type = \"%s('%s'\" % (rel_type, rel_to)\n                    else:\n                        # Calling `get_field_type` to get the field type string and any\n                        # additional parameters and notes.\n                        field_type, field_params, field_notes = self.get_field_type(\n                            connection, table_name, row\n                        )\n                        extra_params.update(field_params)\n                        comment_notes.extend(field_notes)\n\n                        field_type += \"(\"\n\n                    # Don't output 'id = meta.AutoField(primary_key=True)', because\n                    # that's assumed if it doesn't exist.\n                    if att_name == \"id\" and extra_params == {\"primary_key\": True}:\n                        if field_type == \"AutoField(\":\n                            continue\n                        elif (\n                            field_type\n                            == connection.features.introspected_field_types[\"AutoField\"]\n                            + \"(\"\n                        ):\n                            comment_notes.append(\"AutoField?\")\n\n                    # Add 'null' and 'blank', if the 'null_ok' flag was present in the\n                    # table description.\n                    if row.null_ok:  # If it's NULL...\n                        extra_params[\"blank\"] = True\n                        extra_params[\"null\"] = True\n\n                    field_desc = \"%s = %s%s\" % (\n(D) \n                    if is_relation:\n                        ref_db_column, ref_db_table = relations[column_name]\n                        if extra_params.pop(\"unique\", False) or extra_params.get(\n                            \"primary_key\"\n                        ):\n                            rel_type = \"OneToOneField\"\n                        else:\n                            rel_type = \"ForeignKey\"\n                            ref_pk_column = (\n                                connection.introspection.get_primary_key_column(\n                                    cursor, ref_db_table\n                                )\n                            )\n                            if ref_pk_column and ref_pk_column != ref_db_column:\n                                extra_params[\"to_field\"] = ref_db_column\n                        rel_to = (\n                            \"self\"\n                            if ref_db_table == table_name\n                            else table2model(ref_db_table)\n                        )\n                        if rel_to in known_models:\n                            field_type = \"%s(%s\" % (rel_type, rel_to)\n                        else:\n                            field_type = \"%s('%s'\" % (rel_type, rel_to)\n                    else:\n                        # Calling `get_field_type` to get the field type string and any\n                        # additional parameters and notes.\n                        field_type, field_params, field_notes = self.get_field_type(\n                            connection, table_name, row\n                        )\n                        extra_params.update(field_params)\n                        comment_notes.extend(field_notes)\n\n                        field_type += \"(\"\n\n                    # Don't output 'id = meta.AutoField(primary_key=True)', because\n                    # that's assumed if it doesn't exist.\n                    if att_name == \"id\" and extra_params == {\"primary_key\": True}:\n                        if field_type == \"AutoField(\":\n                            continue\n                        elif (\n                            field_type\n                            == connection.features.introspected_field_types[\"AutoField\"]\n                            + \"(\"\n                        ):\n                            comment_notes.append(\"AutoField?\")\n\n                    # Add 'null' and 'blank', if the 'null_ok' flag was present in the\n                    # table description.\n                    if row.null_ok:  # If it's NULL...\n                        extra_params[\"blank\"] = True\n                        extra_params[\"null\"] = True\n\n                    field_desc = \"%s = %s%s\" % (\n                        att_name,\n                        # Custom fields will have a dotted path\n                        \"\" if \".\" in field_type else \"models.\",\n                        field_type,\n                    )\n                    if field_type.startswith((\"ForeignKey(\", \"OneToOneField(\")):\n                        field_desc += \", models.DO_NOTHING\"\n\n                    if extra_params:\n                        if not field_desc.endswith(\"(\"):\n                            field_desc += \", \"\n                        field_desc += \", \".join(\n                            \"%s=%r\" % (k, v) for k, v in extra_params.items()\n                        )\n                    field_desc += \")\"\n                    if comment_notes:\n                        field_desc += \"  # \" + \" \".join(comment_notes)\n                    yield \"    %s\" % field_desc", "answer": "B"}
{"uuid": "5390e1c7-3d20-4973-9358-db3936402979", "issue_statement": "dbshell additional parameters should be passed before dbname on PostgreSQL.\nDescription\n\t\npsql expects all options to proceed the database name, if provided. So, if doing something like `./manage.py dbshell -- -c \"select * from some_table;\" one will get this:\n$ ./manage.py dbshell -- -c \"select * from some_table;\"\npsql: warning: extra command-line argument \"-c\" ignored\npsql: warning: extra command-line argument \"select * from some_table;\" ignored\npsql (10.21)\nType \"help\" for help.\nsome_database=>\nIt appears the args list just need to be constructed in the proper order, leaving the database name for the end of the args list.", "choices": "(A)         args.extend(parameters)\n\n        env = {}\n        if passwd:\n            env[\"PGPASSWORD\"] = str(passwd)\n        if service:\n            env[\"PGSERVICE\"] = str(service)\n        if sslmode:\n            env[\"PGSSLMODE\"] = str(sslmode)\n(B)         if dbname:\n            args += [dbname]\n        args.extend(parameters)\n\n        env = {}\n        if passwd:\n            env[\"PGPASSWORD\"] = str(passwd)\n        if service:\n            env[\"PGSERVICE\"] = str(service)\n(C)         if user:\n            args += [\"-U\", user]\n        if host:\n            args += [\"-h\", host]\n        if port:\n            args += [\"-p\", str(port)]\n        if dbname:\n            args += [dbname]\n        args.extend(parameters)\n(D)             args += [\"-h\", host]\n        if port:\n            args += [\"-p\", str(port)]\n        if dbname:\n            args += [dbname]\n        args.extend(parameters)\n\n        env = {}\n        if passwd:", "answer": "D"}
{"uuid": "9c29fc47-e317-430e-ab4a-2e01b2beb752", "issue_statement": "\"default.html\" deprecation warning raised for ManagementForm's\nDescription\n\t\nI have a project where I never render forms with the {{ form }} expression. However, I'm still getting the new template deprecation warning because of the formset management form production, during which the template used is insignificant (only hidden inputs are produced).\nIs it worth special-casing this and avoid producing the warning for the management forms?", "choices": "(A)     INITIAL_FORMS = IntegerField(widget=HiddenInput)\n    # MIN_NUM_FORM_COUNT and MAX_NUM_FORM_COUNT are output with the rest of the\n    # management form, but only for the convenience of client-side code. The\n    # POST value of them returned from the client is not checked.\n    MIN_NUM_FORMS = IntegerField(required=False, widget=HiddenInput)\n    MAX_NUM_FORMS = IntegerField(required=False, widget=HiddenInput)\n\n    def clean(self):\n(B) \n    TOTAL_FORMS = IntegerField(widget=HiddenInput)\n    INITIAL_FORMS = IntegerField(widget=HiddenInput)\n    # MIN_NUM_FORM_COUNT and MAX_NUM_FORM_COUNT are output with the rest of the\n    # management form, but only for the convenience of client-side code. The\n    # POST value of them returned from the client is not checked.\n    MIN_NUM_FORMS = IntegerField(required=False, widget=HiddenInput)\n    MAX_NUM_FORMS = IntegerField(required=False, widget=HiddenInput)\n(C)     Keep track of how many form instances are displayed on the page. If adding\n    new forms via JavaScript, you should increment the count field of this form\n    as well.\n    \"\"\"\n\n    TOTAL_FORMS = IntegerField(widget=HiddenInput)\n    INITIAL_FORMS = IntegerField(widget=HiddenInput)\n    # MIN_NUM_FORM_COUNT and MAX_NUM_FORM_COUNT are output with the rest of the\n(D)     as well.\n    \"\"\"\n\n    TOTAL_FORMS = IntegerField(widget=HiddenInput)\n    INITIAL_FORMS = IntegerField(widget=HiddenInput)\n    # MIN_NUM_FORM_COUNT and MAX_NUM_FORM_COUNT are output with the rest of the\n    # management form, but only for the convenience of client-side code. The\n    # POST value of them returned from the client is not checked.", "answer": "D"}
{"uuid": "800ace95-933d-4a86-8295-9a171c9f795b", "issue_statement": "Support for serialization of combination of Enum flags.\nDescription\n\t \n\t\t(last modified by Willem Van Onsem)\n\t \nIf we work with a field:\nregex_flags = models.IntegerField(default=re.UNICODE | re.IGNORECASE)\nThis is turned into a migration with:\ndefault=re.RegexFlag[None]\nThis is due to the fact that the EnumSerializer aims to work with the .name of the item, but if there is no single item for the given value, then there is no such name.\nIn that case, we can use enum._decompose to obtain a list of names, and create an expression to create the enum value by \"ORing\" the items together.", "choices": "(A) class DeconstructableSerializer(BaseSerializer):\n    @staticmethod\n    def serialize_deconstructed(path, args, kwargs):\n        name, imports = DeconstructableSerializer._serialize_path(path)\n        strings = []\n        for arg in args:\n            arg_string, arg_imports = serializer_factory(arg).serialize()\n            strings.append(arg_string)\n            imports.update(arg_imports)\n        for kw, arg in sorted(kwargs.items()):\n            arg_string, arg_imports = serializer_factory(arg).serialize()\n            imports.update(arg_imports)\n            strings.append(\"%s=%s\" % (kw, arg_string))\n        return \"%s(%s)\" % (name, \", \".join(strings)), imports\n\n    @staticmethod\n    def _serialize_path(path):\n        module, name = path.rsplit(\".\", 1)\n        if module == \"django.db.models\":\n            imports = {\"from django.db import models\"}\n            name = \"models.%s\" % name\n        else:\n            imports = {\"import %s\" % module}\n            name = path\n        return name, imports\n\n    def serialize(self):\n        return self.serialize_deconstructed(*self.value.deconstruct())\n\n\nclass DictionarySerializer(BaseSerializer):\n    def serialize(self):\n        imports = set()\n        strings = []\n        for k, v in sorted(self.value.items()):\n            k_string, k_imports = serializer_factory(k).serialize()\n            v_string, v_imports = serializer_factory(v).serialize()\n            imports.update(k_imports)\n            imports.update(v_imports)\n            strings.append((k_string, v_string))\n        return \"{%s}\" % (\", \".join(\"%s: %s\" % (k, v) for k, v in strings)), imports\n\n\nclass EnumSerializer(BaseSerializer):\n    def serialize(self):\n        enum_class = self.value.__class__\n        module = enum_class.__module__\n        return (\n            \"%s.%s[%r]\" % (module, enum_class.__qualname__, self.value.name),\n            {\"import %s\" % module},\n        )\n\n\nclass FloatSerializer(BaseSimpleSerializer):\n    def serialize(self):\n        if math.isnan(self.value) or math.isinf(self.value):\n            return 'float(\"{}\")'.format(self.value), set()\n        return super().serialize()\n\n\nclass FrozensetSerializer(BaseSequenceSerializer):\n    def _format(self):\n        return \"frozenset([%s])\"\n\n\nclass FunctionTypeSerializer(BaseSerializer):\n    def serialize(self):\n        if getattr(self.value, \"__self__\", None) and isinstance(\n            self.value.__self__, type\n        ):\n            klass = self.value.__self__\n            module = klass.__module__\n            return \"%s.%s.%s\" % (module, klass.__name__, self.value.__name__), {\n                \"import %s\" % module\n            }\n        # Further error checking\n        if self.value.__name__ == \"<lambda>\":\n            raise ValueError(\"Cannot serialize function: lambda\")\n        if self.value.__module__ is None:\n            raise ValueError(\"Cannot serialize function %r: No module\" % self.value)\n\n        module_name = self.value.__module__\n\n        if \"<\" not in self.value.__qualname__:  # Qualname can include <locals>\n            return \"%s.%s\" % (module_name, self.value.__qualname__), {\n                \"import %s\" % self.value.__module__\n            }\n\n        raise ValueError(\n            \"Could not find function %s in %s.\\n\" % (self.value.__name__, module_name)\n        )\n\n\nclass FunctoolsPartialSerializer(BaseSerializer):\n    def serialize(self):\n        # Serialize functools.partial() arguments\n        func_string, func_imports = serializer_factory(self.value.func).serialize()\n        args_string, args_imports = serializer_factory(self.value.args).serialize()\n        keywords_string, keywords_imports = serializer_factory(\n            self.value.keywords\n        ).serialize()\n        # Add any imports needed by arguments\n        imports = {\"import functools\", *func_imports, *args_imports, *keywords_imports}\n        return (\n            \"functools.%s(%s, *%s, **%s)\"\n            % (\n                self.value.__class__.__name__,\n                func_string,\n                args_string,\n                keywords_string,\n            ),\n            imports,\n        )\n\n\nclass IterableSerializer(BaseSerializer):\n    def serialize(self):\n        imports = set()\n        strings = []\n        for item in self.value:\n            item_string, item_imports = serializer_factory(item).serialize()\n            imports.update(item_imports)\n            strings.append(item_string)\n        # When len(strings)==0, the empty iterable should be serialized as\n        # \"()\", not \"(,)\" because (,) is invalid Python syntax.\n        value = \"(%s)\" if len(strings) != 1 else \"(%s,)\"\n        return value % (\", \".join(strings)), imports\n\n\nclass ModelFieldSerializer(DeconstructableSerializer):\n(B) from django.db.migrations.operations.base import Operation\nfrom django.db.migrations.utils import COMPILED_REGEX_TYPE, RegexObject\nfrom django.utils.functional import LazyObject, Promise\nfrom django.utils.version import get_docs_version\n\n\nclass BaseSerializer:\n    def __init__(self, value):\n        self.value = value\n\n    def serialize(self):\n        raise NotImplementedError(\n            \"Subclasses of BaseSerializer must implement the serialize() method.\"\n        )\n\n\nclass BaseSequenceSerializer(BaseSerializer):\n    def _format(self):\n        raise NotImplementedError(\n            \"Subclasses of BaseSequenceSerializer must implement the _format() method.\"\n        )\n\n    def serialize(self):\n        imports = set()\n        strings = []\n        for item in self.value:\n            item_string, item_imports = serializer_factory(item).serialize()\n            imports.update(item_imports)\n            strings.append(item_string)\n        value = self._format()\n        return value % (\", \".join(strings)), imports\n\n\nclass BaseSimpleSerializer(BaseSerializer):\n    def serialize(self):\n        return repr(self.value), set()\n\n\nclass ChoicesSerializer(BaseSerializer):\n    def serialize(self):\n        return serializer_factory(self.value.value).serialize()\n\n\nclass DateTimeSerializer(BaseSerializer):\n    \"\"\"For datetime.*, except datetime.datetime.\"\"\"\n\n    def serialize(self):\n        return repr(self.value), {\"import datetime\"}\n\n\nclass DatetimeDatetimeSerializer(BaseSerializer):\n    \"\"\"For datetime.datetime.\"\"\"\n\n    def serialize(self):\n        if self.value.tzinfo is not None and self.value.tzinfo != datetime.timezone.utc:\n            self.value = self.value.astimezone(datetime.timezone.utc)\n        imports = [\"import datetime\"]\n        return repr(self.value), set(imports)\n\n\nclass DecimalSerializer(BaseSerializer):\n    def serialize(self):\n        return repr(self.value), {\"from decimal import Decimal\"}\n\n\nclass DeconstructableSerializer(BaseSerializer):\n    @staticmethod\n    def serialize_deconstructed(path, args, kwargs):\n        name, imports = DeconstructableSerializer._serialize_path(path)\n        strings = []\n        for arg in args:\n            arg_string, arg_imports = serializer_factory(arg).serialize()\n            strings.append(arg_string)\n            imports.update(arg_imports)\n        for kw, arg in sorted(kwargs.items()):\n            arg_string, arg_imports = serializer_factory(arg).serialize()\n            imports.update(arg_imports)\n            strings.append(\"%s=%s\" % (kw, arg_string))\n        return \"%s(%s)\" % (name, \", \".join(strings)), imports\n\n    @staticmethod\n    def _serialize_path(path):\n        module, name = path.rsplit(\".\", 1)\n        if module == \"django.db.models\":\n            imports = {\"from django.db import models\"}\n            name = \"models.%s\" % name\n        else:\n            imports = {\"import %s\" % module}\n            name = path\n        return name, imports\n\n    def serialize(self):\n        return self.serialize_deconstructed(*self.value.deconstruct())\n\n\nclass DictionarySerializer(BaseSerializer):\n    def serialize(self):\n        imports = set()\n        strings = []\n        for k, v in sorted(self.value.items()):\n            k_string, k_imports = serializer_factory(k).serialize()\n            v_string, v_imports = serializer_factory(v).serialize()\n            imports.update(k_imports)\n            imports.update(v_imports)\n            strings.append((k_string, v_string))\n        return \"{%s}\" % (\", \".join(\"%s: %s\" % (k, v) for k, v in strings)), imports\n\n\nclass EnumSerializer(BaseSerializer):\n    def serialize(self):\n        enum_class = self.value.__class__\n        module = enum_class.__module__\n        return (\n            \"%s.%s[%r]\" % (module, enum_class.__qualname__, self.value.name),\n            {\"import %s\" % module},\n        )\n\n\nclass FloatSerializer(BaseSimpleSerializer):\n    def serialize(self):\n        if math.isnan(self.value) or math.isinf(self.value):\n            return 'float(\"{}\")'.format(self.value), set()\n        return super().serialize()\n\n\nclass FrozensetSerializer(BaseSequenceSerializer):\n    def _format(self):\n        return \"frozenset([%s])\"\n\n\n(C)         strings = []\n        for k, v in sorted(self.value.items()):\n            k_string, k_imports = serializer_factory(k).serialize()\n            v_string, v_imports = serializer_factory(v).serialize()\n            imports.update(k_imports)\n            imports.update(v_imports)\n            strings.append((k_string, v_string))\n        return \"{%s}\" % (\", \".join(\"%s: %s\" % (k, v) for k, v in strings)), imports\n\n\nclass EnumSerializer(BaseSerializer):\n    def serialize(self):\n        enum_class = self.value.__class__\n        module = enum_class.__module__\n        return (\n            \"%s.%s[%r]\" % (module, enum_class.__qualname__, self.value.name),\n            {\"import %s\" % module},\n        )\n\n\nclass FloatSerializer(BaseSimpleSerializer):\n    def serialize(self):\n        if math.isnan(self.value) or math.isinf(self.value):\n            return 'float(\"{}\")'.format(self.value), set()\n        return super().serialize()\n\n\nclass FrozensetSerializer(BaseSequenceSerializer):\n    def _format(self):\n        return \"frozenset([%s])\"\n\n\nclass FunctionTypeSerializer(BaseSerializer):\n    def serialize(self):\n        if getattr(self.value, \"__self__\", None) and isinstance(\n            self.value.__self__, type\n        ):\n            klass = self.value.__self__\n            module = klass.__module__\n            return \"%s.%s.%s\" % (module, klass.__name__, self.value.__name__), {\n                \"import %s\" % module\n            }\n        # Further error checking\n        if self.value.__name__ == \"<lambda>\":\n            raise ValueError(\"Cannot serialize function: lambda\")\n        if self.value.__module__ is None:\n            raise ValueError(\"Cannot serialize function %r: No module\" % self.value)\n\n        module_name = self.value.__module__\n\n        if \"<\" not in self.value.__qualname__:  # Qualname can include <locals>\n            return \"%s.%s\" % (module_name, self.value.__qualname__), {\n                \"import %s\" % self.value.__module__\n            }\n\n        raise ValueError(\n            \"Could not find function %s in %s.\\n\" % (self.value.__name__, module_name)\n        )\n\n\nclass FunctoolsPartialSerializer(BaseSerializer):\n    def serialize(self):\n        # Serialize functools.partial() arguments\n        func_string, func_imports = serializer_factory(self.value.func).serialize()\n        args_string, args_imports = serializer_factory(self.value.args).serialize()\n        keywords_string, keywords_imports = serializer_factory(\n            self.value.keywords\n        ).serialize()\n        # Add any imports needed by arguments\n        imports = {\"import functools\", *func_imports, *args_imports, *keywords_imports}\n        return (\n            \"functools.%s(%s, *%s, **%s)\"\n            % (\n                self.value.__class__.__name__,\n                func_string,\n                args_string,\n                keywords_string,\n            ),\n            imports,\n        )\n\n\nclass IterableSerializer(BaseSerializer):\n    def serialize(self):\n        imports = set()\n        strings = []\n        for item in self.value:\n            item_string, item_imports = serializer_factory(item).serialize()\n            imports.update(item_imports)\n            strings.append(item_string)\n        # When len(strings)==0, the empty iterable should be serialized as\n        # \"()\", not \"(,)\" because (,) is invalid Python syntax.\n        value = \"(%s)\" if len(strings) != 1 else \"(%s,)\"\n        return value % (\", \".join(strings)), imports\n\n\nclass ModelFieldSerializer(DeconstructableSerializer):\n    def serialize(self):\n        attr_name, path, args, kwargs = self.value.deconstruct()\n        return self.serialize_deconstructed(path, args, kwargs)\n\n\nclass ModelManagerSerializer(DeconstructableSerializer):\n    def serialize(self):\n        as_manager, manager_path, qs_path, args, kwargs = self.value.deconstruct()\n        if as_manager:\n            name, imports = self._serialize_path(qs_path)\n            return \"%s.as_manager()\" % name, imports\n        else:\n            return self.serialize_deconstructed(manager_path, args, kwargs)\n\n\nclass OperationSerializer(BaseSerializer):\n    def serialize(self):\n        from django.db.migrations.writer import OperationWriter\n\n        string, imports = OperationWriter(self.value, indentation=0).serialize()\n        # Nested operation, trailing comma is handled in upper OperationWriter._write()\n        return string.rstrip(\",\"), imports\n\n\nclass PathLikeSerializer(BaseSerializer):\n    def serialize(self):\n        return repr(os.fspath(self.value)), {}\n\n\nclass PathSerializer(BaseSerializer):\n    def serialize(self):\n        # Convert concrete paths to pure paths to avoid issues with migrations\n        # generated on one platform being used on a different platform.\n(D) class BaseSimpleSerializer(BaseSerializer):\n    def serialize(self):\n        return repr(self.value), set()\n\n\nclass ChoicesSerializer(BaseSerializer):\n    def serialize(self):\n        return serializer_factory(self.value.value).serialize()\n\n\nclass DateTimeSerializer(BaseSerializer):\n    \"\"\"For datetime.*, except datetime.datetime.\"\"\"\n\n    def serialize(self):\n        return repr(self.value), {\"import datetime\"}\n\n\nclass DatetimeDatetimeSerializer(BaseSerializer):\n    \"\"\"For datetime.datetime.\"\"\"\n\n    def serialize(self):\n        if self.value.tzinfo is not None and self.value.tzinfo != datetime.timezone.utc:\n            self.value = self.value.astimezone(datetime.timezone.utc)\n        imports = [\"import datetime\"]\n        return repr(self.value), set(imports)\n\n\nclass DecimalSerializer(BaseSerializer):\n    def serialize(self):\n        return repr(self.value), {\"from decimal import Decimal\"}\n\n\nclass DeconstructableSerializer(BaseSerializer):\n    @staticmethod\n    def serialize_deconstructed(path, args, kwargs):\n        name, imports = DeconstructableSerializer._serialize_path(path)\n        strings = []\n        for arg in args:\n            arg_string, arg_imports = serializer_factory(arg).serialize()\n            strings.append(arg_string)\n            imports.update(arg_imports)\n        for kw, arg in sorted(kwargs.items()):\n            arg_string, arg_imports = serializer_factory(arg).serialize()\n            imports.update(arg_imports)\n            strings.append(\"%s=%s\" % (kw, arg_string))\n        return \"%s(%s)\" % (name, \", \".join(strings)), imports\n\n    @staticmethod\n    def _serialize_path(path):\n        module, name = path.rsplit(\".\", 1)\n        if module == \"django.db.models\":\n            imports = {\"from django.db import models\"}\n            name = \"models.%s\" % name\n        else:\n            imports = {\"import %s\" % module}\n            name = path\n        return name, imports\n\n    def serialize(self):\n        return self.serialize_deconstructed(*self.value.deconstruct())\n\n\nclass DictionarySerializer(BaseSerializer):\n    def serialize(self):\n        imports = set()\n        strings = []\n        for k, v in sorted(self.value.items()):\n            k_string, k_imports = serializer_factory(k).serialize()\n            v_string, v_imports = serializer_factory(v).serialize()\n            imports.update(k_imports)\n            imports.update(v_imports)\n            strings.append((k_string, v_string))\n        return \"{%s}\" % (\", \".join(\"%s: %s\" % (k, v) for k, v in strings)), imports\n\n\nclass EnumSerializer(BaseSerializer):\n    def serialize(self):\n        enum_class = self.value.__class__\n        module = enum_class.__module__\n        return (\n            \"%s.%s[%r]\" % (module, enum_class.__qualname__, self.value.name),\n            {\"import %s\" % module},\n        )\n\n\nclass FloatSerializer(BaseSimpleSerializer):\n    def serialize(self):\n        if math.isnan(self.value) or math.isinf(self.value):\n            return 'float(\"{}\")'.format(self.value), set()\n        return super().serialize()\n\n\nclass FrozensetSerializer(BaseSequenceSerializer):\n    def _format(self):\n        return \"frozenset([%s])\"\n\n\nclass FunctionTypeSerializer(BaseSerializer):\n    def serialize(self):\n        if getattr(self.value, \"__self__\", None) and isinstance(\n            self.value.__self__, type\n        ):\n            klass = self.value.__self__\n            module = klass.__module__\n            return \"%s.%s.%s\" % (module, klass.__name__, self.value.__name__), {\n                \"import %s\" % module\n            }\n        # Further error checking\n        if self.value.__name__ == \"<lambda>\":\n            raise ValueError(\"Cannot serialize function: lambda\")\n        if self.value.__module__ is None:\n            raise ValueError(\"Cannot serialize function %r: No module\" % self.value)\n\n        module_name = self.value.__module__\n\n        if \"<\" not in self.value.__qualname__:  # Qualname can include <locals>\n            return \"%s.%s\" % (module_name, self.value.__qualname__), {\n                \"import %s\" % self.value.__module__\n            }\n\n        raise ValueError(\n            \"Could not find function %s in %s.\\n\" % (self.value.__name__, module_name)\n        )\n\n\nclass FunctoolsPartialSerializer(BaseSerializer):\n    def serialize(self):\n        # Serialize functools.partial() arguments\n        func_string, func_imports = serializer_factory(self.value.func).serialize()\n        args_string, args_imports = serializer_factory(self.value.args).serialize()", "answer": "B"}
{"uuid": "beb4e3b2-56c1-4545-8c1a-b08a64cb697a", "issue_statement": "Rendering empty_form crashes when empty_permitted is passed to form_kwargs\nDescription\n\t\nIssue\nWhen explicitly setting form_kwargs = {'empty_permitted':True} or form_kwargs = {'empty_permitted':False} , a KeyError occurs when rendering a template that uses a formset's empty_form.\nExpected Behavior\nempty_permitted is ignored for formset.empty_form since empty_permitted is irrelevant for empty_form, as empty_form is not meant to be used to pass data and therefore does not need to be validated.\nSteps to Reproduce\n# views.py\nfrom django.shortcuts import render\nfrom .models import MyModel\ndef test_view(request):\n\tcontext = {}\n\tff = modelformset_factory(MyModel, fields = ['a_field'])\n\tcontext['formset'] = ff(\n\t\tqueryset = MyModel.objects.none(),\n\t\tform_kwargs = {'empty_permitted':True} # or form_kwargs = {'empty_permitted':False}\n\t)\n\treturn render(request, 'my_app/my_model_formset.html', context)\n# urls.py\nfrom django.urls import path, include\nfrom .views import test_view\nurlpatterns = [\n\tpath('test', test_view)\n]\n# my_model_formset.html\n{% extends \"my_app/base.html\" %}\n{% block content %}\n<form id=\"my-form\" method=\"post\">\n {% csrf_token %}\n {{ formset }}\n <input type=\"submit\" value=\"Save\">\n</form>\n{{ formset.empty_form }}\n{% endblock %}", "choices": "(A)             auto_id=self.auto_id,\n            prefix=self.add_prefix(\"__prefix__\"),\n            empty_permitted=True,\n            use_required_attribute=False,\n            **self.get_form_kwargs(None),\n            renderer=self.renderer,\n        )\n        self.add_fields(form, None)\n        return form\n\n    @property\n    def cleaned_data(self):\n        \"\"\"\n        Return a list of form.cleaned_data dicts for every form in self.forms.\n        \"\"\"\n(B) \n    @property\n    def empty_form(self):\n        form = self.form(\n            auto_id=self.auto_id,\n            prefix=self.add_prefix(\"__prefix__\"),\n            empty_permitted=True,\n            use_required_attribute=False,\n            **self.get_form_kwargs(None),\n            renderer=self.renderer,\n        )\n        self.add_fields(form, None)\n        return form\n\n    @property\n(C)     @property\n    def extra_forms(self):\n        \"\"\"Return a list of all the extra forms in this formset.\"\"\"\n        return self.forms[self.initial_form_count() :]\n\n    @property\n    def empty_form(self):\n        form = self.form(\n            auto_id=self.auto_id,\n            prefix=self.add_prefix(\"__prefix__\"),\n            empty_permitted=True,\n            use_required_attribute=False,\n            **self.get_form_kwargs(None),\n            renderer=self.renderer,\n        )\n(D)             **self.get_form_kwargs(None),\n            renderer=self.renderer,\n        )\n        self.add_fields(form, None)\n        return form\n\n    @property\n    def cleaned_data(self):\n        \"\"\"\n        Return a list of form.cleaned_data dicts for every form in self.forms.\n        \"\"\"\n        if not self.is_valid():\n            raise AttributeError(\n                \"'%s' object has no attribute 'cleaned_data'\" % self.__class__.__name__\n            )", "answer": "B"}
{"uuid": "cf7b38aa-14fb-44dd-a023-ad3e2e9d058c", "issue_statement": "Fix numberformat.py \"string index out of range\" when null\nDescription\n\t\nWhen:\nif str_number[0] == \"-\"\nencounters a number field that's null when formatting for the admin list_display this causes an \nIndexError: string index out of range\nI can attach the proposed fix here, or open a pull request on GitHub if you like?", "choices": "(A)         For non-uniform digit grouping, it can be a sequence with the number\n        of digit group sizes following the format used by the Python locale\n        module in locale.localeconv() LC_NUMERIC grouping (e.g. (3, 2, 0)).\n    * thousand_sep: Thousand separator symbol (for example \",\")\n    \"\"\"\n    use_grouping = (\n        use_l10n or (use_l10n is None and settings.USE_L10N)\n    ) and settings.USE_THOUSAND_SEPARATOR\n(B)     \"\"\"\n    use_grouping = (\n        use_l10n or (use_l10n is None and settings.USE_L10N)\n    ) and settings.USE_THOUSAND_SEPARATOR\n    use_grouping = use_grouping or force_grouping\n    use_grouping = use_grouping and grouping != 0\n    # Make the common case fast\n    if isinstance(number, int) and not use_grouping and not decimal_pos:\n(C)         module in locale.localeconv() LC_NUMERIC grouping (e.g. (3, 2, 0)).\n    * thousand_sep: Thousand separator symbol (for example \",\")\n    \"\"\"\n    use_grouping = (\n        use_l10n or (use_l10n is None and settings.USE_L10N)\n    ) and settings.USE_THOUSAND_SEPARATOR\n    use_grouping = use_grouping or force_grouping\n    use_grouping = use_grouping and grouping != 0\n(D)         use_l10n or (use_l10n is None and settings.USE_L10N)\n    ) and settings.USE_THOUSAND_SEPARATOR\n    use_grouping = use_grouping or force_grouping\n    use_grouping = use_grouping and grouping != 0\n    # Make the common case fast\n    if isinstance(number, int) and not use_grouping and not decimal_pos:\n        return mark_safe(number)\n    # sign", "answer": "C"}
{"uuid": "8bce22c6-b8e0-4bc9-8188-e9bcb41876e6", "issue_statement": "Accessing UserAdmin via to_field leads to link to PasswordResetForm being broken (404)\nDescription\n\t \n\t\t(last modified by Simon Kern)\n\t \nAccessing the UserAdmin via another model's Admin that has a reference to User (with to_field set, e.g., to_field=\"uuid\") leads to the UserAdmin being accessed via an url that looks similar to this one:\n.../user/22222222-3333-4444-5555-666677778888/change/?_to_field=uuid\nHowever the underlying form looks like this: \nCode highlighting:\nclass UserChangeForm(forms.ModelForm):\n\tpassword = ReadOnlyPasswordHashField(\n\t\tlabel=_(\"Password\"),\n\t\thelp_text=_(\n\t\t\t\"Raw passwords are not stored, so there is no way to see this \"\n\t\t\t\"user\u2019s password, but you can change the password using \"\n\t\t\t'<a href=\"{}\">this form</a>.'\n\t\t),\n\t)\n\t...\n\t...\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper().__init__(*args, **kwargs)\n\t\tpassword = self.fields.get(\"password\")\n\t\tif password:\n\t\t\tpassword.help_text = password.help_text.format(\"../password/\")\n\t...\n\t...\nThis results in the link to the PasswordResetForm being wrong and thus ending up in a 404. If we drop the assumption that UserAdmin is always accessed via its pk, then we're good to go. It's as simple as replacing password.help_text = password.help_text.format(\"../password/\") with password.help_text = password.help_text.format(f\"../../{self.instance.pk}/password/\")\nI've opened a pull request on GitHub for this Ticket, please see:\n\u200bPR", "choices": "(A)         field_classes = {\"username\": UsernameField}\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        password = self.fields.get(\"password\")\n        if password:\n            password.help_text = password.help_text.format(\"../password/\")\n        user_permissions = self.fields.get(\"user_permissions\")\n        if user_permissions:\n(B)             password.help_text = password.help_text.format(\"../password/\")\n        user_permissions = self.fields.get(\"user_permissions\")\n        if user_permissions:\n            user_permissions.queryset = user_permissions.queryset.select_related(\n                \"content_type\"\n            )\n\n\nclass AuthenticationForm(forms.Form):\n(C)         if user_permissions:\n            user_permissions.queryset = user_permissions.queryset.select_related(\n                \"content_type\"\n            )\n\n\nclass AuthenticationForm(forms.Form):\n    \"\"\"\n    Base class for authenticating users. Extend this to get a form that accepts\n(D)         super().__init__(*args, **kwargs)\n        password = self.fields.get(\"password\")\n        if password:\n            password.help_text = password.help_text.format(\"../password/\")\n        user_permissions = self.fields.get(\"user_permissions\")\n        if user_permissions:\n            user_permissions.queryset = user_permissions.queryset.select_related(\n                \"content_type\"\n            )", "answer": "D"}
{"uuid": "1b529a10-beb1-4b26-92a0-950f620e450d", "issue_statement": "ModelForm fields with callable defaults don't correctly propagate default values\nDescription\n\t\nWhen creating an object via the admin, if an inline contains an ArrayField in error, the validation will be bypassed (and the inline dismissed) if we submit the form a second time (without modification).\ngo to /admin/my_app/thing/add/\ntype anything in plop\nsubmit -> it shows an error on the inline\nsubmit again -> no errors, plop become unfilled\n# models.py\nclass Thing(models.Model):\n\tpass\nclass RelatedModel(models.Model):\n\tthing = models.ForeignKey(Thing, on_delete=models.CASCADE)\n\tplop = ArrayField(\n\t\tmodels.CharField(max_length=42),\n\t\tdefault=list,\n\t)\n# admin.py\nclass RelatedModelForm(forms.ModelForm):\n\tdef clean(self):\n\t\traise ValidationError(\"whatever\")\nclass RelatedModelInline(admin.TabularInline):\n\tform = RelatedModelForm\n\tmodel = RelatedModel\n\textra = 1\n@admin.register(Thing)\nclass ThingAdmin(admin.ModelAdmin):\n\tinlines = [\n\t\tRelatedModelInline\n\t]\nIt seems related to the hidden input containing the initial value:\n<input type=\"hidden\" name=\"initial-relatedmodel_set-0-plop\" value=\"test\" id=\"initial-relatedmodel_set-0-id_relatedmodel_set-0-plop\">\nI can fix the issue locally by forcing show_hidden_initial=False on the field (in the form init)", "choices": "(A) \n    def as_text(self, attrs=None, **kwargs):\n        \"\"\"\n        Return a string of HTML for representing this as an <input type=\"text\">.\n        \"\"\"\n        return self.as_widget(TextInput(), attrs, **kwargs)\n\n    def as_textarea(self, attrs=None, **kwargs):\n        \"\"\"Return a string of HTML for representing this as a <textarea>.\"\"\"\n        return self.as_widget(Textarea(), attrs, **kwargs)\n\n    def as_hidden(self, attrs=None, **kwargs):\n        \"\"\"\n        Return a string of HTML for representing this as an <input type=\"hidden\">.\n        \"\"\"\n        return self.as_widget(self.field.hidden_widget(), attrs, **kwargs)\n\n(B)             attrs.setdefault(\n                \"id\", self.html_initial_id if only_initial else self.auto_id\n            )\n        return widget.render(\n            name=self.html_initial_name if only_initial else self.html_name,\n            value=self.value(),\n            attrs=attrs,\n            renderer=self.form.renderer,\n        )\n\n    def as_text(self, attrs=None, **kwargs):\n        \"\"\"\n        Return a string of HTML for representing this as an <input type=\"text\">.\n        \"\"\"\n        return self.as_widget(TextInput(), attrs, **kwargs)\n\n    def as_textarea(self, attrs=None, **kwargs):\n(C)         if self.field.localize:\n            widget.is_localized = True\n        attrs = attrs or {}\n        attrs = self.build_widget_attrs(attrs, widget)\n        if self.auto_id and \"id\" not in widget.attrs:\n            attrs.setdefault(\n                \"id\", self.html_initial_id if only_initial else self.auto_id\n            )\n        return widget.render(\n            name=self.html_initial_name if only_initial else self.html_name,\n            value=self.value(),\n            attrs=attrs,\n            renderer=self.form.renderer,\n        )\n\n    def as_text(self, attrs=None, **kwargs):\n        \"\"\"\n(D)             value=self.value(),\n            attrs=attrs,\n            renderer=self.form.renderer,\n        )\n\n    def as_text(self, attrs=None, **kwargs):\n        \"\"\"\n        Return a string of HTML for representing this as an <input type=\"text\">.\n        \"\"\"\n        return self.as_widget(TextInput(), attrs, **kwargs)\n\n    def as_textarea(self, attrs=None, **kwargs):\n        \"\"\"Return a string of HTML for representing this as a <textarea>.\"\"\"\n        return self.as_widget(Textarea(), attrs, **kwargs)\n\n    def as_hidden(self, attrs=None, **kwargs):\n        \"\"\"", "answer": "B"}
{"uuid": "7114ef8f-8127-4996-8766-2fd6e8ad1f99", "issue_statement": "Sitemaps without items raise ValueError on callable lastmod.\nDescription\n\t\nWhen sitemap contains not items, but supports returning lastmod for an item, it fails with a ValueError:\nTraceback (most recent call last):\n File \"/usr/local/lib/python3.10/site-packages/django/core/handlers/exception.py\", line 55, in inner\n\tresponse = get_response(request)\n File \"/usr/local/lib/python3.10/site-packages/django/core/handlers/base.py\", line 197, in _get_response\n\tresponse = wrapped_callback(request, *callback_args, **callback_kwargs)\n File \"/usr/local/lib/python3.10/site-packages/django/utils/decorators.py\", line 133, in _wrapped_view\n\tresponse = view_func(request, *args, **kwargs)\n File \"/usr/local/lib/python3.10/site-packages/django/contrib/sitemaps/views.py\", line 34, in inner\n\tresponse = func(request, *args, **kwargs)\n File \"/usr/local/lib/python3.10/site-packages/django/contrib/sitemaps/views.py\", line 76, in index\n\tsite_lastmod = site.get_latest_lastmod()\n File \"/usr/local/lib/python3.10/site-packages/django/contrib/sitemaps/__init__.py\", line 170, in get_latest_lastmod\n\treturn max([self.lastmod(item) for item in self.items()])\nException Type: ValueError at /sitemap.xml\nException Value: max() arg is an empty sequence\nSomething like this might be a solution:\n\t def get_latest_lastmod(self):\n\t\t if not hasattr(self, \"lastmod\"):\n\t\t\t return None\n\t\t if callable(self.lastmod):\n\t\t\t try:\n\t\t\t\t return max([self.lastmod(item) for item in self.items()])\n-\t\t\texcept TypeError:\n+\t\t\texcept (TypeError, ValueError):\n\t\t\t\t return None\n\t\t else:\n\t\t\t return self.lastmod", "choices": "(A)             try:\n                return max([self.lastmod(item) for item in self.items()])\n            except TypeError:\n                return None\n        else:\n            return self.lastmod\n\n(B)             return None\n        if callable(self.lastmod):\n            try:\n                return max([self.lastmod(item) for item in self.items()])\n            except TypeError:\n                return None\n        else:\n(C)             except TypeError:\n                return None\n        else:\n            return self.lastmod\n\n    def _urls(self, page, protocol, domain):\n        urls = []\n(D)     def get_latest_lastmod(self):\n        if not hasattr(self, \"lastmod\"):\n            return None\n        if callable(self.lastmod):\n            try:\n                return max([self.lastmod(item) for item in self.items()])\n            except TypeError:", "answer": "B"}
{"uuid": "fb967ebd-7189-4fbf-9074-43de58e10207", "issue_statement": "FileBasedCache has_key is susceptible to race conditions\nDescription\n\t \n\t\t(last modified by Marti Raudsepp)\n\t \nI received the exception from Django's cache framework:\nFileNotFoundError: [Errno 2] No such file or directory: '/app/var/cache/d729e4cf4ba88cba5a0f48e0396ec48a.djcache'\n[...]\n File \"django/core/cache/backends/base.py\", line 229, in get_or_set\n\tself.add(key, default, timeout=timeout, version=version)\n File \"django/core/cache/backends/filebased.py\", line 26, in add\n\tif self.has_key(key, version):\n File \"django/core/cache/backends/filebased.py\", line 94, in has_key\n\twith open(fname, \"rb\") as f:\nThe code is:\n\tdef has_key(self, key, version=None):\n\t\tfname = self._key_to_file(key, version)\n\t\tif os.path.exists(fname):\n\t\t\twith open(fname, \"rb\") as f:\n\t\t\t\treturn not self._is_expired(f)\n\t\treturn False\nBetween the exists() check and open(), it's possible for the file to be deleted. In fact, the _is_expired() method itself deletes the file if it finds it to be expired. So if many threads race to read an expired cache at once, it's not that unlikely to hit this window.", "choices": "(A) \n    def has_key(self, key, version=None):\n        fname = self._key_to_file(key, version)\n        if os.path.exists(fname):\n            with open(fname, \"rb\") as f:\n                return not self._is_expired(f)\n        return False\n\n    def _cull(self):\n        \"\"\"\n        Remove random cache entries if max_entries is reached at a ratio\n(B)         return False\n\n    def _cull(self):\n        \"\"\"\n        Remove random cache entries if max_entries is reached at a ratio\n        of num_entries / cull_frequency. A value of 0 for CULL_FREQUENCY means\n        that the entire cache will be purged.\n        \"\"\"\n        filelist = self._list_cache_files()\n        num_entries = len(filelist)\n        if num_entries < self._max_entries:\n(C)             # The file may have been removed by another process.\n            return False\n        return True\n\n    def has_key(self, key, version=None):\n        fname = self._key_to_file(key, version)\n        if os.path.exists(fname):\n            with open(fname, \"rb\") as f:\n                return not self._is_expired(f)\n        return False\n\n(D)         if os.path.exists(fname):\n            with open(fname, \"rb\") as f:\n                return not self._is_expired(f)\n        return False\n\n    def _cull(self):\n        \"\"\"\n        Remove random cache entries if max_entries is reached at a ratio\n        of num_entries / cull_frequency. A value of 0 for CULL_FREQUENCY means\n        that the entire cache will be purged.\n        \"\"\"", "answer": "A"}
{"uuid": "744d6726-a20b-44aa-ac8c-4948736d2f5a", "issue_statement": "migrate management command does not respect database parameter when adding Permissions.\nDescription\n\t \n\t\t(last modified by Vasanth)\n\t \nWhen invoking migrate with a database parameter, the migration runs successfully. However, there seems to be a DB read request that runs after the migration. This call does not respect the db param and invokes the db router .\nWhen naming the db as a parameter, all DB calls in the context of the migrate command are expected to use the database specified.\nI came across this as I am currently using a thread-local variable to get the active DB with a custom DB router for a multi-tenant service .\nMinimal example \nSetup the custom middleware and custom DB Router as show below. Then run any DB migration. We see that \"read {}\" is being printed before the exception message.\nIdeally none of this code must be called as the DB was specified during management command.\nfrom threading import local\nfrom django.conf import settings\nlocal_state = local()\nclass InvalidTenantException(Exception):\n\tpass\nclass TenantSubdomainMiddleware:\n\tdef __init__(self, get_response):\n\t\tself.get_response = get_response\n\tdef __call__(self, request):\n\t\t## Get Subdomain\n\t\thost = request.get_host().split(\":\")[0]\n\t\tlocal_state.subdomain = (\n\t\t\t# We assume single level of subdomain : app.service.com \n\t\t\t# HOST_IP : used to for local dev. \n\t\t\thost if host in settings.HOST_IP else host.split(\".\")[0]\n\t\t)\n\t\tresponse = self.get_response(request)\n\t\treturn response\nclass TenantDatabaseRouter:\n\tdef _default_db(self):\n\t\tsubdomain = getattr(local_state, \"subdomain\", None)\n\t\tif subdomain is not None and subdomain in settings.TENANT_MAP:\n\t\t\tdb_name = settings.TENANT_MAP[local_state.subdomain]\n\t\t\treturn db_name\n\t\telse:\n\t\t\traise InvalidTenantException()\n\tdef db_for_read(self, model, **hints):\n\t\tprint(\"read\", hints)\n\t\treturn self._default_db()\n\tdef db_for_write(self, model, **hints):\n\t\tprint(\"write\", hints)\n\t\treturn self._default_db()\n\tdef allow_relation(self, obj1, obj2, **hints):\n\t\treturn None\n\tdef allow_migrate(self, db, app_label, model_name=None, **hints):\n\t\treturn None\n## settings.py\nMIDDLEWARE = [\n\t\"utils.tenant_db_router.TenantSubdomainMiddleware\",\n\t\"django.middleware.security.SecurityMiddleware\",\n\t...\n]\nTENANT_MAP = {\"localhost\":\"default\", \"tenant_1\":\"default\"}\nDATABASE_ROUTERS = [\"utils.tenant_db_router.TenantDatabaseRouter\"]", "choices": "(A)         Permission.objects.using(using)\n        .filter(\n            content_type__in=ctypes,\n        )\n        .values_list(\"content_type\", \"codename\")\n    )\n\n    perms = [\n        Permission(codename=codename, name=name, content_type=ct)\n        for ct, (codename, name) in searched_perms\n        if (ct.pk, codename) not in all_perms\n    ]\n    Permission.objects.using(using).bulk_create(perms)\n    if verbosity >= 2:\n        for perm in perms:\n            print(\"Adding permission '%s'\" % perm)\n(B)         Permission(codename=codename, name=name, content_type=ct)\n        for ct, (codename, name) in searched_perms\n        if (ct.pk, codename) not in all_perms\n    ]\n    Permission.objects.using(using).bulk_create(perms)\n    if verbosity >= 2:\n        for perm in perms:\n            print(\"Adding permission '%s'\" % perm)\n\n\ndef get_system_username():\n    \"\"\"\n    Return the current system user's username, or an empty string if the\n    username could not be determined.\n    \"\"\"\n    try:\n(C)     Permission.objects.using(using).bulk_create(perms)\n    if verbosity >= 2:\n        for perm in perms:\n            print(\"Adding permission '%s'\" % perm)\n\n\ndef get_system_username():\n    \"\"\"\n    Return the current system user's username, or an empty string if the\n    username could not be determined.\n    \"\"\"\n    try:\n        result = getpass.getuser()\n    except (ImportError, KeyError):\n        # KeyError will be raised by os.getpwuid() (called by getuser())\n        # if there is no corresponding entry in the /etc/passwd file\n(D)         .values_list(\"content_type\", \"codename\")\n    )\n\n    perms = [\n        Permission(codename=codename, name=name, content_type=ct)\n        for ct, (codename, name) in searched_perms\n        if (ct.pk, codename) not in all_perms\n    ]\n    Permission.objects.using(using).bulk_create(perms)\n    if verbosity >= 2:\n        for perm in perms:\n            print(\"Adding permission '%s'\" % perm)\n\n\ndef get_system_username():\n    \"\"\"", "answer": "D"}
{"uuid": "ce787575-e9e1-402a-85f3-432338cb905e", "issue_statement": "Multi-level FilteredRelation with select_related() may set wrong related object.\nDescription\n\t\ntest case:\n# add to known_related_objects.tests.ExistingRelatedInstancesTests\n\tdef test_wrong_select_related(self):\n\t\twith self.assertNumQueries(3):\n\t\t\tp = list(PoolStyle.objects.annotate(\n\t\t\t\ttournament_pool=FilteredRelation('pool__tournament__pool'),\n\t\t\t\t).select_related('tournament_pool'))\n\t\t\tself.assertEqual(p[0].pool.tournament, p[0].tournament_pool.tournament)\nresult:\n======================================================================\nFAIL: test_wrong_select_related (known_related_objects.tests.ExistingRelatedInstancesTests.test_wrong_select_related)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n File \"D:\\Work\\django\\tests\\known_related_objects\\tests.py\", line 171, in test_wrong_select_related\n\tself.assertEqual(p[0].pool.tournament, p[0].tournament_pool.tournament)\nAssertionError: <Tournament: Tournament object (1)> != <PoolStyle: PoolStyle object (1)>\n----------------------------------------------------------------------", "choices": "(A)                     alias,\n                    cur_depth + 1,\n                    next,\n                    restricted,\n                )\n                get_related_klass_infos(klass_info, next_klass_infos)\n\n            def local_setter(final_field, obj, from_obj):\n                # Set a reverse fk object when relation is non-empty.\n                if from_obj:\n                    final_field.remote_field.set_cached_value(from_obj, obj)\n\n            def remote_setter(name, obj, from_obj):\n                setattr(from_obj, name, obj)\n\n            for name in list(requested):\n                # Filtered relations work only on the topmost level.\n                if cur_depth > 1:\n                    break\n                if name in self.query._filtered_relations:\n                    fields_found.add(name)\n                    final_field, _, join_opts, joins, _, _ = self.query.setup_joins(\n                        [name], opts, root_alias\n                    )\n                    model = join_opts.model\n                    alias = joins[-1]\n                    from_parent = (\n                        issubclass(model, opts.model) and model is not opts.model\n                    )\n                    klass_info = {\n                        \"model\": model,\n                        \"field\": final_field,\n                        \"reverse\": True,\n                        \"local_setter\": partial(local_setter, final_field),\n                        \"remote_setter\": partial(remote_setter, name),\n(B)                 if from_obj:\n                    final_field.remote_field.set_cached_value(from_obj, obj)\n\n            def remote_setter(name, obj, from_obj):\n                setattr(from_obj, name, obj)\n\n            for name in list(requested):\n                # Filtered relations work only on the topmost level.\n                if cur_depth > 1:\n                    break\n                if name in self.query._filtered_relations:\n                    fields_found.add(name)\n                    final_field, _, join_opts, joins, _, _ = self.query.setup_joins(\n                        [name], opts, root_alias\n                    )\n                    model = join_opts.model\n                    alias = joins[-1]\n                    from_parent = (\n                        issubclass(model, opts.model) and model is not opts.model\n                    )\n                    klass_info = {\n                        \"model\": model,\n                        \"field\": final_field,\n                        \"reverse\": True,\n                        \"local_setter\": partial(local_setter, final_field),\n                        \"remote_setter\": partial(remote_setter, name),\n                        \"from_parent\": from_parent,\n                    }\n                    related_klass_infos.append(klass_info)\n                    select_fields = []\n                    field_select_mask = select_mask.get((name, final_field)) or {}\n                    columns = self.get_default_columns(\n                        field_select_mask,\n                        start_alias=alias,\n                        opts=model._meta,\n(C)                         issubclass(model, opts.model) and model is not opts.model\n                    )\n                    klass_info = {\n                        \"model\": model,\n                        \"field\": final_field,\n                        \"reverse\": True,\n                        \"local_setter\": partial(local_setter, final_field),\n                        \"remote_setter\": partial(remote_setter, name),\n                        \"from_parent\": from_parent,\n                    }\n                    related_klass_infos.append(klass_info)\n                    select_fields = []\n                    field_select_mask = select_mask.get((name, final_field)) or {}\n                    columns = self.get_default_columns(\n                        field_select_mask,\n                        start_alias=alias,\n                        opts=model._meta,\n                        from_parent=opts.model,\n                    )\n                    for col in columns:\n                        select_fields.append(len(select))\n                        select.append((col, None))\n                    klass_info[\"select_fields\"] = select_fields\n                    next_requested = requested.get(name, {})\n                    next_klass_infos = self.get_related_selections(\n                        select,\n                        field_select_mask,\n                        opts=model._meta,\n                        root_alias=alias,\n                        cur_depth=cur_depth + 1,\n                        requested=next_requested,\n                        restricted=restricted,\n                    )\n                    get_related_klass_infos(klass_info, next_klass_infos)\n            fields_not_found = set(requested).difference(fields_found)\n(D)                     break\n                if name in self.query._filtered_relations:\n                    fields_found.add(name)\n                    final_field, _, join_opts, joins, _, _ = self.query.setup_joins(\n                        [name], opts, root_alias\n                    )\n                    model = join_opts.model\n                    alias = joins[-1]\n                    from_parent = (\n                        issubclass(model, opts.model) and model is not opts.model\n                    )\n                    klass_info = {\n                        \"model\": model,\n                        \"field\": final_field,\n                        \"reverse\": True,\n                        \"local_setter\": partial(local_setter, final_field),\n                        \"remote_setter\": partial(remote_setter, name),\n                        \"from_parent\": from_parent,\n                    }\n                    related_klass_infos.append(klass_info)\n                    select_fields = []\n                    field_select_mask = select_mask.get((name, final_field)) or {}\n                    columns = self.get_default_columns(\n                        field_select_mask,\n                        start_alias=alias,\n                        opts=model._meta,\n                        from_parent=opts.model,\n                    )\n                    for col in columns:\n                        select_fields.append(len(select))\n                        select.append((col, None))\n                    klass_info[\"select_fields\"] = select_fields\n                    next_requested = requested.get(name, {})\n                    next_klass_infos = self.get_related_selections(\n                        select,", "answer": "B"}
{"uuid": "975b7f4a-3c31-4966-ba86-0054add29e00", "issue_statement": "\"show_save_as_new\" in admin can add without this permission\nDescription\n\t \n\t\t(last modified by Mariusz Felisiak)\n\t \nAt \"django/contrib/admin/templatetags/admin_modify.py\" file, line 102, I think you must put one more verification for this tag: \"and has_add_permission\", because \"save_as_new\" is a add modification.\nI rewrite this for my project:\n\t\t\t\"show_save_as_new\": not is_popup\n\t\t\tand has_add_permission # This line that I put!!!\n\t\t\tand has_change_permission\n\t\t\tand change\n\t\t\tand save_as,", "choices": "(A)             \"show_save_as_new\": not is_popup\n            and has_change_permission\n            and change\n            and save_as,\n            \"show_save_and_add_another\": can_save_and_add_another,\n            \"show_save_and_continue\": can_save_and_continue,\n            \"show_save\": show_save and can_save,\n(B)             and change\n            and save_as,\n            \"show_save_and_add_another\": can_save_and_add_another,\n            \"show_save_and_continue\": can_save_and_continue,\n            \"show_save\": show_save and can_save,\n            \"show_close\": not (show_save and can_save),\n        }\n(C)                 and context[\"has_delete_permission\"]\n                and change\n                and context.get(\"show_delete\", True)\n            ),\n            \"show_save_as_new\": not is_popup\n            and has_change_permission\n            and change\n(D)                 and context.get(\"show_delete\", True)\n            ),\n            \"show_save_as_new\": not is_popup\n            and has_change_permission\n            and change\n            and save_as,\n            \"show_save_and_add_another\": can_save_and_add_another,", "answer": "D"}
{"uuid": "20016038-4df3-4630-aaa9-60bcc4a2f2e0", "issue_statement": "Migration optimizer does not reduce multiple AlterField\nDescription\n\t\nLet's consider the following operations: \noperations = [\n\tmigrations.AddField(\n\t\tmodel_name=\"book\",\n\t\tname=\"title\",\n\t\tfield=models.CharField(max_length=256, null=True),\n\t),\n\tmigrations.AlterField(\n\t\tmodel_name=\"book\",\n\t\tname=\"title\",\n\t\tfield=models.CharField(max_length=128, null=True),\n\t),\n\tmigrations.AlterField(\n\t\tmodel_name=\"book\",\n\t\tname=\"title\",\n\t\tfield=models.CharField(max_length=128, null=True, help_text=\"help\"),\n\t),\n\tmigrations.AlterField(\n\t\tmodel_name=\"book\",\n\t\tname=\"title\",\n\t\tfield=models.CharField(max_length=128, null=True, help_text=\"help\", default=None),\n\t),\n]\nIf I run the optimizer, I get only the AddField, as we could expect. However, if the AddField model is separated from the AlterField (e.g. because of a non-elidable migration, or inside a non-squashed migration), none of the AlterField are reduced:\noptimizer.optimize(operations[1:], \"books\") \n[<AlterField model_name='book', name='title', field=<django.db.models.fields.CharField>>,\n <AlterField model_name='book', name='title', field=<django.db.models.fields.CharField>>,\n <AlterField model_name='book', name='title', field=<django.db.models.fields.CharField>>]\nIndeed, the AlterField.reduce does not consider the the case where operation is also an AlterField. \nIs this behaviour intended? If so, could it be documented? \nOtherwise, would it make sense to add something like\n\t\tif isinstance(operation, AlterField) and self.is_same_field_operation(\n\t\t\toperation\n\t\t):\n\t\t\treturn [operation]", "choices": "(A)         if isinstance(operation, RemoveField) and self.is_same_field_operation(\n            operation\n        ):\n            return [operation]\n        elif (\n            isinstance(operation, RenameField)\n            and self.is_same_field_operation(operation)\n            and self.field.db_column is None\n        ):\n(B) \n    @property\n    def migration_name_fragment(self):\n        return \"alter_%s_%s\" % (self.model_name_lower, self.name_lower)\n\n    def reduce(self, operation, app_label):\n        if isinstance(operation, RemoveField) and self.is_same_field_operation(\n            operation\n        ):\n(C)         ):\n            return [operation]\n        elif (\n            isinstance(operation, RenameField)\n            and self.is_same_field_operation(operation)\n            and self.field.db_column is None\n        ):\n            return [\n                operation,\n(D)         return \"alter_%s_%s\" % (self.model_name_lower, self.name_lower)\n\n    def reduce(self, operation, app_label):\n        if isinstance(operation, RemoveField) and self.is_same_field_operation(\n            operation\n        ):\n            return [operation]\n        elif (\n            isinstance(operation, RenameField)", "answer": "D"}
{"uuid": "3444bc60-c4af-4e7d-b443-cce97c5245aa", "issue_statement": "Error E108 does not cover some cases\nDescription\n\t \n\t\t(last modified by Baha Sdtbekov)\n\t \nI have two models, Question and Choice. And if I write list_display = [\"choice\"] in QuestionAdmin, I get no errors.\nBut when I visit /admin/polls/question/, the following trace is returned:\nInternal Server Error: /admin/polls/question/\nTraceback (most recent call last):\n File \"/some/path/django/contrib/admin/utils.py\", line 334, in label_for_field\n\tfield = _get_non_gfk_field(model._meta, name)\n File \"/some/path/django/contrib/admin/utils.py\", line 310, in _get_non_gfk_field\n\traise FieldDoesNotExist()\ndjango.core.exceptions.FieldDoesNotExist\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n File \"/some/path/django/core/handlers/exception.py\", line 55, in inner\n\tresponse = get_response(request)\n File \"/some/path/django/core/handlers/base.py\", line 220, in _get_response\n\tresponse = response.render()\n File \"/some/path/django/template/response.py\", line 111, in render\n\tself.content = self.rendered_content\n File \"/some/path/django/template/response.py\", line 89, in rendered_content\n\treturn template.render(context, self._request)\n File \"/some/path/django/template/backends/django.py\", line 61, in render\n\treturn self.template.render(context)\n File \"/some/path/django/template/base.py\", line 175, in render\n\treturn self._render(context)\n File \"/some/path/django/template/base.py\", line 167, in _render\n\treturn self.nodelist.render(context)\n File \"/some/path/django/template/base.py\", line 1005, in render\n\treturn SafeString(\"\".join([node.render_annotated(context) for node in self]))\n File \"/some/path/django/template/base.py\", line 1005, in <listcomp>\n\treturn SafeString(\"\".join([node.render_annotated(context) for node in self]))\n File \"/some/path/django/template/base.py\", line 966, in render_annotated\n\treturn self.render(context)\n File \"/some/path/django/template/loader_tags.py\", line 157, in render\n\treturn compiled_parent._render(context)\n File \"/some/path/django/template/base.py\", line 167, in _render\n\treturn self.nodelist.render(context)\n File \"/some/path/django/template/base.py\", line 1005, in render\n\treturn SafeString(\"\".join([node.render_annotated(context) for node in self]))\n File \"/some/path/django/template/base.py\", line 1005, in <listcomp>\n\treturn SafeString(\"\".join([node.render_annotated(context) for node in self]))\n File \"/some/path/django/template/base.py\", line 966, in render_annotated\n\treturn self.render(context)\n File \"/some/path/django/template/loader_tags.py\", line 157, in render\n\treturn compiled_parent._render(context)\n File \"/some/path/django/template/base.py\", line 167, in _render\n\treturn self.nodelist.render(context)\n File \"/some/path/django/template/base.py\", line 1005, in render\n\treturn SafeString(\"\".join([node.render_annotated(context) for node in self]))\n File \"/some/path/django/template/base.py\", line 1005, in <listcomp>\n\treturn SafeString(\"\".join([node.render_annotated(context) for node in self]))\n File \"/some/path/django/template/base.py\", line 966, in render_annotated\n\treturn self.render(context)\n File \"/some/path/django/template/loader_tags.py\", line 63, in render\n\tresult = block.nodelist.render(context)\n File \"/some/path/django/template/base.py\", line 1005, in render\n\treturn SafeString(\"\".join([node.render_annotated(context) for node in self]))\n File \"/some/path/django/template/base.py\", line 1005, in <listcomp>\n\treturn SafeString(\"\".join([node.render_annotated(context) for node in self]))\n File \"/some/path/django/template/base.py\", line 966, in render_annotated\n\treturn self.render(context)\n File \"/some/path/django/template/loader_tags.py\", line 63, in render\n\tresult = block.nodelist.render(context)\n File \"/some/path/django/template/base.py\", line 1005, in render\n\treturn SafeString(\"\".join([node.render_annotated(context) for node in self]))\n File \"/some/path/django/template/base.py\", line 1005, in <listcomp>\n\treturn SafeString(\"\".join([node.render_annotated(context) for node in self]))\n File \"/some/path/django/template/base.py\", line 966, in render_annotated\n\treturn self.render(context)\n File \"/some/path/django/contrib/admin/templatetags/base.py\", line 45, in render\n\treturn super().render(context)\n File \"/some/path/django/template/library.py\", line 258, in render\n\t_dict = self.func(*resolved_args, **resolved_kwargs)\n File \"/some/path/django/contrib/admin/templatetags/admin_list.py\", line 326, in result_list\n\theaders = list(result_headers(cl))\n File \"/some/path/django/contrib/admin/templatetags/admin_list.py\", line 90, in result_headers\n\ttext, attr = label_for_field(\n File \"/some/path/django/contrib/admin/utils.py\", line 362, in label_for_field\n\traise AttributeError(message)\nAttributeError: Unable to lookup 'choice' on Question or QuestionAdmin\n[24/Apr/2023 15:43:32] \"GET /admin/polls/question/ HTTP/1.1\" 500 349913\nI suggest that error E108 be updated to cover this case as well\nFor reproduce see \u200bgithub", "choices": "(A)         if isinstance(field, models.ManyToManyField) or (\n            getattr(field, \"rel\", None) and field.rel.field.many_to_one\n        ):\n            return [\n                checks.Error(\n                    f\"The value of '{label}' must not be a many-to-many field or a \"\n                    f\"reverse foreign key.\",\n                    obj=obj.__class__,\n                    id=\"admin.E109\",\n                )\n(B)         ):\n            return [\n                checks.Error(\n                    f\"The value of '{label}' must not be a many-to-many field or a \"\n                    f\"reverse foreign key.\",\n                    obj=obj.__class__,\n                    id=\"admin.E109\",\n                )\n            ]\n        return []\n(C)                             obj.model._meta.label,\n                        ),\n                        obj=obj.__class__,\n                        id=\"admin.E108\",\n                    )\n                ]\n        if isinstance(field, models.ManyToManyField) or (\n            getattr(field, \"rel\", None) and field.rel.field.many_to_one\n        ):\n            return [\n(D)                         id=\"admin.E108\",\n                    )\n                ]\n        if isinstance(field, models.ManyToManyField) or (\n            getattr(field, \"rel\", None) and field.rel.field.many_to_one\n        ):\n            return [\n                checks.Error(\n                    f\"The value of '{label}' must not be a many-to-many field or a \"\n                    f\"reverse foreign key.\",", "answer": "D"}
{"uuid": "ee7e26dd-bcf6-4327-ba85-901734ea926e", "issue_statement": "Squashing migrations with Meta.index_together -> indexes transition should remove deprecation warnings.\nDescription\n\t\nSquashing migrations with Meta.index_together -> Meta.indexes transition should remove deprecation warnings. As far as I'm aware, it's a 4.2 release blocker because you cannot get rid of the index_together deprecation warnings without rewriting migrations, see comment.", "choices": "(A)                         managers=self.managers,\n                    ),\n                ]\n        return super().reduce(operation, app_label)\n\n\nclass DeleteModel(ModelOperation):\n    \"\"\"Drop a model's table.\"\"\"\n\n    def deconstruct(self):\n        kwargs = {\n            \"name\": self.name,\n        }\n        return (self.__class__.__qualname__, [], kwargs)\n\n    def state_forwards(self, app_label, state):\n        state.remove_model(app_label, self.name_lower)\n\n    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n        model = from_state.apps.get_model(app_label, self.name)\n        if self.allow_migrate_model(schema_editor.connection.alias, model):\n            schema_editor.delete_model(model)\n\n    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n        model = to_state.apps.get_model(app_label, self.name)\n        if self.allow_migrate_model(schema_editor.connection.alias, model):\n            schema_editor.create_model(model)\n\n    def references_model(self, name, app_label):\n        # The deleted model could be referencing the specified model through\n        # related fields.\n        return True\n\n    def describe(self):\n        return \"Delete model %s\" % self.name\n\n    @property\n    def migration_name_fragment(self):\n        return \"delete_%s\" % self.name_lower\n\n\nclass RenameModel(ModelOperation):\n    \"\"\"Rename a model.\"\"\"\n\n    def __init__(self, old_name, new_name):\n        self.old_name = old_name\n        self.new_name = new_name\n        super().__init__(old_name)\n\n    @cached_property\n    def old_name_lower(self):\n        return self.old_name.lower()\n\n    @cached_property\n    def new_name_lower(self):\n        return self.new_name.lower()\n\n    def deconstruct(self):\n        kwargs = {\n            \"old_name\": self.old_name,\n            \"new_name\": self.new_name,\n        }\n        return (self.__class__.__qualname__, [], kwargs)\n\n    def state_forwards(self, app_label, state):\n        state.rename_model(app_label, self.old_name, self.new_name)\n\n    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n        new_model = to_state.apps.get_model(app_label, self.new_name)\n        if self.allow_migrate_model(schema_editor.connection.alias, new_model):\n            old_model = from_state.apps.get_model(app_label, self.old_name)\n(B)     @property\n    def migration_name_fragment(self):\n        return \"delete_%s\" % self.name_lower\n\n\nclass RenameModel(ModelOperation):\n    \"\"\"Rename a model.\"\"\"\n\n    def __init__(self, old_name, new_name):\n        self.old_name = old_name\n        self.new_name = new_name\n        super().__init__(old_name)\n\n    @cached_property\n    def old_name_lower(self):\n        return self.old_name.lower()\n\n    @cached_property\n    def new_name_lower(self):\n        return self.new_name.lower()\n\n    def deconstruct(self):\n        kwargs = {\n            \"old_name\": self.old_name,\n            \"new_name\": self.new_name,\n        }\n        return (self.__class__.__qualname__, [], kwargs)\n\n    def state_forwards(self, app_label, state):\n        state.rename_model(app_label, self.old_name, self.new_name)\n\n    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n        new_model = to_state.apps.get_model(app_label, self.new_name)\n        if self.allow_migrate_model(schema_editor.connection.alias, new_model):\n            old_model = from_state.apps.get_model(app_label, self.old_name)\n            # Move the main table\n            schema_editor.alter_db_table(\n                new_model,\n                old_model._meta.db_table,\n                new_model._meta.db_table,\n            )\n            # Alter the fields pointing to us\n            for related_object in old_model._meta.related_objects:\n                if related_object.related_model == old_model:\n                    model = new_model\n                    related_key = (app_label, self.new_name_lower)\n                else:\n                    model = related_object.related_model\n                    related_key = (\n                        related_object.related_model._meta.app_label,\n                        related_object.related_model._meta.model_name,\n                    )\n                to_field = to_state.apps.get_model(*related_key)._meta.get_field(\n                    related_object.field.name\n                )\n                schema_editor.alter_field(\n                    model,\n                    related_object.field,\n                    to_field,\n                )\n            # Rename M2M fields whose name is based on this model's name.\n            fields = zip(\n                old_model._meta.local_many_to_many, new_model._meta.local_many_to_many\n            )\n            for old_field, new_field in fields:\n                # Skip self-referential fields as these are renamed above.\n                if (\n                    new_field.model == new_field.related_model\n                    or not new_field.remote_field.through._meta.auto_created\n                ):\n                    continue\n(C)                             tuple(\n                                operation.new_name if f == operation.old_name else f\n                                for f in fields\n                            )\n                            for fields in option\n                        }\n                order_with_respect_to = options.get(\"order_with_respect_to\")\n                if order_with_respect_to == operation.old_name:\n                    options[\"order_with_respect_to\"] = operation.new_name\n                return [\n                    CreateModel(\n                        self.name,\n                        fields=[\n                            (operation.new_name if n == operation.old_name else n, v)\n                            for n, v in self.fields\n                        ],\n                        options=options,\n                        bases=self.bases,\n                        managers=self.managers,\n                    ),\n                ]\n        return super().reduce(operation, app_label)\n\n\nclass DeleteModel(ModelOperation):\n    \"\"\"Drop a model's table.\"\"\"\n\n    def deconstruct(self):\n        kwargs = {\n            \"name\": self.name,\n        }\n        return (self.__class__.__qualname__, [], kwargs)\n\n    def state_forwards(self, app_label, state):\n        state.remove_model(app_label, self.name_lower)\n\n    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n        model = from_state.apps.get_model(app_label, self.name)\n        if self.allow_migrate_model(schema_editor.connection.alias, model):\n            schema_editor.delete_model(model)\n\n    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n        model = to_state.apps.get_model(app_label, self.name)\n        if self.allow_migrate_model(schema_editor.connection.alias, model):\n            schema_editor.create_model(model)\n\n    def references_model(self, name, app_label):\n        # The deleted model could be referencing the specified model through\n        # related fields.\n        return True\n\n    def describe(self):\n        return \"Delete model %s\" % self.name\n\n    @property\n    def migration_name_fragment(self):\n        return \"delete_%s\" % self.name_lower\n\n\nclass RenameModel(ModelOperation):\n    \"\"\"Rename a model.\"\"\"\n\n    def __init__(self, old_name, new_name):\n        self.old_name = old_name\n        self.new_name = new_name\n        super().__init__(old_name)\n\n    @cached_property\n    def old_name_lower(self):\n        return self.old_name.lower()\n\n(D)     def database_forwards(self, app_label, schema_editor, from_state, to_state):\n        model = from_state.apps.get_model(app_label, self.name)\n        if self.allow_migrate_model(schema_editor.connection.alias, model):\n            schema_editor.delete_model(model)\n\n    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n        model = to_state.apps.get_model(app_label, self.name)\n        if self.allow_migrate_model(schema_editor.connection.alias, model):\n            schema_editor.create_model(model)\n\n    def references_model(self, name, app_label):\n        # The deleted model could be referencing the specified model through\n        # related fields.\n        return True\n\n    def describe(self):\n        return \"Delete model %s\" % self.name\n\n    @property\n    def migration_name_fragment(self):\n        return \"delete_%s\" % self.name_lower\n\n\nclass RenameModel(ModelOperation):\n    \"\"\"Rename a model.\"\"\"\n\n    def __init__(self, old_name, new_name):\n        self.old_name = old_name\n        self.new_name = new_name\n        super().__init__(old_name)\n\n    @cached_property\n    def old_name_lower(self):\n        return self.old_name.lower()\n\n    @cached_property\n    def new_name_lower(self):\n        return self.new_name.lower()\n\n    def deconstruct(self):\n        kwargs = {\n            \"old_name\": self.old_name,\n            \"new_name\": self.new_name,\n        }\n        return (self.__class__.__qualname__, [], kwargs)\n\n    def state_forwards(self, app_label, state):\n        state.rename_model(app_label, self.old_name, self.new_name)\n\n    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n        new_model = to_state.apps.get_model(app_label, self.new_name)\n        if self.allow_migrate_model(schema_editor.connection.alias, new_model):\n            old_model = from_state.apps.get_model(app_label, self.old_name)\n            # Move the main table\n            schema_editor.alter_db_table(\n                new_model,\n                old_model._meta.db_table,\n                new_model._meta.db_table,\n            )\n            # Alter the fields pointing to us\n            for related_object in old_model._meta.related_objects:\n                if related_object.related_model == old_model:\n                    model = new_model\n                    related_key = (app_label, self.new_name_lower)\n                else:\n                    model = related_object.related_model\n                    related_key = (\n                        related_object.related_model._meta.app_label,\n                        related_object.related_model._meta.model_name,\n                    )\n                to_field = to_state.apps.get_model(*related_key)._meta.get_field(", "answer": "A"}
{"uuid": "9e5fd397-2bf8-41fd-9e4b-0ca04208beb2", "issue_statement": "Template filter `join` should not escape the joining string if `autoescape` is `off`\nDescription\n\t\nConsider the following template code snippet:\n{% autoescape off %}\n{{ some_list|join:some_var }}\n{% endautoescape %}\nin this case, the items inside some_list will not be escaped (matching the expected behavior) but some_var will forcibly be escaped. From the docs for autoescape or join I don't think this is expected behavior.\nThe following testcase illustrates what I think is a bug in the join filter (run inside the template_tests/filter_tests folder):\nfrom django.template.defaultfilters import escape\nfrom django.test import SimpleTestCase\nfrom ..utils import setup\nclass RegressionTests(SimpleTestCase):\n\t@setup({\"join01\": '{{ some_list|join:some_var }}'})\n\tdef test_join01(self):\n\t\tsome_list = [\"<p>Hello World!</p>\", \"beta & me\", \"<script>Hi!</script>\"]\n\t\tsome_var = \"<br/>\"\n\t\toutput = self.engine.render_to_string(\"join01\", {\"some_list\": some_list, \"some_var\": some_var})\n\t\tself.assertEqual(output, escape(some_var.join(some_list)))\n\t@setup({\"join02\": '{% autoescape off %}{{ some_list|join:some_var }}{% endautoescape %}'})\n\tdef test_join02(self):\n\t\tsome_list = [\"<p>Hello World!</p>\", \"beta & me\", \"<script>Hi!</script>\"]\n\t\tsome_var = \"<br/>\"\n\t\toutput = self.engine.render_to_string(\"join02\", {\"some_list\": some_list, \"some_var\": some_var})\n\t\tself.assertEqual(output, some_var.join(some_list))\nResult of this run in current main is:\n.F\n======================================================================\nFAIL: test_join02 (template_tests.filter_tests.test_regression.RegressionTests.test_join02)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n File \"/home/nessita/fellowship/django/django/test/utils.py\", line 443, in inner\n\treturn func(*args, **kwargs)\n\t\t ^^^^^^^^^^^^^^^^^^^^^\n File \"/home/nessita/fellowship/django/tests/template_tests/utils.py\", line 58, in inner\n\tfunc(self)\n File \"/home/nessita/fellowship/django/tests/template_tests/filter_tests/test_regression.py\", line 21, in test_join02\n\tself.assertEqual(output, some_var.join(some_list))\nAssertionError: '<p>Hello World!</p>&lt;br/&gt;beta & me&lt;br/&gt;<script>Hi!</script>' != '<p>Hello World!</p><br/>beta & me<br/><script>Hi!</script>'\n----------------------------------------------------------------------\nRan 2 tests in 0.007s", "choices": "(A)     except TypeError:  # Fail silently if arg isn't iterable.\n        return value\n    return mark_safe(data)\n\n\n@register.filter(is_safe=True)\ndef last(value):\n    \"\"\"Return the last item in a list.\"\"\"\n    try:\n(B)             value = [conditional_escape(v) for v in value]\n        data = conditional_escape(arg).join(value)\n    except TypeError:  # Fail silently if arg isn't iterable.\n        return value\n    return mark_safe(data)\n\n\n@register.filter(is_safe=True)\ndef last(value):\n(C)     \"\"\"Join a list with a string, like Python's ``str.join(list)``.\"\"\"\n    try:\n        if autoescape:\n            value = [conditional_escape(v) for v in value]\n        data = conditional_escape(arg).join(value)\n    except TypeError:  # Fail silently if arg isn't iterable.\n        return value\n    return mark_safe(data)\n\n(D) \n@register.filter(is_safe=True, needs_autoescape=True)\ndef join(value, arg, autoescape=True):\n    \"\"\"Join a list with a string, like Python's ``str.join(list)``.\"\"\"\n    try:\n        if autoescape:\n            value = [conditional_escape(v) for v in value]\n        data = conditional_escape(arg).join(value)\n    except TypeError:  # Fail silently if arg isn't iterable.", "answer": "C"}
{"uuid": "14e87d21-d445-4ad4-8dda-d93c4c9636ba", "issue_statement": "QuerySet.only() doesn't work with select_related() on a reverse OneToOneField relation.\nDescription\n\t\nOn Django 4.2 calling only() with select_related() on a query using the reverse lookup for a OneToOne relation does not generate the correct query.\nAll the fields from the related model are still included in the generated SQL.\nSample models:\nclass Main(models.Model):\n\tmain_field_1 = models.CharField(blank=True, max_length=45)\n\tmain_field_2 = models.CharField(blank=True, max_length=45)\n\tmain_field_3 = models.CharField(blank=True, max_length=45)\nclass Secondary(models.Model):\n\tmain = models.OneToOneField(Main, primary_key=True, related_name='secondary', on_delete=models.CASCADE)\n\tsecondary_field_1 = models.CharField(blank=True, max_length=45)\n\tsecondary_field_2 = models.CharField(blank=True, max_length=45)\n\tsecondary_field_3 = models.CharField(blank=True, max_length=45)\nSample code:\nMain.objects.select_related('secondary').only('main_field_1', 'secondary__secondary_field_1')\nGenerated query on Django 4.2.1:\nSELECT \"bugtest_main\".\"id\", \"bugtest_main\".\"main_field_1\", \"bugtest_secondary\".\"main_id\", \"bugtest_secondary\".\"secondary_field_1\", \"bugtest_secondary\".\"secondary_field_2\", \"bugtest_secondary\".\"secondary_field_3\" FROM \"bugtest_main\" LEFT OUTER JOIN \"bugtest_secondary\" ON (\"bugtest_main\".\"id\" = \"bugtest_secondary\".\"main_id\")\nGenerated query on Django 4.1.9:\nSELECT \"bugtest_main\".\"id\", \"bugtest_main\".\"main_field_1\", \"bugtest_secondary\".\"main_id\", \"bugtest_secondary\".\"secondary_field_1\" FROM \"bugtest_main\" LEFT OUTER JOIN \"bugtest_secondary\" ON (\"bugtest_main\".\"id\" = \"bugtest_secondary\".\"main_id\")", "choices": "(A)                 related_model = field.remote_field.model._meta.concrete_model\n                self._get_only_select_mask(\n                    related_model._meta, field_mask, field_select_mask\n                )\n        return select_mask\n\n    def get_select_mask(self):\n        \"\"\"\n        Convert the self.deferred_loading data structure to an alternate data\n        structure, describing the field that *will* be loaded. This is used to\n        compute the columns to select from the database and also by the\n        QuerySet class to work out which fields are being initialized on each\n        model. Models that have all their fields included aren't mentioned in\n(B)     def _get_only_select_mask(self, opts, mask, select_mask=None):\n        if select_mask is None:\n            select_mask = {}\n        select_mask[opts.pk] = {}\n        # Only include fields mentioned in the mask.\n        for field_name, field_mask in mask.items():\n            field = opts.get_field(field_name)\n            field_select_mask = select_mask.setdefault(field, {})\n            if field_mask:\n                if not field.is_relation:\n                    raise FieldError(next(iter(field_mask)))\n                related_model = field.remote_field.model._meta.concrete_model\n                self._get_only_select_mask(\n(C)             if field_mask:\n                if not field.is_relation:\n                    raise FieldError(next(iter(field_mask)))\n                related_model = field.remote_field.model._meta.concrete_model\n                self._get_only_select_mask(\n                    related_model._meta, field_mask, field_select_mask\n                )\n        return select_mask\n\n    def get_select_mask(self):\n        \"\"\"\n        Convert the self.deferred_loading data structure to an alternate data\n        structure, describing the field that *will* be loaded. This is used to\n(D)         # Only include fields mentioned in the mask.\n        for field_name, field_mask in mask.items():\n            field = opts.get_field(field_name)\n            field_select_mask = select_mask.setdefault(field, {})\n            if field_mask:\n                if not field.is_relation:\n                    raise FieldError(next(iter(field_mask)))\n                related_model = field.remote_field.model._meta.concrete_model\n                self._get_only_select_mask(\n                    related_model._meta, field_mask, field_select_mask\n                )\n        return select_mask\n", "answer": "D"}
{"uuid": "ac0f7d57-02af-4ba3-9bf6-7bd8a938b5e6", "issue_statement": "Allow returning IDs in QuerySet.bulk_create() when updating conflicts.\nDescription\n\t\nCurrently, when using bulk_create with a conflict handling flag turned on (e.g. ignore_conflicts or update_conflicts), the primary keys are not set in the returned queryset, as documented in bulk_create.\nWhile I understand using ignore_conflicts can lead to PostgreSQL not returning the IDs when a row is ignored (see \u200bthis SO thread), I don't understand why we don't return the IDs in the case of update_conflicts.\nFor instance:\nMyModel.objects.bulk_create([MyModel(...)], update_conflicts=True, update_fields=[...], unique_fields=[...])\ngenerates a query without a RETURNING my_model.id part:\nINSERT INTO \"my_model\" (...)\nVALUES (...)\n\tON CONFLICT(...) DO UPDATE ...\nIf I append the RETURNING my_model.id clause, the query is indeed valid and the ID is returned (checked with PostgreSQL).\nI investigated a bit and \u200bthis in Django source is where the returning_fields gets removed.\nI believe we could discriminate the cases differently so as to keep those returning_fields in the case of update_conflicts.\nThis would be highly helpful when using bulk_create as a bulk upsert feature.", "choices": "(A)         \"\"\"\n        connection = connections[self.db]\n        ops = connection.ops\n        max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)\n        batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n        inserted_rows = []\n        bulk_return = connection.features.can_return_rows_from_bulk_insert\n        for item in [objs[i : i + batch_size] for i in range(0, len(objs), batch_size)]:\n            if bulk_return and on_conflict is None:\n                inserted_rows.extend(\n                    self._insert(\n                        item,\n                        fields=fields,\n                        using=self.db,\n                        returning_fields=self.model._meta.db_returning_fields,\n                    )\n                )\n(B)         inserted_rows = []\n        bulk_return = connection.features.can_return_rows_from_bulk_insert\n        for item in [objs[i : i + batch_size] for i in range(0, len(objs), batch_size)]:\n            if bulk_return and on_conflict is None:\n                inserted_rows.extend(\n                    self._insert(\n                        item,\n                        fields=fields,\n                        using=self.db,\n                        returning_fields=self.model._meta.db_returning_fields,\n                    )\n                )\n            else:\n                self._insert(\n                    item,\n                    fields=fields,\n                    using=self.db,\n(C)                         returning_fields=self.model._meta.db_returning_fields,\n                    )\n                )\n            else:\n                self._insert(\n                    item,\n                    fields=fields,\n                    using=self.db,\n                    on_conflict=on_conflict,\n                    update_fields=update_fields,\n                    unique_fields=unique_fields,\n                )\n        return inserted_rows\n\n    def _chain(self):\n        \"\"\"\n        Return a copy of the current QuerySet that's ready for another\n(D)                     self._insert(\n                        item,\n                        fields=fields,\n                        using=self.db,\n                        returning_fields=self.model._meta.db_returning_fields,\n                    )\n                )\n            else:\n                self._insert(\n                    item,\n                    fields=fields,\n                    using=self.db,\n                    on_conflict=on_conflict,\n                    update_fields=update_fields,\n                    unique_fields=unique_fields,\n                )\n        return inserted_rows", "answer": "B"}
{"uuid": "15182862-a5a4-4fc4-8ad9-cd325aa5e3a1", "issue_statement": "Class methods from nested classes cannot be used as Field.default.\nDescription\n\t \n\t\t(last modified by Mariusz Felisiak)\n\t \nGiven the following model:\n \nclass Profile(models.Model):\n\tclass Capability(models.TextChoices):\n\t\tBASIC = (\"BASIC\", \"Basic\")\n\t\tPROFESSIONAL = (\"PROFESSIONAL\", \"Professional\")\n\t\t\n\t\t@classmethod\n\t\tdef default(cls) -> list[str]:\n\t\t\treturn [cls.BASIC]\n\tcapabilities = ArrayField(\n\t\tmodels.CharField(choices=Capability.choices, max_length=30, blank=True),\n\t\tnull=True,\n\t\tdefault=Capability.default\n\t)\nThe resulting migration contained the following:\n # ...\n\t migrations.AddField(\n\t\t model_name='profile',\n\t\t name='capabilities',\n\t\t field=django.contrib.postgres.fields.ArrayField(base_field=models.CharField(blank=True, choices=[('BASIC', 'Basic'), ('PROFESSIONAL', 'Professional')], max_length=30), default=appname.models.Capability.default, null=True, size=None),\n\t ),\n # ...\nAs you can see, migrations.AddField is passed as argument \"default\" a wrong value \"appname.models.Capability.default\", which leads to an error when trying to migrate. The right value should be \"appname.models.Profile.Capability.default\".", "choices": "(A)                 \"import %s\" % module\n            }\n        # Further error checking\n        if self.value.__name__ == \"<lambda>\":\n            raise ValueError(\"Cannot serialize function: lambda\")\n        if self.value.__module__ is None:\n            raise ValueError(\"Cannot serialize function %r: No module\" % self.value)\n(B)         ):\n            klass = self.value.__self__\n            module = klass.__module__\n            return \"%s.%s.%s\" % (module, klass.__name__, self.value.__name__), {\n                \"import %s\" % module\n            }\n        # Further error checking\n(C)             module = klass.__module__\n            return \"%s.%s.%s\" % (module, klass.__name__, self.value.__name__), {\n                \"import %s\" % module\n            }\n        # Further error checking\n        if self.value.__name__ == \"<lambda>\":\n            raise ValueError(\"Cannot serialize function: lambda\")\n(D)         if getattr(self.value, \"__self__\", None) and isinstance(\n            self.value.__self__, type\n        ):\n            klass = self.value.__self__\n            module = klass.__module__\n            return \"%s.%s.%s\" % (module, klass.__name__, self.value.__name__), {\n                \"import %s\" % module", "answer": "B"}
{"uuid": "0a54e243-25be-48cf-ad98-7edbd06e8e79", "issue_statement": "Add easily comparable version info to toplevel\n<!--\r\nWelcome! Thanks for thinking of a way to improve Matplotlib.\r\n\r\n\r\nBefore creating a new feature request please search the issues for relevant feature requests.\r\n-->\r\n\r\n### Problem\r\n\r\nCurrently matplotlib only exposes `__version__`.  For quick version checks, exposing either a `version_info` tuple (which can be compared with other tuples) or a `LooseVersion` instance (which can be properly compared with other strings) would be a small usability improvement.\r\n\r\n(In practice I guess boring string comparisons will work just fine until we hit mpl 3.10 or 4.10 which is unlikely to happen soon, but that feels quite dirty :))\r\n<!--\r\nProvide a clear and concise description of the problem this feature will solve. \r\n\r\nFor example:\r\n* I'm always frustrated when [...] because [...]\r\n* I would like it if [...] happened when I [...] because [...]\r\n* Here is a sample image of what I am asking for [...]\r\n-->\r\n\r\n### Proposed Solution\r\n\r\nI guess I slightly prefer `LooseVersion`, but exposing just a `version_info` tuple is much more common in other packages (and perhaps simpler to understand).  The hardest(?) part is probably just bikeshedding this point :-)\r\n<!-- Provide a clear and concise description of a way to accomplish what you want. For example:\r\n\r\n* Add an option so that when [...]  [...] will happen\r\n -->\r\n\r\n### Additional context and prior art\r\n\r\n`version_info` is a pretty common thing (citation needed).\r\n<!-- Add any other context or screenshots about the feature request here. You can also include links to examples of other programs that have something similar to your request. For example:\r\n\r\n* Another project [...] solved this by [...]\r\n-->", "choices": "(A)                 version_scheme=\"post-release\",\n                local_scheme=\"node-and-date\",\n                fallback_version=_version.version,\n            )\n        else:  # Get the version from the _version.py setuptools_scm file.\n            __version__ = _version.version\n        return __version__\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\ndef _check_versions():\n\n    # Quickfix to ensure Microsoft Visual C++ redistributable\n    # DLLs are loaded before importing kiwisolver\n    from . import ft2font\n\n    for modname, minver in [\n            (\"cycler\", \"0.10\"),\n            (\"dateutil\", \"2.7\"),\n            (\"kiwisolver\", \"1.0.1\"),\n            (\"numpy\", \"1.17\"),\n            (\"pyparsing\", \"2.2.1\"),\n    ]:\n        module = importlib.import_module(modname)\n        if parse_version(module.__version__) < parse_version(minver):\n            raise ImportError(f\"Matplotlib requires {modname}>={minver}; \"\n                              f\"you have {module.__version__}\")\n\n\n_check_versions()\n\n\n# The decorator ensures this always returns the same handler (and it is only\n# attached once).\n@functools.lru_cache()\ndef _ensure_handler():\n    \"\"\"\n    The first time this function is called, attach a `StreamHandler` using the\n    same format as `logging.basicConfig` to the Matplotlib root logger.\n\n    Return this handler every time this function is called.\n    \"\"\"\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(logging.BASIC_FORMAT))\n    _log.addHandler(handler)\n    return handler\n\n\ndef set_loglevel(level):\n    \"\"\"\n    Set Matplotlib's root logger and root logger handler level, creating\n    the handler if it does not exist yet.\n\n    Typically, one should call ``set_loglevel(\"info\")`` or\n    ``set_loglevel(\"debug\")`` to get additional debugging information.\n\n    Parameters\n    ----------\n    level : {\"notset\", \"debug\", \"info\", \"warning\", \"error\", \"critical\"}\n        The log level of the handler.\n(B) \n    for modname, minver in [\n            (\"cycler\", \"0.10\"),\n            (\"dateutil\", \"2.7\"),\n            (\"kiwisolver\", \"1.0.1\"),\n            (\"numpy\", \"1.17\"),\n            (\"pyparsing\", \"2.2.1\"),\n    ]:\n        module = importlib.import_module(modname)\n        if parse_version(module.__version__) < parse_version(minver):\n            raise ImportError(f\"Matplotlib requires {modname}>={minver}; \"\n                              f\"you have {module.__version__}\")\n\n\n_check_versions()\n\n\n# The decorator ensures this always returns the same handler (and it is only\n# attached once).\n@functools.lru_cache()\ndef _ensure_handler():\n    \"\"\"\n    The first time this function is called, attach a `StreamHandler` using the\n    same format as `logging.basicConfig` to the Matplotlib root logger.\n\n    Return this handler every time this function is called.\n    \"\"\"\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(logging.BASIC_FORMAT))\n    _log.addHandler(handler)\n    return handler\n\n\ndef set_loglevel(level):\n    \"\"\"\n    Set Matplotlib's root logger and root logger handler level, creating\n    the handler if it does not exist yet.\n\n    Typically, one should call ``set_loglevel(\"info\")`` or\n    ``set_loglevel(\"debug\")`` to get additional debugging information.\n\n    Parameters\n    ----------\n    level : {\"notset\", \"debug\", \"info\", \"warning\", \"error\", \"critical\"}\n        The log level of the handler.\n\n    Notes\n    -----\n    The first time this function is called, an additional handler is attached\n    to Matplotlib's root handler; this handler is reused every time and this\n    function simply manipulates the logger and handler's level.\n    \"\"\"\n    _log.setLevel(level.upper())\n    _ensure_handler().setLevel(level.upper())\n\n\ndef _logged_cached(fmt, func=None):\n    \"\"\"\n    Decorator that logs a function's return value, and memoizes that value.\n\n(C) \n_log = logging.getLogger(__name__)\n\n__bibtex__ = r\"\"\"@Article{Hunter:2007,\n  Author    = {Hunter, J. D.},\n  Title     = {Matplotlib: A 2D graphics environment},\n  Journal   = {Computing in Science \\& Engineering},\n  Volume    = {9},\n  Number    = {3},\n  Pages     = {90--95},\n  abstract  = {Matplotlib is a 2D graphics package used for Python\n  for application development, interactive scripting, and\n  publication-quality image generation across user\n  interfaces and operating systems.},\n  publisher = {IEEE COMPUTER SOC},\n  year      = 2007\n}\"\"\"\n\n\ndef __getattr__(name):\n    if name == \"__version__\":\n        import setuptools_scm\n        global __version__  # cache it.\n        # Only shell out to a git subprocess if really needed, and not on a\n        # shallow clone, such as those used by CI, as the latter would trigger\n        # a warning from setuptools_scm.\n        root = Path(__file__).resolve().parents[2]\n        if (root / \".git\").exists() and not (root / \".git/shallow\").exists():\n            __version__ = setuptools_scm.get_version(\n                root=root,\n                version_scheme=\"post-release\",\n                local_scheme=\"node-and-date\",\n                fallback_version=_version.version,\n            )\n        else:  # Get the version from the _version.py setuptools_scm file.\n            __version__ = _version.version\n        return __version__\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\ndef _check_versions():\n\n    # Quickfix to ensure Microsoft Visual C++ redistributable\n    # DLLs are loaded before importing kiwisolver\n    from . import ft2font\n\n    for modname, minver in [\n            (\"cycler\", \"0.10\"),\n            (\"dateutil\", \"2.7\"),\n            (\"kiwisolver\", \"1.0.1\"),\n            (\"numpy\", \"1.17\"),\n            (\"pyparsing\", \"2.2.1\"),\n    ]:\n        module = importlib.import_module(modname)\n        if parse_version(module.__version__) < parse_version(minver):\n            raise ImportError(f\"Matplotlib requires {modname}>={minver}; \"\n                              f\"you have {module.__version__}\")\n\n\n_check_versions()\n(D)   year      = 2007\n}\"\"\"\n\n\ndef __getattr__(name):\n    if name == \"__version__\":\n        import setuptools_scm\n        global __version__  # cache it.\n        # Only shell out to a git subprocess if really needed, and not on a\n        # shallow clone, such as those used by CI, as the latter would trigger\n        # a warning from setuptools_scm.\n        root = Path(__file__).resolve().parents[2]\n        if (root / \".git\").exists() and not (root / \".git/shallow\").exists():\n            __version__ = setuptools_scm.get_version(\n                root=root,\n                version_scheme=\"post-release\",\n                local_scheme=\"node-and-date\",\n                fallback_version=_version.version,\n            )\n        else:  # Get the version from the _version.py setuptools_scm file.\n            __version__ = _version.version\n        return __version__\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\ndef _check_versions():\n\n    # Quickfix to ensure Microsoft Visual C++ redistributable\n    # DLLs are loaded before importing kiwisolver\n    from . import ft2font\n\n    for modname, minver in [\n            (\"cycler\", \"0.10\"),\n            (\"dateutil\", \"2.7\"),\n            (\"kiwisolver\", \"1.0.1\"),\n            (\"numpy\", \"1.17\"),\n            (\"pyparsing\", \"2.2.1\"),\n    ]:\n        module = importlib.import_module(modname)\n        if parse_version(module.__version__) < parse_version(minver):\n            raise ImportError(f\"Matplotlib requires {modname}>={minver}; \"\n                              f\"you have {module.__version__}\")\n\n\n_check_versions()\n\n\n# The decorator ensures this always returns the same handler (and it is only\n# attached once).\n@functools.lru_cache()\ndef _ensure_handler():\n    \"\"\"\n    The first time this function is called, attach a `StreamHandler` using the\n    same format as `logging.basicConfig` to the Matplotlib root logger.\n\n    Return this handler every time this function is called.\n    \"\"\"\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(logging.BASIC_FORMAT))\n    _log.addHandler(handler)", "answer": "D"}
{"uuid": "2f445c8e-4ca1-4eac-b55f-837295a30e12", "issue_statement": "[Bug]: cannot give init value for RangeSlider widget\n### Bug summary\r\n\r\nI think `xy[4] = .25, val[0]` should be commented in /matplotlib/widgets. py\", line 915, in set_val\r\nas it prevents to initialized value for RangeSlider\r\n\r\n### Code for reproduction\r\n\r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.widgets import RangeSlider\r\n\r\n# generate a fake image\r\nnp.random.seed(19680801)\r\nN = 128\r\nimg = np.random.randn(N, N)\r\n\r\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\r\nfig.subplots_adjust(bottom=0.25)\r\n\r\nim = axs[0].imshow(img)\r\naxs[1].hist(img.flatten(), bins='auto')\r\naxs[1].set_title('Histogram of pixel intensities')\r\n\r\n# Create the RangeSlider\r\nslider_ax = fig.add_axes([0.20, 0.1, 0.60, 0.03])\r\nslider = RangeSlider(slider_ax, \"Threshold\", img.min(), img.max(),valinit=[0.0,0.0])\r\n\r\n# Create the Vertical lines on the histogram\r\nlower_limit_line = axs[1].axvline(slider.val[0], color='k')\r\nupper_limit_line = axs[1].axvline(slider.val[1], color='k')\r\n\r\n\r\ndef update(val):\r\n    # The val passed to a callback by the RangeSlider will\r\n    # be a tuple of (min, max)\r\n\r\n    # Update the image's colormap\r\n    im.norm.vmin = val[0]\r\n    im.norm.vmax = val[1]\r\n\r\n    # Update the position of the vertical lines\r\n    lower_limit_line.set_xdata([val[0], val[0]])\r\n    upper_limit_line.set_xdata([val[1], val[1]])\r\n\r\n    # Redraw the figure to ensure it updates\r\n    fig.canvas.draw_idle()\r\n\r\n\r\nslider.on_changed(update)\r\nplt.show()\r\n```\r\n\r\n\r\n### Actual outcome\r\n\r\n```python\r\n  File \"<ipython-input-52-b704c53e18d4>\", line 19, in <module>\r\n    slider = RangeSlider(slider_ax, \"Threshold\", img.min(), img.max(),valinit=[0.0,0.0])\r\n\r\n  File \"/Users/Vincent/opt/anaconda3/envs/py38/lib/python3.8/site-packages/matplotlib/widgets.py\", line 778, in __init__\r\n    self.set_val(valinit)\r\n\r\n  File \"/Users/Vincent/opt/anaconda3/envs/py38/lib/python3.8/site-packages/matplotlib/widgets.py\", line 915, in set_val\r\n    xy[4] = val[0], .25\r\n\r\nIndexError: index 4 is out of bounds for axis 0 with size 4\r\n```\r\n\r\n### Expected outcome\r\n\r\nrange slider with user initial values\r\n\r\n### Additional information\r\n\r\nerror can be removed by commenting this line\r\n```python\r\n\r\n    def set_val(self, val):\r\n        \"\"\"\r\n        Set slider value to *val*.\r\n\r\n        Parameters\r\n        ----------\r\n        val : tuple or array-like of float\r\n        \"\"\"\r\n        val = np.sort(np.asanyarray(val))\r\n        if val.shape != (2,):\r\n            raise ValueError(\r\n                f\"val must have shape (2,) but has shape {val.shape}\"\r\n            )\r\n        val[0] = self._min_in_bounds(val[0])\r\n        val[1] = self._max_in_bounds(val[1])\r\n        xy = self.poly.xy\r\n        if self.orientation == \"vertical\":\r\n            xy[0] = .25, val[0]\r\n            xy[1] = .25, val[1]\r\n            xy[2] = .75, val[1]\r\n            xy[3] = .75, val[0]\r\n            # xy[4] = .25, val[0]\r\n        else:\r\n            xy[0] = val[0], .25\r\n            xy[1] = val[0], .75\r\n            xy[2] = val[1], .75\r\n            xy[3] = val[1], .25\r\n            # xy[4] = val[0], .25\r\n        self.poly.xy = xy\r\n        self.valtext.set_text(self._format(val))\r\n        if self.drawon:\r\n            self.ax.figure.canvas.draw_idle()\r\n        self.val = val\r\n        if self.eventson:\r\n            self._observers.process(\"changed\", val)\r\n\r\n```\r\n\r\n### Operating system\r\n\r\nOSX\r\n\r\n### Matplotlib Version\r\n\r\n3.5.1\r\n\r\n### Matplotlib Backend\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.8\r\n\r\n### Jupyter version\r\n\r\n_No response_\r\n\r\n### Installation\r\n\r\npip", "choices": "(A)             val = self._max_in_bounds(pos)\n            self.set_max(val)\n        if self._active_handle:\n            self._active_handle.set_xdata([val])\n\n    def _update(self, event):\n        \"\"\"Update the slider position.\"\"\"\n        if self.ignore(event) or event.button != 1:\n            return\n\n        if event.name == \"button_press_event\" and event.inaxes == self.ax:\n            self.drag_active = True\n            event.canvas.grab_mouse(self.ax)\n\n        if not self.drag_active:\n            return\n\n        elif (event.name == \"button_release_event\") or (\n            event.name == \"button_press_event\" and event.inaxes != self.ax\n        ):\n            self.drag_active = False\n            event.canvas.release_mouse(self.ax)\n            self._active_handle = None\n            return\n\n        # determine which handle was grabbed\n        handle = self._handles[\n            np.argmin(\n                np.abs([h.get_xdata()[0] - event.xdata for h in self._handles])\n            )\n        ]\n        # these checks ensure smooth behavior if the handles swap which one\n        # has a higher value. i.e. if one is dragged over and past the other.\n        if handle is not self._active_handle:\n            self._active_handle = handle\n\n        if self.orientation == \"vertical\":\n            self._update_val_from_pos(event.ydata)\n        else:\n            self._update_val_from_pos(event.xdata)\n\n    def _format(self, val):\n        \"\"\"Pretty-print *val*.\"\"\"\n        if self.valfmt is not None:\n            return f\"({self.valfmt % val[0]}, {self.valfmt % val[1]})\"\n        else:\n            _, s1, s2, _ = self._fmt.format_ticks(\n                [self.valmin, *val, self.valmax]\n            )\n            # fmt.get_offset is actually the multiplicative factor, if any.\n            s1 += self._fmt.get_offset()\n            s2 += self._fmt.get_offset()\n            # Use f string to avoid issues with backslashes when cast to a str\n            return f\"({s1}, {s2})\"\n\n    def set_min(self, min):\n        \"\"\"\n        Set the lower value of the slider to *min*.\n\n        Parameters\n        ----------\n        min : float\n        \"\"\"\n        self.set_val((min, self.val[1]))\n\n    def set_max(self, max):\n        \"\"\"\n        Set the lower value of the slider to *max*.\n\n        Parameters\n        ----------\n        max : float\n        \"\"\"\n        self.set_val((self.val[0], max))\n\n    def set_val(self, val):\n        \"\"\"\n        Set slider value to *val*.\n\n        Parameters\n        ----------\n        val : tuple or array-like of float\n        \"\"\"\n        val = np.sort(val)\n        _api.check_shape((2,), val=val)\n        val[0] = self._min_in_bounds(val[0])\n        val[1] = self._max_in_bounds(val[1])\n        xy = self.poly.xy\n        if self.orientation == \"vertical\":\n            xy[0] = .25, val[0]\n            xy[1] = .25, val[1]\n            xy[2] = .75, val[1]\n            xy[3] = .75, val[0]\n            xy[4] = .25, val[0]\n        else:\n            xy[0] = val[0], .25\n            xy[1] = val[0], .75\n            xy[2] = val[1], .75\n            xy[3] = val[1], .25\n            xy[4] = val[0], .25\n        self.poly.xy = xy\n        self.valtext.set_text(self._format(val))\n        if self.drawon:\n            self.ax.figure.canvas.draw_idle()\n        self.val = val\n        if self.eventson:\n            self._observers.process(\"changed\", val)\n\n    def on_changed(self, func):\n        \"\"\"\n        Connect *func* as callback function to changes of the slider value.\n\n        Parameters\n        ----------\n        func : callable\n            Function to call when slider is changed. The function\n            must accept a numpy array with shape (2,) as its argument.\n\n        Returns\n        -------\n        int\n(B)         # these checks ensure smooth behavior if the handles swap which one\n        # has a higher value. i.e. if one is dragged over and past the other.\n        if handle is not self._active_handle:\n            self._active_handle = handle\n\n        if self.orientation == \"vertical\":\n            self._update_val_from_pos(event.ydata)\n        else:\n            self._update_val_from_pos(event.xdata)\n\n    def _format(self, val):\n        \"\"\"Pretty-print *val*.\"\"\"\n        if self.valfmt is not None:\n            return f\"({self.valfmt % val[0]}, {self.valfmt % val[1]})\"\n        else:\n            _, s1, s2, _ = self._fmt.format_ticks(\n                [self.valmin, *val, self.valmax]\n            )\n            # fmt.get_offset is actually the multiplicative factor, if any.\n            s1 += self._fmt.get_offset()\n            s2 += self._fmt.get_offset()\n            # Use f string to avoid issues with backslashes when cast to a str\n            return f\"({s1}, {s2})\"\n\n    def set_min(self, min):\n        \"\"\"\n        Set the lower value of the slider to *min*.\n\n        Parameters\n        ----------\n        min : float\n        \"\"\"\n        self.set_val((min, self.val[1]))\n\n    def set_max(self, max):\n        \"\"\"\n        Set the lower value of the slider to *max*.\n\n        Parameters\n        ----------\n        max : float\n        \"\"\"\n        self.set_val((self.val[0], max))\n\n    def set_val(self, val):\n        \"\"\"\n        Set slider value to *val*.\n\n        Parameters\n        ----------\n        val : tuple or array-like of float\n        \"\"\"\n        val = np.sort(val)\n        _api.check_shape((2,), val=val)\n        val[0] = self._min_in_bounds(val[0])\n        val[1] = self._max_in_bounds(val[1])\n        xy = self.poly.xy\n        if self.orientation == \"vertical\":\n            xy[0] = .25, val[0]\n            xy[1] = .25, val[1]\n            xy[2] = .75, val[1]\n            xy[3] = .75, val[0]\n            xy[4] = .25, val[0]\n        else:\n            xy[0] = val[0], .25\n            xy[1] = val[0], .75\n            xy[2] = val[1], .75\n            xy[3] = val[1], .25\n            xy[4] = val[0], .25\n        self.poly.xy = xy\n        self.valtext.set_text(self._format(val))\n        if self.drawon:\n            self.ax.figure.canvas.draw_idle()\n        self.val = val\n        if self.eventson:\n            self._observers.process(\"changed\", val)\n\n    def on_changed(self, func):\n        \"\"\"\n        Connect *func* as callback function to changes of the slider value.\n\n        Parameters\n        ----------\n        func : callable\n            Function to call when slider is changed. The function\n            must accept a numpy array with shape (2,) as its argument.\n\n        Returns\n        -------\n        int\n            Connection id (which can be used to disconnect *func*).\n        \"\"\"\n        return self._observers.connect('changed', lambda val: func(val))\n\n\nclass CheckButtons(AxesWidget):\n    r\"\"\"\n    A GUI neutral set of check buttons.\n\n    For the check buttons to remain responsive you must keep a\n    reference to this object.\n\n    Connect to the CheckButtons with the `.on_clicked` method.\n\n    Attributes\n    ----------\n    ax : `~matplotlib.axes.Axes`\n        The parent Axes for the widget.\n    labels : list of `.Text`\n\n    rectangles : list of `.Rectangle`\n\n    lines : list of (`.Line2D`, `.Line2D`) pairs\n        List of lines for the x's in the check boxes.  These lines exist for\n        each box, but have ``set_visible(False)`` when its box is not checked.\n    \"\"\"\n\n    cnt = _api.deprecated(\"3.4\")(property(  # Not real, but close enough.\n        lambda self: len(self._observers.callbacks['clicked'])))\n    observers = _api.deprecated(\"3.4\")(property(\n        lambda self: self._observers.callbacks['clicked']))\n(C)         min : float\n        \"\"\"\n        self.set_val((min, self.val[1]))\n\n    def set_max(self, max):\n        \"\"\"\n        Set the lower value of the slider to *max*.\n\n        Parameters\n        ----------\n        max : float\n        \"\"\"\n        self.set_val((self.val[0], max))\n\n    def set_val(self, val):\n        \"\"\"\n        Set slider value to *val*.\n\n        Parameters\n        ----------\n        val : tuple or array-like of float\n        \"\"\"\n        val = np.sort(val)\n        _api.check_shape((2,), val=val)\n        val[0] = self._min_in_bounds(val[0])\n        val[1] = self._max_in_bounds(val[1])\n        xy = self.poly.xy\n        if self.orientation == \"vertical\":\n            xy[0] = .25, val[0]\n            xy[1] = .25, val[1]\n            xy[2] = .75, val[1]\n            xy[3] = .75, val[0]\n            xy[4] = .25, val[0]\n        else:\n            xy[0] = val[0], .25\n            xy[1] = val[0], .75\n            xy[2] = val[1], .75\n            xy[3] = val[1], .25\n            xy[4] = val[0], .25\n        self.poly.xy = xy\n        self.valtext.set_text(self._format(val))\n        if self.drawon:\n            self.ax.figure.canvas.draw_idle()\n        self.val = val\n        if self.eventson:\n            self._observers.process(\"changed\", val)\n\n    def on_changed(self, func):\n        \"\"\"\n        Connect *func* as callback function to changes of the slider value.\n\n        Parameters\n        ----------\n        func : callable\n            Function to call when slider is changed. The function\n            must accept a numpy array with shape (2,) as its argument.\n\n        Returns\n        -------\n        int\n            Connection id (which can be used to disconnect *func*).\n        \"\"\"\n        return self._observers.connect('changed', lambda val: func(val))\n\n\nclass CheckButtons(AxesWidget):\n    r\"\"\"\n    A GUI neutral set of check buttons.\n\n    For the check buttons to remain responsive you must keep a\n    reference to this object.\n\n    Connect to the CheckButtons with the `.on_clicked` method.\n\n    Attributes\n    ----------\n    ax : `~matplotlib.axes.Axes`\n        The parent Axes for the widget.\n    labels : list of `.Text`\n\n    rectangles : list of `.Rectangle`\n\n    lines : list of (`.Line2D`, `.Line2D`) pairs\n        List of lines for the x's in the check boxes.  These lines exist for\n        each box, but have ``set_visible(False)`` when its box is not checked.\n    \"\"\"\n\n    cnt = _api.deprecated(\"3.4\")(property(  # Not real, but close enough.\n        lambda self: len(self._observers.callbacks['clicked'])))\n    observers = _api.deprecated(\"3.4\")(property(\n        lambda self: self._observers.callbacks['clicked']))\n\n    def __init__(self, ax, labels, actives=None):\n        \"\"\"\n        Add check buttons to `matplotlib.axes.Axes` instance *ax*.\n\n        Parameters\n        ----------\n        ax : `~matplotlib.axes.Axes`\n            The parent Axes for the widget.\n\n        labels : list of str\n            The labels of the check buttons.\n\n        actives : list of bool, optional\n            The initial check states of the buttons. The list must have the\n            same length as *labels*. If not given, all buttons are unchecked.\n        \"\"\"\n        super().__init__(ax)\n\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_navigate(False)\n\n        if actives is None:\n            actives = [False] * len(labels)\n\n        if len(labels) > 1:\n            dy = 1. / (len(labels) + 1)\n            ys = np.linspace(1 - dy, dy, len(labels))\n        else:\n(D)         if min <= self.valmin:\n            if not self.closedmin:\n                return self.val[0]\n            min = self.valmin\n\n        if min > self.val[1]:\n            min = self.val[1]\n        return self._stepped_value(min)\n\n    def _max_in_bounds(self, max):\n        \"\"\"Ensure the new max value is between valmax and self.val[0].\"\"\"\n        if max >= self.valmax:\n            if not self.closedmax:\n                return self.val[1]\n            max = self.valmax\n\n        if max <= self.val[0]:\n            max = self.val[0]\n        return self._stepped_value(max)\n\n    def _value_in_bounds(self, vals):\n        \"\"\"Clip min, max values to the bounds.\"\"\"\n        return (self._min_in_bounds(vals[0]), self._max_in_bounds(vals[1]))\n\n    def _update_val_from_pos(self, pos):\n        \"\"\"Update the slider value based on a given position.\"\"\"\n        idx = np.argmin(np.abs(self.val - pos))\n        if idx == 0:\n            val = self._min_in_bounds(pos)\n            self.set_min(val)\n        else:\n            val = self._max_in_bounds(pos)\n            self.set_max(val)\n        if self._active_handle:\n            self._active_handle.set_xdata([val])\n\n    def _update(self, event):\n        \"\"\"Update the slider position.\"\"\"\n        if self.ignore(event) or event.button != 1:\n            return\n\n        if event.name == \"button_press_event\" and event.inaxes == self.ax:\n            self.drag_active = True\n            event.canvas.grab_mouse(self.ax)\n\n        if not self.drag_active:\n            return\n\n        elif (event.name == \"button_release_event\") or (\n            event.name == \"button_press_event\" and event.inaxes != self.ax\n        ):\n            self.drag_active = False\n            event.canvas.release_mouse(self.ax)\n            self._active_handle = None\n            return\n\n        # determine which handle was grabbed\n        handle = self._handles[\n            np.argmin(\n                np.abs([h.get_xdata()[0] - event.xdata for h in self._handles])\n            )\n        ]\n        # these checks ensure smooth behavior if the handles swap which one\n        # has a higher value. i.e. if one is dragged over and past the other.\n        if handle is not self._active_handle:\n            self._active_handle = handle\n\n        if self.orientation == \"vertical\":\n            self._update_val_from_pos(event.ydata)\n        else:\n            self._update_val_from_pos(event.xdata)\n\n    def _format(self, val):\n        \"\"\"Pretty-print *val*.\"\"\"\n        if self.valfmt is not None:\n            return f\"({self.valfmt % val[0]}, {self.valfmt % val[1]})\"\n        else:\n            _, s1, s2, _ = self._fmt.format_ticks(\n                [self.valmin, *val, self.valmax]\n            )\n            # fmt.get_offset is actually the multiplicative factor, if any.\n            s1 += self._fmt.get_offset()\n            s2 += self._fmt.get_offset()\n            # Use f string to avoid issues with backslashes when cast to a str\n            return f\"({s1}, {s2})\"\n\n    def set_min(self, min):\n        \"\"\"\n        Set the lower value of the slider to *min*.\n\n        Parameters\n        ----------\n        min : float\n        \"\"\"\n        self.set_val((min, self.val[1]))\n\n    def set_max(self, max):\n        \"\"\"\n        Set the lower value of the slider to *max*.\n\n        Parameters\n        ----------\n        max : float\n        \"\"\"\n        self.set_val((self.val[0], max))\n\n    def set_val(self, val):\n        \"\"\"\n        Set slider value to *val*.\n\n        Parameters\n        ----------\n        val : tuple or array-like of float\n        \"\"\"\n        val = np.sort(val)\n        _api.check_shape((2,), val=val)\n        val[0] = self._min_in_bounds(val[0])\n        val[1] = self._max_in_bounds(val[1])\n        xy = self.poly.xy\n        if self.orientation == \"vertical\":\n            xy[0] = .25, val[0]", "answer": "A"}
{"uuid": "7bb906d5-d275-4da6-b958-99a745412ed6", "issue_statement": "[Bug]: scalar mappable format_cursor_data crashes on BoundarNorm\n### Bug summary\r\n\r\nIn 3.5.0 if you do:\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport matplotlib as mpl\r\n\r\nfig, ax = plt.subplots()\r\nnorm = mpl.colors.BoundaryNorm(np.linspace(-4, 4, 5), 256)\r\nX = np.random.randn(10, 10)\r\npc = ax.imshow(X, cmap='RdBu_r', norm=norm)\r\n```\r\n\r\nand mouse over the image, it crashes with\r\n\r\n```\r\nFile \"/Users/jklymak/matplotlib/lib/matplotlib/artist.py\", line 1282, in format_cursor_data\r\n    neighbors = self.norm.inverse(\r\n  File \"/Users/jklymak/matplotlib/lib/matplotlib/colors.py\", line 1829, in inverse\r\n    raise ValueError(\"BoundaryNorm is not invertible\")\r\nValueError: BoundaryNorm is not invertible\r\n```\r\n\r\nand interaction stops.  \r\n\r\nNot sure if we should have a special check here, a try-except, or actually just make BoundaryNorm approximately invertible.  \r\n\r\n\r\n### Matplotlib Version\r\n\r\nmain 3.5.0\r\n\r\n\n[Bug]: scalar mappable format_cursor_data crashes on BoundarNorm\n### Bug summary\r\n\r\nIn 3.5.0 if you do:\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport matplotlib as mpl\r\n\r\nfig, ax = plt.subplots()\r\nnorm = mpl.colors.BoundaryNorm(np.linspace(-4, 4, 5), 256)\r\nX = np.random.randn(10, 10)\r\npc = ax.imshow(X, cmap='RdBu_r', norm=norm)\r\n```\r\n\r\nand mouse over the image, it crashes with\r\n\r\n```\r\nFile \"/Users/jklymak/matplotlib/lib/matplotlib/artist.py\", line 1282, in format_cursor_data\r\n    neighbors = self.norm.inverse(\r\n  File \"/Users/jklymak/matplotlib/lib/matplotlib/colors.py\", line 1829, in inverse\r\n    raise ValueError(\"BoundaryNorm is not invertible\")\r\nValueError: BoundaryNorm is not invertible\r\n```\r\n\r\nand interaction stops.  \r\n\r\nNot sure if we should have a special check here, a try-except, or actually just make BoundaryNorm approximately invertible.  \r\n\r\n\r\n### Matplotlib Version\r\n\r\nmain 3.5.0", "choices": "(A)                 bbox = Bbox.intersection(bbox, clip_box)\n            clip_path = self.get_clip_path()\n            if clip_path is not None:\n                clip_path = clip_path.get_fully_transformed_path()\n                bbox = Bbox.intersection(bbox, clip_path.get_extents())\n        return bbox\n\n    def add_callback(self, func):\n        \"\"\"\n        Add a callback function that will be called whenever one of the\n        `.Artist`'s properties changes.\n\n        Parameters\n        ----------\n        func : callable\n            The callback function. It must have the signature::\n\n                def func(artist: Artist) -> Any\n\n            where *artist* is the calling `.Artist`. Return values may exist\n            but are ignored.\n\n        Returns\n        -------\n        int\n            The observer id associated with the callback. This id can be\n            used for removing the callback with `.remove_callback` later.\n\n        See Also\n        --------\n        remove_callback\n        \"\"\"\n        # Wrapping func in a lambda ensures it can be connected multiple times\n        # and never gets weakref-gc'ed.\n        return self._callbacks.connect(\"pchanged\", lambda: func(self))\n\n    def remove_callback(self, oid):\n        \"\"\"\n        Remove a callback based on its observer id.\n\n        See Also\n        --------\n        add_callback\n        \"\"\"\n        self._callbacks.disconnect(oid)\n\n    def pchanged(self):\n        \"\"\"\n        Call all of the registered callbacks.\n\n        This function is triggered internally when a property is changed.\n\n        See Also\n        --------\n        add_callback\n        remove_callback\n        \"\"\"\n        self._callbacks.process(\"pchanged\")\n\n    def is_transform_set(self):\n        \"\"\"\n        Return whether the Artist has an explicitly set transform.\n\n        This is *True* after `.set_transform` has been called.\n        \"\"\"\n        return self._transformSet\n\n    def set_transform(self, t):\n        \"\"\"\n        Set the artist transform.\n\n        Parameters\n        ----------\n        t : `.Transform`\n        \"\"\"\n        self._transform = t\n        self._transformSet = True\n        self.pchanged()\n        self.stale = True\n\n    def get_transform(self):\n        \"\"\"Return the `.Transform` instance used by this artist.\"\"\"\n        if self._transform is None:\n            self._transform = IdentityTransform()\n        elif (not isinstance(self._transform, Transform)\n              and hasattr(self._transform, '_as_mpl_transform')):\n            self._transform = self._transform._as_mpl_transform(self.axes)\n        return self._transform\n\n    def get_children(self):\n        r\"\"\"Return a list of the child `.Artist`\\s of this `.Artist`.\"\"\"\n        return []\n\n    def _default_contains(self, mouseevent, figure=None):\n        \"\"\"\n        Base impl. for checking whether a mouseevent happened in an artist.\n\n        1. If the artist figure is known and the event did not occur in that\n           figure (by checking its ``canvas`` attribute), reject it.\n        2. Otherwise, return `None, {}`, indicating that the subclass'\n           implementation should be used.\n\n        Subclasses should start their definition of `contains` as follows:\n\n            inside, info = self._default_contains(mouseevent)\n            if inside is not None:\n                return inside, info\n            # subclass-specific implementation follows\n\n        The *figure* kwarg is provided for the implementation of\n        `.Figure.contains`.\n        \"\"\"\n        if figure is not None and mouseevent.canvas is not figure.canvas:\n            return False, {}\n        return None, {}\n\n    def contains(self, mouseevent):\n        \"\"\"\n        Test whether the artist contains the mouse event.\n\n        Parameters\n        ----------\n        mouseevent : `matplotlib.backend_bases.MouseEvent`\n\n        Returns\n        -------\n        contains : bool\n            Whether any values are within the radius.\n        details : dict\n            An artist-specific dictionary of details of the event context,\n            such as which points are contained in the pick radius. See the\n            individual Artist subclasses for details.\n        \"\"\"\n        inside, info = self._default_contains(mouseevent)\n        if inside is not None:\n            return inside, info\n        _log.warning(\"%r needs 'contains' method\", self.__class__.__name__)\n        return False, {}\n\n    def pickable(self):\n        \"\"\"\n        Return whether the artist is pickable.\n\n        See Also\n        --------\n        set_picker, get_picker, pick\n        \"\"\"\n        return self.figure is not None and self._picker is not None\n\n    def pick(self, mouseevent):\n        \"\"\"\n        Process a pick event.\n\n        Each child artist will fire a pick event if *mouseevent* is over\n        the artist and the artist has picker set.\n\n        See Also\n        --------\n        set_picker, get_picker, pickable\n        \"\"\"\n        # Pick self\n        if self.pickable():\n            picker = self.get_picker()\n            if callable(picker):\n                inside, prop = picker(self, mouseevent)\n            else:\n                inside, prop = self.contains(mouseevent)\n            if inside:\n                self.figure.canvas.pick_event(mouseevent, self, **prop)\n\n        # Pick children\n        for a in self.get_children():\n            # make sure the event happened in the same Axes\n            ax = getattr(a, 'axes', None)\n            if (mouseevent.inaxes is None or ax is None\n                    or mouseevent.inaxes == ax):\n                # we need to check if mouseevent.inaxes is None\n                # because some objects associated with an Axes (e.g., a\n                # tick label) can be outside the bounding box of the\n                # Axes and inaxes will be None\n                # also check that ax is None so that it traverse objects\n                # which do no have an axes property but children might\n                a.pick(mouseevent)\n\n    def set_picker(self, picker):\n        \"\"\"\n        Define the picking behavior of the artist.\n\n        Parameters\n        ----------\n        picker : None or bool or float or callable\n            This can be one of the following:\n\n            - *None*: Picking is disabled for this artist (default).\n\n            - A boolean: If *True* then picking will be enabled and the\n              artist will fire a pick event if the mouse event is over\n              the artist.\n\n            - A float: If picker is a number it is interpreted as an\n              epsilon tolerance in points and the artist will fire\n              off an event if its data is within epsilon of the mouse\n              event.  For some artists like lines and patch collections,\n              the artist may provide additional data to the pick event\n              that is generated, e.g., the indices of the data within\n              epsilon of the pick event\n\n            - A function: If picker is callable, it is a user supplied\n              function which determines whether the artist is hit by the\n              mouse event::\n\n                hit, props = picker(artist, mouseevent)\n\n              to determine the hit test.  if the mouse event is over the\n              artist, return *hit=True* and props is a dictionary of\n              properties you want added to the PickEvent attributes.\n        \"\"\"\n        self._picker = picker\n\n    def get_picker(self):\n        \"\"\"\n        Return the picking behavior of the artist.\n\n        The possible values are described in `.set_picker`.\n\n        See Also\n        --------\n        set_picker, pickable, pick\n        \"\"\"\n        return self._picker\n\n    def get_url(self):\n        \"\"\"Return the url.\"\"\"\n        return self._url\n\n    def set_url(self, url):\n        \"\"\"\n        Set the url for the artist.\n\n        Parameters\n        ----------\n        url : str\n        \"\"\"\n        self._url = url\n\n    def get_gid(self):\n        \"\"\"Return the group id.\"\"\"\n        return self._gid\n\n    def set_gid(self, gid):\n        \"\"\"\n        Set the (group) id for the artist.\n\n        Parameters\n        ----------\n        gid : str\n        \"\"\"\n        self._gid = gid\n\n    def get_snap(self):\n        \"\"\"\n        Return the snap setting.\n\n        See `.set_snap` for details.\n        \"\"\"\n        if mpl.rcParams['path.snap']:\n            return self._snap\n        else:\n            return False\n\n    def set_snap(self, snap):\n        \"\"\"\n        Set the snapping behavior.\n\n        Snapping aligns positions with the pixel grid, which results in\n        clearer images. For example, if a black line of 1px width was\n        defined at a position in between two pixels, the resulting image\n        would contain the interpolated value of that line in the pixel grid,\n        which would be a grey value on both adjacent pixel positions. In\n        contrast, snapping will move the line to the nearest integer pixel\n        value, so that the resulting image will really contain a 1px wide\n        black line.\n\n        Snapping is currently only supported by the Agg and MacOSX backends.\n\n        Parameters\n        ----------\n        snap : bool or None\n            Possible values:\n\n            - *True*: Snap vertices to the nearest pixel center.\n            - *False*: Do not modify vertex positions.\n            - *None*: (auto) If the path contains only rectilinear line\n              segments, round to the nearest pixel center.\n        \"\"\"\n        self._snap = snap\n        self.stale = True\n\n    def get_sketch_params(self):\n        \"\"\"\n        Return the sketch parameters for the artist.\n\n        Returns\n        -------\n        tuple or None\n\n            A 3-tuple with the following elements:\n\n            - *scale*: The amplitude of the wiggle perpendicular to the\n              source line.\n            - *length*: The length of the wiggle along the line.\n            - *randomness*: The scale factor by which the length is\n              shrunken or expanded.\n\n            Returns *None* if no sketch parameters were set.\n        \"\"\"\n        return self._sketch\n\n    def set_sketch_params(self, scale=None, length=None, randomness=None):\n        \"\"\"\n        Set the sketch parameters.\n\n        Parameters\n        ----------\n        scale : float, optional\n            The amplitude of the wiggle perpendicular to the source\n            line, in pixels.  If scale is `None`, or not provided, no\n            sketch filter will be provided.\n        length : float, optional\n             The length of the wiggle along the line, in pixels\n             (default 128.0)\n        randomness : float, optional\n            The scale factor by which the length is shrunken or\n            expanded (default 16.0)\n\n            The PGF backend uses this argument as an RNG seed and not as\n            described above. Using the same seed yields the same random shape.\n\n            .. ACCEPTS: (scale: float, length: float, randomness: float)\n        \"\"\"\n        if scale is None:\n            self._sketch = None\n        else:\n            self._sketch = (scale, length or 128.0, randomness or 16.0)\n        self.stale = True\n\n    def set_path_effects(self, path_effects):\n        \"\"\"\n        Set the path effects.\n\n        Parameters\n        ----------\n        path_effects : `.AbstractPathEffect`\n        \"\"\"\n        self._path_effects = path_effects\n        self.stale = True\n\n    def get_path_effects(self):\n        return self._path_effects\n\n    def get_figure(self):\n        \"\"\"Return the `.Figure` instance the artist belongs to.\"\"\"\n        return self.figure\n\n    def set_figure(self, fig):\n        \"\"\"\n        Set the `.Figure` instance the artist belongs to.\n\n        Parameters\n        ----------\n        fig : `.Figure`\n        \"\"\"\n        # if this is a no-op just return\n        if self.figure is fig:\n            return\n        # if we currently have a figure (the case of both `self.figure`\n        # and *fig* being none is taken care of above) we then user is\n        # trying to change the figure an artist is associated with which\n        # is not allowed for the same reason as adding the same instance\n        # to more than one Axes\n        if self.figure is not None:\n            raise RuntimeError(\"Can not put single artist in \"\n                               \"more than one figure\")\n        self.figure = fig\n        if self.figure and self.figure is not self:\n            self.pchanged()\n        self.stale = True\n\n    def set_clip_box(self, clipbox):\n        \"\"\"\n        Set the artist's clip `.Bbox`.\n\n        Parameters\n        ----------\n        clipbox : `.Bbox`\n        \"\"\"\n        self.clipbox = clipbox\n        self.pchanged()\n        self.stale = True\n\n    def set_clip_path(self, path, transform=None):\n        \"\"\"\n        Set the artist's clip path.\n\n        Parameters\n        ----------\n        path : `.Patch` or `.Path` or `.TransformedPath` or None\n            The clip path. If given a `.Path`, *transform* must be provided as\n            well. If *None*, a previously set clip path is removed.\n        transform : `~matplotlib.transforms.Transform`, optional\n            Only used if *path* is a `.Path`, in which case the given `.Path`\n            is converted to a `.TransformedPath` using *transform*.\n\n        Notes\n        -----\n        For efficiency, if *path* is a `.Rectangle` this method will set the\n        clipping box to the corresponding rectangle and set the clipping path\n        to ``None``.\n\n        For technical reasons (support of `~.Artist.set`), a tuple\n        (*path*, *transform*) is also accepted as a single positional\n        parameter.\n\n        .. ACCEPTS: Patch or (Path, Transform) or None\n        \"\"\"\n        from matplotlib.patches import Patch, Rectangle\n\n        success = False\n        if transform is None:\n            if isinstance(path, Rectangle):\n                self.clipbox = TransformedBbox(Bbox.unit(),\n                                               path.get_transform())\n                self._clippath = None\n                success = True\n            elif isinstance(path, Patch):\n                self._clippath = TransformedPatchPath(path)\n                success = True\n            elif isinstance(path, tuple):\n                path, transform = path\n\n        if path is None:\n            self._clippath = None\n            success = True\n        elif isinstance(path, Path):\n            self._clippath = TransformedPath(path, transform)\n            success = True\n        elif isinstance(path, TransformedPatchPath):\n            self._clippath = path\n            success = True\n        elif isinstance(path, TransformedPath):\n            self._clippath = path\n            success = True\n\n        if not success:\n            raise TypeError(\n                \"Invalid arguments to set_clip_path, of type {} and {}\"\n                .format(type(path).__name__, type(transform).__name__))\n        # This may result in the callbacks being hit twice, but guarantees they\n        # will be hit at least once.\n        self.pchanged()\n        self.stale = True\n\n    def get_alpha(self):\n        \"\"\"\n        Return the alpha value used for blending - not supported on all\n        backends.\n        \"\"\"\n        return self._alpha\n\n    def get_visible(self):\n        \"\"\"Return the visibility.\"\"\"\n        return self._visible\n\n    def get_animated(self):\n        \"\"\"Return whether the artist is animated.\"\"\"\n        return self._animated\n\n    def get_in_layout(self):\n        \"\"\"\n        Return boolean flag, ``True`` if artist is included in layout\n        calculations.\n\n        E.g. :doc:`/tutorials/intermediate/constrainedlayout_guide`,\n        `.Figure.tight_layout()`, and\n        ``fig.savefig(fname, bbox_inches='tight')``.\n        \"\"\"\n        return self._in_layout\n\n    def _fully_clipped_to_axes(self):\n        \"\"\"\n        Return a boolean flag, ``True`` if the artist is clipped to the Axes\n        and can thus be skipped in layout calculations. Requires `get_clip_on`\n        is True, one of `clip_box` or `clip_path` is set, ``clip_box.extents``\n        is equivalent to ``ax.bbox.extents`` (if set), and ``clip_path._patch``\n        is equivalent to ``ax.patch`` (if set).\n        \"\"\"\n        # Note that ``clip_path.get_fully_transformed_path().get_extents()``\n        # cannot be directly compared to ``axes.bbox.extents`` because the\n        # extents may be undefined (i.e. equivalent to ``Bbox.null()``)\n        # before the associated artist is drawn, and this method is meant\n        # to determine whether ``axes.get_tightbbox()`` may bypass drawing\n        clip_box = self.get_clip_box()\n        clip_path = self.get_clip_path()\n        return (self.axes is not None\n                and self.get_clip_on()\n                and (clip_box is not None or clip_path is not None)\n                and (clip_box is None\n                     or np.all(clip_box.extents == self.axes.bbox.extents))\n                and (clip_path is None\n                     or isinstance(clip_path, TransformedPatchPath)\n                     and clip_path._patch is self.axes.patch))\n\n    def get_clip_on(self):\n        \"\"\"Return whether the artist uses clipping.\"\"\"\n        return self._clipon\n\n    def get_clip_box(self):\n        \"\"\"Return the clipbox.\"\"\"\n        return self.clipbox\n\n    def get_clip_path(self):\n        \"\"\"Return the clip path.\"\"\"\n        return self._clippath\n\n    def get_transformed_clip_path_and_affine(self):\n        \"\"\"\n        Return the clip path with the non-affine part of its\n        transformation applied, and the remaining affine part of its\n        transformation.\n        \"\"\"\n        if self._clippath is not None:\n            return self._clippath.get_transformed_path_and_affine()\n        return None, None\n\n    def set_clip_on(self, b):\n        \"\"\"\n        Set whether the artist uses clipping.\n\n        When False artists will be visible outside of the Axes which\n        can lead to unexpected results.\n\n        Parameters\n        ----------\n        b : bool\n        \"\"\"\n        self._clipon = b\n        # This may result in the callbacks being hit twice, but ensures they\n        # are hit at least once\n        self.pchanged()\n        self.stale = True\n\n    def _set_gc_clip(self, gc):\n        \"\"\"Set the clip properly for the gc.\"\"\"\n        if self._clipon:\n            if self.clipbox is not None:\n                gc.set_clip_rectangle(self.clipbox)\n            gc.set_clip_path(self._clippath)\n        else:\n            gc.set_clip_rectangle(None)\n            gc.set_clip_path(None)\n\n    def get_rasterized(self):\n        \"\"\"Return whether the artist is to be rasterized.\"\"\"\n        return self._rasterized\n\n    def set_rasterized(self, rasterized):\n        \"\"\"\n        Force rasterized (bitmap) drawing for vector graphics output.\n\n        Rasterized drawing is not supported by all artists. If you try to\n        enable this on an artist that does not support it, the command has no\n        effect and a warning will be issued.\n\n        This setting is ignored for pixel-based output.\n\n        See also :doc:`/gallery/misc/rasterization_demo`.\n\n        Parameters\n        ----------\n        rasterized : bool\n        \"\"\"\n        if rasterized and not hasattr(self.draw, \"_supports_rasterization\"):\n            _api.warn_external(f\"Rasterization of '{self}' will be ignored\")\n\n        self._rasterized = rasterized\n\n    def get_agg_filter(self):\n        \"\"\"Return filter function to be used for agg filter.\"\"\"\n        return self._agg_filter\n\n    def set_agg_filter(self, filter_func):\n        \"\"\"\n        Set the agg filter.\n\n        Parameters\n        ----------\n        filter_func : callable\n            A filter function, which takes a (m, n, 3) float array and a dpi\n            value, and returns a (m, n, 3) array.\n\n            .. ACCEPTS: a filter function, which takes a (m, n, 3) float array\n                and a dpi value, and returns a (m, n, 3) array\n        \"\"\"\n        self._agg_filter = filter_func\n        self.stale = True\n\n    def draw(self, renderer):\n        \"\"\"\n        Draw the Artist (and its children) using the given renderer.\n\n        This has no effect if the artist is not visible (`.Artist.get_visible`\n        returns False).\n\n        Parameters\n        ----------\n        renderer : `.RendererBase` subclass.\n\n        Notes\n        -----\n        This method is overridden in the Artist subclasses.\n        \"\"\"\n        if not self.get_visible():\n            return\n        self.stale = False\n\n    def set_alpha(self, alpha):\n        \"\"\"\n        Set the alpha value used for blending - not supported on all backends.\n\n        Parameters\n        ----------\n        alpha : scalar or None\n            *alpha* must be within the 0-1 range, inclusive.\n        \"\"\"\n        if alpha is not None and not isinstance(alpha, Number):\n            raise TypeError(\n                f'alpha must be numeric or None, not {type(alpha)}')\n        if alpha is not None and not (0 <= alpha <= 1):\n            raise ValueError(f'alpha ({alpha}) is outside 0-1 range')\n        self._alpha = alpha\n        self.pchanged()\n        self.stale = True\n\n    def _set_alpha_for_array(self, alpha):\n        \"\"\"\n        Set the alpha value used for blending - not supported on all backends.\n\n        Parameters\n        ----------\n        alpha : array-like or scalar or None\n            All values must be within the 0-1 range, inclusive.\n            Masked values and nans are not supported.\n        \"\"\"\n        if isinstance(alpha, str):\n            raise TypeError(\"alpha must be numeric or None, not a string\")\n        if not np.iterable(alpha):\n            Artist.set_alpha(self, alpha)\n            return\n        alpha = np.asarray(alpha)\n        if not (0 <= alpha.min() and alpha.max() <= 1):\n            raise ValueError('alpha must be between 0 and 1, inclusive, '\n                             f'but min is {alpha.min()}, max is {alpha.max()}')\n        self._alpha = alpha\n        self.pchanged()\n        self.stale = True\n\n    def set_visible(self, b):\n        \"\"\"\n        Set the artist's visibility.\n\n        Parameters\n        ----------\n        b : bool\n        \"\"\"\n        self._visible = b\n        self.pchanged()\n        self.stale = True\n\n    def set_animated(self, b):\n        \"\"\"\n        Set whether the artist is intended to be used in an animation.\n\n        If True, the artist is excluded from regular drawing of the figure.\n        You have to call `.Figure.draw_artist` / `.Axes.draw_artist`\n        explicitly on the artist. This approach is used to speed up animations\n        using blitting.\n\n        See also `matplotlib.animation` and\n        :doc:`/tutorials/advanced/blitting`.\n\n        Parameters\n        ----------\n        b : bool\n        \"\"\"\n        if self._animated != b:\n            self._animated = b\n            self.pchanged()\n\n    def set_in_layout(self, in_layout):\n        \"\"\"\n        Set if artist is to be included in layout calculations,\n        E.g. :doc:`/tutorials/intermediate/constrainedlayout_guide`,\n        `.Figure.tight_layout()`, and\n        ``fig.savefig(fname, bbox_inches='tight')``.\n\n        Parameters\n        ----------\n        in_layout : bool\n        \"\"\"\n        self._in_layout = in_layout\n\n    def get_label(self):\n        \"\"\"Return the label used for this artist in the legend.\"\"\"\n        return self._label\n\n    def set_label(self, s):\n        \"\"\"\n        Set a label that will be displayed in the legend.\n\n        Parameters\n        ----------\n        s : object\n            *s* will be converted to a string by calling `str`.\n        \"\"\"\n        if s is not None:\n            self._label = str(s)\n        else:\n            self._label = None\n        self.pchanged()\n        self.stale = True\n\n    def get_zorder(self):\n        \"\"\"Return the artist's zorder.\"\"\"\n        return self.zorder\n\n    def set_zorder(self, level):\n        \"\"\"\n        Set the zorder for the artist.  Artists with lower zorder\n        values are drawn first.\n\n        Parameters\n        ----------\n        level : float\n        \"\"\"\n        if level is None:\n            level = self.__class__.zorder\n        self.zorder = level\n        self.pchanged()\n        self.stale = True\n\n    @property\n    def sticky_edges(self):\n        \"\"\"\n        ``x`` and ``y`` sticky edge lists for autoscaling.\n\n        When performing autoscaling, if a data limit coincides with a value in\n        the corresponding sticky_edges list, then no margin will be added--the\n        view limit \"sticks\" to the edge. A typical use case is histograms,\n        where one usually expects no margin on the bottom edge (0) of the\n        histogram.\n\n        Moreover, margin expansion \"bumps\" against sticky edges and cannot\n        cross them.  For example, if the upper data limit is 1.0, the upper\n        view limit computed by simple margin application is 1.2, but there is a\n        sticky edge at 1.1, then the actual upper view limit will be 1.1.\n\n        This attribute cannot be assigned to; however, the ``x`` and ``y``\n        lists can be modified in place as needed.\n\n        Examples\n        --------\n        >>> artist.sticky_edges.x[:] = (xmin, xmax)\n        >>> artist.sticky_edges.y[:] = (ymin, ymax)\n\n        \"\"\"\n        return self._sticky_edges\n\n    def update_from(self, other):\n        \"\"\"Copy properties from *other* to *self*.\"\"\"\n        self._transform = other._transform\n        self._transformSet = other._transformSet\n        self._visible = other._visible\n        self._alpha = other._alpha\n        self.clipbox = other.clipbox\n        self._clipon = other._clipon\n        self._clippath = other._clippath\n        self._label = other._label\n        self._sketch = other._sketch\n        self._path_effects = other._path_effects\n        self.sticky_edges.x[:] = other.sticky_edges.x.copy()\n        self.sticky_edges.y[:] = other.sticky_edges.y.copy()\n        self.pchanged()\n        self.stale = True\n\n    def properties(self):\n        \"\"\"Return a dictionary of all the properties of the artist.\"\"\"\n        return ArtistInspector(self).properties()\n\n    def _update_props(self, props, errfmt):\n        \"\"\"\n        Helper for `.Artist.set` and `.Artist.update`.\n\n        *errfmt* is used to generate error messages for invalid property\n        names; it get formatted with ``type(self)`` and the property name.\n        \"\"\"\n        ret = []\n        with cbook._setattr_cm(self, eventson=False):\n            for k, v in props.items():\n                # Allow attributes we want to be able to update through\n                # art.update, art.set, setp.\n                if k == \"axes\":\n                    ret.append(setattr(self, k, v))\n                else:\n                    func = getattr(self, f\"set_{k}\", None)\n                    if not callable(func):\n                        raise AttributeError(\n                            errfmt.format(cls=type(self), prop_name=k))\n                    ret.append(func(v))\n        if ret:\n            self.pchanged()\n            self.stale = True\n        return ret\n\n    def update(self, props):\n        \"\"\"\n        Update this artist's properties from the dict *props*.\n\n        Parameters\n        ----------\n        props : dict\n        \"\"\"\n        return self._update_props(\n            props, \"{cls.__name__!r} object has no property {prop_name!r}\")\n\n    def _internal_update(self, kwargs):\n        \"\"\"\n        Update artist properties without prenormalizing them, but generating\n        errors as if calling `set`.\n\n        The lack of prenormalization is to maintain backcompatibility.\n        \"\"\"\n        return self._update_props(\n            kwargs, \"{cls.__name__}.set() got an unexpected keyword argument \"\n            \"{prop_name!r}\")\n\n    def set(self, **kwargs):\n        # docstring and signature are auto-generated via\n        # Artist._update_set_signature_and_docstring() at the end of the\n        # module.\n        return self._internal_update(cbook.normalize_kwargs(kwargs, self))\n\n    @contextlib.contextmanager\n    def _cm_set(self, **kwargs):\n        \"\"\"\n        `.Artist.set` context-manager that restores original values at exit.\n        \"\"\"\n        orig_vals = {k: getattr(self, f\"get_{k}\")() for k in kwargs}\n        try:\n            self.set(**kwargs)\n            yield\n        finally:\n            self.set(**orig_vals)\n\n    def findobj(self, match=None, include_self=True):\n        \"\"\"\n        Find artist objects.\n\n        Recursively find all `.Artist` instances contained in the artist.\n\n        Parameters\n        ----------\n        match\n            A filter criterion for the matches. This can be\n\n            - *None*: Return all objects contained in artist.\n            - A function with signature ``def match(artist: Artist) -> bool``.\n              The result will only contain artists for which the function\n              returns *True*.\n            - A class instance: e.g., `.Line2D`. The result will only contain\n              artists of this class or its subclasses (``isinstance`` check).\n\n        include_self : bool\n            Include *self* in the list to be checked for a match.\n\n        Returns\n        -------\n        list of `.Artist`\n\n        \"\"\"\n        if match is None:  # always return True\n            def matchfunc(x):\n                return True\n        elif isinstance(match, type) and issubclass(match, Artist):\n            def matchfunc(x):\n                return isinstance(x, match)\n        elif callable(match):\n            matchfunc = match\n        else:\n            raise ValueError('match must be None, a matplotlib.artist.Artist '\n                             'subclass, or a callable')\n\n        artists = sum([c.findobj(matchfunc) for c in self.get_children()], [])\n        if include_self and matchfunc(self):\n            artists.append(self)\n        return artists\n\n    def get_cursor_data(self, event):\n        \"\"\"\n        Return the cursor data for a given event.\n\n        .. note::\n            This method is intended to be overridden by artist subclasses.\n            As an end-user of Matplotlib you will most likely not call this\n            method yourself.\n\n        Cursor data can be used by Artists to provide additional context\n        information for a given event. The default implementation just returns\n        *None*.\n\n        Subclasses can override the method and return arbitrary data. However,\n        when doing so, they must ensure that `.format_cursor_data` can convert\n        the data to a string representation.\n\n        The only current use case is displaying the z-value of an `.AxesImage`\n        in the status bar of a plot window, while moving the mouse.\n\n        Parameters\n        ----------\n        event : `matplotlib.backend_bases.MouseEvent`\n\n        See Also\n        --------\n        format_cursor_data\n\n        \"\"\"\n        return None\n\n    def format_cursor_data(self, data):\n        \"\"\"\n        Return a string representation of *data*.\n\n        .. note::\n            This method is intended to be overridden by artist subclasses.\n            As an end-user of Matplotlib you will most likely not call this\n            method yourself.\n\n        The default implementation converts ints and floats and arrays of ints\n        and floats into a comma-separated string enclosed in square brackets,\n        unless the artist has an associated colorbar, in which case scalar\n        values are formatted using the colorbar's formatter.\n\n        See Also\n        --------\n        get_cursor_data\n        \"\"\"\n        if np.ndim(data) == 0 and isinstance(self, ScalarMappable):\n            # This block logically belongs to ScalarMappable, but can't be\n            # implemented in it because most ScalarMappable subclasses inherit\n            # from Artist first and from ScalarMappable second, so\n            # Artist.format_cursor_data would always have precedence over\n            # ScalarMappable.format_cursor_data.\n            n = self.cmap.N\n            if np.ma.getmask(data):\n                return \"[]\"\n            normed = self.norm(data)\n            if np.isfinite(normed):\n                # Midpoints of neighboring color intervals.\n                neighbors = self.norm.inverse(\n                    (int(self.norm(data) * n) + np.array([0, 1])) / n)\n                delta = abs(neighbors - data).max()\n                g_sig_digits = cbook._g_sig_digits(data, delta)\n            else:\n                g_sig_digits = 3  # Consistent with default below.\n            return \"[{:-#.{}g}]\".format(data, g_sig_digits)\n        else:\n            try:\n                data[0]\n            except (TypeError, IndexError):\n                data = [data]\n            data_str = ', '.join('{:0.3g}'.format(item) for item in data\n                                 if isinstance(item, Number))\n            return \"[\" + data_str + \"]\"\n\n    def get_mouseover(self):\n        \"\"\"\n        Return whether this artist is queried for custom context information\n        when the mouse cursor moves over it.\n        \"\"\"\n        return self._mouseover\n\n    def set_mouseover(self, mouseover):\n        \"\"\"\n        Set whether this artist is queried for custom context information when\n        the mouse cursor moves over it.\n\n        Parameters\n        ----------\n        mouseover : bool\n\n        See Also\n        --------\n        get_cursor_data\n        .ToolCursorPosition\n        .NavigationToolbar2\n        \"\"\"\n        self._mouseover = bool(mouseover)\n        ax = self.axes\n        if ax:\n            if self._mouseover:\n                ax._mouseover_set.add(self)\n            else:\n                ax._mouseover_set.discard(self)\n\n    mouseover = property(get_mouseover, set_mouseover)  # backcompat.\n\n\ndef _get_tightbbox_for_layout_only(obj, *args, **kwargs):\n    \"\"\"\n    Matplotlib's `.Axes.get_tightbbox` and `.Axis.get_tightbbox` support a\n    *for_layout_only* kwarg; this helper tries to uses the kwarg but skips it\n    when encountering third-party subclasses that do not support it.\n    \"\"\"\n    try:\n        return obj.get_tightbbox(*args, **{**kwargs, \"for_layout_only\": True})\n    except TypeError:\n        return obj.get_tightbbox(*args, **kwargs)\n\n\nclass ArtistInspector:\n    \"\"\"\n    A helper class to inspect an `~matplotlib.artist.Artist` and return\n    information about its settable properties and their current values.\n    \"\"\"\n\n    def __init__(self, o):\n        r\"\"\"\n        Initialize the artist inspector with an `Artist` or an iterable of\n        `Artist`\\s.  If an iterable is used, we assume it is a homogeneous\n        sequence (all `Artist`\\s are of the same type) and it is your\n        responsibility to make sure this is so.\n        \"\"\"\n        if not isinstance(o, Artist):\n            if np.iterable(o):\n                o = list(o)\n                if len(o):\n                    o = o[0]\n\n        self.oorig = o\n        if not isinstance(o, type):\n            o = type(o)\n        self.o = o\n\n        self.aliasd = self.get_aliases()\n\n    def get_aliases(self):\n        \"\"\"\n        Get a dict mapping property fullnames to sets of aliases for each alias\n        in the :class:`~matplotlib.artist.ArtistInspector`.\n\n        e.g., for lines::\n\n          {'markerfacecolor': {'mfc'},\n           'linewidth'      : {'lw'},\n          }\n        \"\"\"\n        names = [name for name in dir(self.o)\n                 if name.startswith(('set_', 'get_'))\n                    and callable(getattr(self.o, name))]\n        aliases = {}\n        for name in names:\n            func = getattr(self.o, name)\n            if not self.is_alias(func):\n                continue\n            propname = re.search(\"`({}.*)`\".format(name[:4]),  # get_.*/set_.*\n                                 inspect.getdoc(func)).group(1)\n            aliases.setdefault(propname[4:], set()).add(name[4:])\n        return aliases\n\n    _get_valid_values_regex = re.compile(\n        r\"\\n\\s*(?:\\.\\.\\s+)?ACCEPTS:\\s*((?:.|\\n)*?)(?:$|(?:\\n\\n))\"\n    )\n\n    def get_valid_values(self, attr):\n        \"\"\"\n        Get the legal arguments for the setter associated with *attr*.\n\n        This is done by querying the docstring of the setter for a line that\n        begins with \"ACCEPTS:\" or \".. ACCEPTS:\", and then by looking for a\n        numpydoc-style documentation for the setter's first argument.\n        \"\"\"\n\n        name = 'set_%s' % attr\n        if not hasattr(self.o, name):\n            raise AttributeError('%s has no function %s' % (self.o, name))\n        func = getattr(self.o, name)\n\n        docstring = inspect.getdoc(func)\n        if docstring is None:\n            return 'unknown'\n\n        if docstring.startswith('Alias for '):\n            return None\n\n        match = self._get_valid_values_regex.search(docstring)\n        if match is not None:\n            return re.sub(\"\\n *\", \" \", match.group(1))\n\n        # Much faster than list(inspect.signature(func).parameters)[1],\n        # although barely relevant wrt. matplotlib's total import time.\n        param_name = func.__code__.co_varnames[1]\n        # We could set the presence * based on whether the parameter is a\n        # varargs (it can't be a varkwargs) but it's not really worth the it.\n        match = re.search(r\"(?m)^ *\\*?{} : (.+)\".format(param_name), docstring)\n        if match:\n            return match.group(1)\n\n        return 'unknown'\n\n    def _replace_path(self, source_class):\n        \"\"\"\n        Changes the full path to the public API path that is used\n        in sphinx. This is needed for links to work.\n        \"\"\"\n        replace_dict = {'_base._AxesBase': 'Axes',\n                        '_axes.Axes': 'Axes'}\n        for key, value in replace_dict.items():\n            source_class = source_class.replace(key, value)\n        return source_class\n\n    def get_setters(self):\n        \"\"\"\n        Get the attribute strings with setters for object.\n\n        For example, for a line, return ``['markerfacecolor', 'linewidth',\n        ....]``.\n        \"\"\"\n        setters = []\n        for name in dir(self.o):\n            if not name.startswith('set_'):\n                continue\n            func = getattr(self.o, name)\n            if (not callable(func)\n                    or len(inspect.signature(func).parameters) < 2\n                    or self.is_alias(func)):\n                continue\n            setters.append(name[4:])\n        return setters\n\n    def is_alias(self, o):\n        \"\"\"Return whether method object *o* is an alias for another method.\"\"\"\n        ds = inspect.getdoc(o)\n        if ds is None:\n            return False\n        return ds.startswith('Alias for ')\n\n    def aliased_name(self, s):\n        \"\"\"\n        Return 'PROPNAME or alias' if *s* has an alias, else return 'PROPNAME'.\n\n        e.g., for the line markerfacecolor property, which has an\n        alias, return 'markerfacecolor or mfc' and for the transform\n        property, which does not, return 'transform'.\n        \"\"\"\n        aliases = ''.join(' or %s' % x for x in sorted(self.aliasd.get(s, [])))\n        return s + aliases\n\n    _NOT_LINKABLE = {\n        # A set of property setter methods that are not available in our\n        # current docs. This is a workaround used to prevent trying to link\n        # these setters which would lead to \"target reference not found\"\n        # warnings during doc build.\n        'matplotlib.image._ImageBase.set_alpha',\n        'matplotlib.image._ImageBase.set_array',\n        'matplotlib.image._ImageBase.set_data',\n        'matplotlib.image._ImageBase.set_filternorm',\n        'matplotlib.image._ImageBase.set_filterrad',\n        'matplotlib.image._ImageBase.set_interpolation',\n        'matplotlib.image._ImageBase.set_interpolation_stage',\n        'matplotlib.image._ImageBase.set_resample',\n        'matplotlib.text._AnnotationBase.set_annotation_clip',\n    }\n\n    def aliased_name_rest(self, s, target):\n        \"\"\"\n        Return 'PROPNAME or alias' if *s* has an alias, else return 'PROPNAME',\n        formatted for reST.\n\n        e.g., for the line markerfacecolor property, which has an\n        alias, return 'markerfacecolor or mfc' and for the transform\n        property, which does not, return 'transform'.\n        \"\"\"\n        # workaround to prevent \"reference target not found\"\n        if target in self._NOT_LINKABLE:\n            return f'``{s}``'\n\n        aliases = ''.join(' or %s' % x for x in sorted(self.aliasd.get(s, [])))\n        return ':meth:`%s <%s>`%s' % (s, target, aliases)\n\n    def pprint_setters(self, prop=None, leadingspace=2):\n        \"\"\"\n        If *prop* is *None*, return a list of strings of all settable\n        properties and their valid values.\n\n        If *prop* is not *None*, it is a valid property name and that\n        property will be returned as a string of property : valid\n        values.\n        \"\"\"\n        if leadingspace:\n            pad = ' ' * leadingspace\n        else:\n            pad = ''\n        if prop is not None:\n            accepts = self.get_valid_values(prop)\n            return '%s%s: %s' % (pad, prop, accepts)\n\n        lines = []\n        for prop in sorted(self.get_setters()):\n            accepts = self.get_valid_values(prop)\n            name = self.aliased_name(prop)\n            lines.append('%s%s: %s' % (pad, name, accepts))\n        return lines\n\n    def pprint_setters_rest(self, prop=None, leadingspace=4):\n        \"\"\"\n        If *prop* is *None*, return a list of reST-formatted strings of all\n        settable properties and their valid values.\n\n        If *prop* is not *None*, it is a valid property name and that\n        property will be returned as a string of \"property : valid\"\n        values.\n        \"\"\"\n        if leadingspace:\n            pad = ' ' * leadingspace\n        else:\n            pad = ''\n        if prop is not None:\n            accepts = self.get_valid_values(prop)\n            return '%s%s: %s' % (pad, prop, accepts)\n\n        prop_and_qualnames = []\n        for prop in sorted(self.get_setters()):\n            # Find the parent method which actually provides the docstring.\n            for cls in self.o.__mro__:\n                method = getattr(cls, f\"set_{prop}\", None)\n                if method and method.__doc__ is not None:\n                    break\n            else:  # No docstring available.\n                method = getattr(self.o, f\"set_{prop}\")\n            prop_and_qualnames.append(\n                (prop, f\"{method.__module__}.{method.__qualname__}\"))\n\n        names = [self.aliased_name_rest(prop, target)\n                 .replace('_base._AxesBase', 'Axes')\n                 .replace('_axes.Axes', 'Axes')\n                 for prop, target in prop_and_qualnames]\n        accepts = [self.get_valid_values(prop)\n                   for prop, _ in prop_and_qualnames]\n\n        col0_len = max(len(n) for n in names)\n        col1_len = max(len(a) for a in accepts)\n        table_formatstr = pad + '   ' + '=' * col0_len + '   ' + '=' * col1_len\n\n        return [\n            '',\n            pad + '.. table::',\n            pad + '   :class: property-table',\n            '',\n            table_formatstr,\n            pad + '   ' + 'Property'.ljust(col0_len)\n            + '   ' + 'Description'.ljust(col1_len),\n            table_formatstr,\n            *[pad + '   ' + n.ljust(col0_len) + '   ' + a.ljust(col1_len)\n              for n, a in zip(names, accepts)],\n            table_formatstr,\n            '',\n        ]\n\n    def properties(self):\n        \"\"\"Return a dictionary mapping property name -> value.\"\"\"\n        o = self.oorig\n        getters = [name for name in dir(o)\n                   if name.startswith('get_') and callable(getattr(o, name))]\n        getters.sort()\n        d = {}\n        for name in getters:\n            func = getattr(o, name)\n            if self.is_alias(func):\n                continue\n            try:\n                with warnings.catch_warnings():\n                    warnings.simplefilter('ignore')\n                    val = func()\n            except Exception:\n                continue\n            else:\n                d[name[4:]] = val\n        return d\n\n    def pprint_getters(self):\n        \"\"\"Return the getters and actual values as list of strings.\"\"\"\n        lines = []\n        for name, val in sorted(self.properties().items()):\n            if getattr(val, 'shape', ()) != () and len(val) > 6:\n                s = str(val[:6]) + '...'\n            else:\n                s = str(val)\n            s = s.replace('\\n', ' ')\n            if len(s) > 50:\n                s = s[:50] + '...'\n            name = self.aliased_name(name)\n            lines.append('    %s = %s' % (name, s))\n        return lines\n(B) \nimport matplotlib as mpl\nfrom . import _api, cbook\nfrom .cm import ScalarMappable\nfrom .path import Path\nfrom .transforms import (Bbox, IdentityTransform, Transform, TransformedBbox,\n                         TransformedPatchPath, TransformedPath)\n\n_log = logging.getLogger(__name__)\n\n\ndef allow_rasterization(draw):\n    \"\"\"\n    Decorator for Artist.draw method. Provides routines\n    that run before and after the draw call. The before and after functions\n    are useful for changing artist-dependent renderer attributes or making\n    other setup function calls, such as starting and flushing a mixed-mode\n    renderer.\n    \"\"\"\n\n    @wraps(draw)\n    def draw_wrapper(artist, renderer):\n        try:\n            if artist.get_rasterized():\n                if renderer._raster_depth == 0 and not renderer._rasterizing:\n                    renderer.start_rasterizing()\n                    renderer._rasterizing = True\n                renderer._raster_depth += 1\n            else:\n                if renderer._raster_depth == 0 and renderer._rasterizing:\n                    # Only stop when we are not in a rasterized parent\n                    # and something has be rasterized since last stop\n                    renderer.stop_rasterizing()\n                    renderer._rasterizing = False\n\n            if artist.get_agg_filter() is not None:\n                renderer.start_filter()\n\n            return draw(artist, renderer)\n        finally:\n            if artist.get_agg_filter() is not None:\n                renderer.stop_filter(artist.get_agg_filter())\n            if artist.get_rasterized():\n                renderer._raster_depth -= 1\n            if (renderer._rasterizing and artist.figure and\n                    artist.figure.suppressComposite):\n                # restart rasterizing to prevent merging\n                renderer.stop_rasterizing()\n                renderer.start_rasterizing()\n\n    draw_wrapper._supports_rasterization = True\n    return draw_wrapper\n\n\ndef _finalize_rasterization(draw):\n    \"\"\"\n    Decorator for Artist.draw method. Needed on the outermost artist, i.e.\n    Figure, to finish up if the render is still in rasterized mode.\n    \"\"\"\n    @wraps(draw)\n    def draw_wrapper(artist, renderer, *args, **kwargs):\n        result = draw(artist, renderer, *args, **kwargs)\n        if renderer._rasterizing:\n            renderer.stop_rasterizing()\n            renderer._rasterizing = False\n        return result\n    return draw_wrapper\n\n\ndef _stale_axes_callback(self, val):\n    if self.axes:\n        self.axes.stale = val\n\n\n_XYPair = namedtuple(\"_XYPair\", \"x y\")\n\n\nclass _Unset:\n    def __repr__(self):\n        return \"<UNSET>\"\n_UNSET = _Unset()\n\n\nclass Artist:\n    \"\"\"\n    Abstract base class for objects that render into a FigureCanvas.\n\n    Typically, all visible elements in a figure are subclasses of Artist.\n    \"\"\"\n\n    zorder = 0\n\n    def __init_subclass__(cls):\n        # Inject custom set() methods into the subclass with signature and\n        # docstring based on the subclasses' properties.\n\n        if not hasattr(cls.set, '_autogenerated_signature'):\n            # Don't overwrite cls.set if the subclass or one of its parents\n            # has defined a set method set itself.\n            # If there was no explicit definition, cls.set is inherited from\n            # the hierarchy of auto-generated set methods, which hold the\n            # flag _autogenerated_signature.\n            return\n\n        cls.set = lambda self, **kwargs: Artist.set(self, **kwargs)\n        cls.set.__name__ = \"set\"\n        cls.set.__qualname__ = f\"{cls.__qualname__}.set\"\n        cls._update_set_signature_and_docstring()\n\n    _PROPERTIES_EXCLUDED_FROM_SET = [\n        'navigate_mode',  # not a user-facing function\n        'figure',         # changing the figure is such a profound operation\n                          # that we don't want this in set()\n        '3d_properties',  # cannot be used as a keyword due to leading digit\n    ]\n\n    @classmethod\n    def _update_set_signature_and_docstring(cls):\n        \"\"\"\n        Update the signature of the set function to list all properties\n        as keyword arguments.\n\n        Property aliases are not listed in the signature for brevity, but\n        are still accepted as keyword arguments.\n        \"\"\"\n        cls.set.__signature__ = Signature(\n            [Parameter(\"self\", Parameter.POSITIONAL_OR_KEYWORD),\n             *[Parameter(prop, Parameter.KEYWORD_ONLY, default=_UNSET)\n               for prop in ArtistInspector(cls).get_setters()\n               if prop not in Artist._PROPERTIES_EXCLUDED_FROM_SET]])\n        cls.set._autogenerated_signature = True\n\n        cls.set.__doc__ = (\n            \"Set multiple properties at once.\\n\\n\"\n            \"Supported properties are\\n\\n\"\n            + kwdoc(cls))\n\n    def __init__(self):\n        self._stale = True\n        self.stale_callback = None\n        self._axes = None\n        self.figure = None\n\n        self._transform = None\n        self._transformSet = False\n        self._visible = True\n        self._animated = False\n        self._alpha = None\n        self.clipbox = None\n        self._clippath = None\n        self._clipon = True\n        self._label = ''\n        self._picker = None\n        self._rasterized = False\n        self._agg_filter = None\n        # Normally, artist classes need to be queried for mouseover info if and\n        # only if they override get_cursor_data.\n        self._mouseover = type(self).get_cursor_data != Artist.get_cursor_data\n        self._callbacks = cbook.CallbackRegistry(signals=[\"pchanged\"])\n        try:\n            self.axes = None\n        except AttributeError:\n            # Handle self.axes as a read-only property, as in Figure.\n            pass\n        self._remove_method = None\n        self._url = None\n        self._gid = None\n        self._snap = None\n        self._sketch = mpl.rcParams['path.sketch']\n        self._path_effects = mpl.rcParams['path.effects']\n        self._sticky_edges = _XYPair([], [])\n        self._in_layout = True\n\n    def __getstate__(self):\n        d = self.__dict__.copy()\n        # remove the unpicklable remove method, this will get re-added on load\n        # (by the Axes) if the artist lives on an Axes.\n        d['stale_callback'] = None\n        return d\n\n    def remove(self):\n        \"\"\"\n        Remove the artist from the figure if possible.\n\n        The effect will not be visible until the figure is redrawn, e.g.,\n        with `.FigureCanvasBase.draw_idle`.  Call `~.axes.Axes.relim` to\n        update the axes limits if desired.\n\n        Note: `~.axes.Axes.relim` will not see collections even if the\n        collection was added to the axes with *autolim* = True.\n\n        Note: there is no support for removing the artist's legend entry.\n        \"\"\"\n\n        # There is no method to set the callback.  Instead the parent should\n        # set the _remove_method attribute directly.  This would be a\n        # protected attribute if Python supported that sort of thing.  The\n        # callback has one parameter, which is the child to be removed.\n        if self._remove_method is not None:\n            self._remove_method(self)\n            # clear stale callback\n            self.stale_callback = None\n            _ax_flag = False\n            if hasattr(self, 'axes') and self.axes:\n                # remove from the mouse hit list\n                self.axes._mouseover_set.discard(self)\n                self.axes.stale = True\n                self.axes = None  # decouple the artist from the Axes\n                _ax_flag = True\n\n            if self.figure:\n                self.figure = None\n                if not _ax_flag:\n                    self.figure = True\n\n        else:\n            raise NotImplementedError('cannot remove artist')\n        # TODO: the fix for the collections relim problem is to move the\n        # limits calculation into the artist itself, including the property of\n        # whether or not the artist should affect the limits.  Then there will\n        # be no distinction between axes.add_line, axes.add_patch, etc.\n        # TODO: add legend support\n\n    def have_units(self):\n        \"\"\"Return whether units are set on any axis.\"\"\"\n        ax = self.axes\n        return ax and any(axis.have_units() for axis in ax._axis_map.values())\n\n    def convert_xunits(self, x):\n        \"\"\"\n        Convert *x* using the unit type of the xaxis.\n\n        If the artist is not in contained in an Axes or if the xaxis does not\n        have units, *x* itself is returned.\n        \"\"\"\n        ax = getattr(self, 'axes', None)\n        if ax is None or ax.xaxis is None:\n            return x\n        return ax.xaxis.convert_units(x)\n\n    def convert_yunits(self, y):\n        \"\"\"\n        Convert *y* using the unit type of the yaxis.\n\n        If the artist is not in contained in an Axes or if the yaxis does not\n        have units, *y* itself is returned.\n        \"\"\"\n        ax = getattr(self, 'axes', None)\n        if ax is None or ax.yaxis is None:\n            return y\n        return ax.yaxis.convert_units(y)\n\n    @property\n    def axes(self):\n        \"\"\"The `~.axes.Axes` instance the artist resides in, or *None*.\"\"\"\n        return self._axes\n\n    @axes.setter\n    def axes(self, new_axes):\n        if (new_axes is not None and self._axes is not None\n                and new_axes != self._axes):\n            raise ValueError(\"Can not reset the axes.  You are probably \"\n                             \"trying to re-use an artist in more than one \"\n                             \"Axes which is not supported\")\n        self._axes = new_axes\n        if new_axes is not None and new_axes is not self:\n            self.stale_callback = _stale_axes_callback\n\n    @property\n    def stale(self):\n        \"\"\"\n        Whether the artist is 'stale' and needs to be re-drawn for the output\n        to match the internal state of the artist.\n        \"\"\"\n        return self._stale\n\n    @stale.setter\n    def stale(self, val):\n        self._stale = val\n\n        # if the artist is animated it does not take normal part in the\n        # draw stack and is not expected to be drawn as part of the normal\n        # draw loop (when not saving) so do not propagate this change\n        if self.get_animated():\n            return\n\n        if val and self.stale_callback is not None:\n            self.stale_callback(self, val)\n\n    def get_window_extent(self, renderer):\n        \"\"\"\n        Get the artist's bounding box in display space.\n\n        The bounding box' width and height are nonnegative.\n\n        Subclasses should override for inclusion in the bounding box\n        \"tight\" calculation. Default is to return an empty bounding\n        box at 0, 0.\n\n        Be careful when using this function, the results will not update\n        if the artist window extent of the artist changes.  The extent\n        can change due to any changes in the transform stack, such as\n        changing the axes limits, the figure size, or the canvas used\n        (as is done when saving a figure).  This can lead to unexpected\n        behavior where interactive figures will look fine on the screen,\n        but will save incorrectly.\n        \"\"\"\n        return Bbox([[0, 0], [0, 0]])\n\n    def get_tightbbox(self, renderer):\n        \"\"\"\n        Like `.Artist.get_window_extent`, but includes any clipping.\n\n        Parameters\n        ----------\n        renderer : `.RendererBase` subclass\n            renderer that will be used to draw the figures (i.e.\n            ``fig.canvas.get_renderer()``)\n\n        Returns\n        -------\n        `.Bbox`\n            The enclosing bounding box (in figure pixel coordinates).\n        \"\"\"\n        bbox = self.get_window_extent(renderer)\n        if self.get_clip_on():\n            clip_box = self.get_clip_box()\n            if clip_box is not None:\n                bbox = Bbox.intersection(bbox, clip_box)\n            clip_path = self.get_clip_path()\n            if clip_path is not None:\n                clip_path = clip_path.get_fully_transformed_path()\n                bbox = Bbox.intersection(bbox, clip_path.get_extents())\n        return bbox\n\n    def add_callback(self, func):\n        \"\"\"\n        Add a callback function that will be called whenever one of the\n        `.Artist`'s properties changes.\n\n        Parameters\n        ----------\n        func : callable\n            The callback function. It must have the signature::\n\n                def func(artist: Artist) -> Any\n\n            where *artist* is the calling `.Artist`. Return values may exist\n            but are ignored.\n\n        Returns\n        -------\n        int\n            The observer id associated with the callback. This id can be\n            used for removing the callback with `.remove_callback` later.\n\n        See Also\n        --------\n        remove_callback\n        \"\"\"\n        # Wrapping func in a lambda ensures it can be connected multiple times\n        # and never gets weakref-gc'ed.\n        return self._callbacks.connect(\"pchanged\", lambda: func(self))\n\n    def remove_callback(self, oid):\n        \"\"\"\n        Remove a callback based on its observer id.\n\n        See Also\n        --------\n        add_callback\n        \"\"\"\n        self._callbacks.disconnect(oid)\n\n    def pchanged(self):\n        \"\"\"\n        Call all of the registered callbacks.\n\n        This function is triggered internally when a property is changed.\n\n        See Also\n        --------\n        add_callback\n        remove_callback\n        \"\"\"\n        self._callbacks.process(\"pchanged\")\n\n    def is_transform_set(self):\n        \"\"\"\n        Return whether the Artist has an explicitly set transform.\n\n        This is *True* after `.set_transform` has been called.\n        \"\"\"\n        return self._transformSet\n\n    def set_transform(self, t):\n        \"\"\"\n        Set the artist transform.\n\n        Parameters\n        ----------\n        t : `.Transform`\n        \"\"\"\n        self._transform = t\n        self._transformSet = True\n        self.pchanged()\n        self.stale = True\n\n    def get_transform(self):\n        \"\"\"Return the `.Transform` instance used by this artist.\"\"\"\n        if self._transform is None:\n            self._transform = IdentityTransform()\n        elif (not isinstance(self._transform, Transform)\n              and hasattr(self._transform, '_as_mpl_transform')):\n            self._transform = self._transform._as_mpl_transform(self.axes)\n        return self._transform\n\n    def get_children(self):\n        r\"\"\"Return a list of the child `.Artist`\\s of this `.Artist`.\"\"\"\n        return []\n\n    def _default_contains(self, mouseevent, figure=None):\n        \"\"\"\n        Base impl. for checking whether a mouseevent happened in an artist.\n\n        1. If the artist figure is known and the event did not occur in that\n           figure (by checking its ``canvas`` attribute), reject it.\n        2. Otherwise, return `None, {}`, indicating that the subclass'\n           implementation should be used.\n\n        Subclasses should start their definition of `contains` as follows:\n\n            inside, info = self._default_contains(mouseevent)\n            if inside is not None:\n                return inside, info\n            # subclass-specific implementation follows\n\n        The *figure* kwarg is provided for the implementation of\n        `.Figure.contains`.\n        \"\"\"\n        if figure is not None and mouseevent.canvas is not figure.canvas:\n            return False, {}\n        return None, {}\n\n    def contains(self, mouseevent):\n        \"\"\"\n        Test whether the artist contains the mouse event.\n\n        Parameters\n        ----------\n        mouseevent : `matplotlib.backend_bases.MouseEvent`\n\n        Returns\n        -------\n        contains : bool\n            Whether any values are within the radius.\n        details : dict\n            An artist-specific dictionary of details of the event context,\n            such as which points are contained in the pick radius. See the\n            individual Artist subclasses for details.\n        \"\"\"\n        inside, info = self._default_contains(mouseevent)\n        if inside is not None:\n            return inside, info\n        _log.warning(\"%r needs 'contains' method\", self.__class__.__name__)\n        return False, {}\n\n    def pickable(self):\n        \"\"\"\n        Return whether the artist is pickable.\n\n        See Also\n        --------\n        set_picker, get_picker, pick\n        \"\"\"\n        return self.figure is not None and self._picker is not None\n\n    def pick(self, mouseevent):\n        \"\"\"\n        Process a pick event.\n\n        Each child artist will fire a pick event if *mouseevent* is over\n        the artist and the artist has picker set.\n\n        See Also\n        --------\n        set_picker, get_picker, pickable\n        \"\"\"\n        # Pick self\n        if self.pickable():\n            picker = self.get_picker()\n            if callable(picker):\n                inside, prop = picker(self, mouseevent)\n            else:\n                inside, prop = self.contains(mouseevent)\n            if inside:\n                self.figure.canvas.pick_event(mouseevent, self, **prop)\n\n        # Pick children\n        for a in self.get_children():\n            # make sure the event happened in the same Axes\n            ax = getattr(a, 'axes', None)\n            if (mouseevent.inaxes is None or ax is None\n                    or mouseevent.inaxes == ax):\n                # we need to check if mouseevent.inaxes is None\n                # because some objects associated with an Axes (e.g., a\n                # tick label) can be outside the bounding box of the\n                # Axes and inaxes will be None\n                # also check that ax is None so that it traverse objects\n                # which do no have an axes property but children might\n                a.pick(mouseevent)\n\n    def set_picker(self, picker):\n        \"\"\"\n        Define the picking behavior of the artist.\n\n        Parameters\n        ----------\n        picker : None or bool or float or callable\n            This can be one of the following:\n\n            - *None*: Picking is disabled for this artist (default).\n\n            - A boolean: If *True* then picking will be enabled and the\n              artist will fire a pick event if the mouse event is over\n              the artist.\n\n            - A float: If picker is a number it is interpreted as an\n              epsilon tolerance in points and the artist will fire\n              off an event if its data is within epsilon of the mouse\n              event.  For some artists like lines and patch collections,\n              the artist may provide additional data to the pick event\n              that is generated, e.g., the indices of the data within\n              epsilon of the pick event\n\n            - A function: If picker is callable, it is a user supplied\n              function which determines whether the artist is hit by the\n              mouse event::\n\n                hit, props = picker(artist, mouseevent)\n\n              to determine the hit test.  if the mouse event is over the\n              artist, return *hit=True* and props is a dictionary of\n              properties you want added to the PickEvent attributes.\n        \"\"\"\n        self._picker = picker\n\n    def get_picker(self):\n        \"\"\"\n        Return the picking behavior of the artist.\n\n        The possible values are described in `.set_picker`.\n\n        See Also\n        --------\n        set_picker, pickable, pick\n        \"\"\"\n        return self._picker\n\n    def get_url(self):\n        \"\"\"Return the url.\"\"\"\n        return self._url\n\n    def set_url(self, url):\n        \"\"\"\n        Set the url for the artist.\n\n        Parameters\n        ----------\n        url : str\n        \"\"\"\n        self._url = url\n\n    def get_gid(self):\n        \"\"\"Return the group id.\"\"\"\n        return self._gid\n\n    def set_gid(self, gid):\n        \"\"\"\n        Set the (group) id for the artist.\n\n        Parameters\n        ----------\n        gid : str\n        \"\"\"\n        self._gid = gid\n\n    def get_snap(self):\n        \"\"\"\n        Return the snap setting.\n\n        See `.set_snap` for details.\n        \"\"\"\n        if mpl.rcParams['path.snap']:\n            return self._snap\n        else:\n            return False\n\n    def set_snap(self, snap):\n        \"\"\"\n        Set the snapping behavior.\n\n        Snapping aligns positions with the pixel grid, which results in\n        clearer images. For example, if a black line of 1px width was\n        defined at a position in between two pixels, the resulting image\n        would contain the interpolated value of that line in the pixel grid,\n        which would be a grey value on both adjacent pixel positions. In\n        contrast, snapping will move the line to the nearest integer pixel\n        value, so that the resulting image will really contain a 1px wide\n        black line.\n\n        Snapping is currently only supported by the Agg and MacOSX backends.\n\n        Parameters\n        ----------\n        snap : bool or None\n            Possible values:\n\n            - *True*: Snap vertices to the nearest pixel center.\n            - *False*: Do not modify vertex positions.\n            - *None*: (auto) If the path contains only rectilinear line\n              segments, round to the nearest pixel center.\n        \"\"\"\n        self._snap = snap\n        self.stale = True\n\n    def get_sketch_params(self):\n        \"\"\"\n        Return the sketch parameters for the artist.\n\n        Returns\n        -------\n        tuple or None\n\n            A 3-tuple with the following elements:\n\n            - *scale*: The amplitude of the wiggle perpendicular to the\n              source line.\n            - *length*: The length of the wiggle along the line.\n            - *randomness*: The scale factor by which the length is\n              shrunken or expanded.\n\n            Returns *None* if no sketch parameters were set.\n        \"\"\"\n        return self._sketch\n\n    def set_sketch_params(self, scale=None, length=None, randomness=None):\n        \"\"\"\n        Set the sketch parameters.\n\n        Parameters\n        ----------\n        scale : float, optional\n            The amplitude of the wiggle perpendicular to the source\n            line, in pixels.  If scale is `None`, or not provided, no\n            sketch filter will be provided.\n        length : float, optional\n             The length of the wiggle along the line, in pixels\n             (default 128.0)\n        randomness : float, optional\n            The scale factor by which the length is shrunken or\n            expanded (default 16.0)\n\n            The PGF backend uses this argument as an RNG seed and not as\n            described above. Using the same seed yields the same random shape.\n\n            .. ACCEPTS: (scale: float, length: float, randomness: float)\n        \"\"\"\n        if scale is None:\n            self._sketch = None\n        else:\n            self._sketch = (scale, length or 128.0, randomness or 16.0)\n        self.stale = True\n\n    def set_path_effects(self, path_effects):\n        \"\"\"\n        Set the path effects.\n\n        Parameters\n        ----------\n        path_effects : `.AbstractPathEffect`\n        \"\"\"\n        self._path_effects = path_effects\n        self.stale = True\n\n    def get_path_effects(self):\n        return self._path_effects\n\n    def get_figure(self):\n        \"\"\"Return the `.Figure` instance the artist belongs to.\"\"\"\n        return self.figure\n\n    def set_figure(self, fig):\n        \"\"\"\n        Set the `.Figure` instance the artist belongs to.\n\n        Parameters\n        ----------\n        fig : `.Figure`\n        \"\"\"\n        # if this is a no-op just return\n        if self.figure is fig:\n            return\n        # if we currently have a figure (the case of both `self.figure`\n        # and *fig* being none is taken care of above) we then user is\n        # trying to change the figure an artist is associated with which\n        # is not allowed for the same reason as adding the same instance\n        # to more than one Axes\n        if self.figure is not None:\n            raise RuntimeError(\"Can not put single artist in \"\n                               \"more than one figure\")\n        self.figure = fig\n        if self.figure and self.figure is not self:\n            self.pchanged()\n        self.stale = True\n\n    def set_clip_box(self, clipbox):\n        \"\"\"\n        Set the artist's clip `.Bbox`.\n\n        Parameters\n        ----------\n        clipbox : `.Bbox`\n        \"\"\"\n        self.clipbox = clipbox\n        self.pchanged()\n        self.stale = True\n\n    def set_clip_path(self, path, transform=None):\n        \"\"\"\n        Set the artist's clip path.\n\n        Parameters\n        ----------\n        path : `.Patch` or `.Path` or `.TransformedPath` or None\n            The clip path. If given a `.Path`, *transform* must be provided as\n            well. If *None*, a previously set clip path is removed.\n        transform : `~matplotlib.transforms.Transform`, optional\n            Only used if *path* is a `.Path`, in which case the given `.Path`\n            is converted to a `.TransformedPath` using *transform*.\n\n        Notes\n        -----\n        For efficiency, if *path* is a `.Rectangle` this method will set the\n        clipping box to the corresponding rectangle and set the clipping path\n        to ``None``.\n\n        For technical reasons (support of `~.Artist.set`), a tuple\n        (*path*, *transform*) is also accepted as a single positional\n        parameter.\n\n        .. ACCEPTS: Patch or (Path, Transform) or None\n        \"\"\"\n        from matplotlib.patches import Patch, Rectangle\n\n        success = False\n        if transform is None:\n            if isinstance(path, Rectangle):\n                self.clipbox = TransformedBbox(Bbox.unit(),\n                                               path.get_transform())\n                self._clippath = None\n                success = True\n            elif isinstance(path, Patch):\n                self._clippath = TransformedPatchPath(path)\n                success = True\n            elif isinstance(path, tuple):\n                path, transform = path\n\n        if path is None:\n            self._clippath = None\n            success = True\n        elif isinstance(path, Path):\n            self._clippath = TransformedPath(path, transform)\n            success = True\n        elif isinstance(path, TransformedPatchPath):\n            self._clippath = path\n            success = True\n        elif isinstance(path, TransformedPath):\n            self._clippath = path\n            success = True\n\n        if not success:\n            raise TypeError(\n                \"Invalid arguments to set_clip_path, of type {} and {}\"\n                .format(type(path).__name__, type(transform).__name__))\n        # This may result in the callbacks being hit twice, but guarantees they\n        # will be hit at least once.\n        self.pchanged()\n        self.stale = True\n\n    def get_alpha(self):\n        \"\"\"\n        Return the alpha value used for blending - not supported on all\n        backends.\n        \"\"\"\n        return self._alpha\n\n    def get_visible(self):\n        \"\"\"Return the visibility.\"\"\"\n        return self._visible\n\n    def get_animated(self):\n        \"\"\"Return whether the artist is animated.\"\"\"\n        return self._animated\n\n    def get_in_layout(self):\n        \"\"\"\n        Return boolean flag, ``True`` if artist is included in layout\n        calculations.\n\n        E.g. :doc:`/tutorials/intermediate/constrainedlayout_guide`,\n        `.Figure.tight_layout()`, and\n        ``fig.savefig(fname, bbox_inches='tight')``.\n        \"\"\"\n        return self._in_layout\n\n    def _fully_clipped_to_axes(self):\n        \"\"\"\n        Return a boolean flag, ``True`` if the artist is clipped to the Axes\n        and can thus be skipped in layout calculations. Requires `get_clip_on`\n        is True, one of `clip_box` or `clip_path` is set, ``clip_box.extents``\n        is equivalent to ``ax.bbox.extents`` (if set), and ``clip_path._patch``\n        is equivalent to ``ax.patch`` (if set).\n        \"\"\"\n        # Note that ``clip_path.get_fully_transformed_path().get_extents()``\n        # cannot be directly compared to ``axes.bbox.extents`` because the\n        # extents may be undefined (i.e. equivalent to ``Bbox.null()``)\n        # before the associated artist is drawn, and this method is meant\n        # to determine whether ``axes.get_tightbbox()`` may bypass drawing\n        clip_box = self.get_clip_box()\n        clip_path = self.get_clip_path()\n        return (self.axes is not None\n                and self.get_clip_on()\n                and (clip_box is not None or clip_path is not None)\n                and (clip_box is None\n                     or np.all(clip_box.extents == self.axes.bbox.extents))\n                and (clip_path is None\n                     or isinstance(clip_path, TransformedPatchPath)\n                     and clip_path._patch is self.axes.patch))\n\n    def get_clip_on(self):\n        \"\"\"Return whether the artist uses clipping.\"\"\"\n        return self._clipon\n\n    def get_clip_box(self):\n        \"\"\"Return the clipbox.\"\"\"\n        return self.clipbox\n\n    def get_clip_path(self):\n        \"\"\"Return the clip path.\"\"\"\n        return self._clippath\n\n    def get_transformed_clip_path_and_affine(self):\n        \"\"\"\n        Return the clip path with the non-affine part of its\n        transformation applied, and the remaining affine part of its\n        transformation.\n        \"\"\"\n        if self._clippath is not None:\n            return self._clippath.get_transformed_path_and_affine()\n        return None, None\n\n    def set_clip_on(self, b):\n        \"\"\"\n        Set whether the artist uses clipping.\n\n        When False artists will be visible outside of the Axes which\n        can lead to unexpected results.\n\n        Parameters\n        ----------\n        b : bool\n        \"\"\"\n        self._clipon = b\n        # This may result in the callbacks being hit twice, but ensures they\n        # are hit at least once\n        self.pchanged()\n        self.stale = True\n\n    def _set_gc_clip(self, gc):\n        \"\"\"Set the clip properly for the gc.\"\"\"\n        if self._clipon:\n            if self.clipbox is not None:\n                gc.set_clip_rectangle(self.clipbox)\n            gc.set_clip_path(self._clippath)\n        else:\n            gc.set_clip_rectangle(None)\n            gc.set_clip_path(None)\n\n    def get_rasterized(self):\n        \"\"\"Return whether the artist is to be rasterized.\"\"\"\n        return self._rasterized\n\n    def set_rasterized(self, rasterized):\n        \"\"\"\n        Force rasterized (bitmap) drawing for vector graphics output.\n\n        Rasterized drawing is not supported by all artists. If you try to\n        enable this on an artist that does not support it, the command has no\n        effect and a warning will be issued.\n\n        This setting is ignored for pixel-based output.\n\n        See also :doc:`/gallery/misc/rasterization_demo`.\n\n        Parameters\n        ----------\n        rasterized : bool\n        \"\"\"\n        if rasterized and not hasattr(self.draw, \"_supports_rasterization\"):\n            _api.warn_external(f\"Rasterization of '{self}' will be ignored\")\n\n        self._rasterized = rasterized\n\n    def get_agg_filter(self):\n        \"\"\"Return filter function to be used for agg filter.\"\"\"\n        return self._agg_filter\n\n    def set_agg_filter(self, filter_func):\n        \"\"\"\n        Set the agg filter.\n\n        Parameters\n        ----------\n        filter_func : callable\n            A filter function, which takes a (m, n, 3) float array and a dpi\n            value, and returns a (m, n, 3) array.\n\n            .. ACCEPTS: a filter function, which takes a (m, n, 3) float array\n                and a dpi value, and returns a (m, n, 3) array\n        \"\"\"\n        self._agg_filter = filter_func\n        self.stale = True\n\n    def draw(self, renderer):\n        \"\"\"\n        Draw the Artist (and its children) using the given renderer.\n\n        This has no effect if the artist is not visible (`.Artist.get_visible`\n        returns False).\n\n        Parameters\n        ----------\n        renderer : `.RendererBase` subclass.\n\n        Notes\n        -----\n        This method is overridden in the Artist subclasses.\n        \"\"\"\n        if not self.get_visible():\n            return\n        self.stale = False\n\n    def set_alpha(self, alpha):\n        \"\"\"\n        Set the alpha value used for blending - not supported on all backends.\n\n        Parameters\n        ----------\n        alpha : scalar or None\n            *alpha* must be within the 0-1 range, inclusive.\n        \"\"\"\n        if alpha is not None and not isinstance(alpha, Number):\n            raise TypeError(\n                f'alpha must be numeric or None, not {type(alpha)}')\n        if alpha is not None and not (0 <= alpha <= 1):\n            raise ValueError(f'alpha ({alpha}) is outside 0-1 range')\n        self._alpha = alpha\n        self.pchanged()\n        self.stale = True\n\n    def _set_alpha_for_array(self, alpha):\n        \"\"\"\n        Set the alpha value used for blending - not supported on all backends.\n\n        Parameters\n        ----------\n        alpha : array-like or scalar or None\n            All values must be within the 0-1 range, inclusive.\n            Masked values and nans are not supported.\n        \"\"\"\n        if isinstance(alpha, str):\n            raise TypeError(\"alpha must be numeric or None, not a string\")\n        if not np.iterable(alpha):\n            Artist.set_alpha(self, alpha)\n            return\n        alpha = np.asarray(alpha)\n        if not (0 <= alpha.min() and alpha.max() <= 1):\n            raise ValueError('alpha must be between 0 and 1, inclusive, '\n                             f'but min is {alpha.min()}, max is {alpha.max()}')\n        self._alpha = alpha\n        self.pchanged()\n        self.stale = True\n\n    def set_visible(self, b):\n        \"\"\"\n        Set the artist's visibility.\n\n        Parameters\n        ----------\n        b : bool\n        \"\"\"\n        self._visible = b\n        self.pchanged()\n        self.stale = True\n\n    def set_animated(self, b):\n        \"\"\"\n        Set whether the artist is intended to be used in an animation.\n\n        If True, the artist is excluded from regular drawing of the figure.\n        You have to call `.Figure.draw_artist` / `.Axes.draw_artist`\n        explicitly on the artist. This approach is used to speed up animations\n        using blitting.\n\n        See also `matplotlib.animation` and\n        :doc:`/tutorials/advanced/blitting`.\n\n        Parameters\n        ----------\n        b : bool\n        \"\"\"\n        if self._animated != b:\n            self._animated = b\n            self.pchanged()\n\n    def set_in_layout(self, in_layout):\n        \"\"\"\n        Set if artist is to be included in layout calculations,\n        E.g. :doc:`/tutorials/intermediate/constrainedlayout_guide`,\n        `.Figure.tight_layout()`, and\n        ``fig.savefig(fname, bbox_inches='tight')``.\n\n        Parameters\n        ----------\n        in_layout : bool\n        \"\"\"\n        self._in_layout = in_layout\n\n    def get_label(self):\n        \"\"\"Return the label used for this artist in the legend.\"\"\"\n        return self._label\n\n    def set_label(self, s):\n        \"\"\"\n        Set a label that will be displayed in the legend.\n\n        Parameters\n        ----------\n        s : object\n            *s* will be converted to a string by calling `str`.\n        \"\"\"\n        if s is not None:\n            self._label = str(s)\n        else:\n            self._label = None\n        self.pchanged()\n        self.stale = True\n\n    def get_zorder(self):\n        \"\"\"Return the artist's zorder.\"\"\"\n        return self.zorder\n\n    def set_zorder(self, level):\n        \"\"\"\n        Set the zorder for the artist.  Artists with lower zorder\n        values are drawn first.\n\n        Parameters\n        ----------\n        level : float\n        \"\"\"\n        if level is None:\n            level = self.__class__.zorder\n        self.zorder = level\n        self.pchanged()\n        self.stale = True\n\n    @property\n    def sticky_edges(self):\n        \"\"\"\n        ``x`` and ``y`` sticky edge lists for autoscaling.\n\n        When performing autoscaling, if a data limit coincides with a value in\n        the corresponding sticky_edges list, then no margin will be added--the\n        view limit \"sticks\" to the edge. A typical use case is histograms,\n        where one usually expects no margin on the bottom edge (0) of the\n        histogram.\n\n        Moreover, margin expansion \"bumps\" against sticky edges and cannot\n        cross them.  For example, if the upper data limit is 1.0, the upper\n        view limit computed by simple margin application is 1.2, but there is a\n        sticky edge at 1.1, then the actual upper view limit will be 1.1.\n\n        This attribute cannot be assigned to; however, the ``x`` and ``y``\n        lists can be modified in place as needed.\n\n        Examples\n        --------\n        >>> artist.sticky_edges.x[:] = (xmin, xmax)\n        >>> artist.sticky_edges.y[:] = (ymin, ymax)\n\n        \"\"\"\n        return self._sticky_edges\n\n    def update_from(self, other):\n        \"\"\"Copy properties from *other* to *self*.\"\"\"\n        self._transform = other._transform\n        self._transformSet = other._transformSet\n        self._visible = other._visible\n        self._alpha = other._alpha\n        self.clipbox = other.clipbox\n        self._clipon = other._clipon\n        self._clippath = other._clippath\n        self._label = other._label\n        self._sketch = other._sketch\n        self._path_effects = other._path_effects\n        self.sticky_edges.x[:] = other.sticky_edges.x.copy()\n        self.sticky_edges.y[:] = other.sticky_edges.y.copy()\n        self.pchanged()\n        self.stale = True\n\n    def properties(self):\n        \"\"\"Return a dictionary of all the properties of the artist.\"\"\"\n        return ArtistInspector(self).properties()\n\n    def _update_props(self, props, errfmt):\n        \"\"\"\n        Helper for `.Artist.set` and `.Artist.update`.\n\n        *errfmt* is used to generate error messages for invalid property\n        names; it get formatted with ``type(self)`` and the property name.\n        \"\"\"\n        ret = []\n        with cbook._setattr_cm(self, eventson=False):\n            for k, v in props.items():\n                # Allow attributes we want to be able to update through\n                # art.update, art.set, setp.\n                if k == \"axes\":\n                    ret.append(setattr(self, k, v))\n                else:\n                    func = getattr(self, f\"set_{k}\", None)\n                    if not callable(func):\n                        raise AttributeError(\n                            errfmt.format(cls=type(self), prop_name=k))\n                    ret.append(func(v))\n        if ret:\n            self.pchanged()\n            self.stale = True\n        return ret\n\n    def update(self, props):\n        \"\"\"\n        Update this artist's properties from the dict *props*.\n\n        Parameters\n        ----------\n        props : dict\n        \"\"\"\n        return self._update_props(\n            props, \"{cls.__name__!r} object has no property {prop_name!r}\")\n\n    def _internal_update(self, kwargs):\n        \"\"\"\n        Update artist properties without prenormalizing them, but generating\n        errors as if calling `set`.\n\n        The lack of prenormalization is to maintain backcompatibility.\n        \"\"\"\n        return self._update_props(\n            kwargs, \"{cls.__name__}.set() got an unexpected keyword argument \"\n            \"{prop_name!r}\")\n\n    def set(self, **kwargs):\n        # docstring and signature are auto-generated via\n        # Artist._update_set_signature_and_docstring() at the end of the\n        # module.\n        return self._internal_update(cbook.normalize_kwargs(kwargs, self))\n\n    @contextlib.contextmanager\n    def _cm_set(self, **kwargs):\n        \"\"\"\n        `.Artist.set` context-manager that restores original values at exit.\n        \"\"\"\n        orig_vals = {k: getattr(self, f\"get_{k}\")() for k in kwargs}\n        try:\n            self.set(**kwargs)\n            yield\n        finally:\n            self.set(**orig_vals)\n\n    def findobj(self, match=None, include_self=True):\n        \"\"\"\n        Find artist objects.\n\n        Recursively find all `.Artist` instances contained in the artist.\n\n        Parameters\n        ----------\n        match\n            A filter criterion for the matches. This can be\n\n            - *None*: Return all objects contained in artist.\n            - A function with signature ``def match(artist: Artist) -> bool``.\n              The result will only contain artists for which the function\n              returns *True*.\n            - A class instance: e.g., `.Line2D`. The result will only contain\n              artists of this class or its subclasses (``isinstance`` check).\n\n        include_self : bool\n            Include *self* in the list to be checked for a match.\n\n        Returns\n        -------\n        list of `.Artist`\n\n        \"\"\"\n        if match is None:  # always return True\n            def matchfunc(x):\n                return True\n        elif isinstance(match, type) and issubclass(match, Artist):\n            def matchfunc(x):\n                return isinstance(x, match)\n        elif callable(match):\n            matchfunc = match\n        else:\n            raise ValueError('match must be None, a matplotlib.artist.Artist '\n                             'subclass, or a callable')\n\n        artists = sum([c.findobj(matchfunc) for c in self.get_children()], [])\n        if include_self and matchfunc(self):\n            artists.append(self)\n        return artists\n\n    def get_cursor_data(self, event):\n        \"\"\"\n        Return the cursor data for a given event.\n\n        .. note::\n            This method is intended to be overridden by artist subclasses.\n            As an end-user of Matplotlib you will most likely not call this\n            method yourself.\n\n        Cursor data can be used by Artists to provide additional context\n        information for a given event. The default implementation just returns\n        *None*.\n\n        Subclasses can override the method and return arbitrary data. However,\n        when doing so, they must ensure that `.format_cursor_data` can convert\n        the data to a string representation.\n\n        The only current use case is displaying the z-value of an `.AxesImage`\n        in the status bar of a plot window, while moving the mouse.\n\n        Parameters\n        ----------\n        event : `matplotlib.backend_bases.MouseEvent`\n\n        See Also\n        --------\n        format_cursor_data\n\n        \"\"\"\n        return None\n\n    def format_cursor_data(self, data):\n        \"\"\"\n        Return a string representation of *data*.\n\n        .. note::\n            This method is intended to be overridden by artist subclasses.\n            As an end-user of Matplotlib you will most likely not call this\n            method yourself.\n\n        The default implementation converts ints and floats and arrays of ints\n        and floats into a comma-separated string enclosed in square brackets,\n        unless the artist has an associated colorbar, in which case scalar\n        values are formatted using the colorbar's formatter.\n\n        See Also\n        --------\n        get_cursor_data\n        \"\"\"\n        if np.ndim(data) == 0 and isinstance(self, ScalarMappable):\n            # This block logically belongs to ScalarMappable, but can't be\n            # implemented in it because most ScalarMappable subclasses inherit\n            # from Artist first and from ScalarMappable second, so\n            # Artist.format_cursor_data would always have precedence over\n            # ScalarMappable.format_cursor_data.\n            n = self.cmap.N\n            if np.ma.getmask(data):\n                return \"[]\"\n            normed = self.norm(data)\n            if np.isfinite(normed):\n                # Midpoints of neighboring color intervals.\n                neighbors = self.norm.inverse(\n                    (int(self.norm(data) * n) + np.array([0, 1])) / n)\n                delta = abs(neighbors - data).max()\n                g_sig_digits = cbook._g_sig_digits(data, delta)\n            else:\n                g_sig_digits = 3  # Consistent with default below.\n            return \"[{:-#.{}g}]\".format(data, g_sig_digits)\n        else:\n            try:\n                data[0]\n            except (TypeError, IndexError):\n                data = [data]\n            data_str = ', '.join('{:0.3g}'.format(item) for item in data\n                                 if isinstance(item, Number))\n            return \"[\" + data_str + \"]\"\n\n    def get_mouseover(self):", "answer": "C"}
{"uuid": "fbbe36fa-0043-4ddc-9df8-117bd0e6934e", "issue_statement": "[Bug]: get_backend() clears figures from Gcf.figs if they were created under rc_context\n### Bug summary\r\n\r\ncalling `matplotlib.get_backend()` removes all figures from `Gcf` if the *first* figure in `Gcf.figs` was created in an `rc_context`.\r\n\r\n### Code for reproduction\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib import get_backend, rc_context\r\n\r\n# fig1 = plt.figure()  # <- UNCOMMENT THIS LINE AND IT WILL WORK\r\n# plt.ion()            # <- ALTERNATIVELY, UNCOMMENT THIS LINE AND IT WILL ALSO WORK\r\nwith rc_context():\r\n    fig2 = plt.figure()\r\nbefore = f'{id(plt._pylab_helpers.Gcf)} {plt._pylab_helpers.Gcf.figs!r}'\r\nget_backend()\r\nafter = f'{id(plt._pylab_helpers.Gcf)} {plt._pylab_helpers.Gcf.figs!r}'\r\n\r\nassert before == after, '\\n' + before + '\\n' + after\r\n```\r\n\r\n\r\n### Actual outcome\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-1-fa4d099aa289> in <cell line: 11>()\r\n      9 after = f'{id(plt._pylab_helpers.Gcf)} {plt._pylab_helpers.Gcf.figs!r}'\r\n     10 \r\n---> 11 assert before == after, '\\n' + before + '\\n' + after\r\n     12 \r\n\r\nAssertionError: \r\n94453354309744 OrderedDict([(1, <matplotlib.backends.backend_qt.FigureManagerQT object at 0x7fb33e26c220>)])\r\n94453354309744 OrderedDict()\r\n```\r\n\r\n### Expected outcome\r\n\r\nThe figure should not be missing from `Gcf`.  Consequences of this are, e.g, `plt.close(fig2)` doesn't work because `Gcf.destroy_fig()` can't find it.\r\n\r\n### Additional information\r\n\r\n_No response_\r\n\r\n### Operating system\r\n\r\nXubuntu\r\n\r\n### Matplotlib Version\r\n\r\n3.5.2\r\n\r\n### Matplotlib Backend\r\n\r\nQtAgg\r\n\r\n### Python version\r\n\r\nPython 3.10.4\r\n\r\n### Jupyter version\r\n\r\nn/a\r\n\r\n### Installation\r\n\r\nconda", "choices": "(A)     \"\"\"\n    Return a context manager for temporarily changing rcParams.\n\n    Parameters\n    ----------\n    rc : dict\n        The rcParams to temporarily set.\n    fname : str or path-like\n        A file with Matplotlib rc settings. If both *fname* and *rc* are given,\n        settings from *rc* take precedence.\n\n    See Also\n    --------\n    :ref:`customizing-with-matplotlibrc-files`\n\n    Examples\n    --------\n    Passing explicit values via a dict::\n\n        with mpl.rc_context({'interactive': False}):\n            fig, ax = plt.subplots()\n            ax.plot(range(3), range(3))\n            fig.savefig('example.png')\n            plt.close(fig)\n\n    Loading settings from a file::\n\n         with mpl.rc_context(fname='print.rc'):\n             plt.plot(x, y)  # uses 'print.rc'\n\n    \"\"\"\n    orig = rcParams.copy()\n    try:\n        if fname:\n            rc_file(fname)\n        if rc:\n            rcParams.update(rc)\n        yield\n(B)         with mpl.rc_context({'interactive': False}):\n            fig, ax = plt.subplots()\n            ax.plot(range(3), range(3))\n            fig.savefig('example.png')\n            plt.close(fig)\n\n    Loading settings from a file::\n\n         with mpl.rc_context(fname='print.rc'):\n             plt.plot(x, y)  # uses 'print.rc'\n\n    \"\"\"\n    orig = rcParams.copy()\n    try:\n        if fname:\n            rc_file(fname)\n        if rc:\n            rcParams.update(rc)\n        yield\n    finally:\n        dict.update(rcParams, orig)  # Revert to the original rcs.\n\n\ndef use(backend, *, force=True):\n    \"\"\"\n    Select the backend used for rendering and GUI integration.\n\n    Parameters\n    ----------\n    backend : str\n        The backend to switch to.  This can either be one of the standard\n        backend names, which are case-insensitive:\n\n        - interactive backends:\n          GTK3Agg, GTK3Cairo, GTK4Agg, GTK4Cairo, MacOSX, nbAgg, QtAgg,\n          QtCairo, TkAgg, TkCairo, WebAgg, WX, WXAgg, WXCairo, Qt5Agg, Qt5Cairo\n\n        - non-interactive backends:\n(C) \n    See Also\n    --------\n    :ref:`customizing-with-matplotlibrc-files`\n\n    Examples\n    --------\n    Passing explicit values via a dict::\n\n        with mpl.rc_context({'interactive': False}):\n            fig, ax = plt.subplots()\n            ax.plot(range(3), range(3))\n            fig.savefig('example.png')\n            plt.close(fig)\n\n    Loading settings from a file::\n\n         with mpl.rc_context(fname='print.rc'):\n             plt.plot(x, y)  # uses 'print.rc'\n\n    \"\"\"\n    orig = rcParams.copy()\n    try:\n        if fname:\n            rc_file(fname)\n        if rc:\n            rcParams.update(rc)\n        yield\n    finally:\n        dict.update(rcParams, orig)  # Revert to the original rcs.\n\n\ndef use(backend, *, force=True):\n    \"\"\"\n    Select the backend used for rendering and GUI integration.\n\n    Parameters\n    ----------\n(D)     with _api.suppress_matplotlib_deprecation_warning():\n        from .style.core import STYLE_BLACKLIST\n        rc_from_file = rc_params_from_file(\n            fname, use_default_template=use_default_template)\n        rcParams.update({k: rc_from_file[k] for k in rc_from_file\n                         if k not in STYLE_BLACKLIST})\n\n\n@contextlib.contextmanager\ndef rc_context(rc=None, fname=None):\n    \"\"\"\n    Return a context manager for temporarily changing rcParams.\n\n    Parameters\n    ----------\n    rc : dict\n        The rcParams to temporarily set.\n    fname : str or path-like\n        A file with Matplotlib rc settings. If both *fname* and *rc* are given,\n        settings from *rc* take precedence.\n\n    See Also\n    --------\n    :ref:`customizing-with-matplotlibrc-files`\n\n    Examples\n    --------\n    Passing explicit values via a dict::\n\n        with mpl.rc_context({'interactive': False}):\n            fig, ax = plt.subplots()\n            ax.plot(range(3), range(3))\n            fig.savefig('example.png')\n            plt.close(fig)\n\n    Loading settings from a file::\n\n         with mpl.rc_context(fname='print.rc'):", "answer": "A"}
{"uuid": "193bbd0a-b371-4541-9e71-2d0eb8e342d0", "issue_statement": "[Bug]: set_visible() not working for 3d projection \n### Bug summary\r\n\r\nin the subplot projection=\"3d\" the set_visible function doesn't work even if the value is set to False\r\n\r\n### Code for reproduction\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.gridspec import GridSpec\r\n\r\nfig, (ax1, ax2) = plt.subplots(1, 2, subplot_kw={'projection': '3d'})\r\nax1.scatter(1,1,1)\r\nax2.scatter(1,1,1, c='r')\r\nax1.set_visible(False)\r\n\r\nplt.show()\r\n# Thanks Tim for your help! \r\n```\r\n\r\n\r\n### Actual outcome\r\n\r\nthe subplot remains visible which should not happen if the value is set to False\r\n\r\n### Expected outcome\r\n\r\nthe subplot is not visible if the value is set to False\r\n\r\n### Additional information\r\n\r\n_No response_\r\n\r\n### Operating system\r\n\r\n_No response_\r\n\r\n### Matplotlib Version\r\n\r\n3.4.2\r\n\r\n### Matplotlib Backend\r\n\r\nQt5Agg\r\n\r\n### Python version\r\n\r\n3.8.10\r\n\r\n### Jupyter version\r\n\r\n_No response_\r\n\r\n### Installation\r\n\r\n_No response_", "choices": "(A)     def draw(self, renderer):\n        self._unstale_viewLim()\n\n        # draw the background patch\n        self.patch.draw(renderer)\n        self._frameon = False\n\n        # first, set the aspect\n(B) \n        # draw the background patch\n        self.patch.draw(renderer)\n        self._frameon = False\n\n        # first, set the aspect\n        # this is duplicated from `axes._base._AxesBase.draw`\n        # but must be called before any of the artist are drawn as\n(C) \n    @martist.allow_rasterization\n    def draw(self, renderer):\n        self._unstale_viewLim()\n\n        # draw the background patch\n        self.patch.draw(renderer)\n        self._frameon = False\n(D)         pb1 = pb.shrunk_to_aspect(box_aspect, pb, fig_aspect)\n        self._set_position(pb1.anchored(self.get_anchor(), pb), 'active')\n\n    @martist.allow_rasterization\n    def draw(self, renderer):\n        self._unstale_viewLim()\n\n        # draw the background patch", "answer": "C"}
{"uuid": "544c3148-e213-4f83-8dd8-3f0fbbbd521b", "issue_statement": "[Bug]: DPI of a figure is doubled after unpickling on M1 Mac\n### Bug summary\r\n\r\nWhen a figure is unpickled, it's dpi is doubled. This behaviour happens every time and if done in a loop it can cause an `OverflowError`.\r\n\r\n### Code for reproduction\r\n\r\n```python\r\nimport numpy as np\r\nimport matplotlib\r\nimport matplotlib.pyplot as plt\r\nimport pickle\r\nimport platform\r\n\r\nprint(matplotlib.get_backend())\r\nprint('Matplotlib ver:', matplotlib.__version__)\r\nprint('Platform:', platform.platform())\r\nprint('System:', platform.system())\r\nprint('Release:', platform.release())\r\nprint('Python ver:', platform.python_version())\r\n\r\n\r\ndef dump_load_get_dpi(fig):\r\n    with open('sinus.pickle','wb') as file:\r\n        pickle.dump(fig, file)\r\n\r\n    with open('sinus.pickle', 'rb') as blob:\r\n        fig2 = pickle.load(blob)\r\n    return fig2, fig2.dpi\r\n\r\n\r\ndef run():\r\n    fig = plt.figure()\r\n    x = np.linspace(0,2*np.pi)\r\n    y = np.sin(x)\r\n\r\n    for i in range(32):\r\n        print(f'{i}: {fig.dpi}')\r\n        fig, dpi = dump_load_get_dpi(fig)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\n\r\n### Actual outcome\r\n\r\n```\r\nMacOSX\r\nMatplotlib ver: 3.5.2\r\nPlatform: macOS-12.4-arm64-arm-64bit\r\nSystem: Darwin\r\nRelease: 21.5.0\r\nPython ver: 3.9.12\r\n0: 200.0\r\n1: 400.0\r\n2: 800.0\r\n3: 1600.0\r\n4: 3200.0\r\n5: 6400.0\r\n6: 12800.0\r\n7: 25600.0\r\n8: 51200.0\r\n9: 102400.0\r\n10: 204800.0\r\n11: 409600.0\r\n12: 819200.0\r\n13: 1638400.0\r\n14: 3276800.0\r\n15: 6553600.0\r\n16: 13107200.0\r\n17: 26214400.0\r\n18: 52428800.0\r\n19: 104857600.0\r\n20: 209715200.0\r\n21: 419430400.0\r\nTraceback (most recent call last):\r\n  File \"/Users/wsykala/projects/matplotlib/example.py\", line 34, in <module>\r\n    run()\r\n  File \"/Users/wsykala/projects/matplotlib/example.py\", line 30, in run\r\n    fig, dpi = dump_load_get_dpi(fig)\r\n  File \"/Users/wsykala/projects/matplotlib/example.py\", line 20, in dump_load_get_dpi\r\n    fig2 = pickle.load(blob)\r\n  File \"/Users/wsykala/miniconda3/envs/playground/lib/python3.9/site-packages/matplotlib/figure.py\", line 2911, in __setstate__\r\n    mgr = plt._backend_mod.new_figure_manager_given_figure(num, self)\r\n  File \"/Users/wsykala/miniconda3/envs/playground/lib/python3.9/site-packages/matplotlib/backend_bases.py\", line 3499, in new_figure_manager_given_figure\r\n    canvas = cls.FigureCanvas(figure)\r\n  File \"/Users/wsykala/miniconda3/envs/playground/lib/python3.9/site-packages/matplotlib/backends/backend_macosx.py\", line 32, in __init__\r\n    _macosx.FigureCanvas.__init__(self, width, height)\r\nOverflowError: signed integer is greater than maximum\r\n```\r\n\r\n### Expected outcome\r\n\r\n```\r\nMacOSX\r\nMatplotlib ver: 3.5.2\r\nPlatform: macOS-12.4-arm64-arm-64bit\r\nSystem: Darwin\r\nRelease: 21.5.0\r\nPython ver: 3.9.12\r\n0: 200.0\r\n1: 200.0\r\n2: 200.0\r\n3: 200.0\r\n4: 200.0\r\n5: 200.0\r\n6: 200.0\r\n7: 200.0\r\n8: 200.0\r\n9: 200.0\r\n10: 200.0\r\n11: 200.0\r\n12: 200.0\r\n13: 200.0\r\n14: 200.0\r\n15: 200.0\r\n16: 200.0\r\n17: 200.0\r\n18: 200.0\r\n19: 200.0\r\n20: 200.0\r\n21: 200.0\r\n22: 200.0\r\n```\r\n\r\n### Additional information\r\n\r\nThis seems to happen only on M1 MacBooks and the version of python doesn't matter.\r\n\r\n### Operating system\r\n\r\nOS/X\r\n\r\n### Matplotlib Version\r\n\r\n3.5.2\r\n\r\n### Matplotlib Backend\r\n\r\nMacOSX\r\n\r\n### Python version\r\n\r\n3.9.12\r\n\r\n### Jupyter version\r\n\r\n_No response_\r\n\r\n### Installation\r\n\r\npip", "choices": "(A) \n        # check whether the figure manager (if any) is registered with pyplot\n        from matplotlib import _pylab_helpers\n        if self.canvas.manager in _pylab_helpers.Gcf.figs.values():\n            state['_restore_to_pylab'] = True\n        return state\n\n    def __setstate__(self, state):\n        version = state.pop('__mpl_version__')\n(B)         # Set cached renderer to None -- it can't be pickled.\n        state[\"_cachedRenderer\"] = None\n\n        # add version information to the state\n        state['__mpl_version__'] = mpl.__version__\n\n        # check whether the figure manager (if any) is registered with pyplot\n        from matplotlib import _pylab_helpers\n        if self.canvas.manager in _pylab_helpers.Gcf.figs.values():\n(C)         # add version information to the state\n        state['__mpl_version__'] = mpl.__version__\n\n        # check whether the figure manager (if any) is registered with pyplot\n        from matplotlib import _pylab_helpers\n        if self.canvas.manager in _pylab_helpers.Gcf.figs.values():\n            state['_restore_to_pylab'] = True\n        return state\n\n(D)         # re-attached to another.\n        state.pop(\"canvas\")\n\n        # Set cached renderer to None -- it can't be pickled.\n        state[\"_cachedRenderer\"] = None\n\n        # add version information to the state\n        state['__mpl_version__'] = mpl.__version__\n", "answer": "B"}
{"uuid": "3b81927d-1450-49ff-91c1-8423cd5152ef", "issue_statement": "'Poly3DCollection' object has no attribute '_facecolors2d'\nThe following minimal example demonstrates the issue:\n\n```\nimport numpy as np\nimport matplotlib.tri as mtri\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ny,x = np.ogrid[1:10:100j, 1:10:100j]\nz2 = np.cos(x)**3 - np.sin(y)**2\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nr = ax.plot_surface(x,y,z2, cmap='hot')\nr.get_facecolors()\n```\n\nIt fails on the last line with the following traceback:\n\n```\nAttributeError                            Traceback (most recent call last)\n<ipython-input-13-de0f41d662cd> in <module>()\n----> 1 r.get_facecolors()\n\n/home/oliver/.virtualenvs/mpl/local/lib/python2.7/site-packages/mpl_toolkits/mplot3d/art3d.pyc in get_facecolors(self)\n    634\n    635     def get_facecolors(self):\n--> 636         return self._facecolors2d\n    637     get_facecolor = get_facecolors\n    638\n\nAttributeError: 'Poly3DCollection' object has no attribute '_facecolors2d'\n```\n\nTested with mpl versions 1.3.1 and 1.4.2.\n\nSent here by Benjamin, from the mpl users mailing list (mail with the same title). Sorry for dumping this without more assistance, I'm not yet at a python level where I can help in debugging, I think (well, it seems daunting).", "choices": "(A)     def get_edgecolor(self):\n        return self._edgecolors2d\n\n\ndef poly_collection_2d_to_3d(col, zs=0, zdir='z'):\n    \"\"\"Convert a PolyCollection to a Poly3DCollection object.\"\"\"\n    segments_3d, codes = _paths_to_3d_segments_with_codes(\n            col.get_paths(), zs, zdir)\n    col.__class__ = Poly3DCollection\n    col.set_verts_and_codes(segments_3d, codes)\n    col.set_3d_properties()\n\n\ndef juggle_axes(xs, ys, zs, zdir):\n    \"\"\"\n    Reorder coordinates so that 2D xs, ys can be plotted in the plane\n    orthogonal to zdir. zdir is normally x, y or z. However, if zdir\n    starts with a '-' it is interpreted as a compensation for rotate_axes.\n    \"\"\"\n(B)         try:\n            self._edgecolors = mcolors.to_rgba_array(\n                    self._edgecolor3d, self._alpha)\n        except (AttributeError, TypeError, IndexError):\n            pass\n        self.stale = True\n\n    def get_facecolor(self):\n        return self._facecolors2d\n\n    def get_edgecolor(self):\n        return self._edgecolors2d\n\n\ndef poly_collection_2d_to_3d(col, zs=0, zdir='z'):\n    \"\"\"Convert a PolyCollection to a Poly3DCollection object.\"\"\"\n    segments_3d, codes = _paths_to_3d_segments_with_codes(\n            col.get_paths(), zs, zdir)\n    col.__class__ = Poly3DCollection\n(C)     \"\"\"Convert a PolyCollection to a Poly3DCollection object.\"\"\"\n    segments_3d, codes = _paths_to_3d_segments_with_codes(\n            col.get_paths(), zs, zdir)\n    col.__class__ = Poly3DCollection\n    col.set_verts_and_codes(segments_3d, codes)\n    col.set_3d_properties()\n\n\ndef juggle_axes(xs, ys, zs, zdir):\n    \"\"\"\n    Reorder coordinates so that 2D xs, ys can be plotted in the plane\n    orthogonal to zdir. zdir is normally x, y or z. However, if zdir\n    starts with a '-' it is interpreted as a compensation for rotate_axes.\n    \"\"\"\n    if zdir == 'x':\n        return zs, xs, ys\n    elif zdir == 'y':\n        return xs, zs, ys\n    elif zdir[0] == '-':\n(D)         self.stale = True\n\n    def get_facecolor(self):\n        return self._facecolors2d\n\n    def get_edgecolor(self):\n        return self._edgecolors2d\n\n\ndef poly_collection_2d_to_3d(col, zs=0, zdir='z'):\n    \"\"\"Convert a PolyCollection to a Poly3DCollection object.\"\"\"\n    segments_3d, codes = _paths_to_3d_segments_with_codes(\n            col.get_paths(), zs, zdir)\n    col.__class__ = Poly3DCollection\n    col.set_verts_and_codes(segments_3d, codes)\n    col.set_3d_properties()\n\n\ndef juggle_axes(xs, ys, zs, zdir):", "answer": "D"}
{"uuid": "38aaa59c-b83c-4ecc-b353-9f053d496a4e", "issue_statement": "[Bug]: 'Line3D' object has no attribute '_verts3d'\n### Bug summary\n\nI use matplotlib 3D to visualize some lines in 3D. When I first run the following code, the code can run right. But, if I give `x_s_0[n]` a numpy array, it will report the error 'input operand has more dimensions than allowed by the axis remapping'. The point is when next I give  `x_s_0[n]` and other variables an int number, the AttributeError: 'Line3D' object has no attribute '_verts3d' will appear and can not be fixed whatever I change the variables or delete them. The error can be only fixed when I restart the kernel of ipython console. I don't know why it happens, so I come here for help.\n\n### Code for reproduction\n\n```python\nx_s_0 = np.array(['my int number list'])\r\nx_e_0 = np.array(['my int number list'])\r\ny_s_0 = np.array(['my int number list'])\r\ny_e_0 = np.array(['my int number list'])\r\nz_s_0 = np.array(['my int number list'])\r\nz_e_0 = np.array(['my int number list'])\r\n\r\nfig = plt.figure()\r\n        ax = fig.gca(projection='3d')\r\n        ax.view_init(elev=90, azim=0)\r\n        ax.set_zlim3d(-10, 10)\r\n        clr_list = 'r-'\r\n\r\n        for n in range(np.size(z_s_0, axis=0)):\r\n            ax.plot([int(x_s_0[n]), int(x_e_0[n])],\r\n                    [int(y_s_0[n]), int(y_e_0[n])],\r\n                    [int(z_s_0[n]), int(z_e_0[n])], clr_list)\r\n\r\n        plt.xlabel('x')\r\n        plt.ylabel('y')\r\n        # ax.zlabel('z')\r\n        plt.title('90-0')\r\n        plt.show()\n```\n\n\n### Actual outcome\n\nTraceback (most recent call last):\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-80-e04907066a16>\", line 20, in <module>\r\n    plt.show()\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 368, in show\r\n    return _backend_mod.show(*args, **kwargs)\r\n  File \"/home/hanyaning/.pycharm_helpers/pycharm_matplotlib_backend/backend_interagg.py\", line 29, in __call__\r\n    manager.show(**kwargs)\r\n  File \"/home/hanyaning/.pycharm_helpers/pycharm_matplotlib_backend/backend_interagg.py\", line 112, in show\r\n    self.canvas.show()\r\n  File \"/home/hanyaning/.pycharm_helpers/pycharm_matplotlib_backend/backend_interagg.py\", line 68, in show\r\n    FigureCanvasAgg.draw(self)\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\", line 436, in draw\r\n    self.figure.draw(self.renderer)\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/matplotlib/artist.py\", line 73, in draw_wrapper\r\n    result = draw(artist, renderer, *args, **kwargs)\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/matplotlib/artist.py\", line 50, in draw_wrapper\r\n    return draw(artist, renderer)\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/matplotlib/figure.py\", line 2803, in draw\r\n    mimage._draw_list_compositing_images(\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/matplotlib/image.py\", line 132, in _draw_list_compositing_images\r\n    a.draw(renderer)\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/matplotlib/artist.py\", line 50, in draw_wrapper\r\n    return draw(artist, renderer)\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/mpl_toolkits/mplot3d/axes3d.py\", line 469, in draw\r\n    super().draw(renderer)\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/matplotlib/artist.py\", line 50, in draw_wrapper\r\n    return draw(artist, renderer)\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/matplotlib/axes/_base.py\", line 3082, in draw\r\n    mimage._draw_list_compositing_images(\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/matplotlib/image.py\", line 132, in _draw_list_compositing_images\r\n    a.draw(renderer)\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/matplotlib/artist.py\", line 50, in draw_wrapper\r\n    return draw(artist, renderer)\r\n  File \"/home/hanyaning/anaconda3/envs/SBeA/lib/python3.8/site-packages/mpl_toolkits/mplot3d/art3d.py\", line 215, in draw\r\n    xs3d, ys3d, zs3d = self._verts3d\r\nAttributeError: 'Line3D' object has no attribute '_verts3d'\n\n### Expected outcome\n\nSome 3D lines\n\n### Additional information\n\n_No response_\n\n### Operating system\n\nLocal: windows + pycharm, Remote: Ubuntu 20.04\n\n### Matplotlib Version\n\n3.5.0\n\n### Matplotlib Backend\n\nmodule://backend_interagg\n\n### Python version\n\n3.8.12\n\n### Jupyter version\n\n_No response_\n\n### Installation\n\npip", "choices": "(A)         self._verts3d = xs, ys, zs\n\n    def set_3d_properties(self, zs=0, zdir='z'):\n        xs = self.get_xdata()\n        ys = self.get_ydata()\n        zs = np.broadcast_to(zs, len(xs))\n        self._verts3d = juggle_axes(xs, ys, zs, zdir)\n(B)     def set_3d_properties(self, zs=0, zdir='z'):\n        xs = self.get_xdata()\n        ys = self.get_ydata()\n        zs = np.broadcast_to(zs, len(xs))\n        self._verts3d = juggle_axes(xs, ys, zs, zdir)\n        self.stale = True\n\n(C)         self._verts3d = juggle_axes(xs, ys, zs, zdir)\n        self.stale = True\n\n    def set_data_3d(self, *args):\n        \"\"\"\n        Set the x, y and z data\n\n(D)         ys = self.get_ydata()\n        zs = np.broadcast_to(zs, len(xs))\n        self._verts3d = juggle_axes(xs, ys, zs, zdir)\n        self.stale = True\n\n    def set_data_3d(self, *args):\n        \"\"\"", "answer": "B"}
{"uuid": "595f5b00-46f1-43a4-91a2-a855e323ace0", "issue_statement": "legend draggable as keyword\n<!--To help us understand and resolve your issue, please fill out the form to the best of your ability.-->\r\n<!--You can feel free to delete the sections that do not apply.-->\r\n\r\n### Feature request\r\n\r\n**There is not keyword to make legend draggable at creation**\r\n\r\n<!--A short 1-2 sentences that succinctly describes the bug-->\r\n\r\nIs there a code reason why one can not add a \"draggable=True\" keyword to the __init__ function for Legend?  This would be more handy than having to call it after legend creation.  And, naively, it would seem simple to do.  But maybe there is a reason why it would not work?", "choices": "(A)     The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\"\"\")\n\n\nclass Legend(Artist):\n    \"\"\"\n    Place a legend on the axes at location loc.\n    \"\"\"\n\n    # 'best' is only implemented for axes legends\n    codes = {'best': 0, **AnchoredOffsetbox.codes}\n    zorder = 5\n\n    def __str__(self):\n        return \"Legend\"\n\n    @_api.make_keyword_only(\"3.6\", \"loc\")\n    @_docstring.dedent_interpd\n    def __init__(\n        self, parent, handles, labels,\n        loc=None,\n        numpoints=None,      # number of points in the legend line\n        markerscale=None,    # relative size of legend markers vs. original\n        markerfirst=True,    # left/right ordering of legend marker and label\n        scatterpoints=None,  # number of scatter points\n        scatteryoffsets=None,\n        prop=None,           # properties for the legend texts\n        fontsize=None,       # keyword to set font size directly\n        labelcolor=None,     # keyword to set the text color\n\n        # spacing & pad defined as a fraction of the font-size\n        borderpad=None,      # whitespace inside the legend border\n        labelspacing=None,   # vertical space between the legend entries\n        handlelength=None,   # length of the legend handles\n        handleheight=None,   # height of the legend handles\n        handletextpad=None,  # pad between the legend handle and text\n        borderaxespad=None,  # pad between the axes and legend border\n        columnspacing=None,  # spacing between columns\n\n        ncols=1,     # number of columns\n        mode=None,  # horizontal distribution of columns: None or \"expand\"\n\n        fancybox=None,  # True: fancy box, False: rounded box, None: rcParam\n        shadow=None,\n        title=None,           # legend title\n        title_fontsize=None,  # legend title font size\n        framealpha=None,      # set frame alpha\n        edgecolor=None,       # frame patch edgecolor\n        facecolor=None,       # frame patch facecolor\n\n        bbox_to_anchor=None,  # bbox to which the legend will be anchored\n        bbox_transform=None,  # transform for the bbox\n        frameon=None,         # draw frame\n        handler_map=None,\n        title_fontproperties=None,  # properties for the legend title\n        alignment=\"center\",       # control the alignment within the legend box\n        *,\n        ncol=1  # synonym for ncols (backward compatibility)\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        parent : `~matplotlib.axes.Axes` or `.Figure`\n            The artist that contains the legend.\n\n        handles : list of `.Artist`\n            A list of Artists (lines, patches) to be added to the legend.\n\n        labels : list of str\n            A list of labels to show next to the artists. The length of handles\n            and labels should be the same. If they are not, they are truncated\n            to the smaller of both lengths.\n\n        Other Parameters\n        ----------------\n        %(_legend_kw_doc)s\n\n        Notes\n        -----\n        Users can specify any arbitrary location for the legend using the\n        *bbox_to_anchor* keyword argument. *bbox_to_anchor* can be a\n        `.BboxBase` (or derived there from) or a tuple of 2 or 4 floats.\n        See `set_bbox_to_anchor` for more detail.\n\n        The legend location can be specified by setting *loc* with a tuple of\n        2 floats, which is interpreted as the lower-left corner of the legend\n        in the normalized axes coordinate.\n        \"\"\"\n        # local import only to avoid circularity\n        from matplotlib.axes import Axes\n        from matplotlib.figure import FigureBase\n\n        super().__init__()\n\n        if prop is None:\n            if fontsize is not None:\n                self.prop = FontProperties(size=fontsize)\n            else:\n                self.prop = FontProperties(\n                    size=mpl.rcParams[\"legend.fontsize\"])\n        else:\n            self.prop = FontProperties._from_any(prop)\n            if isinstance(prop, dict) and \"size\" not in prop:\n                self.prop.set_size(mpl.rcParams[\"legend.fontsize\"])\n\n        self._fontsize = self.prop.get_size_in_points()\n\n        self.texts = []\n        self.legendHandles = []\n        self._legend_title_box = None\n\n        #: A dictionary with the extra handler mappings for this Legend\n        #: instance.\n        self._custom_handler_map = handler_map\n\n        def val_or_rc(val, rc_name):\n            return val if val is not None else mpl.rcParams[rc_name]\n\n        self.numpoints = val_or_rc(numpoints, 'legend.numpoints')\n        self.markerscale = val_or_rc(markerscale, 'legend.markerscale')\n        self.scatterpoints = val_or_rc(scatterpoints, 'legend.scatterpoints')\n        self.borderpad = val_or_rc(borderpad, 'legend.borderpad')\n        self.labelspacing = val_or_rc(labelspacing, 'legend.labelspacing')\n        self.handlelength = val_or_rc(handlelength, 'legend.handlelength')\n        self.handleheight = val_or_rc(handleheight, 'legend.handleheight')\n        self.handletextpad = val_or_rc(handletextpad, 'legend.handletextpad')\n        self.borderaxespad = val_or_rc(borderaxespad, 'legend.borderaxespad')\n        self.columnspacing = val_or_rc(columnspacing, 'legend.columnspacing')\n        self.shadow = val_or_rc(shadow, 'legend.shadow')\n        # trim handles and labels if illegal label...\n        _lab, _hand = [], []\n        for label, handle in zip(labels, handles):\n            if isinstance(label, str) and label.startswith('_'):\n                _api.warn_external(f\"The label {label!r} of {handle!r} starts \"\n                                   \"with '_'. It is thus excluded from the \"\n                                   \"legend.\")\n            else:\n                _lab.append(label)\n                _hand.append(handle)\n        labels, handles = _lab, _hand\n\n        handles = list(handles)\n        if len(handles) < 2:\n            ncols = 1\n        self._ncols = ncols if ncols != 1 else ncol\n\n        if self.numpoints <= 0:\n            raise ValueError(\"numpoints must be > 0; it was %d\" % numpoints)\n\n        # introduce y-offset for handles of the scatter plot\n        if scatteryoffsets is None:\n            self._scatteryoffsets = np.array([3. / 8., 4. / 8., 2.5 / 8.])\n        else:\n            self._scatteryoffsets = np.asarray(scatteryoffsets)\n        reps = self.scatterpoints // len(self._scatteryoffsets) + 1\n        self._scatteryoffsets = np.tile(self._scatteryoffsets,\n                                        reps)[:self.scatterpoints]\n\n        # _legend_box is a VPacker instance that contains all\n        # legend items and will be initialized from _init_legend_box()\n        # method.\n        self._legend_box = None\n\n        if isinstance(parent, Axes):\n            self.isaxes = True\n            self.axes = parent\n            self.set_figure(parent.figure)\n        elif isinstance(parent, FigureBase):\n            self.isaxes = False\n            self.set_figure(parent)\n        else:\n            raise TypeError(\n                \"Legend needs either Axes or FigureBase as parent\"\n            )\n        self.parent = parent\n\n        self._loc_used_default = loc is None\n        if loc is None:\n            loc = mpl.rcParams[\"legend.loc\"]\n            if not self.isaxes and loc in [0, 'best']:\n                loc = 'upper right'\n        if isinstance(loc, str):\n            loc = _api.check_getitem(self.codes, loc=loc)\n        if not self.isaxes and loc == 0:\n            raise ValueError(\n                \"Automatic legend placement (loc='best') not implemented for \"\n                \"figure legend\")\n\n        self._mode = mode\n        self.set_bbox_to_anchor(bbox_to_anchor, bbox_transform)\n\n        # We use FancyBboxPatch to draw a legend frame. The location\n        # and size of the box will be updated during the drawing time.\n\n        if facecolor is None:\n            facecolor = mpl.rcParams[\"legend.facecolor\"]\n        if facecolor == 'inherit':\n            facecolor = mpl.rcParams[\"axes.facecolor\"]\n\n        if edgecolor is None:\n            edgecolor = mpl.rcParams[\"legend.edgecolor\"]\n        if edgecolor == 'inherit':\n            edgecolor = mpl.rcParams[\"axes.edgecolor\"]\n\n        if fancybox is None:\n            fancybox = mpl.rcParams[\"legend.fancybox\"]\n\n        self.legendPatch = FancyBboxPatch(\n            xy=(0, 0), width=1, height=1,\n            facecolor=facecolor, edgecolor=edgecolor,\n            # If shadow is used, default to alpha=1 (#8943).\n            alpha=(framealpha if framealpha is not None\n                   else 1 if shadow\n                   else mpl.rcParams[\"legend.framealpha\"]),\n            # The width and height of the legendPatch will be set (in draw())\n            # to the length that includes the padding. Thus we set pad=0 here.\n            boxstyle=(\"round,pad=0,rounding_size=0.2\" if fancybox\n                      else \"square,pad=0\"),\n            mutation_scale=self._fontsize,\n            snap=True,\n            visible=(frameon if frameon is not None\n                     else mpl.rcParams[\"legend.frameon\"])\n        )\n        self._set_artist_props(self.legendPatch)\n\n        _api.check_in_list([\"center\", \"left\", \"right\"], alignment=alignment)\n        self._alignment = alignment\n\n        # init with null renderer\n        self._init_legend_box(handles, labels, markerfirst)\n\n        tmp = self._loc_used_default\n        self._set_loc(loc)\n        self._loc_used_default = tmp  # ignore changes done by _set_loc\n\n        # figure out title font properties:\n        if title_fontsize is not None and title_fontproperties is not None:\n            raise ValueError(\n                \"title_fontsize and title_fontproperties can't be specified \"\n                \"at the same time. Only use one of them. \")\n        title_prop_fp = FontProperties._from_any(title_fontproperties)\n        if isinstance(title_fontproperties, dict):\n            if \"size\" not in title_fontproperties:\n                title_fontsize = mpl.rcParams[\"legend.title_fontsize\"]\n                title_prop_fp.set_size(title_fontsize)\n        elif title_fontsize is not None:\n            title_prop_fp.set_size(title_fontsize)\n        elif not isinstance(title_fontproperties, FontProperties):\n            title_fontsize = mpl.rcParams[\"legend.title_fontsize\"]\n            title_prop_fp.set_size(title_fontsize)\n\n        self.set_title(title, prop=title_prop_fp)\n        self._draggable = None\n\n        # set the text color\n\n        color_getters = {  # getter function depends on line or patch\n            'linecolor':       ['get_color',           'get_facecolor'],\n            'markerfacecolor': ['get_markerfacecolor', 'get_facecolor'],\n            'mfc':             ['get_markerfacecolor', 'get_facecolor'],\n            'markeredgecolor': ['get_markeredgecolor', 'get_edgecolor'],\n            'mec':             ['get_markeredgecolor', 'get_edgecolor'],\n(B) \n        handles : list of `.Artist`\n            A list of Artists (lines, patches) to be added to the legend.\n\n        labels : list of str\n            A list of labels to show next to the artists. The length of handles\n            and labels should be the same. If they are not, they are truncated\n            to the smaller of both lengths.\n\n        Other Parameters\n        ----------------\n        %(_legend_kw_doc)s\n\n        Notes\n        -----\n        Users can specify any arbitrary location for the legend using the\n        *bbox_to_anchor* keyword argument. *bbox_to_anchor* can be a\n        `.BboxBase` (or derived there from) or a tuple of 2 or 4 floats.\n        See `set_bbox_to_anchor` for more detail.\n\n        The legend location can be specified by setting *loc* with a tuple of\n        2 floats, which is interpreted as the lower-left corner of the legend\n        in the normalized axes coordinate.\n        \"\"\"\n        # local import only to avoid circularity\n        from matplotlib.axes import Axes\n        from matplotlib.figure import FigureBase\n\n        super().__init__()\n\n        if prop is None:\n            if fontsize is not None:\n                self.prop = FontProperties(size=fontsize)\n            else:\n                self.prop = FontProperties(\n                    size=mpl.rcParams[\"legend.fontsize\"])\n        else:\n            self.prop = FontProperties._from_any(prop)\n            if isinstance(prop, dict) and \"size\" not in prop:\n                self.prop.set_size(mpl.rcParams[\"legend.fontsize\"])\n\n        self._fontsize = self.prop.get_size_in_points()\n\n        self.texts = []\n        self.legendHandles = []\n        self._legend_title_box = None\n\n        #: A dictionary with the extra handler mappings for this Legend\n        #: instance.\n        self._custom_handler_map = handler_map\n\n        def val_or_rc(val, rc_name):\n            return val if val is not None else mpl.rcParams[rc_name]\n\n        self.numpoints = val_or_rc(numpoints, 'legend.numpoints')\n        self.markerscale = val_or_rc(markerscale, 'legend.markerscale')\n        self.scatterpoints = val_or_rc(scatterpoints, 'legend.scatterpoints')\n        self.borderpad = val_or_rc(borderpad, 'legend.borderpad')\n        self.labelspacing = val_or_rc(labelspacing, 'legend.labelspacing')\n        self.handlelength = val_or_rc(handlelength, 'legend.handlelength')\n        self.handleheight = val_or_rc(handleheight, 'legend.handleheight')\n        self.handletextpad = val_or_rc(handletextpad, 'legend.handletextpad')\n        self.borderaxespad = val_or_rc(borderaxespad, 'legend.borderaxespad')\n        self.columnspacing = val_or_rc(columnspacing, 'legend.columnspacing')\n        self.shadow = val_or_rc(shadow, 'legend.shadow')\n        # trim handles and labels if illegal label...\n        _lab, _hand = [], []\n        for label, handle in zip(labels, handles):\n            if isinstance(label, str) and label.startswith('_'):\n                _api.warn_external(f\"The label {label!r} of {handle!r} starts \"\n                                   \"with '_'. It is thus excluded from the \"\n                                   \"legend.\")\n            else:\n                _lab.append(label)\n                _hand.append(handle)\n        labels, handles = _lab, _hand\n\n        handles = list(handles)\n        if len(handles) < 2:\n            ncols = 1\n        self._ncols = ncols if ncols != 1 else ncol\n\n        if self.numpoints <= 0:\n            raise ValueError(\"numpoints must be > 0; it was %d\" % numpoints)\n\n        # introduce y-offset for handles of the scatter plot\n        if scatteryoffsets is None:\n            self._scatteryoffsets = np.array([3. / 8., 4. / 8., 2.5 / 8.])\n        else:\n            self._scatteryoffsets = np.asarray(scatteryoffsets)\n        reps = self.scatterpoints // len(self._scatteryoffsets) + 1\n        self._scatteryoffsets = np.tile(self._scatteryoffsets,\n                                        reps)[:self.scatterpoints]\n\n        # _legend_box is a VPacker instance that contains all\n        # legend items and will be initialized from _init_legend_box()\n        # method.\n        self._legend_box = None\n\n        if isinstance(parent, Axes):\n            self.isaxes = True\n            self.axes = parent\n            self.set_figure(parent.figure)\n        elif isinstance(parent, FigureBase):\n            self.isaxes = False\n            self.set_figure(parent)\n        else:\n            raise TypeError(\n                \"Legend needs either Axes or FigureBase as parent\"\n            )\n        self.parent = parent\n\n        self._loc_used_default = loc is None\n        if loc is None:\n            loc = mpl.rcParams[\"legend.loc\"]\n            if not self.isaxes and loc in [0, 'best']:\n                loc = 'upper right'\n        if isinstance(loc, str):\n            loc = _api.check_getitem(self.codes, loc=loc)\n        if not self.isaxes and loc == 0:\n            raise ValueError(\n                \"Automatic legend placement (loc='best') not implemented for \"\n                \"figure legend\")\n\n        self._mode = mode\n        self.set_bbox_to_anchor(bbox_to_anchor, bbox_transform)\n\n        # We use FancyBboxPatch to draw a legend frame. The location\n        # and size of the box will be updated during the drawing time.\n\n        if facecolor is None:\n            facecolor = mpl.rcParams[\"legend.facecolor\"]\n        if facecolor == 'inherit':\n            facecolor = mpl.rcParams[\"axes.facecolor\"]\n\n        if edgecolor is None:\n            edgecolor = mpl.rcParams[\"legend.edgecolor\"]\n        if edgecolor == 'inherit':\n            edgecolor = mpl.rcParams[\"axes.edgecolor\"]\n\n        if fancybox is None:\n            fancybox = mpl.rcParams[\"legend.fancybox\"]\n\n        self.legendPatch = FancyBboxPatch(\n            xy=(0, 0), width=1, height=1,\n            facecolor=facecolor, edgecolor=edgecolor,\n            # If shadow is used, default to alpha=1 (#8943).\n            alpha=(framealpha if framealpha is not None\n                   else 1 if shadow\n                   else mpl.rcParams[\"legend.framealpha\"]),\n            # The width and height of the legendPatch will be set (in draw())\n            # to the length that includes the padding. Thus we set pad=0 here.\n            boxstyle=(\"round,pad=0,rounding_size=0.2\" if fancybox\n                      else \"square,pad=0\"),\n            mutation_scale=self._fontsize,\n            snap=True,\n            visible=(frameon if frameon is not None\n                     else mpl.rcParams[\"legend.frameon\"])\n        )\n        self._set_artist_props(self.legendPatch)\n\n        _api.check_in_list([\"center\", \"left\", \"right\"], alignment=alignment)\n        self._alignment = alignment\n\n        # init with null renderer\n        self._init_legend_box(handles, labels, markerfirst)\n\n        tmp = self._loc_used_default\n        self._set_loc(loc)\n        self._loc_used_default = tmp  # ignore changes done by _set_loc\n\n        # figure out title font properties:\n        if title_fontsize is not None and title_fontproperties is not None:\n            raise ValueError(\n                \"title_fontsize and title_fontproperties can't be specified \"\n                \"at the same time. Only use one of them. \")\n        title_prop_fp = FontProperties._from_any(title_fontproperties)\n        if isinstance(title_fontproperties, dict):\n            if \"size\" not in title_fontproperties:\n                title_fontsize = mpl.rcParams[\"legend.title_fontsize\"]\n                title_prop_fp.set_size(title_fontsize)\n        elif title_fontsize is not None:\n            title_prop_fp.set_size(title_fontsize)\n        elif not isinstance(title_fontproperties, FontProperties):\n            title_fontsize = mpl.rcParams[\"legend.title_fontsize\"]\n            title_prop_fp.set_size(title_fontsize)\n\n        self.set_title(title, prop=title_prop_fp)\n        self._draggable = None\n\n        # set the text color\n\n        color_getters = {  # getter function depends on line or patch\n            'linecolor':       ['get_color',           'get_facecolor'],\n            'markerfacecolor': ['get_markerfacecolor', 'get_facecolor'],\n            'mfc':             ['get_markerfacecolor', 'get_facecolor'],\n            'markeredgecolor': ['get_markeredgecolor', 'get_edgecolor'],\n            'mec':             ['get_markeredgecolor', 'get_edgecolor'],\n        }\n        if labelcolor is None:\n            if mpl.rcParams['legend.labelcolor'] is not None:\n                labelcolor = mpl.rcParams['legend.labelcolor']\n            else:\n                labelcolor = mpl.rcParams['text.color']\n        if isinstance(labelcolor, str) and labelcolor in color_getters:\n            getter_names = color_getters[labelcolor]\n            for handle, text in zip(self.legendHandles, self.texts):\n                for getter_name in getter_names:\n                    try:\n                        color = getattr(handle, getter_name)()\n                        text.set_color(color)\n                        break\n                    except AttributeError:\n                        pass\n        elif isinstance(labelcolor, str) and labelcolor == 'none':\n            for text in self.texts:\n                text.set_color(labelcolor)\n        elif np.iterable(labelcolor):\n            for text, color in zip(self.texts,\n                                   itertools.cycle(\n                                       colors.to_rgba_array(labelcolor))):\n                text.set_color(color)\n        else:\n            raise ValueError(f\"Invalid labelcolor: {labelcolor!r}\")\n\n    def _set_artist_props(self, a):\n        \"\"\"\n        Set the boilerplate props for artists added to axes.\n        \"\"\"\n        a.set_figure(self.figure)\n        if self.isaxes:\n            # a.set_axes(self.axes)\n            a.axes = self.axes\n\n        a.set_transform(self.get_transform())\n\n    def _set_loc(self, loc):\n        # find_offset function will be provided to _legend_box and\n        # _legend_box will draw itself at the location of the return\n        # value of the find_offset.\n        self._loc_used_default = False\n        self._loc_real = loc\n        self.stale = True\n        self._legend_box.set_offset(self._findoffset)\n\n    def set_ncols(self, ncols):\n        \"\"\"Set the number of columns.\"\"\"\n        self._ncols = ncols\n\n    def _get_loc(self):\n        return self._loc_real\n\n    _loc = property(_get_loc, _set_loc)\n\n    def _findoffset(self, width, height, xdescent, ydescent, renderer):\n        \"\"\"Helper function to locate the legend.\"\"\"\n\n        if self._loc == 0:  # \"best\".\n            x, y = self._find_best_position(width, height, renderer)\n        elif self._loc in Legend.codes.values():  # Fixed location.\n            bbox = Bbox.from_bounds(0, 0, width, height)\n            x, y = self._get_anchored_bbox(self._loc, bbox,\n                                           self.get_bbox_to_anchor(),\n                                           renderer)\n(C)     Whether to draw a shadow behind the legend.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', \\\n'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\"\"\")\n\n\nclass Legend(Artist):\n    \"\"\"\n    Place a legend on the axes at location loc.\n    \"\"\"\n\n    # 'best' is only implemented for axes legends\n    codes = {'best': 0, **AnchoredOffsetbox.codes}\n    zorder = 5\n\n    def __str__(self):\n        return \"Legend\"\n\n    @_api.make_keyword_only(\"3.6\", \"loc\")\n    @_docstring.dedent_interpd\n    def __init__(\n        self, parent, handles, labels,\n        loc=None,\n        numpoints=None,      # number of points in the legend line\n        markerscale=None,    # relative size of legend markers vs. original\n        markerfirst=True,    # left/right ordering of legend marker and label\n        scatterpoints=None,  # number of scatter points\n        scatteryoffsets=None,\n        prop=None,           # properties for the legend texts\n        fontsize=None,       # keyword to set font size directly\n        labelcolor=None,     # keyword to set the text color\n\n        # spacing & pad defined as a fraction of the font-size\n        borderpad=None,      # whitespace inside the legend border\n        labelspacing=None,   # vertical space between the legend entries\n        handlelength=None,   # length of the legend handles\n        handleheight=None,   # height of the legend handles\n        handletextpad=None,  # pad between the legend handle and text\n        borderaxespad=None,  # pad between the axes and legend border\n        columnspacing=None,  # spacing between columns\n\n        ncols=1,     # number of columns\n        mode=None,  # horizontal distribution of columns: None or \"expand\"\n\n        fancybox=None,  # True: fancy box, False: rounded box, None: rcParam\n        shadow=None,\n        title=None,           # legend title\n        title_fontsize=None,  # legend title font size\n        framealpha=None,      # set frame alpha\n        edgecolor=None,       # frame patch edgecolor\n        facecolor=None,       # frame patch facecolor\n\n        bbox_to_anchor=None,  # bbox to which the legend will be anchored\n        bbox_transform=None,  # transform for the bbox\n        frameon=None,         # draw frame\n        handler_map=None,\n        title_fontproperties=None,  # properties for the legend title\n        alignment=\"center\",       # control the alignment within the legend box\n        *,\n        ncol=1  # synonym for ncols (backward compatibility)\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        parent : `~matplotlib.axes.Axes` or `.Figure`\n            The artist that contains the legend.\n\n        handles : list of `.Artist`\n            A list of Artists (lines, patches) to be added to the legend.\n\n        labels : list of str\n            A list of labels to show next to the artists. The length of handles\n            and labels should be the same. If they are not, they are truncated\n            to the smaller of both lengths.\n\n        Other Parameters\n        ----------------\n        %(_legend_kw_doc)s\n\n        Notes\n        -----\n        Users can specify any arbitrary location for the legend using the\n        *bbox_to_anchor* keyword argument. *bbox_to_anchor* can be a\n        `.BboxBase` (or derived there from) or a tuple of 2 or 4 floats.\n        See `set_bbox_to_anchor` for more detail.\n\n        The legend location can be specified by setting *loc* with a tuple of\n        2 floats, which is interpreted as the lower-left corner of the legend\n        in the normalized axes coordinate.\n        \"\"\"\n        # local import only to avoid circularity\n        from matplotlib.axes import Axes\n        from matplotlib.figure import FigureBase\n\n        super().__init__()\n\n        if prop is None:\n            if fontsize is not None:\n                self.prop = FontProperties(size=fontsize)\n            else:\n                self.prop = FontProperties(\n                    size=mpl.rcParams[\"legend.fontsize\"])\n        else:\n            self.prop = FontProperties._from_any(prop)\n            if isinstance(prop, dict) and \"size\" not in prop:\n                self.prop.set_size(mpl.rcParams[\"legend.fontsize\"])\n\n        self._fontsize = self.prop.get_size_in_points()\n\n        self.texts = []\n        self.legendHandles = []\n        self._legend_title_box = None\n\n        #: A dictionary with the extra handler mappings for this Legend\n        #: instance.\n        self._custom_handler_map = handler_map\n\n        def val_or_rc(val, rc_name):\n            return val if val is not None else mpl.rcParams[rc_name]\n\n        self.numpoints = val_or_rc(numpoints, 'legend.numpoints')\n        self.markerscale = val_or_rc(markerscale, 'legend.markerscale')\n        self.scatterpoints = val_or_rc(scatterpoints, 'legend.scatterpoints')\n        self.borderpad = val_or_rc(borderpad, 'legend.borderpad')\n        self.labelspacing = val_or_rc(labelspacing, 'legend.labelspacing')\n        self.handlelength = val_or_rc(handlelength, 'legend.handlelength')\n        self.handleheight = val_or_rc(handleheight, 'legend.handleheight')\n        self.handletextpad = val_or_rc(handletextpad, 'legend.handletextpad')\n        self.borderaxespad = val_or_rc(borderaxespad, 'legend.borderaxespad')\n        self.columnspacing = val_or_rc(columnspacing, 'legend.columnspacing')\n        self.shadow = val_or_rc(shadow, 'legend.shadow')\n        # trim handles and labels if illegal label...\n        _lab, _hand = [], []\n        for label, handle in zip(labels, handles):\n            if isinstance(label, str) and label.startswith('_'):\n                _api.warn_external(f\"The label {label!r} of {handle!r} starts \"\n                                   \"with '_'. It is thus excluded from the \"\n                                   \"legend.\")\n            else:\n                _lab.append(label)\n                _hand.append(handle)\n        labels, handles = _lab, _hand\n\n        handles = list(handles)\n        if len(handles) < 2:\n            ncols = 1\n        self._ncols = ncols if ncols != 1 else ncol\n\n        if self.numpoints <= 0:\n            raise ValueError(\"numpoints must be > 0; it was %d\" % numpoints)\n\n        # introduce y-offset for handles of the scatter plot\n        if scatteryoffsets is None:\n            self._scatteryoffsets = np.array([3. / 8., 4. / 8., 2.5 / 8.])\n        else:\n            self._scatteryoffsets = np.asarray(scatteryoffsets)\n        reps = self.scatterpoints // len(self._scatteryoffsets) + 1\n        self._scatteryoffsets = np.tile(self._scatteryoffsets,\n                                        reps)[:self.scatterpoints]\n\n        # _legend_box is a VPacker instance that contains all\n        # legend items and will be initialized from _init_legend_box()\n        # method.\n        self._legend_box = None\n\n        if isinstance(parent, Axes):\n            self.isaxes = True\n            self.axes = parent\n            self.set_figure(parent.figure)\n        elif isinstance(parent, FigureBase):\n            self.isaxes = False\n            self.set_figure(parent)\n        else:\n            raise TypeError(\n                \"Legend needs either Axes or FigureBase as parent\"\n            )\n        self.parent = parent\n\n        self._loc_used_default = loc is None\n        if loc is None:\n            loc = mpl.rcParams[\"legend.loc\"]\n            if not self.isaxes and loc in [0, 'best']:\n                loc = 'upper right'\n        if isinstance(loc, str):\n            loc = _api.check_getitem(self.codes, loc=loc)\n        if not self.isaxes and loc == 0:\n            raise ValueError(\n                \"Automatic legend placement (loc='best') not implemented for \"\n                \"figure legend\")\n\n        self._mode = mode\n        self.set_bbox_to_anchor(bbox_to_anchor, bbox_transform)\n\n        # We use FancyBboxPatch to draw a legend frame. The location\n        # and size of the box will be updated during the drawing time.\n\n        if facecolor is None:\n            facecolor = mpl.rcParams[\"legend.facecolor\"]\n(D)         _lab, _hand = [], []\n        for label, handle in zip(labels, handles):\n            if isinstance(label, str) and label.startswith('_'):\n                _api.warn_external(f\"The label {label!r} of {handle!r} starts \"\n                                   \"with '_'. It is thus excluded from the \"\n                                   \"legend.\")\n            else:\n                _lab.append(label)\n                _hand.append(handle)\n        labels, handles = _lab, _hand\n\n        handles = list(handles)\n        if len(handles) < 2:\n            ncols = 1\n        self._ncols = ncols if ncols != 1 else ncol\n\n        if self.numpoints <= 0:\n            raise ValueError(\"numpoints must be > 0; it was %d\" % numpoints)\n\n        # introduce y-offset for handles of the scatter plot\n        if scatteryoffsets is None:\n            self._scatteryoffsets = np.array([3. / 8., 4. / 8., 2.5 / 8.])\n        else:\n            self._scatteryoffsets = np.asarray(scatteryoffsets)\n        reps = self.scatterpoints // len(self._scatteryoffsets) + 1\n        self._scatteryoffsets = np.tile(self._scatteryoffsets,\n                                        reps)[:self.scatterpoints]\n\n        # _legend_box is a VPacker instance that contains all\n        # legend items and will be initialized from _init_legend_box()\n        # method.\n        self._legend_box = None\n\n        if isinstance(parent, Axes):\n            self.isaxes = True\n            self.axes = parent\n            self.set_figure(parent.figure)\n        elif isinstance(parent, FigureBase):\n            self.isaxes = False\n            self.set_figure(parent)\n        else:\n            raise TypeError(\n                \"Legend needs either Axes or FigureBase as parent\"\n            )\n        self.parent = parent\n\n        self._loc_used_default = loc is None\n        if loc is None:\n            loc = mpl.rcParams[\"legend.loc\"]\n            if not self.isaxes and loc in [0, 'best']:\n                loc = 'upper right'\n        if isinstance(loc, str):\n            loc = _api.check_getitem(self.codes, loc=loc)\n        if not self.isaxes and loc == 0:\n            raise ValueError(\n                \"Automatic legend placement (loc='best') not implemented for \"\n                \"figure legend\")\n\n        self._mode = mode\n        self.set_bbox_to_anchor(bbox_to_anchor, bbox_transform)\n\n        # We use FancyBboxPatch to draw a legend frame. The location\n        # and size of the box will be updated during the drawing time.\n\n        if facecolor is None:\n            facecolor = mpl.rcParams[\"legend.facecolor\"]\n        if facecolor == 'inherit':\n            facecolor = mpl.rcParams[\"axes.facecolor\"]\n\n        if edgecolor is None:\n            edgecolor = mpl.rcParams[\"legend.edgecolor\"]\n        if edgecolor == 'inherit':\n            edgecolor = mpl.rcParams[\"axes.edgecolor\"]\n\n        if fancybox is None:\n            fancybox = mpl.rcParams[\"legend.fancybox\"]\n\n        self.legendPatch = FancyBboxPatch(\n            xy=(0, 0), width=1, height=1,\n            facecolor=facecolor, edgecolor=edgecolor,\n            # If shadow is used, default to alpha=1 (#8943).\n            alpha=(framealpha if framealpha is not None\n                   else 1 if shadow\n                   else mpl.rcParams[\"legend.framealpha\"]),\n            # The width and height of the legendPatch will be set (in draw())\n            # to the length that includes the padding. Thus we set pad=0 here.\n            boxstyle=(\"round,pad=0,rounding_size=0.2\" if fancybox\n                      else \"square,pad=0\"),\n            mutation_scale=self._fontsize,\n            snap=True,\n            visible=(frameon if frameon is not None\n                     else mpl.rcParams[\"legend.frameon\"])\n        )\n        self._set_artist_props(self.legendPatch)\n\n        _api.check_in_list([\"center\", \"left\", \"right\"], alignment=alignment)\n        self._alignment = alignment\n\n        # init with null renderer\n        self._init_legend_box(handles, labels, markerfirst)\n\n        tmp = self._loc_used_default\n        self._set_loc(loc)\n        self._loc_used_default = tmp  # ignore changes done by _set_loc\n\n        # figure out title font properties:\n        if title_fontsize is not None and title_fontproperties is not None:\n            raise ValueError(\n                \"title_fontsize and title_fontproperties can't be specified \"\n                \"at the same time. Only use one of them. \")\n        title_prop_fp = FontProperties._from_any(title_fontproperties)\n        if isinstance(title_fontproperties, dict):\n            if \"size\" not in title_fontproperties:\n                title_fontsize = mpl.rcParams[\"legend.title_fontsize\"]\n                title_prop_fp.set_size(title_fontsize)\n        elif title_fontsize is not None:\n            title_prop_fp.set_size(title_fontsize)\n        elif not isinstance(title_fontproperties, FontProperties):\n            title_fontsize = mpl.rcParams[\"legend.title_fontsize\"]\n            title_prop_fp.set_size(title_fontsize)\n\n        self.set_title(title, prop=title_prop_fp)\n        self._draggable = None\n\n        # set the text color\n\n        color_getters = {  # getter function depends on line or patch\n            'linecolor':       ['get_color',           'get_facecolor'],\n            'markerfacecolor': ['get_markerfacecolor', 'get_facecolor'],\n            'mfc':             ['get_markerfacecolor', 'get_facecolor'],\n            'markeredgecolor': ['get_markeredgecolor', 'get_edgecolor'],\n            'mec':             ['get_markeredgecolor', 'get_edgecolor'],\n        }\n        if labelcolor is None:\n            if mpl.rcParams['legend.labelcolor'] is not None:\n                labelcolor = mpl.rcParams['legend.labelcolor']\n            else:\n                labelcolor = mpl.rcParams['text.color']\n        if isinstance(labelcolor, str) and labelcolor in color_getters:\n            getter_names = color_getters[labelcolor]\n            for handle, text in zip(self.legendHandles, self.texts):\n                for getter_name in getter_names:\n                    try:\n                        color = getattr(handle, getter_name)()\n                        text.set_color(color)\n                        break\n                    except AttributeError:\n                        pass\n        elif isinstance(labelcolor, str) and labelcolor == 'none':\n            for text in self.texts:\n                text.set_color(labelcolor)\n        elif np.iterable(labelcolor):\n            for text, color in zip(self.texts,\n                                   itertools.cycle(\n                                       colors.to_rgba_array(labelcolor))):\n                text.set_color(color)\n        else:\n            raise ValueError(f\"Invalid labelcolor: {labelcolor!r}\")\n\n    def _set_artist_props(self, a):\n        \"\"\"\n        Set the boilerplate props for artists added to axes.\n        \"\"\"\n        a.set_figure(self.figure)\n        if self.isaxes:\n            # a.set_axes(self.axes)\n            a.axes = self.axes\n\n        a.set_transform(self.get_transform())\n\n    def _set_loc(self, loc):\n        # find_offset function will be provided to _legend_box and\n        # _legend_box will draw itself at the location of the return\n        # value of the find_offset.\n        self._loc_used_default = False\n        self._loc_real = loc\n        self.stale = True\n        self._legend_box.set_offset(self._findoffset)\n\n    def set_ncols(self, ncols):\n        \"\"\"Set the number of columns.\"\"\"\n        self._ncols = ncols\n\n    def _get_loc(self):\n        return self._loc_real\n\n    _loc = property(_get_loc, _set_loc)\n\n    def _findoffset(self, width, height, xdescent, ydescent, renderer):\n        \"\"\"Helper function to locate the legend.\"\"\"\n\n        if self._loc == 0:  # \"best\".\n            x, y = self._find_best_position(width, height, renderer)\n        elif self._loc in Legend.codes.values():  # Fixed location.\n            bbox = Bbox.from_bounds(0, 0, width, height)\n            x, y = self._get_anchored_bbox(self._loc, bbox,\n                                           self.get_bbox_to_anchor(),\n                                           renderer)\n        else:  # Axes or figure coordinates.\n            fx, fy = self._loc\n            bbox = self.get_bbox_to_anchor()\n            x, y = bbox.x0 + bbox.width * fx, bbox.y0 + bbox.height * fy\n\n        return x + xdescent, y + ydescent\n\n    @allow_rasterization\n    def draw(self, renderer):\n        # docstring inherited\n        if not self.get_visible():\n            return\n\n        renderer.open_group('legend', gid=self.get_gid())\n\n        fontsize = renderer.points_to_pixels(self._fontsize)\n\n        # if mode == fill, set the width of the legend_box to the\n        # width of the parent (minus pads)\n        if self._mode in [\"expand\"]:\n            pad = 2 * (self.borderaxespad + self.borderpad) * fontsize\n            self._legend_box.set_width(self.get_bbox_to_anchor().width - pad)\n\n        # update the location and size of the legend. This needs to\n        # be done in any case to clip the figure right.\n        bbox = self._legend_box.get_window_extent(renderer)\n        self.legendPatch.set_bounds(bbox.bounds)\n        self.legendPatch.set_mutation_scale(fontsize)\n\n        if self.shadow:\n            Shadow(self.legendPatch, 2, -2).draw(renderer)\n\n        self.legendPatch.draw(renderer)\n        self._legend_box.draw(renderer)\n\n        renderer.close_group('legend')\n        self.stale = False\n\n    # _default_handler_map defines the default mapping between plot\n    # elements and the legend handlers.\n\n    _default_handler_map = {\n        StemContainer: legend_handler.HandlerStem(),\n        ErrorbarContainer: legend_handler.HandlerErrorbar(),\n        Line2D: legend_handler.HandlerLine2D(),\n        Patch: legend_handler.HandlerPatch(),\n        StepPatch: legend_handler.HandlerStepPatch(),\n        LineCollection: legend_handler.HandlerLineCollection(),\n        RegularPolyCollection: legend_handler.HandlerRegularPolyCollection(),\n        CircleCollection: legend_handler.HandlerCircleCollection(),\n        BarContainer: legend_handler.HandlerPatch(\n            update_func=legend_handler.update_from_first_child),\n        tuple: legend_handler.HandlerTuple(),\n        PathCollection: legend_handler.HandlerPathCollection(),\n        PolyCollection: legend_handler.HandlerPolyCollection()\n        }\n\n    # (get|set|update)_default_handler_maps are public interfaces to\n    # modify the default handler map.\n\n    @classmethod\n    def get_default_handler_map(cls):\n        \"\"\"Return the global default handler map, shared by all legends.\"\"\"\n        return cls._default_handler_map\n\n    @classmethod", "answer": "A"}
{"uuid": "c5da69e7-8a55-42ba-aaa6-7174bafa3d2b", "issue_statement": "[Bug]: Text label with empty line causes a \"TypeError: cannot unpack non-iterable NoneType object\" in PostScript backend\n### Bug summary\n\nWhen saving a figure with the PostScript backend, a\r\n> TypeError: cannot unpack non-iterable NoneType object\r\n\r\nhappens if the figure contains a multi-line text label with an empty line (see example).\n\n### Code for reproduction\n\n```python\nfrom matplotlib.figure import Figure\r\n\r\nfigure = Figure()\r\nax = figure.add_subplot(111)\r\n# ax.set_title('\\nLower title')  # this would cause an error as well\r\nax.annotate(text='\\nLower label', xy=(0, 0))\r\nfigure.savefig('figure.eps')\n```\n\n\n### Actual outcome\n\n$ ./venv/Scripts/python save_ps.py\r\nTraceback (most recent call last):\r\n  File \"C:\\temp\\matplotlib_save_ps\\save_ps.py\", line 7, in <module>\r\n    figure.savefig('figure.eps')\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\figure.py\", line 3272, in savefig\r\n    self.canvas.print_figure(fname, **kwargs)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\backend_bases.py\", line 2338, in print_figure\r\n    result = print_method(\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\backend_bases.py\", line 2204, in <lambda>\r\n    print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\_api\\deprecation.py\", line 410, in wrapper\r\n    return func(*inner_args, **inner_kwargs)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\backends\\backend_ps.py\", line 869, in _print_ps\r\n    printer(fmt, outfile, dpi=dpi, dsc_comments=dsc_comments,\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\backends\\backend_ps.py\", line 927, in _print_figure\r\n    self.figure.draw(renderer)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\artist.py\", line 74, in draw_wrapper\r\n    result = draw(artist, renderer, *args, **kwargs)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\artist.py\", line 51, in draw_wrapper\r\n    return draw(artist, renderer)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\figure.py\", line 3069, in draw\r\n    mimage._draw_list_compositing_images(\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\image.py\", line 131, in _draw_list_compositing_images\r\n    a.draw(renderer)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\artist.py\", line 51, in draw_wrapper\r\n    return draw(artist, renderer)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\axes\\_base.py\", line 3106, in draw\r\n    mimage._draw_list_compositing_images(\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\image.py\", line 131, in _draw_list_compositing_images\r\n    a.draw(renderer)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\artist.py\", line 51, in draw_wrapper\r\n    return draw(artist, renderer)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\text.py\", line 1995, in draw\r\n    Text.draw(self, renderer)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\artist.py\", line 51, in draw_wrapper\r\n    return draw(artist, renderer)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\text.py\", line 736, in draw\r\n    textrenderer.draw_text(gc, x, y, clean_line,\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\backends\\backend_ps.py\", line 248, in wrapper\r\n    return meth(self, *args, **kwargs)\r\n  File \"C:\\temp\\matplotlib_save_ps\\venv\\lib\\site-packages\\matplotlib\\backends\\backend_ps.py\", line 673, in draw_text\r\n    for ps_name, xs_names in stream:\r\nTypeError: cannot unpack non-iterable NoneType object\r\n\n\n### Expected outcome\n\nThe figure can be saved as `figure.eps` without error.\n\n### Additional information\n\n- seems to happen if a text label or title contains a linebreak with an empty line\r\n- works without error for other backends such as PNG, PDF, SVG, Qt\r\n- works with matplotlib<=3.5.3\r\n- adding `if curr_stream:` before line 669 of `backend_ps.py` seems to fix the bug \n\n### Operating system\n\nWindows\n\n### Matplotlib Version\n\n3.6.0\n\n### Matplotlib Backend\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Jupyter version\n\n_No response_\n\n### Installation\n\npip", "choices": "(A)                 curr_stream[1].append(\n                    (item.x, item.ft_object.get_glyph_name(item.glyph_idx))\n                )\n            # append the last entry\n            stream.append(curr_stream)\n\n        self.set_color(*gc.get_rgb())\n\n        for ps_name, xs_names in stream:\n(B)                         stream.append(curr_stream)\n                    prev_font = item.ft_object\n                    curr_stream = [ps_name, []]\n                curr_stream[1].append(\n                    (item.x, item.ft_object.get_glyph_name(item.glyph_idx))\n                )\n            # append the last entry\n            stream.append(curr_stream)\n\n(C)             # append the last entry\n            stream.append(curr_stream)\n\n        self.set_color(*gc.get_rgb())\n\n        for ps_name, xs_names in stream:\n            self.set_font(ps_name, prop.get_size_in_points(), False)\n            thetext = \"\\n\".join(f\"{x:g} 0 m /{name:s} glyphshow\"\n                                for x, name in xs_names)\n(D) \n        self.set_color(*gc.get_rgb())\n\n        for ps_name, xs_names in stream:\n            self.set_font(ps_name, prop.get_size_in_points(), False)\n            thetext = \"\\n\".join(f\"{x:g} 0 m /{name:s} glyphshow\"\n                                for x, name in xs_names)\n            self._pswriter.write(f\"\"\"\\\ngsave", "answer": "A"}
{"uuid": "d1687cc2-cdac-4567-8606-842e0f71fc81", "issue_statement": "[Bug]: Constrained layout UserWarning even when False\n### Bug summary\r\n\r\nWhen using layout settings such as `plt.subplots_adjust` or `bbox_inches='tight`, a UserWarning is produced due to incompatibility with constrained_layout, even if constrained_layout = False. This was not the case in previous versions.\r\n\r\n### Code for reproduction\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\na = np.linspace(0,2*np.pi,100)\r\nb = np.sin(a)\r\nc = np.cos(a)\r\nfig,ax = plt.subplots(1,2,figsize=(8,2),constrained_layout=False)\r\nax[0].plot(a,b)\r\nax[1].plot(a,c)\r\nplt.subplots_adjust(wspace=0)\r\n```\r\n\r\n\r\n### Actual outcome\r\n\r\nThe plot works fine but the warning is generated\r\n\r\n`/var/folders/ss/pfgdfm2x7_s4cyw2v0b_t7q80000gn/T/ipykernel_76923/4170965423.py:7: UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.\r\n  plt.subplots_adjust(wspace=0)`\r\n\r\n### Expected outcome\r\n\r\nno warning\r\n\r\n### Additional information\r\n\r\nWarning disappears when constrained_layout=False is removed\r\n\r\n### Operating system\r\n\r\nOS/X\r\n\r\n### Matplotlib Version\r\n\r\n3.6.0\r\n\r\n### Matplotlib Backend\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Jupyter version\r\n\r\n_No response_\r\n\r\n### Installation\r\n\r\nconda", "choices": "(A)             self.set_layout_engine(layout='constrained')\n            if isinstance(constrained_layout, dict):\n                self.get_layout_engine().set(**constrained_layout)\n        else:\n            # everything is None, so use default:\n            self.set_layout_engine(layout=layout)\n\n        self._fig_callbacks = cbook.CallbackRegistry(signals=[\"dpi_changed\"])\n        # Callbacks traditionally associated with the canvas (and exposed with\n        # a proxy property), but that actually need to be on the figure for\n        # pickling.\n        self._canvas_callbacks = cbook.CallbackRegistry(\n(B)         else:\n            # everything is None, so use default:\n            self.set_layout_engine(layout=layout)\n\n        self._fig_callbacks = cbook.CallbackRegistry(signals=[\"dpi_changed\"])\n        # Callbacks traditionally associated with the canvas (and exposed with\n        # a proxy property), but that actually need to be on the figure for\n        # pickling.\n        self._canvas_callbacks = cbook.CallbackRegistry(\n            signals=FigureCanvasBase.events)\n        self._button_pick_id = self._canvas_callbacks._connect_picklable(\n            'button_press_event', self.pick)\n(C)                     \"'constrained_layout' cannot be used together. Please use \"\n                    \"'layout' parameter\")\n            self.set_layout_engine(layout='tight')\n            if isinstance(tight_layout, dict):\n                self.get_layout_engine().set(**tight_layout)\n        elif constrained_layout is not None:\n            self.set_layout_engine(layout='constrained')\n            if isinstance(constrained_layout, dict):\n                self.get_layout_engine().set(**constrained_layout)\n        else:\n            # everything is None, so use default:\n            self.set_layout_engine(layout=layout)\n(D)             if isinstance(tight_layout, dict):\n                self.get_layout_engine().set(**tight_layout)\n        elif constrained_layout is not None:\n            self.set_layout_engine(layout='constrained')\n            if isinstance(constrained_layout, dict):\n                self.get_layout_engine().set(**constrained_layout)\n        else:\n            # everything is None, so use default:\n            self.set_layout_engine(layout=layout)\n\n        self._fig_callbacks = cbook.CallbackRegistry(signals=[\"dpi_changed\"])\n        # Callbacks traditionally associated with the canvas (and exposed with", "answer": "D"}
{"uuid": "2d6c8a29-2a18-4374-98e3-4aa2fe30160e", "issue_statement": "[Bug]: ax.bar raises for all-nan data on matplotlib 3.6.1 \n### Bug summary\n\n`ax.bar` raises an exception in 3.6.1 when passed only nan data. This irrevocably breaks seaborn's histogram function (which draws and then removes a \"phantom\" bar to trip the color cycle).\n\n### Code for reproduction\n\n```python\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nf, ax = plt.subplots()\r\nax.bar([np.nan], [np.nan])\n```\n\n\n### Actual outcome\n\n```python-traceback\r\n---------------------------------------------------------------------------\r\nStopIteration                             Traceback (most recent call last)\r\nCell In [1], line 4\r\n      2 import matplotlib.pyplot as plt\r\n      3 f, ax = plt.subplots()\r\n----> 4 ax.bar([np.nan], [np.nan])[0].get_x()\r\n\r\nFile ~/miniconda/envs/py310/lib/python3.10/site-packages/matplotlib/__init__.py:1423, in _preprocess_data.<locals>.inner(ax, data, *args, **kwargs)\r\n   1420 @functools.wraps(func)\r\n   1421 def inner(ax, *args, data=None, **kwargs):\r\n   1422     if data is None:\r\n-> 1423         return func(ax, *map(sanitize_sequence, args), **kwargs)\r\n   1425     bound = new_sig.bind(ax, *args, **kwargs)\r\n   1426     auto_label = (bound.arguments.get(label_namer)\r\n   1427                   or bound.kwargs.get(label_namer))\r\n\r\nFile ~/miniconda/envs/py310/lib/python3.10/site-packages/matplotlib/axes/_axes.py:2373, in Axes.bar(self, x, height, width, bottom, align, **kwargs)\r\n   2371 x0 = x\r\n   2372 x = np.asarray(self.convert_xunits(x))\r\n-> 2373 width = self._convert_dx(width, x0, x, self.convert_xunits)\r\n   2374 if xerr is not None:\r\n   2375     xerr = self._convert_dx(xerr, x0, x, self.convert_xunits)\r\n\r\nFile ~/miniconda/envs/py310/lib/python3.10/site-packages/matplotlib/axes/_axes.py:2182, in Axes._convert_dx(dx, x0, xconv, convert)\r\n   2170 try:\r\n   2171     # attempt to add the width to x0; this works for\r\n   2172     # datetime+timedelta, for instance\r\n   (...)\r\n   2179     # removes the units from unit packages like `pint` that\r\n   2180     # wrap numpy arrays.\r\n   2181     try:\r\n-> 2182         x0 = cbook._safe_first_finite(x0)\r\n   2183     except (TypeError, IndexError, KeyError):\r\n   2184         pass\r\n\r\nFile ~/miniconda/envs/py310/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:1749, in _safe_first_finite(obj, skip_nonfinite)\r\n   1746     raise RuntimeError(\"matplotlib does not \"\r\n   1747                        \"support generators as input\")\r\n   1748 else:\r\n-> 1749     return next(val for val in obj if safe_isfinite(val))\r\n\r\nStopIteration: \r\n```\n\n### Expected outcome\n\nOn 3.6.0 this returns a `BarCollection` with one Rectangle, having `nan` for `x` and `height`.\n\n### Additional information\n\nI assume it's related to this bullet in the release notes:\r\n\r\n- Fix barplot being empty when first element is NaN\r\n\r\nBut I don't know the context for it to investigate further (could these link to PRs?)\r\n\r\nFurther debugging:\r\n\r\n```python\r\nax.bar([np.nan], [0])  # Raises\r\nax.bar([0], [np.nan])  # Works\r\n```\r\n\r\nSo it's about the x position specifically.\n\n### Operating system\n\nMacos\n\n### Matplotlib Version\n\n3.6.1\n\n### Matplotlib Backend\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Jupyter version\n\n_No response_\n\n### Installation\n\npip", "choices": "(A)             if not np.iterable(dx):\n                dx = [dx]\n                delist = True\n            dx = [convert(x0 + ddx) - x for ddx in dx]\n            if delist:\n                dx = dx[0]\n        except (ValueError, TypeError, AttributeError):\n            # if the above fails (for any reason) just fallback to what\n            # we do by default and convert dx by itself.\n            dx = convert(dx)\n        return dx\n\n    @_preprocess_data()\n    @_docstring.dedent_interpd\n    def bar(self, x, height, width=0.8, bottom=None, *, align=\"center\",\n            **kwargs):\n        r\"\"\"\n        Make a bar plot.\n\n(B)                 x = cbook._safe_first_finite(xconv)\n            except (TypeError, IndexError, KeyError):\n                x = xconv\n\n            delist = False\n            if not np.iterable(dx):\n                dx = [dx]\n                delist = True\n            dx = [convert(x0 + ddx) - x for ddx in dx]\n            if delist:\n                dx = dx[0]\n        except (ValueError, TypeError, AttributeError):\n            # if the above fails (for any reason) just fallback to what\n            # we do by default and convert dx by itself.\n            dx = convert(dx)\n        return dx\n\n    @_preprocess_data()\n    @_docstring.dedent_interpd\n(C)                 x0 = cbook._safe_first_finite(x0)\n            except (TypeError, IndexError, KeyError):\n                pass\n\n            try:\n                x = cbook._safe_first_finite(xconv)\n            except (TypeError, IndexError, KeyError):\n                x = xconv\n\n            delist = False\n            if not np.iterable(dx):\n                dx = [dx]\n                delist = True\n            dx = [convert(x0 + ddx) - x for ddx in dx]\n            if delist:\n                dx = dx[0]\n        except (ValueError, TypeError, AttributeError):\n            # if the above fails (for any reason) just fallback to what\n            # we do by default and convert dx by itself.\n(D)             # x0 and dx are lists so x0 + dx just concatenates the lists.\n            # We can't just cast x0 and dx to numpy arrays because that\n            # removes the units from unit packages like `pint` that\n            # wrap numpy arrays.\n            try:\n                x0 = cbook._safe_first_finite(x0)\n            except (TypeError, IndexError, KeyError):\n                pass\n\n            try:\n                x = cbook._safe_first_finite(xconv)\n            except (TypeError, IndexError, KeyError):\n                x = xconv\n\n            delist = False\n            if not np.iterable(dx):\n                dx = [dx]\n                delist = True\n            dx = [convert(x0 + ddx) - x for ddx in dx]", "answer": "C"}
{"uuid": "5eb7497d-4320-401e-b569-234bcef93880", "issue_statement": "[ENH]: Axes.set_xticks/Axis.set_ticks only validates kwargs if ticklabels are set, but they should\n### Problem\n\nPer the doc of `Axis.set_ticks`:\r\n```\r\n        **kwargs\r\n            `.Text` properties for the labels. These take effect only if you\r\n            pass *labels*. In other cases, please use `~.Axes.tick_params`.\r\n```\r\nThis means that in e.g. `ax.set_xticks([0, 1], xticklabels=[\"a\", \"b\"])`, the incorrect `xticklabels` silently do nothing; they are not even validated (because `labels` has not been passed).\n\n### Proposed solution\n\nWe should at least check that `kwargs` are valid Text properties in all cases; we could even consider making any kwargs an error if `labels` is not set.", "choices": "(A)             self.set_ticklabels(labels, minor=minor, **kwargs)\n        return result\n\n    def _get_tick_boxes_siblings(self, renderer):\n        \"\"\"\n        Get the bounding boxes for this `.axis` and its siblings\n        as set by `.Figure.align_xlabels` or  `.Figure.align_ylabels`.\n\n        By default it just gets bboxes for self.\n(B)         as set by `.Figure.align_xlabels` or  `.Figure.align_ylabels`.\n\n        By default it just gets bboxes for self.\n        \"\"\"\n        # Get the Grouper keeping track of x or y label groups for this figure.\n        axis_names = [\n            name for name, axis in self.axes._axis_map.items()\n            if name in self.figure._align_label_groups and axis is self]\n        if len(axis_names) != 1:\n(C)         By default it just gets bboxes for self.\n        \"\"\"\n        # Get the Grouper keeping track of x or y label groups for this figure.\n        axis_names = [\n            name for name, axis in self.axes._axis_map.items()\n            if name in self.figure._align_label_groups and axis is self]\n        if len(axis_names) != 1:\n            return [], []\n        axis_name, = axis_names\n(D)     def _get_tick_boxes_siblings(self, renderer):\n        \"\"\"\n        Get the bounding boxes for this `.axis` and its siblings\n        as set by `.Figure.align_xlabels` or  `.Figure.align_ylabels`.\n\n        By default it just gets bboxes for self.\n        \"\"\"\n        # Get the Grouper keeping track of x or y label groups for this figure.\n        axis_names = [", "answer": "D"}
{"uuid": "4061f237-104a-4e88-82d6-e61a793d0f65", "issue_statement": "[Bug]: NumPy 1.24 deprecation warnings\n### Bug summary\r\n\r\nStarting NumPy 1.24 I observe several deprecation warnings.\r\n\r\n\r\n### Code for reproduction\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nplt.get_cmap()(np.empty((0, ), dtype=np.uint8))\r\n```\r\n\r\n\r\n### Actual outcome\r\n\r\n```\r\n/usr/lib/python3.10/site-packages/matplotlib/colors.py:730: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 257 to uint8 will fail in the future.\r\nFor the old behavior, usually:\r\n    np.array(value).astype(dtype)`\r\nwill give the desired result (the cast overflows).\r\n  xa[xa > self.N - 1] = self._i_over\r\n/usr/lib/python3.10/site-packages/matplotlib/colors.py:731: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 256 to uint8 will fail in the future.\r\nFor the old behavior, usually:\r\n    np.array(value).astype(dtype)`\r\nwill give the desired result (the cast overflows).\r\n  xa[xa < 0] = self._i_under\r\n/usr/lib/python3.10/site-packages/matplotlib/colors.py:732: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 258 to uint8 will fail in the future.\r\nFor the old behavior, usually:\r\n    np.array(value).astype(dtype)`\r\nwill give the desired result (the cast overflows).\r\n  xa[mask_bad] = self._i_bad\r\n```\r\n\r\n### Expected outcome\r\n\r\nNo warnings.\r\n\r\n### Additional information\r\n\r\n_No response_\r\n\r\n### Operating system\r\n\r\nArchLinux\r\n\r\n### Matplotlib Version\r\n\r\n3.6.2\r\n\r\n### Matplotlib Backend\r\n\r\nQtAgg\r\n\r\n### Python version\r\n\r\nPython 3.10.9\r\n\r\n### Jupyter version\r\n\r\n_No response_\r\n\r\n### Installation\r\n\r\nLinux package manager", "choices": "(A)         if not xa.dtype.isnative:\n            xa = xa.byteswap().newbyteorder()  # Native byteorder is faster.\n        if xa.dtype.kind == \"f\":\n            with np.errstate(invalid=\"ignore\"):\n                xa *= self.N\n                # Negative values are out of range, but astype(int) would\n                # truncate them towards zero.\n                xa[xa < 0] = -1\n                # xa == 1 (== N after multiplication) is not out of range.\n                xa[xa == self.N] = self.N - 1\n                # Avoid converting large positive values to negative integers.\n                np.clip(xa, -1, self.N, out=xa)\n                xa = xa.astype(int)\n        # Set the over-range indices before the under-range;\n        # otherwise the under-range values get converted to over-range.\n        xa[xa > self.N - 1] = self._i_over\n        xa[xa < 0] = self._i_under\n(B)                 xa[xa == self.N] = self.N - 1\n                # Avoid converting large positive values to negative integers.\n                np.clip(xa, -1, self.N, out=xa)\n                xa = xa.astype(int)\n        # Set the over-range indices before the under-range;\n        # otherwise the under-range values get converted to over-range.\n        xa[xa > self.N - 1] = self._i_over\n        xa[xa < 0] = self._i_under\n        xa[mask_bad] = self._i_bad\n\n        lut = self._lut\n        if bytes:\n            lut = (lut * 255).astype(np.uint8)\n\n        rgba = lut.take(xa, axis=0, mode='clip')\n\n        if alpha is not None:\n(C)                 # Negative values are out of range, but astype(int) would\n                # truncate them towards zero.\n                xa[xa < 0] = -1\n                # xa == 1 (== N after multiplication) is not out of range.\n                xa[xa == self.N] = self.N - 1\n                # Avoid converting large positive values to negative integers.\n                np.clip(xa, -1, self.N, out=xa)\n                xa = xa.astype(int)\n        # Set the over-range indices before the under-range;\n        # otherwise the under-range values get converted to over-range.\n        xa[xa > self.N - 1] = self._i_over\n        xa[xa < 0] = self._i_under\n        xa[mask_bad] = self._i_bad\n\n        lut = self._lut\n        if bytes:\n            lut = (lut * 255).astype(np.uint8)\n(D)         # np.isnan() to after we have converted to an array.\n        mask_bad = X.mask if np.ma.is_masked(X) else None\n        xa = np.array(X, copy=True)\n        if mask_bad is None:\n            mask_bad = np.isnan(xa)\n        if not xa.dtype.isnative:\n            xa = xa.byteswap().newbyteorder()  # Native byteorder is faster.\n        if xa.dtype.kind == \"f\":\n            with np.errstate(invalid=\"ignore\"):\n                xa *= self.N\n                # Negative values are out of range, but astype(int) would\n                # truncate them towards zero.\n                xa[xa < 0] = -1\n                # xa == 1 (== N after multiplication) is not out of range.\n                xa[xa == self.N] = self.N - 1\n                # Avoid converting large positive values to negative integers.\n                np.clip(xa, -1, self.N, out=xa)", "answer": "A"}
{"uuid": "52139d09-9c2c-4c12-bde8-c71161aa5403", "issue_statement": "[Bug]: Setting norm with existing colorbar fails with 3.6.3\n### Bug summary\r\n\r\nSetting the norm to a `LogNorm` after the colorbar has been created (e.g. in interactive code) fails with an `Invalid vmin` value in matplotlib 3.6.3.\r\n\r\nThe same code worked in previous matplotlib versions.\r\n\r\nNot that vmin and vmax are explicitly set to values valid for `LogNorm` and no negative values (or values == 0) exist in the input data.\r\n\r\n### Code for reproduction\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.colors import LogNorm\r\nimport numpy as np\r\n\r\n# create some random data to fill a 2d plot\r\nrng = np.random.default_rng(0)\r\nimg = rng.uniform(1, 5, (25, 25))\r\n\r\n# plot it\r\nfig, ax = plt.subplots(layout=\"constrained\")\r\nplot = ax.pcolormesh(img)\r\ncbar = fig.colorbar(plot, ax=ax)\r\n\r\nvmin = 1\r\nvmax = 5\r\n\r\nplt.ion()\r\nfig.show()\r\nplt.pause(0.5)\r\n\r\nplot.norm = LogNorm(vmin, vmax)\r\nplot.autoscale()\r\nplt.pause(0.5)\r\n```\r\n\r\n\r\n### Actual outcome\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/backends/backend_qt.py\", line 454, in _draw_idle\r\n    self.draw()\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py\", line 405, in draw\r\n    self.figure.draw(self.renderer)\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/artist.py\", line 74, in draw_wrapper\r\n    result = draw(artist, renderer, *args, **kwargs)\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/artist.py\", line 51, in draw_wrapper\r\n    return draw(artist, renderer)\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/figure.py\", line 3082, in draw\r\n    mimage._draw_list_compositing_images(\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/image.py\", line 131, in _draw_list_compositing_images\r\n    a.draw(renderer)\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/artist.py\", line 51, in draw_wrapper\r\n    return draw(artist, renderer)\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/axes/_base.py\", line 3100, in draw\r\n    mimage._draw_list_compositing_images(\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/image.py\", line 131, in _draw_list_compositing_images\r\n    a.draw(renderer)\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/artist.py\", line 51, in draw_wrapper\r\n    return draw(artist, renderer)\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/collections.py\", line 2148, in draw\r\n    self.update_scalarmappable()\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/collections.py\", line 891, in update_scalarmappable\r\n    self._mapped_colors = self.to_rgba(self._A, self._alpha)\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/cm.py\", line 511, in to_rgba\r\n    x = self.norm(x)\r\n  File \"/home/mnoethe/.local/conda/envs/cta-dev/lib/python3.9/site-packages/matplotlib/colors.py\", line 1694, in __call__\r\n    raise ValueError(\"Invalid vmin or vmax\")\r\nValueError: Invalid vmin or vmax\r\n```\r\n\r\n### Expected outcome\r\n\r\nWorks, colorbar and mappable are updated with new norm.\r\n\r\n### Additional information\r\n\r\n_No response_\r\n\r\n### Operating system\r\n\r\nLinux\r\n\r\n### Matplotlib Version\r\n\r\n3.6.3 (works with 3.6.2)\r\n\r\n### Matplotlib Backend\r\n\r\nMultpiple backends tested, same error in all (Qt5Agg, TkAgg, agg, ...)\r\n\r\n### Python version\r\n\r\n3.9.15\r\n\r\n### Jupyter version\r\n\r\nnot in jupyter\r\n\r\n### Installation\r\n\r\nconda", "choices": "(A) \n    def autoscale(self, A):\n        \"\"\"Set *vmin*, *vmax* to min, max of *A*.\"\"\"\n        self.vmin = self.vmax = None\n        self.autoscale_None(A)\n\n    def autoscale_None(self, A):\n        \"\"\"If vmin or vmax are not set, use the min/max of *A* to set them.\"\"\"\n        A = np.asanyarray(A)\n        if self.vmin is None and A.size:\n            self.vmin = A.min()\n        if self.vmax is None and A.size:\n(B)             return vmin + val * (vmax - vmin)\n        else:\n            return vmin + value * (vmax - vmin)\n\n    def autoscale(self, A):\n        \"\"\"Set *vmin*, *vmax* to min, max of *A*.\"\"\"\n        self.vmin = self.vmax = None\n        self.autoscale_None(A)\n\n    def autoscale_None(self, A):\n        \"\"\"If vmin or vmax are not set, use the min/max of *A* to set them.\"\"\"\n        A = np.asanyarray(A)\n(C)         self.vmin = self.vmax = None\n        self.autoscale_None(A)\n\n    def autoscale_None(self, A):\n        \"\"\"If vmin or vmax are not set, use the min/max of *A* to set them.\"\"\"\n        A = np.asanyarray(A)\n        if self.vmin is None and A.size:\n            self.vmin = A.min()\n        if self.vmax is None and A.size:\n            self.vmax = A.max()\n\n    def scaled(self):\n(D)     def autoscale_None(self, A):\n        \"\"\"If vmin or vmax are not set, use the min/max of *A* to set them.\"\"\"\n        A = np.asanyarray(A)\n        if self.vmin is None and A.size:\n            self.vmin = A.min()\n        if self.vmax is None and A.size:\n            self.vmax = A.max()\n\n    def scaled(self):\n        \"\"\"Return whether vmin and vmax are set.\"\"\"\n        return self.vmin is not None and self.vmax is not None\n", "answer": "A"}
{"uuid": "57584993-db41-45e9-9b7d-e3baa0eeebce", "issue_statement": "[Bug]: Unable to pickle figure with draggable legend\n### Bug summary\r\n\r\nI am unable to pickle figure with draggable legend. Same error comes for draggable annotations.\r\n\r\n\r\n\r\n\r\n\r\n### Code for reproduction\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport pickle\r\n\r\nfig = plt.figure()\r\nax = fig.add_subplot(111)\r\n\r\ntime=[0,1,2,3,4]\r\nspeed=[40,43,45,47,48]\r\n\r\nax.plot(time,speed,label=\"speed\")\r\n\r\nleg=ax.legend()\r\nleg.set_draggable(True) #pickling works after removing this line \r\n\r\npickle.dumps(fig)\r\nplt.show()\r\n```\r\n\r\n\r\n### Actual outcome\r\n\r\n`TypeError: cannot pickle 'FigureCanvasQTAgg' object`\r\n\r\n### Expected outcome\r\n\r\nPickling successful\r\n\r\n### Additional information\r\n\r\n_No response_\r\n\r\n### Operating system\r\n\r\nWindows 10\r\n\r\n### Matplotlib Version\r\n\r\n3.7.0\r\n\r\n### Matplotlib Backend\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Jupyter version\r\n\r\n_No response_\r\n\r\n### Installation\r\n\r\npip", "choices": "(A)                 'button_release_event', self.on_release),\n        ]\n\n    def on_motion(self, evt):\n        if self._check_still_parented() and self.got_artist:\n            dx = evt.x - self.mouse_x\n            dy = evt.y - self.mouse_y\n            self.update_offset(dx, dy)\n            if self._use_blit:\n                self.canvas.restore_region(self.background)\n                self.ref_artist.draw(\n                    self.ref_artist.figure._get_renderer())\n                self.canvas.blit()\n            else:\n                self.canvas.draw()\n\n    def on_pick(self, evt):\n(B)     coordinate and set a relevant attribute.\n    \"\"\"\n\n    def __init__(self, ref_artist, use_blit=False):\n        self.ref_artist = ref_artist\n        if not ref_artist.pickable():\n            ref_artist.set_picker(True)\n        self.got_artist = False\n        self.canvas = self.ref_artist.figure.canvas\n        self._use_blit = use_blit and self.canvas.supports_blit\n        self.cids = [\n            self.canvas.callbacks._connect_picklable(\n                'pick_event', self.on_pick),\n            self.canvas.callbacks._connect_picklable(\n                'button_release_event', self.on_release),\n        ]\n\n(C)         if not ref_artist.pickable():\n            ref_artist.set_picker(True)\n        self.got_artist = False\n        self.canvas = self.ref_artist.figure.canvas\n        self._use_blit = use_blit and self.canvas.supports_blit\n        self.cids = [\n            self.canvas.callbacks._connect_picklable(\n                'pick_event', self.on_pick),\n            self.canvas.callbacks._connect_picklable(\n                'button_release_event', self.on_release),\n        ]\n\n    def on_motion(self, evt):\n        if self._check_still_parented() and self.got_artist:\n            dx = evt.x - self.mouse_x\n            dy = evt.y - self.mouse_y\n            self.update_offset(dx, dy)\n(D)         self.cids = [\n            self.canvas.callbacks._connect_picklable(\n                'pick_event', self.on_pick),\n            self.canvas.callbacks._connect_picklable(\n                'button_release_event', self.on_release),\n        ]\n\n    def on_motion(self, evt):\n        if self._check_still_parented() and self.got_artist:\n            dx = evt.x - self.mouse_x\n            dy = evt.y - self.mouse_y\n            self.update_offset(dx, dy)\n            if self._use_blit:\n                self.canvas.restore_region(self.background)\n                self.ref_artist.draw(\n                    self.ref_artist.figure._get_renderer())\n                self.canvas.blit()", "answer": "C"}
{"uuid": "5405e02b-b69f-46fe-861c-e86e2e00f028", "issue_statement": "[Bug]: Unable to pickle figure with aligned labels\n### Bug summary\r\n\r\n Unable to pickle figure after calling `align_labels()`\r\n\r\n### Code for reproduction\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport pickle\r\n\r\nfig = plt.figure()\r\nax1 = fig.add_subplot(211)\r\nax2 = fig.add_subplot(212)\r\ntime=[0,1,2,3,4]\r\nspeed=[40000,4300,4500,4700,4800]\r\nacc=[10,11,12,13,14]\r\nax1.plot(time,speed)\r\nax1.set_ylabel('speed')\r\nax2.plot(time,acc)\r\nax2.set_ylabel('acc')\r\n\r\nfig.align_labels() ##pickling works after removing this line \r\n\r\npickle.dumps(fig)\r\nplt.show()\r\n```\r\n\r\n\r\n### Actual outcome\r\n```\r\nalign.py\", line 16\r\npickle.dumps(fig)\r\nTypeError: cannot pickle 'weakref.ReferenceType' object\r\n```\r\n### Expected outcome\r\n\r\nPickling successful\r\n\r\n### Additional information\r\n\r\n_No response_\r\n\r\n### Operating system\r\n\r\nWindows\r\n\r\n### Matplotlib Version\r\n\r\n3.7.0\r\n\r\n### Matplotlib Backend\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Jupyter version\r\n\r\n_No response_\r\n\r\n### Installation\r\n\r\nNone", "choices": "(A) \n    def clean(self):\n        \"\"\"Clean dead weak references from the dictionary.\"\"\"\n        mapping = self._mapping\n        to_drop = [key for key in mapping if key() is None]\n        for key in to_drop:\n            val = mapping.pop(key)\n            val.remove(key)\n\n    def join(self, a, *args):\n        \"\"\"\n        Join given arguments into the same set.  Accepts one or more arguments.\n        \"\"\"\n        mapping = self._mapping\n        set_a = mapping.setdefault(weakref.ref(a), [weakref.ref(a)])\n\n        for arg in args:\n            set_b = mapping.get(weakref.ref(arg), [weakref.ref(arg)])\n            if set_b is not set_a:\n(B)     def __init__(self, init=()):\n        self._mapping = {weakref.ref(x): [weakref.ref(x)] for x in init}\n\n    def __contains__(self, item):\n        return weakref.ref(item) in self._mapping\n\n    def clean(self):\n        \"\"\"Clean dead weak references from the dictionary.\"\"\"\n        mapping = self._mapping\n        to_drop = [key for key in mapping if key() is None]\n        for key in to_drop:\n            val = mapping.pop(key)\n            val.remove(key)\n\n    def join(self, a, *args):\n        \"\"\"\n        Join given arguments into the same set.  Accepts one or more arguments.\n        \"\"\"\n        mapping = self._mapping\n(C)         for key in to_drop:\n            val = mapping.pop(key)\n            val.remove(key)\n\n    def join(self, a, *args):\n        \"\"\"\n        Join given arguments into the same set.  Accepts one or more arguments.\n        \"\"\"\n        mapping = self._mapping\n        set_a = mapping.setdefault(weakref.ref(a), [weakref.ref(a)])\n\n        for arg in args:\n            set_b = mapping.get(weakref.ref(arg), [weakref.ref(arg)])\n            if set_b is not set_a:\n                if len(set_b) > len(set_a):\n                    set_a, set_b = set_b, set_a\n                set_a.extend(set_b)\n                for elem in set_b:\n                    mapping[elem] = set_a\n(D)     True\n    >>> grp.joined(a, d)\n    False\n    \"\"\"\n\n    def __init__(self, init=()):\n        self._mapping = {weakref.ref(x): [weakref.ref(x)] for x in init}\n\n    def __contains__(self, item):\n        return weakref.ref(item) in self._mapping\n\n    def clean(self):\n        \"\"\"Clean dead weak references from the dictionary.\"\"\"\n        mapping = self._mapping\n        to_drop = [key for key in mapping if key() is None]\n        for key in to_drop:\n            val = mapping.pop(key)\n            val.remove(key)\n", "answer": "B"}
{"uuid": "2ed2cb21-af89-4d0a-90c6-1f1e3a100a1a", "issue_statement": "[Bug]: using clf and pyplot.draw in range slider on_changed callback blocks input to widgets\n### Bug summary\n\nWhen using clear figure, adding new widgets and then redrawing the current figure in the on_changed callback of a range slider the inputs to all the widgets in the figure are blocked. When doing the same in the button callback on_clicked, everything works fine.\n\n### Code for reproduction\n\n```python\nimport matplotlib.pyplot as pyplot\r\nimport matplotlib.widgets as widgets\r\n\r\ndef onchanged(values):\r\n    print(\"on changed\")\r\n    print(values)\r\n    pyplot.clf()\r\n    addElements()\r\n    pyplot.draw()\r\n\r\ndef onclick(e):\r\n    print(\"on click\")\r\n    pyplot.clf()\r\n    addElements()\r\n    pyplot.draw()\r\n\r\ndef addElements():\r\n    ax = pyplot.axes([0.1, 0.45, 0.8, 0.1])\r\n    global slider\r\n    slider = widgets.RangeSlider(ax, \"Test\", valmin=1, valmax=10, valinit=(1, 10))\r\n    slider.on_changed(onchanged)\r\n    ax = pyplot.axes([0.1, 0.30, 0.8, 0.1])\r\n    global button\r\n    button = widgets.Button(ax, \"Test\")\r\n    button.on_clicked(onclick)\r\n\r\naddElements()\r\n\r\npyplot.show()\n```\n\n\n### Actual outcome\n\nThe widgets can't receive any input from a mouse click, when redrawing in the on_changed callback of a range Slider. \r\nWhen using a button, there is no problem.\n\n### Expected outcome\n\nThe range slider callback on_changed behaves the same as the button callback on_clicked.\n\n### Additional information\n\nThe problem also occurred on Manjaro with:\r\n- Python version: 3.10.9\r\n- Matplotlib version: 3.6.2\r\n- Matplotlib backend: QtAgg\r\n- Installation of matplotlib via Linux package manager\r\n\n\n### Operating system\n\nWindows 10\n\n### Matplotlib Version\n\n3.6.2\n\n### Matplotlib Backend\n\nTkAgg\n\n### Python version\n\n3.11.0\n\n### Jupyter version\n\n_No response_\n\n### Installation\n\npip", "choices": "(A) \n        self._axstack.remove(ax)\n        self._axobservers.process(\"_axes_change_event\", self)\n        self.stale = True\n        self._localaxes.remove(ax)\n\n        # Break link between any shared axes\n(B)         self._localaxes.remove(ax)\n\n        # Break link between any shared axes\n        for name in ax._axis_names:\n            last_ax = _break_share_link(ax, ax._shared_axes[name])\n            if last_ax is not None:\n                _reset_locators_and_formatters(last_ax._axis_map[name])\n(C)         # Break link between any shared axes\n        for name in ax._axis_names:\n            last_ax = _break_share_link(ax, ax._shared_axes[name])\n            if last_ax is not None:\n                _reset_locators_and_formatters(last_ax._axis_map[name])\n\n        # Break link between any twinned axes\n(D)         self._axobservers.process(\"_axes_change_event\", self)\n        self.stale = True\n        self._localaxes.remove(ax)\n\n        # Break link between any shared axes\n        for name in ax._axis_names:\n            last_ax = _break_share_link(ax, ax._shared_axes[name])", "answer": "D"}
{"uuid": "4fdeae62-f3ae-40fc-b8d0-8352cd262230", "issue_statement": "[Bug]: Attribute Error combining matplotlib 3.7.1 and mplcursor on data selection\n### Bug summary\r\n\r\nIf you combine mplcursor and matplotlib 3.7.1, you'll get an `AttributeError: 'NoneType' object has no attribute 'canvas'` after clicking a few data points. Henceforth, selecting a new data point will trigger the same traceback. Otherwise, it works fine. \r\n\r\n### Code for reproduction\r\n\r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport mplcursors as mpl\r\n\r\nx = np.arange(1, 11)    \r\ny1 = x\r\n\r\nplt.scatter(x,y1)\r\n\r\nmpl.cursor()\r\nplt.show()\r\n```\r\n\r\n\r\n### Actual outcome\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\MrAni\\Python\\miniconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\", line 304, in process\r\n    func(*args, **kwargs)\r\n  File \"C:\\Users\\MrAni\\Python\\miniconda3\\lib\\site-packages\\matplotlib\\offsetbox.py\", line 1550, in on_release\r\n    if self._check_still_parented() and self.got_artist:\r\n  File \"C:\\Users\\MrAni\\Python\\miniconda3\\lib\\site-packages\\matplotlib\\offsetbox.py\", line 1560, in _check_still_parented\r\n    self.disconnect()\r\n  File \"C:\\Users\\MrAni\\Python\\miniconda3\\lib\\site-packages\\matplotlib\\offsetbox.py\", line 1568, in disconnect\r\n    self.canvas.mpl_disconnect(cid)\r\n  File \"C:\\Users\\MrAni\\Python\\miniconda3\\lib\\site-packages\\matplotlib\\offsetbox.py\", line 1517, in <lambda>\r\n    canvas = property(lambda self: self.ref_artist.figure.canvas)\r\nAttributeError: 'NoneType' object has no attribute 'canvas'\r\n```\r\n\r\n### Expected outcome\r\n\r\nNo terminal output\r\n\r\n### Additional information\r\n\r\nUsing matplotlib 3.7.0 or lower works fine. Using a conda install or pip install doesn't affect the output. \r\n\r\n### Operating system\r\n\r\nWindows 11 and Windwos 10 \r\n\r\n### Matplotlib Version\r\n\r\n3.7.1\r\n\r\n### Matplotlib Backend\r\n\r\nQtAgg\r\n\r\n### Python version\r\n\r\n3.9.16\r\n\r\n### Jupyter version\r\n\r\n_No response_\r\n\r\n### Installation\r\n\r\nconda", "choices": "(A)                     self.canvas.copy_from_bbox(self.ref_artist.figure.bbox)\n                self.ref_artist.draw(\n                    self.ref_artist.figure._get_renderer())\n                self.canvas.blit()\n            self._c1 = self.canvas.callbacks._connect_picklable(\n                \"motion_notify_event\", self.on_motion)\n            self.save_offset()\n\n    def on_release(self, event):\n        if self._check_still_parented() and self.got_artist:\n            self.finalize_offset()\n            self.got_artist = False\n            self.canvas.mpl_disconnect(self._c1)\n\n            if self._use_blit:\n                self.ref_artist.set_animated(False)\n\n    def _check_still_parented(self):\n        if self.ref_artist.figure is None:\n            self.disconnect()\n            return False\n        else:\n            return True\n\n    def disconnect(self):\n        \"\"\"Disconnect the callbacks.\"\"\"\n        for cid in self.cids:\n            self.canvas.mpl_disconnect(cid)\n        try:\n            c1 = self._c1\n        except AttributeError:\n            pass\n        else:\n            self.canvas.mpl_disconnect(c1)\n\n    def save_offset(self):\n        pass\n\n    def update_offset(self, dx, dy):\n        pass\n\n    def finalize_offset(self):\n        pass\n\n\nclass DraggableOffsetBox(DraggableBase):\n    def __init__(self, ref_artist, offsetbox, use_blit=False):\n        super().__init__(ref_artist, use_blit=use_blit)\n        self.offsetbox = offsetbox\n\n    def save_offset(self):\n        offsetbox = self.offsetbox\n        renderer = offsetbox.figure._get_renderer()\n        offset = offsetbox.get_offset(offsetbox.get_bbox(renderer), renderer)\n        self.offsetbox_x, self.offsetbox_y = offset\n        self.offsetbox.set_offset(offset)\n\n    def update_offset(self, dx, dy):\n        loc_in_canvas = self.offsetbox_x + dx, self.offsetbox_y + dy\n        self.offsetbox.set_offset(loc_in_canvas)\n\n    def get_loc_in_canvas(self):\n        offsetbox = self.offsetbox\n        renderer = offsetbox.figure._get_renderer()\n        bbox = offsetbox.get_bbox(renderer)\n        ox, oy = offsetbox._offset\n        loc_in_canvas = (ox + bbox.x0, oy + bbox.y0)\n        return loc_in_canvas\n\n(B)             if self._use_blit:\n                self.canvas.restore_region(self.background)\n                self.ref_artist.draw(\n                    self.ref_artist.figure._get_renderer())\n                self.canvas.blit()\n            else:\n                self.canvas.draw()\n\n    def on_pick(self, evt):\n        if self._check_still_parented() and evt.artist == self.ref_artist:\n            self.mouse_x = evt.mouseevent.x\n            self.mouse_y = evt.mouseevent.y\n            self.got_artist = True\n            if self._use_blit:\n                self.ref_artist.set_animated(True)\n                self.canvas.draw()\n                self.background = \\\n                    self.canvas.copy_from_bbox(self.ref_artist.figure.bbox)\n                self.ref_artist.draw(\n                    self.ref_artist.figure._get_renderer())\n                self.canvas.blit()\n            self._c1 = self.canvas.callbacks._connect_picklable(\n                \"motion_notify_event\", self.on_motion)\n            self.save_offset()\n\n    def on_release(self, event):\n        if self._check_still_parented() and self.got_artist:\n            self.finalize_offset()\n            self.got_artist = False\n            self.canvas.mpl_disconnect(self._c1)\n\n            if self._use_blit:\n                self.ref_artist.set_animated(False)\n\n    def _check_still_parented(self):\n        if self.ref_artist.figure is None:\n            self.disconnect()\n            return False\n        else:\n            return True\n\n    def disconnect(self):\n        \"\"\"Disconnect the callbacks.\"\"\"\n        for cid in self.cids:\n            self.canvas.mpl_disconnect(cid)\n        try:\n            c1 = self._c1\n        except AttributeError:\n            pass\n        else:\n            self.canvas.mpl_disconnect(c1)\n\n    def save_offset(self):\n        pass\n\n    def update_offset(self, dx, dy):\n        pass\n\n    def finalize_offset(self):\n        pass\n\n\nclass DraggableOffsetBox(DraggableBase):\n    def __init__(self, ref_artist, offsetbox, use_blit=False):\n        super().__init__(ref_artist, use_blit=use_blit)\n        self.offsetbox = offsetbox\n\n    def save_offset(self):\n        offsetbox = self.offsetbox\n(C)             Called during the dragging; (*dx*, *dy*) is the pixel offset from\n            the point where the mouse drag started.\n            '''\n\n    Optionally, you may override the following method::\n\n        def finalize_offset(self):\n            '''Called when the mouse is released.'''\n\n    In the current implementation of `.DraggableLegend` and\n    `DraggableAnnotation`, `update_offset` places the artists in display\n    coordinates, and `finalize_offset` recalculates their position in axes\n    coordinate and set a relevant attribute.\n    \"\"\"\n\n    def __init__(self, ref_artist, use_blit=False):\n        self.ref_artist = ref_artist\n        if not ref_artist.pickable():\n            ref_artist.set_picker(True)\n        self.got_artist = False\n        self._use_blit = use_blit and self.canvas.supports_blit\n        self.cids = [\n            self.canvas.callbacks._connect_picklable(\n                'pick_event', self.on_pick),\n            self.canvas.callbacks._connect_picklable(\n                'button_release_event', self.on_release),\n        ]\n\n    # A property, not an attribute, to maintain picklability.\n    canvas = property(lambda self: self.ref_artist.figure.canvas)\n\n    def on_motion(self, evt):\n        if self._check_still_parented() and self.got_artist:\n            dx = evt.x - self.mouse_x\n            dy = evt.y - self.mouse_y\n            self.update_offset(dx, dy)\n            if self._use_blit:\n                self.canvas.restore_region(self.background)\n                self.ref_artist.draw(\n                    self.ref_artist.figure._get_renderer())\n                self.canvas.blit()\n            else:\n                self.canvas.draw()\n\n    def on_pick(self, evt):\n        if self._check_still_parented() and evt.artist == self.ref_artist:\n            self.mouse_x = evt.mouseevent.x\n            self.mouse_y = evt.mouseevent.y\n            self.got_artist = True\n            if self._use_blit:\n                self.ref_artist.set_animated(True)\n                self.canvas.draw()\n                self.background = \\\n                    self.canvas.copy_from_bbox(self.ref_artist.figure.bbox)\n                self.ref_artist.draw(\n                    self.ref_artist.figure._get_renderer())\n                self.canvas.blit()\n            self._c1 = self.canvas.callbacks._connect_picklable(\n                \"motion_notify_event\", self.on_motion)\n            self.save_offset()\n\n    def on_release(self, event):\n        if self._check_still_parented() and self.got_artist:\n            self.finalize_offset()\n            self.got_artist = False\n            self.canvas.mpl_disconnect(self._c1)\n\n            if self._use_blit:\n                self.ref_artist.set_animated(False)\n(D)             ref_artist.set_picker(True)\n        self.got_artist = False\n        self._use_blit = use_blit and self.canvas.supports_blit\n        self.cids = [\n            self.canvas.callbacks._connect_picklable(\n                'pick_event', self.on_pick),\n            self.canvas.callbacks._connect_picklable(\n                'button_release_event', self.on_release),\n        ]\n\n    # A property, not an attribute, to maintain picklability.\n    canvas = property(lambda self: self.ref_artist.figure.canvas)\n\n    def on_motion(self, evt):\n        if self._check_still_parented() and self.got_artist:\n            dx = evt.x - self.mouse_x\n            dy = evt.y - self.mouse_y\n            self.update_offset(dx, dy)\n            if self._use_blit:\n                self.canvas.restore_region(self.background)\n                self.ref_artist.draw(\n                    self.ref_artist.figure._get_renderer())\n                self.canvas.blit()\n            else:\n                self.canvas.draw()\n\n    def on_pick(self, evt):\n        if self._check_still_parented() and evt.artist == self.ref_artist:\n            self.mouse_x = evt.mouseevent.x\n            self.mouse_y = evt.mouseevent.y\n            self.got_artist = True\n            if self._use_blit:\n                self.ref_artist.set_animated(True)\n                self.canvas.draw()\n                self.background = \\\n                    self.canvas.copy_from_bbox(self.ref_artist.figure.bbox)\n                self.ref_artist.draw(\n                    self.ref_artist.figure._get_renderer())\n                self.canvas.blit()\n            self._c1 = self.canvas.callbacks._connect_picklable(\n                \"motion_notify_event\", self.on_motion)\n            self.save_offset()\n\n    def on_release(self, event):\n        if self._check_still_parented() and self.got_artist:\n            self.finalize_offset()\n            self.got_artist = False\n            self.canvas.mpl_disconnect(self._c1)\n\n            if self._use_blit:\n                self.ref_artist.set_animated(False)\n\n    def _check_still_parented(self):\n        if self.ref_artist.figure is None:\n            self.disconnect()\n            return False\n        else:\n            return True\n\n    def disconnect(self):\n        \"\"\"Disconnect the callbacks.\"\"\"\n        for cid in self.cids:\n            self.canvas.mpl_disconnect(cid)\n        try:\n            c1 = self._c1\n        except AttributeError:\n            pass\n        else:\n            self.canvas.mpl_disconnect(c1)", "answer": "D"}
{"uuid": "c58b87b8-8b11-4592-b85d-cb58de513aa0", "issue_statement": "Update colorbar after changing mappable.norm\nHow can I update a colorbar, after I changed the norm instance of the colorbar?\n\n`colorbar.update_normal(mappable)` has now effect and `colorbar.update_bruteforce(mappable)` throws a `ZeroDivsionError`-Exception.\n\nConsider this example:\n\n``` python\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm\nimport numpy as np\n\nimg = 10**np.random.normal(1, 1, size=(50, 50))\n\nfig, ax = plt.subplots(1, 1)\nplot = ax.imshow(img, cmap='gray')\ncb = fig.colorbar(plot, ax=ax)\nplot.norm = LogNorm()\ncb.update_normal(plot)  # no effect\ncb.update_bruteforce(plot)  # throws ZeroDivisionError\nplt.show()\n```\n\nOutput for `cb.update_bruteforce(plot)`:\n\n```\nTraceback (most recent call last):\n  File \"test_norm.py\", line 12, in <module>\n    cb.update_bruteforce(plot)\n  File \"/home/maxnoe/.local/anaconda3/lib/python3.4/site-packages/matplotlib/colorbar.py\", line 967, in update_bruteforce\n    self.draw_all()\n  File \"/home/maxnoe/.local/anaconda3/lib/python3.4/site-packages/matplotlib/colorbar.py\", line 342, in draw_all\n    self._process_values()\n  File \"/home/maxnoe/.local/anaconda3/lib/python3.4/site-packages/matplotlib/colorbar.py\", line 664, in _process_values\n    b = self.norm.inverse(self._uniform_y(self.cmap.N + 1))\n  File \"/home/maxnoe/.local/anaconda3/lib/python3.4/site-packages/matplotlib/colors.py\", line 1011, in inverse\n    return vmin * ma.power((vmax / vmin), val)\nZeroDivisionError: division by zero\n```", "choices": "(A)         changes values of *vmin*, *vmax* or *cmap* then the old formatter\n        and locator will be preserved.\n        \"\"\"\n        _log.debug('colorbar update normal %r %r', mappable.norm, self.norm)\n        self.mappable = mappable\n        self.set_alpha(mappable.get_alpha())\n        self.cmap = mappable.cmap\n        if mappable.norm != self.norm:\n            self.norm = mappable.norm\n            self._reset_locator_formatter_scale()\n\n        self._draw_all()\n        if isinstance(self.mappable, contour.ContourSet):\n            CS = self.mappable\n            if not CS.filled:\n                self.add_lines(CS)\n        self.stale = True\n\n    @_api.deprecated(\"3.6\", alternative=\"fig.draw_without_rendering()\")\n    def draw_all(self):\n        \"\"\"\n        Calculate any free parameters based on the current cmap and norm,\n        and do all the drawing.\n        \"\"\"\n        self._draw_all()\n\n    def _draw_all(self):\n        \"\"\"\n        Calculate any free parameters based on the current cmap and norm,\n        and do all the drawing.\n        \"\"\"\n        if self.orientation == 'vertical':\n            if mpl.rcParams['ytick.minor.visible']:\n                self.minorticks_on()\n        else:\n            if mpl.rcParams['xtick.minor.visible']:\n                self.minorticks_on()\n        self._long_axis().set(label_position=self.ticklocation,\n                              ticks_position=self.ticklocation)\n        self._short_axis().set_ticks([])\n        self._short_axis().set_ticks([], minor=True)\n\n        # Set self._boundaries and self._values, including extensions.\n        # self._boundaries are the edges of each square of color, and\n        # self._values are the value to map into the norm to get the\n        # color:\n        self._process_values()\n        # Set self.vmin and self.vmax to first and last boundary, excluding\n        # extensions:\n        self.vmin, self.vmax = self._boundaries[self._inside][[0, -1]]\n        # Compute the X/Y mesh.\n        X, Y = self._mesh()\n        # draw the extend triangles, and shrink the inner axes to accommodate.\n        # also adds the outline path to self.outline spine:\n        self._do_extends()\n        lower, upper = self.vmin, self.vmax\n        if self._long_axis().get_inverted():\n            # If the axis is inverted, we need to swap the vmin/vmax\n            lower, upper = upper, lower\n        if self.orientation == 'vertical':\n            self.ax.set_xlim(0, 1)\n            self.ax.set_ylim(lower, upper)\n        else:\n            self.ax.set_ylim(0, 1)\n            self.ax.set_xlim(lower, upper)\n\n        # set up the tick locators and formatters.  A bit complicated because\n        # boundary norms + uniform spacing requires a manual locator.\n        self.update_ticks()\n\n        if self._filled:\n            ind = np.arange(len(self._values))\n            if self._extend_lower():\n                ind = ind[1:]\n            if self._extend_upper():\n                ind = ind[:-1]\n            self._add_solids(X, Y, self._values[ind, np.newaxis])\n\n    def _add_solids(self, X, Y, C):\n        \"\"\"Draw the colors; optionally add separators.\"\"\"\n        # Cleanup previously set artists.\n        if self.solids is not None:\n            self.solids.remove()\n        for solid in self.solids_patches:\n            solid.remove()\n        # Add new artist(s), based on mappable type.  Use individual patches if\n        # hatching is needed, pcolormesh otherwise.\n        mappable = getattr(self, 'mappable', None)\n        if (isinstance(mappable, contour.ContourSet)\n                and any(hatch is not None for hatch in mappable.hatches)):\n            self._add_solids_patches(X, Y, C, mappable)\n        else:\n            self.solids = self.ax.pcolormesh(\n                X, Y, C, cmap=self.cmap, norm=self.norm, alpha=self.alpha,\n                edgecolors='none', shading='flat')\n            if not self.drawedges:\n                if len(self._y) >= self.n_rasterize:\n                    self.solids.set_rasterized(True)\n        self._update_dividers()\n\n    def _update_dividers(self):\n        if not self.drawedges:\n            self.dividers.set_segments([])\n            return\n        # Place all *internal* dividers.\n        if self.orientation == 'vertical':\n            lims = self.ax.get_ylim()\n            bounds = (lims[0] < self._y) & (self._y < lims[1])\n        else:\n            lims = self.ax.get_xlim()\n            bounds = (lims[0] < self._y) & (self._y < lims[1])\n        y = self._y[bounds]\n        # And then add outer dividers if extensions are on.\n        if self._extend_lower():\n            y = np.insert(y, 0, lims[0])\n        if self._extend_upper():\n            y = np.append(y, lims[1])\n        X, Y = np.meshgrid([0, 1], y)\n        if self.orientation == 'vertical':\n            segments = np.dstack([X, Y])\n        else:\n            segments = np.dstack([Y, X])\n        self.dividers.set_segments(segments)\n\n    def _add_solids_patches(self, X, Y, C, mappable):\n        hatches = mappable.hatches * (len(C) + 1)  # Have enough hatches.\n        if self._extend_lower():\n            # remove first hatch that goes into the extend patch\n            hatches = hatches[1:]\n        patches = []\n        for i in range(len(X) - 1):\n            xy = np.array([[X[i, 0], Y[i, 1]],\n                           [X[i, 1], Y[i, 0]],\n                           [X[i + 1, 1], Y[i + 1, 0]],\n                           [X[i + 1, 0], Y[i + 1, 1]]])\n            patch = mpatches.PathPatch(mpath.Path(xy),\n                                       facecolor=self.cmap(self.norm(C[i][0])),\n                                       hatch=hatches[i], linewidth=0,\n                                       antialiased=False, alpha=self.alpha)\n            self.ax.add_patch(patch)\n            patches.append(patch)\n        self.solids_patches = patches\n\n    def _do_extends(self, ax=None):\n        \"\"\"\n        Add the extend tri/rectangles on the outside of the axes.\n\n        ax is unused, but required due to the callbacks on xlim/ylim changed\n        \"\"\"\n        # Clean up any previous extend patches\n        for patch in self._extend_patches:\n            patch.remove()\n        self._extend_patches = []\n        # extend lengths are fraction of the *inner* part of colorbar,\n        # not the total colorbar:\n        _, extendlen = self._proportional_y()\n        bot = 0 - (extendlen[0] if self._extend_lower() else 0)\n        top = 1 + (extendlen[1] if self._extend_upper() else 0)\n\n        # xyout is the outline of the colorbar including the extend patches:\n        if not self.extendrect:\n            # triangle:\n            xyout = np.array([[0, 0], [0.5, bot], [1, 0],\n                              [1, 1], [0.5, top], [0, 1], [0, 0]])\n        else:\n            # rectangle:\n            xyout = np.array([[0, 0], [0, bot], [1, bot], [1, 0],\n                              [1, 1], [1, top], [0, top], [0, 1],\n                              [0, 0]])\n\n        if self.orientation == 'horizontal':\n            xyout = xyout[:, ::-1]\n\n        # xyout is the path for the spine:\n        self.outline.set_xy(xyout)\n        if not self._filled:\n            return\n\n        # Make extend triangles or rectangles filled patches.  These are\n        # defined in the outer parent axes' coordinates:\n        mappable = getattr(self, 'mappable', None)\n        if (isinstance(mappable, contour.ContourSet)\n                and any(hatch is not None for hatch in mappable.hatches)):\n            hatches = mappable.hatches * (len(self._y) + 1)\n        else:\n            hatches = [None] * (len(self._y) + 1)\n\n        if self._extend_lower():\n            if not self.extendrect:\n                # triangle\n                xy = np.array([[0, 0], [0.5, bot], [1, 0]])\n            else:\n                # rectangle\n                xy = np.array([[0, 0], [0, bot], [1., bot], [1, 0]])\n            if self.orientation == 'horizontal':\n                xy = xy[:, ::-1]\n            # add the patch\n            val = -1 if self._long_axis().get_inverted() else 0\n            color = self.cmap(self.norm(self._values[val]))\n            patch = mpatches.PathPatch(\n                mpath.Path(xy), facecolor=color, alpha=self.alpha,\n                linewidth=0, antialiased=False,\n                transform=self.ax.transAxes,\n                hatch=hatches[0], clip_on=False,\n                # Place it right behind the standard patches, which is\n                # needed if we updated the extends\n                zorder=np.nextafter(self.ax.patch.zorder, -np.inf))\n            self.ax.add_patch(patch)\n            self._extend_patches.append(patch)\n            # remove first hatch that goes into the extend patch\n            hatches = hatches[1:]\n        if self._extend_upper():\n            if not self.extendrect:\n                # triangle\n                xy = np.array([[0, 1], [0.5, top], [1, 1]])\n            else:\n                # rectangle\n                xy = np.array([[0, 1], [0, top], [1, top], [1, 1]])\n            if self.orientation == 'horizontal':\n                xy = xy[:, ::-1]\n            # add the patch\n            val = 0 if self._long_axis().get_inverted() else -1\n            color = self.cmap(self.norm(self._values[val]))\n            hatch_idx = len(self._y) - 1\n            patch = mpatches.PathPatch(\n                mpath.Path(xy), facecolor=color, alpha=self.alpha,\n                linewidth=0, antialiased=False,\n                transform=self.ax.transAxes, hatch=hatches[hatch_idx],\n                clip_on=False,\n                # Place it right behind the standard patches, which is\n                # needed if we updated the extends\n                zorder=np.nextafter(self.ax.patch.zorder, -np.inf))\n            self.ax.add_patch(patch)\n            self._extend_patches.append(patch)\n\n        self._update_dividers()\n\n    def add_lines(self, *args, **kwargs):\n        \"\"\"\n        Draw lines on the colorbar.\n\n        The lines are appended to the list :attr:`lines`.\n\n        Parameters\n        ----------\n        levels : array-like\n            The positions of the lines.\n        colors : color or list of colors\n            Either a single color applying to all lines or one color value for\n            each line.\n        linewidths : float or array-like\n            Either a single linewidth applying to all lines or one linewidth\n            for each line.\n        erase : bool, default: True\n            Whether to remove any previously added lines.\n\n        Notes\n        -----\n        Alternatively, this method can also be called with the signature\n        ``colorbar.add_lines(contour_set, erase=True)``, in which case\n        *levels*, *colors*, and *linewidths* are taken from *contour_set*.\n        \"\"\"\n        params = _api.select_matching_signature(\n            [lambda self, CS, erase=True: locals(),\n             lambda self, levels, colors, linewidths, erase=True: locals()],\n            self, *args, **kwargs)\n        if \"CS\" in params:\n            self, CS, erase = params.values()\n            if not isinstance(CS, contour.ContourSet) or CS.filled:\n                raise ValueError(\"If a single artist is passed to add_lines, \"\n                                 \"it must be a ContourSet of lines\")\n            # TODO: Make colorbar lines auto-follow changes in contour lines.\n            return self.add_lines(\n                CS.levels,\n                CS.to_rgba(CS.cvalues, CS.alpha),\n                [coll.get_linewidths()[0] for coll in CS.collections],\n                erase=erase)\n        else:\n            self, levels, colors, linewidths, erase = params.values()\n\n        y = self._locate(levels)\n        rtol = (self._y[-1] - self._y[0]) * 1e-10\n        igood = (y < self._y[-1] + rtol) & (y > self._y[0] - rtol)\n        y = y[igood]\n        if np.iterable(colors):\n            colors = np.asarray(colors)[igood]\n        if np.iterable(linewidths):\n            linewidths = np.asarray(linewidths)[igood]\n        X, Y = np.meshgrid([0, 1], y)\n        if self.orientation == 'vertical':\n            xy = np.stack([X, Y], axis=-1)\n        else:\n            xy = np.stack([Y, X], axis=-1)\n        col = collections.LineCollection(xy, linewidths=linewidths,\n                                         colors=colors)\n\n        if erase and self.lines:\n            for lc in self.lines:\n                lc.remove()\n            self.lines = []\n        self.lines.append(col)\n\n        # make a clip path that is just a linewidth bigger than the axes...\n        fac = np.max(linewidths) / 72\n        xy = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n        inches = self.ax.get_figure().dpi_scale_trans\n        # do in inches:\n        xy = inches.inverted().transform(self.ax.transAxes.transform(xy))\n        xy[[0, 1, 4], 1] -= fac\n        xy[[2, 3], 1] += fac\n        # back to axes units...\n        xy = self.ax.transAxes.inverted().transform(inches.transform(xy))\n        col.set_clip_path(mpath.Path(xy, closed=True),\n                          self.ax.transAxes)\n        self.ax.add_collection(col)\n        self.stale = True\n\n    def update_ticks(self):\n        \"\"\"\n        Set up the ticks and ticklabels. This should not be needed by users.\n        \"\"\"\n        # Get the locator and formatter; defaults to self._locator if not None.\n        self._get_ticker_locator_formatter()\n        self._long_axis().set_major_locator(self._locator)\n        self._long_axis().set_minor_locator(self._minorlocator)\n        self._long_axis().set_major_formatter(self._formatter)\n\n    def _get_ticker_locator_formatter(self):\n        \"\"\"\n        Return the ``locator`` and ``formatter`` of the colorbar.\n\n        If they have not been defined (i.e. are *None*), the formatter and\n        locator are retrieved from the axis, or from the value of the\n        boundaries for a boundary norm.\n\n        Called by update_ticks...\n        \"\"\"\n        locator = self._locator\n        formatter = self._formatter\n        minorlocator = self._minorlocator\n        if isinstance(self.norm, colors.BoundaryNorm):\n            b = self.norm.boundaries\n            if locator is None:\n                locator = ticker.FixedLocator(b, nbins=10)\n            if minorlocator is None:\n                minorlocator = ticker.FixedLocator(b)\n        elif isinstance(self.norm, colors.NoNorm):\n            if locator is None:\n                # put ticks on integers between the boundaries of NoNorm\n                nv = len(self._values)\n                base = 1 + int(nv / 10)\n                locator = ticker.IndexLocator(base=base, offset=.5)\n        elif self.boundaries is not None:\n            b = self._boundaries[self._inside]\n            if locator is None:\n                locator = ticker.FixedLocator(b, nbins=10)\n        else:  # most cases:\n            if locator is None:\n                # we haven't set the locator explicitly, so use the default\n                # for this axis:\n                locator = self._long_axis().get_major_locator()\n            if minorlocator is None:\n                minorlocator = self._long_axis().get_minor_locator()\n\n        if minorlocator is None:\n            minorlocator = ticker.NullLocator()\n\n        if formatter is None:\n            formatter = self._long_axis().get_major_formatter()\n\n        self._locator = locator\n        self._formatter = formatter\n        self._minorlocator = minorlocator\n        _log.debug('locator: %r', locator)\n\n    def set_ticks(self, ticks, *, labels=None, minor=False, **kwargs):\n        \"\"\"\n        Set tick locations.\n\n        Parameters\n        ----------\n        ticks : list of floats\n            List of tick locations.\n        labels : list of str, optional\n            List of tick labels. If not set, the labels show the data value.\n        minor : bool, default: False\n            If ``False``, set the major ticks; if ``True``, the minor ticks.\n        **kwargs\n            `.Text` properties for the labels. These take effect only if you\n            pass *labels*. In other cases, please use `~.Axes.tick_params`.\n        \"\"\"\n        if np.iterable(ticks):\n            self._long_axis().set_ticks(ticks, labels=labels, minor=minor,\n                                        **kwargs)\n            self._locator = self._long_axis().get_major_locator()\n        else:\n            self._locator = ticks\n            self._long_axis().set_major_locator(self._locator)\n        self.stale = True\n\n    def get_ticks(self, minor=False):\n        \"\"\"\n        Return the ticks as a list of locations.\n\n        Parameters\n        ----------\n        minor : boolean, default: False\n            if True return the minor ticks.\n        \"\"\"\n        if minor:\n            return self._long_axis().get_minorticklocs()\n        else:\n            return self._long_axis().get_majorticklocs()\n\n    def set_ticklabels(self, ticklabels, *, minor=False, **kwargs):\n        \"\"\"\n        [*Discouraged*] Set tick labels.\n\n        .. admonition:: Discouraged\n\n            The use of this method is discouraged, because of the dependency\n            on tick positions. In most cases, you'll want to use\n            ``set_ticks(positions, labels=labels)`` instead.\n\n            If you are using this method, you should always fix the tick\n            positions before, e.g. by using `.Colorbar.set_ticks` or by\n            explicitly setting a `~.ticker.FixedLocator` on the long axis\n            of the colorbar. Otherwise, ticks are free to move and the\n            labels may end up in unexpected positions.\n\n        Parameters\n        ----------\n        ticklabels : sequence of str or of `.Text`\n            Texts for labeling each tick location in the sequence set by\n            `.Colorbar.set_ticks`; the number of labels must match the number\n            of locations.\n\n        update_ticks : bool, default: True\n            This keyword argument is ignored and will be removed.\n            Deprecated\n\n         minor : bool\n            If True, set minor ticks instead of major ticks.\n\n        **kwargs\n            `.Text` properties for the labels.\n        \"\"\"\n        self._long_axis().set_ticklabels(ticklabels, minor=minor, **kwargs)\n\n    def minorticks_on(self):\n        \"\"\"\n        Turn on colorbar minor ticks.\n        \"\"\"\n        self.ax.minorticks_on()\n        self._short_axis().set_minor_locator(ticker.NullLocator())\n\n    def minorticks_off(self):\n        \"\"\"Turn the minor ticks of the colorbar off.\"\"\"\n        self._minorlocator = ticker.NullLocator()\n        self._long_axis().set_minor_locator(self._minorlocator)\n\n    def set_label(self, label, *, loc=None, **kwargs):\n        \"\"\"\n        Add a label to the long axis of the colorbar.\n\n        Parameters\n        ----------\n        label : str\n            The label text.\n        loc : str, optional\n            The location of the label.\n\n            - For horizontal orientation one of {'left', 'center', 'right'}\n            - For vertical orientation one of {'bottom', 'center', 'top'}\n\n            Defaults to :rc:`xaxis.labellocation` or :rc:`yaxis.labellocation`\n            depending on the orientation.\n        **kwargs\n            Keyword arguments are passed to `~.Axes.set_xlabel` /\n            `~.Axes.set_ylabel`.\n            Supported keywords are *labelpad* and `.Text` properties.\n        \"\"\"\n        if self.orientation == \"vertical\":\n            self.ax.set_ylabel(label, loc=loc, **kwargs)\n        else:\n            self.ax.set_xlabel(label, loc=loc, **kwargs)\n        self.stale = True\n\n    def set_alpha(self, alpha):\n        \"\"\"\n        Set the transparency between 0 (transparent) and 1 (opaque).\n\n        If an array is provided, *alpha* will be set to None to use the\n        transparency values associated with the colormap.\n        \"\"\"\n        self.alpha = None if isinstance(alpha, np.ndarray) else alpha\n\n    def _set_scale(self, scale, **kwargs):\n        \"\"\"\n        Set the colorbar long axis scale.\n\n        Parameters\n        ----------\n        scale : {\"linear\", \"log\", \"symlog\", \"logit\", ...} or `.ScaleBase`\n            The axis scale type to apply.\n\n        **kwargs\n            Different keyword arguments are accepted, depending on the scale.\n            See the respective class keyword arguments:\n\n            - `matplotlib.scale.LinearScale`\n            - `matplotlib.scale.LogScale`\n            - `matplotlib.scale.SymmetricalLogScale`\n            - `matplotlib.scale.LogitScale`\n            - `matplotlib.scale.FuncScale`\n\n        Notes\n        -----\n        By default, Matplotlib supports the above-mentioned scales.\n        Additionally, custom scales may be registered using\n        `matplotlib.scale.register_scale`. These scales can then also\n        be used here.\n        \"\"\"\n        self._long_axis()._set_axes_scale(scale, **kwargs)\n\n    def remove(self):\n        \"\"\"\n        Remove this colorbar from the figure.\n\n        If the colorbar was created with ``use_gridspec=True`` the previous\n        gridspec is restored.\n        \"\"\"\n        if hasattr(self.ax, '_colorbar_info'):\n            parents = self.ax._colorbar_info['parents']\n            for a in parents:\n                if self.ax in a._colorbars:\n                    a._colorbars.remove(self.ax)\n\n        self.ax.remove()\n\n        self.mappable.callbacks.disconnect(self.mappable.colorbar_cid)\n        self.mappable.colorbar = None\n        self.mappable.colorbar_cid = None\n        # Remove the extension callbacks\n        self.ax.callbacks.disconnect(self._extend_cid1)\n        self.ax.callbacks.disconnect(self._extend_cid2)\n\n        try:\n            ax = self.mappable.axes\n        except AttributeError:\n            return\n        try:\n            gs = ax.get_subplotspec().get_gridspec()\n            subplotspec = gs.get_topmost_subplotspec()\n        except AttributeError:\n            # use_gridspec was False\n            pos = ax.get_position(original=True)\n            ax._set_position(pos)\n        else:\n            # use_gridspec was True\n            ax.set_subplotspec(subplotspec)\n\n    def _process_values(self):\n        \"\"\"\n        Set `_boundaries` and `_values` based on the self.boundaries and\n        self.values if not None, or based on the size of the colormap and\n        the vmin/vmax of the norm.\n        \"\"\"\n        if self.values is not None:\n            # set self._boundaries from the values...\n            self._values = np.array(self.values)\n            if self.boundaries is None:\n                # bracket values by 1/2 dv:\n                b = np.zeros(len(self.values) + 1)\n                b[1:-1] = 0.5 * (self._values[:-1] + self._values[1:])\n                b[0] = 2.0 * b[1] - b[2]\n                b[-1] = 2.0 * b[-2] - b[-3]\n                self._boundaries = b\n                return\n            self._boundaries = np.array(self.boundaries)\n            return\n\n        # otherwise values are set from the boundaries\n        if isinstance(self.norm, colors.BoundaryNorm):\n            b = self.norm.boundaries\n        elif isinstance(self.norm, colors.NoNorm):\n            # NoNorm has N blocks, so N+1 boundaries, centered on integers:\n            b = np.arange(self.cmap.N + 1) - .5\n        elif self.boundaries is not None:\n            b = self.boundaries\n        else:\n            # otherwise make the boundaries from the size of the cmap:\n            N = self.cmap.N + 1\n            b, _ = self._uniform_y(N)\n        # add extra boundaries if needed:\n        if self._extend_lower():\n            b = np.hstack((b[0] - 1, b))\n        if self._extend_upper():\n            b = np.hstack((b, b[-1] + 1))\n\n        # transform from 0-1 to vmin-vmax:\n        if not self.norm.scaled():\n            self.norm.vmin = 0\n            self.norm.vmax = 1\n        self.norm.vmin, self.norm.vmax = mtransforms.nonsingular(\n            self.norm.vmin, self.norm.vmax, expander=0.1)\n        if (not isinstance(self.norm, colors.BoundaryNorm) and\n                (self.boundaries is None)):\n            b = self.norm.inverse(b)\n\n        self._boundaries = np.asarray(b, dtype=float)\n        self._values = 0.5 * (self._boundaries[:-1] + self._boundaries[1:])\n        if isinstance(self.norm, colors.NoNorm):\n            self._values = (self._values + 0.00001).astype(np.int16)\n\n    def _mesh(self):\n        \"\"\"\n        Return the coordinate arrays for the colorbar pcolormesh/patches.\n\n        These are scaled between vmin and vmax, and already handle colorbar\n        orientation.\n        \"\"\"\n        y, _ = self._proportional_y()\n        # Use the vmin and vmax of the colorbar, which may not be the same\n        # as the norm. There are situations where the colormap has a\n        # narrower range than the colorbar and we want to accommodate the\n        # extra contours.\n        if (isinstance(self.norm, (colors.BoundaryNorm, colors.NoNorm))\n                or self.boundaries is not None):\n            # not using a norm.\n            y = y * (self.vmax - self.vmin) + self.vmin\n        else:\n            # Update the norm values in a context manager as it is only\n            # a temporary change and we don't want to propagate any signals\n            # attached to the norm (callbacks.blocked).\n            with self.norm.callbacks.blocked(), \\\n                    cbook._setattr_cm(self.norm,\n                                      vmin=self.vmin,\n                                      vmax=self.vmax):\n                y = self.norm.inverse(y)\n        self._y = y\n        X, Y = np.meshgrid([0., 1.], y)\n        if self.orientation == 'vertical':\n            return (X, Y)\n        else:\n            return (Y, X)\n\n    def _forward_boundaries(self, x):\n        # map boundaries equally between 0 and 1...\n        b = self._boundaries\n        y = np.interp(x, b, np.linspace(0, 1, len(b)))\n        # the following avoids ticks in the extends:\n        eps = (b[-1] - b[0]) * 1e-6\n        # map these _well_ out of bounds to keep any ticks out\n        # of the extends region...\n        y[x < b[0]-eps] = -1\n        y[x > b[-1]+eps] = 2\n        return y\n\n    def _inverse_boundaries(self, x):\n        # invert the above...\n        b = self._boundaries\n        return np.interp(x, np.linspace(0, 1, len(b)), b)\n\n    def _reset_locator_formatter_scale(self):\n        \"\"\"\n        Reset the locator et al to defaults.  Any user-hardcoded changes\n        need to be re-entered if this gets called (either at init, or when\n        the mappable normal gets changed: Colorbar.update_normal)\n        \"\"\"\n        self._process_values()\n        self._locator = None\n        self._minorlocator = None\n        self._formatter = None\n        self._minorformatter = None\n        if (isinstance(self.mappable, contour.ContourSet) and\n                isinstance(self.norm, colors.LogNorm)):\n            # if contours have lognorm, give them a log scale...\n            self._set_scale('log')\n        elif (self.boundaries is not None or\n                isinstance(self.norm, colors.BoundaryNorm)):\n            if self.spacing == 'uniform':\n                funcs = (self._forward_boundaries, self._inverse_boundaries)\n                self._set_scale('function', functions=funcs)\n            elif self.spacing == 'proportional':\n                self._set_scale('linear')\n        elif getattr(self.norm, '_scale', None):\n            # use the norm's scale (if it exists and is not None):\n            self._set_scale(self.norm._scale)\n        elif type(self.norm) is colors.Normalize:\n            # plain Normalize:\n            self._set_scale('linear')\n        else:\n            # norm._scale is None or not an attr: derive the scale from\n            # the Norm:\n            funcs = (self.norm, self.norm.inverse)\n            self._set_scale('function', functions=funcs)\n\n    def _locate(self, x):\n        \"\"\"\n        Given a set of color data values, return their\n        corresponding colorbar data coordinates.\n        \"\"\"\n        if isinstance(self.norm, (colors.NoNorm, colors.BoundaryNorm)):\n            b = self._boundaries\n            xn = x\n        else:\n            # Do calculations using normalized coordinates so\n            # as to make the interpolation more accurate.\n            b = self.norm(self._boundaries, clip=False).filled()\n            xn = self.norm(x, clip=False).filled()\n\n        bunique = b[self._inside]\n        yunique = self._y\n\n        z = np.interp(xn, bunique, yunique)\n        return z\n\n    # trivial helpers\n\n    def _uniform_y(self, N):\n        \"\"\"\n        Return colorbar data coordinates for *N* uniformly\n        spaced boundaries, plus extension lengths if required.\n        \"\"\"\n        automin = automax = 1. / (N - 1.)\n        extendlength = self._get_extension_lengths(self.extendfrac,\n                                                   automin, automax,\n                                                   default=0.05)\n        y = np.linspace(0, 1, N)\n        return y, extendlength\n\n    def _proportional_y(self):\n        \"\"\"\n        Return colorbar data coordinates for the boundaries of\n        a proportional colorbar, plus extension lengths if required:\n        \"\"\"\n        if (isinstance(self.norm, colors.BoundaryNorm) or\n                self.boundaries is not None):\n            y = (self._boundaries - self._boundaries[self._inside][0])\n            y = y / (self._boundaries[self._inside][-1] -\n                     self._boundaries[self._inside][0])\n            # need yscaled the same as the axes scale to get\n            # the extend lengths.\n            if self.spacing == 'uniform':\n                yscaled = self._forward_boundaries(self._boundaries)\n            else:\n                yscaled = y\n        else:\n            y = self.norm(self._boundaries.copy())\n            y = np.ma.filled(y, np.nan)\n            # the norm and the scale should be the same...\n            yscaled = y\n        y = y[self._inside]\n        yscaled = yscaled[self._inside]\n        # normalize from 0..1:\n        norm = colors.Normalize(y[0], y[-1])\n        y = np.ma.filled(norm(y), np.nan)\n        norm = colors.Normalize(yscaled[0], yscaled[-1])\n        yscaled = np.ma.filled(norm(yscaled), np.nan)\n        # make the lower and upper extend lengths proportional to the lengths\n        # of the first and last boundary spacing (if extendfrac='auto'):\n        automin = yscaled[1] - yscaled[0]\n        automax = yscaled[-1] - yscaled[-2]\n        extendlength = [0, 0]\n        if self._extend_lower() or self._extend_upper():\n            extendlength = self._get_extension_lengths(\n                    self.extendfrac, automin, automax, default=0.05)\n        return y, extendlength\n\n    def _get_extension_lengths(self, frac, automin, automax, default=0.05):\n        \"\"\"\n        Return the lengths of colorbar extensions.\n\n        This is a helper method for _uniform_y and _proportional_y.\n        \"\"\"\n        # Set the default value.\n        extendlength = np.array([default, default])\n        if isinstance(frac, str):\n            _api.check_in_list(['auto'], extendfrac=frac.lower())\n            # Use the provided values when 'auto' is required.\n            extendlength[:] = [automin, automax]\n        elif frac is not None:\n            try:\n                # Try to set min and max extension fractions directly.\n                extendlength[:] = frac\n                # If frac is a sequence containing None then NaN may\n                # be encountered. This is an error.\n                if np.isnan(extendlength).any():\n                    raise ValueError()\n            except (TypeError, ValueError) as err:\n                # Raise an error on encountering an invalid value for frac.\n                raise ValueError('invalid value for extendfrac') from err\n        return extendlength\n\n    def _extend_lower(self):\n        \"\"\"Return whether the lower limit is open ended.\"\"\"\n        minmax = \"max\" if self._long_axis().get_inverted() else \"min\"\n        return self.extend in ('both', minmax)\n\n    def _extend_upper(self):\n        \"\"\"Return whether the upper limit is open ended.\"\"\"\n        minmax = \"min\" if self._long_axis().get_inverted() else \"max\"\n        return self.extend in ('both', minmax)\n\n(B)                 linewidth=0, antialiased=False,\n                transform=self.ax.transAxes,\n                hatch=hatches[0], clip_on=False,\n                # Place it right behind the standard patches, which is\n                # needed if we updated the extends\n                zorder=np.nextafter(self.ax.patch.zorder, -np.inf))\n            self.ax.add_patch(patch)\n            self._extend_patches.append(patch)\n            # remove first hatch that goes into the extend patch\n            hatches = hatches[1:]\n        if self._extend_upper():\n            if not self.extendrect:\n                # triangle\n                xy = np.array([[0, 1], [0.5, top], [1, 1]])\n            else:\n                # rectangle\n                xy = np.array([[0, 1], [0, top], [1, top], [1, 1]])\n            if self.orientation == 'horizontal':\n                xy = xy[:, ::-1]\n            # add the patch\n            val = 0 if self._long_axis().get_inverted() else -1\n            color = self.cmap(self.norm(self._values[val]))\n            hatch_idx = len(self._y) - 1\n            patch = mpatches.PathPatch(\n                mpath.Path(xy), facecolor=color, alpha=self.alpha,\n                linewidth=0, antialiased=False,\n                transform=self.ax.transAxes, hatch=hatches[hatch_idx],\n                clip_on=False,\n                # Place it right behind the standard patches, which is\n                # needed if we updated the extends\n                zorder=np.nextafter(self.ax.patch.zorder, -np.inf))\n            self.ax.add_patch(patch)\n            self._extend_patches.append(patch)\n\n        self._update_dividers()\n\n    def add_lines(self, *args, **kwargs):\n        \"\"\"\n        Draw lines on the colorbar.\n\n        The lines are appended to the list :attr:`lines`.\n\n        Parameters\n        ----------\n        levels : array-like\n            The positions of the lines.\n        colors : color or list of colors\n            Either a single color applying to all lines or one color value for\n            each line.\n        linewidths : float or array-like\n            Either a single linewidth applying to all lines or one linewidth\n            for each line.\n        erase : bool, default: True\n            Whether to remove any previously added lines.\n\n        Notes\n        -----\n        Alternatively, this method can also be called with the signature\n        ``colorbar.add_lines(contour_set, erase=True)``, in which case\n        *levels*, *colors*, and *linewidths* are taken from *contour_set*.\n        \"\"\"\n        params = _api.select_matching_signature(\n            [lambda self, CS, erase=True: locals(),\n             lambda self, levels, colors, linewidths, erase=True: locals()],\n            self, *args, **kwargs)\n        if \"CS\" in params:\n            self, CS, erase = params.values()\n            if not isinstance(CS, contour.ContourSet) or CS.filled:\n                raise ValueError(\"If a single artist is passed to add_lines, \"\n                                 \"it must be a ContourSet of lines\")\n            # TODO: Make colorbar lines auto-follow changes in contour lines.\n            return self.add_lines(\n                CS.levels,\n                CS.to_rgba(CS.cvalues, CS.alpha),\n                [coll.get_linewidths()[0] for coll in CS.collections],\n                erase=erase)\n        else:\n            self, levels, colors, linewidths, erase = params.values()\n\n        y = self._locate(levels)\n        rtol = (self._y[-1] - self._y[0]) * 1e-10\n        igood = (y < self._y[-1] + rtol) & (y > self._y[0] - rtol)\n        y = y[igood]\n        if np.iterable(colors):\n            colors = np.asarray(colors)[igood]\n        if np.iterable(linewidths):\n            linewidths = np.asarray(linewidths)[igood]\n        X, Y = np.meshgrid([0, 1], y)\n        if self.orientation == 'vertical':\n            xy = np.stack([X, Y], axis=-1)\n        else:\n            xy = np.stack([Y, X], axis=-1)\n        col = collections.LineCollection(xy, linewidths=linewidths,\n                                         colors=colors)\n\n        if erase and self.lines:\n            for lc in self.lines:\n                lc.remove()\n            self.lines = []\n        self.lines.append(col)\n\n        # make a clip path that is just a linewidth bigger than the axes...\n        fac = np.max(linewidths) / 72\n        xy = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n        inches = self.ax.get_figure().dpi_scale_trans\n        # do in inches:\n        xy = inches.inverted().transform(self.ax.transAxes.transform(xy))\n        xy[[0, 1, 4], 1] -= fac\n        xy[[2, 3], 1] += fac\n        # back to axes units...\n        xy = self.ax.transAxes.inverted().transform(inches.transform(xy))\n        col.set_clip_path(mpath.Path(xy, closed=True),\n                          self.ax.transAxes)\n        self.ax.add_collection(col)\n        self.stale = True\n\n    def update_ticks(self):\n        \"\"\"\n        Set up the ticks and ticklabels. This should not be needed by users.\n        \"\"\"\n        # Get the locator and formatter; defaults to self._locator if not None.\n        self._get_ticker_locator_formatter()\n        self._long_axis().set_major_locator(self._locator)\n        self._long_axis().set_minor_locator(self._minorlocator)\n        self._long_axis().set_major_formatter(self._formatter)\n\n    def _get_ticker_locator_formatter(self):\n        \"\"\"\n        Return the ``locator`` and ``formatter`` of the colorbar.\n\n        If they have not been defined (i.e. are *None*), the formatter and\n        locator are retrieved from the axis, or from the value of the\n        boundaries for a boundary norm.\n\n        Called by update_ticks...\n        \"\"\"\n        locator = self._locator\n        formatter = self._formatter\n        minorlocator = self._minorlocator\n        if isinstance(self.norm, colors.BoundaryNorm):\n            b = self.norm.boundaries\n            if locator is None:\n                locator = ticker.FixedLocator(b, nbins=10)\n            if minorlocator is None:\n                minorlocator = ticker.FixedLocator(b)\n        elif isinstance(self.norm, colors.NoNorm):\n            if locator is None:\n                # put ticks on integers between the boundaries of NoNorm\n                nv = len(self._values)\n                base = 1 + int(nv / 10)\n                locator = ticker.IndexLocator(base=base, offset=.5)\n        elif self.boundaries is not None:\n            b = self._boundaries[self._inside]\n            if locator is None:\n                locator = ticker.FixedLocator(b, nbins=10)\n        else:  # most cases:\n            if locator is None:\n                # we haven't set the locator explicitly, so use the default\n                # for this axis:\n                locator = self._long_axis().get_major_locator()\n            if minorlocator is None:\n                minorlocator = self._long_axis().get_minor_locator()\n\n        if minorlocator is None:\n            minorlocator = ticker.NullLocator()\n\n        if formatter is None:\n            formatter = self._long_axis().get_major_formatter()\n\n        self._locator = locator\n        self._formatter = formatter\n        self._minorlocator = minorlocator\n        _log.debug('locator: %r', locator)\n\n    def set_ticks(self, ticks, *, labels=None, minor=False, **kwargs):\n        \"\"\"\n        Set tick locations.\n\n        Parameters\n        ----------\n        ticks : list of floats\n            List of tick locations.\n        labels : list of str, optional\n            List of tick labels. If not set, the labels show the data value.\n        minor : bool, default: False\n            If ``False``, set the major ticks; if ``True``, the minor ticks.\n        **kwargs\n            `.Text` properties for the labels. These take effect only if you\n            pass *labels*. In other cases, please use `~.Axes.tick_params`.\n        \"\"\"\n        if np.iterable(ticks):\n            self._long_axis().set_ticks(ticks, labels=labels, minor=minor,\n                                        **kwargs)\n            self._locator = self._long_axis().get_major_locator()\n        else:\n            self._locator = ticks\n            self._long_axis().set_major_locator(self._locator)\n        self.stale = True\n\n    def get_ticks(self, minor=False):\n        \"\"\"\n        Return the ticks as a list of locations.\n\n        Parameters\n        ----------\n        minor : boolean, default: False\n            if True return the minor ticks.\n        \"\"\"\n        if minor:\n            return self._long_axis().get_minorticklocs()\n        else:\n            return self._long_axis().get_majorticklocs()\n\n    def set_ticklabels(self, ticklabels, *, minor=False, **kwargs):\n        \"\"\"\n        [*Discouraged*] Set tick labels.\n\n        .. admonition:: Discouraged\n\n            The use of this method is discouraged, because of the dependency\n            on tick positions. In most cases, you'll want to use\n            ``set_ticks(positions, labels=labels)`` instead.\n\n            If you are using this method, you should always fix the tick\n            positions before, e.g. by using `.Colorbar.set_ticks` or by\n            explicitly setting a `~.ticker.FixedLocator` on the long axis\n            of the colorbar. Otherwise, ticks are free to move and the\n            labels may end up in unexpected positions.\n\n        Parameters\n        ----------\n        ticklabels : sequence of str or of `.Text`\n            Texts for labeling each tick location in the sequence set by\n            `.Colorbar.set_ticks`; the number of labels must match the number\n            of locations.\n\n        update_ticks : bool, default: True\n            This keyword argument is ignored and will be removed.\n            Deprecated\n\n         minor : bool\n            If True, set minor ticks instead of major ticks.\n\n        **kwargs\n            `.Text` properties for the labels.\n        \"\"\"\n        self._long_axis().set_ticklabels(ticklabels, minor=minor, **kwargs)\n\n    def minorticks_on(self):\n        \"\"\"\n        Turn on colorbar minor ticks.\n        \"\"\"\n        self.ax.minorticks_on()\n        self._short_axis().set_minor_locator(ticker.NullLocator())\n\n    def minorticks_off(self):\n        \"\"\"Turn the minor ticks of the colorbar off.\"\"\"\n        self._minorlocator = ticker.NullLocator()\n        self._long_axis().set_minor_locator(self._minorlocator)\n\n    def set_label(self, label, *, loc=None, **kwargs):\n        \"\"\"\n        Add a label to the long axis of the colorbar.\n\n        Parameters\n        ----------\n        label : str\n            The label text.\n        loc : str, optional\n            The location of the label.\n\n            - For horizontal orientation one of {'left', 'center', 'right'}\n            - For vertical orientation one of {'bottom', 'center', 'top'}\n\n            Defaults to :rc:`xaxis.labellocation` or :rc:`yaxis.labellocation`\n            depending on the orientation.\n        **kwargs\n            Keyword arguments are passed to `~.Axes.set_xlabel` /\n            `~.Axes.set_ylabel`.\n            Supported keywords are *labelpad* and `.Text` properties.\n        \"\"\"\n        if self.orientation == \"vertical\":\n            self.ax.set_ylabel(label, loc=loc, **kwargs)\n        else:\n            self.ax.set_xlabel(label, loc=loc, **kwargs)\n        self.stale = True\n\n    def set_alpha(self, alpha):\n        \"\"\"\n        Set the transparency between 0 (transparent) and 1 (opaque).\n\n        If an array is provided, *alpha* will be set to None to use the\n        transparency values associated with the colormap.\n        \"\"\"\n        self.alpha = None if isinstance(alpha, np.ndarray) else alpha\n\n    def _set_scale(self, scale, **kwargs):\n        \"\"\"\n        Set the colorbar long axis scale.\n\n        Parameters\n        ----------\n        scale : {\"linear\", \"log\", \"symlog\", \"logit\", ...} or `.ScaleBase`\n            The axis scale type to apply.\n\n        **kwargs\n            Different keyword arguments are accepted, depending on the scale.\n            See the respective class keyword arguments:\n\n            - `matplotlib.scale.LinearScale`\n            - `matplotlib.scale.LogScale`\n            - `matplotlib.scale.SymmetricalLogScale`\n            - `matplotlib.scale.LogitScale`\n            - `matplotlib.scale.FuncScale`\n\n        Notes\n        -----\n        By default, Matplotlib supports the above-mentioned scales.\n        Additionally, custom scales may be registered using\n        `matplotlib.scale.register_scale`. These scales can then also\n        be used here.\n        \"\"\"\n        self._long_axis()._set_axes_scale(scale, **kwargs)\n\n    def remove(self):\n        \"\"\"\n        Remove this colorbar from the figure.\n\n        If the colorbar was created with ``use_gridspec=True`` the previous\n        gridspec is restored.\n        \"\"\"\n        if hasattr(self.ax, '_colorbar_info'):\n            parents = self.ax._colorbar_info['parents']\n            for a in parents:\n                if self.ax in a._colorbars:\n                    a._colorbars.remove(self.ax)\n\n        self.ax.remove()\n\n        self.mappable.callbacks.disconnect(self.mappable.colorbar_cid)\n        self.mappable.colorbar = None\n        self.mappable.colorbar_cid = None\n        # Remove the extension callbacks\n        self.ax.callbacks.disconnect(self._extend_cid1)\n        self.ax.callbacks.disconnect(self._extend_cid2)\n\n        try:\n            ax = self.mappable.axes\n        except AttributeError:\n            return\n        try:\n            gs = ax.get_subplotspec().get_gridspec()\n            subplotspec = gs.get_topmost_subplotspec()\n        except AttributeError:\n            # use_gridspec was False\n            pos = ax.get_position(original=True)\n            ax._set_position(pos)\n        else:\n            # use_gridspec was True\n            ax.set_subplotspec(subplotspec)\n\n    def _process_values(self):\n        \"\"\"\n        Set `_boundaries` and `_values` based on the self.boundaries and\n        self.values if not None, or based on the size of the colormap and\n        the vmin/vmax of the norm.\n        \"\"\"\n        if self.values is not None:\n            # set self._boundaries from the values...\n            self._values = np.array(self.values)\n            if self.boundaries is None:\n                # bracket values by 1/2 dv:\n                b = np.zeros(len(self.values) + 1)\n                b[1:-1] = 0.5 * (self._values[:-1] + self._values[1:])\n                b[0] = 2.0 * b[1] - b[2]\n                b[-1] = 2.0 * b[-2] - b[-3]\n                self._boundaries = b\n                return\n            self._boundaries = np.array(self.boundaries)\n            return\n\n        # otherwise values are set from the boundaries\n        if isinstance(self.norm, colors.BoundaryNorm):\n            b = self.norm.boundaries\n        elif isinstance(self.norm, colors.NoNorm):\n            # NoNorm has N blocks, so N+1 boundaries, centered on integers:\n            b = np.arange(self.cmap.N + 1) - .5\n        elif self.boundaries is not None:\n            b = self.boundaries\n        else:\n            # otherwise make the boundaries from the size of the cmap:\n            N = self.cmap.N + 1\n            b, _ = self._uniform_y(N)\n        # add extra boundaries if needed:\n        if self._extend_lower():\n            b = np.hstack((b[0] - 1, b))\n        if self._extend_upper():\n            b = np.hstack((b, b[-1] + 1))\n\n        # transform from 0-1 to vmin-vmax:\n        if not self.norm.scaled():\n            self.norm.vmin = 0\n            self.norm.vmax = 1\n        self.norm.vmin, self.norm.vmax = mtransforms.nonsingular(\n            self.norm.vmin, self.norm.vmax, expander=0.1)\n        if (not isinstance(self.norm, colors.BoundaryNorm) and\n                (self.boundaries is None)):\n            b = self.norm.inverse(b)\n\n        self._boundaries = np.asarray(b, dtype=float)\n        self._values = 0.5 * (self._boundaries[:-1] + self._boundaries[1:])\n        if isinstance(self.norm, colors.NoNorm):\n            self._values = (self._values + 0.00001).astype(np.int16)\n\n    def _mesh(self):\n        \"\"\"\n        Return the coordinate arrays for the colorbar pcolormesh/patches.\n\n        These are scaled between vmin and vmax, and already handle colorbar\n        orientation.\n        \"\"\"\n        y, _ = self._proportional_y()\n        # Use the vmin and vmax of the colorbar, which may not be the same\n        # as the norm. There are situations where the colormap has a\n        # narrower range than the colorbar and we want to accommodate the\n        # extra contours.\n        if (isinstance(self.norm, (colors.BoundaryNorm, colors.NoNorm))\n                or self.boundaries is not None):\n            # not using a norm.\n            y = y * (self.vmax - self.vmin) + self.vmin\n        else:\n            # Update the norm values in a context manager as it is only\n            # a temporary change and we don't want to propagate any signals\n            # attached to the norm (callbacks.blocked).\n            with self.norm.callbacks.blocked(), \\\n                    cbook._setattr_cm(self.norm,\n                                      vmin=self.vmin,\n                                      vmax=self.vmax):\n                y = self.norm.inverse(y)\n        self._y = y\n        X, Y = np.meshgrid([0., 1.], y)\n        if self.orientation == 'vertical':\n            return (X, Y)\n        else:\n            return (Y, X)\n\n    def _forward_boundaries(self, x):\n        # map boundaries equally between 0 and 1...\n        b = self._boundaries\n        y = np.interp(x, b, np.linspace(0, 1, len(b)))\n        # the following avoids ticks in the extends:\n        eps = (b[-1] - b[0]) * 1e-6\n        # map these _well_ out of bounds to keep any ticks out\n        # of the extends region...\n        y[x < b[0]-eps] = -1\n        y[x > b[-1]+eps] = 2\n        return y\n\n    def _inverse_boundaries(self, x):\n        # invert the above...\n        b = self._boundaries\n        return np.interp(x, np.linspace(0, 1, len(b)), b)\n\n    def _reset_locator_formatter_scale(self):\n        \"\"\"\n        Reset the locator et al to defaults.  Any user-hardcoded changes\n        need to be re-entered if this gets called (either at init, or when\n        the mappable normal gets changed: Colorbar.update_normal)\n        \"\"\"\n        self._process_values()\n        self._locator = None\n        self._minorlocator = None\n        self._formatter = None\n        self._minorformatter = None\n        if (isinstance(self.mappable, contour.ContourSet) and\n                isinstance(self.norm, colors.LogNorm)):\n            # if contours have lognorm, give them a log scale...\n            self._set_scale('log')\n        elif (self.boundaries is not None or\n                isinstance(self.norm, colors.BoundaryNorm)):\n            if self.spacing == 'uniform':\n                funcs = (self._forward_boundaries, self._inverse_boundaries)\n                self._set_scale('function', functions=funcs)\n            elif self.spacing == 'proportional':\n                self._set_scale('linear')\n        elif getattr(self.norm, '_scale', None):\n            # use the norm's scale (if it exists and is not None):\n            self._set_scale(self.norm._scale)\n        elif type(self.norm) is colors.Normalize:\n            # plain Normalize:\n            self._set_scale('linear')\n        else:\n            # norm._scale is None or not an attr: derive the scale from\n            # the Norm:\n            funcs = (self.norm, self.norm.inverse)\n            self._set_scale('function', functions=funcs)\n\n    def _locate(self, x):\n        \"\"\"\n        Given a set of color data values, return their\n        corresponding colorbar data coordinates.\n        \"\"\"\n        if isinstance(self.norm, (colors.NoNorm, colors.BoundaryNorm)):\n            b = self._boundaries\n            xn = x\n        else:\n            # Do calculations using normalized coordinates so\n            # as to make the interpolation more accurate.\n            b = self.norm(self._boundaries, clip=False).filled()\n            xn = self.norm(x, clip=False).filled()\n\n        bunique = b[self._inside]\n        yunique = self._y\n\n        z = np.interp(xn, bunique, yunique)\n        return z\n\n    # trivial helpers\n\n    def _uniform_y(self, N):\n        \"\"\"\n        Return colorbar data coordinates for *N* uniformly\n        spaced boundaries, plus extension lengths if required.\n        \"\"\"\n        automin = automax = 1. / (N - 1.)\n        extendlength = self._get_extension_lengths(self.extendfrac,\n                                                   automin, automax,\n                                                   default=0.05)\n        y = np.linspace(0, 1, N)\n        return y, extendlength\n\n    def _proportional_y(self):\n        \"\"\"\n        Return colorbar data coordinates for the boundaries of\n        a proportional colorbar, plus extension lengths if required:\n        \"\"\"\n        if (isinstance(self.norm, colors.BoundaryNorm) or\n                self.boundaries is not None):\n            y = (self._boundaries - self._boundaries[self._inside][0])\n            y = y / (self._boundaries[self._inside][-1] -\n                     self._boundaries[self._inside][0])\n            # need yscaled the same as the axes scale to get\n            # the extend lengths.\n            if self.spacing == 'uniform':\n                yscaled = self._forward_boundaries(self._boundaries)\n            else:\n                yscaled = y\n        else:\n            y = self.norm(self._boundaries.copy())\n            y = np.ma.filled(y, np.nan)\n            # the norm and the scale should be the same...\n            yscaled = y\n        y = y[self._inside]\n        yscaled = yscaled[self._inside]\n        # normalize from 0..1:\n        norm = colors.Normalize(y[0], y[-1])\n        y = np.ma.filled(norm(y), np.nan)\n        norm = colors.Normalize(yscaled[0], yscaled[-1])\n        yscaled = np.ma.filled(norm(yscaled), np.nan)\n        # make the lower and upper extend lengths proportional to the lengths\n        # of the first and last boundary spacing (if extendfrac='auto'):\n        automin = yscaled[1] - yscaled[0]\n        automax = yscaled[-1] - yscaled[-2]\n        extendlength = [0, 0]\n        if self._extend_lower() or self._extend_upper():\n            extendlength = self._get_extension_lengths(\n                    self.extendfrac, automin, automax, default=0.05)\n        return y, extendlength\n\n    def _get_extension_lengths(self, frac, automin, automax, default=0.05):\n        \"\"\"\n        Return the lengths of colorbar extensions.\n\n        This is a helper method for _uniform_y and _proportional_y.\n        \"\"\"\n        # Set the default value.\n        extendlength = np.array([default, default])\n        if isinstance(frac, str):\n            _api.check_in_list(['auto'], extendfrac=frac.lower())\n            # Use the provided values when 'auto' is required.\n            extendlength[:] = [automin, automax]\n        elif frac is not None:\n            try:\n                # Try to set min and max extension fractions directly.\n                extendlength[:] = frac\n                # If frac is a sequence containing None then NaN may\n                # be encountered. This is an error.\n                if np.isnan(extendlength).any():\n                    raise ValueError()\n            except (TypeError, ValueError) as err:\n                # Raise an error on encountering an invalid value for frac.\n                raise ValueError('invalid value for extendfrac') from err\n        return extendlength\n\n    def _extend_lower(self):\n        \"\"\"Return whether the lower limit is open ended.\"\"\"\n        minmax = \"max\" if self._long_axis().get_inverted() else \"min\"\n        return self.extend in ('both', minmax)\n\n    def _extend_upper(self):\n        \"\"\"Return whether the upper limit is open ended.\"\"\"\n        minmax = \"min\" if self._long_axis().get_inverted() else \"max\"\n        return self.extend in ('both', minmax)\n\n    def _long_axis(self):\n        \"\"\"Return the long axis\"\"\"\n        if self.orientation == 'vertical':\n            return self.ax.yaxis\n        return self.ax.xaxis\n\n    def _short_axis(self):\n        \"\"\"Return the short axis\"\"\"\n        if self.orientation == 'vertical':\n            return self.ax.xaxis\n        return self.ax.yaxis\n\n    def _get_view(self):\n        # docstring inherited\n        # An interactive view for a colorbar is the norm's vmin/vmax\n        return self.norm.vmin, self.norm.vmax\n\n    def _set_view(self, view):\n        # docstring inherited\n        # An interactive view for a colorbar is the norm's vmin/vmax\n        self.norm.vmin, self.norm.vmax = view\n\n    def _set_view_from_bbox(self, bbox, direction='in',\n                            mode=None, twinx=False, twiny=False):\n        # docstring inherited\n        # For colorbars, we use the zoom bbox to scale the norm's vmin/vmax\n        new_xbound, new_ybound = self.ax._prepare_view_from_bbox(\n            bbox, direction=direction, mode=mode, twinx=twinx, twiny=twiny)\n        if self.orientation == 'horizontal':\n            self.norm.vmin, self.norm.vmax = new_xbound\n        elif self.orientation == 'vertical':\n            self.norm.vmin, self.norm.vmax = new_ybound\n\n    def drag_pan(self, button, key, x, y):\n        # docstring inherited\n        points = self.ax._get_pan_points(button, key, x, y)\n        if points is not None:\n            if self.orientation == 'horizontal':\n                self.norm.vmin, self.norm.vmax = points[:, 0]\n            elif self.orientation == 'vertical':\n                self.norm.vmin, self.norm.vmax = points[:, 1]\n\n\nColorbarBase = Colorbar  # Backcompat API\n\n\ndef _normalize_location_orientation(location, orientation):\n    if location is None:\n        location = _get_ticklocation_from_orientation(orientation)\n    loc_settings = _api.check_getitem({\n        \"left\":   {\"location\": \"left\", \"anchor\": (1.0, 0.5),\n                   \"panchor\": (0.0, 0.5), \"pad\": 0.10},\n        \"right\":  {\"location\": \"right\", \"anchor\": (0.0, 0.5),\n                   \"panchor\": (1.0, 0.5), \"pad\": 0.05},\n        \"top\":    {\"location\": \"top\", \"anchor\": (0.5, 0.0),\n                   \"panchor\": (0.5, 1.0), \"pad\": 0.05},\n        \"bottom\": {\"location\": \"bottom\", \"anchor\": (0.5, 1.0),\n                   \"panchor\": (0.5, 0.0), \"pad\": 0.15},\n    }, location=location)\n    loc_settings[\"orientation\"] = _get_orientation_from_location(location)\n    if orientation is not None and orientation != loc_settings[\"orientation\"]:\n        # Allow the user to pass both if they are consistent.\n        raise TypeError(\"location and orientation are mutually exclusive\")\n    return loc_settings\n\n\ndef _get_orientation_from_location(location):\n    return _api.check_getitem(\n        {None: None, \"left\": \"vertical\", \"right\": \"vertical\",\n         \"top\": \"horizontal\", \"bottom\": \"horizontal\"}, location=location)\n\n\ndef _get_ticklocation_from_orientation(orientation):\n    return _api.check_getitem(\n        {None: \"right\", \"vertical\": \"right\", \"horizontal\": \"bottom\"},\n        orientation=orientation)\n\n\n@_docstring.interpd\ndef make_axes(parents, location=None, orientation=None, fraction=0.15,\n              shrink=1.0, aspect=20, **kwargs):\n    \"\"\"\n    Create an `~.axes.Axes` suitable for a colorbar.\n\n    The axes is placed in the figure of the *parents* axes, by resizing and\n    repositioning *parents*.\n\n    Parameters\n    ----------\n    parents : `~.axes.Axes` or iterable or `numpy.ndarray` of `~.axes.Axes`\n        The Axes to use as parents for placing the colorbar.\n    %(_make_axes_kw_doc)s\n\n    Returns\n    -------\n    cax : `~.axes.Axes`\n        The child axes.\n    kwargs : dict\n        The reduced keyword dictionary to be passed when creating the colorbar\n        instance.\n    \"\"\"\n    loc_settings = _normalize_location_orientation(location, orientation)\n    # put appropriate values into the kwargs dict for passing back to\n    # the Colorbar class\n    kwargs['orientation'] = loc_settings['orientation']\n    location = kwargs['ticklocation'] = loc_settings['location']\n\n    anchor = kwargs.pop('anchor', loc_settings['anchor'])\n    panchor = kwargs.pop('panchor', loc_settings['panchor'])\n    aspect0 = aspect\n    # turn parents into a list if it is not already.  Note we cannot\n    # use .flatten or .ravel as these copy the references rather than\n    # reuse them, leading to a memory leak\n    if isinstance(parents, np.ndarray):\n        parents = list(parents.flat)\n    elif np.iterable(parents):\n        parents = list(parents)\n    else:\n        parents = [parents]\n\n    fig = parents[0].get_figure()\n\n    pad0 = 0.05 if fig.get_constrained_layout() else loc_settings['pad']\n    pad = kwargs.pop('pad', pad0)\n\n    if not all(fig is ax.get_figure() for ax in parents):\n        raise ValueError('Unable to create a colorbar axes as not all '\n                         'parents share the same figure.')\n\n    # take a bounding box around all of the given axes\n    parents_bbox = mtransforms.Bbox.union(\n        [ax.get_position(original=True).frozen() for ax in parents])\n\n    pb = parents_bbox\n    if location in ('left', 'right'):\n        if location == 'left':\n            pbcb, _, pb1 = pb.splitx(fraction, fraction + pad)\n        else:\n            pb1, _, pbcb = pb.splitx(1 - fraction - pad, 1 - fraction)\n        pbcb = pbcb.shrunk(1.0, shrink).anchored(anchor, pbcb)\n    else:\n        if location == 'bottom':\n            pbcb, _, pb1 = pb.splity(fraction, fraction + pad)\n        else:\n            pb1, _, pbcb = pb.splity(1 - fraction - pad, 1 - fraction)\n        pbcb = pbcb.shrunk(shrink, 1.0).anchored(anchor, pbcb)\n\n        # define the aspect ratio in terms of y's per x rather than x's per y\n        aspect = 1.0 / aspect\n\n    # define a transform which takes us from old axes coordinates to\n    # new axes coordinates\n    shrinking_trans = mtransforms.BboxTransform(parents_bbox, pb1)\n\n    # transform each of the axes in parents using the new transform\n    for ax in parents:\n        new_posn = shrinking_trans.transform(ax.get_position(original=True))\n        new_posn = mtransforms.Bbox(new_posn)\n        ax._set_position(new_posn)\n        if panchor is not False:\n            ax.set_anchor(panchor)\n\n    cax = fig.add_axes(pbcb, label=\"<colorbar>\")\n    for a in parents:\n        # tell the parent it has a colorbar\n        a._colorbars += [cax]\n    cax._colorbar_info = dict(\n        parents=parents,\n        location=location,\n        shrink=shrink,\n        anchor=anchor,\n        panchor=panchor,\n        fraction=fraction,\n        aspect=aspect0,\n        pad=pad)\n    # and we need to set the aspect ratio by hand...\n    cax.set_anchor(anchor)\n    cax.set_box_aspect(aspect)\n    cax.set_aspect('auto')\n\n    return cax, kwargs\n\n\n@_docstring.interpd\ndef make_axes_gridspec(parent, *, location=None, orientation=None,\n                       fraction=0.15, shrink=1.0, aspect=20, **kwargs):\n    \"\"\"\n    Create an `~.axes.Axes` suitable for a colorbar.\n\n    The axes is placed in the figure of the *parent* axes, by resizing and\n    repositioning *parent*.\n\n    This function is similar to `.make_axes` and mostly compatible with it.\n    Primary differences are\n\n    - `.make_axes_gridspec` requires the *parent* to have a subplotspec.\n    - `.make_axes` positions the axes in figure coordinates;\n      `.make_axes_gridspec` positions it using a subplotspec.\n    - `.make_axes` updates the position of the parent.  `.make_axes_gridspec`\n      replaces the parent gridspec with a new one.\n\n(C)         if mappable is None:\n            mappable = cm.ScalarMappable(norm=norm, cmap=cmap)\n\n        # Ensure the given mappable's norm has appropriate vmin and vmax\n        # set even if mappable.draw has not yet been called.\n        if mappable.get_array() is not None:\n            mappable.autoscale_None()\n\n        self.mappable = mappable\n        cmap = mappable.cmap\n        norm = mappable.norm\n\n        if isinstance(mappable, contour.ContourSet):\n            cs = mappable\n            alpha = cs.get_alpha()\n            boundaries = cs._levels\n            values = cs.cvalues\n            extend = cs.extend\n            filled = cs.filled\n            if ticks is None:\n                ticks = ticker.FixedLocator(cs.levels, nbins=10)\n        elif isinstance(mappable, martist.Artist):\n            alpha = mappable.get_alpha()\n\n        mappable.colorbar = self\n        mappable.colorbar_cid = mappable.callbacks.connect(\n            'changed', self.update_normal)\n\n        location_orientation = _get_orientation_from_location(location)\n\n        _api.check_in_list(\n            [None, 'vertical', 'horizontal'], orientation=orientation)\n        _api.check_in_list(\n            ['auto', 'left', 'right', 'top', 'bottom'],\n            ticklocation=ticklocation)\n        _api.check_in_list(\n            ['uniform', 'proportional'], spacing=spacing)\n\n        if location_orientation is not None and orientation is not None:\n            if location_orientation != orientation:\n                raise TypeError(\n                    \"location and orientation are mutually exclusive\")\n        else:\n            orientation = orientation or location_orientation or \"vertical\"\n\n        self.ax = ax\n        self.ax._axes_locator = _ColorbarAxesLocator(self)\n\n        if extend is None:\n            if (not isinstance(mappable, contour.ContourSet)\n                    and getattr(cmap, 'colorbar_extend', False) is not False):\n                extend = cmap.colorbar_extend\n            elif hasattr(norm, 'extend'):\n                extend = norm.extend\n            else:\n                extend = 'neither'\n        self.alpha = None\n        # Call set_alpha to handle array-like alphas properly\n        self.set_alpha(alpha)\n        self.cmap = cmap\n        self.norm = norm\n        self.values = values\n        self.boundaries = boundaries\n        self.extend = extend\n        self._inside = _api.check_getitem(\n            {'neither': slice(0, None), 'both': slice(1, -1),\n             'min': slice(1, None), 'max': slice(0, -1)},\n            extend=extend)\n        self.spacing = spacing\n        self.orientation = orientation\n        self.drawedges = drawedges\n        self._filled = filled\n        self.extendfrac = extendfrac\n        self.extendrect = extendrect\n        self._extend_patches = []\n        self.solids = None\n        self.solids_patches = []\n        self.lines = []\n\n        for spine in self.ax.spines.values():\n            spine.set_visible(False)\n        self.outline = self.ax.spines['outline'] = _ColorbarSpine(self.ax)\n\n        self.dividers = collections.LineCollection(\n            [],\n            colors=[mpl.rcParams['axes.edgecolor']],\n            linewidths=[0.5 * mpl.rcParams['axes.linewidth']],\n            clip_on=False)\n        self.ax.add_collection(self.dividers)\n\n        self._locator = None\n        self._minorlocator = None\n        self._formatter = None\n        self._minorformatter = None\n\n        if ticklocation == 'auto':\n            ticklocation = _get_ticklocation_from_orientation(\n                orientation) if location is None else location\n        self.ticklocation = ticklocation\n\n        self.set_label(label)\n        self._reset_locator_formatter_scale()\n\n        if np.iterable(ticks):\n            self._locator = ticker.FixedLocator(ticks, nbins=len(ticks))\n        else:\n            self._locator = ticks\n\n        if isinstance(format, str):\n            # Check format between FormatStrFormatter and StrMethodFormatter\n            try:\n                self._formatter = ticker.FormatStrFormatter(format)\n                _ = self._formatter(0)\n            except TypeError:\n                self._formatter = ticker.StrMethodFormatter(format)\n        else:\n            self._formatter = format  # Assume it is a Formatter or None\n        self._draw_all()\n\n        if isinstance(mappable, contour.ContourSet) and not mappable.filled:\n            self.add_lines(mappable)\n\n        # Link the Axes and Colorbar for interactive use\n        self.ax._colorbar = self\n        # Don't navigate on any of these types of mappables\n        if (isinstance(self.norm, (colors.BoundaryNorm, colors.NoNorm)) or\n                isinstance(self.mappable, contour.ContourSet)):\n            self.ax.set_navigate(False)\n\n        # These are the functions that set up interactivity on this colorbar\n        self._interactive_funcs = [\"_get_view\", \"_set_view\",\n                                   \"_set_view_from_bbox\", \"drag_pan\"]\n        for x in self._interactive_funcs:\n            setattr(self.ax, x, getattr(self, x))\n        # Set the cla function to the cbar's method to override it\n        self.ax.cla = self._cbar_cla\n        # Callbacks for the extend calculations to handle inverting the axis\n        self._extend_cid1 = self.ax.callbacks.connect(\n            \"xlim_changed\", self._do_extends)\n        self._extend_cid2 = self.ax.callbacks.connect(\n            \"ylim_changed\", self._do_extends)\n\n    @property\n    def locator(self):\n        \"\"\"Major tick `.Locator` for the colorbar.\"\"\"\n        return self._long_axis().get_major_locator()\n\n    @locator.setter\n    def locator(self, loc):\n        self._long_axis().set_major_locator(loc)\n        self._locator = loc\n\n    @property\n    def minorlocator(self):\n        \"\"\"Minor tick `.Locator` for the colorbar.\"\"\"\n        return self._long_axis().get_minor_locator()\n\n    @minorlocator.setter\n    def minorlocator(self, loc):\n        self._long_axis().set_minor_locator(loc)\n        self._minorlocator = loc\n\n    @property\n    def formatter(self):\n        \"\"\"Major tick label `.Formatter` for the colorbar.\"\"\"\n        return self._long_axis().get_major_formatter()\n\n    @formatter.setter\n    def formatter(self, fmt):\n        self._long_axis().set_major_formatter(fmt)\n        self._formatter = fmt\n\n    @property\n    def minorformatter(self):\n        \"\"\"Minor tick `.Formatter` for the colorbar.\"\"\"\n        return self._long_axis().get_minor_formatter()\n\n    @minorformatter.setter\n    def minorformatter(self, fmt):\n        self._long_axis().set_minor_formatter(fmt)\n        self._minorformatter = fmt\n\n    def _cbar_cla(self):\n        \"\"\"Function to clear the interactive colorbar state.\"\"\"\n        for x in self._interactive_funcs:\n            delattr(self.ax, x)\n        # We now restore the old cla() back and can call it directly\n        del self.ax.cla\n        self.ax.cla()\n\n    filled = _api.deprecate_privatize_attribute(\"3.6\")\n\n    def update_normal(self, mappable):\n        \"\"\"\n        Update solid patches, lines, etc.\n\n        This is meant to be called when the norm of the image or contour plot\n        to which this colorbar belongs changes.\n\n        If the norm on the mappable is different than before, this resets the\n        locator and formatter for the axis, so if these have been customized,\n        they will need to be customized again.  However, if the norm only\n        changes values of *vmin*, *vmax* or *cmap* then the old formatter\n        and locator will be preserved.\n        \"\"\"\n        _log.debug('colorbar update normal %r %r', mappable.norm, self.norm)\n        self.mappable = mappable\n        self.set_alpha(mappable.get_alpha())\n        self.cmap = mappable.cmap\n        if mappable.norm != self.norm:\n            self.norm = mappable.norm\n            self._reset_locator_formatter_scale()\n\n        self._draw_all()\n        if isinstance(self.mappable, contour.ContourSet):\n            CS = self.mappable\n            if not CS.filled:\n                self.add_lines(CS)\n        self.stale = True\n\n    @_api.deprecated(\"3.6\", alternative=\"fig.draw_without_rendering()\")\n    def draw_all(self):\n        \"\"\"\n        Calculate any free parameters based on the current cmap and norm,\n        and do all the drawing.\n        \"\"\"\n        self._draw_all()\n\n    def _draw_all(self):\n        \"\"\"\n        Calculate any free parameters based on the current cmap and norm,\n        and do all the drawing.\n        \"\"\"\n        if self.orientation == 'vertical':\n            if mpl.rcParams['ytick.minor.visible']:\n                self.minorticks_on()\n        else:\n            if mpl.rcParams['xtick.minor.visible']:\n                self.minorticks_on()\n        self._long_axis().set(label_position=self.ticklocation,\n                              ticks_position=self.ticklocation)\n        self._short_axis().set_ticks([])\n        self._short_axis().set_ticks([], minor=True)\n\n        # Set self._boundaries and self._values, including extensions.\n        # self._boundaries are the edges of each square of color, and\n        # self._values are the value to map into the norm to get the\n        # color:\n        self._process_values()\n        # Set self.vmin and self.vmax to first and last boundary, excluding\n        # extensions:\n        self.vmin, self.vmax = self._boundaries[self._inside][[0, -1]]\n        # Compute the X/Y mesh.\n        X, Y = self._mesh()\n        # draw the extend triangles, and shrink the inner axes to accommodate.\n        # also adds the outline path to self.outline spine:\n        self._do_extends()\n        lower, upper = self.vmin, self.vmax\n        if self._long_axis().get_inverted():\n            # If the axis is inverted, we need to swap the vmin/vmax\n            lower, upper = upper, lower\n        if self.orientation == 'vertical':\n            self.ax.set_xlim(0, 1)\n            self.ax.set_ylim(lower, upper)\n        else:\n            self.ax.set_ylim(0, 1)\n            self.ax.set_xlim(lower, upper)\n\n        # set up the tick locators and formatters.  A bit complicated because\n        # boundary norms + uniform spacing requires a manual locator.\n        self.update_ticks()\n\n        if self._filled:\n            ind = np.arange(len(self._values))\n            if self._extend_lower():\n                ind = ind[1:]\n            if self._extend_upper():\n                ind = ind[:-1]\n            self._add_solids(X, Y, self._values[ind, np.newaxis])\n\n    def _add_solids(self, X, Y, C):\n        \"\"\"Draw the colors; optionally add separators.\"\"\"\n        # Cleanup previously set artists.\n        if self.solids is not None:\n            self.solids.remove()\n        for solid in self.solids_patches:\n            solid.remove()\n        # Add new artist(s), based on mappable type.  Use individual patches if\n        # hatching is needed, pcolormesh otherwise.\n        mappable = getattr(self, 'mappable', None)\n        if (isinstance(mappable, contour.ContourSet)\n                and any(hatch is not None for hatch in mappable.hatches)):\n            self._add_solids_patches(X, Y, C, mappable)\n        else:\n            self.solids = self.ax.pcolormesh(\n                X, Y, C, cmap=self.cmap, norm=self.norm, alpha=self.alpha,\n                edgecolors='none', shading='flat')\n            if not self.drawedges:\n                if len(self._y) >= self.n_rasterize:\n                    self.solids.set_rasterized(True)\n        self._update_dividers()\n\n    def _update_dividers(self):\n        if not self.drawedges:\n            self.dividers.set_segments([])\n            return\n        # Place all *internal* dividers.\n        if self.orientation == 'vertical':\n            lims = self.ax.get_ylim()\n            bounds = (lims[0] < self._y) & (self._y < lims[1])\n        else:\n            lims = self.ax.get_xlim()\n            bounds = (lims[0] < self._y) & (self._y < lims[1])\n        y = self._y[bounds]\n        # And then add outer dividers if extensions are on.\n        if self._extend_lower():\n            y = np.insert(y, 0, lims[0])\n        if self._extend_upper():\n            y = np.append(y, lims[1])\n        X, Y = np.meshgrid([0, 1], y)\n        if self.orientation == 'vertical':\n            segments = np.dstack([X, Y])\n        else:\n            segments = np.dstack([Y, X])\n        self.dividers.set_segments(segments)\n\n    def _add_solids_patches(self, X, Y, C, mappable):\n        hatches = mappable.hatches * (len(C) + 1)  # Have enough hatches.\n        if self._extend_lower():\n            # remove first hatch that goes into the extend patch\n            hatches = hatches[1:]\n        patches = []\n        for i in range(len(X) - 1):\n            xy = np.array([[X[i, 0], Y[i, 1]],\n                           [X[i, 1], Y[i, 0]],\n                           [X[i + 1, 1], Y[i + 1, 0]],\n                           [X[i + 1, 0], Y[i + 1, 1]]])\n            patch = mpatches.PathPatch(mpath.Path(xy),\n                                       facecolor=self.cmap(self.norm(C[i][0])),\n                                       hatch=hatches[i], linewidth=0,\n                                       antialiased=False, alpha=self.alpha)\n            self.ax.add_patch(patch)\n            patches.append(patch)\n        self.solids_patches = patches\n\n    def _do_extends(self, ax=None):\n        \"\"\"\n        Add the extend tri/rectangles on the outside of the axes.\n\n        ax is unused, but required due to the callbacks on xlim/ylim changed\n        \"\"\"\n        # Clean up any previous extend patches\n        for patch in self._extend_patches:\n            patch.remove()\n        self._extend_patches = []\n        # extend lengths are fraction of the *inner* part of colorbar,\n        # not the total colorbar:\n        _, extendlen = self._proportional_y()\n        bot = 0 - (extendlen[0] if self._extend_lower() else 0)\n        top = 1 + (extendlen[1] if self._extend_upper() else 0)\n\n        # xyout is the outline of the colorbar including the extend patches:\n        if not self.extendrect:\n            # triangle:\n            xyout = np.array([[0, 0], [0.5, bot], [1, 0],\n                              [1, 1], [0.5, top], [0, 1], [0, 0]])\n        else:\n            # rectangle:\n            xyout = np.array([[0, 0], [0, bot], [1, bot], [1, 0],\n                              [1, 1], [1, top], [0, top], [0, 1],\n                              [0, 0]])\n\n        if self.orientation == 'horizontal':\n            xyout = xyout[:, ::-1]\n\n        # xyout is the path for the spine:\n        self.outline.set_xy(xyout)\n        if not self._filled:\n            return\n\n        # Make extend triangles or rectangles filled patches.  These are\n        # defined in the outer parent axes' coordinates:\n        mappable = getattr(self, 'mappable', None)\n        if (isinstance(mappable, contour.ContourSet)\n                and any(hatch is not None for hatch in mappable.hatches)):\n            hatches = mappable.hatches * (len(self._y) + 1)\n        else:\n            hatches = [None] * (len(self._y) + 1)\n\n        if self._extend_lower():\n            if not self.extendrect:\n                # triangle\n                xy = np.array([[0, 0], [0.5, bot], [1, 0]])\n            else:\n                # rectangle\n                xy = np.array([[0, 0], [0, bot], [1., bot], [1, 0]])\n            if self.orientation == 'horizontal':\n                xy = xy[:, ::-1]\n            # add the patch\n            val = -1 if self._long_axis().get_inverted() else 0\n            color = self.cmap(self.norm(self._values[val]))\n            patch = mpatches.PathPatch(\n                mpath.Path(xy), facecolor=color, alpha=self.alpha,\n                linewidth=0, antialiased=False,\n                transform=self.ax.transAxes,\n                hatch=hatches[0], clip_on=False,\n                # Place it right behind the standard patches, which is\n                # needed if we updated the extends\n                zorder=np.nextafter(self.ax.patch.zorder, -np.inf))\n            self.ax.add_patch(patch)\n            self._extend_patches.append(patch)\n            # remove first hatch that goes into the extend patch\n            hatches = hatches[1:]\n        if self._extend_upper():\n            if not self.extendrect:\n                # triangle\n                xy = np.array([[0, 1], [0.5, top], [1, 1]])\n            else:\n                # rectangle\n                xy = np.array([[0, 1], [0, top], [1, top], [1, 1]])\n            if self.orientation == 'horizontal':\n                xy = xy[:, ::-1]\n            # add the patch\n            val = 0 if self._long_axis().get_inverted() else -1\n            color = self.cmap(self.norm(self._values[val]))\n            hatch_idx = len(self._y) - 1\n            patch = mpatches.PathPatch(\n                mpath.Path(xy), facecolor=color, alpha=self.alpha,\n                linewidth=0, antialiased=False,\n                transform=self.ax.transAxes, hatch=hatches[hatch_idx],\n                clip_on=False,\n                # Place it right behind the standard patches, which is\n                # needed if we updated the extends\n                zorder=np.nextafter(self.ax.patch.zorder, -np.inf))\n            self.ax.add_patch(patch)\n            self._extend_patches.append(patch)\n\n        self._update_dividers()\n\n    def add_lines(self, *args, **kwargs):\n        \"\"\"\n        Draw lines on the colorbar.\n\n        The lines are appended to the list :attr:`lines`.\n\n        Parameters\n        ----------\n        levels : array-like\n            The positions of the lines.\n        colors : color or list of colors\n            Either a single color applying to all lines or one color value for\n            each line.\n        linewidths : float or array-like\n            Either a single linewidth applying to all lines or one linewidth\n            for each line.\n        erase : bool, default: True\n            Whether to remove any previously added lines.\n\n        Notes\n        -----\n        Alternatively, this method can also be called with the signature\n        ``colorbar.add_lines(contour_set, erase=True)``, in which case\n        *levels*, *colors*, and *linewidths* are taken from *contour_set*.\n        \"\"\"\n        params = _api.select_matching_signature(\n            [lambda self, CS, erase=True: locals(),\n             lambda self, levels, colors, linewidths, erase=True: locals()],\n            self, *args, **kwargs)\n        if \"CS\" in params:\n            self, CS, erase = params.values()\n            if not isinstance(CS, contour.ContourSet) or CS.filled:\n                raise ValueError(\"If a single artist is passed to add_lines, \"\n                                 \"it must be a ContourSet of lines\")\n            # TODO: Make colorbar lines auto-follow changes in contour lines.\n            return self.add_lines(\n                CS.levels,\n                CS.to_rgba(CS.cvalues, CS.alpha),\n                [coll.get_linewidths()[0] for coll in CS.collections],\n                erase=erase)\n        else:\n            self, levels, colors, linewidths, erase = params.values()\n\n        y = self._locate(levels)\n        rtol = (self._y[-1] - self._y[0]) * 1e-10\n        igood = (y < self._y[-1] + rtol) & (y > self._y[0] - rtol)\n        y = y[igood]\n        if np.iterable(colors):\n            colors = np.asarray(colors)[igood]\n        if np.iterable(linewidths):\n            linewidths = np.asarray(linewidths)[igood]\n        X, Y = np.meshgrid([0, 1], y)\n        if self.orientation == 'vertical':\n            xy = np.stack([X, Y], axis=-1)\n        else:\n            xy = np.stack([Y, X], axis=-1)\n        col = collections.LineCollection(xy, linewidths=linewidths,\n                                         colors=colors)\n\n        if erase and self.lines:\n            for lc in self.lines:\n                lc.remove()\n            self.lines = []\n        self.lines.append(col)\n\n        # make a clip path that is just a linewidth bigger than the axes...\n        fac = np.max(linewidths) / 72\n        xy = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n        inches = self.ax.get_figure().dpi_scale_trans\n        # do in inches:\n        xy = inches.inverted().transform(self.ax.transAxes.transform(xy))\n        xy[[0, 1, 4], 1] -= fac\n        xy[[2, 3], 1] += fac\n        # back to axes units...\n        xy = self.ax.transAxes.inverted().transform(inches.transform(xy))\n        col.set_clip_path(mpath.Path(xy, closed=True),\n                          self.ax.transAxes)\n        self.ax.add_collection(col)\n        self.stale = True\n\n    def update_ticks(self):\n        \"\"\"\n        Set up the ticks and ticklabels. This should not be needed by users.\n        \"\"\"\n        # Get the locator and formatter; defaults to self._locator if not None.\n        self._get_ticker_locator_formatter()\n        self._long_axis().set_major_locator(self._locator)\n        self._long_axis().set_minor_locator(self._minorlocator)\n        self._long_axis().set_major_formatter(self._formatter)\n\n    def _get_ticker_locator_formatter(self):\n        \"\"\"\n        Return the ``locator`` and ``formatter`` of the colorbar.\n\n        If they have not been defined (i.e. are *None*), the formatter and\n        locator are retrieved from the axis, or from the value of the\n        boundaries for a boundary norm.\n\n        Called by update_ticks...\n        \"\"\"\n        locator = self._locator\n        formatter = self._formatter\n        minorlocator = self._minorlocator\n        if isinstance(self.norm, colors.BoundaryNorm):\n            b = self.norm.boundaries\n            if locator is None:\n                locator = ticker.FixedLocator(b, nbins=10)\n            if minorlocator is None:\n                minorlocator = ticker.FixedLocator(b)\n        elif isinstance(self.norm, colors.NoNorm):\n            if locator is None:\n                # put ticks on integers between the boundaries of NoNorm\n                nv = len(self._values)\n                base = 1 + int(nv / 10)\n                locator = ticker.IndexLocator(base=base, offset=.5)\n        elif self.boundaries is not None:\n            b = self._boundaries[self._inside]\n            if locator is None:\n                locator = ticker.FixedLocator(b, nbins=10)\n        else:  # most cases:\n            if locator is None:\n                # we haven't set the locator explicitly, so use the default\n                # for this axis:\n                locator = self._long_axis().get_major_locator()\n            if minorlocator is None:\n                minorlocator = self._long_axis().get_minor_locator()\n\n        if minorlocator is None:\n            minorlocator = ticker.NullLocator()\n\n        if formatter is None:\n            formatter = self._long_axis().get_major_formatter()\n\n        self._locator = locator\n        self._formatter = formatter\n        self._minorlocator = minorlocator\n        _log.debug('locator: %r', locator)\n\n    def set_ticks(self, ticks, *, labels=None, minor=False, **kwargs):\n        \"\"\"\n        Set tick locations.\n\n        Parameters\n        ----------\n        ticks : list of floats\n            List of tick locations.\n        labels : list of str, optional\n            List of tick labels. If not set, the labels show the data value.\n        minor : bool, default: False\n            If ``False``, set the major ticks; if ``True``, the minor ticks.\n        **kwargs\n            `.Text` properties for the labels. These take effect only if you\n            pass *labels*. In other cases, please use `~.Axes.tick_params`.\n        \"\"\"\n        if np.iterable(ticks):\n            self._long_axis().set_ticks(ticks, labels=labels, minor=minor,\n                                        **kwargs)\n            self._locator = self._long_axis().get_major_locator()\n        else:\n            self._locator = ticks\n            self._long_axis().set_major_locator(self._locator)\n        self.stale = True\n\n    def get_ticks(self, minor=False):\n        \"\"\"\n        Return the ticks as a list of locations.\n\n        Parameters\n        ----------\n        minor : boolean, default: False\n            if True return the minor ticks.\n        \"\"\"\n        if minor:\n            return self._long_axis().get_minorticklocs()\n        else:\n            return self._long_axis().get_majorticklocs()\n\n    def set_ticklabels(self, ticklabels, *, minor=False, **kwargs):\n        \"\"\"\n        [*Discouraged*] Set tick labels.\n\n        .. admonition:: Discouraged\n\n            The use of this method is discouraged, because of the dependency\n            on tick positions. In most cases, you'll want to use\n            ``set_ticks(positions, labels=labels)`` instead.\n\n            If you are using this method, you should always fix the tick\n            positions before, e.g. by using `.Colorbar.set_ticks` or by\n            explicitly setting a `~.ticker.FixedLocator` on the long axis\n            of the colorbar. Otherwise, ticks are free to move and the\n            labels may end up in unexpected positions.\n\n        Parameters\n        ----------\n        ticklabels : sequence of str or of `.Text`\n            Texts for labeling each tick location in the sequence set by\n            `.Colorbar.set_ticks`; the number of labels must match the number\n            of locations.\n\n        update_ticks : bool, default: True\n            This keyword argument is ignored and will be removed.\n            Deprecated\n\n         minor : bool\n            If True, set minor ticks instead of major ticks.\n\n        **kwargs\n            `.Text` properties for the labels.\n        \"\"\"\n        self._long_axis().set_ticklabels(ticklabels, minor=minor, **kwargs)\n\n    def minorticks_on(self):\n        \"\"\"\n        Turn on colorbar minor ticks.\n        \"\"\"\n        self.ax.minorticks_on()\n        self._short_axis().set_minor_locator(ticker.NullLocator())\n\n    def minorticks_off(self):\n        \"\"\"Turn the minor ticks of the colorbar off.\"\"\"\n        self._minorlocator = ticker.NullLocator()\n        self._long_axis().set_minor_locator(self._minorlocator)\n\n    def set_label(self, label, *, loc=None, **kwargs):\n        \"\"\"\n        Add a label to the long axis of the colorbar.\n\n        Parameters\n        ----------\n        label : str\n            The label text.\n        loc : str, optional\n            The location of the label.\n\n            - For horizontal orientation one of {'left', 'center', 'right'}\n            - For vertical orientation one of {'bottom', 'center', 'top'}\n\n            Defaults to :rc:`xaxis.labellocation` or :rc:`yaxis.labellocation`\n            depending on the orientation.\n        **kwargs\n            Keyword arguments are passed to `~.Axes.set_xlabel` /\n            `~.Axes.set_ylabel`.\n            Supported keywords are *labelpad* and `.Text` properties.\n        \"\"\"\n        if self.orientation == \"vertical\":\n            self.ax.set_ylabel(label, loc=loc, **kwargs)\n        else:\n            self.ax.set_xlabel(label, loc=loc, **kwargs)\n        self.stale = True\n\n    def set_alpha(self, alpha):\n        \"\"\"\n        Set the transparency between 0 (transparent) and 1 (opaque).\n\n        If an array is provided, *alpha* will be set to None to use the\n        transparency values associated with the colormap.\n        \"\"\"\n        self.alpha = None if isinstance(alpha, np.ndarray) else alpha\n\n    def _set_scale(self, scale, **kwargs):\n        \"\"\"\n        Set the colorbar long axis scale.\n\n        Parameters\n        ----------\n        scale : {\"linear\", \"log\", \"symlog\", \"logit\", ...} or `.ScaleBase`\n            The axis scale type to apply.\n\n        **kwargs\n            Different keyword arguments are accepted, depending on the scale.\n            See the respective class keyword arguments:\n\n            - `matplotlib.scale.LinearScale`\n            - `matplotlib.scale.LogScale`\n            - `matplotlib.scale.SymmetricalLogScale`\n            - `matplotlib.scale.LogitScale`\n            - `matplotlib.scale.FuncScale`\n\n        Notes\n        -----\n        By default, Matplotlib supports the above-mentioned scales.\n        Additionally, custom scales may be registered using\n        `matplotlib.scale.register_scale`. These scales can then also\n        be used here.\n        \"\"\"\n        self._long_axis()._set_axes_scale(scale, **kwargs)\n\n    def remove(self):\n        \"\"\"\n        Remove this colorbar from the figure.\n\n        If the colorbar was created with ``use_gridspec=True`` the previous\n        gridspec is restored.\n        \"\"\"\n        if hasattr(self.ax, '_colorbar_info'):\n            parents = self.ax._colorbar_info['parents']\n            for a in parents:\n                if self.ax in a._colorbars:\n                    a._colorbars.remove(self.ax)\n\n        self.ax.remove()\n\n        self.mappable.callbacks.disconnect(self.mappable.colorbar_cid)\n        self.mappable.colorbar = None\n        self.mappable.colorbar_cid = None\n        # Remove the extension callbacks\n        self.ax.callbacks.disconnect(self._extend_cid1)\n        self.ax.callbacks.disconnect(self._extend_cid2)\n\n        try:\n            ax = self.mappable.axes\n        except AttributeError:\n            return\n        try:\n            gs = ax.get_subplotspec().get_gridspec()\n            subplotspec = gs.get_topmost_subplotspec()\n        except AttributeError:\n            # use_gridspec was False\n            pos = ax.get_position(original=True)\n            ax._set_position(pos)\n        else:\n            # use_gridspec was True\n            ax.set_subplotspec(subplotspec)\n\n    def _process_values(self):\n        \"\"\"\n        Set `_boundaries` and `_values` based on the self.boundaries and\n        self.values if not None, or based on the size of the colormap and\n        the vmin/vmax of the norm.\n        \"\"\"\n        if self.values is not None:\n            # set self._boundaries from the values...\n            self._values = np.array(self.values)\n            if self.boundaries is None:\n                # bracket values by 1/2 dv:\n                b = np.zeros(len(self.values) + 1)\n                b[1:-1] = 0.5 * (self._values[:-1] + self._values[1:])\n                b[0] = 2.0 * b[1] - b[2]\n                b[-1] = 2.0 * b[-2] - b[-3]\n                self._boundaries = b\n                return\n            self._boundaries = np.array(self.boundaries)\n            return\n\n        # otherwise values are set from the boundaries\n        if isinstance(self.norm, colors.BoundaryNorm):\n            b = self.norm.boundaries\n        elif isinstance(self.norm, colors.NoNorm):\n            # NoNorm has N blocks, so N+1 boundaries, centered on integers:\n            b = np.arange(self.cmap.N + 1) - .5\n        elif self.boundaries is not None:\n            b = self.boundaries\n        else:\n            # otherwise make the boundaries from the size of the cmap:\n            N = self.cmap.N + 1\n            b, _ = self._uniform_y(N)\n        # add extra boundaries if needed:\n        if self._extend_lower():\n            b = np.hstack((b[0] - 1, b))\n        if self._extend_upper():\n            b = np.hstack((b, b[-1] + 1))\n\n        # transform from 0-1 to vmin-vmax:\n        if not self.norm.scaled():\n            self.norm.vmin = 0\n(D)     Format strings, e.g., ``\"%4.2e\"`` or ``\"{x:.2e}\"``, are supported.\n    An alternative `~.ticker.Formatter` may be given instead.\n\ndrawedges : bool\n    Whether to draw lines at color boundaries.\n\nlabel : str\n    The label on the colorbar's long axis.\n\nboundaries, values : None or a sequence\n    If unset, the colormap will be displayed on a 0-1 scale.\n    If sequences, *values* must have a length 1 less than *boundaries*.  For\n    each region delimited by adjacent entries in *boundaries*, the color mapped\n    to the corresponding value in values will be used.\n    Normally only useful for indexed colors (i.e. ``norm=NoNorm()``) or other\n    unusual circumstances.\"\"\")\n\n\ndef _set_ticks_on_axis_warn(*args, **kwargs):\n    # a top level function which gets put in at the axes'\n    # set_xticks and set_yticks by Colorbar.__init__.\n    _api.warn_external(\"Use the colorbar set_ticks() method instead.\")\n\n\nclass _ColorbarSpine(mspines.Spine):\n    def __init__(self, axes):\n        self._ax = axes\n        super().__init__(axes, 'colorbar', mpath.Path(np.empty((0, 2))))\n        mpatches.Patch.set_transform(self, axes.transAxes)\n\n    def get_window_extent(self, renderer=None):\n        # This Spine has no Axis associated with it, and doesn't need to adjust\n        # its location, so we can directly get the window extent from the\n        # super-super-class.\n        return mpatches.Patch.get_window_extent(self, renderer=renderer)\n\n    def set_xy(self, xy):\n        self._path = mpath.Path(xy, closed=True)\n        self._xy = xy\n        self.stale = True\n\n    def draw(self, renderer):\n        ret = mpatches.Patch.draw(self, renderer)\n        self.stale = False\n        return ret\n\n\nclass _ColorbarAxesLocator:\n    \"\"\"\n    Shrink the axes if there are triangular or rectangular extends.\n    \"\"\"\n    def __init__(self, cbar):\n        self._cbar = cbar\n        self._orig_locator = cbar.ax._axes_locator\n\n    def __call__(self, ax, renderer):\n        if self._orig_locator is not None:\n            pos = self._orig_locator(ax, renderer)\n        else:\n            pos = ax.get_position(original=True)\n        if self._cbar.extend == 'neither':\n            return pos\n\n        y, extendlen = self._cbar._proportional_y()\n        if not self._cbar._extend_lower():\n            extendlen[0] = 0\n        if not self._cbar._extend_upper():\n            extendlen[1] = 0\n        len = sum(extendlen) + 1\n        shrink = 1 / len\n        offset = extendlen[0] / len\n        # we need to reset the aspect ratio of the axes to account\n        # of the extends...\n        if hasattr(ax, '_colorbar_info'):\n            aspect = ax._colorbar_info['aspect']\n        else:\n            aspect = False\n        # now shrink and/or offset to take into account the\n        # extend tri/rectangles.\n        if self._cbar.orientation == 'vertical':\n            if aspect:\n                self._cbar.ax.set_box_aspect(aspect*shrink)\n            pos = pos.shrunk(1, shrink).translated(0, offset * pos.height)\n        else:\n            if aspect:\n                self._cbar.ax.set_box_aspect(1/(aspect * shrink))\n            pos = pos.shrunk(shrink, 1).translated(offset * pos.width, 0)\n        return pos\n\n    def get_subplotspec(self):\n        # make tight_layout happy..\n        return (\n            self._cbar.ax.get_subplotspec()\n            or getattr(self._orig_locator, \"get_subplotspec\", lambda: None)())\n\n\n@_docstring.interpd\nclass Colorbar:\n    r\"\"\"\n    Draw a colorbar in an existing axes.\n\n    Typically, colorbars are created using `.Figure.colorbar` or\n    `.pyplot.colorbar` and associated with `.ScalarMappable`\\s (such as an\n    `.AxesImage` generated via `~.axes.Axes.imshow`).\n\n    In order to draw a colorbar not associated with other elements in the\n    figure, e.g. when showing a colormap by itself, one can create an empty\n    `.ScalarMappable`, or directly pass *cmap* and *norm* instead of *mappable*\n    to `Colorbar`.\n\n    Useful public methods are :meth:`set_label` and :meth:`add_lines`.\n\n    Attributes\n    ----------\n    ax : `~matplotlib.axes.Axes`\n        The `~.axes.Axes` instance in which the colorbar is drawn.\n    lines : list\n        A list of `.LineCollection` (empty if no lines were drawn).\n    dividers : `.LineCollection`\n        A LineCollection (empty if *drawedges* is ``False``).\n\n    Parameters\n    ----------\n    ax : `~matplotlib.axes.Axes`\n        The `~.axes.Axes` instance in which the colorbar is drawn.\n\n    mappable : `.ScalarMappable`\n        The mappable whose colormap and norm will be used.\n\n        To show the under- and over- value colors, the mappable's norm should\n        be specified as ::\n\n            norm = colors.Normalize(clip=False)\n\n        To show the colors versus index instead of on a 0-1 scale, use::\n\n            norm=colors.NoNorm()\n\n    cmap : `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n        The colormap to use.  This parameter is ignored, unless *mappable* is\n        None.\n\n    norm : `~matplotlib.colors.Normalize`\n        The normalization to use.  This parameter is ignored, unless *mappable*\n        is None.\n\n    alpha : float\n        The colorbar transparency between 0 (transparent) and 1 (opaque).\n\n    orientation : None or {'vertical', 'horizontal'}\n        If None, use the value determined by *location*. If both\n        *orientation* and *location* are None then defaults to 'vertical'.\n\n    ticklocation : {'auto', 'left', 'right', 'top', 'bottom'}\n        The location of the colorbar ticks. The *ticklocation* must match\n        *orientation*. For example, a horizontal colorbar can only have ticks\n        at the top or the bottom. If 'auto', the ticks will be the same as\n        *location*, so a colorbar to the left will have ticks to the left. If\n        *location* is None, the ticks will be at the bottom for a horizontal\n        colorbar and at the right for a vertical.\n\n    drawedges : bool\n        Whether to draw lines at color boundaries.\n\n    filled : bool\n\n    %(_colormap_kw_doc)s\n\n    location : None or {'left', 'right', 'top', 'bottom'}\n        Set the *orientation* and *ticklocation* of the colorbar using a\n        single argument. Colorbars on the left and right are vertical,\n        colorbars at the top and bottom are horizontal. The *ticklocation* is\n        the same as *location*, so if *location* is 'top', the ticks are on\n        the top. *orientation* and/or *ticklocation* can be provided as well\n        and overrides the value set by *location*, but there will be an error\n        for incompatible combinations.\n\n        .. versionadded:: 3.7\n    \"\"\"\n\n    n_rasterize = 50  # rasterize solids if number of colors >= n_rasterize\n\n    @_api.delete_parameter(\"3.6\", \"filled\")\n    def __init__(self, ax, mappable=None, *, cmap=None,\n                 norm=None,\n                 alpha=None,\n                 values=None,\n                 boundaries=None,\n                 orientation=None,\n                 ticklocation='auto',\n                 extend=None,\n                 spacing='uniform',  # uniform or proportional\n                 ticks=None,\n                 format=None,\n                 drawedges=False,\n                 filled=True,\n                 extendfrac=None,\n                 extendrect=False,\n                 label='',\n                 location=None,\n                 ):\n\n        if mappable is None:\n            mappable = cm.ScalarMappable(norm=norm, cmap=cmap)\n\n        # Ensure the given mappable's norm has appropriate vmin and vmax\n        # set even if mappable.draw has not yet been called.\n        if mappable.get_array() is not None:\n            mappable.autoscale_None()\n\n        self.mappable = mappable\n        cmap = mappable.cmap\n        norm = mappable.norm\n\n        if isinstance(mappable, contour.ContourSet):\n            cs = mappable\n            alpha = cs.get_alpha()\n            boundaries = cs._levels\n            values = cs.cvalues\n            extend = cs.extend\n            filled = cs.filled\n            if ticks is None:\n                ticks = ticker.FixedLocator(cs.levels, nbins=10)\n        elif isinstance(mappable, martist.Artist):\n            alpha = mappable.get_alpha()\n\n        mappable.colorbar = self\n        mappable.colorbar_cid = mappable.callbacks.connect(\n            'changed', self.update_normal)\n\n        location_orientation = _get_orientation_from_location(location)\n\n        _api.check_in_list(\n            [None, 'vertical', 'horizontal'], orientation=orientation)\n        _api.check_in_list(\n            ['auto', 'left', 'right', 'top', 'bottom'],\n            ticklocation=ticklocation)\n        _api.check_in_list(\n            ['uniform', 'proportional'], spacing=spacing)\n\n        if location_orientation is not None and orientation is not None:\n            if location_orientation != orientation:\n                raise TypeError(\n                    \"location and orientation are mutually exclusive\")\n        else:\n            orientation = orientation or location_orientation or \"vertical\"\n\n        self.ax = ax\n        self.ax._axes_locator = _ColorbarAxesLocator(self)\n\n        if extend is None:\n            if (not isinstance(mappable, contour.ContourSet)\n                    and getattr(cmap, 'colorbar_extend', False) is not False):\n                extend = cmap.colorbar_extend\n            elif hasattr(norm, 'extend'):\n                extend = norm.extend\n            else:\n                extend = 'neither'\n        self.alpha = None\n        # Call set_alpha to handle array-like alphas properly\n        self.set_alpha(alpha)\n        self.cmap = cmap\n        self.norm = norm\n        self.values = values\n        self.boundaries = boundaries\n        self.extend = extend\n        self._inside = _api.check_getitem(\n            {'neither': slice(0, None), 'both': slice(1, -1),\n             'min': slice(1, None), 'max': slice(0, -1)},\n            extend=extend)\n        self.spacing = spacing\n        self.orientation = orientation\n        self.drawedges = drawedges\n        self._filled = filled\n        self.extendfrac = extendfrac\n        self.extendrect = extendrect\n        self._extend_patches = []\n        self.solids = None\n        self.solids_patches = []\n        self.lines = []\n\n        for spine in self.ax.spines.values():\n            spine.set_visible(False)\n        self.outline = self.ax.spines['outline'] = _ColorbarSpine(self.ax)\n\n        self.dividers = collections.LineCollection(\n            [],\n            colors=[mpl.rcParams['axes.edgecolor']],\n            linewidths=[0.5 * mpl.rcParams['axes.linewidth']],\n            clip_on=False)\n        self.ax.add_collection(self.dividers)\n\n        self._locator = None\n        self._minorlocator = None\n        self._formatter = None\n        self._minorformatter = None\n\n        if ticklocation == 'auto':\n            ticklocation = _get_ticklocation_from_orientation(\n                orientation) if location is None else location\n        self.ticklocation = ticklocation\n\n        self.set_label(label)\n        self._reset_locator_formatter_scale()\n\n        if np.iterable(ticks):\n            self._locator = ticker.FixedLocator(ticks, nbins=len(ticks))\n        else:\n            self._locator = ticks\n\n        if isinstance(format, str):\n            # Check format between FormatStrFormatter and StrMethodFormatter\n            try:\n                self._formatter = ticker.FormatStrFormatter(format)\n                _ = self._formatter(0)\n            except TypeError:\n                self._formatter = ticker.StrMethodFormatter(format)\n        else:\n            self._formatter = format  # Assume it is a Formatter or None\n        self._draw_all()\n\n        if isinstance(mappable, contour.ContourSet) and not mappable.filled:\n            self.add_lines(mappable)\n\n        # Link the Axes and Colorbar for interactive use\n        self.ax._colorbar = self\n        # Don't navigate on any of these types of mappables\n        if (isinstance(self.norm, (colors.BoundaryNorm, colors.NoNorm)) or\n                isinstance(self.mappable, contour.ContourSet)):\n            self.ax.set_navigate(False)\n\n        # These are the functions that set up interactivity on this colorbar\n        self._interactive_funcs = [\"_get_view\", \"_set_view\",\n                                   \"_set_view_from_bbox\", \"drag_pan\"]\n        for x in self._interactive_funcs:\n            setattr(self.ax, x, getattr(self, x))\n        # Set the cla function to the cbar's method to override it\n        self.ax.cla = self._cbar_cla\n        # Callbacks for the extend calculations to handle inverting the axis\n        self._extend_cid1 = self.ax.callbacks.connect(\n            \"xlim_changed\", self._do_extends)\n        self._extend_cid2 = self.ax.callbacks.connect(\n            \"ylim_changed\", self._do_extends)\n\n    @property\n    def locator(self):\n        \"\"\"Major tick `.Locator` for the colorbar.\"\"\"\n        return self._long_axis().get_major_locator()\n\n    @locator.setter\n    def locator(self, loc):\n        self._long_axis().set_major_locator(loc)\n        self._locator = loc\n\n    @property\n    def minorlocator(self):\n        \"\"\"Minor tick `.Locator` for the colorbar.\"\"\"\n        return self._long_axis().get_minor_locator()\n\n    @minorlocator.setter\n    def minorlocator(self, loc):\n        self._long_axis().set_minor_locator(loc)\n        self._minorlocator = loc\n\n    @property\n    def formatter(self):\n        \"\"\"Major tick label `.Formatter` for the colorbar.\"\"\"\n        return self._long_axis().get_major_formatter()\n\n    @formatter.setter\n    def formatter(self, fmt):\n        self._long_axis().set_major_formatter(fmt)\n        self._formatter = fmt\n\n    @property\n    def minorformatter(self):\n        \"\"\"Minor tick `.Formatter` for the colorbar.\"\"\"\n        return self._long_axis().get_minor_formatter()\n\n    @minorformatter.setter\n    def minorformatter(self, fmt):\n        self._long_axis().set_minor_formatter(fmt)\n        self._minorformatter = fmt\n\n    def _cbar_cla(self):\n        \"\"\"Function to clear the interactive colorbar state.\"\"\"\n        for x in self._interactive_funcs:\n            delattr(self.ax, x)\n        # We now restore the old cla() back and can call it directly\n        del self.ax.cla\n        self.ax.cla()\n\n    filled = _api.deprecate_privatize_attribute(\"3.6\")\n\n    def update_normal(self, mappable):\n        \"\"\"\n        Update solid patches, lines, etc.\n\n        This is meant to be called when the norm of the image or contour plot\n        to which this colorbar belongs changes.\n\n        If the norm on the mappable is different than before, this resets the\n        locator and formatter for the axis, so if these have been customized,\n        they will need to be customized again.  However, if the norm only\n        changes values of *vmin*, *vmax* or *cmap* then the old formatter\n        and locator will be preserved.\n        \"\"\"\n        _log.debug('colorbar update normal %r %r', mappable.norm, self.norm)\n        self.mappable = mappable\n        self.set_alpha(mappable.get_alpha())\n        self.cmap = mappable.cmap\n        if mappable.norm != self.norm:\n            self.norm = mappable.norm\n            self._reset_locator_formatter_scale()\n\n        self._draw_all()\n        if isinstance(self.mappable, contour.ContourSet):\n            CS = self.mappable\n            if not CS.filled:\n                self.add_lines(CS)\n        self.stale = True\n\n    @_api.deprecated(\"3.6\", alternative=\"fig.draw_without_rendering()\")\n    def draw_all(self):\n        \"\"\"\n        Calculate any free parameters based on the current cmap and norm,\n        and do all the drawing.\n        \"\"\"\n        self._draw_all()\n\n    def _draw_all(self):\n        \"\"\"\n        Calculate any free parameters based on the current cmap and norm,\n        and do all the drawing.\n        \"\"\"\n        if self.orientation == 'vertical':\n            if mpl.rcParams['ytick.minor.visible']:\n                self.minorticks_on()\n        else:\n            if mpl.rcParams['xtick.minor.visible']:\n                self.minorticks_on()\n        self._long_axis().set(label_position=self.ticklocation,\n                              ticks_position=self.ticklocation)\n        self._short_axis().set_ticks([])\n        self._short_axis().set_ticks([], minor=True)\n\n        # Set self._boundaries and self._values, including extensions.\n        # self._boundaries are the edges of each square of color, and\n        # self._values are the value to map into the norm to get the\n        # color:\n        self._process_values()\n        # Set self.vmin and self.vmax to first and last boundary, excluding\n        # extensions:\n        self.vmin, self.vmax = self._boundaries[self._inside][[0, -1]]\n        # Compute the X/Y mesh.\n        X, Y = self._mesh()\n        # draw the extend triangles, and shrink the inner axes to accommodate.\n        # also adds the outline path to self.outline spine:\n        self._do_extends()\n        lower, upper = self.vmin, self.vmax\n        if self._long_axis().get_inverted():\n            # If the axis is inverted, we need to swap the vmin/vmax\n            lower, upper = upper, lower\n        if self.orientation == 'vertical':\n            self.ax.set_xlim(0, 1)\n            self.ax.set_ylim(lower, upper)\n        else:\n            self.ax.set_ylim(0, 1)\n            self.ax.set_xlim(lower, upper)\n\n        # set up the tick locators and formatters.  A bit complicated because\n        # boundary norms + uniform spacing requires a manual locator.\n        self.update_ticks()\n\n        if self._filled:\n            ind = np.arange(len(self._values))\n            if self._extend_lower():\n                ind = ind[1:]\n            if self._extend_upper():\n                ind = ind[:-1]\n            self._add_solids(X, Y, self._values[ind, np.newaxis])\n\n    def _add_solids(self, X, Y, C):\n        \"\"\"Draw the colors; optionally add separators.\"\"\"\n        # Cleanup previously set artists.\n        if self.solids is not None:\n            self.solids.remove()\n        for solid in self.solids_patches:\n            solid.remove()\n        # Add new artist(s), based on mappable type.  Use individual patches if\n        # hatching is needed, pcolormesh otherwise.\n        mappable = getattr(self, 'mappable', None)\n        if (isinstance(mappable, contour.ContourSet)\n                and any(hatch is not None for hatch in mappable.hatches)):\n            self._add_solids_patches(X, Y, C, mappable)\n        else:\n            self.solids = self.ax.pcolormesh(\n                X, Y, C, cmap=self.cmap, norm=self.norm, alpha=self.alpha,\n                edgecolors='none', shading='flat')\n            if not self.drawedges:\n                if len(self._y) >= self.n_rasterize:\n                    self.solids.set_rasterized(True)\n        self._update_dividers()\n\n    def _update_dividers(self):\n        if not self.drawedges:\n            self.dividers.set_segments([])\n            return\n        # Place all *internal* dividers.\n        if self.orientation == 'vertical':\n            lims = self.ax.get_ylim()\n            bounds = (lims[0] < self._y) & (self._y < lims[1])\n        else:\n            lims = self.ax.get_xlim()\n            bounds = (lims[0] < self._y) & (self._y < lims[1])\n        y = self._y[bounds]\n        # And then add outer dividers if extensions are on.\n        if self._extend_lower():\n            y = np.insert(y, 0, lims[0])\n        if self._extend_upper():\n            y = np.append(y, lims[1])\n        X, Y = np.meshgrid([0, 1], y)\n        if self.orientation == 'vertical':\n            segments = np.dstack([X, Y])\n        else:\n            segments = np.dstack([Y, X])\n        self.dividers.set_segments(segments)\n\n    def _add_solids_patches(self, X, Y, C, mappable):\n        hatches = mappable.hatches * (len(C) + 1)  # Have enough hatches.\n        if self._extend_lower():\n            # remove first hatch that goes into the extend patch\n            hatches = hatches[1:]\n        patches = []\n        for i in range(len(X) - 1):\n            xy = np.array([[X[i, 0], Y[i, 1]],\n                           [X[i, 1], Y[i, 0]],\n                           [X[i + 1, 1], Y[i + 1, 0]],\n                           [X[i + 1, 0], Y[i + 1, 1]]])\n            patch = mpatches.PathPatch(mpath.Path(xy),\n                                       facecolor=self.cmap(self.norm(C[i][0])),\n                                       hatch=hatches[i], linewidth=0,\n                                       antialiased=False, alpha=self.alpha)\n            self.ax.add_patch(patch)\n            patches.append(patch)\n        self.solids_patches = patches\n\n    def _do_extends(self, ax=None):\n        \"\"\"\n        Add the extend tri/rectangles on the outside of the axes.\n\n        ax is unused, but required due to the callbacks on xlim/ylim changed\n        \"\"\"\n        # Clean up any previous extend patches\n        for patch in self._extend_patches:\n            patch.remove()\n        self._extend_patches = []\n        # extend lengths are fraction of the *inner* part of colorbar,\n        # not the total colorbar:\n        _, extendlen = self._proportional_y()\n        bot = 0 - (extendlen[0] if self._extend_lower() else 0)\n        top = 1 + (extendlen[1] if self._extend_upper() else 0)\n\n        # xyout is the outline of the colorbar including the extend patches:\n        if not self.extendrect:\n            # triangle:\n            xyout = np.array([[0, 0], [0.5, bot], [1, 0],\n                              [1, 1], [0.5, top], [0, 1], [0, 0]])\n        else:\n            # rectangle:\n            xyout = np.array([[0, 0], [0, bot], [1, bot], [1, 0],\n                              [1, 1], [1, top], [0, top], [0, 1],\n                              [0, 0]])\n\n        if self.orientation == 'horizontal':\n            xyout = xyout[:, ::-1]\n\n        # xyout is the path for the spine:\n        self.outline.set_xy(xyout)\n        if not self._filled:\n            return\n\n        # Make extend triangles or rectangles filled patches.  These are\n        # defined in the outer parent axes' coordinates:\n        mappable = getattr(self, 'mappable', None)\n        if (isinstance(mappable, contour.ContourSet)\n                and any(hatch is not None for hatch in mappable.hatches)):\n            hatches = mappable.hatches * (len(self._y) + 1)\n        else:\n            hatches = [None] * (len(self._y) + 1)\n\n        if self._extend_lower():\n            if not self.extendrect:\n                # triangle\n                xy = np.array([[0, 0], [0.5, bot], [1, 0]])\n            else:\n                # rectangle\n                xy = np.array([[0, 0], [0, bot], [1., bot], [1, 0]])\n            if self.orientation == 'horizontal':\n                xy = xy[:, ::-1]\n            # add the patch\n            val = -1 if self._long_axis().get_inverted() else 0\n            color = self.cmap(self.norm(self._values[val]))\n            patch = mpatches.PathPatch(\n                mpath.Path(xy), facecolor=color, alpha=self.alpha,\n                linewidth=0, antialiased=False,\n                transform=self.ax.transAxes,\n                hatch=hatches[0], clip_on=False,\n                # Place it right behind the standard patches, which is\n                # needed if we updated the extends\n                zorder=np.nextafter(self.ax.patch.zorder, -np.inf))\n            self.ax.add_patch(patch)\n            self._extend_patches.append(patch)\n            # remove first hatch that goes into the extend patch\n            hatches = hatches[1:]\n        if self._extend_upper():\n            if not self.extendrect:\n                # triangle\n                xy = np.array([[0, 1], [0.5, top], [1, 1]])\n            else:\n                # rectangle\n                xy = np.array([[0, 1], [0, top], [1, top], [1, 1]])\n            if self.orientation == 'horizontal':\n                xy = xy[:, ::-1]\n            # add the patch\n            val = 0 if self._long_axis().get_inverted() else -1\n            color = self.cmap(self.norm(self._values[val]))\n            hatch_idx = len(self._y) - 1\n            patch = mpatches.PathPatch(\n                mpath.Path(xy), facecolor=color, alpha=self.alpha,\n                linewidth=0, antialiased=False,\n                transform=self.ax.transAxes, hatch=hatches[hatch_idx],\n                clip_on=False,\n                # Place it right behind the standard patches, which is\n                # needed if we updated the extends\n                zorder=np.nextafter(self.ax.patch.zorder, -np.inf))\n            self.ax.add_patch(patch)\n            self._extend_patches.append(patch)\n\n        self._update_dividers()\n\n    def add_lines(self, *args, **kwargs):\n        \"\"\"\n        Draw lines on the colorbar.\n\n        The lines are appended to the list :attr:`lines`.\n\n        Parameters\n        ----------\n        levels : array-like\n            The positions of the lines.\n        colors : color or list of colors\n            Either a single color applying to all lines or one color value for\n            each line.\n        linewidths : float or array-like\n            Either a single linewidth applying to all lines or one linewidth\n            for each line.\n        erase : bool, default: True\n            Whether to remove any previously added lines.\n\n        Notes\n        -----\n        Alternatively, this method can also be called with the signature\n        ``colorbar.add_lines(contour_set, erase=True)``, in which case\n        *levels*, *colors*, and *linewidths* are taken from *contour_set*.\n        \"\"\"\n        params = _api.select_matching_signature(\n            [lambda self, CS, erase=True: locals(),\n             lambda self, levels, colors, linewidths, erase=True: locals()],\n            self, *args, **kwargs)\n        if \"CS\" in params:\n            self, CS, erase = params.values()\n            if not isinstance(CS, contour.ContourSet) or CS.filled:\n                raise ValueError(\"If a single artist is passed to add_lines, \"\n                                 \"it must be a ContourSet of lines\")\n            # TODO: Make colorbar lines auto-follow changes in contour lines.\n            return self.add_lines(\n                CS.levels,\n                CS.to_rgba(CS.cvalues, CS.alpha),\n                [coll.get_linewidths()[0] for coll in CS.collections],\n                erase=erase)\n        else:\n            self, levels, colors, linewidths, erase = params.values()\n\n        y = self._locate(levels)\n        rtol = (self._y[-1] - self._y[0]) * 1e-10\n        igood = (y < self._y[-1] + rtol) & (y > self._y[0] - rtol)\n        y = y[igood]\n        if np.iterable(colors):\n            colors = np.asarray(colors)[igood]\n        if np.iterable(linewidths):\n            linewidths = np.asarray(linewidths)[igood]\n        X, Y = np.meshgrid([0, 1], y)\n        if self.orientation == 'vertical':\n            xy = np.stack([X, Y], axis=-1)\n        else:\n            xy = np.stack([Y, X], axis=-1)\n        col = collections.LineCollection(xy, linewidths=linewidths,\n                                         colors=colors)\n\n        if erase and self.lines:\n            for lc in self.lines:\n                lc.remove()\n            self.lines = []\n        self.lines.append(col)\n\n        # make a clip path that is just a linewidth bigger than the axes...\n        fac = np.max(linewidths) / 72\n        xy = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n        inches = self.ax.get_figure().dpi_scale_trans\n        # do in inches:\n        xy = inches.inverted().transform(self.ax.transAxes.transform(xy))\n        xy[[0, 1, 4], 1] -= fac\n        xy[[2, 3], 1] += fac\n        # back to axes units...\n        xy = self.ax.transAxes.inverted().transform(inches.transform(xy))\n        col.set_clip_path(mpath.Path(xy, closed=True),\n                          self.ax.transAxes)\n        self.ax.add_collection(col)\n        self.stale = True\n\n    def update_ticks(self):\n        \"\"\"\n        Set up the ticks and ticklabels. This should not be needed by users.\n        \"\"\"\n        # Get the locator and formatter; defaults to self._locator if not None.\n        self._get_ticker_locator_formatter()\n        self._long_axis().set_major_locator(self._locator)\n        self._long_axis().set_minor_locator(self._minorlocator)\n        self._long_axis().set_major_formatter(self._formatter)\n\n    def _get_ticker_locator_formatter(self):\n        \"\"\"\n        Return the ``locator`` and ``formatter`` of the colorbar.\n\n        If they have not been defined (i.e. are *None*), the formatter and\n        locator are retrieved from the axis, or from the value of the\n        boundaries for a boundary norm.\n\n        Called by update_ticks...\n        \"\"\"\n        locator = self._locator\n        formatter = self._formatter\n        minorlocator = self._minorlocator\n        if isinstance(self.norm, colors.BoundaryNorm):\n            b = self.norm.boundaries\n            if locator is None:\n                locator = ticker.FixedLocator(b, nbins=10)\n            if minorlocator is None:\n                minorlocator = ticker.FixedLocator(b)\n        elif isinstance(self.norm, colors.NoNorm):\n            if locator is None:\n                # put ticks on integers between the boundaries of NoNorm\n                nv = len(self._values)\n                base = 1 + int(nv / 10)\n                locator = ticker.IndexLocator(base=base, offset=.5)\n        elif self.boundaries is not None:\n            b = self._boundaries[self._inside]\n            if locator is None:\n                locator = ticker.FixedLocator(b, nbins=10)\n        else:  # most cases:\n            if locator is None:\n                # we haven't set the locator explicitly, so use the default\n                # for this axis:\n                locator = self._long_axis().get_major_locator()\n            if minorlocator is None:\n                minorlocator = self._long_axis().get_minor_locator()\n\n        if minorlocator is None:\n            minorlocator = ticker.NullLocator()\n\n        if formatter is None:\n            formatter = self._long_axis().get_major_formatter()\n\n        self._locator = locator\n        self._formatter = formatter\n        self._minorlocator = minorlocator\n        _log.debug('locator: %r', locator)\n\n    def set_ticks(self, ticks, *, labels=None, minor=False, **kwargs):\n        \"\"\"\n        Set tick locations.\n\n        Parameters\n        ----------\n        ticks : list of floats\n            List of tick locations.\n        labels : list of str, optional\n            List of tick labels. If not set, the labels show the data value.\n        minor : bool, default: False\n            If ``False``, set the major ticks; if ``True``, the minor ticks.\n        **kwargs\n            `.Text` properties for the labels. These take effect only if you\n            pass *labels*. In other cases, please use `~.Axes.tick_params`.\n        \"\"\"\n        if np.iterable(ticks):\n            self._long_axis().set_ticks(ticks, labels=labels, minor=minor,\n                                        **kwargs)\n            self._locator = self._long_axis().get_major_locator()\n        else:\n            self._locator = ticks\n            self._long_axis().set_major_locator(self._locator)\n        self.stale = True\n\n    def get_ticks(self, minor=False):", "answer": "C"}
{"uuid": "774caa34-fd97-4d4f-b665-393068b24f05", "issue_statement": "xlim_changed not emitted on shared axis\n<!--To help us understand and resolve your issue, please fill out the form to the best of your ability.-->\r\n<!--You can feel free to delete the sections that do not apply.-->\r\n\r\n### Bug report\r\n\r\n**Bug summary**\r\n\r\nWhen an axis is shared with another its registered \"xlim_changed\" callbacks does not get called when the change is induced by a shared axis (via sharex=). \r\n\r\nIn _base.py the set_xlim for sibling axis are called with emit=False:\r\n\r\n```\r\nmatplotlib/lib/matplotlib/axes/_base.py:\r\n\r\n/.../\r\ndef set_xlim(...)\r\n/.../\r\n        if emit:\r\n            self.callbacks.process('xlim_changed', self)\r\n            # Call all of the other x-axes that are shared with this one\r\n            for other in self._shared_x_axes.get_siblings(self):\r\n                if other is not self:\r\n                    other.set_xlim(self.viewLim.intervalx,\r\n                                   emit=False, auto=auto)\r\n```\r\n\r\nI'm very new to matplotlib, so perhaps there is a good reason for this? emit=False seems to disable both continued \"inheritance\" of axis (why?) and triggering of change callbacks (looking at the code above).\r\n\r\nIt seems like one would at least want to trigger the xlim_changed callbacks as they would be intended to react to any change in axis limits.\r\n\r\nEdit: Setting emit=True seems to introduce a recursion issue (not sure why but as inheritance seems to be passed along anyway it doesn't really matter). Moving the callback call to outside of the \"if emit:\"-statement seems to solve the issue as far as I can see when trying it out. Any reason to keep it inside the if-statement?", "choices": "(A)                         other.figure.canvas.draw_idle()\n\n        self.stale = True\n        return v0, v1\n\n    def _set_artist_props(self, a):\n        if a is None:\n            return\n        a.set_figure(self.figure)\n\n    def _update_ticks(self):\n        \"\"\"\n        Update ticks (position and labels) using the current data interval of\n(B)             self.axes.callbacks.process(f\"{name}lim_changed\", self.axes)\n            # Call all of the other axes that are shared with this one\n            for other in self._get_shared_axes():\n                if other is not self.axes:\n                    other._axis_map[name]._set_lim(\n                        v0, v1, emit=False, auto=auto)\n                    if other.figure != self.figure:\n                        other.figure.canvas.draw_idle()\n\n        self.stale = True\n        return v0, v1\n\n    def _set_artist_props(self, a):\n(C)         if auto is not None:\n            self._set_autoscale_on(bool(auto))\n\n        if emit:\n            self.axes.callbacks.process(f\"{name}lim_changed\", self.axes)\n            # Call all of the other axes that are shared with this one\n            for other in self._get_shared_axes():\n                if other is not self.axes:\n                    other._axis_map[name]._set_lim(\n                        v0, v1, emit=False, auto=auto)\n                    if other.figure != self.figure:\n                        other.figure.canvas.draw_idle()\n\n(D)                     other._axis_map[name]._set_lim(\n                        v0, v1, emit=False, auto=auto)\n                    if other.figure != self.figure:\n                        other.figure.canvas.draw_idle()\n\n        self.stale = True\n        return v0, v1\n\n    def _set_artist_props(self, a):\n        if a is None:\n            return\n        a.set_figure(self.figure)\n", "answer": "B"}
{"uuid": "94a135ff-262d-4762-82f6-758a07be0830", "issue_statement": "Error creating AxisGrid with non-default axis class\n<!--To help us understand and resolve your issue, please fill out the form to the best of your ability.-->\r\n<!--You can feel free to delete the sections that do not apply.-->\r\n\r\n### Bug report\r\n\r\n**Bug summary**\r\n\r\nCreating `AxesGrid` using cartopy `GeoAxes` as `axis_class` raises `TypeError: 'method' object is not subscriptable`. Seems to be due to different behaviour of `axis` attr. for `mpl_toolkits.axes_grid1.mpl_axes.Axes` and other axes instances (like `GeoAxes`) where `axis` is only a callable. The error is raised in method `mpl_toolkits.axes_grid1.axes_grid._tick_only` when trying to access keys from `axis` attr.\r\n\r\n**Code for reproduction**\r\n\r\n<!--A minimum code snippet required to reproduce the bug.\r\nPlease make sure to minimize the number of dependencies required, and provide\r\nany necessary plotted data.\r\nAvoid using threads, as Matplotlib is (explicitly) not thread-safe.-->\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nfrom cartopy.crs import PlateCarree\r\nfrom cartopy.mpl.geoaxes import GeoAxes\r\nfrom mpl_toolkits.axes_grid1 import AxesGrid\r\n\r\nfig = plt.figure()\r\naxes_class = (GeoAxes, dict(map_projection=PlateCarree()))\r\ngr = AxesGrid(fig, 111, nrows_ncols=(1,1),\r\n              axes_class=axes_class)\r\n```\r\n\r\n**Actual outcome**\r\n\r\n<!--The output produced by the above code, which may be a screenshot, console output, etc.-->\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/jonasg/stuff/bugreport_mpl_toolkits_AxesGrid.py\", line 16, in <module>\r\n    axes_class=axes_class)\r\n\r\n  File \"/home/jonasg/miniconda3/envs/pya/lib/python3.7/site-packages/mpl_toolkits/axes_grid1/axes_grid.py\", line 618, in __init__\r\n    self.set_label_mode(label_mode)\r\n\r\n  File \"/home/jonasg/miniconda3/envs/pya/lib/python3.7/site-packages/mpl_toolkits/axes_grid1/axes_grid.py\", line 389, in set_label_mode\r\n    _tick_only(ax, bottom_on=False, left_on=False)\r\n\r\n  File \"/home/jonasg/miniconda3/envs/pya/lib/python3.7/site-packages/mpl_toolkits/axes_grid1/axes_grid.py\", line 27, in _tick_only\r\n    ax.axis[\"bottom\"].toggle(ticklabels=bottom_off, label=bottom_off)\r\n\r\nTypeError: 'method' object is not subscriptable\r\n```\r\n\r\n**Expected outcome**\r\n\r\n<!--A description of the expected outcome from the code snippet-->\r\n<!--If this used to work in an earlier version of Matplotlib, please note the version it used to work on-->\r\n\r\n**Matplotlib version**\r\n<!--Please specify your platform and versions of the relevant libraries you are using:-->\r\n  * Operating system: Ubuntu 18.04.4 LTS\r\n  * Matplotlib version: 3.1.2 (conda-forge)\r\n  * Matplotlib backend: Qt5Agg \r\n  * Python version: 3.7.6\r\n  * Jupyter version (if applicable):\r\n  * Other libraries: \r\n\r\n```\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                 conda_forge    conda-forge\r\n_openmp_mutex             4.5                       0_gnu    conda-forge\r\nalabaster                 0.7.12                   py37_0  \r\nantlr-python-runtime      4.7.2                 py37_1001    conda-forge\r\nargh                      0.26.2                   py37_0  \r\nastroid                   2.3.3                    py37_0  \r\natomicwrites              1.3.0                    py37_1  \r\nattrs                     19.3.0                     py_0    conda-forge\r\nautopep8                  1.4.4                      py_0  \r\nbabel                     2.8.0                      py_0  \r\nbackcall                  0.1.0                    py37_0  \r\nbasemap                   1.2.1            py37hd759880_1    conda-forge\r\nbleach                    3.1.0                    py37_0  \r\nbokeh                     1.4.0                    py37_0    conda-forge\r\nbzip2                     1.0.8                h516909a_2    conda-forge\r\nca-certificates           2019.11.28           hecc5488_0    conda-forge\r\ncartopy                   0.17.0          py37hd759880_1006    conda-forge\r\ncertifi                   2019.11.28               py37_0    conda-forge\r\ncf-units                  2.1.3            py37hc1659b7_0    conda-forge\r\ncf_units                  2.0.1           py37h3010b51_1002    conda-forge\r\ncffi                      1.13.2           py37h8022711_0    conda-forge\r\ncftime                    1.0.4.2          py37hc1659b7_0    conda-forge\r\nchardet                   3.0.4                 py37_1003    conda-forge\r\nclick                     7.0                        py_0    conda-forge\r\ncloudpickle               1.2.2                      py_1    conda-forge\r\ncryptography              2.8              py37h72c5cf5_1    conda-forge\r\ncurl                      7.65.3               hf8cf82a_0    conda-forge\r\ncycler                    0.10.0                     py_2    conda-forge\r\ncytoolz                   0.10.1           py37h516909a_0    conda-forge\r\ndask                      2.9.2                      py_0    conda-forge\r\ndask-core                 2.9.2                      py_0    conda-forge\r\ndbus                      1.13.6               he372182_0    conda-forge\r\ndecorator                 4.4.1                      py_0  \r\ndefusedxml                0.6.0                      py_0  \r\ndiff-match-patch          20181111                   py_0  \r\ndistributed               2.9.3                      py_0    conda-forge\r\ndocutils                  0.16                     py37_0  \r\nentrypoints               0.3                      py37_0  \r\nexpat                     2.2.5             he1b5a44_1004    conda-forge\r\nflake8                    3.7.9                    py37_0  \r\nfontconfig                2.13.1            h86ecdb6_1001    conda-forge\r\nfreetype                  2.10.0               he983fc9_1    conda-forge\r\nfsspec                    0.6.2                      py_0    conda-forge\r\nfuture                    0.18.2                   py37_0  \r\ngeonum                    1.4.4                      py_0    conda-forge\r\ngeos                      3.7.2                he1b5a44_2    conda-forge\r\ngettext                   0.19.8.1          hc5be6a0_1002    conda-forge\r\nglib                      2.58.3          py37h6f030ca_1002    conda-forge\r\ngmp                       6.1.2                h6c8ec71_1  \r\ngpxpy                     1.4.0                      py_0    conda-forge\r\ngst-plugins-base          1.14.5               h0935bb2_0    conda-forge\r\ngstreamer                 1.14.5               h36ae1b5_0    conda-forge\r\nhdf4                      4.2.13            hf30be14_1003    conda-forge\r\nhdf5                      1.10.5          nompi_h3c11f04_1104    conda-forge\r\nheapdict                  1.0.1                      py_0    conda-forge\r\nicu                       64.2                 he1b5a44_1    conda-forge\r\nidna                      2.8                   py37_1000    conda-forge\r\nimagesize                 1.2.0                      py_0  \r\nimportlib_metadata        1.4.0                    py37_0    conda-forge\r\nintervaltree              3.0.2                      py_0  \r\nipykernel                 5.1.4            py37h39e3cac_0  \r\nipython                   7.11.1           py37h39e3cac_0  \r\nipython_genutils          0.2.0                    py37_0  \r\niris                      2.2.0                 py37_1003    conda-forge\r\nisort                     4.3.21                   py37_0  \r\njedi                      0.14.1                   py37_0  \r\njeepney                   0.4.2                      py_0  \r\njinja2                    2.10.3                     py_0    conda-forge\r\njpeg                      9c                h14c3975_1001    conda-forge\r\njson5                     0.8.5                      py_0  \r\njsonschema                3.2.0                    py37_0  \r\njupyter_client            5.3.4                    py37_0  \r\njupyter_core              4.6.1                    py37_0  \r\njupyterlab                1.2.5              pyhf63ae98_0  \r\njupyterlab_server         1.0.6                      py_0  \r\nkeyring                   21.1.0                   py37_0  \r\nkiwisolver                1.1.0            py37hc9558a2_0    conda-forge\r\nkrb5                      1.16.4               h2fd8d38_0    conda-forge\r\nlatlon23                  1.0.7                      py_0    conda-forge\r\nlazy-object-proxy         1.4.3            py37h7b6447c_0  \r\nld_impl_linux-64          2.33.1               h53a641e_7    conda-forge\r\nlibblas                   3.8.0               14_openblas    conda-forge\r\nlibcblas                  3.8.0               14_openblas    conda-forge\r\nlibclang                  9.0.1           default_hde54327_0    conda-forge\r\nlibcurl                   7.65.3               hda55be3_0    conda-forge\r\nlibedit                   3.1.20170329      hf8c457e_1001    conda-forge\r\nlibffi                    3.2.1             he1b5a44_1006    conda-forge\r\nlibgcc-ng                 9.2.0                h24d8f2e_2    conda-forge\r\nlibgfortran-ng            7.3.0                hdf63c60_4    conda-forge\r\nlibgomp                   9.2.0                h24d8f2e_2    conda-forge\r\nlibiconv                  1.15              h516909a_1005    conda-forge\r\nliblapack                 3.8.0               14_openblas    conda-forge\r\nlibllvm9                  9.0.1                hc9558a2_0    conda-forge\r\nlibnetcdf                 4.7.3           nompi_h94020b1_100    conda-forge\r\nlibopenblas               0.3.7                h5ec1e0e_6    conda-forge\r\nlibpng                    1.6.37               hed695b0_0    conda-forge\r\nlibsodium                 1.0.16               h1bed415_0  \r\nlibspatialindex           1.9.3                he6710b0_0  \r\nlibssh2                   1.8.2                h22169c7_2    conda-forge\r\nlibstdcxx-ng              9.2.0                hdf63c60_2    conda-forge\r\nlibtiff                   4.1.0                hc3755c2_3    conda-forge\r\nlibuuid                   2.32.1            h14c3975_1000    conda-forge\r\nlibxcb                    1.13              h14c3975_1002    conda-forge\r\nlibxkbcommon              0.9.1                hebb1f50_0    conda-forge\r\nlibxml2                   2.9.10               hee79883_0    conda-forge\r\nlocket                    0.2.0                      py_2    conda-forge\r\nlz4-c                     1.8.3             he1b5a44_1001    conda-forge\r\nmarkupsafe                1.1.1            py37h516909a_0    conda-forge\r\nmatplotlib                3.1.2                    py37_1    conda-forge\r\nmatplotlib-base           3.1.2            py37h250f245_1    conda-forge\r\nmccabe                    0.6.1                    py37_1  \r\nmistune                   0.8.4            py37h7b6447c_0  \r\nmore-itertools            8.1.0                      py_0    conda-forge\r\nmsgpack-python            0.6.2            py37hc9558a2_0    conda-forge\r\nnbconvert                 5.6.1                    py37_0  \r\nnbformat                  5.0.4                      py_0  \r\nnbsphinx                  0.5.1                      py_0    conda-forge\r\nncurses                   6.1               hf484d3e_1002    conda-forge\r\nnetcdf4                   1.5.3           nompi_py37hd35fb8e_102    conda-forge\r\nnotebook                  6.0.3                    py37_0  \r\nnspr                      4.24                 he1b5a44_0    conda-forge\r\nnss                       3.47                 he751ad9_0    conda-forge\r\nnumpy                     1.17.5           py37h95a1406_0    conda-forge\r\nnumpydoc                  0.9.2                      py_0  \r\nolefile                   0.46                       py_0    conda-forge\r\nopenssl                   1.1.1d               h516909a_0    conda-forge\r\nowslib                    0.19.0                     py_2    conda-forge\r\npackaging                 20.0                       py_0    conda-forge\r\npandas                    0.25.3           py37hb3f55d8_0    conda-forge\r\npandoc                    2.2.3.2                       0  \r\npandocfilters             1.4.2                    py37_1  \r\nparso                     0.6.0                      py_0  \r\npartd                     1.1.0                      py_0    conda-forge\r\npathtools                 0.1.2                      py_1  \r\npatsy                     0.5.1                      py_0    conda-forge\r\npcre                      8.43                 he1b5a44_0    conda-forge\r\npexpect                   4.8.0                    py37_0  \r\npickleshare               0.7.5                    py37_0  \r\npillow                    7.0.0            py37hefe7db6_0    conda-forge\r\npip                       20.0.1                   py37_0    conda-forge\r\npluggy                    0.13.0                   py37_0    conda-forge\r\nproj4                     5.2.0             he1b5a44_1006    conda-forge\r\nprometheus_client         0.7.1                      py_0  \r\nprompt_toolkit            3.0.3                      py_0  \r\npsutil                    5.6.7            py37h516909a_0    conda-forge\r\npthread-stubs             0.4               h14c3975_1001    conda-forge\r\nptyprocess                0.6.0                    py37_0  \r\npy                        1.8.1                      py_0    conda-forge\r\npyaerocom                 0.9.0.dev5                dev_0    <develop>\r\npycodestyle               2.5.0                    py37_0  \r\npycparser                 2.19                     py37_1    conda-forge\r\npydocstyle                4.0.1                      py_0  \r\npyepsg                    0.4.0                      py_0    conda-forge\r\npyflakes                  2.1.1                    py37_0  \r\npygments                  2.5.2                      py_0  \r\npyinstrument              3.1.2                    pypi_0    pypi\r\npyinstrument-cext         0.2.2                    pypi_0    pypi\r\npykdtree                  1.3.1           py37hc1659b7_1002    conda-forge\r\npyke                      1.1.1                 py37_1001    conda-forge\r\npylint                    2.4.4                    py37_0  \r\npyopenssl                 19.1.0                   py37_0    conda-forge\r\npyparsing                 2.4.6                      py_0    conda-forge\r\npyproj                    1.9.6           py37h516909a_1002    conda-forge\r\npyqt                      5.12.3           py37hcca6a23_1    conda-forge\r\npyqt5-sip                 4.19.18                  pypi_0    pypi\r\npyqtwebengine             5.12.1                   pypi_0    pypi\r\npyrsistent                0.15.7           py37h7b6447c_0  \r\npyshp                     2.1.0                      py_0    conda-forge\r\npysocks                   1.7.1                    py37_0    conda-forge\r\npytest                    5.3.4                    py37_0    conda-forge\r\npython                    3.7.6                h357f687_2    conda-forge\r\npython-dateutil           2.8.1                      py_0    conda-forge\r\npython-jsonrpc-server     0.3.4                      py_0  \r\npython-language-server    0.31.7                   py37_0  \r\npytz                      2019.3                     py_0    conda-forge\r\npyxdg                     0.26                       py_0  \r\npyyaml                    5.3              py37h516909a_0    conda-forge\r\npyzmq                     18.1.0           py37he6710b0_0  \r\nqdarkstyle                2.8                        py_0  \r\nqt                        5.12.5               hd8c4c69_1    conda-forge\r\nqtawesome                 0.6.1                      py_0  \r\nqtconsole                 4.6.0                      py_1  \r\nqtpy                      1.9.0                      py_0  \r\nreadline                  8.0                  hf8c457e_0    conda-forge\r\nrequests                  2.22.0                   py37_1    conda-forge\r\nrope                      0.16.0                     py_0  \r\nrtree                     0.9.3                    py37_0  \r\nscipy                     1.4.1            py37h921218d_0    conda-forge\r\nseaborn                   0.9.0                      py_2    conda-forge\r\nsecretstorage             3.1.2                    py37_0  \r\nsend2trash                1.5.0                    py37_0  \r\nsetuptools                45.1.0                   py37_0    conda-forge\r\nshapely                   1.6.4           py37hec07ddf_1006    conda-forge\r\nsimplejson                3.17.0           py37h516909a_0    conda-forge\r\nsix                       1.14.0                   py37_0    conda-forge\r\nsnowballstemmer           2.0.0                      py_0  \r\nsortedcontainers          2.1.0                      py_0    conda-forge\r\nsphinx                    2.3.1                      py_0  \r\nsphinx-rtd-theme          0.4.3                    pypi_0    pypi\r\nsphinxcontrib-applehelp   1.0.1                      py_0  \r\nsphinxcontrib-devhelp     1.0.1                      py_0  \r\nsphinxcontrib-htmlhelp    1.0.2                      py_0  \r\nsphinxcontrib-jsmath      1.0.1                      py_0  \r\nsphinxcontrib-qthelp      1.0.2                      py_0  \r\nsphinxcontrib-serializinghtml 1.1.3                      py_0  \r\nspyder                    4.0.1                    py37_0  \r\nspyder-kernels            1.8.1                    py37_0  \r\nsqlite                    3.30.1               hcee41ef_0    conda-forge\r\nsrtm.py                   0.3.4                      py_0    conda-forge\r\nstatsmodels               0.11.0           py37h516909a_0    conda-forge\r\ntblib                     1.6.0                      py_0    conda-forge\r\nterminado                 0.8.3                    py37_0  \r\ntestpath                  0.4.4                      py_0  \r\ntk                        8.6.10               hed695b0_0    conda-forge\r\ntoolz                     0.10.0                     py_0    conda-forge\r\ntornado                   6.0.3            py37h516909a_0    conda-forge\r\ntqdm                      4.43.0                   pypi_0    pypi\r\ntraitlets                 4.3.3                    py37_0  \r\nudunits2                  2.2.27.6          h4e0c4b3_1001    conda-forge\r\nujson                     1.35             py37h14c3975_0  \r\nurllib3                   1.25.7                   py37_0    conda-forge\r\nwatchdog                  0.9.0                    py37_1  \r\nwcwidth                   0.1.8                      py_0    conda-forge\r\nwebencodings              0.5.1                    py37_1  \r\nwheel                     0.33.6                   py37_0    conda-forge\r\nwrapt                     1.11.2           py37h7b6447c_0  \r\nwurlitzer                 2.0.0                    py37_0  \r\nxarray                    0.14.1                     py_1    conda-forge\r\nxorg-libxau               1.0.9                h14c3975_0    conda-forge\r\nxorg-libxdmcp             1.1.3                h516909a_0    conda-forge\r\nxz                        5.2.4             h14c3975_1001    conda-forge\r\nyaml                      0.2.2                h516909a_1    conda-forge\r\nyapf                      0.28.0                     py_0  \r\nzeromq                    4.3.1                he6710b0_3  \r\nzict                      1.0.0                      py_0    conda-forge\r\nzipp                      2.0.0                      py_2    conda-forge\r\nzlib                      1.2.11            h516909a_1006    conda-forge\r\nzstd                      1.4.4                h3b9ef0a_1    conda-forge\r\n```", "choices": "(A)         self.orientation = orientation\n        super().__init__(*args, **kwargs)\n\n    def colorbar(self, mappable, **kwargs):\n        return self.figure.colorbar(\n            mappable, cax=self, location=self.orientation, **kwargs)\n\n    @_api.deprecated(\"3.8\", alternative=\"ax.tick_params and colorbar.set_label\")\n    def toggle_label(self, b):\n        axis = self.axis[self.orientation]\n        axis.toggle(ticklabels=b, label=b)\n\n\n_cbaraxes_class_factory = cbook._make_class_factory(CbarAxesBase, \"Cbar{}\")\n\n\nclass Grid:\n    \"\"\"\n    A grid of Axes.\n\n    In Matplotlib, the Axes location (and size) is specified in normalized\n    figure coordinates. This may not be ideal for images that needs to be\n    displayed with a given aspect ratio; for example, it is difficult to\n    display multiple images of a same size with some fixed padding between\n    them.  AxesGrid can be used in such case.\n    \"\"\"\n\n(B)     left_off = not left_on\n    ax.axis[\"bottom\"].toggle(ticklabels=bottom_off, label=bottom_off)\n    ax.axis[\"left\"].toggle(ticklabels=left_off, label=left_off)\n\n\nclass CbarAxesBase:\n    def __init__(self, *args, orientation, **kwargs):\n        self.orientation = orientation\n        super().__init__(*args, **kwargs)\n\n    def colorbar(self, mappable, **kwargs):\n        return self.figure.colorbar(\n            mappable, cax=self, location=self.orientation, **kwargs)\n\n    @_api.deprecated(\"3.8\", alternative=\"ax.tick_params and colorbar.set_label\")\n    def toggle_label(self, b):\n        axis = self.axis[self.orientation]\n        axis.toggle(ticklabels=b, label=b)\n\n\n_cbaraxes_class_factory = cbook._make_class_factory(CbarAxesBase, \"Cbar{}\")\n\n\nclass Grid:\n    \"\"\"\n    A grid of Axes.\n\n(C) \nfrom .axes_divider import Size, SubplotDivider, Divider\nfrom .mpl_axes import Axes\n\n\ndef _tick_only(ax, bottom_on, left_on):\n    bottom_off = not bottom_on\n    left_off = not left_on\n    ax.axis[\"bottom\"].toggle(ticklabels=bottom_off, label=bottom_off)\n    ax.axis[\"left\"].toggle(ticklabels=left_off, label=left_off)\n\n\nclass CbarAxesBase:\n    def __init__(self, *args, orientation, **kwargs):\n        self.orientation = orientation\n        super().__init__(*args, **kwargs)\n\n    def colorbar(self, mappable, **kwargs):\n        return self.figure.colorbar(\n            mappable, cax=self, location=self.orientation, **kwargs)\n\n    @_api.deprecated(\"3.8\", alternative=\"ax.tick_params and colorbar.set_label\")\n    def toggle_label(self, b):\n        axis = self.axis[self.orientation]\n        axis.toggle(ticklabels=b, label=b)\n\n\n(D) from numbers import Number\nimport functools\n\nimport numpy as np\n\nfrom matplotlib import _api, cbook\nfrom matplotlib.gridspec import SubplotSpec\n\nfrom .axes_divider import Size, SubplotDivider, Divider\nfrom .mpl_axes import Axes\n\n\ndef _tick_only(ax, bottom_on, left_on):\n    bottom_off = not bottom_on\n    left_off = not left_on\n    ax.axis[\"bottom\"].toggle(ticklabels=bottom_off, label=bottom_off)\n    ax.axis[\"left\"].toggle(ticklabels=left_off, label=left_off)\n\n\nclass CbarAxesBase:\n    def __init__(self, *args, orientation, **kwargs):\n        self.orientation = orientation\n        super().__init__(*args, **kwargs)\n\n    def colorbar(self, mappable, **kwargs):\n        return self.figure.colorbar(\n            mappable, cax=self, location=self.orientation, **kwargs)", "answer": "D"}
{"uuid": "6187df21-2dcb-4f35-bab8-1f4208a63800", "issue_statement": "pairplot fails with hue_order not containing all hue values in seaborn 0.11.1\nIn seaborn < 0.11, one could plot only a subset of the values in the hue column, by passing a hue_order list containing only the desired values. Points with hue values not in the list were simply not plotted.\n```python\niris = sns.load_dataset(\"iris\")`\n# The hue column contains three different species; here we want to plot two\nsns.pairplot(iris, hue=\"species\", hue_order=[\"setosa\", \"versicolor\"])\n```\n\nThis no longer works in 0.11.1. Passing a hue_order list that does not contain some of the values in the hue column raises a long, ugly error traceback. The first exception arises in seaborn/_core.py:\n```\nTypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n```\nseaborn version: 0.11.1\nmatplotlib version: 3.3.2\nmatplotlib backends: MacOSX, Agg or jupyter notebook inline.", "choices": "(A) \n    def _lookup_single(self, key):\n        \"\"\"Get the color for a single value, using colormap to interpolate.\"\"\"\n        try:\n            # Use a value that's in the original data vector\n            value = self.lookup_table[key]\n        except KeyError:\n            # Use the colormap to interpolate between existing datapoints\n            # (e.g. in the context of making a continuous legend)\n            try:\n                normed = self.norm(key)\n            except TypeError as err:\n                if np.isnan(key):\n(B)             # Use a value that's in the original data vector\n            value = self.lookup_table[key]\n        except KeyError:\n            # Use the colormap to interpolate between existing datapoints\n            # (e.g. in the context of making a continuous legend)\n            try:\n                normed = self.norm(key)\n            except TypeError as err:\n                if np.isnan(key):\n                    value = (0, 0, 0, 0)\n                else:\n                    raise err\n            else:\n(C)             # (e.g. in the context of making a continuous legend)\n            try:\n                normed = self.norm(key)\n            except TypeError as err:\n                if np.isnan(key):\n                    value = (0, 0, 0, 0)\n                else:\n                    raise err\n            else:\n                if np.ma.is_masked(normed):\n                    normed = np.nan\n                value = self.cmap(normed)\n        return value\n(D)             except TypeError as err:\n                if np.isnan(key):\n                    value = (0, 0, 0, 0)\n                else:\n                    raise err\n            else:\n                if np.ma.is_masked(normed):\n                    normed = np.nan\n                value = self.cmap(normed)\n        return value\n\n    def infer_map_type(self, palette, norm, input_format, var_type):\n        \"\"\"Determine how to implement the mapping.\"\"\"", "answer": "B"}
{"uuid": "6cadc2b3-f220-4142-822e-be9cacfe789b", "issue_statement": "PolyFit is not robust to missing data\n```python\r\nso.Plot([1, 2, 3, None, 4], [1, 2, 3, 4, 5]).add(so.Line(), so.PolyFit())\r\n```\r\n\r\n<details><summary>Traceback</summary>\r\n\r\n```python-traceback\r\n---------------------------------------------------------------------------\r\nLinAlgError                               Traceback (most recent call last)\r\nFile ~/miniconda3/envs/seaborn-py39-latest/lib/python3.9/site-packages/IPython/core/formatters.py:343, in BaseFormatter.__call__(self, obj)\r\n    341     method = get_real_method(obj, self.print_method)\r\n    342     if method is not None:\r\n--> 343         return method()\r\n    344     return None\r\n    345 else:\r\n\r\nFile ~/code/seaborn/seaborn/_core/plot.py:265, in Plot._repr_png_(self)\r\n    263 def _repr_png_(self) -> tuple[bytes, dict[str, float]]:\r\n--> 265     return self.plot()._repr_png_()\r\n\r\nFile ~/code/seaborn/seaborn/_core/plot.py:804, in Plot.plot(self, pyplot)\r\n    800 \"\"\"\r\n    801 Compile the plot spec and return the Plotter object.\r\n    802 \"\"\"\r\n    803 with theme_context(self._theme_with_defaults()):\r\n--> 804     return self._plot(pyplot)\r\n\r\nFile ~/code/seaborn/seaborn/_core/plot.py:822, in Plot._plot(self, pyplot)\r\n    819 plotter._setup_scales(self, common, layers, coord_vars)\r\n    821 # Apply statistical transform(s)\r\n--> 822 plotter._compute_stats(self, layers)\r\n    824 # Process scale spec for semantic variables and coordinates computed by stat\r\n    825 plotter._setup_scales(self, common, layers)\r\n\r\nFile ~/code/seaborn/seaborn/_core/plot.py:1110, in Plotter._compute_stats(self, spec, layers)\r\n   1108     grouper = grouping_vars\r\n   1109 groupby = GroupBy(grouper)\r\n-> 1110 res = stat(df, groupby, orient, scales)\r\n   1112 if pair_vars:\r\n   1113     data.frames[coord_vars] = res\r\n\r\nFile ~/code/seaborn/seaborn/_stats/regression.py:41, in PolyFit.__call__(self, data, groupby, orient, scales)\r\n     39 def __call__(self, data, groupby, orient, scales):\r\n---> 41     return groupby.apply(data, self._fit_predict)\r\n\r\nFile ~/code/seaborn/seaborn/_core/groupby.py:109, in GroupBy.apply(self, data, func, *args, **kwargs)\r\n    106 grouper, groups = self._get_groups(data)\r\n    108 if not grouper:\r\n--> 109     return self._reorder_columns(func(data, *args, **kwargs), data)\r\n    111 parts = {}\r\n    112 for key, part_df in data.groupby(grouper, sort=False):\r\n\r\nFile ~/code/seaborn/seaborn/_stats/regression.py:30, in PolyFit._fit_predict(self, data)\r\n     28     xx = yy = []\r\n     29 else:\r\n---> 30     p = np.polyfit(x, y, self.order)\r\n     31     xx = np.linspace(x.min(), x.max(), self.gridsize)\r\n     32     yy = np.polyval(p, xx)\r\n\r\nFile <__array_function__ internals>:180, in polyfit(*args, **kwargs)\r\n\r\nFile ~/miniconda3/envs/seaborn-py39-latest/lib/python3.9/site-packages/numpy/lib/polynomial.py:668, in polyfit(x, y, deg, rcond, full, w, cov)\r\n    666 scale = NX.sqrt((lhs*lhs).sum(axis=0))\r\n    667 lhs /= scale\r\n--> 668 c, resids, rank, s = lstsq(lhs, rhs, rcond)\r\n    669 c = (c.T/scale).T  # broadcast scale coefficients\r\n    671 # warn on rank reduction, which indicates an ill conditioned matrix\r\n\r\nFile <__array_function__ internals>:180, in lstsq(*args, **kwargs)\r\n\r\nFile ~/miniconda3/envs/seaborn-py39-latest/lib/python3.9/site-packages/numpy/linalg/linalg.py:2300, in lstsq(a, b, rcond)\r\n   2297 if n_rhs == 0:\r\n   2298     # lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\r\n   2299     b = zeros(b.shape[:-2] + (m, n_rhs + 1), dtype=b.dtype)\r\n-> 2300 x, resids, rank, s = gufunc(a, b, rcond, signature=signature, extobj=extobj)\r\n   2301 if m == 0:\r\n   2302     x[...] = 0\r\n\r\nFile ~/miniconda3/envs/seaborn-py39-latest/lib/python3.9/site-packages/numpy/linalg/linalg.py:101, in _raise_linalgerror_lstsq(err, flag)\r\n    100 def _raise_linalgerror_lstsq(err, flag):\r\n--> 101     raise LinAlgError(\"SVD did not converge in Linear Least Squares\")\r\n\r\nLinAlgError: SVD did not converge in Linear Least Squares\r\n\r\n```\r\n\r\n</details>", "choices": "(A)             p = np.polyfit(x, y, self.order)\n            xx = np.linspace(x.min(), x.max(), self.gridsize)\n            yy = np.polyval(p, xx)\n\n        return pd.DataFrame(dict(x=xx, y=yy))\n\n    # TODO we should have a way of identifying the method that will be applied\n    # and then only define __call__ on a base-class of stats with this pattern\n\n    def __call__(self, data, groupby, orient, scales):\n(B) \n    def __call__(self, data, groupby, orient, scales):\n\n        return groupby.apply(data, self._fit_predict)\n\n\n@dataclass\nclass OLSFit(Stat):\n\n    ...\n(C) \n    # TODO we should have a way of identifying the method that will be applied\n    # and then only define __call__ on a base-class of stats with this pattern\n\n    def __call__(self, data, groupby, orient, scales):\n\n        return groupby.apply(data, self._fit_predict)\n\n\n@dataclass\n(D) \n        return pd.DataFrame(dict(x=xx, y=yy))\n\n    # TODO we should have a way of identifying the method that will be applied\n    # and then only define __call__ on a base-class of stats with this pattern\n\n    def __call__(self, data, groupby, orient, scales):\n\n        return groupby.apply(data, self._fit_predict)\n", "answer": "B"}
{"uuid": "cf5a9b35-f4a6-478d-ac18-dd55bd47d56a", "issue_statement": "Color mapping fails with boolean data\n```python\r\nso.Plot([\"a\", \"b\"], [1, 2], color=[True, False]).add(so.Bar())\r\n```\r\n```python-traceback\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n...\r\nFile ~/code/seaborn/seaborn/_core/plot.py:841, in Plot._plot(self, pyplot)\r\n    838 plotter._compute_stats(self, layers)\r\n    840 # Process scale spec for semantic variables and coordinates computed by stat\r\n--> 841 plotter._setup_scales(self, common, layers)\r\n    843 # TODO Remove these after updating other methods\r\n    844 # ---- Maybe have debug= param that attaches these when True?\r\n    845 plotter._data = common\r\n\r\nFile ~/code/seaborn/seaborn/_core/plot.py:1252, in Plotter._setup_scales(self, p, common, layers, variables)\r\n   1250     self._scales[var] = Scale._identity()\r\n   1251 else:\r\n-> 1252     self._scales[var] = scale._setup(var_df[var], prop)\r\n   1254 # Everything below here applies only to coordinate variables\r\n   1255 # We additionally skip it when we're working with a value\r\n   1256 # that is derived from a coordinate we've already processed.\r\n   1257 # e.g., the Stat consumed y and added ymin/ymax. In that case,\r\n   1258 # we've already setup the y scale and ymin/max are in scale space.\r\n   1259 if axis is None or (var != coord and coord in p._variables):\r\n\r\nFile ~/code/seaborn/seaborn/_core/scales.py:351, in ContinuousBase._setup(self, data, prop, axis)\r\n    349 vmin, vmax = axis.convert_units((vmin, vmax))\r\n    350 a = forward(vmin)\r\n--> 351 b = forward(vmax) - forward(vmin)\r\n    353 def normalize(x):\r\n    354     return (x - a) / b\r\n\r\nTypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\r\n```", "choices": "(A)                 vmin, vmax = data.min(), data.max()\n            else:\n                vmin, vmax = new.norm\n            vmin, vmax = axis.convert_units((vmin, vmax))\n            a = forward(vmin)\n            b = forward(vmax) - forward(vmin)\n\n(B)         if prop.normed:\n            if new.norm is None:\n                vmin, vmax = data.min(), data.max()\n            else:\n                vmin, vmax = new.norm\n            vmin, vmax = axis.convert_units((vmin, vmax))\n            a = forward(vmin)\n(C)             a = forward(vmin)\n            b = forward(vmax) - forward(vmin)\n\n            def normalize(x):\n                return (x - a) / b\n\n        else:\n(D)                 vmin, vmax = new.norm\n            vmin, vmax = axis.convert_units((vmin, vmax))\n            a = forward(vmin)\n            b = forward(vmax) - forward(vmin)\n\n            def normalize(x):\n                return (x - a) / b", "answer": "A"}
{"uuid": "382aa273-ee6a-41e7-8118-0d3bf474e7b8", "issue_statement": "pairplot raises KeyError with MultiIndex DataFrame\nWhen trying to pairplot a MultiIndex DataFrame, `pairplot` raises a `KeyError`:\r\n\r\nMRE:\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport seaborn as sns\r\n\r\n\r\ndata = {\r\n    (\"A\", \"1\"): np.random.rand(100),\r\n    (\"A\", \"2\"): np.random.rand(100),\r\n    (\"B\", \"1\"): np.random.rand(100),\r\n    (\"B\", \"2\"): np.random.rand(100),\r\n}\r\ndf = pd.DataFrame(data)\r\nsns.pairplot(df)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n[c:\\Users\\KLuu\\anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py](file:///C:/Users/KLuu/anaconda3/lib/site-packages/seaborn/axisgrid.py) in pairplot(data, hue, hue_order, palette, vars, x_vars, y_vars, kind, diag_kind, markers, height, aspect, corner, dropna, plot_kws, diag_kws, grid_kws, size)\r\n   2142     diag_kws.setdefault(\"legend\", False)\r\n   2143     if diag_kind == \"hist\":\r\n-> 2144         grid.map_diag(histplot, **diag_kws)\r\n   2145     elif diag_kind == \"kde\":\r\n   2146         diag_kws.setdefault(\"fill\", True)\r\n\r\n[c:\\Users\\KLuu\\anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py](file:///C:/Users/KLuu/anaconda3/lib/site-packages/seaborn/axisgrid.py) in map_diag(self, func, **kwargs)\r\n   1488                 plt.sca(ax)\r\n   1489 \r\n-> 1490             vector = self.data[var]\r\n   1491             if self._hue_var is not None:\r\n   1492                 hue = self.data[self._hue_var]\r\n\r\n[c:\\Users\\KLuu\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py](file:///C:/Users/KLuu/anaconda3/lib/site-packages/pandas/core/frame.py) in __getitem__(self, key)\r\n   3765             if is_iterator(key):\r\n   3766                 key = list(key)\r\n-> 3767             indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\r\n   3768 \r\n   3769         # take() does not accept boolean indexers\r\n\r\n[c:\\Users\\KLuu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py](file:///C:/Users/KLuu/anaconda3/lib/site-packages/pandas/core/indexes/multi.py) in _get_indexer_strict(self, key, axis_name)\r\n   2534             indexer = self._get_indexer_level_0(keyarr)\r\n   2535 \r\n-> 2536             self._raise_if_missing(key, indexer, axis_name)\r\n   2537             return self[indexer], indexer\r\n   2538 \r\n\r\n[c:\\Users\\KLuu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py](file:///C:/Users/KLuu/anaconda3/lib/site-packages/pandas/core/indexes/multi.py) in _raise_if_missing(self, key, indexer, axis_name)\r\n   2552                 cmask = check == -1\r\n   2553                 if cmask.any():\r\n-> 2554                     raise KeyError(f\"{keyarr[cmask]} not in index\")\r\n   2555                 # We get here when levels still contain values which are not\r\n   2556                 # actually in Index anymore\r\n\r\nKeyError: \"['1'] not in index\"\r\n```\r\n\r\nA workaround is to \"flatten\" the columns:\r\n\r\n```python\r\ndf.columns = [\"\".join(column) for column in df.columns]\r\n```", "choices": "(A) \n            self.diag_vars = np.array(diag_vars, np.object_)\n            self.diag_axes = np.array(diag_axes, np.object_)\n\n        if \"hue\" not in signature(func).parameters:\n            return self._map_diag_iter_hue(func, **kwargs)\n\n        # Loop over diagonal variables and axes, making one plot in each\n(B)                 for ax in diag_axes[1:]:\n                    share_axis(diag_axes[0], ax, \"y\")\n\n            self.diag_vars = np.array(diag_vars, np.object_)\n            self.diag_axes = np.array(diag_axes, np.object_)\n\n        if \"hue\" not in signature(func).parameters:\n            return self._map_diag_iter_hue(func, **kwargs)\n(C)             self.diag_axes = np.array(diag_axes, np.object_)\n\n        if \"hue\" not in signature(func).parameters:\n            return self._map_diag_iter_hue(func, **kwargs)\n\n        # Loop over diagonal variables and axes, making one plot in each\n        for var, ax in zip(self.diag_vars, self.diag_axes):\n\n(D) \n            if self.diag_sharey and diag_axes:\n                for ax in diag_axes[1:]:\n                    share_axis(diag_axes[0], ax, \"y\")\n\n            self.diag_vars = np.array(diag_vars, np.object_)\n            self.diag_axes = np.array(diag_axes, np.object_)\n", "answer": "B"}
{"uuid": "d6603121-3449-48fa-896d-2c0679e6ab80", "issue_statement": "Raise error when blueprint name contains a dot\nThis is required since every dot is now significant since blueprints can be nested. An error was already added for endpoint names in 1.0, but should have been added for this as well.", "choices": "(A)         self._got_registered_once = True\n        state = self.make_setup_state(app, options, first_registration)\n\n        if self.has_static_folder:\n            state.add_url_rule(\n                f\"{self.static_url_path}/<path:filename>\",\n                view_func=self.send_static_file,\n                endpoint=\"static\",\n            )\n\n        # Merge blueprint data into parent.\n        if first_registration:\n\n            def extend(bp_dict, parent_dict):\n                for key, values in bp_dict.items():\n                    key = self.name if key is None else f\"{self.name}.{key}\"\n\n                    parent_dict[key].extend(values)\n\n            for key, value in self.error_handler_spec.items():\n                key = self.name if key is None else f\"{self.name}.{key}\"\n                value = defaultdict(\n                    dict,\n                    {\n                        code: {\n                            exc_class: func for exc_class, func in code_values.items()\n                        }\n                        for code, code_values in value.items()\n                    },\n                )\n                app.error_handler_spec[key] = value\n\n            for endpoint, func in self.view_functions.items():\n                app.view_functions[endpoint] = func\n\n            extend(self.before_request_funcs, app.before_request_funcs)\n            extend(self.after_request_funcs, app.after_request_funcs)\n            extend(\n                self.teardown_request_funcs,\n                app.teardown_request_funcs,\n            )\n            extend(self.url_default_functions, app.url_default_functions)\n            extend(self.url_value_preprocessors, app.url_value_preprocessors)\n            extend(self.template_context_processors, app.template_context_processors)\n\n        for deferred in self.deferred_functions:\n            deferred(state)\n\n        cli_resolved_group = options.get(\"cli_group\", self.cli_group)\n\n        if self.cli.commands:\n            if cli_resolved_group is None:\n                app.cli.commands.update(self.cli.commands)\n            elif cli_resolved_group is _sentinel:\n                self.cli.name = self.name\n                app.cli.add_command(self.cli)\n            else:\n                self.cli.name = cli_resolved_group\n                app.cli.add_command(self.cli)\n\n        for blueprint, bp_options in self._blueprints:\n            url_prefix = options.get(\"url_prefix\", \"\")\n            if \"url_prefix\" in bp_options:\n                url_prefix = (\n                    url_prefix.rstrip(\"/\") + \"/\" + bp_options[\"url_prefix\"].lstrip(\"/\")\n                )\n\n            bp_options[\"url_prefix\"] = url_prefix\n            bp_options[\"name_prefix\"] = options.get(\"name_prefix\", \"\") + self.name + \".\"\n            blueprint.register(app, bp_options)\n\n    def add_url_rule(\n        self,\n        rule: str,\n        endpoint: t.Optional[str] = None,\n        view_func: t.Optional[t.Callable] = None,\n        **options: t.Any,\n    ) -> None:\n        \"\"\"Like :meth:`Flask.add_url_rule` but for a blueprint.  The endpoint for\n        the :func:`url_for` function is prefixed with the name of the blueprint.\n        \"\"\"\n        if endpoint:\n            assert \".\" not in endpoint, \"Blueprint endpoints should not contain dots\"\n        if view_func and hasattr(view_func, \"__name__\"):\n            assert (\n                \".\" not in view_func.__name__\n            ), \"Blueprint view function name should not contain dots\"\n        self.record(lambda s: s.add_url_rule(rule, endpoint, view_func, **options))\n\n    def app_template_filter(self, name: t.Optional[str] = None) -> t.Callable:\n        \"\"\"Register a custom template filter, available application wide.  Like\n        :meth:`Flask.template_filter` but for a blueprint.\n\n        :param name: the optional name of the filter, otherwise the\n                     function name will be used.\n        \"\"\"\n\n        def decorator(f: TemplateFilterCallable) -> TemplateFilterCallable:\n            self.add_app_template_filter(f, name=name)\n            return f\n\n        return decorator\n\n    def add_app_template_filter(\n        self, f: TemplateFilterCallable, name: t.Optional[str] = None\n    ) -> None:\n        \"\"\"Register a custom template filter, available application wide.  Like\n        :meth:`Flask.add_template_filter` but for a blueprint.  Works exactly\n        like the :meth:`app_template_filter` decorator.\n\n        :param name: the optional name of the filter, otherwise the\n                     function name will be used.\n        \"\"\"\n\n        def register_template(state: BlueprintSetupState) -> None:\n            state.app.jinja_env.filters[name or f.__name__] = f\n\n        self.record_once(register_template)\n\n    def app_template_test(self, name: t.Optional[str] = None) -> t.Callable:\n        \"\"\"Register a custom template test, available application wide.  Like\n        :meth:`Flask.template_test` but for a blueprint.\n\n        .. versionadded:: 0.10\n\n        :param name: the optional name of the test, otherwise the\n                     function name will be used.\n        \"\"\"\n\n        def decorator(f: TemplateTestCallable) -> TemplateTestCallable:\n            self.add_app_template_test(f, name=name)\n            return f\n\n        return decorator\n\n    def add_app_template_test(\n        self, f: TemplateTestCallable, name: t.Optional[str] = None\n    ) -> None:\n        \"\"\"Register a custom template test, available application wide.  Like\n        :meth:`Flask.add_template_test` but for a blueprint.  Works exactly\n        like the :meth:`app_template_test` decorator.\n\n        .. versionadded:: 0.10\n\n        :param name: the optional name of the test, otherwise the\n                     function name will be used.\n        \"\"\"\n\n        def register_template(state: BlueprintSetupState) -> None:\n            state.app.jinja_env.tests[name or f.__name__] = f\n\n        self.record_once(register_template)\n\n    def app_template_global(self, name: t.Optional[str] = None) -> t.Callable:\n        \"\"\"Register a custom template global, available application wide.  Like\n        :meth:`Flask.template_global` but for a blueprint.\n\n        .. versionadded:: 0.10\n\n        :param name: the optional name of the global, otherwise the\n                     function name will be used.\n        \"\"\"\n\n        def decorator(f: TemplateGlobalCallable) -> TemplateGlobalCallable:\n            self.add_app_template_global(f, name=name)\n            return f\n\n        return decorator\n\n    def add_app_template_global(\n        self, f: TemplateGlobalCallable, name: t.Optional[str] = None\n    ) -> None:\n        \"\"\"Register a custom template global, available application wide.  Like\n        :meth:`Flask.add_template_global` but for a blueprint.  Works exactly\n        like the :meth:`app_template_global` decorator.\n\n        .. versionadded:: 0.10\n\n        :param name: the optional name of the global, otherwise the\n                     function name will be used.\n        \"\"\"\n\n        def register_template(state: BlueprintSetupState) -> None:\n            state.app.jinja_env.globals[name or f.__name__] = f\n\n        self.record_once(register_template)\n\n    def before_app_request(self, f: BeforeRequestCallable) -> BeforeRequestCallable:\n(B)         in the app's templates folder.\n    :param url_prefix: A path to prepend to all of the blueprint's URLs,\n        to make them distinct from the rest of the app's routes.\n    :param subdomain: A subdomain that blueprint routes will match on by\n        default.\n    :param url_defaults: A dict of default values that blueprint routes\n        will receive by default.\n    :param root_path: By default, the blueprint will automatically set\n        this based on ``import_name``. In certain situations this\n        automatic detection can fail, so the path can be specified\n        manually instead.\n\n    .. versionchanged:: 1.1.0\n        Blueprints have a ``cli`` group to register nested CLI commands.\n        The ``cli_group`` parameter controls the name of the group under\n        the ``flask`` command.\n\n    .. versionadded:: 0.7\n    \"\"\"\n\n    warn_on_modifications = False\n    _got_registered_once = False\n\n    #: Blueprint local JSON encoder class to use. Set to ``None`` to use\n    #: the app's :class:`~flask.Flask.json_encoder`.\n    json_encoder = None\n    #: Blueprint local JSON decoder class to use. Set to ``None`` to use\n    #: the app's :class:`~flask.Flask.json_decoder`.\n    json_decoder = None\n\n    def __init__(\n        self,\n        name: str,\n        import_name: str,\n        static_folder: t.Optional[str] = None,\n        static_url_path: t.Optional[str] = None,\n        template_folder: t.Optional[str] = None,\n        url_prefix: t.Optional[str] = None,\n        subdomain: t.Optional[str] = None,\n        url_defaults: t.Optional[dict] = None,\n        root_path: t.Optional[str] = None,\n        cli_group: t.Optional[str] = _sentinel,  # type: ignore\n    ):\n        super().__init__(\n            import_name=import_name,\n            static_folder=static_folder,\n            static_url_path=static_url_path,\n            template_folder=template_folder,\n            root_path=root_path,\n        )\n        self.name = name\n        self.url_prefix = url_prefix\n        self.subdomain = subdomain\n        self.deferred_functions: t.List[DeferredSetupFunction] = []\n\n        if url_defaults is None:\n            url_defaults = {}\n\n        self.url_values_defaults = url_defaults\n        self.cli_group = cli_group\n        self._blueprints: t.List[t.Tuple[\"Blueprint\", dict]] = []\n\n    def _is_setup_finished(self) -> bool:\n        return self.warn_on_modifications and self._got_registered_once\n\n    def record(self, func: t.Callable) -> None:\n        \"\"\"Registers a function that is called when the blueprint is\n        registered on the application.  This function is called with the\n        state as argument as returned by the :meth:`make_setup_state`\n        method.\n        \"\"\"\n        if self._got_registered_once and self.warn_on_modifications:\n            from warnings import warn\n\n            warn(\n                Warning(\n                    \"The blueprint was already registered once but is\"\n                    \" getting modified now. These changes will not show\"\n                    \" up.\"\n                )\n            )\n        self.deferred_functions.append(func)\n\n    def record_once(self, func: t.Callable) -> None:\n        \"\"\"Works like :meth:`record` but wraps the function in another\n        function that will ensure the function is only called once.  If the\n        blueprint is registered a second time on the application, the\n        function passed is not called.\n        \"\"\"\n\n        def wrapper(state: BlueprintSetupState) -> None:\n            if state.first_registration:\n                func(state)\n\n        return self.record(update_wrapper(wrapper, func))\n\n    def make_setup_state(\n        self, app: \"Flask\", options: dict, first_registration: bool = False\n    ) -> BlueprintSetupState:\n        \"\"\"Creates an instance of :meth:`~flask.blueprints.BlueprintSetupState`\n        object that is later passed to the register callback functions.\n        Subclasses can override this to return a subclass of the setup state.\n        \"\"\"\n        return BlueprintSetupState(self, app, options, first_registration)\n\n    def register_blueprint(self, blueprint: \"Blueprint\", **options: t.Any) -> None:\n        \"\"\"Register a :class:`~flask.Blueprint` on this blueprint. Keyword\n        arguments passed to this method will override the defaults set\n        on the blueprint.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        self._blueprints.append((blueprint, options))\n\n    def register(self, app: \"Flask\", options: dict) -> None:\n        \"\"\"Called by :meth:`Flask.register_blueprint` to register all\n        views and callbacks registered on the blueprint with the\n        application. Creates a :class:`.BlueprintSetupState` and calls\n        each :meth:`record` callbackwith it.\n\n        :param app: The application this blueprint is being registered\n            with.\n        :param options: Keyword arguments forwarded from\n            :meth:`~Flask.register_blueprint`.\n        :param first_registration: Whether this is the first time this\n            blueprint has been registered on the application.\n        \"\"\"\n        first_registration = False\n\n        if self.name in app.blueprints:\n            assert app.blueprints[self.name] is self, (\n                \"A name collision occurred between blueprints\"\n                f\" {self!r} and {app.blueprints[self.name]!r}.\"\n                f\" Both share the same name {self.name!r}.\"\n                f\" Blueprints that are created on the fly need unique\"\n                f\" names.\"\n            )\n        else:\n            app.blueprints[self.name] = self\n            first_registration = True\n\n        self._got_registered_once = True\n        state = self.make_setup_state(app, options, first_registration)\n\n        if self.has_static_folder:\n            state.add_url_rule(\n                f\"{self.static_url_path}/<path:filename>\",\n                view_func=self.send_static_file,\n                endpoint=\"static\",\n            )\n\n        # Merge blueprint data into parent.\n        if first_registration:\n\n            def extend(bp_dict, parent_dict):\n                for key, values in bp_dict.items():\n                    key = self.name if key is None else f\"{self.name}.{key}\"\n\n                    parent_dict[key].extend(values)\n\n            for key, value in self.error_handler_spec.items():\n                key = self.name if key is None else f\"{self.name}.{key}\"\n                value = defaultdict(\n                    dict,\n                    {\n                        code: {\n                            exc_class: func for exc_class, func in code_values.items()\n                        }\n                        for code, code_values in value.items()\n                    },\n                )\n                app.error_handler_spec[key] = value\n\n            for endpoint, func in self.view_functions.items():\n                app.view_functions[endpoint] = func\n\n            extend(self.before_request_funcs, app.before_request_funcs)\n            extend(self.after_request_funcs, app.after_request_funcs)\n            extend(\n                self.teardown_request_funcs,\n                app.teardown_request_funcs,\n            )\n            extend(self.url_default_functions, app.url_default_functions)\n            extend(self.url_value_preprocessors, app.url_value_preprocessors)\n            extend(self.template_context_processors, app.template_context_processors)\n\n        for deferred in self.deferred_functions:\n            deferred(state)\n(C)             template_folder=template_folder,\n            root_path=root_path,\n        )\n        self.name = name\n        self.url_prefix = url_prefix\n        self.subdomain = subdomain\n        self.deferred_functions: t.List[DeferredSetupFunction] = []\n\n        if url_defaults is None:\n            url_defaults = {}\n\n        self.url_values_defaults = url_defaults\n        self.cli_group = cli_group\n        self._blueprints: t.List[t.Tuple[\"Blueprint\", dict]] = []\n\n    def _is_setup_finished(self) -> bool:\n        return self.warn_on_modifications and self._got_registered_once\n\n    def record(self, func: t.Callable) -> None:\n        \"\"\"Registers a function that is called when the blueprint is\n        registered on the application.  This function is called with the\n        state as argument as returned by the :meth:`make_setup_state`\n        method.\n        \"\"\"\n        if self._got_registered_once and self.warn_on_modifications:\n            from warnings import warn\n\n            warn(\n                Warning(\n                    \"The blueprint was already registered once but is\"\n                    \" getting modified now. These changes will not show\"\n                    \" up.\"\n                )\n            )\n        self.deferred_functions.append(func)\n\n    def record_once(self, func: t.Callable) -> None:\n        \"\"\"Works like :meth:`record` but wraps the function in another\n        function that will ensure the function is only called once.  If the\n        blueprint is registered a second time on the application, the\n        function passed is not called.\n        \"\"\"\n\n        def wrapper(state: BlueprintSetupState) -> None:\n            if state.first_registration:\n                func(state)\n\n        return self.record(update_wrapper(wrapper, func))\n\n    def make_setup_state(\n        self, app: \"Flask\", options: dict, first_registration: bool = False\n    ) -> BlueprintSetupState:\n        \"\"\"Creates an instance of :meth:`~flask.blueprints.BlueprintSetupState`\n        object that is later passed to the register callback functions.\n        Subclasses can override this to return a subclass of the setup state.\n        \"\"\"\n        return BlueprintSetupState(self, app, options, first_registration)\n\n    def register_blueprint(self, blueprint: \"Blueprint\", **options: t.Any) -> None:\n        \"\"\"Register a :class:`~flask.Blueprint` on this blueprint. Keyword\n        arguments passed to this method will override the defaults set\n        on the blueprint.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        self._blueprints.append((blueprint, options))\n\n    def register(self, app: \"Flask\", options: dict) -> None:\n        \"\"\"Called by :meth:`Flask.register_blueprint` to register all\n        views and callbacks registered on the blueprint with the\n        application. Creates a :class:`.BlueprintSetupState` and calls\n        each :meth:`record` callbackwith it.\n\n        :param app: The application this blueprint is being registered\n            with.\n        :param options: Keyword arguments forwarded from\n            :meth:`~Flask.register_blueprint`.\n        :param first_registration: Whether this is the first time this\n            blueprint has been registered on the application.\n        \"\"\"\n        first_registration = False\n\n        if self.name in app.blueprints:\n            assert app.blueprints[self.name] is self, (\n                \"A name collision occurred between blueprints\"\n                f\" {self!r} and {app.blueprints[self.name]!r}.\"\n                f\" Both share the same name {self.name!r}.\"\n                f\" Blueprints that are created on the fly need unique\"\n                f\" names.\"\n            )\n        else:\n            app.blueprints[self.name] = self\n            first_registration = True\n\n        self._got_registered_once = True\n        state = self.make_setup_state(app, options, first_registration)\n\n        if self.has_static_folder:\n            state.add_url_rule(\n                f\"{self.static_url_path}/<path:filename>\",\n                view_func=self.send_static_file,\n                endpoint=\"static\",\n            )\n\n        # Merge blueprint data into parent.\n        if first_registration:\n\n            def extend(bp_dict, parent_dict):\n                for key, values in bp_dict.items():\n                    key = self.name if key is None else f\"{self.name}.{key}\"\n\n                    parent_dict[key].extend(values)\n\n            for key, value in self.error_handler_spec.items():\n                key = self.name if key is None else f\"{self.name}.{key}\"\n                value = defaultdict(\n                    dict,\n                    {\n                        code: {\n                            exc_class: func for exc_class, func in code_values.items()\n                        }\n                        for code, code_values in value.items()\n                    },\n                )\n                app.error_handler_spec[key] = value\n\n            for endpoint, func in self.view_functions.items():\n                app.view_functions[endpoint] = func\n\n            extend(self.before_request_funcs, app.before_request_funcs)\n            extend(self.after_request_funcs, app.after_request_funcs)\n            extend(\n                self.teardown_request_funcs,\n                app.teardown_request_funcs,\n            )\n            extend(self.url_default_functions, app.url_default_functions)\n            extend(self.url_value_preprocessors, app.url_value_preprocessors)\n            extend(self.template_context_processors, app.template_context_processors)\n\n        for deferred in self.deferred_functions:\n            deferred(state)\n\n        cli_resolved_group = options.get(\"cli_group\", self.cli_group)\n\n        if self.cli.commands:\n            if cli_resolved_group is None:\n                app.cli.commands.update(self.cli.commands)\n            elif cli_resolved_group is _sentinel:\n                self.cli.name = self.name\n                app.cli.add_command(self.cli)\n            else:\n                self.cli.name = cli_resolved_group\n                app.cli.add_command(self.cli)\n\n        for blueprint, bp_options in self._blueprints:\n            url_prefix = options.get(\"url_prefix\", \"\")\n            if \"url_prefix\" in bp_options:\n                url_prefix = (\n                    url_prefix.rstrip(\"/\") + \"/\" + bp_options[\"url_prefix\"].lstrip(\"/\")\n                )\n\n            bp_options[\"url_prefix\"] = url_prefix\n            bp_options[\"name_prefix\"] = options.get(\"name_prefix\", \"\") + self.name + \".\"\n            blueprint.register(app, bp_options)\n\n    def add_url_rule(\n        self,\n        rule: str,\n        endpoint: t.Optional[str] = None,\n        view_func: t.Optional[t.Callable] = None,\n        **options: t.Any,\n    ) -> None:\n        \"\"\"Like :meth:`Flask.add_url_rule` but for a blueprint.  The endpoint for\n        the :func:`url_for` function is prefixed with the name of the blueprint.\n        \"\"\"\n        if endpoint:\n            assert \".\" not in endpoint, \"Blueprint endpoints should not contain dots\"\n        if view_func and hasattr(view_func, \"__name__\"):\n            assert (\n                \".\" not in view_func.__name__\n            ), \"Blueprint view function name should not contain dots\"\n        self.record(lambda s: s.add_url_rule(rule, endpoint, view_func, **options))\n\n    def app_template_filter(self, name: t.Optional[str] = None) -> t.Callable:\n        \"\"\"Register a custom template filter, available application wide.  Like\n        :meth:`Flask.template_filter` but for a blueprint.\n\n        :param name: the optional name of the filter, otherwise the\n(D)         return self.record(update_wrapper(wrapper, func))\n\n    def make_setup_state(\n        self, app: \"Flask\", options: dict, first_registration: bool = False\n    ) -> BlueprintSetupState:\n        \"\"\"Creates an instance of :meth:`~flask.blueprints.BlueprintSetupState`\n        object that is later passed to the register callback functions.\n        Subclasses can override this to return a subclass of the setup state.\n        \"\"\"\n        return BlueprintSetupState(self, app, options, first_registration)\n\n    def register_blueprint(self, blueprint: \"Blueprint\", **options: t.Any) -> None:\n        \"\"\"Register a :class:`~flask.Blueprint` on this blueprint. Keyword\n        arguments passed to this method will override the defaults set\n        on the blueprint.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        self._blueprints.append((blueprint, options))\n\n    def register(self, app: \"Flask\", options: dict) -> None:\n        \"\"\"Called by :meth:`Flask.register_blueprint` to register all\n        views and callbacks registered on the blueprint with the\n        application. Creates a :class:`.BlueprintSetupState` and calls\n        each :meth:`record` callbackwith it.\n\n        :param app: The application this blueprint is being registered\n            with.\n        :param options: Keyword arguments forwarded from\n            :meth:`~Flask.register_blueprint`.\n        :param first_registration: Whether this is the first time this\n            blueprint has been registered on the application.\n        \"\"\"\n        first_registration = False\n\n        if self.name in app.blueprints:\n            assert app.blueprints[self.name] is self, (\n                \"A name collision occurred between blueprints\"\n                f\" {self!r} and {app.blueprints[self.name]!r}.\"\n                f\" Both share the same name {self.name!r}.\"\n                f\" Blueprints that are created on the fly need unique\"\n                f\" names.\"\n            )\n        else:\n            app.blueprints[self.name] = self\n            first_registration = True\n\n        self._got_registered_once = True\n        state = self.make_setup_state(app, options, first_registration)\n\n        if self.has_static_folder:\n            state.add_url_rule(\n                f\"{self.static_url_path}/<path:filename>\",\n                view_func=self.send_static_file,\n                endpoint=\"static\",\n            )\n\n        # Merge blueprint data into parent.\n        if first_registration:\n\n            def extend(bp_dict, parent_dict):\n                for key, values in bp_dict.items():\n                    key = self.name if key is None else f\"{self.name}.{key}\"\n\n                    parent_dict[key].extend(values)\n\n            for key, value in self.error_handler_spec.items():\n                key = self.name if key is None else f\"{self.name}.{key}\"\n                value = defaultdict(\n                    dict,\n                    {\n                        code: {\n                            exc_class: func for exc_class, func in code_values.items()\n                        }\n                        for code, code_values in value.items()\n                    },\n                )\n                app.error_handler_spec[key] = value\n\n            for endpoint, func in self.view_functions.items():\n                app.view_functions[endpoint] = func\n\n            extend(self.before_request_funcs, app.before_request_funcs)\n            extend(self.after_request_funcs, app.after_request_funcs)\n            extend(\n                self.teardown_request_funcs,\n                app.teardown_request_funcs,\n            )\n            extend(self.url_default_functions, app.url_default_functions)\n            extend(self.url_value_preprocessors, app.url_value_preprocessors)\n            extend(self.template_context_processors, app.template_context_processors)\n\n        for deferred in self.deferred_functions:\n            deferred(state)\n\n        cli_resolved_group = options.get(\"cli_group\", self.cli_group)\n\n        if self.cli.commands:\n            if cli_resolved_group is None:\n                app.cli.commands.update(self.cli.commands)\n            elif cli_resolved_group is _sentinel:\n                self.cli.name = self.name\n                app.cli.add_command(self.cli)\n            else:\n                self.cli.name = cli_resolved_group\n                app.cli.add_command(self.cli)\n\n        for blueprint, bp_options in self._blueprints:\n            url_prefix = options.get(\"url_prefix\", \"\")\n            if \"url_prefix\" in bp_options:\n                url_prefix = (\n                    url_prefix.rstrip(\"/\") + \"/\" + bp_options[\"url_prefix\"].lstrip(\"/\")\n                )\n\n            bp_options[\"url_prefix\"] = url_prefix\n            bp_options[\"name_prefix\"] = options.get(\"name_prefix\", \"\") + self.name + \".\"\n            blueprint.register(app, bp_options)\n\n    def add_url_rule(\n        self,\n        rule: str,\n        endpoint: t.Optional[str] = None,\n        view_func: t.Optional[t.Callable] = None,\n        **options: t.Any,\n    ) -> None:\n        \"\"\"Like :meth:`Flask.add_url_rule` but for a blueprint.  The endpoint for\n        the :func:`url_for` function is prefixed with the name of the blueprint.\n        \"\"\"\n        if endpoint:\n            assert \".\" not in endpoint, \"Blueprint endpoints should not contain dots\"\n        if view_func and hasattr(view_func, \"__name__\"):\n            assert (\n                \".\" not in view_func.__name__\n            ), \"Blueprint view function name should not contain dots\"\n        self.record(lambda s: s.add_url_rule(rule, endpoint, view_func, **options))\n\n    def app_template_filter(self, name: t.Optional[str] = None) -> t.Callable:\n        \"\"\"Register a custom template filter, available application wide.  Like\n        :meth:`Flask.template_filter` but for a blueprint.\n\n        :param name: the optional name of the filter, otherwise the\n                     function name will be used.\n        \"\"\"\n\n        def decorator(f: TemplateFilterCallable) -> TemplateFilterCallable:\n            self.add_app_template_filter(f, name=name)\n            return f\n\n        return decorator\n\n    def add_app_template_filter(\n        self, f: TemplateFilterCallable, name: t.Optional[str] = None\n    ) -> None:\n        \"\"\"Register a custom template filter, available application wide.  Like\n        :meth:`Flask.add_template_filter` but for a blueprint.  Works exactly\n        like the :meth:`app_template_filter` decorator.\n\n        :param name: the optional name of the filter, otherwise the\n                     function name will be used.\n        \"\"\"\n\n        def register_template(state: BlueprintSetupState) -> None:\n            state.app.jinja_env.filters[name or f.__name__] = f\n\n        self.record_once(register_template)\n\n    def app_template_test(self, name: t.Optional[str] = None) -> t.Callable:\n        \"\"\"Register a custom template test, available application wide.  Like\n        :meth:`Flask.template_test` but for a blueprint.\n\n        .. versionadded:: 0.10\n\n        :param name: the optional name of the test, otherwise the\n                     function name will be used.\n        \"\"\"\n\n        def decorator(f: TemplateTestCallable) -> TemplateTestCallable:\n            self.add_app_template_test(f, name=name)\n            return f\n\n        return decorator\n\n    def add_app_template_test(\n        self, f: TemplateTestCallable, name: t.Optional[str] = None\n    ) -> None:\n        \"\"\"Register a custom template test, available application wide.  Like\n        :meth:`Flask.add_template_test` but for a blueprint.  Works exactly\n        like the :meth:`app_template_test` decorator.", "answer": "C"}
{"uuid": "03bfbd9c-5627-4f44-9116-966556506a72", "issue_statement": "Add a file mode parameter to flask.Config.from_file()\nPython 3.11 introduced native TOML support with the `tomllib` package. This could work nicely with the `flask.Config.from_file()` method as an easy way to load TOML config files:\r\n\r\n```python\r\napp.config.from_file(\"config.toml\", tomllib.load)\r\n```\r\n\r\nHowever, `tomllib.load()` takes an object readable in binary mode, while `flask.Config.from_file()` opens a file in text mode, resulting in this error:\r\n\r\n```\r\nTypeError: File must be opened in binary mode, e.g. use `open('foo.toml', 'rb')`\r\n```\r\n\r\nWe can get around this with a more verbose expression, like loading from a file opened with the built-in `open()` function and passing the `dict` to `app.Config.from_mapping()`:\r\n\r\n```python\r\n# We have to repeat the path joining that from_file() does\r\nwith open(os.path.join(app.config.root_path, \"config.toml\"), \"rb\") as file:\r\n    app.config.from_mapping(tomllib.load(file))\r\n```\r\n\r\nBut adding a file mode parameter to `flask.Config.from_file()` would enable the use of a simpler expression. E.g.:\r\n\r\n```python\r\napp.config.from_file(\"config.toml\", tomllib.load, mode=\"b\")\r\n```", "choices": "(A)         :param obj: an import name or object\n        \"\"\"\n        if isinstance(obj, str):\n            obj = import_string(obj)\n        for key in dir(obj):\n            if key.isupper():\n                self[key] = getattr(obj, key)\n\n    def from_file(\n        self,\n        filename: str,\n        load: t.Callable[[t.IO[t.Any]], t.Mapping],\n        silent: bool = False,\n    ) -> bool:\n        \"\"\"Update the values in the config from a file that is loaded\n        using the ``load`` parameter. The loaded data is passed to the\n        :meth:`from_mapping` method.\n\n        .. code-block:: python\n\n            import json\n            app.config.from_file(\"config.json\", load=json.load)\n\n            import toml\n            app.config.from_file(\"config.toml\", load=toml.load)\n\n        :param filename: The path to the data file. This can be an\n            absolute path or relative to the config root path.\n        :param load: A callable that takes a file handle and returns a\n            mapping of loaded data from the file.\n        :type load: ``Callable[[Reader], Mapping]`` where ``Reader``\n            implements a ``read`` method.\n        :param silent: Ignore the file if it doesn't exist.\n        :return: ``True`` if the file was loaded successfully.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        filename = os.path.join(self.root_path, filename)\n\n(B)         filename: str,\n        load: t.Callable[[t.IO[t.Any]], t.Mapping],\n        silent: bool = False,\n    ) -> bool:\n        \"\"\"Update the values in the config from a file that is loaded\n        using the ``load`` parameter. The loaded data is passed to the\n        :meth:`from_mapping` method.\n\n        .. code-block:: python\n\n            import json\n            app.config.from_file(\"config.json\", load=json.load)\n\n            import toml\n            app.config.from_file(\"config.toml\", load=toml.load)\n\n        :param filename: The path to the data file. This can be an\n            absolute path or relative to the config root path.\n        :param load: A callable that takes a file handle and returns a\n            mapping of loaded data from the file.\n        :type load: ``Callable[[Reader], Mapping]`` where ``Reader``\n            implements a ``read`` method.\n        :param silent: Ignore the file if it doesn't exist.\n        :return: ``True`` if the file was loaded successfully.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        filename = os.path.join(self.root_path, filename)\n\n        try:\n            with open(filename) as f:\n                obj = load(f)\n        except OSError as e:\n            if silent and e.errno in (errno.ENOENT, errno.EISDIR):\n                return False\n\n            e.strerror = f\"Unable to load configuration file ({e.strerror})\"\n            raise\n\n(C)             import json\n            app.config.from_file(\"config.json\", load=json.load)\n\n            import toml\n            app.config.from_file(\"config.toml\", load=toml.load)\n\n        :param filename: The path to the data file. This can be an\n            absolute path or relative to the config root path.\n        :param load: A callable that takes a file handle and returns a\n            mapping of loaded data from the file.\n        :type load: ``Callable[[Reader], Mapping]`` where ``Reader``\n            implements a ``read`` method.\n        :param silent: Ignore the file if it doesn't exist.\n        :return: ``True`` if the file was loaded successfully.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        filename = os.path.join(self.root_path, filename)\n\n        try:\n            with open(filename) as f:\n                obj = load(f)\n        except OSError as e:\n            if silent and e.errno in (errno.ENOENT, errno.EISDIR):\n                return False\n\n            e.strerror = f\"Unable to load configuration file ({e.strerror})\"\n            raise\n\n        return self.from_mapping(obj)\n\n    def from_mapping(\n        self, mapping: t.Optional[t.Mapping[str, t.Any]] = None, **kwargs: t.Any\n    ) -> bool:\n        \"\"\"Updates the config like :meth:`update` ignoring items with\n        non-upper keys.\n\n        :return: Always returns ``True``.\n\n(D)         :type load: ``Callable[[Reader], Mapping]`` where ``Reader``\n            implements a ``read`` method.\n        :param silent: Ignore the file if it doesn't exist.\n        :return: ``True`` if the file was loaded successfully.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        filename = os.path.join(self.root_path, filename)\n\n        try:\n            with open(filename) as f:\n                obj = load(f)\n        except OSError as e:\n            if silent and e.errno in (errno.ENOENT, errno.EISDIR):\n                return False\n\n            e.strerror = f\"Unable to load configuration file ({e.strerror})\"\n            raise\n\n        return self.from_mapping(obj)\n\n    def from_mapping(\n        self, mapping: t.Optional[t.Mapping[str, t.Any]] = None, **kwargs: t.Any\n    ) -> bool:\n        \"\"\"Updates the config like :meth:`update` ignoring items with\n        non-upper keys.\n\n        :return: Always returns ``True``.\n\n        .. versionadded:: 0.11\n        \"\"\"\n        mappings: t.Dict[str, t.Any] = {}\n        if mapping is not None:\n            mappings.update(mapping)\n        mappings.update(kwargs)\n        for key, value in mappings.items():\n            if key.isupper():\n                self[key] = value\n        return True", "answer": "B"}
{"uuid": "791cdaf2-ab3c-493e-8e18-779429ef35dd", "issue_statement": "`Session.resolve_redirects` copies the original request for all subsequent requests, can cause incorrect method selection\nConsider the following redirection chain:\n\n```\nPOST /do_something HTTP/1.1\nHost: server.example.com\n...\n\nHTTP/1.1 303 See Other\nLocation: /new_thing_1513\n\nGET /new_thing_1513\nHost: server.example.com\n...\n\nHTTP/1.1 307 Temporary Redirect\nLocation: //failover.example.com/new_thing_1513\n```\n\nThe intermediate 303 See Other has caused the POST to be converted to\na GET.  The subsequent 307 should preserve the GET.  However, because\n`Session.resolve_redirects` starts each iteration by copying the _original_\nrequest object, Requests will issue a POST!", "choices": "(A)             resp = self.send(\n                prepared_request,\n                stream=stream,\n                timeout=timeout,\n                verify=verify,\n                cert=cert,\n                proxies=proxies,\n                allow_redirects=False,\n            )\n\n            extract_cookies_to_jar(self.cookies, prepared_request, resp.raw)\n(B)                 timeout=timeout,\n                verify=verify,\n                cert=cert,\n                proxies=proxies,\n                allow_redirects=False,\n            )\n\n            extract_cookies_to_jar(self.cookies, prepared_request, resp.raw)\n\n            i += 1\n            yield resp\n(C) \n            # .netrc might have more auth for us.\n            new_auth = get_netrc_auth(url) if self.trust_env else None\n            if new_auth is not None:\n                prepared_request.prepare_auth(new_auth)\n\n            resp = self.send(\n                prepared_request,\n                stream=stream,\n                timeout=timeout,\n                verify=verify,\n(D)             if new_auth is not None:\n                prepared_request.prepare_auth(new_auth)\n\n            resp = self.send(\n                prepared_request,\n                stream=stream,\n                timeout=timeout,\n                verify=verify,\n                cert=cert,\n                proxies=proxies,\n                allow_redirects=False,", "answer": "D"}
{"uuid": "77213bd2-4552-4683-b3f8-70f237a478bf", "issue_statement": "socket.error exception not caught/wrapped in a requests exception (ConnectionError perhaps?)\nI just noticed a case where I had a socket reset on me, and was raised to me as a raw socket error as opposed to something like a requests.exceptions.ConnectionError:\n\n```\n  File \"/home/rtdean/***/***/***/***/***/***.py\", line 67, in dir_parse\n    root = ElementTree.fromstring(response.text)\n  File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/requests-2.3.0-py2.7.egg/requests/models.py\", line 721, in text\n    if not self.content:\n  File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/requests-2.3.0-py2.7.egg/requests/models.py\", line 694, in content\n    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()\n  File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/requests-2.3.0-py2.7.egg/requests/models.py\", line 627, in generate\n    for chunk in self.raw.stream(chunk_size, decode_content=True):\n  File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/requests-2.3.0-py2.7.egg/requests/packages/urllib3/response.py\", line 240, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/requests-2.3.0-py2.7.egg/requests/packages/urllib3/response.py\", line 187, in read\n    data = self._fp.read(amt)\n  File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/httplib.py\", line 543, in read\n    return self._read_chunked(amt)\n  File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/httplib.py\", line 612, in _read_chunked\n    value.append(self._safe_read(chunk_left))\n  File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/httplib.py\", line 658, in _safe_read\n    chunk = self.fp.read(min(amt, MAXAMOUNT))\n  File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/socket.py\", line 380, in read\n    data = self._sock.recv(left)\n  File \"/home/rtdean/.pyenv/versions/2.7.6/lib/python2.7/site-packages/gevent-1.0.1-py2.7-linux-x86_64.egg/gevent/socket.py\", line 385, in recv\n    return sock.recv(*args)\nsocket.error: [Errno 104] Connection reset by peer\n```\n\nNot sure if this is by accident or design... in general, I guess I'd expect a requests exception when using requests, but I can start looking for socket errors and the like as well.", "choices": "(A) \n    def deregister_hook(self, event, hook):\n        \"\"\"Deregister a previously registered hook.\n        Returns True if the hook existed, False if not.\n        \"\"\"\n\n        try:\n            self.hooks[event].remove(hook)\n            return True\n        except ValueError:\n            return False\n\n\nclass Request(RequestHooksMixin):\n    \"\"\"A user-created :class:`Request <Request>` object.\n\n    Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.\n\n    :param method: HTTP method to use.\n    :param url: URL to send.\n    :param headers: dictionary of headers to send.\n    :param files: dictionary of {filename: fileobject} files to multipart upload.\n    :param data: the body to attach the request. If a dictionary is provided, form-encoding will take place.\n    :param params: dictionary of URL parameters to append to the URL.\n    :param auth: Auth handler or (user, pass) tuple.\n    :param cookies: dictionary or CookieJar of cookies to attach to this request.\n    :param hooks: dictionary of callback hooks, for internal usage.\n\n    Usage::\n\n      >>> import requests\n      >>> req = requests.Request('GET', 'http://httpbin.org/get')\n      >>> req.prepare()\n      <PreparedRequest [GET]>\n\n    \"\"\"\n    def __init__(self,\n        method=None,\n        url=None,\n        headers=None,\n        files=None,\n        data=None,\n        params=None,\n        auth=None,\n        cookies=None,\n        hooks=None):\n\n        # Default empty dicts for dict params.\n        data = [] if data is None else data\n        files = [] if files is None else files\n        headers = {} if headers is None else headers\n        params = {} if params is None else params\n        hooks = {} if hooks is None else hooks\n\n        self.hooks = default_hooks()\n        for (k, v) in list(hooks.items()):\n            self.register_hook(event=k, hook=v)\n\n        self.method = method\n        self.url = url\n        self.headers = headers\n        self.files = files\n        self.data = data\n        self.params = params\n        self.auth = auth\n        self.cookies = cookies\n\n    def __repr__(self):\n        return '<Request [%s]>' % (self.method)\n\n    def prepare(self):\n        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it.\"\"\"\n        p = PreparedRequest()\n        p.prepare(\n            method=self.method,\n            url=self.url,\n            headers=self.headers,\n            files=self.files,\n            data=self.data,\n            params=self.params,\n            auth=self.auth,\n            cookies=self.cookies,\n            hooks=self.hooks,\n        )\n        return p\n\n\nclass PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n    \"\"\"The fully mutable :class:`PreparedRequest <PreparedRequest>` object,\n    containing the exact bytes that will be sent to the server.\n\n    Generated from either a :class:`Request <Request>` object or manually.\n\n    Usage::\n\n      >>> import requests\n      >>> req = requests.Request('GET', 'http://httpbin.org/get')\n      >>> r = req.prepare()\n      <PreparedRequest [GET]>\n\n      >>> s = requests.Session()\n      >>> s.send(r)\n      <Response [200]>\n\n    \"\"\"\n\n    def __init__(self):\n        #: HTTP verb to send to the server.\n        self.method = None\n        #: HTTP URL to send the request to.\n        self.url = None\n        #: dictionary of HTTP headers.\n        self.headers = None\n        # The `CookieJar` used to create the Cookie header will be stored here\n        # after prepare_cookies is called\n        self._cookies = None\n        #: request body to send to the server.\n        self.body = None\n        #: dictionary of callback hooks, for internal usage.\n        self.hooks = default_hooks()\n\n    def prepare(self, method=None, url=None, headers=None, files=None,\n                data=None, params=None, auth=None, cookies=None, hooks=None):\n        \"\"\"Prepares the entire request with the given parameters.\"\"\"\n\n        self.prepare_method(method)\n        self.prepare_url(url, params)\n        self.prepare_headers(headers)\n        self.prepare_cookies(cookies)\n        self.prepare_body(data, files)\n        self.prepare_auth(auth, url)\n        # Note that prepare_auth must be last to enable authentication schemes\n        # such as OAuth to work on a fully prepared request.\n\n        # This MUST go after prepare_auth. Authenticators could add a hook\n        self.prepare_hooks(hooks)\n\n    def __repr__(self):\n        return '<PreparedRequest [%s]>' % (self.method)\n\n    def copy(self):\n        p = PreparedRequest()\n        p.method = self.method\n        p.url = self.url\n        p.headers = self.headers.copy() if self.headers is not None else None\n        p._cookies = self._cookies.copy() if self._cookies is not None else None\n        p.body = self.body\n        p.hooks = self.hooks\n        return p\n\n    def prepare_method(self, method):\n        \"\"\"Prepares the given HTTP method.\"\"\"\n        self.method = method\n        if self.method is not None:\n            self.method = self.method.upper()\n\n    def prepare_url(self, url, params):\n        \"\"\"Prepares the given HTTP URL.\"\"\"\n        #: Accept objects that have string representations.\n        try:\n            url = unicode(url)\n        except NameError:\n            # We're on Python 3.\n            url = str(url)\n        except UnicodeDecodeError:\n            pass\n\n        # Don't do any URL preparation for oddball schemes\n        if ':' in url and not url.lower().startswith('http'):\n            self.url = url\n            return\n\n        # Support for unicode domain names and paths.\n        scheme, auth, host, port, path, query, fragment = parse_url(url)\n\n        if not scheme:\n            raise MissingSchema(\"Invalid URL {0!r}: No schema supplied. \"\n                                \"Perhaps you meant http://{0}?\".format(url))\n\n        if not host:\n            raise InvalidURL(\"Invalid URL %r: No host supplied\" % url)\n\n        # Only want to apply IDNA to the hostname\n        try:\n            host = host.encode('idna').decode('utf-8')\n        except UnicodeError:\n            raise InvalidURL('URL has an invalid label.')\n\n        # Carefully reconstruct the network location\n        netloc = auth or ''\n        if netloc:\n            netloc += '@'\n        netloc += host\n        if port:\n            netloc += ':' + str(port)\n\n        # Bare domains aren't valid URLs.\n        if not path:\n            path = '/'\n\n        if is_py2:\n            if isinstance(scheme, str):\n                scheme = scheme.encode('utf-8')\n            if isinstance(netloc, str):\n                netloc = netloc.encode('utf-8')\n            if isinstance(path, str):\n                path = path.encode('utf-8')\n            if isinstance(query, str):\n                query = query.encode('utf-8')\n            if isinstance(fragment, str):\n                fragment = fragment.encode('utf-8')\n\n        enc_params = self._encode_params(params)\n        if enc_params:\n            if query:\n                query = '%s&%s' % (query, enc_params)\n            else:\n                query = enc_params\n\n        url = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))\n        self.url = url\n\n    def prepare_headers(self, headers):\n        \"\"\"Prepares the given HTTP headers.\"\"\"\n\n        if headers:\n            self.headers = CaseInsensitiveDict((to_native_string(name), value) for name, value in headers.items())\n        else:\n            self.headers = CaseInsensitiveDict()\n\n    def prepare_body(self, data, files):\n        \"\"\"Prepares the given HTTP body data.\"\"\"\n\n        # Check if file, fo, generator, iterator.\n        # If not, run through normal process.\n\n        # Nottin' on you.\n        body = None\n        content_type = None\n        length = None\n\n        is_stream = all([\n            hasattr(data, '__iter__'),\n            not isinstance(data, (basestring, list, tuple, dict))\n        ])\n\n        try:\n            length = super_len(data)\n        except (TypeError, AttributeError, UnsupportedOperation):\n            length = None\n\n        if is_stream:\n            body = data\n\n            if files:\n                raise NotImplementedError('Streamed bodies and files are mutually exclusive.')\n\n            if length is not None:\n                self.headers['Content-Length'] = builtin_str(length)\n            else:\n                self.headers['Transfer-Encoding'] = 'chunked'\n        else:\n            # Multi-part file uploads.\n            if files:\n                (body, content_type) = self._encode_files(files, data)\n            else:\n                if data:\n                    body = self._encode_params(data)\n                    if isinstance(data, basestring) or hasattr(data, 'read'):\n                        content_type = None\n                    else:\n                        content_type = 'application/x-www-form-urlencoded'\n\n            self.prepare_content_length(body)\n\n            # Add content-type if it wasn't explicitly provided.\n            if (content_type) and (not 'content-type' in self.headers):\n                self.headers['Content-Type'] = content_type\n\n        self.body = body\n\n    def prepare_content_length(self, body):\n        if hasattr(body, 'seek') and hasattr(body, 'tell'):\n            body.seek(0, 2)\n            self.headers['Content-Length'] = builtin_str(body.tell())\n            body.seek(0, 0)\n        elif body is not None:\n            l = super_len(body)\n            if l:\n                self.headers['Content-Length'] = builtin_str(l)\n        elif self.method not in ('GET', 'HEAD'):\n            self.headers['Content-Length'] = '0'\n\n    def prepare_auth(self, auth, url=''):\n        \"\"\"Prepares the given HTTP auth data.\"\"\"\n\n        # If no Auth is explicitly provided, extract it from the URL first.\n        if auth is None:\n            url_auth = get_auth_from_url(self.url)\n            auth = url_auth if any(url_auth) else None\n\n        if auth:\n            if isinstance(auth, tuple) and len(auth) == 2:\n                # special-case basic HTTP auth\n                auth = HTTPBasicAuth(*auth)\n\n            # Allow auth to make its changes.\n            r = auth(self)\n\n            # Update self to reflect the auth changes.\n            self.__dict__.update(r.__dict__)\n\n            # Recompute Content-Length\n            self.prepare_content_length(self.body)\n\n    def prepare_cookies(self, cookies):\n        \"\"\"Prepares the given HTTP cookie data.\"\"\"\n\n        if isinstance(cookies, cookielib.CookieJar):\n            self._cookies = cookies\n        else:\n            self._cookies = cookiejar_from_dict(cookies)\n\n        cookie_header = get_cookie_header(self._cookies, self)\n        if cookie_header is not None:\n            self.headers['Cookie'] = cookie_header\n\n    def prepare_hooks(self, hooks):\n        \"\"\"Prepares the given hooks.\"\"\"\n        for event in hooks:\n            self.register_hook(event, hooks[event])\n\n\nclass Response(object):\n    \"\"\"The :class:`Response <Response>` object, which contains a\n    server's response to an HTTP request.\n    \"\"\"\n\n    __attrs__ = [\n        '_content',\n        'status_code',\n        'headers',\n        'url',\n        'history',\n        'encoding',\n        'reason',\n        'cookies',\n        'elapsed',\n        'request',\n    ]\n\n    def __init__(self):\n        super(Response, self).__init__()\n\n        self._content = False\n        self._content_consumed = False\n\n        #: Integer Code of responded HTTP Status, e.g. 404 or 200.\n        self.status_code = None\n\n        #: Case-insensitive Dictionary of Response Headers.\n        #: For example, ``headers['content-encoding']`` will return the\n        #: value of a ``'Content-Encoding'`` response header.\n        self.headers = CaseInsensitiveDict()\n\n        #: File-like object representation of response (for advanced usage).\n        #: Use of ``raw`` requires that ``stream=True`` be set on the request.\n        # This requirement does not apply for use internally to Requests.\n        self.raw = None\n\n        #: Final URL location of Response.\n        self.url = None\n\n        #: Encoding to decode with when accessing r.text.\n        self.encoding = None\n\n        #: A list of :class:`Response <Response>` objects from\n        #: the history of the Request. Any redirect responses will end\n        #: up here. The list is sorted from the oldest to the most recent request.\n        self.history = []\n\n        #: Textual reason of responded HTTP Status, e.g. \"Not Found\" or \"OK\".\n        self.reason = None\n\n        #: A CookieJar of Cookies the server sent back.\n        self.cookies = cookiejar_from_dict({})\n\n        #: The amount of time elapsed between sending the request\n        #: and the arrival of the response (as a timedelta)\n        self.elapsed = datetime.timedelta(0)\n\n        #: The :class:`PreparedRequest <PreparedRequest>` object to which this\n        #: is a response.\n        self.request = None\n\n    def __getstate__(self):\n        # Consume everything; accessing the content attribute makes\n        # sure the content has been fully read.\n        if not self._content_consumed:\n            self.content\n\n        return dict(\n            (attr, getattr(self, attr, None))\n            for attr in self.__attrs__\n        )\n\n    def __setstate__(self, state):\n        for name, value in state.items():\n            setattr(self, name, value)\n\n        # pickled objects do not have .raw\n        setattr(self, '_content_consumed', True)\n        setattr(self, 'raw', None)\n\n    def __repr__(self):\n        return '<Response [%s]>' % (self.status_code)\n\n    def __bool__(self):\n        \"\"\"Returns true if :attr:`status_code` is 'OK'.\"\"\"\n        return self.ok\n\n    def __nonzero__(self):\n        \"\"\"Returns true if :attr:`status_code` is 'OK'.\"\"\"\n        return self.ok\n\n    def __iter__(self):\n        \"\"\"Allows you to use a response as an iterator.\"\"\"\n        return self.iter_content(128)\n\n    @property\n    def ok(self):\n        try:\n            self.raise_for_status()\n        except RequestException:\n            return False\n        return True\n\n    @property\n    def is_redirect(self):\n        \"\"\"True if this Response is a well-formed HTTP redirect that could have\n        been processed automatically (by :meth:`Session.resolve_redirects`).\n        \"\"\"\n        return ('location' in self.headers and self.status_code in REDIRECT_STATI)\n\n    @property\n    def is_permanent_redirect(self):\n        \"\"\"True if this Response one of the permanant versions of redirect\"\"\"\n        return ('location' in self.headers and self.status_code in (codes.moved_permanently, codes.permanent_redirect))\n\n    @property\n    def apparent_encoding(self):\n        \"\"\"The apparent encoding, provided by the chardet library\"\"\"\n        return chardet.detect(self.content)['encoding']\n\n    def iter_content(self, chunk_size=1, decode_unicode=False):\n        \"\"\"Iterates over the response data.  When stream=True is set on the\n        request, this avoids reading the content at once into memory for\n        large responses.  The chunk size is the number of bytes it should\n        read into memory.  This is not necessarily the length of each item\n        returned as decoding can take place.\n\n        If decode_unicode is True, content will be decoded using the best\n        available encoding based on the response.\n        \"\"\"\n        def generate():\n            try:\n                # Special case for urllib3.\n                try:\n                    for chunk in self.raw.stream(chunk_size, decode_content=True):\n                        yield chunk\n                except IncompleteRead as e:\n                    raise ChunkedEncodingError(e)\n                except DecodeError as e:\n                    raise ContentDecodingError(e)\n            except AttributeError:\n                # Standard file-like object.\n                while True:\n                    chunk = self.raw.read(chunk_size)\n                    if not chunk:\n                        break\n                    yield chunk\n\n            self._content_consumed = True\n\n        # simulate reading small chunks of the content\n        reused_chunks = iter_slices(self._content, chunk_size)\n\n        stream_chunks = generate()\n\n        chunks = reused_chunks if self._content_consumed else stream_chunks\n\n        if decode_unicode:\n            chunks = stream_decode_response_unicode(chunks, self)\n\n        return chunks\n\n    def iter_lines(self, chunk_size=ITER_CHUNK_SIZE, decode_unicode=None):\n        \"\"\"Iterates over the response data, one line at a time.  When\n        stream=True is set on the request, this avoids reading the\n        content at once into memory for large responses.\n        \"\"\"\n\n        pending = None\n\n        for chunk in self.iter_content(chunk_size=chunk_size, decode_unicode=decode_unicode):\n\n            if pending is not None:\n                chunk = pending + chunk\n            lines = chunk.splitlines()\n\n            if lines and lines[-1] and chunk and lines[-1][-1] == chunk[-1]:\n                pending = lines.pop()\n            else:\n                pending = None\n\n            for line in lines:\n                yield line\n\n        if pending is not None:\n            yield pending\n\n    @property\n    def content(self):\n        \"\"\"Content of the response, in bytes.\"\"\"\n\n        if self._content is False:\n            # Read the contents.\n            try:\n                if self._content_consumed:\n                    raise RuntimeError(\n                        'The content for this response was already consumed')\n\n                if self.status_code == 0:\n                    self._content = None\n                else:\n                    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()\n\n            except AttributeError:\n                self._content = None\n\n        self._content_consumed = True\n        # don't need to release the connection; that's been handled by urllib3\n        # since we exhausted the data.\n        return self._content\n\n    @property\n    def text(self):\n        \"\"\"Content of the response, in unicode.\n\n        If Response.encoding is None, encoding will be guessed using\n        ``chardet``.\n\n        The encoding of the response content is determined based solely on HTTP\n        headers, following RFC 2616 to the letter. If you can take advantage of\n        non-HTTP knowledge to make a better guess at the encoding, you should\n        set ``r.encoding`` appropriately before accessing this property.\n        \"\"\"\n\n        # Try charset from content-type\n        content = None\n        encoding = self.encoding\n\n        if not self.content:\n            return str('')\n\n        # Fallback to auto-detected encoding.\n        if self.encoding is None:\n            encoding = self.apparent_encoding\n\n        # Decode unicode from given encoding.\n        try:\n            content = str(self.content, encoding, errors='replace')\n        except (LookupError, TypeError):\n            # A LookupError is raised if the encoding was not found which could\n            # indicate a misspelling or similar mistake.\n            #\n            # A TypeError can be raised if encoding is None\n            #\n            # So we try blindly encoding.\n            content = str(self.content, errors='replace')\n\n        return content\n\n    def json(self, **kwargs):\n        \"\"\"Returns the json-encoded content of a response, if any.\n\n        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n        \"\"\"\n\n        if not self.encoding and len(self.content) > 3:\n            # No encoding set. JSON RFC 4627 section 3 states we should expect\n            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n            # decoding fails, fall back to `self.text` (using chardet to make\n            # a best guess).\n            encoding = guess_json_utf(self.content)\n            if encoding is not None:\n                try:\n                    return json.loads(self.content.decode(encoding), **kwargs)\n                except UnicodeDecodeError:\n                    # Wrong UTF codec detected; usually because it's not UTF-8\n                    # but some other 8-bit codec.  This is an RFC violation,\n                    # and the server didn't bother to tell us what codec *was*\n                    # used.\n                    pass\n        return json.loads(self.text, **kwargs)\n\n    @property\n    def links(self):\n        \"\"\"Returns the parsed header links of the response, if any.\"\"\"\n\n        header = self.headers.get('link')\n\n        # l = MultiDict()\n        l = {}\n\n        if header:\n            links = parse_header_links(header)\n\n            for link in links:\n                key = link.get('rel') or link.get('url')\n                l[key] = link\n\n        return l\n\n    def raise_for_status(self):\n        \"\"\"Raises stored :class:`HTTPError`, if one occurred.\"\"\"\n\n        http_error_msg = ''\n\n        if 400 <= self.status_code < 500:\n            http_error_msg = '%s Client Error: %s' % (self.status_code, self.reason)\n\n        elif 500 <= self.status_code < 600:\n            http_error_msg = '%s Server Error: %s' % (self.status_code, self.reason)\n\n        if http_error_msg:\n            raise HTTPError(http_error_msg, response=self)\n\n    def close(self):\n        \"\"\"Releases the connection back to the pool. Once this method has been\n(B) \nimport collections\nimport datetime\n\nfrom io import BytesIO, UnsupportedOperation\nfrom .hooks import default_hooks\nfrom .structures import CaseInsensitiveDict\n\nfrom .auth import HTTPBasicAuth\nfrom .cookies import cookiejar_from_dict, get_cookie_header\nfrom .packages.urllib3.fields import RequestField\nfrom .packages.urllib3.filepost import encode_multipart_formdata\nfrom .packages.urllib3.util import parse_url\nfrom .packages.urllib3.exceptions import DecodeError\nfrom .exceptions import (\n    HTTPError, RequestException, MissingSchema, InvalidURL,\n    ChunkedEncodingError, ContentDecodingError)\nfrom .utils import (\n    guess_filename, get_auth_from_url, requote_uri,\n    stream_decode_response_unicode, to_key_val_list, parse_header_links,\n    iter_slices, guess_json_utf, super_len, to_native_string)\nfrom .compat import (\n    cookielib, urlunparse, urlsplit, urlencode, str, bytes, StringIO,\n    is_py2, chardet, json, builtin_str, basestring, IncompleteRead)\nfrom .status_codes import codes\n\n#: The set of HTTP status codes that indicate an automatically\n#: processable redirect.\nREDIRECT_STATI = (\n    codes.moved,              # 301\n    codes.found,              # 302\n    codes.other,              # 303\n    codes.temporary_redirect, # 307\n    codes.permanent_redirect, # 308\n)\nDEFAULT_REDIRECT_LIMIT = 30\nCONTENT_CHUNK_SIZE = 10 * 1024\nITER_CHUNK_SIZE = 512\n\n\nclass RequestEncodingMixin(object):\n    @property\n    def path_url(self):\n        \"\"\"Build the path URL to use.\"\"\"\n\n        url = []\n\n        p = urlsplit(self.url)\n\n        path = p.path\n        if not path:\n            path = '/'\n\n        url.append(path)\n\n        query = p.query\n        if query:\n            url.append('?')\n            url.append(query)\n\n        return ''.join(url)\n\n    @staticmethod\n    def _encode_params(data):\n        \"\"\"Encode parameters in a piece of data.\n\n        Will successfully encode parameters when passed as a dict or a list of\n        2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\n        if parameters are supplied as a dict.\n        \"\"\"\n\n        if isinstance(data, (str, bytes)):\n            return data\n        elif hasattr(data, 'read'):\n            return data\n        elif hasattr(data, '__iter__'):\n            result = []\n            for k, vs in to_key_val_list(data):\n                if isinstance(vs, basestring) or not hasattr(vs, '__iter__'):\n                    vs = [vs]\n                for v in vs:\n                    if v is not None:\n                        result.append(\n                            (k.encode('utf-8') if isinstance(k, str) else k,\n                             v.encode('utf-8') if isinstance(v, str) else v))\n            return urlencode(result, doseq=True)\n        else:\n            return data\n\n    @staticmethod\n    def _encode_files(files, data):\n        \"\"\"Build the body for a multipart/form-data request.\n\n        Will successfully encode files when passed as a dict or a list of\n        2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\n        if parameters are supplied as a dict.\n\n        \"\"\"\n        if (not files):\n            raise ValueError(\"Files must be provided.\")\n        elif isinstance(data, basestring):\n            raise ValueError(\"Data must not be a string.\")\n\n        new_fields = []\n        fields = to_key_val_list(data or {})\n        files = to_key_val_list(files or {})\n\n        for field, val in fields:\n            if isinstance(val, basestring) or not hasattr(val, '__iter__'):\n                val = [val]\n            for v in val:\n                if v is not None:\n                    # Don't call str() on bytestrings: in Py3 it all goes wrong.\n                    if not isinstance(v, bytes):\n                        v = str(v)\n\n                    new_fields.append(\n                        (field.decode('utf-8') if isinstance(field, bytes) else field,\n                         v.encode('utf-8') if isinstance(v, str) else v))\n\n        for (k, v) in files:\n            # support for explicit filename\n            ft = None\n            fh = None\n            if isinstance(v, (tuple, list)):\n                if len(v) == 2:\n                    fn, fp = v\n                elif len(v) == 3:\n                    fn, fp, ft = v\n                else:\n                    fn, fp, ft, fh = v\n            else:\n                fn = guess_filename(v) or k\n                fp = v\n            if isinstance(fp, str):\n                fp = StringIO(fp)\n            if isinstance(fp, bytes):\n                fp = BytesIO(fp)\n\n            rf = RequestField(name=k, data=fp.read(),\n                              filename=fn, headers=fh)\n            rf.make_multipart(content_type=ft)\n            new_fields.append(rf)\n\n        body, content_type = encode_multipart_formdata(new_fields)\n\n        return body, content_type\n\n\nclass RequestHooksMixin(object):\n    def register_hook(self, event, hook):\n        \"\"\"Properly register a hook.\"\"\"\n\n        if event not in self.hooks:\n            raise ValueError('Unsupported event specified, with event name \"%s\"' % (event))\n\n        if isinstance(hook, collections.Callable):\n            self.hooks[event].append(hook)\n        elif hasattr(hook, '__iter__'):\n            self.hooks[event].extend(h for h in hook if isinstance(h, collections.Callable))\n\n    def deregister_hook(self, event, hook):\n        \"\"\"Deregister a previously registered hook.\n        Returns True if the hook existed, False if not.\n        \"\"\"\n\n        try:\n            self.hooks[event].remove(hook)\n            return True\n        except ValueError:\n            return False\n\n\nclass Request(RequestHooksMixin):\n    \"\"\"A user-created :class:`Request <Request>` object.\n\n    Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.\n\n    :param method: HTTP method to use.\n    :param url: URL to send.\n    :param headers: dictionary of headers to send.\n    :param files: dictionary of {filename: fileobject} files to multipart upload.\n    :param data: the body to attach the request. If a dictionary is provided, form-encoding will take place.\n    :param params: dictionary of URL parameters to append to the URL.\n    :param auth: Auth handler or (user, pass) tuple.\n    :param cookies: dictionary or CookieJar of cookies to attach to this request.\n    :param hooks: dictionary of callback hooks, for internal usage.\n\n    Usage::\n\n      >>> import requests\n      >>> req = requests.Request('GET', 'http://httpbin.org/get')\n      >>> req.prepare()\n      <PreparedRequest [GET]>\n\n    \"\"\"\n    def __init__(self,\n        method=None,\n        url=None,\n        headers=None,\n        files=None,\n        data=None,\n        params=None,\n        auth=None,\n        cookies=None,\n        hooks=None):\n\n        # Default empty dicts for dict params.\n        data = [] if data is None else data\n        files = [] if files is None else files\n        headers = {} if headers is None else headers\n        params = {} if params is None else params\n        hooks = {} if hooks is None else hooks\n\n        self.hooks = default_hooks()\n        for (k, v) in list(hooks.items()):\n            self.register_hook(event=k, hook=v)\n\n        self.method = method\n        self.url = url\n        self.headers = headers\n        self.files = files\n        self.data = data\n        self.params = params\n        self.auth = auth\n        self.cookies = cookies\n\n    def __repr__(self):\n        return '<Request [%s]>' % (self.method)\n\n    def prepare(self):\n        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it.\"\"\"\n        p = PreparedRequest()\n        p.prepare(\n            method=self.method,\n            url=self.url,\n            headers=self.headers,\n            files=self.files,\n            data=self.data,\n            params=self.params,\n            auth=self.auth,\n            cookies=self.cookies,\n            hooks=self.hooks,\n        )\n        return p\n\n\nclass PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n    \"\"\"The fully mutable :class:`PreparedRequest <PreparedRequest>` object,\n    containing the exact bytes that will be sent to the server.\n\n    Generated from either a :class:`Request <Request>` object or manually.\n\n    Usage::\n\n      >>> import requests\n      >>> req = requests.Request('GET', 'http://httpbin.org/get')\n      >>> r = req.prepare()\n      <PreparedRequest [GET]>\n\n      >>> s = requests.Session()\n      >>> s.send(r)\n      <Response [200]>\n\n    \"\"\"\n\n    def __init__(self):\n        #: HTTP verb to send to the server.\n        self.method = None\n        #: HTTP URL to send the request to.\n        self.url = None\n        #: dictionary of HTTP headers.\n        self.headers = None\n        # The `CookieJar` used to create the Cookie header will be stored here\n        # after prepare_cookies is called\n        self._cookies = None\n        #: request body to send to the server.\n        self.body = None\n        #: dictionary of callback hooks, for internal usage.\n        self.hooks = default_hooks()\n\n    def prepare(self, method=None, url=None, headers=None, files=None,\n                data=None, params=None, auth=None, cookies=None, hooks=None):\n        \"\"\"Prepares the entire request with the given parameters.\"\"\"\n\n        self.prepare_method(method)\n        self.prepare_url(url, params)\n        self.prepare_headers(headers)\n        self.prepare_cookies(cookies)\n        self.prepare_body(data, files)\n        self.prepare_auth(auth, url)\n        # Note that prepare_auth must be last to enable authentication schemes\n        # such as OAuth to work on a fully prepared request.\n\n        # This MUST go after prepare_auth. Authenticators could add a hook\n        self.prepare_hooks(hooks)\n\n    def __repr__(self):\n        return '<PreparedRequest [%s]>' % (self.method)\n\n    def copy(self):\n        p = PreparedRequest()\n        p.method = self.method\n        p.url = self.url\n        p.headers = self.headers.copy() if self.headers is not None else None\n        p._cookies = self._cookies.copy() if self._cookies is not None else None\n        p.body = self.body\n        p.hooks = self.hooks\n        return p\n\n    def prepare_method(self, method):\n        \"\"\"Prepares the given HTTP method.\"\"\"\n        self.method = method\n        if self.method is not None:\n            self.method = self.method.upper()\n\n    def prepare_url(self, url, params):\n        \"\"\"Prepares the given HTTP URL.\"\"\"\n        #: Accept objects that have string representations.\n        try:\n            url = unicode(url)\n        except NameError:\n            # We're on Python 3.\n            url = str(url)\n        except UnicodeDecodeError:\n            pass\n\n        # Don't do any URL preparation for oddball schemes\n        if ':' in url and not url.lower().startswith('http'):\n            self.url = url\n            return\n\n        # Support for unicode domain names and paths.\n        scheme, auth, host, port, path, query, fragment = parse_url(url)\n\n        if not scheme:\n            raise MissingSchema(\"Invalid URL {0!r}: No schema supplied. \"\n                                \"Perhaps you meant http://{0}?\".format(url))\n\n        if not host:\n            raise InvalidURL(\"Invalid URL %r: No host supplied\" % url)\n\n        # Only want to apply IDNA to the hostname\n        try:\n            host = host.encode('idna').decode('utf-8')\n        except UnicodeError:\n            raise InvalidURL('URL has an invalid label.')\n\n        # Carefully reconstruct the network location\n        netloc = auth or ''\n        if netloc:\n            netloc += '@'\n        netloc += host\n        if port:\n            netloc += ':' + str(port)\n\n        # Bare domains aren't valid URLs.\n        if not path:\n            path = '/'\n\n        if is_py2:\n            if isinstance(scheme, str):\n                scheme = scheme.encode('utf-8')\n            if isinstance(netloc, str):\n                netloc = netloc.encode('utf-8')\n            if isinstance(path, str):\n                path = path.encode('utf-8')\n            if isinstance(query, str):\n                query = query.encode('utf-8')\n            if isinstance(fragment, str):\n                fragment = fragment.encode('utf-8')\n\n        enc_params = self._encode_params(params)\n        if enc_params:\n            if query:\n                query = '%s&%s' % (query, enc_params)\n            else:\n                query = enc_params\n\n        url = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))\n        self.url = url\n\n    def prepare_headers(self, headers):\n        \"\"\"Prepares the given HTTP headers.\"\"\"\n\n        if headers:\n            self.headers = CaseInsensitiveDict((to_native_string(name), value) for name, value in headers.items())\n        else:\n            self.headers = CaseInsensitiveDict()\n\n    def prepare_body(self, data, files):\n        \"\"\"Prepares the given HTTP body data.\"\"\"\n\n        # Check if file, fo, generator, iterator.\n        # If not, run through normal process.\n\n        # Nottin' on you.\n        body = None\n        content_type = None\n        length = None\n\n        is_stream = all([\n            hasattr(data, '__iter__'),\n            not isinstance(data, (basestring, list, tuple, dict))\n        ])\n\n        try:\n            length = super_len(data)\n        except (TypeError, AttributeError, UnsupportedOperation):\n            length = None\n\n        if is_stream:\n            body = data\n\n            if files:\n                raise NotImplementedError('Streamed bodies and files are mutually exclusive.')\n\n            if length is not None:\n                self.headers['Content-Length'] = builtin_str(length)\n            else:\n                self.headers['Transfer-Encoding'] = 'chunked'\n        else:\n            # Multi-part file uploads.\n            if files:\n                (body, content_type) = self._encode_files(files, data)\n            else:\n                if data:\n                    body = self._encode_params(data)\n                    if isinstance(data, basestring) or hasattr(data, 'read'):\n                        content_type = None\n                    else:\n                        content_type = 'application/x-www-form-urlencoded'\n\n            self.prepare_content_length(body)\n\n            # Add content-type if it wasn't explicitly provided.\n            if (content_type) and (not 'content-type' in self.headers):\n                self.headers['Content-Type'] = content_type\n\n        self.body = body\n\n    def prepare_content_length(self, body):\n        if hasattr(body, 'seek') and hasattr(body, 'tell'):\n            body.seek(0, 2)\n            self.headers['Content-Length'] = builtin_str(body.tell())\n            body.seek(0, 0)\n        elif body is not None:\n            l = super_len(body)\n            if l:\n                self.headers['Content-Length'] = builtin_str(l)\n        elif self.method not in ('GET', 'HEAD'):\n            self.headers['Content-Length'] = '0'\n\n    def prepare_auth(self, auth, url=''):\n        \"\"\"Prepares the given HTTP auth data.\"\"\"\n\n        # If no Auth is explicitly provided, extract it from the URL first.\n        if auth is None:\n            url_auth = get_auth_from_url(self.url)\n            auth = url_auth if any(url_auth) else None\n\n        if auth:\n            if isinstance(auth, tuple) and len(auth) == 2:\n                # special-case basic HTTP auth\n                auth = HTTPBasicAuth(*auth)\n\n            # Allow auth to make its changes.\n            r = auth(self)\n\n            # Update self to reflect the auth changes.\n            self.__dict__.update(r.__dict__)\n\n            # Recompute Content-Length\n            self.prepare_content_length(self.body)\n\n    def prepare_cookies(self, cookies):\n        \"\"\"Prepares the given HTTP cookie data.\"\"\"\n\n        if isinstance(cookies, cookielib.CookieJar):\n            self._cookies = cookies\n        else:\n            self._cookies = cookiejar_from_dict(cookies)\n\n        cookie_header = get_cookie_header(self._cookies, self)\n        if cookie_header is not None:\n            self.headers['Cookie'] = cookie_header\n\n    def prepare_hooks(self, hooks):\n        \"\"\"Prepares the given hooks.\"\"\"\n        for event in hooks:\n            self.register_hook(event, hooks[event])\n\n\nclass Response(object):\n    \"\"\"The :class:`Response <Response>` object, which contains a\n    server's response to an HTTP request.\n    \"\"\"\n\n    __attrs__ = [\n        '_content',\n        'status_code',\n        'headers',\n        'url',\n        'history',\n        'encoding',\n        'reason',\n        'cookies',\n        'elapsed',\n        'request',\n    ]\n\n    def __init__(self):\n        super(Response, self).__init__()\n\n        self._content = False\n        self._content_consumed = False\n\n        #: Integer Code of responded HTTP Status, e.g. 404 or 200.\n        self.status_code = None\n\n        #: Case-insensitive Dictionary of Response Headers.\n        #: For example, ``headers['content-encoding']`` will return the\n        #: value of a ``'Content-Encoding'`` response header.\n        self.headers = CaseInsensitiveDict()\n\n        #: File-like object representation of response (for advanced usage).\n        #: Use of ``raw`` requires that ``stream=True`` be set on the request.\n        # This requirement does not apply for use internally to Requests.\n        self.raw = None\n\n        #: Final URL location of Response.\n        self.url = None\n\n        #: Encoding to decode with when accessing r.text.\n        self.encoding = None\n\n        #: A list of :class:`Response <Response>` objects from\n        #: the history of the Request. Any redirect responses will end\n        #: up here. The list is sorted from the oldest to the most recent request.\n        self.history = []\n\n        #: Textual reason of responded HTTP Status, e.g. \"Not Found\" or \"OK\".\n        self.reason = None\n\n        #: A CookieJar of Cookies the server sent back.\n        self.cookies = cookiejar_from_dict({})\n\n        #: The amount of time elapsed between sending the request\n        #: and the arrival of the response (as a timedelta)\n        self.elapsed = datetime.timedelta(0)\n\n        #: The :class:`PreparedRequest <PreparedRequest>` object to which this\n        #: is a response.\n        self.request = None\n\n    def __getstate__(self):\n        # Consume everything; accessing the content attribute makes\n        # sure the content has been fully read.\n        if not self._content_consumed:\n            self.content\n\n        return dict(\n            (attr, getattr(self, attr, None))\n            for attr in self.__attrs__\n        )\n\n    def __setstate__(self, state):\n        for name, value in state.items():\n            setattr(self, name, value)\n\n        # pickled objects do not have .raw\n        setattr(self, '_content_consumed', True)\n        setattr(self, 'raw', None)\n\n    def __repr__(self):\n        return '<Response [%s]>' % (self.status_code)\n\n    def __bool__(self):\n        \"\"\"Returns true if :attr:`status_code` is 'OK'.\"\"\"\n        return self.ok\n\n    def __nonzero__(self):\n        \"\"\"Returns true if :attr:`status_code` is 'OK'.\"\"\"\n        return self.ok\n\n    def __iter__(self):\n        \"\"\"Allows you to use a response as an iterator.\"\"\"\n        return self.iter_content(128)\n\n    @property\n    def ok(self):\n        try:\n            self.raise_for_status()\n        except RequestException:\n            return False\n        return True\n\n    @property\n    def is_redirect(self):\n        \"\"\"True if this Response is a well-formed HTTP redirect that could have\n        been processed automatically (by :meth:`Session.resolve_redirects`).\n        \"\"\"\n        return ('location' in self.headers and self.status_code in REDIRECT_STATI)\n\n    @property\n    def is_permanent_redirect(self):\n        \"\"\"True if this Response one of the permanant versions of redirect\"\"\"\n        return ('location' in self.headers and self.status_code in (codes.moved_permanently, codes.permanent_redirect))\n\n    @property\n    def apparent_encoding(self):\n        \"\"\"The apparent encoding, provided by the chardet library\"\"\"\n        return chardet.detect(self.content)['encoding']\n\n    def iter_content(self, chunk_size=1, decode_unicode=False):\n        \"\"\"Iterates over the response data.  When stream=True is set on the\n        request, this avoids reading the content at once into memory for\n        large responses.  The chunk size is the number of bytes it should\n        read into memory.  This is not necessarily the length of each item\n        returned as decoding can take place.\n\n        If decode_unicode is True, content will be decoded using the best\n        available encoding based on the response.\n        \"\"\"\n        def generate():\n            try:\n                # Special case for urllib3.\n                try:\n                    for chunk in self.raw.stream(chunk_size, decode_content=True):\n                        yield chunk\n                except IncompleteRead as e:\n                    raise ChunkedEncodingError(e)\n                except DecodeError as e:\n                    raise ContentDecodingError(e)\n            except AttributeError:\n                # Standard file-like object.\n                while True:\n                    chunk = self.raw.read(chunk_size)\n                    if not chunk:\n                        break", "answer": "C"}
{"uuid": "8dea01ed-70ee-4ea6-89fa-856715f38d5d", "issue_statement": "method = builtin_str(method) problem\nIn requests/sessions.py is a command:\n\nmethod = builtin_str(method)\nConverts method from\nb\u2019GET\u2019\nto\n\"b'GET\u2019\"\n\nWhich is the literal string, no longer a binary string.  When requests tries to use the method \"b'GET\u2019\u201d, it gets a 404 Not Found response.\n\nI am using python3.4 and python-neutronclient (2.3.9) with requests (2.4.3).  neutronclient is broken because it uses this \"args = utils.safe_encode_list(args)\" command which converts all the values to binary string, including method.\n\nI'm not sure if this is a bug with neutronclient or a bug with requests, but I'm starting here.  Seems if requests handled the method value being a binary string, we wouldn't have any problem.\n\nAlso, I tried in python2.6 and this bug doesn't exist there. Some difference between 2.6 and 3.4 makes this not work right.", "choices": "(A) \n            # Handle redirection without scheme (see: RFC 1808 Section 4)\n            if url.startswith('//'):\n                parsed_rurl = urlparse(resp.url)\n                url = '%s:%s' % (parsed_rurl.scheme, url)\n\n            # The scheme should be lower case...\n            parsed = urlparse(url)\n            url = parsed.geturl()\n\n            # Facilitate relative 'location' headers, as allowed by RFC 7231.\n            # (e.g. '/path/to/resource' instead of 'http://domain.tld/path/to/resource')\n            # Compliant with RFC3986, we percent encode the url.\n            if not urlparse(url).netloc:\n                url = urljoin(resp.url, requote_uri(url))\n            else:\n                url = requote_uri(url)\n\n            prepared_request.url = to_native_string(url)\n            # Cache the url, unless it redirects to itself.\n            if resp.is_permanent_redirect and req.url != prepared_request.url:\n                self.redirect_cache[req.url] = prepared_request.url\n\n            # http://tools.ietf.org/html/rfc7231#section-6.4.4\n            if (resp.status_code == codes.see_other and\n                    method != 'HEAD'):\n                method = 'GET'\n\n            # Do what the browsers do, despite standards...\n            # First, turn 302s into GETs.\n            if resp.status_code == codes.found and method != 'HEAD':\n                method = 'GET'\n\n            # Second, if a POST is responded to with a 301, turn it into a GET.\n            # This bizarre behaviour is explained in Issue 1704.\n            if resp.status_code == codes.moved and method == 'POST':\n                method = 'GET'\n\n            prepared_request.method = method\n\n            # https://github.com/kennethreitz/requests/issues/1084\n            if resp.status_code not in (codes.temporary_redirect, codes.permanent_redirect):\n                if 'Content-Length' in prepared_request.headers:\n                    del prepared_request.headers['Content-Length']\n\n                prepared_request.body = None\n\n            headers = prepared_request.headers\n            try:\n                del headers['Cookie']\n            except KeyError:\n                pass\n\n            extract_cookies_to_jar(prepared_request._cookies, prepared_request, resp.raw)\n            prepared_request._cookies.update(self.cookies)\n            prepared_request.prepare_cookies(prepared_request._cookies)\n\n            # Rebuild auth and proxy information.\n            proxies = self.rebuild_proxies(prepared_request, proxies)\n            self.rebuild_auth(prepared_request, resp)\n\n            # Override the original request.\n            req = prepared_request\n\n            resp = self.send(\n                req,\n                stream=stream,\n                timeout=timeout,\n                verify=verify,\n                cert=cert,\n                proxies=proxies,\n                allow_redirects=False,\n            )\n\n            extract_cookies_to_jar(self.cookies, prepared_request, resp.raw)\n\n            i += 1\n            yield resp\n\n    def rebuild_auth(self, prepared_request, response):\n        \"\"\"\n        When being redirected we may want to strip authentication from the\n        request to avoid leaking credentials. This method intelligently removes\n        and reapplies authentication where possible to avoid credential loss.\n        \"\"\"\n        headers = prepared_request.headers\n        url = prepared_request.url\n\n        if 'Authorization' in headers:\n            # If we get redirected to a new host, we should strip out any\n            #\u00a0authentication headers.\n            original_parsed = urlparse(response.request.url)\n            redirect_parsed = urlparse(url)\n\n            if (original_parsed.hostname != redirect_parsed.hostname):\n                del headers['Authorization']\n\n        # .netrc might have more auth for us on our new host.\n        new_auth = get_netrc_auth(url) if self.trust_env else None\n        if new_auth is not None:\n            prepared_request.prepare_auth(new_auth)\n\n        return\n\n    def rebuild_proxies(self, prepared_request, proxies):\n        \"\"\"\n        This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n        \"\"\"\n        headers = prepared_request.headers\n        url = prepared_request.url\n        scheme = urlparse(url).scheme\n        new_proxies = proxies.copy() if proxies is not None else {}\n\n        if self.trust_env and not should_bypass_proxies(url):\n            environ_proxies = get_environ_proxies(url)\n\n            proxy = environ_proxies.get(scheme)\n\n            if proxy:\n                new_proxies.setdefault(scheme, environ_proxies[scheme])\n\n        if 'Proxy-Authorization' in headers:\n            del headers['Proxy-Authorization']\n\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n\n        if username and password:\n            headers['Proxy-Authorization'] = _basic_auth_str(username, password)\n\n        return new_proxies\n\n\nclass Session(SessionRedirectMixin):\n    \"\"\"A Requests session.\n\n    Provides cookie persistence, connection-pooling, and configuration.\n\n    Basic Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> s.get('http://httpbin.org/get')\n      200\n    \"\"\"\n\n    __attrs__ = [\n        'headers', 'cookies', 'auth', 'proxies', 'hooks', 'params', 'verify',\n        'cert', 'prefetch', 'adapters', 'stream', 'trust_env',\n        'max_redirects', 'redirect_cache'\n    ]\n\n    def __init__(self):\n\n        #: A case-insensitive dictionary of headers to be sent on each\n        #: :class:`Request <Request>` sent from this\n        #: :class:`Session <Session>`.\n        self.headers = default_headers()\n\n        #: Default Authentication tuple or object to attach to\n        #: :class:`Request <Request>`.\n        self.auth = None\n\n        #: Dictionary mapping protocol to the URL of the proxy (e.g.\n        #: {'http': 'foo.bar:3128'}) to be used on each\n        #: :class:`Request <Request>`.\n        self.proxies = {}\n\n        #: Event-handling hooks.\n        self.hooks = default_hooks()\n\n        #: Dictionary of querystring data to attach to each\n        #: :class:`Request <Request>`. The dictionary values may be lists for\n        #: representing multivalued query parameters.\n        self.params = {}\n\n        #: Stream response content default.\n        self.stream = False\n\n        #: SSL Verification default.\n        self.verify = True\n\n        #: SSL certificate default.\n        self.cert = None\n\n        #: Maximum number of redirects allowed. If the request exceeds this\n        #: limit, a :class:`TooManyRedirects` exception is raised.\n        self.max_redirects = DEFAULT_REDIRECT_LIMIT\n\n        #: Should we trust the environment?\n        self.trust_env = True\n\n        #: A CookieJar containing all currently outstanding cookies set on this\n        #: session. By default it is a\n        #: :class:`RequestsCookieJar <requests.cookies.RequestsCookieJar>`, but\n        #: may be any other ``cookielib.CookieJar`` compatible object.\n        self.cookies = cookiejar_from_dict({})\n\n        # Default connection adapters.\n        self.adapters = OrderedDict()\n        self.mount('https://', HTTPAdapter())\n        self.mount('http://', HTTPAdapter())\n\n        self.redirect_cache = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n    def prepare_request(self, request):\n        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for\n        transmission and returns it. The :class:`PreparedRequest` has settings\n        merged from the :class:`Request <Request>` instance and those of the\n        :class:`Session`.\n\n        :param request: :class:`Request` instance to prepare with this\n            session's settings.\n        \"\"\"\n        cookies = request.cookies or {}\n\n        # Bootstrap CookieJar.\n        if not isinstance(cookies, cookielib.CookieJar):\n            cookies = cookiejar_from_dict(cookies)\n\n        # Merge with session cookies\n        merged_cookies = merge_cookies(\n            merge_cookies(RequestsCookieJar(), self.cookies), cookies)\n\n\n        # Set environment's basic authentication if not explicitly set.\n        auth = request.auth\n        if self.trust_env and not auth and not self.auth:\n            auth = get_netrc_auth(request.url)\n\n        p = PreparedRequest()\n        p.prepare(\n            method=request.method.upper(),\n            url=request.url,\n            files=request.files,\n            data=request.data,\n            json=request.json,\n            headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict),\n            params=merge_setting(request.params, self.params),\n            auth=merge_setting(auth, self.auth),\n            cookies=merged_cookies,\n            hooks=merge_hooks(request.hooks, self.hooks),\n        )\n        return p\n\n    def request(self, method, url,\n        params=None,\n        data=None,\n        headers=None,\n        cookies=None,\n        files=None,\n        auth=None,\n        timeout=None,\n        allow_redirects=True,\n        proxies=None,\n        hooks=None,\n        stream=None,\n        verify=None,\n        cert=None,\n        json=None):\n        \"\"\"Constructs a :class:`Request <Request>`, prepares it and sends it.\n        Returns :class:`Response <Response>` object.\n\n        :param method: method for the new :class:`Request` object.\n        :param url: URL for the new :class:`Request` object.\n        :param params: (optional) Dictionary or bytes to be sent in the query\n            string for the :class:`Request`.\n        :param data: (optional) Dictionary or bytes to send in the body of the\n            :class:`Request`.\n        :param json: (optional) json to send in the body of the\n            :class:`Request`.\n        :param headers: (optional) Dictionary of HTTP Headers to send with the\n            :class:`Request`.\n        :param cookies: (optional) Dict or CookieJar object to send with the\n            :class:`Request`.\n        :param files: (optional) Dictionary of ``'filename': file-like-objects``\n            for multipart encoding upload.\n        :param auth: (optional) Auth tuple or callable to enable\n            Basic/Digest/Custom HTTP Auth.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a (`connect timeout, read\n            timeout <user/advanced.html#timeouts>`_) tuple.\n        :type timeout: float or tuple\n        :param allow_redirects: (optional) Set to True by default.\n        :type allow_redirects: bool\n        :param proxies: (optional) Dictionary mapping protocol to the URL of\n            the proxy.\n        :param stream: (optional) whether to immediately download the response\n            content. Defaults to ``False``.\n        :param verify: (optional) if ``True``, the SSL cert will be verified.\n            A CA_BUNDLE path can also be provided.\n        :param cert: (optional) if String, path to ssl client cert file (.pem).\n            If Tuple, ('cert', 'key') pair.\n        \"\"\"\n\n        method = builtin_str(method)\n\n        # Create the Request.\n        req = Request(\n            method = method.upper(),\n            url = url,\n            headers = headers,\n            files = files,\n            data = data or {},\n            json = json,\n            params = params or {},\n            auth = auth,\n            cookies = cookies,\n            hooks = hooks,\n        )\n        prep = self.prepare_request(req)\n\n        proxies = proxies or {}\n\n        settings = self.merge_environment_settings(\n            prep.url, proxies, stream, verify, cert\n        )\n\n        # Send the request.\n        send_kwargs = {\n            'timeout': timeout,\n            'allow_redirects': allow_redirects,\n        }\n        send_kwargs.update(settings)\n        resp = self.send(prep, **send_kwargs)\n\n        return resp\n\n    def get(self, url, **kwargs):\n        \"\"\"Sends a GET request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        kwargs.setdefault('allow_redirects', True)\n        return self.request('GET', url, **kwargs)\n\n    def options(self, url, **kwargs):\n        \"\"\"Sends a OPTIONS request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        kwargs.setdefault('allow_redirects', True)\n        return self.request('OPTIONS', url, **kwargs)\n\n    def head(self, url, **kwargs):\n        \"\"\"Sends a HEAD request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        kwargs.setdefault('allow_redirects', False)\n        return self.request('HEAD', url, **kwargs)\n\n    def post(self, url, data=None, json=None, **kwargs):\n        \"\"\"Sends a POST request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n        :param json: (optional) json to send in the body of the :class:`Request`.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        return self.request('POST', url, data=data, json=json, **kwargs)\n\n    def put(self, url, data=None, **kwargs):\n        \"\"\"Sends a PUT request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        return self.request('PUT', url, data=data, **kwargs)\n\n    def patch(self, url, data=None, **kwargs):\n        \"\"\"Sends a PATCH request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        return self.request('PATCH', url,  data=data, **kwargs)\n\n    def delete(self, url, **kwargs):\n        \"\"\"Sends a DELETE request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        return self.request('DELETE', url, **kwargs)\n\n    def send(self, request, **kwargs):\n        \"\"\"Send a given PreparedRequest.\"\"\"\n        # Set defaults that the hooks can utilize to ensure they always have\n        # the correct parameters to reproduce the previous request.\n        kwargs.setdefault('stream', self.stream)\n        kwargs.setdefault('verify', self.verify)\n(B)         \"\"\"\n        This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n        \"\"\"\n        headers = prepared_request.headers\n        url = prepared_request.url\n        scheme = urlparse(url).scheme\n        new_proxies = proxies.copy() if proxies is not None else {}\n\n        if self.trust_env and not should_bypass_proxies(url):\n            environ_proxies = get_environ_proxies(url)\n\n            proxy = environ_proxies.get(scheme)\n\n            if proxy:\n                new_proxies.setdefault(scheme, environ_proxies[scheme])\n\n        if 'Proxy-Authorization' in headers:\n            del headers['Proxy-Authorization']\n\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n\n        if username and password:\n            headers['Proxy-Authorization'] = _basic_auth_str(username, password)\n\n        return new_proxies\n\n\nclass Session(SessionRedirectMixin):\n    \"\"\"A Requests session.\n\n    Provides cookie persistence, connection-pooling, and configuration.\n\n    Basic Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> s.get('http://httpbin.org/get')\n      200\n    \"\"\"\n\n    __attrs__ = [\n        'headers', 'cookies', 'auth', 'proxies', 'hooks', 'params', 'verify',\n        'cert', 'prefetch', 'adapters', 'stream', 'trust_env',\n        'max_redirects', 'redirect_cache'\n    ]\n\n    def __init__(self):\n\n        #: A case-insensitive dictionary of headers to be sent on each\n        #: :class:`Request <Request>` sent from this\n        #: :class:`Session <Session>`.\n        self.headers = default_headers()\n\n        #: Default Authentication tuple or object to attach to\n        #: :class:`Request <Request>`.\n        self.auth = None\n\n        #: Dictionary mapping protocol to the URL of the proxy (e.g.\n        #: {'http': 'foo.bar:3128'}) to be used on each\n        #: :class:`Request <Request>`.\n        self.proxies = {}\n\n        #: Event-handling hooks.\n        self.hooks = default_hooks()\n\n        #: Dictionary of querystring data to attach to each\n        #: :class:`Request <Request>`. The dictionary values may be lists for\n        #: representing multivalued query parameters.\n        self.params = {}\n\n        #: Stream response content default.\n        self.stream = False\n\n        #: SSL Verification default.\n        self.verify = True\n\n        #: SSL certificate default.\n        self.cert = None\n\n        #: Maximum number of redirects allowed. If the request exceeds this\n        #: limit, a :class:`TooManyRedirects` exception is raised.\n        self.max_redirects = DEFAULT_REDIRECT_LIMIT\n\n        #: Should we trust the environment?\n        self.trust_env = True\n\n        #: A CookieJar containing all currently outstanding cookies set on this\n        #: session. By default it is a\n        #: :class:`RequestsCookieJar <requests.cookies.RequestsCookieJar>`, but\n        #: may be any other ``cookielib.CookieJar`` compatible object.\n        self.cookies = cookiejar_from_dict({})\n\n        # Default connection adapters.\n        self.adapters = OrderedDict()\n        self.mount('https://', HTTPAdapter())\n        self.mount('http://', HTTPAdapter())\n\n        self.redirect_cache = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n    def prepare_request(self, request):\n        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for\n        transmission and returns it. The :class:`PreparedRequest` has settings\n        merged from the :class:`Request <Request>` instance and those of the\n        :class:`Session`.\n\n        :param request: :class:`Request` instance to prepare with this\n            session's settings.\n        \"\"\"\n        cookies = request.cookies or {}\n\n        # Bootstrap CookieJar.\n        if not isinstance(cookies, cookielib.CookieJar):\n            cookies = cookiejar_from_dict(cookies)\n\n        # Merge with session cookies\n        merged_cookies = merge_cookies(\n            merge_cookies(RequestsCookieJar(), self.cookies), cookies)\n\n\n        # Set environment's basic authentication if not explicitly set.\n        auth = request.auth\n        if self.trust_env and not auth and not self.auth:\n            auth = get_netrc_auth(request.url)\n\n        p = PreparedRequest()\n        p.prepare(\n            method=request.method.upper(),\n            url=request.url,\n            files=request.files,\n            data=request.data,\n            json=request.json,\n            headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict),\n            params=merge_setting(request.params, self.params),\n            auth=merge_setting(auth, self.auth),\n            cookies=merged_cookies,\n            hooks=merge_hooks(request.hooks, self.hooks),\n        )\n        return p\n\n    def request(self, method, url,\n        params=None,\n        data=None,\n        headers=None,\n        cookies=None,\n        files=None,\n        auth=None,\n        timeout=None,\n        allow_redirects=True,\n        proxies=None,\n        hooks=None,\n        stream=None,\n        verify=None,\n        cert=None,\n        json=None):\n        \"\"\"Constructs a :class:`Request <Request>`, prepares it and sends it.\n        Returns :class:`Response <Response>` object.\n\n        :param method: method for the new :class:`Request` object.\n        :param url: URL for the new :class:`Request` object.\n        :param params: (optional) Dictionary or bytes to be sent in the query\n            string for the :class:`Request`.\n        :param data: (optional) Dictionary or bytes to send in the body of the\n            :class:`Request`.\n        :param json: (optional) json to send in the body of the\n            :class:`Request`.\n        :param headers: (optional) Dictionary of HTTP Headers to send with the\n            :class:`Request`.\n        :param cookies: (optional) Dict or CookieJar object to send with the\n            :class:`Request`.\n        :param files: (optional) Dictionary of ``'filename': file-like-objects``\n            for multipart encoding upload.\n        :param auth: (optional) Auth tuple or callable to enable\n            Basic/Digest/Custom HTTP Auth.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a (`connect timeout, read\n            timeout <user/advanced.html#timeouts>`_) tuple.\n        :type timeout: float or tuple\n        :param allow_redirects: (optional) Set to True by default.\n        :type allow_redirects: bool\n        :param proxies: (optional) Dictionary mapping protocol to the URL of\n            the proxy.\n        :param stream: (optional) whether to immediately download the response\n            content. Defaults to ``False``.\n        :param verify: (optional) if ``True``, the SSL cert will be verified.\n            A CA_BUNDLE path can also be provided.\n        :param cert: (optional) if String, path to ssl client cert file (.pem).\n            If Tuple, ('cert', 'key') pair.\n        \"\"\"\n\n        method = builtin_str(method)\n\n        # Create the Request.\n        req = Request(\n            method = method.upper(),\n            url = url,\n            headers = headers,\n            files = files,\n            data = data or {},\n            json = json,\n            params = params or {},\n            auth = auth,\n            cookies = cookies,\n            hooks = hooks,\n        )\n        prep = self.prepare_request(req)\n\n        proxies = proxies or {}\n\n        settings = self.merge_environment_settings(\n            prep.url, proxies, stream, verify, cert\n        )\n\n        # Send the request.\n        send_kwargs = {\n            'timeout': timeout,\n            'allow_redirects': allow_redirects,\n        }\n        send_kwargs.update(settings)\n        resp = self.send(prep, **send_kwargs)\n\n        return resp\n\n    def get(self, url, **kwargs):\n        \"\"\"Sends a GET request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        kwargs.setdefault('allow_redirects', True)\n        return self.request('GET', url, **kwargs)\n\n    def options(self, url, **kwargs):\n        \"\"\"Sends a OPTIONS request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        kwargs.setdefault('allow_redirects', True)\n        return self.request('OPTIONS', url, **kwargs)\n\n    def head(self, url, **kwargs):\n        \"\"\"Sends a HEAD request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        kwargs.setdefault('allow_redirects', False)\n        return self.request('HEAD', url, **kwargs)\n\n    def post(self, url, data=None, json=None, **kwargs):\n        \"\"\"Sends a POST request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n        :param json: (optional) json to send in the body of the :class:`Request`.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        return self.request('POST', url, data=data, json=json, **kwargs)\n\n    def put(self, url, data=None, **kwargs):\n        \"\"\"Sends a PUT request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        return self.request('PUT', url, data=data, **kwargs)\n\n    def patch(self, url, data=None, **kwargs):\n        \"\"\"Sends a PATCH request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        return self.request('PATCH', url,  data=data, **kwargs)\n\n    def delete(self, url, **kwargs):\n        \"\"\"Sends a DELETE request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        \"\"\"\n\n        return self.request('DELETE', url, **kwargs)\n\n    def send(self, request, **kwargs):\n        \"\"\"Send a given PreparedRequest.\"\"\"\n        # Set defaults that the hooks can utilize to ensure they always have\n        # the correct parameters to reproduce the previous request.\n        kwargs.setdefault('stream', self.stream)\n        kwargs.setdefault('verify', self.verify)\n        kwargs.setdefault('cert', self.cert)\n        kwargs.setdefault('proxies', self.proxies)\n\n        # It's possible that users might accidentally send a Request object.\n        # Guard against that specific failure case.\n        if not isinstance(request, PreparedRequest):\n            raise ValueError('You can only send PreparedRequests.')\n\n        checked_urls = set()\n        while request.url in self.redirect_cache:\n            checked_urls.add(request.url)\n            new_url = self.redirect_cache.get(request.url)\n            if new_url in checked_urls:\n                break\n            request.url = new_url\n\n        # Set up variables needed for resolve_redirects and dispatching of hooks\n        allow_redirects = kwargs.pop('allow_redirects', True)\n        stream = kwargs.get('stream')\n        timeout = kwargs.get('timeout')\n        verify = kwargs.get('verify')\n        cert = kwargs.get('cert')\n        proxies = kwargs.get('proxies')\n        hooks = request.hooks\n\n        # Get the appropriate adapter to use\n        adapter = self.get_adapter(url=request.url)\n\n        # Start time (approximately) of the request\n        start = datetime.utcnow()\n\n        # Send the request\n        r = adapter.send(request, **kwargs)\n\n        # Total elapsed time of the request (approximately)\n        r.elapsed = datetime.utcnow() - start\n\n        # Response manipulation hooks\n        r = dispatch_hook('response', hooks, r, **kwargs)\n\n        # Persist cookies\n        if r.history:\n\n            # If the hooks create history then we want those cookies too\n            for resp in r.history:\n                extract_cookies_to_jar(self.cookies, resp.request, resp.raw)\n\n        extract_cookies_to_jar(self.cookies, request, r.raw)\n\n        # Redirect resolving generator.\n        gen = self.resolve_redirects(r, request,\n            stream=stream,\n            timeout=timeout,\n            verify=verify,\n            cert=cert,\n            proxies=proxies)\n\n        # Resolve redirects if allowed.\n        history = [resp for resp in gen] if allow_redirects else []\n\n        # Shuffle things around if there's history.\n        if history:\n            # Insert the first (original) request at the start\n            history.insert(0, r)\n            # Get the last request made\n            r = history.pop()\n            r.history = history\n\n        if not stream:\n            r.content\n\n        return r\n\n    def merge_environment_settings(self, url, proxies, stream, verify, cert):\n        \"\"\"Check the environment and merge it with some settings.\"\"\"\n        # Gather clues from the surrounding environment.\n        if self.trust_env:\n            # Set environment's proxies.\n            env_proxies = get_environ_proxies(url) or {}\n            for (k, v) in env_proxies.items():\n                proxies.setdefault(k, v)\n\n            # Look for requests environment configuration and be compatible\n            # with cURL.\n            if verify is True or verify is None:\n                verify = (os.environ.get('REQUESTS_CA_BUNDLE') or\n                          os.environ.get('CURL_CA_BUNDLE'))\n\n        # Merge all the kwargs.\n        proxies = merge_setting(proxies, self.proxies)\n        stream = merge_setting(stream, self.stream)\n        verify = merge_setting(verify, self.verify)\n        cert = merge_setting(cert, self.cert)\n\n        return {'verify': verify, 'proxies': proxies, 'stream': stream,\n                'cert': cert}\n\n    def get_adapter(self, url):\n        \"\"\"Returns the appropriate connnection adapter for the given URL.\"\"\"\n        for (prefix, adapter) in self.adapters.items():\n\n            if url.lower().startswith(prefix):\n                return adapter\n\n        # Nothing matches :-/\n(C) from datetime import datetime\n\nfrom .auth import _basic_auth_str\nfrom .compat import cookielib, OrderedDict, urljoin, urlparse, builtin_str\nfrom .cookies import (\n    cookiejar_from_dict, extract_cookies_to_jar, RequestsCookieJar, merge_cookies)\nfrom .models import Request, PreparedRequest, DEFAULT_REDIRECT_LIMIT\nfrom .hooks import default_hooks, dispatch_hook\nfrom .utils import to_key_val_list, default_headers, to_native_string\nfrom .exceptions import (\n    TooManyRedirects, InvalidSchema, ChunkedEncodingError, ContentDecodingError)\nfrom .structures import CaseInsensitiveDict\n\nfrom .adapters import HTTPAdapter\n\nfrom .utils import (\n    requote_uri, get_environ_proxies, get_netrc_auth, should_bypass_proxies,\n    get_auth_from_url\n)\n\nfrom .status_codes import codes\n\n# formerly defined here, reexposed here for backward compatibility\nfrom .models import REDIRECT_STATI\n\n\ndef merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n    \"\"\"\n    Determines appropriate setting for a given request, taking into account the\n    explicit setting on that request, and the setting in the session. If a\n    setting is a dictionary, they will be merged together using `dict_class`\n    \"\"\"\n\n    if session_setting is None:\n        return request_setting\n\n    if request_setting is None:\n        return session_setting\n\n    # Bypass if not a dictionary (e.g. verify)\n    if not (\n            isinstance(session_setting, Mapping) and\n            isinstance(request_setting, Mapping)\n    ):\n        return request_setting\n\n    merged_setting = dict_class(to_key_val_list(session_setting))\n    merged_setting.update(to_key_val_list(request_setting))\n\n    # Remove keys that are set to None.\n    for (k, v) in request_setting.items():\n        if v is None:\n            del merged_setting[k]\n\n    merged_setting = dict((k, v) for (k, v) in merged_setting.items() if v is not None)\n\n    return merged_setting\n\n\ndef merge_hooks(request_hooks, session_hooks, dict_class=OrderedDict):\n    \"\"\"\n    Properly merges both requests and session hooks.\n\n    This is necessary because when request_hooks == {'response': []}, the\n    merge breaks Session hooks entirely.\n    \"\"\"\n    if session_hooks is None or session_hooks.get('response') == []:\n        return request_hooks\n\n    if request_hooks is None or request_hooks.get('response') == []:\n        return session_hooks\n\n    return merge_setting(request_hooks, session_hooks, dict_class)\n\n\nclass SessionRedirectMixin(object):\n    def resolve_redirects(self, resp, req, stream=False, timeout=None,\n                          verify=True, cert=None, proxies=None):\n        \"\"\"Receives a Response. Returns a generator of Responses.\"\"\"\n\n        i = 0\n        hist = [] # keep track of history\n\n        while resp.is_redirect:\n            prepared_request = req.copy()\n\n            if i > 0:\n                # Update history and keep track of redirects.\n                hist.append(resp)\n                new_hist = list(hist)\n                resp.history = new_hist\n\n            try:\n                resp.content  # Consume socket so it can be released\n            except (ChunkedEncodingError, ContentDecodingError, RuntimeError):\n                resp.raw.read(decode_content=False)\n\n            if i >= self.max_redirects:\n                raise TooManyRedirects('Exceeded %s redirects.' % self.max_redirects)\n\n            # Release the connection back into the pool.\n            resp.close()\n\n            url = resp.headers['location']\n            method = req.method\n\n            # Handle redirection without scheme (see: RFC 1808 Section 4)\n            if url.startswith('//'):\n                parsed_rurl = urlparse(resp.url)\n                url = '%s:%s' % (parsed_rurl.scheme, url)\n\n            # The scheme should be lower case...\n            parsed = urlparse(url)\n            url = parsed.geturl()\n\n            # Facilitate relative 'location' headers, as allowed by RFC 7231.\n            # (e.g. '/path/to/resource' instead of 'http://domain.tld/path/to/resource')\n            # Compliant with RFC3986, we percent encode the url.\n            if not urlparse(url).netloc:\n                url = urljoin(resp.url, requote_uri(url))\n            else:\n                url = requote_uri(url)\n\n            prepared_request.url = to_native_string(url)\n            # Cache the url, unless it redirects to itself.\n            if resp.is_permanent_redirect and req.url != prepared_request.url:\n                self.redirect_cache[req.url] = prepared_request.url\n\n            # http://tools.ietf.org/html/rfc7231#section-6.4.4\n            if (resp.status_code == codes.see_other and\n                    method != 'HEAD'):\n                method = 'GET'\n\n            # Do what the browsers do, despite standards...\n            # First, turn 302s into GETs.\n            if resp.status_code == codes.found and method != 'HEAD':\n                method = 'GET'\n\n            # Second, if a POST is responded to with a 301, turn it into a GET.\n            # This bizarre behaviour is explained in Issue 1704.\n            if resp.status_code == codes.moved and method == 'POST':\n                method = 'GET'\n\n            prepared_request.method = method\n\n            # https://github.com/kennethreitz/requests/issues/1084\n            if resp.status_code not in (codes.temporary_redirect, codes.permanent_redirect):\n                if 'Content-Length' in prepared_request.headers:\n                    del prepared_request.headers['Content-Length']\n\n                prepared_request.body = None\n\n            headers = prepared_request.headers\n            try:\n                del headers['Cookie']\n            except KeyError:\n                pass\n\n            extract_cookies_to_jar(prepared_request._cookies, prepared_request, resp.raw)\n            prepared_request._cookies.update(self.cookies)\n            prepared_request.prepare_cookies(prepared_request._cookies)\n\n            # Rebuild auth and proxy information.\n            proxies = self.rebuild_proxies(prepared_request, proxies)\n            self.rebuild_auth(prepared_request, resp)\n\n            # Override the original request.\n            req = prepared_request\n\n            resp = self.send(\n                req,\n                stream=stream,\n                timeout=timeout,\n                verify=verify,\n                cert=cert,\n                proxies=proxies,\n                allow_redirects=False,\n            )\n\n            extract_cookies_to_jar(self.cookies, prepared_request, resp.raw)\n\n            i += 1\n            yield resp\n\n    def rebuild_auth(self, prepared_request, response):\n        \"\"\"\n        When being redirected we may want to strip authentication from the\n        request to avoid leaking credentials. This method intelligently removes\n        and reapplies authentication where possible to avoid credential loss.\n        \"\"\"\n        headers = prepared_request.headers\n        url = prepared_request.url\n\n        if 'Authorization' in headers:\n            # If we get redirected to a new host, we should strip out any\n            #\u00a0authentication headers.\n            original_parsed = urlparse(response.request.url)\n            redirect_parsed = urlparse(url)\n\n            if (original_parsed.hostname != redirect_parsed.hostname):\n                del headers['Authorization']\n\n        # .netrc might have more auth for us on our new host.\n        new_auth = get_netrc_auth(url) if self.trust_env else None\n        if new_auth is not None:\n            prepared_request.prepare_auth(new_auth)\n\n        return\n\n    def rebuild_proxies(self, prepared_request, proxies):\n        \"\"\"\n        This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n        \"\"\"\n        headers = prepared_request.headers\n        url = prepared_request.url\n        scheme = urlparse(url).scheme\n        new_proxies = proxies.copy() if proxies is not None else {}\n\n        if self.trust_env and not should_bypass_proxies(url):\n            environ_proxies = get_environ_proxies(url)\n\n            proxy = environ_proxies.get(scheme)\n\n            if proxy:\n                new_proxies.setdefault(scheme, environ_proxies[scheme])\n\n        if 'Proxy-Authorization' in headers:\n            del headers['Proxy-Authorization']\n\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n\n        if username and password:\n            headers['Proxy-Authorization'] = _basic_auth_str(username, password)\n\n        return new_proxies\n\n\nclass Session(SessionRedirectMixin):\n    \"\"\"A Requests session.\n\n    Provides cookie persistence, connection-pooling, and configuration.\n\n    Basic Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> s.get('http://httpbin.org/get')\n      200\n    \"\"\"\n\n    __attrs__ = [\n        'headers', 'cookies', 'auth', 'proxies', 'hooks', 'params', 'verify',\n        'cert', 'prefetch', 'adapters', 'stream', 'trust_env',\n        'max_redirects', 'redirect_cache'\n    ]\n\n    def __init__(self):\n\n        #: A case-insensitive dictionary of headers to be sent on each\n        #: :class:`Request <Request>` sent from this\n        #: :class:`Session <Session>`.\n        self.headers = default_headers()\n\n        #: Default Authentication tuple or object to attach to\n        #: :class:`Request <Request>`.\n        self.auth = None\n\n        #: Dictionary mapping protocol to the URL of the proxy (e.g.\n        #: {'http': 'foo.bar:3128'}) to be used on each\n        #: :class:`Request <Request>`.\n        self.proxies = {}\n\n        #: Event-handling hooks.\n        self.hooks = default_hooks()\n\n        #: Dictionary of querystring data to attach to each\n        #: :class:`Request <Request>`. The dictionary values may be lists for\n        #: representing multivalued query parameters.\n        self.params = {}\n\n        #: Stream response content default.\n        self.stream = False\n\n        #: SSL Verification default.\n        self.verify = True\n\n        #: SSL certificate default.\n        self.cert = None\n\n        #: Maximum number of redirects allowed. If the request exceeds this\n        #: limit, a :class:`TooManyRedirects` exception is raised.\n        self.max_redirects = DEFAULT_REDIRECT_LIMIT\n\n        #: Should we trust the environment?\n        self.trust_env = True\n\n        #: A CookieJar containing all currently outstanding cookies set on this\n        #: session. By default it is a\n        #: :class:`RequestsCookieJar <requests.cookies.RequestsCookieJar>`, but\n        #: may be any other ``cookielib.CookieJar`` compatible object.\n        self.cookies = cookiejar_from_dict({})\n\n        # Default connection adapters.\n        self.adapters = OrderedDict()\n        self.mount('https://', HTTPAdapter())\n        self.mount('http://', HTTPAdapter())\n\n        self.redirect_cache = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n    def prepare_request(self, request):\n        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for\n        transmission and returns it. The :class:`PreparedRequest` has settings\n        merged from the :class:`Request <Request>` instance and those of the\n        :class:`Session`.\n\n        :param request: :class:`Request` instance to prepare with this\n            session's settings.\n        \"\"\"\n        cookies = request.cookies or {}\n\n        # Bootstrap CookieJar.\n        if not isinstance(cookies, cookielib.CookieJar):\n            cookies = cookiejar_from_dict(cookies)\n\n        # Merge with session cookies\n        merged_cookies = merge_cookies(\n            merge_cookies(RequestsCookieJar(), self.cookies), cookies)\n\n\n        # Set environment's basic authentication if not explicitly set.\n        auth = request.auth\n        if self.trust_env and not auth and not self.auth:\n            auth = get_netrc_auth(request.url)\n\n        p = PreparedRequest()\n        p.prepare(\n            method=request.method.upper(),\n            url=request.url,\n            files=request.files,\n            data=request.data,\n            json=request.json,\n            headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict),\n            params=merge_setting(request.params, self.params),\n            auth=merge_setting(auth, self.auth),\n            cookies=merged_cookies,\n            hooks=merge_hooks(request.hooks, self.hooks),\n        )\n        return p\n\n    def request(self, method, url,\n        params=None,\n        data=None,\n        headers=None,\n        cookies=None,\n        files=None,\n        auth=None,\n        timeout=None,\n        allow_redirects=True,\n        proxies=None,\n        hooks=None,\n        stream=None,\n        verify=None,\n        cert=None,\n        json=None):\n        \"\"\"Constructs a :class:`Request <Request>`, prepares it and sends it.\n        Returns :class:`Response <Response>` object.\n\n        :param method: method for the new :class:`Request` object.\n        :param url: URL for the new :class:`Request` object.\n        :param params: (optional) Dictionary or bytes to be sent in the query\n            string for the :class:`Request`.\n        :param data: (optional) Dictionary or bytes to send in the body of the\n            :class:`Request`.\n        :param json: (optional) json to send in the body of the\n            :class:`Request`.\n        :param headers: (optional) Dictionary of HTTP Headers to send with the\n            :class:`Request`.\n        :param cookies: (optional) Dict or CookieJar object to send with the\n            :class:`Request`.\n        :param files: (optional) Dictionary of ``'filename': file-like-objects``\n            for multipart encoding upload.\n        :param auth: (optional) Auth tuple or callable to enable\n            Basic/Digest/Custom HTTP Auth.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a (`connect timeout, read\n            timeout <user/advanced.html#timeouts>`_) tuple.\n        :type timeout: float or tuple\n        :param allow_redirects: (optional) Set to True by default.\n        :type allow_redirects: bool\n        :param proxies: (optional) Dictionary mapping protocol to the URL of\n            the proxy.\n        :param stream: (optional) whether to immediately download the response\n            content. Defaults to ``False``.\n        :param verify: (optional) if ``True``, the SSL cert will be verified.\n            A CA_BUNDLE path can also be provided.\n        :param cert: (optional) if String, path to ssl client cert file (.pem).\n            If Tuple, ('cert', 'key') pair.\n        \"\"\"\n\n        method = builtin_str(method)\n\n        # Create the Request.\n        req = Request(", "answer": "D"}
{"uuid": "727730a0-3ed1-4270-b386-6674582617c3", "issue_statement": "Uncertain about content/text vs iter_content(decode_unicode=True/False)\nWhen requesting an application/json document, I'm seeing `next(r.iter_content(16*1024, decode_unicode=True))` returning bytes, whereas `r.text` returns unicode. My understanding was that both should return a unicode object. In essence, I thought \"iter_content\" was equivalent to \"iter_text\" when decode_unicode was True. Have I misunderstood something? I can provide an example if needed.\n\nFor reference, I'm using python 3.5.1 and requests 2.10.0.\n\nThanks!", "choices": "(A) \ndef stream_decode_response_unicode(iterator, r):\n    \"\"\"Stream decodes a iterator.\"\"\"\n\n    if r.encoding is None:\n        for item in iterator:\n            yield item\n        return\n\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b'', final=True)\n    if rv:\n        yield rv\n\n\ndef iter_slices(string, slice_length):\n(B)         return params['charset'].strip(\"'\\\"\")\n\n    if 'text' in content_type:\n        return 'ISO-8859-1'\n\n\ndef stream_decode_response_unicode(iterator, r):\n    \"\"\"Stream decodes a iterator.\"\"\"\n\n    if r.encoding is None:\n        for item in iterator:\n            yield item\n        return\n\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b'', final=True)\n(C)     for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b'', final=True)\n    if rv:\n        yield rv\n\n\ndef iter_slices(string, slice_length):\n    \"\"\"Iterate over slices of a string.\"\"\"\n    pos = 0\n    while pos < len(string):\n        yield string[pos:pos + slice_length]\n        pos += slice_length\n\n\ndef get_unicode_from_response(r):\n    \"\"\"Returns the requested content back in unicode.\n\n(D)         for item in iterator:\n            yield item\n        return\n\n    decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n    for chunk in iterator:\n        rv = decoder.decode(chunk)\n        if rv:\n            yield rv\n    rv = decoder.decode(b'', final=True)\n    if rv:\n        yield rv\n\n\ndef iter_slices(string, slice_length):\n    \"\"\"Iterate over slices of a string.\"\"\"\n    pos = 0\n    while pos < len(string):\n        yield string[pos:pos + slice_length]\n        pos += slice_length", "answer": "A"}
{"uuid": "bb9144ba-6b4b-4d4f-b2ad-202af26cb9fd", "issue_statement": "Allow lists in the dict values of the hooks argument\nCurrently the Request class has a .register_hook() method but it parses the dictionary it expects from it's hooks argument weirdly: the argument can only specify one hook function per hook.  If you pass in a list of hook functions per hook the code in Request.**init**() will wrap the list in a list which then fails when the hooks are consumed (since a list is not callable).  This is especially annoying since you can not use multiple hooks from a session.  The only way to get multiple hooks now is to create the request object without sending it, then call .register_hook() multiple times and then finally call .send().\n\nThis would all be much easier if Request.**init**() parsed the hooks parameter in a way that it accepts lists as it's values.", "choices": "(A) \n    def register_hook(self, event, hook):\n        \"\"\"Properly register a hook.\"\"\"\n\n        self.hooks[event].append(hook)\n\n    def deregister_hook(self, event, hook):\n        \"\"\"Deregister a previously registered hook.\n        Returns True if the hook existed, False if not.\n        \"\"\"\n(B)             url.append(query)\n\n        return ''.join(url)\n\n    def register_hook(self, event, hook):\n        \"\"\"Properly register a hook.\"\"\"\n\n        self.hooks[event].append(hook)\n\n    def deregister_hook(self, event, hook):\n(C) \n        self.hooks[event].append(hook)\n\n    def deregister_hook(self, event, hook):\n        \"\"\"Deregister a previously registered hook.\n        Returns True if the hook existed, False if not.\n        \"\"\"\n\n        try:\n            self.hooks[event].remove(hook)\n(D) \n    def deregister_hook(self, event, hook):\n        \"\"\"Deregister a previously registered hook.\n        Returns True if the hook existed, False if not.\n        \"\"\"\n\n        try:\n            self.hooks[event].remove(hook)\n            return True\n        except ValueError:", "answer": "A"}
{"uuid": "a56e6d1e-6073-4ea0-bcb2-5f35ea1a4628", "issue_statement": "Ignore missing variables when concatenating datasets?\nSeveral users (@raj-kesavan, @richardotis, now myself) have wondered about how to concatenate xray Datasets with different variables.\n\nWith the current `xray.concat`, you need to awkwardly create dummy variables filled with `NaN` in datasets that don't have them (or drop mismatched variables entirely). Neither of these are great options -- `concat` should have an option (the default?) to take care of this for the user.\n\nThis would also be more consistent with `pd.concat`, which takes a more relaxed approach to matching dataframes with different variables (it does an outer join).", "choices": "(A)             absent_merge_vars = variables_to_merge - set(ds.variables)\n            if absent_merge_vars:\n                raise ValueError(\n                    \"variables %r are present in some datasets but not others. \"\n                    % absent_merge_vars\n                )\n\n            for var in variables_to_merge:\n                to_merge[var].append(ds.variables[var])\n(B)         to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            if absent_merge_vars:\n                raise ValueError(\n                    \"variables %r are present in some datasets but not others. \"\n                    % absent_merge_vars\n                )\n(C)                 raise ValueError(\n                    \"variables %r are present in some datasets but not others. \"\n                    % absent_merge_vars\n                )\n\n            for var in variables_to_merge:\n                to_merge[var].append(ds.variables[var])\n\n        for var in variables_to_merge:\n(D) \n    result_vars = {}\n    if variables_to_merge:\n        to_merge = {var: [] for var in variables_to_merge}\n\n        for ds in datasets:\n            absent_merge_vars = variables_to_merge - set(ds.variables)\n            if absent_merge_vars:\n                raise ValueError(", "answer": "B"}
{"uuid": "2cc10c86-617d-49f2-8555-afe25eb6d26d", "issue_statement": "to_unstacked_dataset broken for single-dim variables\n<!-- A short summary of the issue, if appropriate -->\r\n\r\n\r\n#### MCVE Code Sample\r\n\r\n```python\r\narr = xr.DataArray(\r\n     np.arange(3),\r\n     coords=[(\"x\", [0, 1, 2])],\r\n )\r\ndata = xr.Dataset({\"a\": arr, \"b\": arr})\r\nstacked = data.to_stacked_array('y', sample_dims=['x'])\r\nunstacked = stacked.to_unstacked_dataset('y')\r\n# MergeError: conflicting values for variable 'y' on objects to be combined. You can skip this check by specifying compat='override'.\r\n```\r\n\r\n#### Expected Output\r\nA working roundtrip.\r\n\r\n#### Problem Description\r\nI need to stack a bunch of variables and later unstack them again, however this doesn't work if the variables only have a single dimension.\r\n\r\n#### Versions\r\n\r\n<details><summary>Output of <tt>xr.show_versions()</tt></summary>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.7.3 (default, Mar 27 2019, 22:11:17) \r\n[GCC 7.3.0]\r\npython-bits: 64\r\nOS: Linux\r\nOS-release: 4.15.0-96-generic\r\nmachine: x86_64\r\nprocessor: x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_GB.UTF-8\r\nLOCALE: en_GB.UTF-8\r\nlibhdf5: 1.10.4\r\nlibnetcdf: 4.6.2\r\n\r\nxarray: 0.15.1\r\npandas: 1.0.3\r\nnumpy: 1.17.3\r\nscipy: 1.3.1\r\nnetCDF4: 1.4.2\r\npydap: None\r\nh5netcdf: None\r\nh5py: 2.10.0\r\nNio: None\r\nzarr: None\r\ncftime: 1.0.4.2\r\nnc_time_axis: None\r\nPseudoNetCDF: None\r\nrasterio: None\r\ncfgrib: None\r\niris: None\r\nbottleneck: None\r\ndask: 2.10.1\r\ndistributed: 2.10.0\r\nmatplotlib: 3.1.1\r\ncartopy: None\r\nseaborn: 0.10.0\r\nnumbagg: None\r\nsetuptools: 41.0.0\r\npip: 19.0.3\r\nconda: 4.8.3\r\npytest: 5.3.5\r\nIPython: 7.9.0\r\nsphinx: None\r\n\r\n\r\n</details>", "choices": "(A) \n        # unstacked dataset\n        return Dataset(data_dict)\n\n    def transpose(self, *dims: Hashable, transpose_coords: bool = True) -> \"DataArray\":\n        \"\"\"Return a new DataArray object with transposed dimensions.\n\n(B)         variable_dim = idx.names[level_number]\n\n        # pull variables out of datarray\n        data_dict = {}\n        for k in variables:\n            data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n\n(C)         # pull variables out of datarray\n        data_dict = {}\n        for k in variables:\n            data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n\n        # unstacked dataset\n        return Dataset(data_dict)\n(D)         for k in variables:\n            data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n\n        # unstacked dataset\n        return Dataset(data_dict)\n\n    def transpose(self, *dims: Hashable, transpose_coords: bool = True) -> \"DataArray\":", "answer": "C"}
{"uuid": "e17279d0-8fcd-4f92-8fe8-a8fd14fd4316", "issue_statement": "Feature request: show units in dataset overview\nHere's a hypothetical dataset:\r\n\r\n```\r\n<xarray.Dataset>\r\nDimensions:  (time: 3, x: 988, y: 822)\r\nCoordinates:\r\n  * x         (x) float64 ...\r\n  * y         (y) float64 ...\r\n  * time      (time) datetime64[ns] ...\r\nData variables:\r\n    rainfall  (time, y, x) float32 ...\r\n    max_temp  (time, y, x) float32 ...\r\n```\r\n\r\nIt would be really nice if the units of the coordinates and of the data variables were shown in the `Dataset` repr, for example as:\r\n\r\n```\r\n<xarray.Dataset>\r\nDimensions:  (time: 3, x: 988, y: 822)\r\nCoordinates:\r\n  * x, in metres         (x)            float64 ...\r\n  * y, in metres         (y)            float64 ...\r\n  * time                 (time)         datetime64[ns] ...\r\nData variables:\r\n    rainfall, in mm      (time, y, x)   float32 ...\r\n    max_temp, in deg C   (time, y, x)   float32 ...\r\n```", "choices": "(A)         return inline_sparse_repr(var.data)\n    elif hasattr(var._data, \"__array_function__\"):\n        return maybe_truncate(repr(var._data).replace(\"\\n\", \" \"), max_width)\n    else:\n        # internal xarray array type\n        return \"...\"\n\n\n(B)         return inline_dask_repr(var.data)\n    elif isinstance(var._data, sparse_array_type):\n        return inline_sparse_repr(var.data)\n    elif hasattr(var._data, \"__array_function__\"):\n        return maybe_truncate(repr(var._data).replace(\"\\n\", \" \"), max_width)\n    else:\n        # internal xarray array type\n        return \"...\"\n(C)         return format_array_flat(var, max_width)\n    elif isinstance(var._data, dask_array_type):\n        return inline_dask_repr(var.data)\n    elif isinstance(var._data, sparse_array_type):\n        return inline_sparse_repr(var.data)\n    elif hasattr(var._data, \"__array_function__\"):\n        return maybe_truncate(repr(var._data).replace(\"\\n\", \" \"), max_width)\n    else:\n(D)         return maybe_truncate(repr(var._data).replace(\"\\n\", \" \"), max_width)\n    else:\n        # internal xarray array type\n        return \"...\"\n\n\ndef summarize_variable(\n    name: Hashable, var, col_width: int, marker: str = \" \", max_width: int = None", "answer": "B"}
{"uuid": "3e82cceb-32bc-423d-b9e7-29d4dfeaba4c", "issue_statement": "DataSet.update causes chunked dask DataArray to evalute its values eagerly \n**What happened**:\r\nUsed `DataSet.update` to update a chunked dask DataArray, but the DataArray is no longer chunked after the update.\r\n\r\n**What you expected to happen**:\r\nThe chunked DataArray should still be chunked after the update\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nfoo = xr.DataArray(np.random.randn(3, 3), dims=(\"x\", \"y\")).chunk()  # foo is chunked\r\nds = xr.Dataset({\"foo\": foo, \"bar\": (\"x\", [1, 2, 3])})  # foo is still chunked here\r\nds  # you can verify that foo is chunked\r\n```\r\n```python\r\nupdate_dict = {\"foo\": ((\"x\", \"y\"), ds.foo[1:, :]), \"bar\": (\"x\", ds.bar[1:])}\r\nupdate_dict[\"foo\"][1]  # foo is still chunked\r\n```\r\n```python\r\nds.update(update_dict)\r\nds  # now foo is no longer chunked\r\n```\r\n\r\n**Environment**:\r\n\r\n<details><summary>Output of <tt>xr.show_versions()</tt></summary>\r\n\r\n```\r\ncommit: None\r\npython: 3.8.3 (default, Jul  2 2020, 11:26:31) \r\n[Clang 10.0.0 ]\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 19.6.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: en_US.UTF-8\r\nlibhdf5: 1.10.6\r\nlibnetcdf: None\r\n\r\nxarray: 0.16.0\r\npandas: 1.0.5\r\nnumpy: 1.18.5\r\nscipy: 1.5.0\r\nnetCDF4: None\r\npydap: None\r\nh5netcdf: None\r\nh5py: 2.10.0\r\nNio: None\r\nzarr: None\r\ncftime: None\r\nnc_time_axis: None\r\nPseudoNetCDF: None\r\nrasterio: None\r\ncfgrib: None\r\niris: None\r\nbottleneck: None\r\ndask: 2.20.0\r\ndistributed: 2.20.0\r\nmatplotlib: 3.2.2\r\ncartopy: None\r\nseaborn: None\r\nnumbagg: None\r\npint: None\r\nsetuptools: 49.2.0.post20200714\r\npip: 20.1.1\r\nconda: None\r\npytest: 5.4.3\r\nIPython: 7.16.1\r\nsphinx: None\r\n```\r\n\r\n</details>\nDataset constructor with DataArray triggers computation\nIs it intentional that creating a Dataset with a DataArray and dimension names for a single variable causes computation of that variable?  In other words, why does ```xr.Dataset(dict(a=('d0', xr.DataArray(da.random.random(10)))))``` cause the dask array to compute?\r\n\r\nA longer example:\r\n\r\n```python\r\nimport dask.array as da\r\nimport xarray as xr\r\nx = da.random.randint(1, 10, size=(100, 25))\r\nds = xr.Dataset(dict(a=xr.DataArray(x, dims=('x', 'y'))))\r\ntype(ds.a.data)\r\ndask.array.core.Array\r\n\r\n# Recreate the dataset with the same array, but also redefine the dimensions\r\nds2 = xr.Dataset(dict(a=(('x', 'y'), ds.a))\r\ntype(ds2.a.data)\r\nnumpy.ndarray\r\n```", "choices": "(A)                 \"Could not convert tuple of form \"\n                \"(dims, data[, attrs, encoding]): \"\n                \"{} to Variable.\".format(obj)\n            )\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)\n    elif isinstance(obj, (set, dict)):\n        raise TypeError(\"variable {!r} has invalid type {!r}\".format(name, type(obj)))\n    elif name is not None:\n        data = as_compatible_data(obj)\n        if data.ndim != 1:\n            raise MissingDimensionsError(\n                \"cannot set variable %r with %r-dimensional data \"\n                \"without explicit dimension names. Pass a tuple of \"\n(B)             obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            # use .format() instead of % because it handles tuples consistently\n            raise error.__class__(\n                \"Could not convert tuple of form \"\n                \"(dims, data[, attrs, encoding]): \"\n                \"{} to Variable.\".format(obj)\n            )\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)\n    elif isinstance(obj, (set, dict)):\n        raise TypeError(\"variable {!r} has invalid type {!r}\".format(name, type(obj)))\n    elif name is not None:\n        data = as_compatible_data(obj)\n(C)     if isinstance(obj, DataArray):\n        # extract the primary Variable from DataArrays\n        obj = obj.variable\n\n    if isinstance(obj, Variable):\n        obj = obj.copy(deep=False)\n    elif isinstance(obj, tuple):\n        try:\n            obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            # use .format() instead of % because it handles tuples consistently\n            raise error.__class__(\n                \"Could not convert tuple of form \"\n                \"(dims, data[, attrs, encoding]): \"\n                \"{} to Variable.\".format(obj)\n            )\n(D)     if isinstance(obj, Variable):\n        obj = obj.copy(deep=False)\n    elif isinstance(obj, tuple):\n        try:\n            obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            # use .format() instead of % because it handles tuples consistently\n            raise error.__class__(\n                \"Could not convert tuple of form \"\n                \"(dims, data[, attrs, encoding]): \"\n                \"{} to Variable.\".format(obj)\n            )\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)", "answer": "D"}
{"uuid": "25d794a3-4838-4ca2-8fbd-61a9f13f2981", "issue_statement": "Trailing whitespace in DatasetGroupBy text representation\nWhen displaying a DatasetGroupBy in an interactive Python session, the first line of output contains a trailing whitespace. The first example in the documentation demonstrate this:\r\n\r\n```pycon\r\n>>> import xarray as xr, numpy as np\r\n>>> ds = xr.Dataset(\r\n...     {\"foo\": ((\"x\", \"y\"), np.random.rand(4, 3))},\r\n...     coords={\"x\": [10, 20, 30, 40], \"letters\": (\"x\", list(\"abba\"))},\r\n... )\r\n>>> ds.groupby(\"letters\")\r\nDatasetGroupBy, grouped over 'letters' \r\n2 groups with labels 'a', 'b'.\r\n```\r\n\r\nThere is a trailing whitespace in the first line of output which is \"DatasetGroupBy, grouped over 'letters' \". This can be seen more clearly by converting the object to a string (note the whitespace before `\\n`):\r\n\r\n```pycon\r\n>>> str(ds.groupby(\"letters\"))\r\n\"DatasetGroupBy, grouped over 'letters' \\n2 groups with labels 'a', 'b'.\"\r\n```\r\n\r\n\r\nWhile this isn't a problem in itself, it causes an issue for us because we use flake8 in continuous integration to verify that our code is correctly formatted and we also have doctests that rely on DatasetGroupBy textual representation. Flake8 reports a violation on the trailing whitespaces in our docstrings. If we remove the trailing whitespaces, our doctests fail because the expected output doesn't match the actual output. So we have conflicting constraints coming from our tools which both seem reasonable. Trailing whitespaces are forbidden by flake8 because, among other reasons, they lead to noisy git diffs. Doctest want the expected output to be exactly the same as the actual output and considers a trailing whitespace to be a significant difference. We could configure flake8 to ignore this particular violation for the files in which we have these doctests, but this may cause other trailing whitespaces to creep in our code, which we don't want. Unfortunately it's not possible to just add `# NoQA` comments to get flake8 to ignore the violation only for specific lines because that creates a difference between expected and actual output from doctest point of view. Flake8 doesn't allow to disable checks for blocks of code either.\r\n\r\nIs there a reason for having this trailing whitespace in DatasetGroupBy representation? Whould it be OK to remove it? If so please let me know and I can make a pull request.", "choices": "(A) \n    def __iter__(self):\n        return zip(self._unique_coord.values, self._iter_grouped())\n\n    def __repr__(self):\n        return \"{}, grouped over {!r} \\n{!r} groups with labels {}.\".format(\n            self.__class__.__name__,\n(B)     def __repr__(self):\n        return \"{}, grouped over {!r} \\n{!r} groups with labels {}.\".format(\n            self.__class__.__name__,\n            self._unique_coord.name,\n            self._unique_coord.size,\n            \", \".join(format_array_flat(self._unique_coord, 30).split()),\n        )\n(C)             self.__class__.__name__,\n            self._unique_coord.name,\n            self._unique_coord.size,\n            \", \".join(format_array_flat(self._unique_coord, 30).split()),\n        )\n\n    def _get_index_and_items(self, index, grouper):\n(D)         return zip(self._unique_coord.values, self._iter_grouped())\n\n    def __repr__(self):\n        return \"{}, grouped over {!r} \\n{!r} groups with labels {}.\".format(\n            self.__class__.__name__,\n            self._unique_coord.name,\n            self._unique_coord.size,", "answer": "D"}
{"uuid": "ebb06bb5-18da-44fa-8461-611b8556d427", "issue_statement": "\"--notes\" option ignores note tags that are entirely punctuation\n### Bug description\n\nIf a note tag specified with the `--notes` option is entirely punctuation, pylint won't report a fixme warning (W0511).\r\n\r\n```python\r\n# YES: yes\r\n# ???: no\r\n```\r\n\r\n`pylint test.py --notes=\"YES,???\"` will return a fixme warning (W0511) for the first line, but not the second.\n\n### Configuration\n\n```ini\nDefault\n```\n\n\n### Command used\n\n```shell\npylint test.py --notes=\"YES,???\"\n```\n\n\n### Pylint output\n\n```shell\n************* Module test\r\ntest.py:1:1: W0511: YES: yes (fixme)\n```\n\n\n### Expected behavior\n\n```\r\n************* Module test\r\ntest.py:1:1: W0511: YES: yes (fixme)\r\ntest.py:2:1: W0511: ???: no (fixme)\r\n```\n\n### Pylint version\n\n```shell\npylint 2.12.2\r\nastroid 2.9.0\r\nPython 3.10.2 (main, Feb  2 2022, 05:51:25) [Clang 13.0.0 (clang-1300.0.29.3)]\n```\n\n\n### OS / Environment\n\nmacOS 11.6.1\n\n### Additional dependencies\n\n_No response_", "choices": "(A)             regex_string = rf\"#\\s*({notes})\\b\"\n\n        self._fixme_pattern = re.compile(regex_string, re.I)\n\n    def _check_encoding(\n        self, lineno: int, line: bytes, file_encoding: str\n    ) -> Optional[str]:\n        try:\n            return line.decode(file_encoding)\n(B) \n    def open(self):\n        super().open()\n\n        notes = \"|\".join(re.escape(note) for note in self.config.notes)\n        if self.config.notes_rgx:\n            regex_string = rf\"#\\s*({notes}|{self.config.notes_rgx})\\b\"\n        else:\n            regex_string = rf\"#\\s*({notes})\\b\"\n(C) \n        notes = \"|\".join(re.escape(note) for note in self.config.notes)\n        if self.config.notes_rgx:\n            regex_string = rf\"#\\s*({notes}|{self.config.notes_rgx})\\b\"\n        else:\n            regex_string = rf\"#\\s*({notes})\\b\"\n\n        self._fixme_pattern = re.compile(regex_string, re.I)\n\n(D)             regex_string = rf\"#\\s*({notes}|{self.config.notes_rgx})\\b\"\n        else:\n            regex_string = rf\"#\\s*({notes})\\b\"\n\n        self._fixme_pattern = re.compile(regex_string, re.I)\n\n    def _check_encoding(\n        self, lineno: int, line: bytes, file_encoding: str\n    ) -> Optional[str]:", "answer": "C"}
{"uuid": "63682efa-824a-442c-b346-71eec187a14c", "issue_statement": "Traceback printed for unrecognized option\n### Bug description\n\nA traceback is printed when an unrecognized option is passed to pylint.\n\n### Configuration\n\n_No response_\n\n### Command used\n\n```shell\npylint -Q\n```\n\n\n### Pylint output\n\n```shell\n************* Module Command line\r\nCommand line:1:0: E0015: Unrecognized option found: Q (unrecognized-option)\r\nTraceback (most recent call last):\r\n  File \"/Users/markbyrne/venv310/bin/pylint\", line 33, in <module>\r\n    sys.exit(load_entry_point('pylint', 'console_scripts', 'pylint')())\r\n  File \"/Users/markbyrne/programming/pylint/pylint/__init__.py\", line 24, in run_pylint\r\n    PylintRun(argv or sys.argv[1:])\r\n  File \"/Users/markbyrne/programming/pylint/pylint/lint/run.py\", line 135, in __init__\r\n    args = _config_initialization(\r\n  File \"/Users/markbyrne/programming/pylint/pylint/config/config_initialization.py\", line 85, in _config_initialization\r\n    raise _UnrecognizedOptionError(options=unrecognized_options)\r\npylint.config.exceptions._UnrecognizedOptionError\n```\n\n\n### Expected behavior\n\nThe top part of the current output is handy:\r\n`Command line:1:0: E0015: Unrecognized option found: Q (unrecognized-option)`\r\n\r\nThe traceback I don't think is expected & not user-friendly.\r\nA usage tip, for example:\r\n```python\r\nmypy -Q\r\nusage: mypy [-h] [-v] [-V] [more options; see below]\r\n            [-m MODULE] [-p PACKAGE] [-c PROGRAM_TEXT] [files ...]\r\nmypy: error: unrecognized arguments: -Q\r\n```\n\n### Pylint version\n\n```shell\npylint 2.14.0-dev0\r\nastroid 2.11.3\r\nPython 3.10.0b2 (v3.10.0b2:317314165a, May 31 2021, 10:02:22) [Clang 12.0.5 (clang-1205.0.22.9)]\n```\n\n\n### OS / Environment\n\n_No response_\n\n### Additional dependencies\n\n_No response_", "choices": "(A)         msg = \", \".join(unrecognized_options)\n        linter.add_message(\"unrecognized-option\", line=0, args=msg)\n        raise _UnrecognizedOptionError(options=unrecognized_options)\n\n    # Set the current module to configuration as we don't know where\n    # the --load-plugins key is coming from\n    linter.set_current_module(\"Command line or configuration file\")\n(B)             unrecognized_options.append(opt[2:])\n        elif opt.startswith(\"-\"):\n            unrecognized_options.append(opt[1:])\n    if unrecognized_options:\n        msg = \", \".join(unrecognized_options)\n        linter.add_message(\"unrecognized-option\", line=0, args=msg)\n        raise _UnrecognizedOptionError(options=unrecognized_options)\n(C)             unrecognized_options.append(opt[1:])\n    if unrecognized_options:\n        msg = \", \".join(unrecognized_options)\n        linter.add_message(\"unrecognized-option\", line=0, args=msg)\n        raise _UnrecognizedOptionError(options=unrecognized_options)\n\n    # Set the current module to configuration as we don't know where\n(D)         raise _UnrecognizedOptionError(options=unrecognized_options)\n\n    # Set the current module to configuration as we don't know where\n    # the --load-plugins key is coming from\n    linter.set_current_module(\"Command line or configuration file\")\n\n    # We have loaded configuration from config file and command line. Now, we can", "answer": "C"}
{"uuid": "fd32719a-dce8-440a-bdbf-f9cb843c7524", "issue_statement": "`--recursive=y` ignores `ignore-paths`\n### Bug description\r\n\r\nWhen running recursively, it seems `ignore-paths` in my settings in pyproject.toml is completely ignored\r\n\r\n### Configuration\r\n\r\n```ini\r\n[tool.pylint.MASTER]\r\nignore-paths = [\r\n  # Auto generated\r\n  \"^src/gen/.*$\",\r\n]\r\n```\r\n\r\n\r\n### Command used\r\n\r\n```shell\r\npylint --recursive=y src/\r\n```\r\n\r\n\r\n### Pylint output\r\n\r\n```shell\r\n************* Module region_selection\r\nsrc\\region_selection.py:170:0: R0914: Too many local variables (17/15) (too-many-locals)\r\n************* Module about\r\nsrc\\gen\\about.py:2:0: R2044: Line with empty comment (empty-comment)\r\nsrc\\gen\\about.py:4:0: R2044: Line with empty comment (empty-comment)\r\nsrc\\gen\\about.py:57:0: C0301: Line too long (504/120) (line-too-long)\r\nsrc\\gen\\about.py:12:0: C0103: Class name \"Ui_AboutAutoSplitWidget\" doesn't conform to '_?_?[a-zA-Z]+?$' pattern (invalid-name)\r\nsrc\\gen\\about.py:12:0: R0205: Class 'Ui_AboutAutoSplitWidget' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)\r\nsrc\\gen\\about.py:13:4: C0103: Method name \"setupUi\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\about.py:13:22: C0103: Argument name \"AboutAutoSplitWidget\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\about.py:53:4: C0103: Method name \"retranslateUi\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\about.py:53:28: C0103: Argument name \"AboutAutoSplitWidget\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\about.py:24:8: W0201: Attribute 'ok_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\about.py:27:8: W0201: Attribute 'created_by_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\about.py:30:8: W0201: Attribute 'version_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\about.py:33:8: W0201: Attribute 'donate_text_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\about.py:37:8: W0201: Attribute 'donate_button_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\about.py:43:8: W0201: Attribute 'icon_label' defined outside __init__ (attribute-defined-outside-init)\r\n************* Module design\r\nsrc\\gen\\design.py:2:0: R2044: Line with empty comment (empty-comment)\r\nsrc\\gen\\design.py:4:0: R2044: Line with empty comment (empty-comment)\r\nsrc\\gen\\design.py:328:0: C0301: Line too long (123/120) (line-too-long)\r\nsrc\\gen\\design.py:363:0: C0301: Line too long (125/120) (line-too-long)\r\nsrc\\gen\\design.py:373:0: C0301: Line too long (121/120) (line-too-long)\r\nsrc\\gen\\design.py:412:0: C0301: Line too long (131/120) (line-too-long)\r\nsrc\\gen\\design.py:12:0: C0103: Class name \"Ui_MainWindow\" doesn't conform to '_?_?[a-zA-Z]+?$' pattern (invalid-name)\r\nsrc\\gen\\design.py:308:8: C0103: Attribute name \"actionSplit_Settings\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\design.py:318:8: C0103: Attribute name \"actionCheck_for_Updates_on_Open\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\design.py:323:8: C0103: Attribute name \"actionLoop_Last_Split_Image_To_First_Image\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\design.py:325:8: C0103: Attribute name \"actionAuto_Start_On_Reset\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\design.py:327:8: C0103: Attribute name \"actionGroup_dummy_splits_when_undoing_skipping\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\design.py:12:0: R0205: Class 'Ui_MainWindow' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)\r\nsrc\\gen\\design.py:12:0: R0902: Too many instance attributes (69/15) (too-many-instance-attributes)\r\nsrc\\gen\\design.py:13:4: C0103: Method name \"setupUi\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\design.py:13:22: C0103: Argument name \"MainWindow\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\design.py:16:8: C0103: Variable name \"sizePolicy\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\design.py:13:4: R0915: Too many statements (339/50) (too-many-statements)\r\nsrc\\gen\\design.py:354:4: C0103: Method name \"retranslateUi\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\design.py:354:28: C0103: Argument name \"MainWindow\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\design.py:354:4: R0915: Too many statements (61/50) (too-many-statements)\r\nsrc\\gen\\design.py:31:8: W0201: Attribute 'central_widget' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:33:8: W0201: Attribute 'x_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:36:8: W0201: Attribute 'select_region_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:40:8: W0201: Attribute 'start_auto_splitter_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:44:8: W0201: Attribute 'reset_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:49:8: W0201: Attribute 'undo_split_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:54:8: W0201: Attribute 'skip_split_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:59:8: W0201: Attribute 'check_fps_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:63:8: W0201: Attribute 'fps_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:66:8: W0201: Attribute 'live_image' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:75:8: W0201: Attribute 'current_split_image' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:81:8: W0201: Attribute 'current_image_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:85:8: W0201: Attribute 'width_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:88:8: W0201: Attribute 'height_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:91:8: W0201: Attribute 'fps_value_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:95:8: W0201: Attribute 'width_spinbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:101:8: W0201: Attribute 'height_spinbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:107:8: W0201: Attribute 'capture_region_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:111:8: W0201: Attribute 'current_image_file_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:115:8: W0201: Attribute 'take_screenshot_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:119:8: W0201: Attribute 'x_spinbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:128:8: W0201: Attribute 'y_spinbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:136:8: W0201: Attribute 'y_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:139:8: W0201: Attribute 'align_region_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:143:8: W0201: Attribute 'select_window_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:147:8: W0201: Attribute 'browse_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:151:8: W0201: Attribute 'split_image_folder_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:154:8: W0201: Attribute 'split_image_folder_input' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:158:8: W0201: Attribute 'capture_region_window_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:162:8: W0201: Attribute 'image_loop_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:165:8: W0201: Attribute 'similarity_viewer_groupbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:169:8: W0201: Attribute 'table_live_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:173:8: W0201: Attribute 'table_highest_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:177:8: W0201: Attribute 'table_threshold_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:181:8: W0201: Attribute 'line_1' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:186:8: W0201: Attribute 'table_current_image_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:189:8: W0201: Attribute 'table_reset_image_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:192:8: W0201: Attribute 'line_2' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:197:8: W0201: Attribute 'line_3' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:202:8: W0201: Attribute 'line_4' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:207:8: W0201: Attribute 'line_5' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:212:8: W0201: Attribute 'table_current_image_live_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:216:8: W0201: Attribute 'table_current_image_highest_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:220:8: W0201: Attribute 'table_current_image_threshold_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:224:8: W0201: Attribute 'table_reset_image_live_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:228:8: W0201: Attribute 'table_reset_image_highest_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:232:8: W0201: Attribute 'table_reset_image_threshold_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:236:8: W0201: Attribute 'reload_start_image_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:240:8: W0201: Attribute 'start_image_status_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:243:8: W0201: Attribute 'start_image_status_value_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:246:8: W0201: Attribute 'image_loop_value_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:249:8: W0201: Attribute 'previous_image_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:254:8: W0201: Attribute 'next_image_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:296:8: W0201: Attribute 'menu_bar' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:299:8: W0201: Attribute 'menu_help' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:301:8: W0201: Attribute 'menu_file' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:304:8: W0201: Attribute 'action_view_help' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:306:8: W0201: Attribute 'action_about' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:308:8: W0201: Attribute 'actionSplit_Settings' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:310:8: W0201: Attribute 'action_save_profile' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:312:8: W0201: Attribute 'action_load_profile' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:314:8: W0201: Attribute 'action_save_profile_as' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:316:8: W0201: Attribute 'action_check_for_updates' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:318:8: W0201: Attribute 'actionCheck_for_Updates_on_Open' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:323:8: W0201: Attribute 'actionLoop_Last_Split_Image_To_First_Image' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:325:8: W0201: Attribute 'actionAuto_Start_On_Reset' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:327:8: W0201: Attribute 'actionGroup_dummy_splits_when_undoing_skipping' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:329:8: W0201: Attribute 'action_settings' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\design.py:331:8: W0201: Attribute 'action_check_for_updates_on_open' defined outside __init__ (attribute-defined-outside-init)\r\n************* Module resources_rc\r\nsrc\\gen\\resources_rc.py:1:0: C0302: Too many lines in module (2311/1000) (too-many-lines)\r\nsrc\\gen\\resources_rc.py:8:0: C0103: Constant name \"qt_resource_data\" doesn't conform to UPPER_CASE naming style (invalid-name)\r\nsrc\\gen\\resources_rc.py:2278:0: C0103: Constant name \"qt_resource_name\" doesn't conform to UPPER_CASE naming style (invalid-name)\r\nsrc\\gen\\resources_rc.py:2294:0: C0103: Constant name \"qt_resource_struct\" doesn't conform to UPPER_CASE naming style (invalid-name)\r\nsrc\\gen\\resources_rc.py:2305:0: C0103: Function name \"qInitResources\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\resources_rc.py:2308:0: C0103: Function name \"qCleanupResources\" doesn't conform to snake_case naming style (invalid-name)\r\n************* Module settings\r\nsrc\\gen\\settings.py:2:0: R2044: Line with empty comment (empty-comment)\r\nsrc\\gen\\settings.py:4:0: R2044: Line with empty comment (empty-comment)\r\nsrc\\gen\\settings.py:61:0: C0301: Line too long (158/120) (line-too-long)\r\nsrc\\gen\\settings.py:123:0: C0301: Line too long (151/120) (line-too-long)\r\nsrc\\gen\\settings.py:209:0: C0301: Line too long (162/120) (line-too-long)\r\nsrc\\gen\\settings.py:214:0: C0301: Line too long (121/120) (line-too-long)\r\nsrc\\gen\\settings.py:221:0: C0301: Line too long (177/120) (line-too-long)\r\nsrc\\gen\\settings.py:223:0: C0301: Line too long (181/120) (line-too-long)\r\nsrc\\gen\\settings.py:226:0: C0301: Line too long (461/120) (line-too-long)\r\nsrc\\gen\\settings.py:228:0: C0301: Line too long (192/120) (line-too-long)\r\nsrc\\gen\\settings.py:12:0: C0103: Class name \"Ui_DialogSettings\" doesn't conform to '_?_?[a-zA-Z]+?$' pattern (invalid-name)\r\nsrc\\gen\\settings.py:12:0: R0205: Class 'Ui_DialogSettings' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)\r\nsrc\\gen\\settings.py:12:0: R0902: Too many instance attributes (35/15) (too-many-instance-attributes)\r\nsrc\\gen\\settings.py:13:4: C0103: Method name \"setupUi\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\settings.py:13:22: C0103: Argument name \"DialogSettings\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\settings.py:16:8: C0103: Variable name \"sizePolicy\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\settings.py:13:4: R0915: Too many statements (190/50) (too-many-statements)\r\nsrc\\gen\\settings.py:205:4: C0103: Method name \"retranslateUi\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\settings.py:205:28: C0103: Argument name \"DialogSettings\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\settings.py:26:8: W0201: Attribute 'capture_settings_groupbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:29:8: W0201: Attribute 'fps_limit_spinbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:36:8: W0201: Attribute 'fps_limit_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:40:8: W0201: Attribute 'live_capture_region_checkbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:46:8: W0201: Attribute 'capture_method_combobox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:49:8: W0201: Attribute 'capture_method_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:52:8: W0201: Attribute 'capture_device_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:55:8: W0201: Attribute 'capture_device_combobox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:59:8: W0201: Attribute 'image_settings_groupbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:65:8: W0201: Attribute 'default_comparison_method' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:73:8: W0201: Attribute 'default_comparison_method_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:76:8: W0201: Attribute 'default_pause_time_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:80:8: W0201: Attribute 'default_pause_time_spinbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:87:8: W0201: Attribute 'default_similarity_threshold_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:92:8: W0201: Attribute 'default_similarity_threshold_spinbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:98:8: W0201: Attribute 'loop_splits_checkbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:104:8: W0201: Attribute 'custom_image_settings_info_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:111:8: W0201: Attribute 'default_delay_time_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:116:8: W0201: Attribute 'default_delay_time_spinbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:121:8: W0201: Attribute 'hotkeys_groupbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:127:8: W0201: Attribute 'set_pause_hotkey_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:131:8: W0201: Attribute 'split_input' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:137:8: W0201: Attribute 'undo_split_input' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:143:8: W0201: Attribute 'split_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:146:8: W0201: Attribute 'reset_input' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:152:8: W0201: Attribute 'set_undo_split_hotkey_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:156:8: W0201: Attribute 'reset_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:159:8: W0201: Attribute 'set_reset_hotkey_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:163:8: W0201: Attribute 'set_split_hotkey_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:167:8: W0201: Attribute 'pause_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:170:8: W0201: Attribute 'pause_input' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:176:8: W0201: Attribute 'undo_split_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:179:8: W0201: Attribute 'set_skip_split_hotkey_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:183:8: W0201: Attribute 'skip_split_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\settings.py:186:8: W0201: Attribute 'skip_split_input' defined outside __init__ (attribute-defined-outside-init)\r\n************* Module update_checker\r\nsrc\\gen\\update_checker.py:2:0: R2044: Line with empty comment (empty-comment)\r\nsrc\\gen\\update_checker.py:4:0: R2044: Line with empty comment (empty-comment)\r\nsrc\\gen\\update_checker.py:12:0: C0103: Class name \"Ui_UpdateChecker\" doesn't conform to '_?_?[a-zA-Z]+?$' pattern (invalid-name)\r\nsrc\\gen\\update_checker.py:12:0: R0205: Class 'Ui_UpdateChecker' inherits from object, can be safely removed from bases in python3 (useless-object-inheritance)\r\nsrc\\gen\\update_checker.py:13:4: C0103: Method name \"setupUi\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\update_checker.py:13:22: C0103: Argument name \"UpdateChecker\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\update_checker.py:17:8: C0103: Variable name \"sizePolicy\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\update_checker.py:33:8: C0103: Variable name \"sizePolicy\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\update_checker.py:13:4: R0915: Too many statements (56/50) (too-many-statements)\r\nsrc\\gen\\update_checker.py:71:4: C0103: Method name \"retranslateUi\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\update_checker.py:71:28: C0103: Argument name \"UpdateChecker\" doesn't conform to snake_case naming style (invalid-name)\r\nsrc\\gen\\update_checker.py:31:8: W0201: Attribute 'update_status_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\update_checker.py:39:8: W0201: Attribute 'current_version_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\update_checker.py:42:8: W0201: Attribute 'latest_version_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\update_checker.py:45:8: W0201: Attribute 'go_to_download_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\update_checker.py:48:8: W0201: Attribute 'left_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\update_checker.py:52:8: W0201: Attribute 'right_button' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\update_checker.py:55:8: W0201: Attribute 'current_version_number_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\update_checker.py:59:8: W0201: Attribute 'latest_version_number_label' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\update_checker.py:63:8: W0201: Attribute 'do_not_ask_again_checkbox' defined outside __init__ (attribute-defined-outside-init)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (region_capture -> region_selection) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (error_messages -> user_profile -> region_capture -> region_selection) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplitImage -> split_parser) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoControlledWorker -> error_messages -> AutoSplit) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> user_profile -> region_capture -> region_selection -> error_messages) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplitImage -> error_messages -> user_profile) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> menu_bar -> user_profile -> region_capture -> region_selection -> error_messages) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> region_selection -> error_messages) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> error_messages) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (error_messages -> user_profile -> region_selection) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (error_messages -> user_profile) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplitImage -> split_parser -> error_messages -> user_profile) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> menu_bar -> region_selection -> error_messages) (cyclic-import)\r\nsrc\\gen\\update_checker.py:1:0: R0401: Cyclic import (AutoSplit -> menu_bar -> error_messages) (cyclic-import)\r\n\r\n--------------------------------------------------------------------------\r\nYour code has been rated at -158.32/10 (previous run: -285.20/10, +126.88)\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nsrc\\gen\\* should not be checked\r\n\r\n### Pylint version\r\n\r\n```shell\r\npylint 2.14.1\r\nastroid 2.11.5\r\nPython 3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]\r\n```\r\n\r\n\r\n### OS / Environment\r\n\r\nWindows 10.0.19044\r\n\r\n\r\n### Additional dependencies\r\n\r\n_No response_", "choices": "(A)     element: str,\n    ignore_list: list[str],\n    ignore_list_re: list[Pattern[str]],\n    ignore_list_paths_re: list[Pattern[str]],\n) -> bool:\n    basename = os.path.basename(element)\n    return (\n(B)     ignore_list_re: list[Pattern[str]],\n    ignore_list_paths_re: list[Pattern[str]],\n) -> bool:\n    basename = os.path.basename(element)\n    return (\n        basename in ignore_list\n        or _is_in_ignore_list_re(basename, ignore_list_re)\n(C) ) -> bool:\n    basename = os.path.basename(element)\n    return (\n        basename in ignore_list\n        or _is_in_ignore_list_re(basename, ignore_list_re)\n        or _is_in_ignore_list_re(element, ignore_list_paths_re)\n    )\n(D)     return (\n        basename in ignore_list\n        or _is_in_ignore_list_re(basename, ignore_list_re)\n        or _is_in_ignore_list_re(element, ignore_list_paths_re)\n    )\n\n", "answer": "B"}
{"uuid": "e1d95c1c-a647-405a-92c0-83f04efbd1a4", "issue_statement": "Linting fails if module contains module of the same name\n### Steps to reproduce\r\n\r\nGiven multiple files:\r\n```\r\n.\r\n`-- a/\r\n    |-- a.py\r\n    `-- b.py\r\n```\r\nWhich are all empty, running `pylint a` fails:\r\n\r\n```\r\n$ pylint a\r\n************* Module a\r\na/__init__.py:1:0: F0010: error while code parsing: Unable to load file a/__init__.py:\r\n[Errno 2] No such file or directory: 'a/__init__.py' (parse-error)\r\n$\r\n```\r\n\r\nHowever, if I rename `a.py`, `pylint a` succeeds:\r\n\r\n```\r\n$ mv a/a.py a/c.py\r\n$ pylint a\r\n$\r\n```\r\nAlternatively, I can also `touch a/__init__.py`, but that shouldn't be necessary anymore.\r\n\r\n### Current behavior\r\n\r\nRunning `pylint a` if `a/a.py` is present fails while searching for an `__init__.py` file.\r\n\r\n### Expected behavior\r\n\r\nRunning `pylint a` if `a/a.py` is present should succeed.\r\n\r\n### pylint --version output\r\n\r\nResult of `pylint --version` output:\r\n\r\n```\r\npylint 3.0.0a3\r\nastroid 2.5.6\r\nPython 3.8.5 (default, Jan 27 2021, 15:41:15) \r\n[GCC 9.3.0]\r\n```\r\n\r\n### Additional info\r\n\r\nThis also has some side-effects in module resolution. For example, if I create another file `r.py`:\r\n\r\n```\r\n.\r\n|-- a\r\n|   |-- a.py\r\n|   `-- b.py\r\n`-- r.py\r\n```\r\n\r\nWith the content:\r\n\r\n```\r\nfrom a import b\r\n```\r\n\r\nRunning `pylint -E r` will run fine, but `pylint -E r a` will fail. Not just for module a, but for module r as well.\r\n\r\n```\r\n************* Module r\r\nr.py:1:0: E0611: No name 'b' in module 'a' (no-name-in-module)\r\n************* Module a\r\na/__init__.py:1:0: F0010: error while code parsing: Unable to load file a/__init__.py:\r\n[Errno 2] No such file or directory: 'a/__init__.py' (parse-error)\r\n```\r\n\r\nAgain, if I rename `a.py` to `c.py`, `pylint -E r a` will work perfectly.", "choices": "(A)                 )\n            except ImportError:\n                modname = os.path.splitext(basename)[0]\n            if os.path.isdir(something):\n                filepath = os.path.join(something, \"__init__.py\")\n            else:\n                filepath = something\n        else:\n            # suppose it's a module or package\n            modname = something\n            try:\n                filepath = modutils.file_from_modpath(\n                    modname.split(\".\"), path=additional_search_path\n                )\n                if filepath is None:\n                    continue\n            except (ImportError, SyntaxError) as ex:\n                # The SyntaxError is a Python bug and should be\n                # removed once we move away from imp.find_module: https://bugs.python.org/issue10588\n                errors.append({\"key\": \"fatal\", \"mod\": modname, \"ex\": ex})\n                continue\n        filepath = os.path.normpath(filepath)\n        modparts = (modname or something).split(\".\")\n        try:\n            spec = modutils.file_info_from_modpath(\n                modparts, path=additional_search_path\n            )\n        except ImportError:\n            # Might not be acceptable, don't crash.\n            is_namespace = False\n(B)         else:\n            # suppose it's a module or package\n            modname = something\n            try:\n                filepath = modutils.file_from_modpath(\n                    modname.split(\".\"), path=additional_search_path\n                )\n                if filepath is None:\n                    continue\n            except (ImportError, SyntaxError) as ex:\n                # The SyntaxError is a Python bug and should be\n                # removed once we move away from imp.find_module: https://bugs.python.org/issue10588\n                errors.append({\"key\": \"fatal\", \"mod\": modname, \"ex\": ex})\n                continue\n        filepath = os.path.normpath(filepath)\n        modparts = (modname or something).split(\".\")\n        try:\n            spec = modutils.file_info_from_modpath(\n                modparts, path=additional_search_path\n            )\n        except ImportError:\n            # Might not be acceptable, don't crash.\n            is_namespace = False\n            is_directory = os.path.isdir(something)\n        else:\n            is_namespace = modutils.is_namespace(spec)\n            is_directory = modutils.is_directory(spec)\n        if not is_namespace:\n            result.append(\n                {\n(C)     errors: list[ErrorDescriptionDict] = []\n    path = sys.path.copy()\n\n    for something in files_or_modules:\n        basename = os.path.basename(something)\n        if _is_ignored_file(\n            something, ignore_list, ignore_list_re, ignore_list_paths_re\n        ):\n            continue\n        module_path = get_python_path(something)\n        additional_search_path = [\".\", module_path] + path\n        if os.path.exists(something):\n            # this is a file or a directory\n            try:\n                modname = \".\".join(\n                    modutils.modpath_from_file(something, path=additional_search_path)\n                )\n            except ImportError:\n                modname = os.path.splitext(basename)[0]\n            if os.path.isdir(something):\n                filepath = os.path.join(something, \"__init__.py\")\n            else:\n                filepath = something\n        else:\n            # suppose it's a module or package\n            modname = something\n            try:\n                filepath = modutils.file_from_modpath(\n                    modname.split(\".\"), path=additional_search_path\n                )\n(D)             continue\n        module_path = get_python_path(something)\n        additional_search_path = [\".\", module_path] + path\n        if os.path.exists(something):\n            # this is a file or a directory\n            try:\n                modname = \".\".join(\n                    modutils.modpath_from_file(something, path=additional_search_path)\n                )\n            except ImportError:\n                modname = os.path.splitext(basename)[0]\n            if os.path.isdir(something):\n                filepath = os.path.join(something, \"__init__.py\")\n            else:\n                filepath = something\n        else:\n            # suppose it's a module or package\n            modname = something\n            try:\n                filepath = modutils.file_from_modpath(\n                    modname.split(\".\"), path=additional_search_path\n                )\n                if filepath is None:\n                    continue\n            except (ImportError, SyntaxError) as ex:\n                # The SyntaxError is a Python bug and should be\n                # removed once we move away from imp.find_module: https://bugs.python.org/issue10588\n                errors.append({\"key\": \"fatal\", \"mod\": modname, \"ex\": ex})\n                continue\n        filepath = os.path.normpath(filepath)", "answer": "D"}
{"uuid": "f1fe5136-d00e-48e0-9a6c-0328d7e6efda", "issue_statement": "rxg include '\\p{Han}' will throw error\n### Bug description\r\n\r\nconfig rxg in pylintrc with \\p{Han} will throw err\r\n\r\n### Configuration\r\n.pylintrc:\r\n\r\n```ini\r\nfunction-rgx=[\\p{Han}a-z_][\\p{Han}a-z0-9_]{2,30}$\r\n```\r\n\r\n### Command used\r\n\r\n```shell\r\npylint\r\n```\r\n\r\n\r\n### Pylint output\r\n\r\n```shell\r\n(venvtest) tsung-hande-MacBook-Pro:robot_is_comming tsung-han$ pylint\r\nTraceback (most recent call last):\r\n  File \"/Users/tsung-han/PycharmProjects/robot_is_comming/venvtest/bin/pylint\", line 8, in <module>\r\n    sys.exit(run_pylint())\r\n  File \"/Users/tsung-han/PycharmProjects/robot_is_comming/venvtest/lib/python3.9/site-packages/pylint/__init__.py\", line 25, in run_pylint\r\n    PylintRun(argv or sys.argv[1:])\r\n  File \"/Users/tsung-han/PycharmProjects/robot_is_comming/venvtest/lib/python3.9/site-packages/pylint/lint/run.py\", line 161, in __init__\r\n    args = _config_initialization(\r\n  File \"/Users/tsung-han/PycharmProjects/robot_is_comming/venvtest/lib/python3.9/site-packages/pylint/config/config_initialization.py\", line 57, in _config_initialization\r\n    linter._parse_configuration_file(config_args)\r\n  File \"/Users/tsung-han/PycharmProjects/robot_is_comming/venvtest/lib/python3.9/site-packages/pylint/config/arguments_manager.py\", line 244, in _parse_configuration_file\r\n    self.config, parsed_args = self._arg_parser.parse_known_args(\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 1858, in parse_known_args\r\n    namespace, args = self._parse_known_args(args, namespace)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 2067, in _parse_known_args\r\n    start_index = consume_optional(start_index)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 2007, in consume_optional\r\n    take_action(action, args, option_string)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 1919, in take_action\r\n    argument_values = self._get_values(action, argument_strings)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 2450, in _get_values\r\n    value = self._get_value(action, arg_string)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/argparse.py\", line 2483, in _get_value\r\n    result = type_func(arg_string)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/re.py\", line 252, in compile\r\n    return _compile(pattern, flags)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/re.py\", line 304, in _compile\r\n    p = sre_compile.compile(pattern, flags)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_compile.py\", line 788, in compile\r\n    p = sre_parse.parse(p, flags)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_parse.py\", line 955, in parse\r\n    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_parse.py\", line 444, in _parse_sub\r\n    itemsappend(_parse(source, state, verbose, nested + 1,\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_parse.py\", line 555, in _parse\r\n    code1 = _class_escape(source, this)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_parse.py\", line 350, in _class_escape\r\n    raise source.error('bad escape %s' % escape, len(escape))\r\nre.error: bad escape \\p at position 1\r\n```\r\n\r\n### Expected behavior\r\n\r\nnot throw error\r\n\r\n### Pylint version\r\n\r\n```shell\r\npylint 2.14.4\r\nastroid 2.11.7\r\nPython 3.9.13 (main, May 24 2022, 21:28:44) \r\n[Clang 13.0.0 (clang-1300.0.29.30)]\r\n```\r\n\r\n\r\n### OS / Environment\r\n\r\nmacOS 11.6.7", "choices": "(A)     \"\"\"Transforms a comma separated list of regular expressions paths.\"\"\"\n    patterns: list[Pattern[str]] = []\n    for pattern in _csv_transformer(value):\n        patterns.append(\n            re.compile(\n                str(pathlib.PureWindowsPath(pattern)).replace(\"\\\\\", \"\\\\\\\\\")\n                + \"|\"\n                + pathlib.PureWindowsPath(pattern).as_posix()\n            )\n        )\n    return patterns\n\n\n_TYPE_TRANSFORMERS: dict[str, Callable[[str], _ArgumentTypes]] = {\n    \"choice\": str,\n    \"csv\": _csv_transformer,\n    \"float\": float,\n    \"int\": int,\n    \"confidence\": _confidence_transformer,\n    \"non_empty_string\": _non_empty_string_transformer,\n    \"path\": _path_transformer,\n    \"py_version\": _py_version_transformer,\n    \"regexp\": re.compile,\n    \"regexp_csv\": _regexp_csv_transfomer,\n    \"regexp_paths_csv\": _regexp_paths_csv_transfomer,\n    \"string\": pylint_utils._unquote,\n    \"yn\": _yn_transformer,\n}\n\"\"\"Type transformers for all argument types.\n\nA transformer should accept a string and return one of the supported\nArgument types. It will only be called when parsing 1) command-line,\n2) configuration files and 3) a string default value.\nNon-string default values are assumed to be of the correct type.\n\"\"\"\n\n\nclass _Argument:\n    \"\"\"Class representing an argument to be parsed by an argparse.ArgumentsParser.\n\n    This is based on the parameters passed to argparse.ArgumentsParser.add_message.\n    See:\n    https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser.add_argument\n    \"\"\"\n\n    def __init__(\n        self,\n(B)     return version\n\n\ndef _regexp_csv_transfomer(value: str) -> Sequence[Pattern[str]]:\n    \"\"\"Transforms a comma separated list of regular expressions.\"\"\"\n    patterns: list[Pattern[str]] = []\n    for pattern in _csv_transformer(value):\n        patterns.append(re.compile(pattern))\n    return patterns\n\n\ndef _regexp_paths_csv_transfomer(value: str) -> Sequence[Pattern[str]]:\n    \"\"\"Transforms a comma separated list of regular expressions paths.\"\"\"\n    patterns: list[Pattern[str]] = []\n    for pattern in _csv_transformer(value):\n        patterns.append(\n            re.compile(\n                str(pathlib.PureWindowsPath(pattern)).replace(\"\\\\\", \"\\\\\\\\\")\n                + \"|\"\n                + pathlib.PureWindowsPath(pattern).as_posix()\n            )\n        )\n    return patterns\n\n\n_TYPE_TRANSFORMERS: dict[str, Callable[[str], _ArgumentTypes]] = {\n    \"choice\": str,\n    \"csv\": _csv_transformer,\n    \"float\": float,\n    \"int\": int,\n    \"confidence\": _confidence_transformer,\n    \"non_empty_string\": _non_empty_string_transformer,\n    \"path\": _path_transformer,\n    \"py_version\": _py_version_transformer,\n    \"regexp\": re.compile,\n    \"regexp_csv\": _regexp_csv_transfomer,\n    \"regexp_paths_csv\": _regexp_paths_csv_transfomer,\n    \"string\": pylint_utils._unquote,\n    \"yn\": _yn_transformer,\n}\n\"\"\"Type transformers for all argument types.\n\nA transformer should accept a string and return one of the supported\nArgument types. It will only be called when parsing 1) command-line,\n2) configuration files and 3) a string default value.\nNon-string default values are assumed to be of the correct type.\n\"\"\"\n(C) \n_TYPE_TRANSFORMERS: dict[str, Callable[[str], _ArgumentTypes]] = {\n    \"choice\": str,\n    \"csv\": _csv_transformer,\n    \"float\": float,\n    \"int\": int,\n    \"confidence\": _confidence_transformer,\n    \"non_empty_string\": _non_empty_string_transformer,\n    \"path\": _path_transformer,\n    \"py_version\": _py_version_transformer,\n    \"regexp\": re.compile,\n    \"regexp_csv\": _regexp_csv_transfomer,\n    \"regexp_paths_csv\": _regexp_paths_csv_transfomer,\n    \"string\": pylint_utils._unquote,\n    \"yn\": _yn_transformer,\n}\n\"\"\"Type transformers for all argument types.\n\nA transformer should accept a string and return one of the supported\nArgument types. It will only be called when parsing 1) command-line,\n2) configuration files and 3) a string default value.\nNon-string default values are assumed to be of the correct type.\n\"\"\"\n\n\nclass _Argument:\n    \"\"\"Class representing an argument to be parsed by an argparse.ArgumentsParser.\n\n    This is based on the parameters passed to argparse.ArgumentsParser.add_message.\n    See:\n    https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser.add_argument\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        flags: list[str],\n        arg_help: str,\n        hide_help: bool,\n        section: str | None,\n    ) -> None:\n        self.flags = flags\n        \"\"\"The name of the argument.\"\"\"\n\n        self.hide_help = hide_help\n        \"\"\"Whether to hide this argument in the help message.\"\"\"\n\n(D)     \"\"\"Expand user and variables in a path.\"\"\"\n    return os.path.expandvars(os.path.expanduser(value))\n\n\ndef _py_version_transformer(value: str) -> tuple[int, ...]:\n    \"\"\"Transforms a version string into a version tuple.\"\"\"\n    try:\n        version = tuple(int(val) for val in value.replace(\",\", \".\").split(\".\"))\n    except ValueError:\n        raise argparse.ArgumentTypeError(\n            f\"{value} has an invalid format, should be a version string. E.g., '3.8'\"\n        ) from None\n    return version\n\n\ndef _regexp_csv_transfomer(value: str) -> Sequence[Pattern[str]]:\n    \"\"\"Transforms a comma separated list of regular expressions.\"\"\"\n    patterns: list[Pattern[str]] = []\n    for pattern in _csv_transformer(value):\n        patterns.append(re.compile(pattern))\n    return patterns\n\n\ndef _regexp_paths_csv_transfomer(value: str) -> Sequence[Pattern[str]]:\n    \"\"\"Transforms a comma separated list of regular expressions paths.\"\"\"\n    patterns: list[Pattern[str]] = []\n    for pattern in _csv_transformer(value):\n        patterns.append(\n            re.compile(\n                str(pathlib.PureWindowsPath(pattern)).replace(\"\\\\\", \"\\\\\\\\\")\n                + \"|\"\n                + pathlib.PureWindowsPath(pattern).as_posix()\n            )\n        )\n    return patterns\n\n\n_TYPE_TRANSFORMERS: dict[str, Callable[[str], _ArgumentTypes]] = {\n    \"choice\": str,\n    \"csv\": _csv_transformer,\n    \"float\": float,\n    \"int\": int,\n    \"confidence\": _confidence_transformer,\n    \"non_empty_string\": _non_empty_string_transformer,\n    \"path\": _path_transformer,\n    \"py_version\": _py_version_transformer,\n    \"regexp\": re.compile,", "answer": "B"}
{"uuid": "1b2bce90-108f-4496-929b-f3dd9d2a0c29", "issue_statement": "Using custom braces in message template does not work\n### Bug description\n\nHave any list of errors:\r\n\r\nOn pylint 1.7 w/ python3.6 - I am able to use this as my message template\r\n```\r\n$ pylint test.py --msg-template='{{ \"Category\": \"{category}\" }}'\r\nNo config file found, using default configuration\r\n************* Module [redacted].test\r\n{ \"Category\": \"convention\" }\r\n{ \"Category\": \"error\" }\r\n{ \"Category\": \"error\" }\r\n{ \"Category\": \"convention\" }\r\n{ \"Category\": \"convention\" }\r\n{ \"Category\": \"convention\" }\r\n{ \"Category\": \"error\" }\r\n```\r\n\r\nHowever, on Python3.9 with Pylint 2.12.2, I get the following:\r\n```\r\n$ pylint test.py --msg-template='{{ \"Category\": \"{category}\" }}'\r\n[redacted]/site-packages/pylint/reporters/text.py:206: UserWarning: Don't recognize the argument '{ \"Category\"' in the --msg-template. Are you sure it is supported on the current version of pylint?\r\n  warnings.warn(\r\n************* Module [redacted].test\r\n\" }\r\n\" }\r\n\" }\r\n\" }\r\n\" }\r\n\" }\r\n```\r\n\r\nIs this intentional or a bug?\n\n### Configuration\n\n_No response_\n\n### Command used\n\n```shell\npylint test.py --msg-template='{{ \"Category\": \"{category}\" }}'\n```\n\n\n### Pylint output\n\n```shell\n[redacted]/site-packages/pylint/reporters/text.py:206: UserWarning: Don't recognize the argument '{ \"Category\"' in the --msg-template. Are you sure it is supported on the current version of pylint?\r\n  warnings.warn(\r\n************* Module [redacted].test\r\n\" }\r\n\" }\r\n\" }\r\n\" }\r\n\" }\r\n\" }\n```\n\n\n### Expected behavior\n\nExpect the dictionary to print out with `\"Category\"` as the key.\n\n### Pylint version\n\n```shell\nAffected Version:\r\npylint 2.12.2\r\nastroid 2.9.2\r\nPython 3.9.9+ (heads/3.9-dirty:a2295a4, Dec 21 2021, 22:32:52) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]\r\n\r\n\r\nPreviously working version:\r\nNo config file found, using default configuration\r\npylint 1.7.4, \r\nastroid 1.6.6\r\nPython 3.6.8 (default, Nov 16 2020, 16:55:22) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]\n```\n\n\n### OS / Environment\n\n_No response_\n\n### Additional dependencies\n\n_No response_", "choices": "(A) \n        # Set template to the currently selected template\n        self._template = template\n\n        # Check to see if all parameters in the template are attributes of the Message\n        arguments = re.findall(r\"\\{(.+?)(:.*)?\\}\", template)\n        for argument in arguments:\n(B)         self._template = template\n\n        # Check to see if all parameters in the template are attributes of the Message\n        arguments = re.findall(r\"\\{(.+?)(:.*)?\\}\", template)\n        for argument in arguments:\n            if argument[0] not in MESSAGE_FIELDS:\n                warnings.warn(\n(C)         # Check to see if all parameters in the template are attributes of the Message\n        arguments = re.findall(r\"\\{(.+?)(:.*)?\\}\", template)\n        for argument in arguments:\n            if argument[0] not in MESSAGE_FIELDS:\n                warnings.warn(\n                    f\"Don't recognize the argument '{argument[0]}' in the --msg-template. \"\n                    \"Are you sure it is supported on the current version of pylint?\"\n(D)         for argument in arguments:\n            if argument[0] not in MESSAGE_FIELDS:\n                warnings.warn(\n                    f\"Don't recognize the argument '{argument[0]}' in the --msg-template. \"\n                    \"Are you sure it is supported on the current version of pylint?\"\n                )\n                template = re.sub(r\"\\{\" + argument[0] + r\"(:.*?)?\\}\", \"\", template)", "answer": "B"}
{"uuid": "2f510504-9bba-4e5e-b73c-a254cdafb4cb", "issue_statement": "Rewrite fails when first expression of file is a number and mistaken as docstring \n<!--\r\nThanks for submitting an issue!\r\n\r\nQuick check-list while reporting bugs:\r\n-->\r\n\r\n- [x] a detailed description of the bug or problem you are having\r\n- [x] output of `pip list` from the virtual environment you are using\r\n- [x] pytest and operating system versions\r\n- [x] minimal example if possible\r\n```\r\nInstalling collected packages: zipp, six, PyYAML, python-dateutil, MarkupSafe, importlib-metadata, watchdog, tomli, soupsieve, pyyaml-env-tag, pycparser, pluggy, packaging, mergedeep, Markdown, jinja2, iniconfig, ghp-import, exceptiongroup, click, websockets, urllib3, tqdm, smmap, pytest, pyee, mkdocs, lxml, importlib-resources, idna, cssselect, charset-normalizer, cffi, certifi, beautifulsoup4, attrs, appdirs, w3lib, typing-extensions, texttable, requests, pyzstd, pytest-metadata, pyquery, pyppmd, pyppeteer, pynacl, pymdown-extensions, pycryptodomex, pybcj, pyasn1, py, psutil, parse, multivolumefile, mkdocs-autorefs, inflate64, gitdb, fake-useragent, cryptography, comtypes, bs4, brotli, bcrypt, allure-python-commons, xlwt, xlrd, rsa, requests-html, pywinauto, python-i18n, python-dotenv, pytest-rerunfailures, pytest-html, pytest-check, PySocks, py7zr, paramiko, mkdocstrings, loguru, GitPython, ftputil, crcmod, chardet, brotlicffi, allure-pytest\r\nSuccessfully installed GitPython-3.1.31 Markdown-3.3.7 MarkupSafe-2.1.3 PySocks-1.7.1 PyYAML-6.0 allure-pytest-2.13.2 allure-python-commons-2.13.2 appdirs-1.4.4 attrs-23.1.0 bcrypt-4.0.1 beautifulsoup4-4.12.2 brotli-1.0.9 brotlicffi-1.0.9.2 bs4-0.0.1 certifi-2023.5.7 cffi-1.15.1 chardet-5.1.0 charset-normalizer-3.1.0 click-8.1.3 comtypes-1.2.0 crcmod-1.7 cryptography-41.0.1 cssselect-1.2.0 exceptiongroup-1.1.1 fake-useragent-1.1.3 ftputil-5.0.4 ghp-import-2.1.0 gitdb-4.0.10 idna-3.4 importlib-metadata-6.7.0 importlib-resources-5.12.0 inflate64-0.3.1 iniconfig-2.0.0 jinja2-3.1.2 loguru-0.7.0 lxml-4.9.2 mergedeep-1.3.4 mkdocs-1.4.3 mkdocs-autorefs-0.4.1 mkdocstrings-0.22.0 multivolumefile-0.2.3 packaging-23.1 paramiko-3.2.0 parse-1.19.1 pluggy-1.2.0 psutil-5.9.5 py-1.11.0 py7zr-0.20.5 pyasn1-0.5.0 pybcj-1.0.1 pycparser-2.21 pycryptodomex-3.18.0 pyee-8.2.2 pymdown-extensions-10.0.1 pynacl-1.5.0 pyppeteer-1.0.2 pyppmd-1.0.0 pyquery-2.0.0 pytest-7.4.0 pytest-check-2.1.5 pytest-html-3.2.0 pytest-metadata-3.0.0 pytest-rerunfailures-11.1.2 python-dateutil-2.8.2 python-dotenv-1.0.0 python-i18n-0.3.9 pywinauto-0.6.6 pyyaml-env-tag-0.1 pyzstd-0.15.9 requests-2.31.0 requests-html-0.10.0 rsa-4.9 six-1.16.0 smmap-5.0.0 soupsieve-2.4.1 texttable-1.6.7 tomli-2.0.1 tqdm-4.65.0 typing-extensions-4.6.3 urllib3-1.26.16 w3lib-2.1.1 watchdog-3.0.0 websockets-10.4 xlrd-2.0.1 xlwt-1.3.0 zipp-3.15.0\r\n```\r\nuse `pytest -k xxx`\uff0c report an error\uff1a`TypeError: argument of type 'int' is not iterable`\r\n\r\nit seems a error in collecting testcase\r\n```\r\n==================================== ERRORS ====================================\r\n_ ERROR collecting testcases/\u57fa\u7ebf/\u4ee3\u7406\u7b56\u7565/SOCKS\u4e8c\u7ea7\u4ee3\u7406\u8fed\u4ee3\u4e8c/\u5728\u7ebf\u7528\u6237/\u5728\u7ebf\u7528\u6237\u66f4\u65b0/\u4e0a\u7ebf\u7528\u6237/test_socks_user_011.py _\r\n/usr/local/lib/python3.8/site-packages/_pytest/runner.py:341: in from_call\r\n    result: Optional[TResult] = func()\r\n/usr/local/lib/python3.8/site-packages/_pytest/runner.py:372: in <lambda>\r\n    call = CallInfo.from_call(lambda: list(collector.collect()), \"collect\")\r\n/usr/local/lib/python3.8/site-packages/_pytest/python.py:531: in collect\r\n    self._inject_setup_module_fixture()\r\n/usr/local/lib/python3.8/site-packages/_pytest/python.py:545: in _inject_setup_module_fixture\r\n    self.obj, (\"setUpModule\", \"setup_module\")\r\n/usr/local/lib/python3.8/site-packages/_pytest/python.py:310: in obj\r\n    self._obj = obj = self._getobj()\r\n/usr/local/lib/python3.8/site-packages/_pytest/python.py:528: in _getobj\r\n    return self._importtestmodule()\r\n/usr/local/lib/python3.8/site-packages/_pytest/python.py:617: in _importtestmodule\r\n    mod = import_path(self.path, mode=importmode, root=self.config.rootpath)\r\n/usr/local/lib/python3.8/site-packages/_pytest/pathlib.py:565: in import_path\r\n    importlib.import_module(module_name)\r\n/usr/local/lib/python3.8/importlib/__init__.py:127: in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n<frozen importlib._bootstrap>:1014: in _gcd_import\r\n    ???\r\n<frozen importlib._bootstrap>:991: in _find_and_load\r\n    ???\r\n<frozen importlib._bootstrap>:975: in _find_and_load_unlocked\r\n    ???\r\n<frozen importlib._bootstrap>:671: in _load_unlocked\r\n    ???\r\n/usr/local/lib/python3.8/site-packages/_pytest/assertion/rewrite.py:169: in exec_module\r\n    source_stat, co = _rewrite_test(fn, self.config)\r\n/usr/local/lib/python3.8/site-packages/_pytest/assertion/rewrite.py:352: in _rewrite_test\r\n    rewrite_asserts(tree, source, strfn, config)\r\n/usr/local/lib/python3.8/site-packages/_pytest/assertion/rewrite.py:413: in rewrite_asserts\r\n    AssertionRewriter(module_path, config, source).run(mod)\r\n/usr/local/lib/python3.8/site-packages/_pytest/assertion/rewrite.py:695: in run\r\n    if self.is_rewrite_disabled(doc):\r\n/usr/local/lib/python3.8/site-packages/_pytest/assertion/rewrite.py:760: in is_rewrite_disabled\r\n    return \"PYTEST_DONT_REWRITE\" in docstring\r\nE   TypeError: argument of type 'int' is not iterable\r\n```", "choices": "(A)         for item in mod.body:\n            if (\n                expect_docstring\n                and isinstance(item, ast.Expr)\n                and isinstance(item.value, ast.Constant)\n            ):\n                doc = item.value.value\n(B)                 doc = item.value.value\n                if self.is_rewrite_disabled(doc):\n                    return\n                expect_docstring = False\n            elif (\n                isinstance(item, ast.ImportFrom)\n                and item.level == 0\n(C)                 and isinstance(item.value, ast.Constant)\n            ):\n                doc = item.value.value\n                if self.is_rewrite_disabled(doc):\n                    return\n                expect_docstring = False\n            elif (\n(D)                 expect_docstring\n                and isinstance(item, ast.Expr)\n                and isinstance(item.value, ast.Constant)\n            ):\n                doc = item.value.value\n                if self.is_rewrite_disabled(doc):\n                    return", "answer": "D"}
{"uuid": "104a81ca-2342-4e0b-98c1-9022abc0f331", "issue_statement": "Module imported twice under import-mode=importlib\nIn pmxbot/pmxbot@7f189ad, I'm attempting to switch pmxbot off of pkg_resources style namespace packaging to PEP 420 namespace packages. To do so, I've needed to switch to `importlib` for the `import-mode` and re-organize the tests to avoid import errors on the tests.\r\n\r\nYet even after working around these issues, the tests are failing when the effect of `core.initialize()` doesn't seem to have had any effect.\r\n\r\nInvestigating deeper, I see that initializer is executed and performs its actions (setting a class variable `pmxbot.logging.Logger.store`), but when that happens, there are two different versions of `pmxbot.logging` present, one in `sys.modules` and another found in `tests.unit.test_commands.logging`:\r\n\r\n```\r\n=========================================================================== test session starts ===========================================================================\r\nplatform darwin -- Python 3.11.1, pytest-7.2.0, pluggy-1.0.0\r\ncachedir: .tox/python/.pytest_cache\r\nrootdir: /Users/jaraco/code/pmxbot/pmxbot, configfile: pytest.ini\r\nplugins: black-0.3.12, mypy-0.10.3, jaraco.test-5.3.0, checkdocs-2.9.0, flake8-1.1.1, enabler-2.0.0, jaraco.mongodb-11.2.1, pmxbot-1122.14.3.dev13+g7f189ad\r\ncollected 421 items / 180 deselected / 241 selected                                                                                                                       \r\nrun-last-failure: rerun previous 240 failures (skipped 14 files)\r\n\r\ntests/unit/test_commands.py E\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n\r\ncls = <class 'tests.unit.test_commands.TestCommands'>\r\n\r\n    @classmethod\r\n    def setup_class(cls):\r\n        path = os.path.dirname(os.path.abspath(__file__))\r\n        configfile = os.path.join(path, 'testconf.yaml')\r\n        config = pmxbot.dictlib.ConfigDict.from_yaml(configfile)\r\n        cls.bot = core.initialize(config)\r\n>       logging.Logger.store.message(\"logged\", \"testrunner\", \"some text\")\r\nE       AttributeError: type object 'Logger' has no attribute 'store'\r\n\r\ntests/unit/test_commands.py:37: AttributeError\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> PDB post_mortem (IO-capturing turned off) >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n> /Users/jaraco/code/pmxbot/pmxbot/tests/unit/test_commands.py(37)setup_class()\r\n-> logging.Logger.store.message(\"logged\", \"testrunner\", \"some text\")\r\n(Pdb) logging.Logger\r\n<class 'pmxbot.logging.Logger'>\r\n(Pdb) logging\r\n<module 'pmxbot.logging' from '/Users/jaraco/code/pmxbot/pmxbot/pmxbot/logging.py'>\r\n(Pdb) import sys\r\n(Pdb) sys.modules['pmxbot.logging']\r\n<module 'pmxbot.logging' from '/Users/jaraco/code/pmxbot/pmxbot/pmxbot/logging.py'>\r\n(Pdb) sys.modules['pmxbot.logging'] is logging\r\nFalse\r\n```\r\n\r\nI haven't yet made a minimal reproducer, but I wanted to first capture this condition.", "choices": "(A)         for meta_importer in sys.meta_path:\n            spec = meta_importer.find_spec(module_name, [str(path.parent)])\n            if spec is not None:\n                break\n        else:\n            spec = importlib.util.spec_from_file_location(module_name, str(path))\n\n        if spec is None:\n(B)     if not path.exists():\n        raise ImportError(path)\n\n    if mode is ImportMode.importlib:\n        module_name = module_name_from_path(path, root)\n\n        for meta_importer in sys.meta_path:\n            spec = meta_importer.find_spec(module_name, [str(path.parent)])\n(C)         module_name = module_name_from_path(path, root)\n\n        for meta_importer in sys.meta_path:\n            spec = meta_importer.find_spec(module_name, [str(path.parent)])\n            if spec is not None:\n                break\n        else:\n            spec = importlib.util.spec_from_file_location(module_name, str(path))\n(D) \n    if mode is ImportMode.importlib:\n        module_name = module_name_from_path(path, root)\n\n        for meta_importer in sys.meta_path:\n            spec = meta_importer.find_spec(module_name, [str(path.parent)])\n            if spec is not None:\n                break", "answer": "D"}
{"uuid": "31421302-efad-4e38-be56-f60ae3d80324", "issue_statement": "Unroll the iterable for all/any calls to get better reports\nSometime I need to assert some predicate on all of an iterable, and for that the builtin functions `all`/`any` are great - but the failure messages aren't useful at all!\r\nFor example - the same test written in three ways:\r\n\r\n- A generator expression\r\n```sh                                                                                                                                                                                                                         \r\n    def test_all_even():\r\n        even_stevens = list(range(1,100,2))\r\n>       assert all(is_even(number) for number in even_stevens)\r\nE       assert False\r\nE        +  where False = all(<generator object test_all_even.<locals>.<genexpr> at 0x101f82ed0>)\r\n```\r\n- A list comprehension\r\n```sh\r\n    def test_all_even():\r\n        even_stevens = list(range(1,100,2))\r\n>       assert all([is_even(number) for number in even_stevens])\r\nE       assert False\r\nE        +  where False = all([False, False, False, False, False, False, ...])\r\n```\r\n- A for loop\r\n```sh\r\n    def test_all_even():\r\n        even_stevens = list(range(1,100,2))\r\n        for number in even_stevens:\r\n>           assert is_even(number)\r\nE           assert False\r\nE            +  where False = is_even(1)\r\n\r\ntest_all_any.py:7: AssertionError\r\n```\r\nThe only one that gives a meaningful report is the for loop - but it's way more wordy, and `all` asserts don't translate to a for loop nicely (I'll have to write a `break` or a helper function - yuck)\r\nI propose the assertion re-writer \"unrolls\" the iterator to the third form, and then uses the already existing reports.\r\n\r\n- [x] Include a detailed description of the bug or suggestion\r\n- [x] `pip list` of the virtual environment you are using\r\n```\r\nPackage        Version\r\n-------------- -------\r\natomicwrites   1.3.0  \r\nattrs          19.1.0 \r\nmore-itertools 7.0.0  \r\npip            19.0.3 \r\npluggy         0.9.0  \r\npy             1.8.0  \r\npytest         4.4.0  \r\nsetuptools     40.8.0 \r\nsix            1.12.0 \r\n```\r\n- [x] pytest and operating system versions\r\n`platform darwin -- Python 3.7.3, pytest-4.4.0, py-1.8.0, pluggy-0.9.0`\r\n- [x] Minimal example if possible", "choices": "(A)         self.on_failure = fail_save\n        expl_template = self.helper(\"_format_boolop\", expl_list, ast.Num(is_or))\n        expl = self.pop_format_context(expl_template)\n        return ast.Name(res_var, ast.Load()), self.explanation_param(expl)\n\n    def visit_UnaryOp(self, unary):\n        pattern = unary_map[unary.op.__class__]\n        operand_res, operand_expl = self.visit(unary.operand)\n        res = self.assign(ast.UnaryOp(unary.op, operand_res))\n        return res, pattern % (operand_expl,)\n\n    def visit_BinOp(self, binop):\n        symbol = binop_map[binop.op.__class__]\n        left_expr, left_expl = self.visit(binop.left)\n        right_expr, right_expl = self.visit(binop.right)\n        explanation = \"(%s %s %s)\" % (left_expl, symbol, right_expl)\n        res = self.assign(ast.BinOp(left_expr, binop.op, right_expr))\n        return res, explanation\n\n    def visit_Call_35(self, call):\n        \"\"\"\n        visit `ast.Call` nodes on Python3.5 and after\n        \"\"\"\n        new_func, func_expl = self.visit(call.func)\n        arg_expls = []\n        new_args = []\n        new_kwargs = []\n        for arg in call.args:\n            res, expl = self.visit(arg)\n            arg_expls.append(expl)\n            new_args.append(res)\n        for keyword in call.keywords:\n            res, expl = self.visit(keyword.value)\n            new_kwargs.append(ast.keyword(keyword.arg, res))\n            if keyword.arg:\n                arg_expls.append(keyword.arg + \"=\" + expl)\n            else:  # **args have `arg` keywords with an .arg of None\n                arg_expls.append(\"**\" + expl)\n\n        expl = \"%s(%s)\" % (func_expl, \", \".join(arg_expls))\n        new_call = ast.Call(new_func, new_args, new_kwargs)\n        res = self.assign(new_call)\n        res_expl = self.explanation_param(self.display(res))\n        outer_expl = \"%s\\n{%s = %s\\n}\" % (res_expl, res_expl, expl)\n        return res, outer_expl\n\n    def visit_Starred(self, starred):\n        # From Python 3.5, a Starred node can appear in a function call\n        res, expl = self.visit(starred.value)\n        new_starred = ast.Starred(res, starred.ctx)\n        return new_starred, \"*\" + expl\n\n    def visit_Call_legacy(self, call):\n        \"\"\"\n        visit `ast.Call nodes on 3.4 and below`\n        \"\"\"\n        new_func, func_expl = self.visit(call.func)\n        arg_expls = []\n        new_args = []\n        new_kwargs = []\n        new_star = new_kwarg = None\n        for arg in call.args:\n            res, expl = self.visit(arg)\n            new_args.append(res)\n(B)             res, expl = self.visit(keyword.value)\n            new_kwargs.append(ast.keyword(keyword.arg, res))\n            if keyword.arg:\n                arg_expls.append(keyword.arg + \"=\" + expl)\n            else:  # **args have `arg` keywords with an .arg of None\n                arg_expls.append(\"**\" + expl)\n\n        expl = \"%s(%s)\" % (func_expl, \", \".join(arg_expls))\n        new_call = ast.Call(new_func, new_args, new_kwargs)\n        res = self.assign(new_call)\n        res_expl = self.explanation_param(self.display(res))\n        outer_expl = \"%s\\n{%s = %s\\n}\" % (res_expl, res_expl, expl)\n        return res, outer_expl\n\n    def visit_Starred(self, starred):\n        # From Python 3.5, a Starred node can appear in a function call\n        res, expl = self.visit(starred.value)\n        new_starred = ast.Starred(res, starred.ctx)\n        return new_starred, \"*\" + expl\n\n    def visit_Call_legacy(self, call):\n        \"\"\"\n        visit `ast.Call nodes on 3.4 and below`\n        \"\"\"\n        new_func, func_expl = self.visit(call.func)\n        arg_expls = []\n        new_args = []\n        new_kwargs = []\n        new_star = new_kwarg = None\n        for arg in call.args:\n            res, expl = self.visit(arg)\n            new_args.append(res)\n            arg_expls.append(expl)\n        for keyword in call.keywords:\n            res, expl = self.visit(keyword.value)\n            new_kwargs.append(ast.keyword(keyword.arg, res))\n            arg_expls.append(keyword.arg + \"=\" + expl)\n        if call.starargs:\n            new_star, expl = self.visit(call.starargs)\n            arg_expls.append(\"*\" + expl)\n        if call.kwargs:\n            new_kwarg, expl = self.visit(call.kwargs)\n            arg_expls.append(\"**\" + expl)\n        expl = \"%s(%s)\" % (func_expl, \", \".join(arg_expls))\n        new_call = ast.Call(new_func, new_args, new_kwargs, new_star, new_kwarg)\n        res = self.assign(new_call)\n        res_expl = self.explanation_param(self.display(res))\n        outer_expl = \"%s\\n{%s = %s\\n}\" % (res_expl, res_expl, expl)\n        return res, outer_expl\n\n    # ast.Call signature changed on 3.5,\n    # conditionally change  which methods is named\n    # visit_Call depending on Python version\n    if sys.version_info >= (3, 5):\n        visit_Call = visit_Call_35\n    else:\n        visit_Call = visit_Call_legacy\n\n    def visit_Attribute(self, attr):\n        if not isinstance(attr.ctx, ast.Load):\n            return self.generic_visit(attr)\n        value, value_expl = self.visit(attr.value)\n        res = self.assign(ast.Attribute(value, attr.attr, ast.Load()))\n        res_expl = self.explanation_param(self.display(res))\n(C)         res, expl = self.visit(starred.value)\n        new_starred = ast.Starred(res, starred.ctx)\n        return new_starred, \"*\" + expl\n\n    def visit_Call_legacy(self, call):\n        \"\"\"\n        visit `ast.Call nodes on 3.4 and below`\n        \"\"\"\n        new_func, func_expl = self.visit(call.func)\n        arg_expls = []\n        new_args = []\n        new_kwargs = []\n        new_star = new_kwarg = None\n        for arg in call.args:\n            res, expl = self.visit(arg)\n            new_args.append(res)\n            arg_expls.append(expl)\n        for keyword in call.keywords:\n            res, expl = self.visit(keyword.value)\n            new_kwargs.append(ast.keyword(keyword.arg, res))\n            arg_expls.append(keyword.arg + \"=\" + expl)\n        if call.starargs:\n            new_star, expl = self.visit(call.starargs)\n            arg_expls.append(\"*\" + expl)\n        if call.kwargs:\n            new_kwarg, expl = self.visit(call.kwargs)\n            arg_expls.append(\"**\" + expl)\n        expl = \"%s(%s)\" % (func_expl, \", \".join(arg_expls))\n        new_call = ast.Call(new_func, new_args, new_kwargs, new_star, new_kwarg)\n        res = self.assign(new_call)\n        res_expl = self.explanation_param(self.display(res))\n        outer_expl = \"%s\\n{%s = %s\\n}\" % (res_expl, res_expl, expl)\n        return res, outer_expl\n\n    # ast.Call signature changed on 3.5,\n    # conditionally change  which methods is named\n    # visit_Call depending on Python version\n    if sys.version_info >= (3, 5):\n        visit_Call = visit_Call_35\n    else:\n        visit_Call = visit_Call_legacy\n\n    def visit_Attribute(self, attr):\n        if not isinstance(attr.ctx, ast.Load):\n            return self.generic_visit(attr)\n        value, value_expl = self.visit(attr.value)\n        res = self.assign(ast.Attribute(value, attr.attr, ast.Load()))\n        res_expl = self.explanation_param(self.display(res))\n        pat = \"%s\\n{%s = %s.%s\\n}\"\n        expl = pat % (res_expl, res_expl, value_expl, attr.attr)\n        return res, expl\n\n    def visit_Compare(self, comp):\n        self.push_format_context()\n        left_res, left_expl = self.visit(comp.left)\n        if isinstance(comp.left, (ast.Compare, ast.BoolOp)):\n            left_expl = \"({})\".format(left_expl)\n        res_variables = [self.variable() for i in range(len(comp.ops))]\n        load_names = [ast.Name(v, ast.Load()) for v in res_variables]\n        store_names = [ast.Name(v, ast.Store()) for v in res_variables]\n        it = zip(range(len(comp.ops)), comp.ops, comp.comparators)\n        expls = []\n        syms = []\n        results = [left_res]\n(D)         res = self.assign(ast.BinOp(left_expr, binop.op, right_expr))\n        return res, explanation\n\n    def visit_Call_35(self, call):\n        \"\"\"\n        visit `ast.Call` nodes on Python3.5 and after\n        \"\"\"\n        new_func, func_expl = self.visit(call.func)\n        arg_expls = []\n        new_args = []\n        new_kwargs = []\n        for arg in call.args:\n            res, expl = self.visit(arg)\n            arg_expls.append(expl)\n            new_args.append(res)\n        for keyword in call.keywords:\n            res, expl = self.visit(keyword.value)\n            new_kwargs.append(ast.keyword(keyword.arg, res))\n            if keyword.arg:\n                arg_expls.append(keyword.arg + \"=\" + expl)\n            else:  # **args have `arg` keywords with an .arg of None\n                arg_expls.append(\"**\" + expl)\n\n        expl = \"%s(%s)\" % (func_expl, \", \".join(arg_expls))\n        new_call = ast.Call(new_func, new_args, new_kwargs)\n        res = self.assign(new_call)\n        res_expl = self.explanation_param(self.display(res))\n        outer_expl = \"%s\\n{%s = %s\\n}\" % (res_expl, res_expl, expl)\n        return res, outer_expl\n\n    def visit_Starred(self, starred):\n        # From Python 3.5, a Starred node can appear in a function call\n        res, expl = self.visit(starred.value)\n        new_starred = ast.Starred(res, starred.ctx)\n        return new_starred, \"*\" + expl\n\n    def visit_Call_legacy(self, call):\n        \"\"\"\n        visit `ast.Call nodes on 3.4 and below`\n        \"\"\"\n        new_func, func_expl = self.visit(call.func)\n        arg_expls = []\n        new_args = []\n        new_kwargs = []\n        new_star = new_kwarg = None\n        for arg in call.args:\n            res, expl = self.visit(arg)\n            new_args.append(res)\n            arg_expls.append(expl)\n        for keyword in call.keywords:\n            res, expl = self.visit(keyword.value)\n            new_kwargs.append(ast.keyword(keyword.arg, res))\n            arg_expls.append(keyword.arg + \"=\" + expl)\n        if call.starargs:\n            new_star, expl = self.visit(call.starargs)\n            arg_expls.append(\"*\" + expl)\n        if call.kwargs:\n            new_kwarg, expl = self.visit(call.kwargs)\n            arg_expls.append(\"**\" + expl)\n        expl = \"%s(%s)\" % (func_expl, \", \".join(arg_expls))\n        new_call = ast.Call(new_func, new_args, new_kwargs, new_star, new_kwarg)\n        res = self.assign(new_call)\n        res_expl = self.explanation_param(self.display(res))\n        outer_expl = \"%s\\n{%s = %s\\n}\" % (res_expl, res_expl, expl)", "answer": "D"}
{"uuid": "ae2c0ab3-3b59-45f8-9b63-9ce0e4a1e55f", "issue_statement": "Display fixture scope with `pytest --fixtures`\nIt would be useful to show fixture scopes with `pytest --fixtures`; currently the only way to learn the scope of a fixture is look at the docs (when that is documented) or at the source code.", "choices": "(A)         else:\n            funcargspec = argname\n        tw.line(funcargspec, green=True)\n        loc = getlocation(fixturedef.func, curdir)\n        doc = fixturedef.func.__doc__ or \"\"\n        if doc:\n            write_docstring(tw, doc)\n        else:\n            tw.line(\"    %s: no docstring available\" % (loc,), red=True)\n\n\ndef write_docstring(tw, doc, indent=\"    \"):\n    doc = doc.rstrip()\n    if \"\\n\" in doc:\n        firstline, rest = doc.split(\"\\n\", 1)\n    else:\n        firstline, rest = doc, \"\"\n\n    if firstline.strip():\n(B)     for baseid, module, bestrel, argname, fixturedef in available:\n        if currentmodule != module:\n            if not module.startswith(\"_pytest.\"):\n                tw.line()\n                tw.sep(\"-\", \"fixtures defined from %s\" % (module,))\n                currentmodule = module\n        if verbose <= 0 and argname[0] == \"_\":\n            continue\n        if verbose > 0:\n            funcargspec = \"%s -- %s\" % (argname, bestrel)\n        else:\n            funcargspec = argname\n        tw.line(funcargspec, green=True)\n        loc = getlocation(fixturedef.func, curdir)\n        doc = fixturedef.func.__doc__ or \"\"\n        if doc:\n            write_docstring(tw, doc)\n        else:\n            tw.line(\"    %s: no docstring available\" % (loc,), red=True)\n(C)                 currentmodule = module\n        if verbose <= 0 and argname[0] == \"_\":\n            continue\n        if verbose > 0:\n            funcargspec = \"%s -- %s\" % (argname, bestrel)\n        else:\n            funcargspec = argname\n        tw.line(funcargspec, green=True)\n        loc = getlocation(fixturedef.func, curdir)\n        doc = fixturedef.func.__doc__ or \"\"\n        if doc:\n            write_docstring(tw, doc)\n        else:\n            tw.line(\"    %s: no docstring available\" % (loc,), red=True)\n\n\ndef write_docstring(tw, doc, indent=\"    \"):\n    doc = doc.rstrip()\n    if \"\\n\" in doc:\n(D)         if doc:\n            write_docstring(tw, doc)\n        else:\n            tw.line(\"    %s: no docstring available\" % (loc,), red=True)\n\n\ndef write_docstring(tw, doc, indent=\"    \"):\n    doc = doc.rstrip()\n    if \"\\n\" in doc:\n        firstline, rest = doc.split(\"\\n\", 1)\n    else:\n        firstline, rest = doc, \"\"\n\n    if firstline.strip():\n        tw.line(indent + firstline.strip())\n\n    if rest:\n        for line in dedent(rest).split(\"\\n\"):\n            tw.write(indent + line + \"\\n\")", "answer": "C"}
{"uuid": "3fa16fe1-7bb6-49a2-922c-1e24078c8c19", "issue_statement": "Improve default logging format\nCurrently it is:\r\n\r\n> DEFAULT_LOG_FORMAT = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\r\n\r\nI think `name` (module name) would be very useful here, instead of just the base filename.\r\n\r\n(It might also be good to have the relative path there (maybe at the end), but it is usually still very long (but e.g. `$VIRTUAL_ENV` could be substituted therein))\r\n\r\nCurrently it would look like this:\r\n```\r\nutils.py                   114 DEBUG    (0.000) SELECT \"app_url\".\"id\", \"app_url\".\"created\", \"app_url\".\"url\" FROM \"app_url\" WHERE \"app_url\".\"id\" = 2; args=(2,)\r\nmultipart.py               604 DEBUG    Calling on_field_start with no data\r\n```\r\n\r\n\r\nUsing `DEFAULT_LOG_FORMAT = \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\"` instead:\r\n\r\n```\r\nDEBUG    django.db.backends:utils.py:114 (0.000) SELECT \"app_url\".\"id\", \"app_url\".\"created\", \"app_url\".\"url\" FROM \"app_url\" WHERE \"app_url\".\"id\" = 2; args=(2,)\r\nDEBUG    multipart.multipart:multipart.py:604 Calling on_field_start with no data\r\n```", "choices": "(A) import pytest\nfrom _pytest.compat import dummy_context_manager\nfrom _pytest.config import create_terminal_writer\nfrom _pytest.pathlib import Path\n\nDEFAULT_LOG_FORMAT = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\nDEFAULT_LOG_DATE_FORMAT = \"%H:%M:%S\"\n(B) from _pytest.config import create_terminal_writer\nfrom _pytest.pathlib import Path\n\nDEFAULT_LOG_FORMAT = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\nDEFAULT_LOG_DATE_FORMAT = \"%H:%M:%S\"\n\n\n(C) \nDEFAULT_LOG_FORMAT = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\nDEFAULT_LOG_DATE_FORMAT = \"%H:%M:%S\"\n\n\nclass ColoredLevelFormatter(logging.Formatter):\n    \"\"\"\n(D) DEFAULT_LOG_DATE_FORMAT = \"%H:%M:%S\"\n\n\nclass ColoredLevelFormatter(logging.Formatter):\n    \"\"\"\n    Colorize the %(levelname)..s part of the log format passed to __init__.\n    \"\"\"", "answer": "B"}
{"uuid": "20767948-3565-4980-ac1c-a688dab2cbae", "issue_statement": "str() on the pytest.raises context variable doesn't behave same as normal exception catch\nPytest 4.6.2, macOS 10.14.5\r\n\r\n```Python\r\ntry:\r\n    raise LookupError(\r\n        f\"A\\n\"\r\n        f\"B\\n\"\r\n        f\"C\"\r\n    )\r\nexcept LookupError as e:\r\n    print(str(e))\r\n```\r\nprints\r\n\r\n> A\r\n> B\r\n> C\r\n\r\nBut\r\n\r\n```Python\r\nwith pytest.raises(LookupError) as e:\r\n    raise LookupError(\r\n        f\"A\\n\"\r\n        f\"B\\n\"\r\n        f\"C\"\r\n    )\r\n\r\nprint(str(e))\r\n```\r\n\r\nprints\r\n\r\n> <console>:3: LookupError: A\r\n\r\nIn order to get the full error message, one must do `str(e.value)`, which is documented, but this is a different interaction. Any chance the behavior could be changed to eliminate this gotcha?\r\n\r\n-----\r\n\r\nPip list gives\r\n\r\n```\r\nPackage            Version  Location\r\n------------------ -------- ------------------------------------------------------\r\napipkg             1.5\r\nasn1crypto         0.24.0\r\natomicwrites       1.3.0\r\nattrs              19.1.0\r\naws-xray-sdk       0.95\r\nboto               2.49.0\r\nboto3              1.9.51\r\nbotocore           1.12.144\r\ncertifi            2019.3.9\r\ncffi               1.12.3\r\nchardet            3.0.4\r\nClick              7.0\r\ncodacy-coverage    1.3.11\r\ncolorama           0.4.1\r\ncoverage           4.5.3\r\ncryptography       2.6.1\r\ndecorator          4.4.0\r\ndocker             3.7.2\r\ndocker-pycreds     0.4.0\r\ndocutils           0.14\r\necdsa              0.13.2\r\nexecnet            1.6.0\r\nfuture             0.17.1\r\nidna               2.8\r\nimportlib-metadata 0.17\r\nipaddress          1.0.22\r\nJinja2             2.10.1\r\njmespath           0.9.4\r\njsondiff           1.1.1\r\njsonpickle         1.1\r\njsonschema         2.6.0\r\nMarkupSafe         1.1.1\r\nmock               3.0.4\r\nmore-itertools     7.0.0\r\nmoto               1.3.7\r\nneobolt            1.7.10\r\nneotime            1.7.4\r\nnetworkx           2.1\r\nnumpy              1.15.0\r\npackaging          19.0\r\npandas             0.24.2\r\npip                19.1.1\r\npluggy             0.12.0\r\nprompt-toolkit     2.0.9\r\npy                 1.8.0\r\npy2neo             4.2.0\r\npyaml              19.4.1\r\npycodestyle        2.5.0\r\npycparser          2.19\r\npycryptodome       3.8.1\r\nPygments           2.3.1\r\npyOpenSSL          19.0.0\r\npyparsing          2.4.0\r\npytest             4.6.2\r\npytest-cache       1.0\r\npytest-codestyle   1.4.0\r\npytest-cov         2.6.1\r\npytest-forked      1.0.2\r\npython-dateutil    2.7.3\r\npython-jose        2.0.2\r\npytz               2018.5\r\nPyYAML             5.1\r\nrequests           2.21.0\r\nrequests-mock      1.5.2\r\nresponses          0.10.6\r\ns3transfer         0.1.13\r\nsetuptools         41.0.1\r\nsix                1.11.0\r\nsqlite3worker      1.1.7\r\ntabulate           0.8.3\r\nurllib3            1.24.3\r\nwcwidth            0.1.7\r\nwebsocket-client   0.56.0\r\nWerkzeug           0.15.2\r\nwheel              0.33.1\r\nwrapt              1.11.1\r\nxlrd               1.1.0\r\nxmltodict          0.12.0\r\nzipp               0.5.1\r\n```", "choices": "(A)     def __str__(self):\n        if self._excinfo is None:\n            return repr(self)\n        entry = self.traceback[-1]\n        loc = ReprFileLocation(entry.path, entry.lineno + 1, self.exconly())\n        return str(loc)\n(B) \n    def __str__(self):\n        if self._excinfo is None:\n            return repr(self)\n        entry = self.traceback[-1]\n        loc = ReprFileLocation(entry.path, entry.lineno + 1, self.exconly())\n(C)             truncate_locals=truncate_locals,\n            chain=chain,\n        )\n        return fmt.repr_excinfo(self)\n\n    def __str__(self):\n(D)         )\n        return fmt.repr_excinfo(self)\n\n    def __str__(self):\n        if self._excinfo is None:\n            return repr(self)", "answer": "D"}
{"uuid": "5b35db13-76ad-42c3-86df-12c23370da74", "issue_statement": "Confusing assertion rewriting message with byte strings\nThe comparison with assertion rewriting for byte strings is confusing: \r\n```\r\n    def test_b():\r\n>       assert b\"\" == b\"42\"\r\nE       AssertionError: assert b'' == b'42'\r\nE         Right contains more items, first extra item: 52\r\nE         Full diff:\r\nE         - b''\r\nE         + b'42'\r\nE         ?   ++\r\n```\r\n\r\n52 is the ASCII ordinal of \"4\" here.\r\n\r\nIt became clear to me when using another example:\r\n\r\n```\r\n    def test_b():\r\n>       assert b\"\" == b\"1\"\r\nE       AssertionError: assert b'' == b'1'\r\nE         Right contains more items, first extra item: 49\r\nE         Full diff:\r\nE         - b''\r\nE         + b'1'\r\nE         ?   +\r\n```\r\n\r\nNot sure what should/could be done here.", "choices": "(A) \n\ndef _compare_eq_sequence(left, right, verbose=0):\n    explanation = []\n    len_left = len(left)\n    len_right = len(right)\n    for i in range(min(len_left, len_right)):\n        if left[i] != right[i]:\n            explanation += [\n                \"At index {} diff: {!r} != {!r}\".format(i, left[i], right[i])\n            ]\n            break\n    len_diff = len_left - len_right\n\n    if len_diff:\n        if len_diff > 0:\n            dir_with_more = \"Left\"\n            extra = saferepr(left[len_right])\n        else:\n            len_diff = 0 - len_diff\n            dir_with_more = \"Right\"\n            extra = saferepr(right[len_left])\n\n        if len_diff == 1:\n            explanation += [\n                \"{} contains one more item: {}\".format(dir_with_more, extra)\n            ]\n        else:\n            explanation += [\n                \"%s contains %d more items, first extra item: %s\"\n                % (dir_with_more, len_diff, extra)\n            ]\n    return explanation\n\n\ndef _compare_eq_set(left, right, verbose=0):\n    explanation = []\n    diff_left = left - right\n(B)             len_diff = 0 - len_diff\n            dir_with_more = \"Right\"\n            extra = saferepr(right[len_left])\n\n        if len_diff == 1:\n            explanation += [\n                \"{} contains one more item: {}\".format(dir_with_more, extra)\n            ]\n        else:\n            explanation += [\n                \"%s contains %d more items, first extra item: %s\"\n                % (dir_with_more, len_diff, extra)\n            ]\n    return explanation\n\n\ndef _compare_eq_set(left, right, verbose=0):\n    explanation = []\n    diff_left = left - right\n    diff_right = right - left\n    if diff_left:\n        explanation.append(\"Extra items in the left set:\")\n        for item in diff_left:\n            explanation.append(saferepr(item))\n    if diff_right:\n        explanation.append(\"Extra items in the right set:\")\n        for item in diff_right:\n            explanation.append(saferepr(item))\n    return explanation\n\n\ndef _compare_eq_dict(left, right, verbose=0):\n    explanation = []\n    set_left = set(left)\n    set_right = set(right)\n    common = set_left.intersection(set_right)\n    same = {k: left[k] for k in common if left[k] == right[k]}\n    if same and verbose < 2:\n(C)             ]\n            break\n    len_diff = len_left - len_right\n\n    if len_diff:\n        if len_diff > 0:\n            dir_with_more = \"Left\"\n            extra = saferepr(left[len_right])\n        else:\n            len_diff = 0 - len_diff\n            dir_with_more = \"Right\"\n            extra = saferepr(right[len_left])\n\n        if len_diff == 1:\n            explanation += [\n                \"{} contains one more item: {}\".format(dir_with_more, extra)\n            ]\n        else:\n            explanation += [\n                \"%s contains %d more items, first extra item: %s\"\n                % (dir_with_more, len_diff, extra)\n            ]\n    return explanation\n\n\ndef _compare_eq_set(left, right, verbose=0):\n    explanation = []\n    diff_left = left - right\n    diff_right = right - left\n    if diff_left:\n        explanation.append(\"Extra items in the left set:\")\n        for item in diff_left:\n            explanation.append(saferepr(item))\n    if diff_right:\n        explanation.append(\"Extra items in the right set:\")\n        for item in diff_right:\n            explanation.append(saferepr(item))\n    return explanation\n(D)     # dynamic import to speedup pytest\n    import difflib\n\n    left_formatting = pprint.pformat(left).splitlines()\n    right_formatting = pprint.pformat(right).splitlines()\n    explanation = [\"Full diff:\"]\n    explanation.extend(\n        line.strip() for line in difflib.ndiff(left_formatting, right_formatting)\n    )\n    return explanation\n\n\ndef _compare_eq_sequence(left, right, verbose=0):\n    explanation = []\n    len_left = len(left)\n    len_right = len(right)\n    for i in range(min(len_left, len_right)):\n        if left[i] != right[i]:\n            explanation += [\n                \"At index {} diff: {!r} != {!r}\".format(i, left[i], right[i])\n            ]\n            break\n    len_diff = len_left - len_right\n\n    if len_diff:\n        if len_diff > 0:\n            dir_with_more = \"Left\"\n            extra = saferepr(left[len_right])\n        else:\n            len_diff = 0 - len_diff\n            dir_with_more = \"Right\"\n            extra = saferepr(right[len_left])\n\n        if len_diff == 1:\n            explanation += [\n                \"{} contains one more item: {}\".format(dir_with_more, extra)\n            ]\n        else:", "answer": "A"}
{"uuid": "3162219d-835c-4df4-b23d-2bbb46cb3c9f", "issue_statement": "pytest --collect-only needs a one char shortcut command\nI find myself needing to run `--collect-only` very often and that cli argument is a very long to type one. \r\n\r\nI do think that it would be great to allocate a character for it, not sure which one yet. Please use up/down thumbs to vote if you would find it useful or not and eventually proposing which char should be used. \r\n\r\nClearly this is a change very easy to implement but first I want to see if others would find it useful or not.\npytest --collect-only needs a one char shortcut command\nI find myself needing to run `--collect-only` very often and that cli argument is a very long to type one. \r\n\r\nI do think that it would be great to allocate a character for it, not sure which one yet. Please use up/down thumbs to vote if you would find it useful or not and eventually proposing which char should be used. \r\n\r\nClearly this is a change very easy to implement but first I want to see if others would find it useful or not.", "choices": "(A)         help=\"only collect tests, don't execute them.\",\n    ),\n    group.addoption(\n        \"--pyargs\",\n        action=\"store_true\",\n        help=\"try to interpret all arguments as python packages.\",\n    )\n(B)     group.addoption(\n        \"--collectonly\",\n        \"--collect-only\",\n        action=\"store_true\",\n        help=\"only collect tests, don't execute them.\",\n    ),\n    group.addoption(\n(C)         \"--collect-only\",\n        action=\"store_true\",\n        help=\"only collect tests, don't execute them.\",\n    ),\n    group.addoption(\n        \"--pyargs\",\n        action=\"store_true\",\n(D) \n    group = parser.getgroup(\"collect\", \"collection\")\n    group.addoption(\n        \"--collectonly\",\n        \"--collect-only\",\n        action=\"store_true\",\n        help=\"only collect tests, don't execute them.\",", "answer": "B"}
{"uuid": "bef5f835-b68e-4f4a-b59d-6ff55369549d", "issue_statement": "INTERNALERROR when exception in __repr__\nMinimal code to reproduce the issue: \r\n```python\r\nclass SomeClass:\r\n    def __getattribute__(self, attr):\r\n        raise\r\n    def __repr__(self):\r\n        raise\r\ndef test():\r\n    SomeClass().attr\r\n```\r\nSession traceback:\r\n```\r\n============================= test session starts ==============================\r\nplatform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 -- /usr/local/opt/python@3.8/bin/python3.8\r\ncachedir: .pytest_cache\r\nrootdir: ******\r\nplugins: asyncio-0.10.0, mock-3.0.0, cov-2.8.1\r\ncollecting ... collected 1 item\r\n\r\ntest_pytest.py::test \r\nINTERNALERROR> Traceback (most recent call last):\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/main.py\", line 191, in wrap_session\r\nINTERNALERROR>     session.exitstatus = doit(config, session) or 0\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/main.py\", line 247, in _main\r\nINTERNALERROR>     config.hook.pytest_runtestloop(session=session)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/hooks.py\", line 286, in __call__\r\nINTERNALERROR>     return self._hookexec(self, self.get_hookimpls(), kwargs)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/manager.py\", line 93, in _hookexec\r\nINTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/manager.py\", line 84, in <lambda>\r\nINTERNALERROR>     self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/callers.py\", line 208, in _multicall\r\nINTERNALERROR>     return outcome.get_result()\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/callers.py\", line 80, in get_result\r\nINTERNALERROR>     raise ex[1].with_traceback(ex[2])\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/callers.py\", line 187, in _multicall\r\nINTERNALERROR>     res = hook_impl.function(*args)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/main.py\", line 272, in pytest_runtestloop\r\nINTERNALERROR>     item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/hooks.py\", line 286, in __call__\r\nINTERNALERROR>     return self._hookexec(self, self.get_hookimpls(), kwargs)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/manager.py\", line 93, in _hookexec\r\nINTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/manager.py\", line 84, in <lambda>\r\nINTERNALERROR>     self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/callers.py\", line 208, in _multicall\r\nINTERNALERROR>     return outcome.get_result()\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/callers.py\", line 80, in get_result\r\nINTERNALERROR>     raise ex[1].with_traceback(ex[2])\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/callers.py\", line 187, in _multicall\r\nINTERNALERROR>     res = hook_impl.function(*args)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/runner.py\", line 85, in pytest_runtest_protocol\r\nINTERNALERROR>     runtestprotocol(item, nextitem=nextitem)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/runner.py\", line 100, in runtestprotocol\r\nINTERNALERROR>     reports.append(call_and_report(item, \"call\", log))\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/runner.py\", line 188, in call_and_report\r\nINTERNALERROR>     report = hook.pytest_runtest_makereport(item=item, call=call)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/hooks.py\", line 286, in __call__\r\nINTERNALERROR>     return self._hookexec(self, self.get_hookimpls(), kwargs)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/manager.py\", line 93, in _hookexec\r\nINTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/manager.py\", line 84, in <lambda>\r\nINTERNALERROR>     self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/callers.py\", line 203, in _multicall\r\nINTERNALERROR>     gen.send(outcome)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/skipping.py\", line 129, in pytest_runtest_makereport\r\nINTERNALERROR>     rep = outcome.get_result()\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/callers.py\", line 80, in get_result\r\nINTERNALERROR>     raise ex[1].with_traceback(ex[2])\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/pluggy/callers.py\", line 187, in _multicall\r\nINTERNALERROR>     res = hook_impl.function(*args)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/runner.py\", line 260, in pytest_runtest_makereport\r\nINTERNALERROR>     return TestReport.from_item_and_call(item, call)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/reports.py\", line 294, in from_item_and_call\r\nINTERNALERROR>     longrepr = item.repr_failure(excinfo)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/python.py\", line 1513, in repr_failure\r\nINTERNALERROR>     return self._repr_failure_py(excinfo, style=style)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/nodes.py\", line 355, in _repr_failure_py\r\nINTERNALERROR>     return excinfo.getrepr(\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_code/code.py\", line 634, in getrepr\r\nINTERNALERROR>     return fmt.repr_excinfo(self)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_code/code.py\", line 879, in repr_excinfo\r\nINTERNALERROR>     reprtraceback = self.repr_traceback(excinfo_)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_code/code.py\", line 823, in repr_traceback\r\nINTERNALERROR>     reprentry = self.repr_traceback_entry(entry, einfo)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_code/code.py\", line 784, in repr_traceback_entry\r\nINTERNALERROR>     reprargs = self.repr_args(entry) if not short else None\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_code/code.py\", line 693, in repr_args\r\nINTERNALERROR>     args.append((argname, saferepr(argvalue)))\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_io/saferepr.py\", line 82, in saferepr\r\nINTERNALERROR>     return SafeRepr(maxsize).repr(obj)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_io/saferepr.py\", line 51, in repr\r\nINTERNALERROR>     s = _format_repr_exception(exc, x)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_io/saferepr.py\", line 23, in _format_repr_exception\r\nINTERNALERROR>     exc_info, obj.__class__.__name__, id(obj)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_io/saferepr.py\", line 47, in repr\r\nINTERNALERROR>     s = super().repr(x)\r\nINTERNALERROR>   File \"/usr/local/Cellar/python@3.8/3.8.1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/reprlib.py\", line 52, in repr\r\nINTERNALERROR>     return self.repr1(x, self.maxlevel)\r\nINTERNALERROR>   File \"/usr/local/Cellar/python@3.8/3.8.1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/reprlib.py\", line 62, in repr1\r\nINTERNALERROR>     return self.repr_instance(x, level)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_io/saferepr.py\", line 60, in repr_instance\r\nINTERNALERROR>     s = _format_repr_exception(exc, x)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_io/saferepr.py\", line 23, in _format_repr_exception\r\nINTERNALERROR>     exc_info, obj.__class__.__name__, id(obj)\r\nINTERNALERROR>   File \"/usr/local/lib/python3.8/site-packages/_pytest/_io/saferepr.py\", line 56, in repr_instance\r\nINTERNALERROR>     s = repr(x)\r\nINTERNALERROR>   File \"/Users/stiflou/Documents/projets/apischema/tests/test_pytest.py\", line 6, in __repr__\r\nINTERNALERROR>     raise\r\nINTERNALERROR> RuntimeError: No active exception to reraise\r\n\r\n============================ no tests ran in 0.09s ============================\r\n```", "choices": "(A)     except (KeyboardInterrupt, SystemExit):\n        raise\n    except BaseException as exc:\n        exc_info = \"unpresentable exception ({})\".format(_try_repr_or_str(exc))\n    return \"<[{} raised in repr()] {} object at 0x{:x}>\".format(\n        exc_info, obj.__class__.__name__, id(obj)\n    )\n(B)     return \"<[{} raised in repr()] {} object at 0x{:x}>\".format(\n        exc_info, obj.__class__.__name__, id(obj)\n    )\n\n\ndef _ellipsize(s: str, maxsize: int) -> str:\n    if len(s) > maxsize:\n(C)     )\n\n\ndef _ellipsize(s: str, maxsize: int) -> str:\n    if len(s) > maxsize:\n        i = max(0, (maxsize - 3) // 2)\n        j = max(0, maxsize - 3 - i)\n(D)     except BaseException as exc:\n        exc_info = \"unpresentable exception ({})\".format(_try_repr_or_str(exc))\n    return \"<[{} raised in repr()] {} object at 0x{:x}>\".format(\n        exc_info, obj.__class__.__name__, id(obj)\n    )\n\n", "answer": "D"}
{"uuid": "fb03b61c-6dea-4081-8297-423f49b43d59", "issue_statement": "Wrong path to test file when directory changed in fixture\nFiles are shown as relative to new directory when working directory is changed in a fixture. This makes it impossible to jump to the error as the editor is unaware of the directory change. The displayed directory should stay relative to the original directory.\r\n\r\ntest_path_error.py:\r\n```python\r\nimport os\r\nimport errno\r\nimport shutil\r\n\r\nimport pytest\r\n\r\n\r\n@pytest.fixture\r\ndef private_dir():  # or (monkeypatch)\r\n    out_dir = 'ddd'\r\n\r\n    try:\r\n        shutil.rmtree(out_dir)\r\n    except OSError as ex:\r\n        if ex.errno != errno.ENOENT:\r\n            raise\r\n    os.mkdir(out_dir)\r\n\r\n    old_dir = os.getcwd()\r\n    os.chdir(out_dir)\r\n    yield out_dir\r\n    os.chdir(old_dir)\r\n\r\n    # Same issue if using:\r\n    # monkeypatch.chdir(out_dir)\r\n\r\n\r\ndef test_show_wrong_path(private_dir):\r\n    assert False\r\n```\r\n\r\n```diff\r\n+ Expected: test_path_error.py:29: AssertionError\r\n- Displayed: ../test_path_error.py:29: AssertionError\r\n```\r\n\r\nThe full output is:\r\n```\r\n-*- mode: compilation; default-directory: \"~/src/pytest_path_error/\" -*-\r\nCompilation started at Fri Jan 10 00:05:52\r\n\r\nnox\r\nnox > Running session test\r\nnox > Creating virtual environment (virtualenv) using python3.7 in .nox/test\r\nnox > pip install pytest>=5.3\r\nnox > pip freeze\r\nattrs==19.3.0\r\nimportlib-metadata==1.3.0\r\nmore-itertools==8.0.2\r\npackaging==20.0\r\npluggy==0.13.1\r\npy==1.8.1\r\npyparsing==2.4.6\r\npytest==5.3.2\r\nsix==1.13.0\r\nwcwidth==0.1.8\r\nzipp==0.6.0\r\nnox > pytest \r\n================================= test session starts =================================\r\nplatform linux -- Python 3.7.5, pytest-5.3.2, py-1.8.1, pluggy-0.13.1\r\nrootdir: /home/lhn/src/pytest_path_error\r\ncollected 1 item                                                                      \r\n\r\ntest_path_error.py F                                                            [100%]\r\n\r\n====================================== FAILURES =======================================\r\n________________________________ test_show_wrong_path _________________________________\r\n\r\nprivate_dir = 'ddd'\r\n\r\n    def test_show_wrong_path(private_dir):\r\n>       assert False\r\nE       assert False\r\n\r\n../test_path_error.py:29: AssertionError\r\n================================== 1 failed in 0.03s ==================================\r\nnox > Command pytest  failed with exit code 1\r\nnox > Session test failed.\r\n\r\nCompilation exited abnormally with code 1 at Fri Jan 10 00:06:01\r\n```\r\n\r\nnoxfile.py:\r\n```python\r\nimport nox\r\n\r\n@nox.session(python='3.7')\r\ndef test(session):\r\n    session.install('pytest>=5.3')\r\n    session.run('pip', 'freeze')\r\n    session.run('pytest')\r\n```", "choices": "(A) from _pytest.mark.structures import MarkDecorator\nfrom _pytest.mark.structures import NodeKeywords\nfrom _pytest.outcomes import fail\nfrom _pytest.store import Store\n\nif TYPE_CHECKING:\n    # Imported here due to circular import.\n    from _pytest.main import Session\n\nSEP = \"/\"\n\ntracebackcutdir = py.path.local(_pytest.__file__).dirpath()\n\n\n@lru_cache(maxsize=None)\ndef _splitnode(nodeid):\n    \"\"\"Split a nodeid into constituent 'parts'.\n\n    Node IDs are strings, and can be things like:\n        ''\n        'testing/code'\n        'testing/code/test_excinfo.py'\n        'testing/code/test_excinfo.py::TestFormattedExcinfo'\n\n    Return values are lists e.g.\n        []\n        ['testing', 'code']\n        ['testing', 'code', 'test_excinfo.py']\n        ['testing', 'code', 'test_excinfo.py', 'TestFormattedExcinfo']\n    \"\"\"\n    if nodeid == \"\":\n        # If there is no root node at all, return an empty list so the caller's logic can remain sane\n        return ()\n    parts = nodeid.split(SEP)\n    # Replace single last element 'test_foo.py::Bar' with multiple elements 'test_foo.py', 'Bar'\n    parts[-1:] = parts[-1].split(\"::\")\n    # Convert parts into a tuple to avoid possible errors with caching of a mutable type\n    return tuple(parts)\n\n\ndef ischildnode(baseid, nodeid):\n    \"\"\"Return True if the nodeid is a child node of the baseid.\n\n    E.g. 'foo/bar::Baz' is a child of 'foo', 'foo/bar' and 'foo/bar::Baz', but not of 'foo/blorp'\n    \"\"\"\n    base_parts = _splitnode(baseid)\n    node_parts = _splitnode(nodeid)\n    if len(node_parts) < len(base_parts):\n        return False\n    return node_parts[: len(base_parts)] == base_parts\n\n\nclass NodeMeta(type):\n    def __call__(self, *k, **kw):\n        warnings.warn(NODE_USE_FROM_PARENT.format(name=self.__name__), stacklevel=2)\n        return super().__call__(*k, **kw)\n\n    def _create(self, *k, **kw):\n        return super().__call__(*k, **kw)\n\n\nclass Node(metaclass=NodeMeta):\n    \"\"\" base class for Collector and Item the test collection tree.\n    Collector subclasses have children, Items are terminal nodes.\"\"\"\n\n    # Use __slots__ to make attribute access faster.\n    # Note that __dict__ is still available.\n    __slots__ = (\n        \"name\",\n        \"parent\",\n        \"config\",\n        \"session\",\n        \"fspath\",\n        \"_nodeid\",\n        \"_store\",\n        \"__dict__\",\n    )\n\n    def __init__(\n        self,\n        name: str,\n        parent: Optional[\"Node\"] = None,\n        config: Optional[Config] = None,\n        session: Optional[\"Session\"] = None,\n        fspath: Optional[py.path.local] = None,\n        nodeid: Optional[str] = None,\n    ) -> None:\n        #: a unique name within the scope of the parent node\n        self.name = name\n\n        #: the parent collector node.\n        self.parent = parent\n\n        #: the pytest config object\n        if config:\n            self.config = config\n        else:\n            if not parent:\n                raise TypeError(\"config or parent must be provided\")\n            self.config = parent.config\n\n        #: the session this node is part of\n        if session:\n            self.session = session\n        else:\n            if not parent:\n                raise TypeError(\"session or parent must be provided\")\n            self.session = parent.session\n\n        #: filesystem path where this node was collected from (can be None)\n        self.fspath = fspath or getattr(parent, \"fspath\", None)\n\n        #: keywords/markers collected from all scopes\n        self.keywords = NodeKeywords(self)\n\n        #: the marker objects belonging to this node\n        self.own_markers = []  # type: List[Mark]\n\n        #: allow adding of extra keywords to use for matching\n        self.extra_keyword_matches = set()  # type: Set[str]\n\n        # used for storing artificial fixturedefs for direct parametrization\n        self._name2pseudofixturedef = {}  # type: Dict[str, FixtureDef]\n\n        if nodeid is not None:\n            assert \"::()\" not in nodeid\n            self._nodeid = nodeid\n        else:\n            if not self.parent:\n                raise TypeError(\"nodeid or parent must be provided\")\n            self._nodeid = self.parent.nodeid\n            if self.name != \"()\":\n                self._nodeid += \"::\" + self.name\n\n        # A place where plugins can store information on the node for their\n        # own use. Currently only intended for internal plugins.\n        self._store = Store()\n\n    @classmethod\n    def from_parent(cls, parent: \"Node\", **kw):\n        \"\"\"\n        Public Constructor for Nodes\n\n        This indirection got introduced in order to enable removing\n        the fragile logic from the node constructors.\n\n        Subclasses can use ``super().from_parent(...)`` when overriding the construction\n\n        :param parent: the parent node of this test Node\n        \"\"\"\n        if \"config\" in kw:\n            raise TypeError(\"config is not a valid argument for from_parent\")\n        if \"session\" in kw:\n            raise TypeError(\"session is not a valid argument for from_parent\")\n        return cls._create(parent=parent, **kw)\n\n    @property\n    def ihook(self):\n        \"\"\" fspath sensitive hook proxy used to call pytest hooks\"\"\"\n        return self.session.gethookproxy(self.fspath)\n\n    def __repr__(self):\n        return \"<{} {}>\".format(self.__class__.__name__, getattr(self, \"name\", None))\n\n    def warn(self, warning):\n        \"\"\"Issue a warning for this item.\n\n        Warnings will be displayed after the test session, unless explicitly suppressed\n\n        :param Warning warning: the warning instance to issue. Must be a subclass of PytestWarning.\n\n        :raise ValueError: if ``warning`` instance is not a subclass of PytestWarning.\n\n        Example usage:\n\n        .. code-block:: python\n\n            node.warn(PytestWarning(\"some message\"))\n\n        \"\"\"\n        from _pytest.warning_types import PytestWarning\n\n        if not isinstance(warning, PytestWarning):\n            raise ValueError(\n                \"warning must be an instance of PytestWarning or subclass, got {!r}\".format(\n                    warning\n                )\n            )\n        path, lineno = get_fslocation_from_item(self)\n        warnings.warn_explicit(\n            warning,\n            category=None,\n            filename=str(path),\n            lineno=lineno + 1 if lineno is not None else None,\n        )\n\n    # methods for ordering nodes\n    @property\n    def nodeid(self):\n        \"\"\" a ::-separated string denoting its collection tree address. \"\"\"\n        return self._nodeid\n\n    def __hash__(self):\n        return hash(self._nodeid)\n\n    def setup(self):\n        pass\n\n    def teardown(self):\n        pass\n\n    def listchain(self):\n        \"\"\" return list of all parent collectors up to self,\n            starting from root of collection tree. \"\"\"\n        chain = []\n        item = self  # type: Optional[Node]\n        while item is not None:\n            chain.append(item)\n            item = item.parent\n        chain.reverse()\n        return chain\n\n    def add_marker(\n        self, marker: Union[str, MarkDecorator], append: bool = True\n    ) -> None:\n        \"\"\"dynamically add a marker object to the node.\n\n        :type marker: ``str`` or ``pytest.mark.*``  object\n        :param marker:\n            ``append=True`` whether to append the marker,\n            if ``False`` insert at position ``0``.\n        \"\"\"\n        from _pytest.mark import MARK_GEN\n\n        if isinstance(marker, MarkDecorator):\n            marker_ = marker\n        elif isinstance(marker, str):\n            marker_ = getattr(MARK_GEN, marker)\n        else:\n            raise ValueError(\"is not a string or pytest.mark.* Marker\")\n        self.keywords[marker_.name] = marker\n        if append:\n            self.own_markers.append(marker_.mark)\n        else:\n            self.own_markers.insert(0, marker_.mark)\n\n    def iter_markers(self, name=None):\n        \"\"\"\n        :param name: if given, filter the results by the name attribute\n\n        iterate over all markers of the node\n        \"\"\"\n        return (x[1] for x in self.iter_markers_with_node(name=name))\n\n    def iter_markers_with_node(self, name=None):\n        \"\"\"\n        :param name: if given, filter the results by the name attribute\n\n        iterate over all markers of the node\n        returns sequence of tuples (node, mark)\n        \"\"\"\n        for node in reversed(self.listchain()):\n            for mark in node.own_markers:\n                if name is None or getattr(mark, \"name\", None) == name:\n                    yield node, mark\n\n    def get_closest_marker(self, name, default=None):\n        \"\"\"return the first marker matching the name, from closest (for example function) to farther level (for example\n        module level).\n\n        :param default: fallback return value of no marker was found\n        :param name: name to filter by\n        \"\"\"\n        return next(self.iter_markers(name=name), default)\n\n    def listextrakeywords(self):\n        \"\"\" Return a set of all extra keywords in self and any parents.\"\"\"\n        extra_keywords = set()  # type: Set[str]\n        for item in self.listchain():\n            extra_keywords.update(item.extra_keyword_matches)\n        return extra_keywords\n\n    def listnames(self):\n        return [x.name for x in self.listchain()]\n\n    def addfinalizer(self, fin):\n        \"\"\" register a function to be called when this node is finalized.\n\n        This method can only be called when this node is active\n        in a setup chain, for example during self.setup().\n        \"\"\"\n        self.session._setupstate.addfinalizer(fin, self)\n\n    def getparent(self, cls):\n        \"\"\" get the next parent node (including ourself)\n        which is an instance of the given class\"\"\"\n        current = self  # type: Optional[Node]\n        while current and not isinstance(current, cls):\n            current = current.parent\n        return current\n\n    def _prunetraceback(self, excinfo):\n        pass\n\n    def _repr_failure_py(\n        self, excinfo: ExceptionInfo[BaseException], style=None,\n    ) -> Union[str, ReprExceptionInfo, ExceptionChainRepr, FixtureLookupErrorRepr]:\n        if isinstance(excinfo.value, ConftestImportFailure):\n            excinfo = ExceptionInfo(excinfo.value.excinfo)\n        if isinstance(excinfo.value, fail.Exception):\n            if not excinfo.value.pytrace:\n                style = \"value\"\n        if isinstance(excinfo.value, FixtureLookupError):\n            return excinfo.value.formatrepr()\n        if self.config.getoption(\"fulltrace\", False):\n            style = \"long\"\n        else:\n            tb = _pytest._code.Traceback([excinfo.traceback[-1]])\n            self._prunetraceback(excinfo)\n            if len(excinfo.traceback) == 0:\n                excinfo.traceback = tb\n            if style == \"auto\":\n                style = \"long\"\n        # XXX should excinfo.getrepr record all data and toterminal() process it?\n        if style is None:\n            if self.config.getoption(\"tbstyle\", \"auto\") == \"short\":\n                style = \"short\"\n            else:\n                style = \"long\"\n\n        if self.config.getoption(\"verbose\", 0) > 1:\n            truncate_locals = False\n        else:\n            truncate_locals = True\n\n        try:\n            os.getcwd()\n            abspath = False\n        except OSError:\n            abspath = True\n\n        return excinfo.getrepr(\n            funcargs=True,\n            abspath=abspath,\n            showlocals=self.config.getoption(\"showlocals\", False),\n            style=style,\n            tbfilter=False,  # pruned already, or in --fulltrace mode.\n(B)         #: a unique name within the scope of the parent node\n        self.name = name\n\n        #: the parent collector node.\n        self.parent = parent\n\n        #: the pytest config object\n        if config:\n            self.config = config\n        else:\n            if not parent:\n                raise TypeError(\"config or parent must be provided\")\n            self.config = parent.config\n\n        #: the session this node is part of\n        if session:\n            self.session = session\n        else:\n            if not parent:\n                raise TypeError(\"session or parent must be provided\")\n            self.session = parent.session\n\n        #: filesystem path where this node was collected from (can be None)\n        self.fspath = fspath or getattr(parent, \"fspath\", None)\n\n        #: keywords/markers collected from all scopes\n        self.keywords = NodeKeywords(self)\n\n        #: the marker objects belonging to this node\n        self.own_markers = []  # type: List[Mark]\n\n        #: allow adding of extra keywords to use for matching\n        self.extra_keyword_matches = set()  # type: Set[str]\n\n        # used for storing artificial fixturedefs for direct parametrization\n        self._name2pseudofixturedef = {}  # type: Dict[str, FixtureDef]\n\n        if nodeid is not None:\n            assert \"::()\" not in nodeid\n            self._nodeid = nodeid\n        else:\n            if not self.parent:\n                raise TypeError(\"nodeid or parent must be provided\")\n            self._nodeid = self.parent.nodeid\n            if self.name != \"()\":\n                self._nodeid += \"::\" + self.name\n\n        # A place where plugins can store information on the node for their\n        # own use. Currently only intended for internal plugins.\n        self._store = Store()\n\n    @classmethod\n    def from_parent(cls, parent: \"Node\", **kw):\n        \"\"\"\n        Public Constructor for Nodes\n\n        This indirection got introduced in order to enable removing\n        the fragile logic from the node constructors.\n\n        Subclasses can use ``super().from_parent(...)`` when overriding the construction\n\n        :param parent: the parent node of this test Node\n        \"\"\"\n        if \"config\" in kw:\n            raise TypeError(\"config is not a valid argument for from_parent\")\n        if \"session\" in kw:\n            raise TypeError(\"session is not a valid argument for from_parent\")\n        return cls._create(parent=parent, **kw)\n\n    @property\n    def ihook(self):\n        \"\"\" fspath sensitive hook proxy used to call pytest hooks\"\"\"\n        return self.session.gethookproxy(self.fspath)\n\n    def __repr__(self):\n        return \"<{} {}>\".format(self.__class__.__name__, getattr(self, \"name\", None))\n\n    def warn(self, warning):\n        \"\"\"Issue a warning for this item.\n\n        Warnings will be displayed after the test session, unless explicitly suppressed\n\n        :param Warning warning: the warning instance to issue. Must be a subclass of PytestWarning.\n\n        :raise ValueError: if ``warning`` instance is not a subclass of PytestWarning.\n\n        Example usage:\n\n        .. code-block:: python\n\n            node.warn(PytestWarning(\"some message\"))\n\n        \"\"\"\n        from _pytest.warning_types import PytestWarning\n\n        if not isinstance(warning, PytestWarning):\n            raise ValueError(\n                \"warning must be an instance of PytestWarning or subclass, got {!r}\".format(\n                    warning\n                )\n            )\n        path, lineno = get_fslocation_from_item(self)\n        warnings.warn_explicit(\n            warning,\n            category=None,\n            filename=str(path),\n            lineno=lineno + 1 if lineno is not None else None,\n        )\n\n    # methods for ordering nodes\n    @property\n    def nodeid(self):\n        \"\"\" a ::-separated string denoting its collection tree address. \"\"\"\n        return self._nodeid\n\n    def __hash__(self):\n        return hash(self._nodeid)\n\n    def setup(self):\n        pass\n\n    def teardown(self):\n        pass\n\n    def listchain(self):\n        \"\"\" return list of all parent collectors up to self,\n            starting from root of collection tree. \"\"\"\n        chain = []\n        item = self  # type: Optional[Node]\n        while item is not None:\n            chain.append(item)\n            item = item.parent\n        chain.reverse()\n        return chain\n\n    def add_marker(\n        self, marker: Union[str, MarkDecorator], append: bool = True\n    ) -> None:\n        \"\"\"dynamically add a marker object to the node.\n\n        :type marker: ``str`` or ``pytest.mark.*``  object\n        :param marker:\n            ``append=True`` whether to append the marker,\n            if ``False`` insert at position ``0``.\n        \"\"\"\n        from _pytest.mark import MARK_GEN\n\n        if isinstance(marker, MarkDecorator):\n            marker_ = marker\n        elif isinstance(marker, str):\n            marker_ = getattr(MARK_GEN, marker)\n        else:\n            raise ValueError(\"is not a string or pytest.mark.* Marker\")\n        self.keywords[marker_.name] = marker\n        if append:\n            self.own_markers.append(marker_.mark)\n        else:\n            self.own_markers.insert(0, marker_.mark)\n\n    def iter_markers(self, name=None):\n        \"\"\"\n        :param name: if given, filter the results by the name attribute\n\n        iterate over all markers of the node\n        \"\"\"\n        return (x[1] for x in self.iter_markers_with_node(name=name))\n\n    def iter_markers_with_node(self, name=None):\n        \"\"\"\n        :param name: if given, filter the results by the name attribute\n\n        iterate over all markers of the node\n        returns sequence of tuples (node, mark)\n        \"\"\"\n        for node in reversed(self.listchain()):\n            for mark in node.own_markers:\n                if name is None or getattr(mark, \"name\", None) == name:\n                    yield node, mark\n\n    def get_closest_marker(self, name, default=None):\n        \"\"\"return the first marker matching the name, from closest (for example function) to farther level (for example\n        module level).\n\n        :param default: fallback return value of no marker was found\n        :param name: name to filter by\n        \"\"\"\n        return next(self.iter_markers(name=name), default)\n\n    def listextrakeywords(self):\n        \"\"\" Return a set of all extra keywords in self and any parents.\"\"\"\n        extra_keywords = set()  # type: Set[str]\n        for item in self.listchain():\n            extra_keywords.update(item.extra_keyword_matches)\n        return extra_keywords\n\n    def listnames(self):\n        return [x.name for x in self.listchain()]\n\n    def addfinalizer(self, fin):\n        \"\"\" register a function to be called when this node is finalized.\n\n        This method can only be called when this node is active\n        in a setup chain, for example during self.setup().\n        \"\"\"\n        self.session._setupstate.addfinalizer(fin, self)\n\n    def getparent(self, cls):\n        \"\"\" get the next parent node (including ourself)\n        which is an instance of the given class\"\"\"\n        current = self  # type: Optional[Node]\n        while current and not isinstance(current, cls):\n            current = current.parent\n        return current\n\n    def _prunetraceback(self, excinfo):\n        pass\n\n    def _repr_failure_py(\n        self, excinfo: ExceptionInfo[BaseException], style=None,\n    ) -> Union[str, ReprExceptionInfo, ExceptionChainRepr, FixtureLookupErrorRepr]:\n        if isinstance(excinfo.value, ConftestImportFailure):\n            excinfo = ExceptionInfo(excinfo.value.excinfo)\n        if isinstance(excinfo.value, fail.Exception):\n            if not excinfo.value.pytrace:\n                style = \"value\"\n        if isinstance(excinfo.value, FixtureLookupError):\n            return excinfo.value.formatrepr()\n        if self.config.getoption(\"fulltrace\", False):\n            style = \"long\"\n        else:\n            tb = _pytest._code.Traceback([excinfo.traceback[-1]])\n            self._prunetraceback(excinfo)\n            if len(excinfo.traceback) == 0:\n                excinfo.traceback = tb\n            if style == \"auto\":\n                style = \"long\"\n        # XXX should excinfo.getrepr record all data and toterminal() process it?\n        if style is None:\n            if self.config.getoption(\"tbstyle\", \"auto\") == \"short\":\n                style = \"short\"\n            else:\n                style = \"long\"\n\n        if self.config.getoption(\"verbose\", 0) > 1:\n            truncate_locals = False\n        else:\n            truncate_locals = True\n\n        try:\n            os.getcwd()\n            abspath = False\n        except OSError:\n            abspath = True\n\n        return excinfo.getrepr(\n            funcargs=True,\n            abspath=abspath,\n            showlocals=self.config.getoption(\"showlocals\", False),\n            style=style,\n            tbfilter=False,  # pruned already, or in --fulltrace mode.\n            truncate_locals=truncate_locals,\n        )\n\n    def repr_failure(\n        self, excinfo, style=None\n    ) -> Union[str, ReprExceptionInfo, ExceptionChainRepr, FixtureLookupErrorRepr]:\n        \"\"\"\n        Return a representation of a collection or test failure.\n\n        :param excinfo: Exception information for the failure.\n        \"\"\"\n        return self._repr_failure_py(excinfo, style)\n\n\ndef get_fslocation_from_item(\n    item: \"Item\",\n) -> Tuple[Union[str, py.path.local], Optional[int]]:\n    \"\"\"Tries to extract the actual location from an item, depending on available attributes:\n\n    * \"fslocation\": a pair (path, lineno)\n    * \"obj\": a Python object that the item wraps.\n    * \"fspath\": just a path\n\n    :rtype: a tuple of (str|LocalPath, int) with filename and line number.\n    \"\"\"\n    try:\n        return item.location[:2]\n    except AttributeError:\n        pass\n    obj = getattr(item, \"obj\", None)\n    if obj is not None:\n        return getfslineno(obj)\n    return getattr(item, \"fspath\", \"unknown location\"), -1\n\n\nclass Collector(Node):\n    \"\"\" Collector instances create children through collect()\n        and thus iteratively build a tree.\n    \"\"\"\n\n    class CollectError(Exception):\n        \"\"\" an error during collection, contains a custom message. \"\"\"\n\n    def collect(self):\n        \"\"\" returns a list of children (items and collectors)\n            for this collection node.\n        \"\"\"\n        raise NotImplementedError(\"abstract\")\n\n    def repr_failure(self, excinfo):\n        \"\"\"\n        Return a representation of a collection failure.\n\n        :param excinfo: Exception information for the failure.\n        \"\"\"\n        if excinfo.errisinstance(self.CollectError) and not self.config.getoption(\n            \"fulltrace\", False\n        ):\n            exc = excinfo.value\n            return str(exc.args[0])\n\n        # Respect explicit tbstyle option, but default to \"short\"\n        # (_repr_failure_py uses \"long\" with \"fulltrace\" option always).\n        tbstyle = self.config.getoption(\"tbstyle\", \"auto\")\n        if tbstyle == \"auto\":\n            tbstyle = \"short\"\n\n        return self._repr_failure_py(excinfo, style=tbstyle)\n\n    def _prunetraceback(self, excinfo):\n        if hasattr(self, \"fspath\"):\n            traceback = excinfo.traceback\n            ntraceback = traceback.cut(path=self.fspath)\n            if ntraceback == traceback:\n                ntraceback = ntraceback.cut(excludepath=tracebackcutdir)\n            excinfo.traceback = ntraceback.filter()\n\n\ndef _check_initialpaths_for_relpath(session, fspath):\n    for initial_path in session._initialpaths:\n        if fspath.common(initial_path) == initial_path:\n            return fspath.relto(initial_path)\n\n\nclass FSHookProxy:\n    def __init__(self, pm: PytestPluginManager, remove_mods) -> None:\n        self.pm = pm\n(C) \n        .. code-block:: python\n\n            node.warn(PytestWarning(\"some message\"))\n\n        \"\"\"\n        from _pytest.warning_types import PytestWarning\n\n        if not isinstance(warning, PytestWarning):\n            raise ValueError(\n                \"warning must be an instance of PytestWarning or subclass, got {!r}\".format(\n                    warning\n                )\n            )\n        path, lineno = get_fslocation_from_item(self)\n        warnings.warn_explicit(\n            warning,\n            category=None,\n            filename=str(path),\n            lineno=lineno + 1 if lineno is not None else None,\n        )\n\n    # methods for ordering nodes\n    @property\n    def nodeid(self):\n        \"\"\" a ::-separated string denoting its collection tree address. \"\"\"\n        return self._nodeid\n\n    def __hash__(self):\n        return hash(self._nodeid)\n\n    def setup(self):\n        pass\n\n    def teardown(self):\n        pass\n\n    def listchain(self):\n        \"\"\" return list of all parent collectors up to self,\n            starting from root of collection tree. \"\"\"\n        chain = []\n        item = self  # type: Optional[Node]\n        while item is not None:\n            chain.append(item)\n            item = item.parent\n        chain.reverse()\n        return chain\n\n    def add_marker(\n        self, marker: Union[str, MarkDecorator], append: bool = True\n    ) -> None:\n        \"\"\"dynamically add a marker object to the node.\n\n        :type marker: ``str`` or ``pytest.mark.*``  object\n        :param marker:\n            ``append=True`` whether to append the marker,\n            if ``False`` insert at position ``0``.\n        \"\"\"\n        from _pytest.mark import MARK_GEN\n\n        if isinstance(marker, MarkDecorator):\n            marker_ = marker\n        elif isinstance(marker, str):\n            marker_ = getattr(MARK_GEN, marker)\n        else:\n            raise ValueError(\"is not a string or pytest.mark.* Marker\")\n        self.keywords[marker_.name] = marker\n        if append:\n            self.own_markers.append(marker_.mark)\n        else:\n            self.own_markers.insert(0, marker_.mark)\n\n    def iter_markers(self, name=None):\n        \"\"\"\n        :param name: if given, filter the results by the name attribute\n\n        iterate over all markers of the node\n        \"\"\"\n        return (x[1] for x in self.iter_markers_with_node(name=name))\n\n    def iter_markers_with_node(self, name=None):\n        \"\"\"\n        :param name: if given, filter the results by the name attribute\n\n        iterate over all markers of the node\n        returns sequence of tuples (node, mark)\n        \"\"\"\n        for node in reversed(self.listchain()):\n            for mark in node.own_markers:\n                if name is None or getattr(mark, \"name\", None) == name:\n                    yield node, mark\n\n    def get_closest_marker(self, name, default=None):\n        \"\"\"return the first marker matching the name, from closest (for example function) to farther level (for example\n        module level).\n\n        :param default: fallback return value of no marker was found\n        :param name: name to filter by\n        \"\"\"\n        return next(self.iter_markers(name=name), default)\n\n    def listextrakeywords(self):\n        \"\"\" Return a set of all extra keywords in self and any parents.\"\"\"\n        extra_keywords = set()  # type: Set[str]\n        for item in self.listchain():\n            extra_keywords.update(item.extra_keyword_matches)\n        return extra_keywords\n\n    def listnames(self):\n        return [x.name for x in self.listchain()]\n\n    def addfinalizer(self, fin):\n        \"\"\" register a function to be called when this node is finalized.\n\n        This method can only be called when this node is active\n        in a setup chain, for example during self.setup().\n        \"\"\"\n        self.session._setupstate.addfinalizer(fin, self)\n\n    def getparent(self, cls):\n        \"\"\" get the next parent node (including ourself)\n        which is an instance of the given class\"\"\"\n        current = self  # type: Optional[Node]\n        while current and not isinstance(current, cls):\n            current = current.parent\n        return current\n\n    def _prunetraceback(self, excinfo):\n        pass\n\n    def _repr_failure_py(\n        self, excinfo: ExceptionInfo[BaseException], style=None,\n    ) -> Union[str, ReprExceptionInfo, ExceptionChainRepr, FixtureLookupErrorRepr]:\n        if isinstance(excinfo.value, ConftestImportFailure):\n            excinfo = ExceptionInfo(excinfo.value.excinfo)\n        if isinstance(excinfo.value, fail.Exception):\n            if not excinfo.value.pytrace:\n                style = \"value\"\n        if isinstance(excinfo.value, FixtureLookupError):\n            return excinfo.value.formatrepr()\n        if self.config.getoption(\"fulltrace\", False):\n            style = \"long\"\n        else:\n            tb = _pytest._code.Traceback([excinfo.traceback[-1]])\n            self._prunetraceback(excinfo)\n            if len(excinfo.traceback) == 0:\n                excinfo.traceback = tb\n            if style == \"auto\":\n                style = \"long\"\n        # XXX should excinfo.getrepr record all data and toterminal() process it?\n        if style is None:\n            if self.config.getoption(\"tbstyle\", \"auto\") == \"short\":\n                style = \"short\"\n            else:\n                style = \"long\"\n\n        if self.config.getoption(\"verbose\", 0) > 1:\n            truncate_locals = False\n        else:\n            truncate_locals = True\n\n        try:\n            os.getcwd()\n            abspath = False\n        except OSError:\n            abspath = True\n\n        return excinfo.getrepr(\n            funcargs=True,\n            abspath=abspath,\n            showlocals=self.config.getoption(\"showlocals\", False),\n            style=style,\n            tbfilter=False,  # pruned already, or in --fulltrace mode.\n            truncate_locals=truncate_locals,\n        )\n\n    def repr_failure(\n        self, excinfo, style=None\n    ) -> Union[str, ReprExceptionInfo, ExceptionChainRepr, FixtureLookupErrorRepr]:\n        \"\"\"\n        Return a representation of a collection or test failure.\n\n        :param excinfo: Exception information for the failure.\n        \"\"\"\n        return self._repr_failure_py(excinfo, style)\n\n\ndef get_fslocation_from_item(\n    item: \"Item\",\n) -> Tuple[Union[str, py.path.local], Optional[int]]:\n    \"\"\"Tries to extract the actual location from an item, depending on available attributes:\n\n    * \"fslocation\": a pair (path, lineno)\n    * \"obj\": a Python object that the item wraps.\n    * \"fspath\": just a path\n\n    :rtype: a tuple of (str|LocalPath, int) with filename and line number.\n    \"\"\"\n    try:\n        return item.location[:2]\n    except AttributeError:\n        pass\n    obj = getattr(item, \"obj\", None)\n    if obj is not None:\n        return getfslineno(obj)\n    return getattr(item, \"fspath\", \"unknown location\"), -1\n\n\nclass Collector(Node):\n    \"\"\" Collector instances create children through collect()\n        and thus iteratively build a tree.\n    \"\"\"\n\n    class CollectError(Exception):\n        \"\"\" an error during collection, contains a custom message. \"\"\"\n\n    def collect(self):\n        \"\"\" returns a list of children (items and collectors)\n            for this collection node.\n        \"\"\"\n        raise NotImplementedError(\"abstract\")\n\n    def repr_failure(self, excinfo):\n        \"\"\"\n        Return a representation of a collection failure.\n\n        :param excinfo: Exception information for the failure.\n        \"\"\"\n        if excinfo.errisinstance(self.CollectError) and not self.config.getoption(\n            \"fulltrace\", False\n        ):\n            exc = excinfo.value\n            return str(exc.args[0])\n\n        # Respect explicit tbstyle option, but default to \"short\"\n        # (_repr_failure_py uses \"long\" with \"fulltrace\" option always).\n        tbstyle = self.config.getoption(\"tbstyle\", \"auto\")\n        if tbstyle == \"auto\":\n            tbstyle = \"short\"\n\n        return self._repr_failure_py(excinfo, style=tbstyle)\n\n    def _prunetraceback(self, excinfo):\n        if hasattr(self, \"fspath\"):\n            traceback = excinfo.traceback\n            ntraceback = traceback.cut(path=self.fspath)\n            if ntraceback == traceback:\n                ntraceback = ntraceback.cut(excludepath=tracebackcutdir)\n            excinfo.traceback = ntraceback.filter()\n\n\ndef _check_initialpaths_for_relpath(session, fspath):\n    for initial_path in session._initialpaths:\n        if fspath.common(initial_path) == initial_path:\n            return fspath.relto(initial_path)\n\n\nclass FSHookProxy:\n    def __init__(self, pm: PytestPluginManager, remove_mods) -> None:\n        self.pm = pm\n        self.remove_mods = remove_mods\n\n    def __getattr__(self, name: str):\n        x = self.pm.subset_hook_caller(name, remove_plugins=self.remove_mods)\n        self.__dict__[name] = x\n        return x\n\n\nclass FSCollector(Collector):\n    def __init__(\n        self, fspath: py.path.local, parent=None, config=None, session=None, nodeid=None\n    ) -> None:\n        name = fspath.basename\n        if parent is not None:\n            rel = fspath.relto(parent.fspath)\n            if rel:\n                name = rel\n            name = name.replace(os.sep, SEP)\n        self.fspath = fspath\n\n        session = session or parent.session\n\n        if nodeid is None:\n            nodeid = self.fspath.relto(session.config.rootdir)\n\n            if not nodeid:\n                nodeid = _check_initialpaths_for_relpath(session, fspath)\n            if nodeid and os.sep != SEP:\n                nodeid = nodeid.replace(os.sep, SEP)\n\n        super().__init__(name, parent, config, session, nodeid=nodeid, fspath=fspath)\n\n        self._norecursepatterns = self.config.getini(\"norecursedirs\")\n\n    @classmethod\n    def from_parent(cls, parent, *, fspath, **kw):\n        \"\"\"\n        The public constructor\n        \"\"\"\n        return super().from_parent(parent=parent, fspath=fspath, **kw)\n\n    def _gethookproxy(self, fspath: py.path.local):\n        # check if we have the common case of running\n        # hooks with all conftest.py files\n        pm = self.config.pluginmanager\n        my_conftestmodules = pm._getconftestmodules(fspath)\n        remove_mods = pm._conftest_plugins.difference(my_conftestmodules)\n        if remove_mods:\n            # one or more conftests are not in use at this fspath\n            proxy = FSHookProxy(pm, remove_mods)\n        else:\n            # all plugins are active for this fspath\n            proxy = self.config.hook\n        return proxy\n\n    def _recurse(self, dirpath: py.path.local) -> bool:\n        if dirpath.basename == \"__pycache__\":\n            return False\n        ihook = self._gethookproxy(dirpath.dirpath())\n        if ihook.pytest_ignore_collect(path=dirpath, config=self.config):\n            return False\n        for pat in self._norecursepatterns:\n            if dirpath.check(fnmatch=pat):\n                return False\n        ihook = self._gethookproxy(dirpath)\n        ihook.pytest_collect_directory(path=dirpath, parent=self)\n        return True\n\n    def _collectfile(self, path, handle_dupes=True):\n        assert (\n            path.isfile()\n        ), \"{!r} is not a file (isdir={!r}, exists={!r}, islink={!r})\".format(\n            path, path.isdir(), path.exists(), path.islink()\n        )\n        ihook = self.gethookproxy(path)\n        if not self.isinitpath(path):\n            if ihook.pytest_ignore_collect(path=path, config=self.config):\n                return ()\n\n        if handle_dupes:\n            keepduplicates = self.config.getoption(\"keepduplicates\")\n            if not keepduplicates:\n                duplicate_paths = self.config.pluginmanager._duplicatepaths\n                if path in duplicate_paths:\n                    return ()\n                else:\n                    duplicate_paths.add(path)", "answer": "A"}
{"uuid": "24f1955d-ec27-4e5f-a26c-f819a63ad983", "issue_statement": "Incorrect caching of skipif/xfail string condition evaluation\nVersion: pytest 5.4.3, current master\r\n\r\npytest caches the evaluation of the string in e.g. `@pytest.mark.skipif(\"sys.platform == 'win32'\")`. The caching key is only the string itself (see `cached_eval` in `_pytest/mark/evaluate.py`). However, the evaluation also depends on the item's globals, so the caching can lead to incorrect results. Example:\r\n\r\n```py\r\n# test_module_1.py\r\nimport pytest\r\n\r\nskip = True\r\n\r\n@pytest.mark.skipif(\"skip\")\r\ndef test_should_skip():\r\n    assert False\r\n```\r\n\r\n```py\r\n# test_module_2.py\r\nimport pytest\r\n\r\nskip = False\r\n\r\n@pytest.mark.skipif(\"skip\")\r\ndef test_should_not_skip():\r\n    assert False\r\n```\r\n\r\nRunning `pytest test_module_1.py test_module_2.py`.\r\n\r\nExpected: `test_should_skip` is skipped, `test_should_not_skip` is not skipped.\r\n\r\nActual: both are skipped.\r\n\r\n---\r\n\r\nI think the most appropriate fix is to simply remove the caching, which I don't think is necessary really, and inline `cached_eval` into `MarkEvaluator._istrue`.", "choices": "(A) from ..outcomes import fail\nfrom ..outcomes import TEST_OUTCOME\nfrom .structures import Mark\nfrom _pytest.config import Config\nfrom _pytest.nodes import Item\nfrom _pytest.store import StoreKey\n\n\nevalcache_key = StoreKey[Dict[str, Any]]()\n\n\ndef cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n    default = {}  # type: Dict[str, object]\n    evalcache = config._store.setdefault(evalcache_key, default)\n    try:\n        return evalcache[expr]\n    except KeyError:\n        import _pytest._code\n\n        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n        evalcache[expr] = x = eval(exprcode, d)\n        return x\n\n\nclass MarkEvaluator:\n    def __init__(self, item: Item, name: str) -> None:\n        self.item = item\n        self._marks = None  # type: Optional[List[Mark]]\n        self._mark = None  # type: Optional[Mark]\n        self._mark_name = name\n\n    def __bool__(self) -> bool:\n        # don't cache here to prevent staleness\n        return bool(self._get_marks())\n\n    def wasvalid(self) -> bool:\n        return not hasattr(self, \"exc\")\n\n    def _get_marks(self) -> List[Mark]:\n        return list(self.item.iter_markers(name=self._mark_name))\n\n    def invalidraise(self, exc) -> Optional[bool]:\n        raises = self.get(\"raises\")\n        if not raises:\n            return None\n        return not isinstance(exc, raises)\n\n    def istrue(self) -> bool:\n        try:\n            return self._istrue()\n        except TEST_OUTCOME:\n            self.exc = sys.exc_info()\n            if isinstance(self.exc[1], SyntaxError):\n                # TODO: Investigate why SyntaxError.offset is Optional, and if it can be None here.\n                assert self.exc[1].offset is not None\n                msg = [\" \" * (self.exc[1].offset + 4) + \"^\"]\n                msg.append(\"SyntaxError: invalid syntax\")\n            else:\n                msg = traceback.format_exception_only(*self.exc[:2])\n            fail(\n                \"Error evaluating %r expression\\n\"\n                \"    %s\\n\"\n                \"%s\" % (self._mark_name, self.expr, \"\\n\".join(msg)),\n                pytrace=False,\n            )\n\n    def _getglobals(self) -> Dict[str, object]:\n        d = {\"os\": os, \"sys\": sys, \"platform\": platform, \"config\": self.item.config}\n        if hasattr(self.item, \"obj\"):\n            d.update(self.item.obj.__globals__)  # type: ignore[attr-defined] # noqa: F821\n        return d\n\n    def _istrue(self) -> bool:\n        if hasattr(self, \"result\"):\n            result = getattr(self, \"result\")  # type: bool\n            return result\n        self._marks = self._get_marks()\n\n        if self._marks:\n            self.result = False\n            for mark in self._marks:\n                self._mark = mark\n                if \"condition\" not in mark.kwargs:\n                    args = mark.args\n(B)         return x\n\n\nclass MarkEvaluator:\n    def __init__(self, item: Item, name: str) -> None:\n        self.item = item\n        self._marks = None  # type: Optional[List[Mark]]\n        self._mark = None  # type: Optional[Mark]\n        self._mark_name = name\n\n    def __bool__(self) -> bool:\n        # don't cache here to prevent staleness\n        return bool(self._get_marks())\n\n    def wasvalid(self) -> bool:\n        return not hasattr(self, \"exc\")\n\n    def _get_marks(self) -> List[Mark]:\n        return list(self.item.iter_markers(name=self._mark_name))\n\n    def invalidraise(self, exc) -> Optional[bool]:\n        raises = self.get(\"raises\")\n        if not raises:\n            return None\n        return not isinstance(exc, raises)\n\n    def istrue(self) -> bool:\n        try:\n            return self._istrue()\n        except TEST_OUTCOME:\n            self.exc = sys.exc_info()\n            if isinstance(self.exc[1], SyntaxError):\n                # TODO: Investigate why SyntaxError.offset is Optional, and if it can be None here.\n                assert self.exc[1].offset is not None\n                msg = [\" \" * (self.exc[1].offset + 4) + \"^\"]\n                msg.append(\"SyntaxError: invalid syntax\")\n            else:\n                msg = traceback.format_exception_only(*self.exc[:2])\n            fail(\n                \"Error evaluating %r expression\\n\"\n                \"    %s\\n\"\n                \"%s\" % (self._mark_name, self.expr, \"\\n\".join(msg)),\n                pytrace=False,\n            )\n\n    def _getglobals(self) -> Dict[str, object]:\n        d = {\"os\": os, \"sys\": sys, \"platform\": platform, \"config\": self.item.config}\n        if hasattr(self.item, \"obj\"):\n            d.update(self.item.obj.__globals__)  # type: ignore[attr-defined] # noqa: F821\n        return d\n\n    def _istrue(self) -> bool:\n        if hasattr(self, \"result\"):\n            result = getattr(self, \"result\")  # type: bool\n            return result\n        self._marks = self._get_marks()\n\n        if self._marks:\n            self.result = False\n            for mark in self._marks:\n                self._mark = mark\n                if \"condition\" not in mark.kwargs:\n                    args = mark.args\n                else:\n                    args = (mark.kwargs[\"condition\"],)\n\n                for expr in args:\n                    self.expr = expr\n                    if isinstance(expr, str):\n                        d = self._getglobals()\n                        result = cached_eval(self.item.config, expr, d)\n                    else:\n                        if \"reason\" not in mark.kwargs:\n                            # XXX better be checked at collection time\n                            msg = (\n                                \"you need to specify reason=STRING \"\n                                \"when using booleans as conditions.\"\n                            )\n                            fail(msg)\n                        result = bool(expr)\n                    if result:\n                        self.result = True\n                        self.reason = mark.kwargs.get(\"reason\", None)\n                        self.expr = expr\n(C)         raises = self.get(\"raises\")\n        if not raises:\n            return None\n        return not isinstance(exc, raises)\n\n    def istrue(self) -> bool:\n        try:\n            return self._istrue()\n        except TEST_OUTCOME:\n            self.exc = sys.exc_info()\n            if isinstance(self.exc[1], SyntaxError):\n                # TODO: Investigate why SyntaxError.offset is Optional, and if it can be None here.\n                assert self.exc[1].offset is not None\n                msg = [\" \" * (self.exc[1].offset + 4) + \"^\"]\n                msg.append(\"SyntaxError: invalid syntax\")\n            else:\n                msg = traceback.format_exception_only(*self.exc[:2])\n            fail(\n                \"Error evaluating %r expression\\n\"\n                \"    %s\\n\"\n                \"%s\" % (self._mark_name, self.expr, \"\\n\".join(msg)),\n                pytrace=False,\n            )\n\n    def _getglobals(self) -> Dict[str, object]:\n        d = {\"os\": os, \"sys\": sys, \"platform\": platform, \"config\": self.item.config}\n        if hasattr(self.item, \"obj\"):\n            d.update(self.item.obj.__globals__)  # type: ignore[attr-defined] # noqa: F821\n        return d\n\n    def _istrue(self) -> bool:\n        if hasattr(self, \"result\"):\n            result = getattr(self, \"result\")  # type: bool\n            return result\n        self._marks = self._get_marks()\n\n        if self._marks:\n            self.result = False\n            for mark in self._marks:\n                self._mark = mark\n                if \"condition\" not in mark.kwargs:\n                    args = mark.args\n                else:\n                    args = (mark.kwargs[\"condition\"],)\n\n                for expr in args:\n                    self.expr = expr\n                    if isinstance(expr, str):\n                        d = self._getglobals()\n                        result = cached_eval(self.item.config, expr, d)\n                    else:\n                        if \"reason\" not in mark.kwargs:\n                            # XXX better be checked at collection time\n                            msg = (\n                                \"you need to specify reason=STRING \"\n                                \"when using booleans as conditions.\"\n                            )\n                            fail(msg)\n                        result = bool(expr)\n                    if result:\n                        self.result = True\n                        self.reason = mark.kwargs.get(\"reason\", None)\n                        self.expr = expr\n                        return self.result\n\n                if not args:\n                    self.result = True\n                    self.reason = mark.kwargs.get(\"reason\", None)\n                    return self.result\n        return False\n\n    def get(self, attr, default=None):\n        if self._mark is None:\n            return default\n        return self._mark.kwargs.get(attr, default)\n\n    def getexplanation(self):\n        expl = getattr(self, \"reason\", None) or self.get(\"reason\", None)\n        if not expl:\n            if not hasattr(self, \"expr\"):\n                return \"\"\n            else:\n                return \"condition: \" + str(self.expr)\n        return expl", "answer": "A"}
{"uuid": "d8a4d23f-dd7e-48c7-82b8-0405ef1fde00", "issue_statement": "skipping: --runxfail breaks pytest.mark.skip location reporting\npytest versions: 5.4.x, current master\r\n\r\nWhen `@pytest.mark.skip`/`skipif` marks are used to skip a test, for example\r\n\r\n```py\r\nimport pytest\r\n@pytest.mark.skip\r\ndef test_skip_location() -> None:\r\n    assert 0\r\n```\r\n\r\nthe expected skip location reported should point to the item itself, and this is indeed what happens when running with `pytest -rs`:\r\n\r\n```\r\nSKIPPED [1] test_it.py:3: unconditional skip\r\n```\r\n\r\nHowever, adding `pytest -rs --runxfail` breaks this:\r\n\r\n```\r\nSKIPPED [1] src/_pytest/skipping.py:238: unconditional skip\r\n```\r\n\r\nThe `--runxfail` is only about xfail and should not affect this at all.\r\n\r\n---\r\n\r\nHint: the bug is in `src/_pytest/skipping.py`, the `pytest_runtest_makereport` hook.", "choices": "(A)                 rep.outcome = \"failed\"\n                rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n            else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n(B)                 rep.wasxfail = xfailed.reason\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n(C)         item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):\n        # skipped by mark.skipif; change the location of the failure\n        # to point to the item definition, otherwise it will display\n        # the location of where the skip exception was raised within pytest\n        _, _, reason = rep.longrepr\n(D)             else:\n                rep.outcome = \"passed\"\n                rep.wasxfail = xfailed.reason\n    elif (\n        item._store.get(skipped_by_mark_key, True)\n        and rep.skipped\n        and type(rep.longrepr) is tuple\n    ):", "answer": "D"}
{"uuid": "233cfe13-6eab-4024-b984-3b1d37aa787c", "issue_statement": "Pytest 6: Dynamically adding xfail marker in test no longer ignores failure\n<!--\r\nThanks for submitting an issue!\r\n\r\nHere's a quick checklist for what to provide:\r\n-->\r\n\r\n## Description\r\n\r\nWith pytest 5.x, we can dynamically add an xfail to a test `request` object using `request.node.add_marker(mark)` (see example below). In 5.x this treated the failing test like a a test marked statically with an `xfail`. With 6.0.0rc0 it raises. \r\n\r\n## Versions\r\n\r\n<details>\r\n\r\n```\r\n$ pip list\r\nPackage                       Version                         Location                                                      \r\n----------------------------- ------------------------------- --------------------------------------------------------------\r\na                             1.0                             \r\naioftp                        0.13.0                          \r\naiohttp                       3.6.2                           \r\nalabaster                     0.7.12                          \r\napipkg                        1.5                             \r\naplus                         0.11.0                          \r\nappdirs                       1.4.3                           \r\nappnope                       0.1.0                           \r\narrow                         0.15.7                          \r\naspy.yaml                     1.3.0                           \r\nastropy                       3.2.3                           \r\nasv                           0.4.1                           \r\nasync-timeout                 3.0.1                           \r\natomicwrites                  1.3.0                           \r\nattrs                         19.1.0                          \r\naws-sam-translator            1.15.1                          \r\naws-xray-sdk                  0.95                            \r\nBabel                         2.7.0                           \r\nbackcall                      0.1.0                           \r\nbinaryornot                   0.4.4                           \r\nblack                         19.10b0                         \r\nbleach                        3.1.0                           \r\nblurb                         1.0.7                           \r\nbokeh                         1.3.4                           \r\nboto                          2.49.0                          \r\nboto3                         1.7.84                          \r\nbotocore                      1.10.84                         \r\nbqplot                        0.12.12                         \r\nbranca                        0.3.1                           \r\ncachetools                    4.1.0                           \r\ncertifi                       2019.9.11                       \r\ncffi                          1.13.2                          \r\ncfgv                          2.0.1                           \r\ncfn-lint                      0.25.0                          \r\ncftime                        1.0.4.2                         \r\nchardet                       3.0.4                           \r\nClick                         7.0                             \r\nclick-plugins                 1.1.1                           \r\ncligj                         0.5.0                           \r\ncloudpickle                   1.2.2                           \r\ncolorama                      0.4.3                           \r\ncolorcet                      2.0.2                           \r\ncoloredlogs                   14.0                            \r\ncookiecutter                  1.7.2                           \r\ncookies                       2.2.1                           \r\ncoverage                      4.5.4                           \r\ncryptography                  2.8                             \r\ncycler                        0.10.0                          \r\nCython                        3.0a5                           \r\ncytoolz                       0.10.1                          \r\ndask                          2.4.0                           /Users/taugspurger/Envs/pandas-dev/lib/python3.7/site-packages\r\nDateTime                      4.3                             \r\ndecorator                     4.4.0                           \r\ndefusedxml                    0.6.0                           \r\nDeprecated                    1.2.7                           \r\ndistributed                   2.4.0                           \r\ndocker                        4.1.0                           \r\ndocutils                      0.15.2                          \r\necdsa                         0.14.1                          \r\nentrypoints                   0.3                             \r\net-xmlfile                    1.0.1                           \r\nexecnet                       1.7.1                           \r\nfastparquet                   0.3.3                           /Users/taugspurger/sandbox/fastparquet                        \r\nfeedparser                    5.2.1                           \r\nFiona                         1.8.8                           \r\nflake8                        3.7.9                           \r\nflake8-rst                    0.7.1                           \r\nfletcher                      0.3.1                           \r\nflit                          2.1.0                           \r\nflit-core                     2.1.0                           \r\nfsspec                        0.7.4                           \r\nfuture                        0.18.2                          \r\ngcsfs                         0.6.2                           \r\ngeopandas                     0.6.0+1.g95b8e1a.dirty          /Users/taugspurger/sandbox/geopandas                          \r\ngitdb2                        2.0.5                           \r\nGitPython                     3.0.2                           \r\ngoogle-auth                   1.16.1                          \r\ngoogle-auth-oauthlib          0.4.1                           \r\ngraphviz                      0.13                            \r\nh5py                          2.10.0                          \r\nHeapDict                      1.0.1                           \r\nholoviews                     1.12.6                          \r\nhumanfriendly                 8.1                             \r\nhunter                        3.1.3                           \r\nhvplot                        0.5.2                           \r\nhypothesis                    4.36.2                          \r\nidentify                      1.4.7                           \r\nidna                          2.8                             \r\nimagesize                     1.1.0                           \r\nimportlib-metadata            0.23                            \r\nimportlib-resources           1.0.2                           \r\niniconfig                     1.0.0                           \r\nintake                        0.5.3                           \r\nipydatawidgets                4.0.1                           \r\nipykernel                     5.1.2                           \r\nipyleaflet                    0.13.0                          \r\nipympl                        0.5.6                           \r\nipython                       7.11.1                          \r\nipython-genutils              0.2.0                           \r\nipyvolume                     0.5.2                           \r\nipyvue                        1.3.2                           \r\nipyvuetify                    1.4.0                           \r\nipywebrtc                     0.5.0                           \r\nipywidgets                    7.5.1                           \r\nisort                         4.3.21                          \r\njdcal                         1.4.1                           \r\njedi                          0.16.0                          \r\nJinja2                        2.11.2                          \r\njinja2-time                   0.2.0                           \r\njmespath                      0.9.4                           \r\njoblib                        0.14.1                          \r\njson5                         0.9.4                           \r\njsondiff                      1.1.1                           \r\njsonpatch                     1.24                            \r\njsonpickle                    1.2                             \r\njsonpointer                   2.0                             \r\njsonschema                    3.0.2                           \r\njupyter                       1.0.0                           \r\njupyter-client                5.3.3                           \r\njupyter-console               6.0.0                           \r\njupyter-core                  4.5.0                           \r\njupyterlab                    2.1.2                           \r\njupyterlab-server             1.1.4                           \r\nkiwisolver                    1.1.0                           \r\nline-profiler                 2.1.1                           \r\nllvmlite                      0.33.0                          \r\nlocket                        0.2.0                           /Users/taugspurger/sandbox/locket.py                          \r\nlxml                          4.5.0                           \r\nmanhole                       1.6.0                           \r\nMarkdown                      3.1.1                           \r\nMarkupSafe                    1.1.1                           \r\nmatplotlib                    3.2.2                           \r\nmccabe                        0.6.1                           \r\nmemory-profiler               0.55.0                          \r\nmistune                       0.8.4                           \r\nmock                          3.0.5                           \r\nmore-itertools                7.2.0                           \r\nmoto                          1.3.6                           \r\nmsgpack                       0.6.2                           \r\nmultidict                     4.5.2                           \r\nmunch                         2.3.2                           \r\nmypy                          0.730                           \r\nmypy-extensions               0.4.1                           \r\nnbconvert                     5.6.0                           \r\nnbformat                      4.4.0                           \r\nnbsphinx                      0.4.2                           \r\nnest-asyncio                  1.3.3                           \r\nnodeenv                       1.3.3                           \r\nnotebook                      6.0.1                           \r\nnumexpr                       2.7.1                           \r\nnumpy                         1.19.0                          \r\nnumpydoc                      1.0.0.dev0                      \r\noauthlib                      3.1.0                           \r\nodfpy                         1.4.0                           \r\nopenpyxl                      3.0.3                           \r\npackaging                     20.4                            \r\npandas                        1.1.0.dev0+1758.g035e1fe831     /Users/taugspurger/sandbox/pandas                             \r\npandas-sphinx-theme           0.0.1.dev0                      /Users/taugspurger/sandbox/pandas-sphinx-theme                \r\npandocfilters                 1.4.2                           \r\nparam                         1.9.2                           \r\nparfive                       1.0.0                           \r\nparso                         0.6.0                           \r\npartd                         1.0.0                           \r\npathspec                      0.8.0                           \r\npatsy                         0.5.1                           \r\npexpect                       4.7.0                           \r\npickleshare                   0.7.5                           \r\nPillow                        6.1.0                           \r\npip                           20.0.2                          \r\npluggy                        0.13.0                          \r\npoyo                          0.5.0                           \r\npre-commit                    1.18.3                          \r\nprogressbar2                  3.51.3                          \r\nprometheus-client             0.7.1                           \r\nprompt-toolkit                2.0.9                           \r\npsutil                        5.6.3                           \r\nptyprocess                    0.6.0                           \r\npy                            1.9.0                           \r\npyaml                         20.4.0                          \r\npyarrow                       0.16.0                          \r\npyasn1                        0.4.7                           \r\npyasn1-modules                0.2.8                           \r\npycodestyle                   2.5.0                           \r\npycparser                     2.19                            \r\npycryptodome                  3.9.8                           \r\npyct                          0.4.6                           \r\npydata-sphinx-theme           0.1.1                           \r\npydeps                        1.9.0                           \r\npyflakes                      2.1.1                           \r\nPyGithub                      1.44.1                          \r\nPygments                      2.4.2                           \r\nPyJWT                         1.7.1                           \r\npyparsing                     2.4.2                           \r\npyproj                        2.4.0                           \r\npyrsistent                    0.15.4                          \r\npytest                        5.4.3                           \r\npytest-asyncio                0.10.0                          \r\npytest-cov                    2.8.1                           \r\npytest-cover                  3.0.0                           \r\npytest-forked                 1.0.2                           \r\npytest-repeat                 0.8.0                           \r\npytest-xdist                  1.29.0                          \r\npython-boilerplate            0.1.0                           \r\npython-dateutil               2.8.0                           \r\npython-jose                   2.0.2                           \r\npython-jsonrpc-server         0.3.2                           \r\npython-language-server        0.31.4                          \r\npython-slugify                4.0.1                           \r\npython-utils                  2.4.0                           \r\npythreejs                     2.2.0                           \r\npytoml                        0.1.21                          \r\npytz                          2019.2                          \r\npyviz-comms                   0.7.2                           \r\nPyYAML                        5.1.2                           \r\npyzmq                         18.1.0                          \r\nqtconsole                     4.5.5                           \r\nregex                         2020.6.8                        \r\nrequests                      2.24.0                          \r\nrequests-oauthlib             1.3.0                           \r\nresponses                     0.10.6                          \r\nrsa                           4.0                             \r\nrstcheck                      3.3.1                           \r\ns3fs                          0.4.2                           \r\ns3transfer                    0.1.13                          \r\nscikit-learn                  0.22.2.post1                    \r\nscipy                         1.3.1                           \r\nseaborn                       0.9.0                           \r\nSend2Trash                    1.5.0                           \r\nsetuptools                    49.2.0                          \r\nShapely                       1.6.4.post2                     \r\nsix                           1.12.0                          \r\nsmmap2                        2.0.5                           \r\nsnakeviz                      2.0.1                           \r\nsnowballstemmer               1.9.1                           \r\nsortedcontainers              2.1.0                           \r\nsparse                        0.10.0                          \r\nSphinx                        3.1.1                           \r\nsphinxcontrib-applehelp       1.0.2                           \r\nsphinxcontrib-devhelp         1.0.2                           \r\nsphinxcontrib-htmlhelp        1.0.3                           \r\nsphinxcontrib-jsmath          1.0.1                           \r\nsphinxcontrib-qthelp          1.0.3                           \r\nsphinxcontrib-serializinghtml 1.1.4                           \r\nsphinxcontrib-websupport      1.1.2                           \r\nsphinxcontrib.youtube         0.1.2                           \r\nSQLAlchemy                    1.3.11                          \r\nsshpubkeys                    3.1.0                           \r\nstatsmodels                   0.10.2                          \r\nstdlib-list                   0.6.0                           \r\nsunpy                         1.1.dev518+gcad2d473f.d20191103 /Users/taugspurger/sandbox/sunpy                              \r\ntables                        3.6.1                           \r\ntabulate                      0.8.6                           \r\ntblib                         1.4.0                           \r\nterminado                     0.8.2                           \r\ntest                          1.0.0                           \r\ntestpath                      0.4.2                           \r\ntext-unidecode                1.3                             \r\nthrift                        0.13.0                          \r\ntoml                          0.10.0                          \r\ntoolz                         0.10.0                          \r\ntornado                       6.0.3                           \r\ntqdm                          4.37.0                          \r\ntraitlets                     4.3.2                           \r\ntraittypes                    0.2.1                           \r\ntyped-ast                     1.4.0                           \r\ntyping-extensions             3.7.4                           \r\nujson                         1.35                            \r\nurllib3                       1.25.5                          \r\nvaex                          3.0.0                           \r\nvaex-arrow                    0.5.1                           \r\nvaex-astro                    0.7.0                           \r\nvaex-core                     2.0.2                           \r\nvaex-hdf5                     0.6.0                           \r\nvaex-jupyter                  0.5.1.post0                     \r\nvaex-ml                       0.9.0                           \r\nvaex-server                   0.3.1                           \r\nvaex-viz                      0.4.0                           \r\nvirtualenv                    16.7.5                          \r\nwcwidth                       0.1.7                           \r\nwebencodings                  0.5.1                           \r\nwebsocket-client              0.56.0                          \r\nWerkzeug                      0.16.0                          \r\nwheel                         0.34.2                          \r\nwidgetsnbextension            3.5.1                           \r\nwrapt                         1.11.2                          \r\nxarray                        0.14.1+36.gb3d3b448             /Users/taugspurger/sandbox/xarray                             \r\nxlwt                          1.3.0                           \r\nxmltodict                     0.12.0                          \r\nyarl                          1.3.0                           \r\nzict                          1.0.0                           \r\nzipp                          0.6.0                           \r\nzope.interface                4.7.1                           \r\n```\r\n\r\n</details>\r\n\r\n- [ ] pytest and operating system versions\r\n\r\nPytest 6.0.1rc0 and MacOS 10.14.5\r\n\r\n```python\r\n# file: test_foo.py\r\nimport pytest\r\n\r\n\r\ndef test_xfail_test(request):\r\n    mark = pytest.mark.xfail(reason=\"xfail\")\r\n    request.node.add_marker(mark)\r\n    assert 0\r\n```\r\n\r\nWith 5.4.3\r\n\r\n```\r\n\r\n$ pytest -rsx test_foo.py\r\n=============================================================================== test session starts ================================================================================\r\nplatform darwin -- Python 3.7.6, pytest-5.4.3, py-1.9.0, pluggy-0.13.0\r\nhypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/Users/taugspurger/sandbox/.hypothesis/examples')\r\nrootdir: /Users/taugspurger/sandbox\r\nplugins: xdist-1.29.0, hypothesis-4.36.2, forked-1.0.2, repeat-0.8.0, asyncio-0.10.0, cov-2.8.1\r\ncollected 1 item\r\n\r\ntest_foo.py x                                                                                                                                                                [100%]\r\n\r\n============================================================================= short test summary info ==============================================================================\r\nXFAIL test_foo.py::test_xfail_test\r\n  xfail\r\n================================================================================ 1 xfailed in 0.07s ================================================================================\r\n```\r\n\r\nWith 6.0.0rc0\r\n\r\n```\r\n$ pytest -rsx test_foo.py\r\n=============================================================================== test session starts ================================================================================\r\nplatform darwin -- Python 3.7.6, pytest-6.0.0rc1, py-1.9.0, pluggy-0.13.0\r\nhypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/Users/taugspurger/sandbox/.hypothesis/examples')\r\nrootdir: /Users/taugspurger/sandbox\r\nplugins: xdist-1.29.0, hypothesis-4.36.2, forked-1.0.2, repeat-0.8.0, asyncio-0.10.0, cov-2.8.1\r\ncollected 1 item\r\n\r\ntest_foo.py F                                                                                                                                                                [100%]\r\n\r\n===================================================================================== FAILURES =====================================================================================\r\n_________________________________________________________________________________ test_xfail_test __________________________________________________________________________________\r\n\r\nrequest = <FixtureRequest for <Function test_xfail_test>>\r\n\r\n    def test_xfail_test(request):\r\n        mark = pytest.mark.xfail(reason=\"xfail\")\r\n        request.node.add_marker(mark)\r\n>       assert 0\r\nE       assert 0\r\n\r\ntest_foo.py:7: AssertionError\r\n```", "choices": "(A)         skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n(B) @hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield\n    rep = outcome.get_result()\n    xfailed = item._store.get(xfailed_key, None)\n    # unittest special case, see setting of unexpectedsuccess_key\n    if unexpectedsuccess_key in item._store and rep.when == \"call\":\n        reason = item._store[unexpectedsuccess_key]\n        if reason:\n            rep.longrepr = \"Unexpected success: {}\".format(reason)\n        else:\n            rep.longrepr = \"Unexpected success\"\n        rep.outcome = \"failed\"\n    elif item.config.option.runxfail:\n        pass  # don't interfere\n    elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n        assert call.excinfo.value.msg is not None\n        rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n        rep.outcome = \"skipped\"\n(C) \n\n# Whether skipped due to skip or skipif marks.\nskipped_by_mark_key = StoreKey[bool]()\n# Saves the xfail mark evaluation. Can be refreshed during call if None.\nxfailed_key = StoreKey[Optional[Xfail]]()\nunexpectedsuccess_key = StoreKey[str]()\n\n\n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n(D) \n@hookimpl(tryfirst=True)\ndef pytest_runtest_setup(item: Item) -> None:\n    item._store[skipped_by_mark_key] = False\n\n    skipped = evaluate_skip_marks(item)\n    if skipped:\n        item._store[skipped_by_mark_key] = True\n        skip(skipped.reason)\n\n    if not item.config.option.runxfail:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n    xfailed = item._store.get(xfailed_key, None)\n    if xfailed is None:\n        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n\n    if not item.config.option.runxfail:\n        if xfailed and not xfailed.run:\n            xfail(\"[NOTRUN] \" + xfailed.reason)\n\n    yield\n\n\n@hookimpl(hookwrapper=True)\ndef pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n    outcome = yield", "answer": "D"}
{"uuid": "2dd01d99-cdad-48d8-80c3-7624abb5c9bf", "issue_statement": "tmpdir creation fails when the username contains illegal characters for directory names\n`tmpdir`, `tmpdir_factory` and `tmp_path_factory` rely on `getpass.getuser()` for determining the `basetemp` directory. I found that the user name returned by `getpass.getuser()` may return characters that are not allowed for directory names. This may lead to errors while creating the temporary directory.\r\n\r\nThe situation in which I reproduced this issue was while being logged in through an ssh connection into my Windows 10 x64 Enterprise version (1909) using an OpenSSH_for_Windows_7.7p1 server. In this configuration the command `python -c \"import getpass; print(getpass.getuser())\"` returns my domain username e.g. `contoso\\john_doe` instead of `john_doe` as when logged in regularly using a local session.\r\n\r\nWhen trying to create a temp directory in pytest through e.g. `tmpdir_factory.mktemp('foobar')` this fails with the following error message:\r\n```\r\nself = WindowsPath('C:/Users/john_doe/AppData/Local/Temp/pytest-of-contoso/john_doe')\r\nmode = 511, parents = False, exist_ok = True\r\n\r\n    def mkdir(self, mode=0o777, parents=False, exist_ok=False):\r\n        \"\"\"\r\n        Create a new directory at this given path.\r\n        \"\"\"\r\n        if self._closed:\r\n            self._raise_closed()\r\n        try:\r\n>           self._accessor.mkdir(self, mode)\r\nE           FileNotFoundError: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\john_doe\\\\AppData\\\\Local\\\\Temp\\\\pytest-of-contoso\\\\john_doe'\r\n\r\nC:\\Python38\\lib\\pathlib.py:1266: FileNotFoundError\r\n```\r\n\r\nI could also reproduce this without the complicated ssh/windows setup with pytest 6.2.2 using the following commands from a `cmd`:\r\n```bat\r\necho def test_tmpdir(tmpdir):>test_tmp.py\r\necho   pass>>test_tmp.py\r\nset LOGNAME=contoso\\john_doe\r\npy.test test_tmp.py\r\n```\r\n\r\nThanks for having a look at this!", "choices": "(A)             rootdir.mkdir(exist_ok=True)\n            basetemp = make_numbered_dir_with_cleanup(\n                prefix=\"pytest-\", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT\n            )\n        assert basetemp is not None, basetemp\n        self._basetemp = t = basetemp\n        self._trace(\"new basetemp\", t)\n        return t\n\n\n@final\n@attr.s(init=False)\n(B)             from_env = os.environ.get(\"PYTEST_DEBUG_TEMPROOT\")\n            temproot = Path(from_env or tempfile.gettempdir()).resolve()\n            user = get_user() or \"unknown\"\n            # use a sub-directory in the temproot to speed-up\n            # make_numbered_dir() call\n            rootdir = temproot.joinpath(f\"pytest-of-{user}\")\n            rootdir.mkdir(exist_ok=True)\n            basetemp = make_numbered_dir_with_cleanup(\n                prefix=\"pytest-\", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT\n            )\n        assert basetemp is not None, basetemp\n        self._basetemp = t = basetemp\n(C)             # use a sub-directory in the temproot to speed-up\n            # make_numbered_dir() call\n            rootdir = temproot.joinpath(f\"pytest-of-{user}\")\n            rootdir.mkdir(exist_ok=True)\n            basetemp = make_numbered_dir_with_cleanup(\n                prefix=\"pytest-\", root=rootdir, keep=3, lock_timeout=LOCK_TIMEOUT\n            )\n        assert basetemp is not None, basetemp\n        self._basetemp = t = basetemp\n        self._trace(\"new basetemp\", t)\n        return t\n\n(D)             )\n        assert basetemp is not None, basetemp\n        self._basetemp = t = basetemp\n        self._trace(\"new basetemp\", t)\n        return t\n\n\n@final\n@attr.s(init=False)\nclass TempdirFactory:\n    \"\"\"Backward comptibility wrapper that implements :class:``py.path.local``\n    for :class:``TempPathFactory``.\"\"\"", "answer": "C"}
{"uuid": "3490716e-b2e1-4c92-95a1-ab28fedc34d4", "issue_statement": "Improve handling of skip for module level\nThis is potentially about updating docs, updating error messages or introducing a new API.\r\n\r\nConsider the following scenario:\r\n\r\n`pos_only.py` is using Python 3,8 syntax:\r\n```python\r\ndef foo(a, /, b):\r\n    return a + b\r\n```\r\n\r\nIt should not be tested under Python 3.6 and 3.7.\r\nThis is a proper way to skip the test in Python older than 3.8:\r\n```python\r\nfrom pytest import raises, skip\r\nimport sys\r\nif sys.version_info < (3, 8):\r\n    skip(msg=\"Requires Python >= 3.8\", allow_module_level=True)\r\n\r\n# import must be after the module level skip:\r\nfrom pos_only import *\r\n\r\ndef test_foo():\r\n    assert foo(10, 20) == 30\r\n    assert foo(10, b=20) == 30\r\n    with raises(TypeError):\r\n        assert foo(a=10, b=20)\r\n```\r\n\r\nMy actual test involves parameterize and a 3.8 only class, so skipping the test itself is not sufficient because the 3.8 class was used in the parameterization.\r\n\r\nA naive user will try to initially skip the module like:\r\n\r\n```python\r\nif sys.version_info < (3, 8):\r\n    skip(msg=\"Requires Python >= 3.8\")\r\n```\r\nThis issues this error:\r\n\r\n>Using pytest.skip outside of a test is not allowed. To decorate a test function, use the @pytest.mark.skip or @pytest.mark.skipif decorators instead, and to skip a module use `pytestmark = pytest.mark.{skip,skipif}.\r\n\r\nThe proposed solution `pytestmark = pytest.mark.{skip,skipif}`, does not work  in my case: pytest continues to process the file and fail when it hits the 3.8 syntax (when running with an older version of Python).\r\n\r\nThe correct solution, to use skip as a function is actively discouraged by the error message.\r\n\r\nThis area feels a bit unpolished.\r\nA few ideas to improve:\r\n\r\n1. Explain skip with  `allow_module_level` in the error message. this seems in conflict with the spirit of the message.\r\n2. Create an alternative API to skip a module to make things easier: `skip_module(\"reason\")`, which can call `_skip(msg=msg, allow_module_level=True)`.", "choices": "(A)             if e.allow_module_level:\n                raise\n            raise self.CollectError(\n                \"Using pytest.skip outside of a test is not allowed. \"\n                \"To decorate a test function, use the @pytest.mark.skip \"\n                \"or @pytest.mark.skipif decorators instead, and to skip a \"\n                \"module use `pytestmark = pytest.mark.{skip,skipif}.\"\n            ) from e\n        self.config.pluginmanager.consider_module(mod)\n        return mod\n(B)                 \"or @pytest.mark.skipif decorators instead, and to skip a \"\n                \"module use `pytestmark = pytest.mark.{skip,skipif}.\"\n            ) from e\n        self.config.pluginmanager.consider_module(mod)\n        return mod\n\n\nclass Package(Module):\n    def __init__(\n        self,\n(C)                 \"Using pytest.skip outside of a test is not allowed. \"\n                \"To decorate a test function, use the @pytest.mark.skip \"\n                \"or @pytest.mark.skipif decorators instead, and to skip a \"\n                \"module use `pytestmark = pytest.mark.{skip,skipif}.\"\n            ) from e\n        self.config.pluginmanager.consider_module(mod)\n        return mod\n\n\nclass Package(Module):\n(D)                 \"{traceback}\".format(path=self.path, traceback=formatted_tb)\n            ) from e\n        except skip.Exception as e:\n            if e.allow_module_level:\n                raise\n            raise self.CollectError(\n                \"Using pytest.skip outside of a test is not allowed. \"\n                \"To decorate a test function, use the @pytest.mark.skip \"\n                \"or @pytest.mark.skipif decorators instead, and to skip a \"\n                \"module use `pytestmark = pytest.mark.{skip,skipif}.\"", "answer": "A"}
{"uuid": "ea90a96a-b9d9-4a18-89eb-498ccbd6c7d8", "issue_statement": "Error message prints extra code line when using assert in python3.9\n<!--\r\nThanks for submitting an issue!\r\n\r\nQuick check-list while reporting bugs:\r\n-->\r\n\r\n- [x] a detailed description of the bug or problem you are having\r\n- [x] output of `pip list` from the virtual environment you are using\r\n- [x] pytest and operating system versions\r\n- [ ] minimal example if possible\r\n### Description\r\nI have a test like this:\r\n```\r\nfrom pytest import fixture\r\n\r\n\r\ndef t(foo):\r\n    return foo\r\n\r\n\r\n@fixture\r\ndef foo():\r\n    return 1\r\n\r\n\r\ndef test_right_statement(foo):\r\n    assert foo == (3 + 2) * (6 + 9)\r\n\r\n    @t\r\n    def inner():\r\n        return 2\r\n\r\n    assert 2 == inner\r\n\r\n\r\n@t\r\ndef outer():\r\n    return 2\r\n```\r\nThe test \"test_right_statement\" fails at the first assertion,but print extra code (the \"t\" decorator) in error details, like this:\r\n\r\n```\r\n ============================= test session starts =============================\r\nplatform win32 -- Python 3.9.6, pytest-6.2.5, py-1.10.0, pluggy-0.13.1 -- \r\ncachedir: .pytest_cache\r\nrootdir: \r\nplugins: allure-pytest-2.9.45\r\ncollecting ... collected 1 item\r\n\r\ntest_statement.py::test_right_statement FAILED                           [100%]\r\n\r\n================================== FAILURES ===================================\r\n____________________________ test_right_statement _____________________________\r\n\r\nfoo = 1\r\n\r\n    def test_right_statement(foo):\r\n>       assert foo == (3 + 2) * (6 + 9)\r\n    \r\n        @t\r\nE       assert 1 == 75\r\nE         +1\r\nE         -75\r\n\r\ntest_statement.py:14: AssertionError\r\n=========================== short test summary info ===========================\r\nFAILED test_statement.py::test_right_statement - assert 1 == 75\r\n============================== 1 failed in 0.12s ==============================\r\n```\r\nAnd the same thing **did not** happen when using python3.7.10\uff1a\r\n```\r\n============================= test session starts =============================\r\nplatform win32 -- Python 3.7.10, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- \r\ncachedir: .pytest_cache\r\nrootdir: \r\ncollecting ... collected 1 item\r\n\r\ntest_statement.py::test_right_statement FAILED                           [100%]\r\n\r\n================================== FAILURES ===================================\r\n____________________________ test_right_statement _____________________________\r\n\r\nfoo = 1\r\n\r\n    def test_right_statement(foo):\r\n>       assert foo == (3 + 2) * (6 + 9)\r\nE       assert 1 == 75\r\nE         +1\r\nE         -75\r\n\r\ntest_statement.py:14: AssertionError\r\n=========================== short test summary info ===========================\r\nFAILED test_statement.py::test_right_statement - assert 1 == 75\r\n============================== 1 failed in 0.03s ==============================\r\n```\r\nIs there some problems when calculate the statement lineno?\r\n\r\n### pip list \r\n```\r\n$ pip list\r\nPackage            Version\r\n------------------ -------\r\natomicwrites       1.4.0\r\nattrs              21.2.0\r\ncolorama           0.4.4\r\nimportlib-metadata 4.8.2\r\niniconfig          1.1.1\r\npackaging          21.3\r\npip                21.3.1\r\npluggy             1.0.0\r\npy                 1.11.0\r\npyparsing          3.0.6\r\npytest             6.2.5\r\nsetuptools         59.4.0\r\ntoml               0.10.2\r\ntyping_extensions  4.0.0\r\nzipp               3.6.0\r\n\r\n```\r\n### pytest and operating system versions\r\npytest 6.2.5\r\nWindows 10 \r\nSeems to happen in python 3.9,not 3.7", "choices": "(A) def get_statement_startend2(lineno: int, node: ast.AST) -> Tuple[int, Optional[int]]:\n    # Flatten all statements and except handlers into one lineno-list.\n    # AST's line numbers start indexing at 1.\n    values: List[int] = []\n    for x in ast.walk(node):\n        if isinstance(x, (ast.stmt, ast.ExceptHandler)):\n            values.append(x.lineno - 1)\n            for name in (\"finalbody\", \"orelse\"):\n                val: Optional[List[ast.stmt]] = getattr(x, name, None)\n                if val:\n                    # Treat the finally/orelse part as its own statement.\n(B)                 if val:\n                    # Treat the finally/orelse part as its own statement.\n                    values.append(val[0].lineno - 1 - 1)\n    values.sort()\n    insert_index = bisect_right(values, lineno)\n    start = values[insert_index - 1]\n    if insert_index >= len(values):\n        end = None\n    else:\n        end = values[insert_index]\n    return start, end\n(C)     values: List[int] = []\n    for x in ast.walk(node):\n        if isinstance(x, (ast.stmt, ast.ExceptHandler)):\n            values.append(x.lineno - 1)\n            for name in (\"finalbody\", \"orelse\"):\n                val: Optional[List[ast.stmt]] = getattr(x, name, None)\n                if val:\n                    # Treat the finally/orelse part as its own statement.\n                    values.append(val[0].lineno - 1 - 1)\n    values.sort()\n    insert_index = bisect_right(values, lineno)\n(D)             values.append(x.lineno - 1)\n            for name in (\"finalbody\", \"orelse\"):\n                val: Optional[List[ast.stmt]] = getattr(x, name, None)\n                if val:\n                    # Treat the finally/orelse part as its own statement.\n                    values.append(val[0].lineno - 1 - 1)\n    values.sort()\n    insert_index = bisect_right(values, lineno)\n    start = values[insert_index - 1]\n    if insert_index >= len(values):\n        end = None", "answer": "C"}
{"uuid": "7919c25c-0319-4a4d-9708-23550c417201", "issue_statement": "linear_model.RidgeClassifierCV's Parameter store_cv_values issue\n#### Description\r\nParameter store_cv_values error on sklearn.linear_model.RidgeClassifierCV\r\n\r\n#### Steps/Code to Reproduce\r\nimport numpy as np\r\nfrom sklearn import linear_model as lm\r\n\r\n#test database\r\nn = 100\r\nx = np.random.randn(n, 30)\r\ny = np.random.normal(size = n)\r\n\r\nrr = lm.RidgeClassifierCV(alphas = np.arange(0.1, 1000, 0.1), normalize = True, \r\n                                         store_cv_values = True).fit(x, y)\r\n\r\n#### Expected Results\r\nExpected to get the usual ridge regression model output, keeping the cross validation predictions as attribute.\r\n\r\n#### Actual Results\r\nTypeError: __init__() got an unexpected keyword argument 'store_cv_values'\r\n\r\nlm.RidgeClassifierCV actually has no parameter store_cv_values, even though some attributes depends on it.\r\n\r\n#### Versions\r\nWindows-10-10.0.14393-SP0\r\nPython 3.6.3 |Anaconda, Inc.| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]\r\nNumPy 1.13.3\r\nSciPy 0.19.1\r\nScikit-Learn 0.19.1\r\n\r\n\nAdd store_cv_values boolean flag support to RidgeClassifierCV\nAdd store_cv_values support to RidgeClassifierCV - documentation claims that usage of this flag is possible:\n\n> cv_values_ : array, shape = [n_samples, n_alphas] or shape = [n_samples, n_responses, n_alphas], optional\n> Cross-validation values for each alpha (if **store_cv_values**=True and `cv=None`).\n\nWhile actually usage of this flag gives \n\n> TypeError: **init**() got an unexpected keyword argument 'store_cv_values'", "choices": "(A)             Target values. Will be cast to X's dtype if necessary\n\n        sample_weight : float or array-like of shape [n_samples]\n            Sample weight\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        if self.cv is None:\n            estimator = _RidgeGCV(self.alphas,\n                                  fit_intercept=self.fit_intercept,\n                                  normalize=self.normalize,\n                                  scoring=self.scoring,\n                                  gcv_mode=self.gcv_mode,\n                                  store_cv_values=self.store_cv_values)\n            estimator.fit(X, y, sample_weight=sample_weight)\n            self.alpha_ = estimator.alpha_\n            if self.store_cv_values:\n                self.cv_values_ = estimator.cv_values_\n        else:\n            if self.store_cv_values:\n                raise ValueError(\"cv!=None and store_cv_values=True \"\n                                 \" are incompatible\")\n            parameters = {'alpha': self.alphas}\n            gs = GridSearchCV(Ridge(fit_intercept=self.fit_intercept,\n                                    normalize=self.normalize),\n                              parameters, cv=self.cv, scoring=self.scoring)\n            gs.fit(X, y, sample_weight=sample_weight)\n            estimator = gs.best_estimator_\n            self.alpha_ = gs.best_estimator_.alpha\n\n        self.coef_ = estimator.coef_\n        self.intercept_ = estimator.intercept_\n\n        return self\n\n\nclass RidgeCV(_BaseRidgeCV, RegressorMixin):\n    \"\"\"Ridge regression with built-in cross-validation.\n\n    By default, it performs Generalized Cross-Validation, which is a form of\n    efficient Leave-One-Out cross-validation.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : numpy array of shape [n_alphas]\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC.\n\n    fit_intercept : boolean\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n        - integer, to specify the number of folds.\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train/test splits.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`sklearn.model_selection.StratifiedKFold` is used, else,\n        :class:`sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    gcv_mode : {None, 'auto', 'svd', eigen'}, optional\n        Flag indicating which strategy to use when performing\n        Generalized Cross-Validation. Options are::\n\n            'auto' : use svd if n_samples > n_features or when X is a sparse\n                     matrix, otherwise use eigen\n            'svd' : force computation via singular value decomposition of X\n                    (does not work for sparse matrices)\n            'eigen' : force computation via eigendecomposition of X^T X\n\n        The 'auto' mode is the default and is intended to pick the cheaper\n        option of the two depending upon the shape and format of the training\n        data.\n\n    store_cv_values : boolean, default=False\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the `cv_values_` attribute (see\n        below). This flag is only compatible with `cv=None` (i.e. using\n        Generalized Cross-Validation).\n\n    Attributes\n    ----------\n    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n        shape = [n_samples, n_targets, n_alphas], optional\n        Cross-validation values for each alpha (if `store_cv_values=True` and \\\n        `cv=None`). After `fit()` has been called, this attribute will \\\n        contain the mean squared errors (by default) or the values of the \\\n        `{loss,score}_func` function (if provided in the constructor).\n\n    coef_ : array, shape = [n_features] or [n_targets, n_features]\n        Weight vector(s).\n\n    intercept_ : float | array, shape = (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n        Estimated regularization parameter.\n\n    See also\n    --------\n    Ridge : Ridge regression\n    RidgeClassifier : Ridge classifier\n    RidgeClassifierCV : Ridge classifier with built-in cross validation\n    \"\"\"\n    pass\n\n(B) \n    store_cv_values : boolean, default=False\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the `cv_values_` attribute (see\n        below). This flag is only compatible with `cv=None` (i.e. using\n        Generalized Cross-Validation).\n\n    Attributes\n    ----------\n    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n        shape = [n_samples, n_targets, n_alphas], optional\n        Cross-validation values for each alpha (if `store_cv_values=True` and \\\n        `cv=None`). After `fit()` has been called, this attribute will \\\n        contain the mean squared errors (by default) or the values of the \\\n        `{loss,score}_func` function (if provided in the constructor).\n\n    coef_ : array, shape = [n_features] or [n_targets, n_features]\n        Weight vector(s).\n\n    intercept_ : float | array, shape = (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n        Estimated regularization parameter.\n\n    See also\n    --------\n    Ridge : Ridge regression\n    RidgeClassifier : Ridge classifier\n    RidgeClassifierCV : Ridge classifier with built-in cross validation\n    \"\"\"\n    pass\n\n\nclass RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n    \"\"\"Ridge classifier with built-in cross-validation.\n\n    By default, it performs Generalized Cross-Validation, which is a form of\n    efficient Leave-One-Out cross-validation. Currently, only the n_features >\n    n_samples case is handled efficiently.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : numpy array of shape [n_alphas]\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC.\n\n    fit_intercept : boolean\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n        - integer, to specify the number of folds.\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train/test splits.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    class_weight : dict or 'balanced', optional\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n    Attributes\n    ----------\n    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n    shape = [n_samples, n_responses, n_alphas], optional\n        Cross-validation values for each alpha (if `store_cv_values=True` and\n    `cv=None`). After `fit()` has been called, this attribute will contain \\\n    the mean squared errors (by default) or the values of the \\\n    `{loss,score}_func` function (if provided in the constructor).\n\n    coef_ : array, shape = [n_features] or [n_targets, n_features]\n        Weight vector(s).\n\n    intercept_ : float | array, shape = (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n        Estimated regularization parameter\n\n    See also\n    --------\n    Ridge : Ridge regression\n    RidgeClassifier : Ridge classifier\n    RidgeCV : Ridge regression with built-in cross validation\n\n    Notes\n    -----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.\n    \"\"\"\n    def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n                 normalize=False, scoring=None, cv=None, class_weight=None):\n        super(RidgeClassifierCV, self).__init__(\n            alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n            scoring=scoring, cv=cv)\n        self.class_weight = class_weight\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the ridge classifier.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples\n(C) \n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n        - integer, to specify the number of folds.\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train/test splits.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`sklearn.model_selection.StratifiedKFold` is used, else,\n        :class:`sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    gcv_mode : {None, 'auto', 'svd', eigen'}, optional\n        Flag indicating which strategy to use when performing\n        Generalized Cross-Validation. Options are::\n\n            'auto' : use svd if n_samples > n_features or when X is a sparse\n                     matrix, otherwise use eigen\n            'svd' : force computation via singular value decomposition of X\n                    (does not work for sparse matrices)\n            'eigen' : force computation via eigendecomposition of X^T X\n\n        The 'auto' mode is the default and is intended to pick the cheaper\n        option of the two depending upon the shape and format of the training\n        data.\n\n    store_cv_values : boolean, default=False\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the `cv_values_` attribute (see\n        below). This flag is only compatible with `cv=None` (i.e. using\n        Generalized Cross-Validation).\n\n    Attributes\n    ----------\n    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n        shape = [n_samples, n_targets, n_alphas], optional\n        Cross-validation values for each alpha (if `store_cv_values=True` and \\\n        `cv=None`). After `fit()` has been called, this attribute will \\\n        contain the mean squared errors (by default) or the values of the \\\n        `{loss,score}_func` function (if provided in the constructor).\n\n    coef_ : array, shape = [n_features] or [n_targets, n_features]\n        Weight vector(s).\n\n    intercept_ : float | array, shape = (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n        Estimated regularization parameter.\n\n    See also\n    --------\n    Ridge : Ridge regression\n    RidgeClassifier : Ridge classifier\n    RidgeClassifierCV : Ridge classifier with built-in cross validation\n    \"\"\"\n    pass\n\n\nclass RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n    \"\"\"Ridge classifier with built-in cross-validation.\n\n    By default, it performs Generalized Cross-Validation, which is a form of\n    efficient Leave-One-Out cross-validation. Currently, only the n_features >\n    n_samples case is handled efficiently.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : numpy array of shape [n_alphas]\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC.\n\n    fit_intercept : boolean\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n        - integer, to specify the number of folds.\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train/test splits.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    class_weight : dict or 'balanced', optional\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n    Attributes\n    ----------\n    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n    shape = [n_samples, n_responses, n_alphas], optional\n        Cross-validation values for each alpha (if `store_cv_values=True` and\n    `cv=None`). After `fit()` has been called, this attribute will contain \\\n    the mean squared errors (by default) or the values of the \\\n    `{loss,score}_func` function (if provided in the constructor).\n\n    coef_ : array, shape = [n_features] or [n_targets, n_features]\n(D) \n        return self\n\n\nclass RidgeCV(_BaseRidgeCV, RegressorMixin):\n    \"\"\"Ridge regression with built-in cross-validation.\n\n    By default, it performs Generalized Cross-Validation, which is a form of\n    efficient Leave-One-Out cross-validation.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : numpy array of shape [n_alphas]\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC.\n\n    fit_intercept : boolean\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n        - integer, to specify the number of folds.\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train/test splits.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`sklearn.model_selection.StratifiedKFold` is used, else,\n        :class:`sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    gcv_mode : {None, 'auto', 'svd', eigen'}, optional\n        Flag indicating which strategy to use when performing\n        Generalized Cross-Validation. Options are::\n\n            'auto' : use svd if n_samples > n_features or when X is a sparse\n                     matrix, otherwise use eigen\n            'svd' : force computation via singular value decomposition of X\n                    (does not work for sparse matrices)\n            'eigen' : force computation via eigendecomposition of X^T X\n\n        The 'auto' mode is the default and is intended to pick the cheaper\n        option of the two depending upon the shape and format of the training\n        data.\n\n    store_cv_values : boolean, default=False\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the `cv_values_` attribute (see\n        below). This flag is only compatible with `cv=None` (i.e. using\n        Generalized Cross-Validation).\n\n    Attributes\n    ----------\n    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n        shape = [n_samples, n_targets, n_alphas], optional\n        Cross-validation values for each alpha (if `store_cv_values=True` and \\\n        `cv=None`). After `fit()` has been called, this attribute will \\\n        contain the mean squared errors (by default) or the values of the \\\n        `{loss,score}_func` function (if provided in the constructor).\n\n    coef_ : array, shape = [n_features] or [n_targets, n_features]\n        Weight vector(s).\n\n    intercept_ : float | array, shape = (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n        Estimated regularization parameter.\n\n    See also\n    --------\n    Ridge : Ridge regression\n    RidgeClassifier : Ridge classifier\n    RidgeClassifierCV : Ridge classifier with built-in cross validation\n    \"\"\"\n    pass\n\n\nclass RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n    \"\"\"Ridge classifier with built-in cross-validation.\n\n    By default, it performs Generalized Cross-Validation, which is a form of\n    efficient Leave-One-Out cross-validation. Currently, only the n_features >\n    n_samples case is handled efficiently.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : numpy array of shape [n_alphas]\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC.\n\n    fit_intercept : boolean\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    normalize : boolean, optional, default False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    scoring : string, callable or None, optional, default: None", "answer": "B"}
{"uuid": "dbcd4395-9af1-4293-b5a3-fb3519b2de52", "issue_statement": "LabelEncoder transform fails for empty lists (for certain inputs)\nPython 3.6.3, scikit_learn 0.19.1\r\n\r\nDepending on which datatypes were used to fit the LabelEncoder, transforming empty lists works or not. Expected behavior would be that empty arrays are returned in both cases.\r\n\r\n```python\r\n>>> from sklearn.preprocessing import LabelEncoder\r\n>>> le = LabelEncoder()\r\n>>> le.fit([1,2])\r\nLabelEncoder()\r\n>>> le.transform([])\r\narray([], dtype=int64)\r\n>>> le.fit([\"a\",\"b\"])\r\nLabelEncoder()\r\n>>> le.transform([])\r\nTraceback (most recent call last):\r\n  File \"[...]\\Python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 57, in _wrapfunc\r\n    return getattr(obj, method)(*args, **kwds)\r\nTypeError: Cannot cast array data from dtype('float64') to dtype('<U32') according to the rule 'safe'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"[...]\\Python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py\", line 134, in transform\r\n    return np.searchsorted(self.classes_, y)\r\n  File \"[...]\\Python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 1075, in searchsorted\r\n    return _wrapfunc(a, 'searchsorted', v, side=side, sorter=sorter)\r\n  File \"[...]\\Python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 67, in _wrapfunc\r\n    return _wrapit(obj, method, *args, **kwds)\r\n  File \"[...]\\Python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 47, in _wrapit\r\n    result = getattr(asarray(obj), method)(*args, **kwds)\r\nTypeError: Cannot cast array data from dtype('float64') to dtype('<U32') according to the rule 'safe'\r\n```", "choices": "(A)             Target values.\n\n        Returns\n        -------\n        y : numpy array of shape [n_samples]\n        \"\"\"\n        check_is_fitted(self, 'classes_')\n\n        diff = np.setdiff1d(y, np.arange(len(self.classes_)))\n        if len(diff):\n            raise ValueError(\n                    \"y contains previously unseen labels: %s\" % str(diff))\n        y = np.asarray(y)\n        return self.classes_[y]\n\n\nclass LabelBinarizer(BaseEstimator, TransformerMixin):\n    \"\"\"Binarize labels in a one-vs-all fashion\n\n    Several regression and binary classification algorithms are\n    available in scikit-learn. A simple way to extend these algorithms\n    to the multi-class classification case is to use the so-called\n    one-vs-all scheme.\n\n    At learning time, this simply consists in learning one regressor\n    or binary classifier per class. In doing so, one needs to convert\n    multi-class labels to binary labels (belong or does not belong\n    to the class). LabelBinarizer makes this process easy with the\n    transform method.\n\n    At prediction time, one assigns the class for which the corresponding\n    model gave the greatest confidence. LabelBinarizer makes this easy\n    with the inverse_transform method.\n\n(B) \n        Parameters\n        ----------\n        y : array-like of shape [n_samples]\n            Target values.\n\n        Returns\n        -------\n        y : array-like of shape [n_samples]\n        \"\"\"\n        check_is_fitted(self, 'classes_')\n        y = column_or_1d(y, warn=True)\n\n        classes = np.unique(y)\n        if len(np.intersect1d(classes, self.classes_)) < len(classes):\n            diff = np.setdiff1d(classes, self.classes_)\n            raise ValueError(\n                    \"y contains previously unseen labels: %s\" % str(diff))\n        return np.searchsorted(self.classes_, y)\n\n    def inverse_transform(self, y):\n        \"\"\"Transform labels back to original encoding.\n\n        Parameters\n        ----------\n        y : numpy array of shape [n_samples]\n            Target values.\n\n        Returns\n        -------\n        y : numpy array of shape [n_samples]\n        \"\"\"\n        check_is_fitted(self, 'classes_')\n\n(C)         \"\"\"\n        check_is_fitted(self, 'classes_')\n        y = column_or_1d(y, warn=True)\n\n        classes = np.unique(y)\n        if len(np.intersect1d(classes, self.classes_)) < len(classes):\n            diff = np.setdiff1d(classes, self.classes_)\n            raise ValueError(\n                    \"y contains previously unseen labels: %s\" % str(diff))\n        return np.searchsorted(self.classes_, y)\n\n    def inverse_transform(self, y):\n        \"\"\"Transform labels back to original encoding.\n\n        Parameters\n        ----------\n        y : numpy array of shape [n_samples]\n            Target values.\n\n        Returns\n        -------\n        y : numpy array of shape [n_samples]\n        \"\"\"\n        check_is_fitted(self, 'classes_')\n\n        diff = np.setdiff1d(y, np.arange(len(self.classes_)))\n        if len(diff):\n            raise ValueError(\n                    \"y contains previously unseen labels: %s\" % str(diff))\n        y = np.asarray(y)\n        return self.classes_[y]\n\n\nclass LabelBinarizer(BaseEstimator, TransformerMixin):\n(D)         return np.searchsorted(self.classes_, y)\n\n    def inverse_transform(self, y):\n        \"\"\"Transform labels back to original encoding.\n\n        Parameters\n        ----------\n        y : numpy array of shape [n_samples]\n            Target values.\n\n        Returns\n        -------\n        y : numpy array of shape [n_samples]\n        \"\"\"\n        check_is_fitted(self, 'classes_')\n\n        diff = np.setdiff1d(y, np.arange(len(self.classes_)))\n        if len(diff):\n            raise ValueError(\n                    \"y contains previously unseen labels: %s\" % str(diff))\n        y = np.asarray(y)\n        return self.classes_[y]\n\n\nclass LabelBinarizer(BaseEstimator, TransformerMixin):\n    \"\"\"Binarize labels in a one-vs-all fashion\n\n    Several regression and binary classification algorithms are\n    available in scikit-learn. A simple way to extend these algorithms\n    to the multi-class classification case is to use the so-called\n    one-vs-all scheme.\n\n    At learning time, this simply consists in learning one regressor\n    or binary classifier per class. In doing so, one needs to convert", "answer": "C"}
{"uuid": "7f32b251-f121-4ea8-a5b6-84cc0998613e", "issue_statement": "warn_on_dtype with DataFrame\n#### Description\r\n\r\n``warn_on_dtype`` has no effect when input is a pandas ``DataFrame``\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nfrom sklearn.utils.validation import check_array\r\nimport pandas as pd\r\ndf = pd.DataFrame([[1, 2, 3], [2, 3, 4]], dtype=object)\r\nchecked = check_array(df, warn_on_dtype=True)\r\n```\r\n\r\n#### Expected result: \r\n\r\n```python-traceback\r\nDataConversionWarning: Data with input dtype object was converted to float64.\r\n```\r\n\r\n#### Actual Results\r\nNo warning is thrown\r\n\r\n#### Versions\r\nLinux-4.4.0-116-generic-x86_64-with-debian-stretch-sid\r\nPython 3.6.3 |Anaconda, Inc.| (default, Nov  3 2017, 19:19:16) \r\n[GCC 7.2.0]\r\nNumPy 1.13.1\r\nSciPy 0.19.1\r\nScikit-Learn 0.20.dev0\r\nPandas 0.21.0\r\n\nwarn_on_dtype with DataFrame\n#### Description\r\n\r\n``warn_on_dtype`` has no effect when input is a pandas ``DataFrame``\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nfrom sklearn.utils.validation import check_array\r\nimport pandas as pd\r\ndf = pd.DataFrame([[1, 2, 3], [2, 3, 4]], dtype=object)\r\nchecked = check_array(df, warn_on_dtype=True)\r\n```\r\n\r\n#### Expected result: \r\n\r\n```python-traceback\r\nDataConversionWarning: Data with input dtype object was converted to float64.\r\n```\r\n\r\n#### Actual Results\r\nNo warning is thrown\r\n\r\n#### Versions\r\nLinux-4.4.0-116-generic-x86_64-with-debian-stretch-sid\r\nPython 3.6.3 |Anaconda, Inc.| (default, Nov  3 2017, 19:19:16) \r\n[GCC 7.2.0]\r\nNumPy 1.13.1\r\nSciPy 0.19.1\r\nScikit-Learn 0.20.dev0\r\nPandas 0.21.0", "choices": "(A)         This check is only enforced when the input data has effectively 2\n        dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0\n        disables this check.\n\n    warn_on_dtype : boolean (default=False)\n        Raise DataConversionWarning if the dtype of the input data structure\n        does not match the requested dtype, causing a memory copy.\n\n    estimator : str or estimator instance (default=None)\n        If passed, include the name of the estimator in warning messages.\n\n    Returns\n    -------\n    X_converted : object\n        The converted and validated X.\n\n    \"\"\"\n    # accept_sparse 'None' deprecation check\n    if accept_sparse is None:\n        warnings.warn(\n            \"Passing 'None' to parameter 'accept_sparse' in methods \"\n            \"check_array and check_X_y is deprecated in version 0.19 \"\n            \"and will be removed in 0.21. Use 'accept_sparse=False' \"\n            \" instead.\", DeprecationWarning)\n        accept_sparse = False\n\n    # store reference to original array to check if copy is needed when\n    # function returns\n    array_orig = array\n\n    # store whether originally we wanted numeric dtype\n    dtype_numeric = isinstance(dtype, six.string_types) and dtype == \"numeric\"\n\n    dtype_orig = getattr(array, \"dtype\", None)\n    if not hasattr(dtype_orig, 'kind'):\n        # not a data type (e.g. a column named dtype in a pandas DataFrame)\n        dtype_orig = None\n\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == \"O\":\n            # if input is object, convert to float.\n            dtype = np.float64\n        else:\n            dtype = None\n\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            # no dtype conversion required\n            dtype = None\n        else:\n            # dtype conversion required. Let's select the first element of the\n            # list of accepted types.\n            dtype = dtype[0]\n\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\"'\n                         '. Got {!r} instead'.format(force_all_finite))\n\n    if estimator is not None:\n        if isinstance(estimator, six.string_types):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = \"Estimator\"\n    context = \" by %s\" % estimator_name if estimator is not None else \"\"\n\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n                                      dtype=dtype, copy=copy,\n                                      force_all_finite=force_all_finite,\n                                      accept_large_sparse=accept_large_sparse)\n    else:\n        # If np.array(..) gives ComplexWarning, then we convert the warning\n        # to an error. This is needed because specifying a non complex\n        # dtype to the function converts complex to real dtype,\n        # thereby passing the test made in the lines following the scope\n        # of warnings context manager.\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError(\"Complex data not supported\\n\"\n                                 \"{}\\n\".format(array))\n\n        # It is possible that the np.array(..) gave no warning. This happens\n        # when no dtype conversion happened, for example dtype = None. The\n        # result is that np.array(..) produces an array of complex dtype\n        # and we need to catch and raise exception for such cases.\n        _ensure_no_complex_data(array)\n\n        if ensure_2d:\n            # If input is scalar raise error\n            if array.ndim == 0:\n                raise ValueError(\n                    \"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array))\n            # If input is 1D raise error\n            if array.ndim == 1:\n                raise ValueError(\n                    \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array))\n\n        # in the future np.flexible dtypes will be handled like object dtypes\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\n                \"Beginning in version 0.22, arrays of strings will be \"\n                \"interpreted as decimal numbers if parameter 'dtype' is \"\n                \"'numeric'. It is recommended that you convert the array to \"\n                \"type np.float64 before passing it to check_array.\",\n                FutureWarning)\n\n        # make sure we actually converted to numeric:\n        if dtype_numeric and array.dtype.kind == \"O\":\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n                             % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array,\n                               allow_nan=force_all_finite == 'allow-nan')\n\n    shape_repr = _shape_repr(array.shape)\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n                             \" minimum of %d is required%s.\"\n                             % (n_samples, shape_repr, ensure_min_samples,\n                                context))\n\n(B)                     \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array))\n\n        # in the future np.flexible dtypes will be handled like object dtypes\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\n                \"Beginning in version 0.22, arrays of strings will be \"\n                \"interpreted as decimal numbers if parameter 'dtype' is \"\n                \"'numeric'. It is recommended that you convert the array to \"\n                \"type np.float64 before passing it to check_array.\",\n                FutureWarning)\n\n        # make sure we actually converted to numeric:\n        if dtype_numeric and array.dtype.kind == \"O\":\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n                             % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array,\n                               allow_nan=force_all_finite == 'allow-nan')\n\n    shape_repr = _shape_repr(array.shape)\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n                             \" minimum of %d is required%s.\"\n                             % (n_samples, shape_repr, ensure_min_samples,\n                                context))\n\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError(\"Found array with %d feature(s) (shape=%s) while\"\n                             \" a minimum of %d is required%s.\"\n                             % (n_features, shape_repr, ensure_min_features,\n                                context))\n\n    if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:\n        msg = (\"Data with input dtype %s was converted to %s%s.\"\n               % (dtype_orig, array.dtype, context))\n        warnings.warn(msg, DataConversionWarning)\n\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n\n    return array\n\n\ndef _check_large_sparse(X, accept_large_sparse=False):\n    \"\"\"Raise a ValueError if X has 64bit indices and accept_large_sparse=False\n    \"\"\"\n    if not (accept_large_sparse and LARGE_SPARSE_SUPPORTED):\n        supported_indices = [\"int32\"]\n        if X.getformat() == \"coo\":\n            index_keys = ['col', 'row']\n        elif X.getformat() in [\"csr\", \"csc\", \"bsr\"]:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if (indices_datatype not in supported_indices):\n                if not LARGE_SPARSE_SUPPORTED:\n                    raise ValueError(\"Scipy version %s does not support large\"\n                                     \" indices, please upgrade your scipy\"\n                                     \" to 0.14.0 or above\" % scipy_version)\n                raise ValueError(\"Only sparse matrices with 32-bit integer\"\n                                 \" indices are accepted. Got %s indices.\"\n                                 % indices_datatype)\n\n\ndef check_X_y(X, y, accept_sparse=False, accept_large_sparse=True,\n              dtype=\"numeric\", order=None, copy=False, force_all_finite=True,\n              ensure_2d=True, allow_nd=False, multi_output=False,\n              ensure_min_samples=1, ensure_min_features=1, y_numeric=False,\n              warn_on_dtype=False, estimator=None):\n    \"\"\"Input validation for standard estimators.\n\n    Checks X and y for consistent length, enforces X 2d and y 1d.\n    Standard input checks are only applied to y, such as checking that y\n    does not have np.nan or np.inf targets. For multi-label y, set\n    multi_output=True to allow 2d and sparse y.  If the dtype of X is\n    object, attempt converting to float, raising on failure.\n\n    Parameters\n    ----------\n    X : nd-array, list or sparse matrix\n        Input data.\n\n    y : nd-array, list or sparse matrix\n        Labels.\n\n    accept_sparse : string, boolean or list of string (default=False)\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc. If the input is sparse but not in the allowed format,\n        it will be converted to the first listed format. True allows the input\n        to be any format. False means that a sparse matrix input will\n        raise an error.\n\n        .. deprecated:: 0.19\n           Passing 'None' to parameter ``accept_sparse`` in methods is\n           deprecated in version 0.19 \"and will be removed in 0.21. Use\n           ``accept_sparse=False`` instead.\n\n    accept_large_sparse : bool (default=True)\n        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n        accept_sparse, accept_large_sparse will cause it to be accepted only\n        if its indices are stored with a 32-bit dtype.\n\n        .. versionadded:: 0.20\n\n    dtype : string, type, list of types or None (default=\"numeric\")\n        Data type of result. If None, the dtype of the input is preserved.\n        If \"numeric\", dtype is preserved unless array.dtype is object.\n        If dtype is a list of types, conversion on the first type is only\n        performed if the dtype of the input is not in the list.\n\n    order : 'F', 'C' or None (default=None)\n        Whether an array will be forced to be fortran or c-style.\n\n    copy : boolean (default=False)\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_all_finite : boolean or 'allow-nan', (default=True)\n        Whether to raise an error on np.inf and np.nan in X. This parameter\n        does not influence whether y can have np.inf or np.nan values.\n        The possibilities are:\n\n        - True: Force all values of X to be finite.\n        - False: accept both np.inf and np.nan in X.\n        - 'allow-nan':  accept  only  np.nan  values in  X.  Values  cannot  be\n          infinite.\n(C)                                       dtype=dtype, copy=copy,\n                                      force_all_finite=force_all_finite,\n                                      accept_large_sparse=accept_large_sparse)\n    else:\n        # If np.array(..) gives ComplexWarning, then we convert the warning\n        # to an error. This is needed because specifying a non complex\n        # dtype to the function converts complex to real dtype,\n        # thereby passing the test made in the lines following the scope\n        # of warnings context manager.\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError(\"Complex data not supported\\n\"\n                                 \"{}\\n\".format(array))\n\n        # It is possible that the np.array(..) gave no warning. This happens\n        # when no dtype conversion happened, for example dtype = None. The\n        # result is that np.array(..) produces an array of complex dtype\n        # and we need to catch and raise exception for such cases.\n        _ensure_no_complex_data(array)\n\n        if ensure_2d:\n            # If input is scalar raise error\n            if array.ndim == 0:\n                raise ValueError(\n                    \"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array))\n            # If input is 1D raise error\n            if array.ndim == 1:\n                raise ValueError(\n                    \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array))\n\n        # in the future np.flexible dtypes will be handled like object dtypes\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\n                \"Beginning in version 0.22, arrays of strings will be \"\n                \"interpreted as decimal numbers if parameter 'dtype' is \"\n                \"'numeric'. It is recommended that you convert the array to \"\n                \"type np.float64 before passing it to check_array.\",\n                FutureWarning)\n\n        # make sure we actually converted to numeric:\n        if dtype_numeric and array.dtype.kind == \"O\":\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n                             % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array,\n                               allow_nan=force_all_finite == 'allow-nan')\n\n    shape_repr = _shape_repr(array.shape)\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n                             \" minimum of %d is required%s.\"\n                             % (n_samples, shape_repr, ensure_min_samples,\n                                context))\n\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError(\"Found array with %d feature(s) (shape=%s) while\"\n                             \" a minimum of %d is required%s.\"\n                             % (n_features, shape_repr, ensure_min_features,\n                                context))\n\n    if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:\n        msg = (\"Data with input dtype %s was converted to %s%s.\"\n               % (dtype_orig, array.dtype, context))\n        warnings.warn(msg, DataConversionWarning)\n\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n\n    return array\n\n\ndef _check_large_sparse(X, accept_large_sparse=False):\n    \"\"\"Raise a ValueError if X has 64bit indices and accept_large_sparse=False\n    \"\"\"\n    if not (accept_large_sparse and LARGE_SPARSE_SUPPORTED):\n        supported_indices = [\"int32\"]\n        if X.getformat() == \"coo\":\n            index_keys = ['col', 'row']\n        elif X.getformat() in [\"csr\", \"csc\", \"bsr\"]:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if (indices_datatype not in supported_indices):\n                if not LARGE_SPARSE_SUPPORTED:\n                    raise ValueError(\"Scipy version %s does not support large\"\n                                     \" indices, please upgrade your scipy\"\n                                     \" to 0.14.0 or above\" % scipy_version)\n                raise ValueError(\"Only sparse matrices with 32-bit integer\"\n                                 \" indices are accepted. Got %s indices.\"\n                                 % indices_datatype)\n\n\ndef check_X_y(X, y, accept_sparse=False, accept_large_sparse=True,\n              dtype=\"numeric\", order=None, copy=False, force_all_finite=True,\n              ensure_2d=True, allow_nd=False, multi_output=False,\n              ensure_min_samples=1, ensure_min_features=1, y_numeric=False,\n              warn_on_dtype=False, estimator=None):\n    \"\"\"Input validation for standard estimators.\n\n    Checks X and y for consistent length, enforces X 2d and y 1d.\n    Standard input checks are only applied to y, such as checking that y\n    does not have np.nan or np.inf targets. For multi-label y, set\n    multi_output=True to allow 2d and sparse y.  If the dtype of X is\n    object, attempt converting to float, raising on failure.\n\n    Parameters\n    ----------\n    X : nd-array, list or sparse matrix\n        Input data.\n\n    y : nd-array, list or sparse matrix\n        Labels.\n\n    accept_sparse : string, boolean or list of string (default=False)\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc. If the input is sparse but not in the allowed format,\n        it will be converted to the first listed format. True allows the input\n        to be any format. False means that a sparse matrix input will\n        raise an error.\n\n(D)         # not a data type (e.g. a column named dtype in a pandas DataFrame)\n        dtype_orig = None\n\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == \"O\":\n            # if input is object, convert to float.\n            dtype = np.float64\n        else:\n            dtype = None\n\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            # no dtype conversion required\n            dtype = None\n        else:\n            # dtype conversion required. Let's select the first element of the\n            # list of accepted types.\n            dtype = dtype[0]\n\n    if force_all_finite not in (True, False, 'allow-nan'):\n        raise ValueError('force_all_finite should be a bool or \"allow-nan\"'\n                         '. Got {!r} instead'.format(force_all_finite))\n\n    if estimator is not None:\n        if isinstance(estimator, six.string_types):\n            estimator_name = estimator\n        else:\n            estimator_name = estimator.__class__.__name__\n    else:\n        estimator_name = \"Estimator\"\n    context = \" by %s\" % estimator_name if estimator is not None else \"\"\n\n    if sp.issparse(array):\n        _ensure_no_complex_data(array)\n        array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n                                      dtype=dtype, copy=copy,\n                                      force_all_finite=force_all_finite,\n                                      accept_large_sparse=accept_large_sparse)\n    else:\n        # If np.array(..) gives ComplexWarning, then we convert the warning\n        # to an error. This is needed because specifying a non complex\n        # dtype to the function converts complex to real dtype,\n        # thereby passing the test made in the lines following the scope\n        # of warnings context manager.\n        with warnings.catch_warnings():\n            try:\n                warnings.simplefilter('error', ComplexWarning)\n                array = np.asarray(array, dtype=dtype, order=order)\n            except ComplexWarning:\n                raise ValueError(\"Complex data not supported\\n\"\n                                 \"{}\\n\".format(array))\n\n        # It is possible that the np.array(..) gave no warning. This happens\n        # when no dtype conversion happened, for example dtype = None. The\n        # result is that np.array(..) produces an array of complex dtype\n        # and we need to catch and raise exception for such cases.\n        _ensure_no_complex_data(array)\n\n        if ensure_2d:\n            # If input is scalar raise error\n            if array.ndim == 0:\n                raise ValueError(\n                    \"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array))\n            # If input is 1D raise error\n            if array.ndim == 1:\n                raise ValueError(\n                    \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n                    \"Reshape your data either using array.reshape(-1, 1) if \"\n                    \"your data has a single feature or array.reshape(1, -1) \"\n                    \"if it contains a single sample.\".format(array))\n\n        # in the future np.flexible dtypes will be handled like object dtypes\n        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n            warnings.warn(\n                \"Beginning in version 0.22, arrays of strings will be \"\n                \"interpreted as decimal numbers if parameter 'dtype' is \"\n                \"'numeric'. It is recommended that you convert the array to \"\n                \"type np.float64 before passing it to check_array.\",\n                FutureWarning)\n\n        # make sure we actually converted to numeric:\n        if dtype_numeric and array.dtype.kind == \"O\":\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n                             % (array.ndim, estimator_name))\n        if force_all_finite:\n            _assert_all_finite(array,\n                               allow_nan=force_all_finite == 'allow-nan')\n\n    shape_repr = _shape_repr(array.shape)\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n                             \" minimum of %d is required%s.\"\n                             % (n_samples, shape_repr, ensure_min_samples,\n                                context))\n\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError(\"Found array with %d feature(s) (shape=%s) while\"\n                             \" a minimum of %d is required%s.\"\n                             % (n_features, shape_repr, ensure_min_features,\n                                context))\n\n    if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:\n        msg = (\"Data with input dtype %s was converted to %s%s.\"\n               % (dtype_orig, array.dtype, context))\n        warnings.warn(msg, DataConversionWarning)\n\n    if copy and np.may_share_memory(array, array_orig):\n        array = np.array(array, dtype=dtype, order=order)\n\n    return array\n\n\ndef _check_large_sparse(X, accept_large_sparse=False):\n    \"\"\"Raise a ValueError if X has 64bit indices and accept_large_sparse=False\n    \"\"\"\n    if not (accept_large_sparse and LARGE_SPARSE_SUPPORTED):\n        supported_indices = [\"int32\"]\n        if X.getformat() == \"coo\":\n            index_keys = ['col', 'row']\n        elif X.getformat() in [\"csr\", \"csc\", \"bsr\"]:\n            index_keys = ['indices', 'indptr']\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if (indices_datatype not in supported_indices):\n                if not LARGE_SPARSE_SUPPORTED:\n                    raise ValueError(\"Scipy version %s does not support large\"", "answer": "D"}
{"uuid": "1c22e169-a487-4ad6-a859-ebd2f6b0ffe6", "issue_statement": "Missing parameter validation in Neighbors estimator for float n_neighbors\n```python\r\nfrom sklearn.neighbors import NearestNeighbors\r\nfrom sklearn.datasets import make_blobs\r\nX, y = make_blobs()\r\nneighbors = NearestNeighbors(n_neighbors=3.)\r\nneighbors.fit(X)\r\nneighbors.kneighbors(X)\r\n```\r\n```\r\n~/checkout/scikit-learn/sklearn/neighbors/binary_tree.pxi in sklearn.neighbors.kd_tree.NeighborsHeap.__init__()\r\n\r\nTypeError: 'float' object cannot be interpreted as an integer\r\n```\r\nThis should be caught earlier and a more helpful error message should be raised (or we could be lenient and cast to integer, but I think a better error might be better).\r\n\r\nWe need to make sure that \r\n```python\r\nneighbors.kneighbors(X, n_neighbors=3.)\r\n```\r\nalso works.", "choices": "(A)                 or (n_query, n_indexed) if metric == 'precomputed'\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int\n            Number of neighbors to get (default is the value\n            passed to the constructor).\n\n        return_distance : boolean, optional. Defaults to True.\n            If False, distances will not be returned\n\n        Returns\n        -------\n        dist : array\n            Array representing the lengths to points, only present if\n            return_distance=True\n\n        ind : array\n            Indices of the nearest points in the population matrix.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1,1,1]\n\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=1)\n        >>> neigh.fit(samples) # doctest: +ELLIPSIS\n        NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n        >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n        (array([[0.5]]), array([[2]]))\n\n        As you can see, it returns [[0.5]], and [[2]], which means that the\n        element is at distance 0.5 and is the third element of samples\n        (indexes start at 0). You can also query for multiple points:\n\n        >>> X = [[0., 1., 0.], [1., 0., 1.]]\n        >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n        array([[1],\n               [2]]...)\n\n        \"\"\"\n        check_is_fitted(self, \"_fit_method\")\n\n        if n_neighbors is None:\n            n_neighbors = self.n_neighbors\n\n        if X is not None:\n            query_is_train = False\n            X = check_array(X, accept_sparse='csr')\n        else:\n            query_is_train = True\n            X = self._fit_X\n            # Include an extra neighbor to account for the sample itself being\n            # returned, which is removed later\n            n_neighbors += 1\n\n        train_size = self._fit_X.shape[0]\n        if n_neighbors > train_size:\n            raise ValueError(\n                \"Expected n_neighbors <= n_samples, \"\n                \" but n_samples = %d, n_neighbors = %d\" %\n                (train_size, n_neighbors)\n            )\n        n_samples, _ = X.shape\n        sample_range = np.arange(n_samples)[:, None]\n\n        n_jobs = _get_n_jobs(self.n_jobs)\n        if self._fit_method == 'brute':\n            # for efficiency, use squared euclidean distances\n            if self.effective_metric_ == 'euclidean':\n                dist = pairwise_distances(X, self._fit_X, 'euclidean',\n                                          n_jobs=n_jobs, squared=True)\n            else:\n                dist = pairwise_distances(\n                    X, self._fit_X, self.effective_metric_, n_jobs=n_jobs,\n                    **self.effective_metric_params_)\n\n            neigh_ind = np.argpartition(dist, n_neighbors - 1, axis=1)\n            neigh_ind = neigh_ind[:, :n_neighbors]\n            # argpartition doesn't guarantee sorted order, so we sort again\n            neigh_ind = neigh_ind[\n                sample_range, np.argsort(dist[sample_range, neigh_ind])]\n\n            if return_distance:\n                if self.effective_metric_ == 'euclidean':\n                    result = np.sqrt(dist[sample_range, neigh_ind]), neigh_ind\n                else:\n                    result = dist[sample_range, neigh_ind], neigh_ind\n(B)                     \"Expected n_neighbors > 0. Got %d\" %\n                    self.n_neighbors\n                )\n\n        return self\n\n    @property\n    def _pairwise(self):\n        # For cross-validation routines to split data correctly\n        return self.metric == 'precomputed'\n\n\nclass KNeighborsMixin(object):\n    \"\"\"Mixin for k-neighbors searches\"\"\"\n\n    def kneighbors(self, X=None, n_neighbors=None, return_distance=True):\n        \"\"\"Finds the K-neighbors of a point.\n\n        Returns indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_query, n_features), \\\n                or (n_query, n_indexed) if metric == 'precomputed'\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int\n            Number of neighbors to get (default is the value\n            passed to the constructor).\n\n        return_distance : boolean, optional. Defaults to True.\n            If False, distances will not be returned\n\n        Returns\n        -------\n        dist : array\n            Array representing the lengths to points, only present if\n            return_distance=True\n\n        ind : array\n            Indices of the nearest points in the population matrix.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1,1,1]\n\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=1)\n        >>> neigh.fit(samples) # doctest: +ELLIPSIS\n        NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n        >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n        (array([[0.5]]), array([[2]]))\n\n        As you can see, it returns [[0.5]], and [[2]], which means that the\n        element is at distance 0.5 and is the third element of samples\n        (indexes start at 0). You can also query for multiple points:\n\n        >>> X = [[0., 1., 0.], [1., 0., 1.]]\n        >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n        array([[1],\n               [2]]...)\n\n        \"\"\"\n        check_is_fitted(self, \"_fit_method\")\n\n        if n_neighbors is None:\n            n_neighbors = self.n_neighbors\n\n        if X is not None:\n            query_is_train = False\n            X = check_array(X, accept_sparse='csr')\n        else:\n            query_is_train = True\n            X = self._fit_X\n            # Include an extra neighbor to account for the sample itself being\n            # returned, which is removed later\n            n_neighbors += 1\n\n        train_size = self._fit_X.shape[0]\n        if n_neighbors > train_size:\n            raise ValueError(\n                \"Expected n_neighbors <= n_samples, \"\n                \" but n_samples = %d, n_neighbors = %d\" %\n                (train_size, n_neighbors)\n            )\n        n_samples, _ = X.shape\n        sample_range = np.arange(n_samples)[:, None]\n(C)                     self._fit_method = 'ball_tree'\n                else:\n                    self._fit_method = 'brute'\n            else:\n                self._fit_method = 'brute'\n\n        if self._fit_method == 'ball_tree':\n            self._tree = BallTree(X, self.leaf_size,\n                                  metric=self.effective_metric_,\n                                  **self.effective_metric_params_)\n        elif self._fit_method == 'kd_tree':\n            self._tree = KDTree(X, self.leaf_size,\n                                metric=self.effective_metric_,\n                                **self.effective_metric_params_)\n        elif self._fit_method == 'brute':\n            self._tree = None\n        else:\n            raise ValueError(\"algorithm = '%s' not recognized\"\n                             % self.algorithm)\n\n        if self.n_neighbors is not None:\n            if self.n_neighbors <= 0:\n                raise ValueError(\n                    \"Expected n_neighbors > 0. Got %d\" %\n                    self.n_neighbors\n                )\n\n        return self\n\n    @property\n    def _pairwise(self):\n        # For cross-validation routines to split data correctly\n        return self.metric == 'precomputed'\n\n\nclass KNeighborsMixin(object):\n    \"\"\"Mixin for k-neighbors searches\"\"\"\n\n    def kneighbors(self, X=None, n_neighbors=None, return_distance=True):\n        \"\"\"Finds the K-neighbors of a point.\n\n        Returns indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_query, n_features), \\\n                or (n_query, n_indexed) if metric == 'precomputed'\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int\n            Number of neighbors to get (default is the value\n            passed to the constructor).\n\n        return_distance : boolean, optional. Defaults to True.\n            If False, distances will not be returned\n\n        Returns\n        -------\n        dist : array\n            Array representing the lengths to points, only present if\n            return_distance=True\n\n        ind : array\n            Indices of the nearest points in the population matrix.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1,1,1]\n\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=1)\n        >>> neigh.fit(samples) # doctest: +ELLIPSIS\n        NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n        >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n        (array([[0.5]]), array([[2]]))\n\n        As you can see, it returns [[0.5]], and [[2]], which means that the\n        element is at distance 0.5 and is the third element of samples\n        (indexes start at 0). You can also query for multiple points:\n\n        >>> X = [[0., 1., 0.], [1., 0., 1.]]\n        >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n        array([[1],\n               [2]]...)\n\n        \"\"\"\n        check_is_fitted(self, \"_fit_method\")\n(D)         In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1,1,1]\n\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=1)\n        >>> neigh.fit(samples) # doctest: +ELLIPSIS\n        NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n        >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n        (array([[0.5]]), array([[2]]))\n\n        As you can see, it returns [[0.5]], and [[2]], which means that the\n        element is at distance 0.5 and is the third element of samples\n        (indexes start at 0). You can also query for multiple points:\n\n        >>> X = [[0., 1., 0.], [1., 0., 1.]]\n        >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n        array([[1],\n               [2]]...)\n\n        \"\"\"\n        check_is_fitted(self, \"_fit_method\")\n\n        if n_neighbors is None:\n            n_neighbors = self.n_neighbors\n\n        if X is not None:\n            query_is_train = False\n            X = check_array(X, accept_sparse='csr')\n        else:\n            query_is_train = True\n            X = self._fit_X\n            # Include an extra neighbor to account for the sample itself being\n            # returned, which is removed later\n            n_neighbors += 1\n\n        train_size = self._fit_X.shape[0]\n        if n_neighbors > train_size:\n            raise ValueError(\n                \"Expected n_neighbors <= n_samples, \"\n                \" but n_samples = %d, n_neighbors = %d\" %\n                (train_size, n_neighbors)\n            )\n        n_samples, _ = X.shape\n        sample_range = np.arange(n_samples)[:, None]\n\n        n_jobs = _get_n_jobs(self.n_jobs)\n        if self._fit_method == 'brute':\n            # for efficiency, use squared euclidean distances\n            if self.effective_metric_ == 'euclidean':\n                dist = pairwise_distances(X, self._fit_X, 'euclidean',\n                                          n_jobs=n_jobs, squared=True)\n            else:\n                dist = pairwise_distances(\n                    X, self._fit_X, self.effective_metric_, n_jobs=n_jobs,\n                    **self.effective_metric_params_)\n\n            neigh_ind = np.argpartition(dist, n_neighbors - 1, axis=1)\n            neigh_ind = neigh_ind[:, :n_neighbors]\n            # argpartition doesn't guarantee sorted order, so we sort again\n            neigh_ind = neigh_ind[\n                sample_range, np.argsort(dist[sample_range, neigh_ind])]\n\n            if return_distance:\n                if self.effective_metric_ == 'euclidean':\n                    result = np.sqrt(dist[sample_range, neigh_ind]), neigh_ind\n                else:\n                    result = dist[sample_range, neigh_ind], neigh_ind\n            else:\n                result = neigh_ind\n\n        elif self._fit_method in ['ball_tree', 'kd_tree']:\n            if issparse(X):\n                raise ValueError(\n                    \"%s does not work with sparse matrices. Densify the data, \"\n                    \"or set algorithm='brute'\" % self._fit_method)\n            result = Parallel(n_jobs, backend='threading')(\n                delayed(self._tree.query, check_pickle=False)(\n                    X[s], n_neighbors, return_distance)\n                for s in gen_even_slices(X.shape[0], n_jobs)\n            )\n            if return_distance:\n                dist, neigh_ind = tuple(zip(*result))\n                result = np.vstack(dist), np.vstack(neigh_ind)\n            else:\n                result = np.vstack(result)\n        else:\n            raise ValueError(\"internal: _fit_method not recognized\")\n\n        if not query_is_train:\n            return result", "answer": "B"}
{"uuid": "65df7293-c569-4f1e-87b3-71ac277cc610", "issue_statement": "Should mixture models have a clusterer-compatible interface\nMixture models are currently a bit different. They are basically clusterers, except they are probabilistic, and are applied to inductive problems unlike many clusterers. But they are unlike clusterers in API:\r\n* they have an `n_components` parameter, with identical purpose to `n_clusters`\r\n* they do not store the `labels_` of the training data\r\n* they do not have a `fit_predict` method\r\n\r\nAnd they are almost entirely documented separately.\r\n\r\nShould we make the MMs more like clusterers?", "choices": "(A)     def fit(self, X, y=None):\n        \"\"\"Estimate model parameters with the EM algorithm.\n\n        The method fit the model `n_init` times and set the parameters with\n        which the model has the largest likelihood or lower bound. Within each\n        trial, the method iterates between E-step and M-step for `max_iter`\n        times until the change of likelihood or lower bound is less than\n        `tol`, otherwise, a `ConvergenceWarning` is raised.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = _check_X(X, self.n_components, ensure_min_samples=2)\n        self._check_initial_parameters(X)\n\n        # if we enable warm_start, we will have a unique initialisation\n        do_init = not(self.warm_start and hasattr(self, 'converged_'))\n        n_init = self.n_init if do_init else 1\n\n        max_lower_bound = -np.infty\n        self.converged_ = False\n\n        random_state = check_random_state(self.random_state)\n\n        n_samples, _ = X.shape\n        for init in range(n_init):\n            self._print_verbose_msg_init_beg(init)\n\n            if do_init:\n                self._initialize_parameters(X, random_state)\n                self.lower_bound_ = -np.infty\n\n            for n_iter in range(1, self.max_iter + 1):\n                prev_lower_bound = self.lower_bound_\n\n                log_prob_norm, log_resp = self._e_step(X)\n                self._m_step(X, log_resp)\n                self.lower_bound_ = self._compute_lower_bound(\n                    log_resp, log_prob_norm)\n\n                change = self.lower_bound_ - prev_lower_bound\n                self._print_verbose_msg_iter_end(n_iter, change)\n\n                if abs(change) < self.tol:\n                    self.converged_ = True\n                    break\n\n            self._print_verbose_msg_init_end(self.lower_bound_)\n\n            if self.lower_bound_ > max_lower_bound:\n                max_lower_bound = self.lower_bound_\n                best_params = self._get_parameters()\n                best_n_iter = n_iter\n\n        if not self.converged_:\n            warnings.warn('Initialization %d did not converge. '\n                          'Try different init parameters, '\n                          'or increase max_iter, tol '\n                          'or check for degenerate data.'\n                          % (init + 1), ConvergenceWarning)\n\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n\n        return self\n\n    def _e_step(self, X):\n        \"\"\"E step.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        Returns\n        -------\n        log_prob_norm : float\n            Mean of the logarithms of the probabilities of each sample in X\n\n        log_responsibility : array, shape (n_samples, n_components)\n            Logarithm of the posterior probabilities (or responsibilities) of\n            the point of each sample in X.\n        \"\"\"\n        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n        return np.mean(log_prob_norm), log_resp\n\n    @abstractmethod\n    def _m_step(self, X, log_resp):\n        \"\"\"M step.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        log_resp : array-like, shape (n_samples, n_components)\n(B)                     self.converged_ = True\n                    break\n\n            self._print_verbose_msg_init_end(self.lower_bound_)\n\n            if self.lower_bound_ > max_lower_bound:\n                max_lower_bound = self.lower_bound_\n                best_params = self._get_parameters()\n                best_n_iter = n_iter\n\n        if not self.converged_:\n            warnings.warn('Initialization %d did not converge. '\n                          'Try different init parameters, '\n                          'or increase max_iter, tol '\n                          'or check for degenerate data.'\n                          % (init + 1), ConvergenceWarning)\n\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n\n        return self\n\n    def _e_step(self, X):\n        \"\"\"E step.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        Returns\n        -------\n        log_prob_norm : float\n            Mean of the logarithms of the probabilities of each sample in X\n\n        log_responsibility : array, shape (n_samples, n_components)\n            Logarithm of the posterior probabilities (or responsibilities) of\n            the point of each sample in X.\n        \"\"\"\n        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n        return np.mean(log_prob_norm), log_resp\n\n    @abstractmethod\n    def _m_step(self, X, log_resp):\n        \"\"\"M step.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        log_resp : array-like, shape (n_samples, n_components)\n            Logarithm of the posterior probabilities (or responsibilities) of\n            the point of each sample in X.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _check_is_fitted(self):\n        pass\n\n    @abstractmethod\n    def _get_parameters(self):\n        pass\n\n    @abstractmethod\n    def _set_parameters(self, params):\n        pass\n\n    def score_samples(self, X):\n        \"\"\"Compute the weighted log probabilities for each sample.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        log_prob : array, shape (n_samples,)\n            Log probabilities of each data point in X.\n        \"\"\"\n        self._check_is_fitted()\n        X = _check_X(X, None, self.means_.shape[1])\n\n        return logsumexp(self._estimate_weighted_log_prob(X), axis=1)\n\n    def score(self, X, y=None):\n        \"\"\"Compute the per-sample average log-likelihood of the given data X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_dimensions)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        log_likelihood : float\n            Log likelihood of the Gaussian mixture given X.\n        \"\"\"\n        return self.score_samples(X).mean()\n(C)         if self.init_params == 'kmeans':\n            resp = np.zeros((n_samples, self.n_components))\n            label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n                                   random_state=random_state).fit(X).labels_\n            resp[np.arange(n_samples), label] = 1\n        elif self.init_params == 'random':\n            resp = random_state.rand(n_samples, self.n_components)\n            resp /= resp.sum(axis=1)[:, np.newaxis]\n        else:\n            raise ValueError(\"Unimplemented initialization method '%s'\"\n                             % self.init_params)\n\n        self._initialize(X, resp)\n\n    @abstractmethod\n    def _initialize(self, X, resp):\n        \"\"\"Initialize the model parameters of the derived class.\n\n        Parameters\n        ----------\n        X : array-like, shape  (n_samples, n_features)\n\n        resp : array-like, shape (n_samples, n_components)\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None):\n        \"\"\"Estimate model parameters with the EM algorithm.\n\n        The method fit the model `n_init` times and set the parameters with\n        which the model has the largest likelihood or lower bound. Within each\n        trial, the method iterates between E-step and M-step for `max_iter`\n        times until the change of likelihood or lower bound is less than\n        `tol`, otherwise, a `ConvergenceWarning` is raised.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        X = _check_X(X, self.n_components, ensure_min_samples=2)\n        self._check_initial_parameters(X)\n\n        # if we enable warm_start, we will have a unique initialisation\n        do_init = not(self.warm_start and hasattr(self, 'converged_'))\n        n_init = self.n_init if do_init else 1\n\n        max_lower_bound = -np.infty\n        self.converged_ = False\n\n        random_state = check_random_state(self.random_state)\n\n        n_samples, _ = X.shape\n        for init in range(n_init):\n            self._print_verbose_msg_init_beg(init)\n\n            if do_init:\n                self._initialize_parameters(X, random_state)\n                self.lower_bound_ = -np.infty\n\n            for n_iter in range(1, self.max_iter + 1):\n                prev_lower_bound = self.lower_bound_\n\n                log_prob_norm, log_resp = self._e_step(X)\n                self._m_step(X, log_resp)\n                self.lower_bound_ = self._compute_lower_bound(\n                    log_resp, log_prob_norm)\n\n                change = self.lower_bound_ - prev_lower_bound\n                self._print_verbose_msg_iter_end(n_iter, change)\n\n                if abs(change) < self.tol:\n                    self.converged_ = True\n                    break\n\n            self._print_verbose_msg_init_end(self.lower_bound_)\n\n            if self.lower_bound_ > max_lower_bound:\n                max_lower_bound = self.lower_bound_\n                best_params = self._get_parameters()\n                best_n_iter = n_iter\n\n        if not self.converged_:\n            warnings.warn('Initialization %d did not converge. '\n                          'Try different init parameters, '\n                          'or increase max_iter, tol '\n                          'or check for degenerate data.'\n                          % (init + 1), ConvergenceWarning)\n\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n\n        return self\n\n    def _e_step(self, X):\n        \"\"\"E step.\n(D)         max_lower_bound = -np.infty\n        self.converged_ = False\n\n        random_state = check_random_state(self.random_state)\n\n        n_samples, _ = X.shape\n        for init in range(n_init):\n            self._print_verbose_msg_init_beg(init)\n\n            if do_init:\n                self._initialize_parameters(X, random_state)\n                self.lower_bound_ = -np.infty\n\n            for n_iter in range(1, self.max_iter + 1):\n                prev_lower_bound = self.lower_bound_\n\n                log_prob_norm, log_resp = self._e_step(X)\n                self._m_step(X, log_resp)\n                self.lower_bound_ = self._compute_lower_bound(\n                    log_resp, log_prob_norm)\n\n                change = self.lower_bound_ - prev_lower_bound\n                self._print_verbose_msg_iter_end(n_iter, change)\n\n                if abs(change) < self.tol:\n                    self.converged_ = True\n                    break\n\n            self._print_verbose_msg_init_end(self.lower_bound_)\n\n            if self.lower_bound_ > max_lower_bound:\n                max_lower_bound = self.lower_bound_\n                best_params = self._get_parameters()\n                best_n_iter = n_iter\n\n        if not self.converged_:\n            warnings.warn('Initialization %d did not converge. '\n                          'Try different init parameters, '\n                          'or increase max_iter, tol '\n                          'or check for degenerate data.'\n                          % (init + 1), ConvergenceWarning)\n\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n\n        return self\n\n    def _e_step(self, X):\n        \"\"\"E step.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        Returns\n        -------\n        log_prob_norm : float\n            Mean of the logarithms of the probabilities of each sample in X\n\n        log_responsibility : array, shape (n_samples, n_components)\n            Logarithm of the posterior probabilities (or responsibilities) of\n            the point of each sample in X.\n        \"\"\"\n        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n        return np.mean(log_prob_norm), log_resp\n\n    @abstractmethod\n    def _m_step(self, X, log_resp):\n        \"\"\"M step.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        log_resp : array-like, shape (n_samples, n_components)\n            Logarithm of the posterior probabilities (or responsibilities) of\n            the point of each sample in X.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _check_is_fitted(self):\n        pass\n\n    @abstractmethod\n    def _get_parameters(self):\n        pass\n\n    @abstractmethod\n    def _set_parameters(self, params):\n        pass\n\n    def score_samples(self, X):\n        \"\"\"Compute the weighted log probabilities for each sample.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n", "answer": "A"}
{"uuid": "d3a9d63b-38dc-45f4-a4a9-69dc786f96ae", "issue_statement": "OneHotEncoder ignore unknown error when categories are strings \n#### Description\r\n\r\nThis bug is very specific, but it happens when you set OneHotEncoder to ignore unknown entries.\r\nand your labels are strings. The memory of the arrays is not handled safely and it can lead to a ValueError\r\n\r\nBasically, when you call the transform method it will sets all the unknown strings on your array to OneHotEncoder.categories_[i][0] which is the first category alphabetically sorted given for fit\r\nIf this OneHotEncoder.categories_[i][0] is a long string, and the array that you want to transform has small strings, then it is impossible to fit the whole  OneHotEncoder.categories_[i][0] into the entries of the array we want to transform. So  OneHotEncoder.categories_[i][0]  is truncated and this raise the ValueError.\r\n\r\n\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\n\r\nimport numpy as np\r\nfrom sklearn.preprocessing import OneHotEncoder\r\n\r\n\r\n# It needs to be numpy arrays, the error does not appear \r\n# is you have lists of lists because it gets treated like an array of objects.\r\ntrain  = np.array([ '22','333','4444','11111111' ]).reshape((-1,1))\r\ntest   = np.array([ '55555',  '22' ]).reshape((-1,1))\r\n\r\nohe = OneHotEncoder(dtype=bool,handle_unknown='ignore')\r\n\r\nohe.fit( train )\r\nenc_test = ohe.transform( test )\r\n\r\n```\r\n\r\n\r\n#### Expected Results\r\nHere we should get an sparse matrix 2x4 false everywhere except at (1,1) the '22' that is known\r\n\r\n#### Actual Results\r\n\r\n> ValueError: y contains previously unseen labels: ['111111']\r\n\r\n\r\n#### Versions\r\nSystem:\r\n    python: 2.7.12 (default, Dec  4 2017, 14:50:18)  [GCC 5.4.0 20160609]\r\n   machine: Linux-4.4.0-138-generic-x86_64-with-Ubuntu-16.04-xenial\r\nexecutable: /usr/bin/python\r\n\r\nBLAS:\r\n    macros: HAVE_CBLAS=None\r\ncblas_libs: openblas, openblas\r\n  lib_dirs: /usr/lib\r\n\r\nPython deps:\r\n    Cython: 0.25.2\r\n     scipy: 0.18.1\r\nsetuptools: 36.7.0\r\n       pip: 9.0.1\r\n     numpy: 1.15.2\r\n    pandas: 0.19.1\r\n   sklearn: 0.21.dev0\r\n\r\n\r\n\r\n#### Comments\r\n\r\nI already implemented a fix for this issue, where I check the size of the elements in the array before, and I cast them into objects if necessary.", "choices": "(A) \n        return X_int, X_mask\n\n\nclass OneHotEncoder(_BaseEncoder):\n    \"\"\"Encode categorical integer features as a one-hot numeric array.\n\n    The input to this transformer should be an array-like of integers or\n    strings, denoting the values taken on by categorical (discrete) features.\n    The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n    encoding scheme. This creates a binary column for each category and\n    returns a sparse matrix or dense array.\n\n    By default, the encoder derives the categories based on the unique values\n(B)                            \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    Xi = Xi.copy()\n                    Xi[~valid_mask] = self.categories_[i][0]\n            _, encoded = _encode(Xi, self.categories_[i], encode=True)\n            X_int[:, i] = encoded\n\n        return X_int, X_mask\n\n(C)                     # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    Xi = Xi.copy()\n                    Xi[~valid_mask] = self.categories_[i][0]\n            _, encoded = _encode(Xi, self.categories_[i], encode=True)\n            X_int[:, i] = encoded\n\n        return X_int, X_mask\n\n\nclass OneHotEncoder(_BaseEncoder):\n    \"\"\"Encode categorical integer features as a one-hot numeric array.\n\n(D)                     Xi[~valid_mask] = self.categories_[i][0]\n            _, encoded = _encode(Xi, self.categories_[i], encode=True)\n            X_int[:, i] = encoded\n\n        return X_int, X_mask\n\n\nclass OneHotEncoder(_BaseEncoder):\n    \"\"\"Encode categorical integer features as a one-hot numeric array.\n\n    The input to this transformer should be an array-like of integers or\n    strings, denoting the values taken on by categorical (discrete) features.\n    The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n    encoding scheme. This creates a binary column for each category and", "answer": "C"}
{"uuid": "d033e026-6357-4206-ac56-896aa45210c7", "issue_statement": "GaussianMixture predict and fit_predict disagree when n_init>1\n#### Description\r\nWhen `n_init` is specified in GaussianMixture, the results of fit_predict(X) and predict(X) are often different.  The `test_gaussian_mixture_fit_predict` unit test doesn't catch this because it does not set `n_init`.\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\npython\r\nfrom sklearn.mixture import GaussianMixture\r\nfrom sklearn.utils.testing import assert_array_equal\r\nimport numpy\r\nX = numpy.random.randn(1000,5)\r\nprint 'no n_init'\r\ngm = GaussianMixture(n_components=5)\r\nc1 = gm.fit_predict(X)\r\nc2 = gm.predict(X)\r\nassert_array_equal(c1,c2)\r\nprint 'n_init=5'\r\ngm = GaussianMixture(n_components=5, n_init=5)\r\nc1 = gm.fit_predict(X)\r\nc2 = gm.predict(X)\r\nassert_array_equal(c1,c2)\r\n```\r\n\r\n#### Expected Results\r\n```\r\nno n_init\r\nn_init=5\r\n```\r\nNo exceptions.\r\n\r\n#### Actual Results\r\n```\r\nno n_init\r\nn_init=5\r\nTraceback (most recent call last):\r\n  File \"test_gm.py\", line 17, in <module>\r\n    assert_array_equal(c1,c2)\r\n  File \"/home/scott/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 872, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/home/scott/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\n(mismatch 88.6%)\r\n x: array([4, 0, 1, 1, 1, 3, 3, 4, 4, 2, 0, 0, 1, 2, 0, 2, 0, 1, 3, 1, 1, 3,\r\n       2, 1, 0, 2, 1, 0, 2, 0, 3, 1, 2, 3, 3, 1, 0, 2, 2, 0, 3, 0, 2, 0,\r\n       4, 2, 3, 0, 4, 2, 4, 1, 0, 2, 2, 1, 3, 2, 1, 4, 0, 2, 2, 1, 1, 2,...\r\n y: array([4, 1, 0, 2, 2, 1, 1, 4, 4, 0, 4, 1, 0, 3, 1, 0, 2, 2, 1, 2, 0, 0,\r\n       1, 0, 4, 1, 0, 4, 0, 1, 1, 2, 3, 1, 4, 0, 1, 4, 4, 4, 0, 1, 0, 2,\r\n       4, 1, 1, 2, 4, 3, 4, 0, 2, 3, 2, 3, 0, 0, 2, 3, 3, 3, 3, 0, 3, 2,...\r\n```\r\n\r\n#### Versions\r\n```\r\nSystem:\r\n    python: 2.7.15rc1 (default, Nov 12 2018, 14:31:15)  [GCC 7.3.0]\r\n   machine: Linux-4.15.0-43-generic-x86_64-with-Ubuntu-18.04-bionic\r\nexecutable: /usr/bin/python\r\n\r\nBLAS:\r\n    macros: HAVE_CBLAS=None, NO_ATLAS_INFO=-1\r\ncblas_libs: cblas\r\n  lib_dirs: /usr/lib/x86_64-linux-gnu\r\n\r\nPython deps:\r\n    Cython: 0.28.5\r\n     scipy: 1.2.0\r\nsetuptools: 39.0.1\r\n       pip: 19.0.1\r\n     numpy: 1.16.0\r\n    pandas: 0.23.1\r\n   sklearn: 0.20.2\r\n```", "choices": "(A)         _, log_resp = self._e_step(X)\n\n        if not self.converged_:\n            warnings.warn('Initialization %d did not converge. '\n                          'Try different init parameters, '\n                          'or increase max_iter, tol '\n                          'or check for degenerate data.'\n                          % (init + 1), ConvergenceWarning)\n\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.lower_bound_ = max_lower_bound\n\n        return log_resp.argmax(axis=1)\n\n    def _e_step(self, X):\n        \"\"\"E step.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n(B)                 best_params = self._get_parameters()\n                best_n_iter = n_iter\n\n        # Always do a final e-step to guarantee that the labels returned by\n        # fit_predict(X) are always consistent with fit(X).predict(X)\n        # for any value of max_iter and tol (and any random_state).\n        _, log_resp = self._e_step(X)\n\n        if not self.converged_:\n            warnings.warn('Initialization %d did not converge. '\n                          'Try different init parameters, '\n                          'or increase max_iter, tol '\n                          'or check for degenerate data.'\n                          % (init + 1), ConvergenceWarning)\n\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.lower_bound_ = max_lower_bound\n\n        return log_resp.argmax(axis=1)\n\n    def _e_step(self, X):\n(C)                     break\n\n            self._print_verbose_msg_init_end(lower_bound)\n\n            if lower_bound > max_lower_bound:\n                max_lower_bound = lower_bound\n                best_params = self._get_parameters()\n                best_n_iter = n_iter\n\n        # Always do a final e-step to guarantee that the labels returned by\n        # fit_predict(X) are always consistent with fit(X).predict(X)\n        # for any value of max_iter and tol (and any random_state).\n        _, log_resp = self._e_step(X)\n\n        if not self.converged_:\n            warnings.warn('Initialization %d did not converge. '\n                          'Try different init parameters, '\n                          'or increase max_iter, tol '\n                          'or check for degenerate data.'\n                          % (init + 1), ConvergenceWarning)\n\n        self._set_parameters(best_params)\n(D)                           'or increase max_iter, tol '\n                          'or check for degenerate data.'\n                          % (init + 1), ConvergenceWarning)\n\n        self._set_parameters(best_params)\n        self.n_iter_ = best_n_iter\n        self.lower_bound_ = max_lower_bound\n\n        return log_resp.argmax(axis=1)\n\n    def _e_step(self, X):\n        \"\"\"E step.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        Returns\n        -------\n        log_prob_norm : float\n            Mean of the logarithms of the probabilities of each sample in X\n", "answer": "B"}
{"uuid": "e386ee52-1c76-47fb-9781-d9db7b8dbedb", "issue_statement": "Differences among the results of KernelPCA with rbf kernel\nHi there,\r\nI met with a problem:\r\n\r\n#### Description\r\nWhen I run KernelPCA for dimension reduction for the same datasets, the results are different in signs.\r\n\r\n#### Steps/Code to Reproduce\r\nJust to reduce the dimension to 7 with rbf kernel:\r\npca = KernelPCA(n_components=7, kernel='rbf', copy_X=False, n_jobs=-1)\r\npca.fit_transform(X)\r\n\r\n#### Expected Results\r\nThe same result.\r\n\r\n#### Actual Results\r\nThe results are the same except for their signs:(\r\n[[-0.44457617 -0.18155886 -0.10873474  0.13548386 -0.1437174  -0.057469\t0.18124364]] \r\n\r\n[[ 0.44457617  0.18155886  0.10873474 -0.13548386 -0.1437174  -0.057469 -0.18124364]] \r\n\r\n[[-0.44457617 -0.18155886  0.10873474  0.13548386  0.1437174   0.057469  0.18124364]] \r\n\r\n#### Versions\r\n0.18.1", "choices": "(A) from scipy.sparse.linalg import eigsh\n\nfrom ..utils import check_random_state\nfrom ..utils.validation import check_is_fitted, check_array\nfrom ..exceptions import NotFittedError\nfrom ..base import BaseEstimator, TransformerMixin, _UnstableOn32BitMixin\nfrom ..preprocessing import KernelCenterer\nfrom ..metrics.pairwise import pairwise_kernels\n\n\nclass KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n    \"\"\"Kernel Principal component analysis (KPCA)\n\n    Non-linear dimensionality reduction through the use of kernels (see\n    :ref:`metrics`).\n\n    Read more in the :ref:`User Guide <kernel_PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components. If None, all non-zero components are kept.\n\n    kernel : \"linear\" | \"poly\" | \"rbf\" | \"sigmoid\" | \"cosine\" | \"precomputed\"\n        Kernel. Default=\"linear\".\n\n    gamma : float, default=1/n_features\n        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n        kernels.\n\n    degree : int, default=3\n        Degree for poly kernels. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Independent term in poly and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : mapping of string to any, default=None\n        Parameters (keyword arguments) and values for kernel passed as\n        callable object. Ignored by other kernels.\n\n    alpha : int, default=1.0\n        Hyperparameter of the ridge regression that learns the\n        inverse transform (when fit_inverse_transform=True).\n\n    fit_inverse_transform : bool, default=False\n        Learn the inverse transform for non-precomputed kernels.\n        (i.e. learn to find the pre-image of a point)\n\n    eigen_solver : string ['auto'|'dense'|'arpack'], default='auto'\n        Select eigensolver to use. If n_components is much less than\n        the number of training samples, arpack may be more efficient\n        than the dense eigensolver.\n\n    tol : float, default=0\n        Convergence tolerance for arpack.\n        If 0, optimal value will be chosen by arpack.\n\n    max_iter : int, default=None\n        Maximum number of iterations for arpack.\n        If None, optimal value will be chosen by arpack.\n\n    remove_zero_eig : boolean, default=False\n        If True, then all components with zero eigenvalues are removed, so\n        that the number of components in the output may be < n_components\n        (and sometimes even zero due to numerical instability).\n        When n_components is None, this parameter is ignored and components\n        with zero eigenvalues are removed regardless.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``eigen_solver`` == 'arpack'.\n\n        .. versionadded:: 0.18\n\n    copy_X : boolean, default=True\n        If True, input X is copied and stored by the model in the `X_fit_`\n        attribute. If no further changes will be done to X, setting\n        `copy_X=False` saves memory by storing a reference.\n\n        .. versionadded:: 0.18\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    Attributes\n    ----------\n    lambdas_ : array, (n_components,)\n        Eigenvalues of the centered kernel matrix in decreasing order.\n        If `n_components` and `remove_zero_eig` are not set,\n        then all values are stored.\n\n    alphas_ : array, (n_samples, n_components)\n        Eigenvectors of the centered kernel matrix. If `n_components` and\n        `remove_zero_eig` are not set, then all components are stored.\n\n    dual_coef_ : array, (n_samples, n_features)\n        Inverse transform matrix. Only available when\n        ``fit_inverse_transform`` is True.\n\n    X_transformed_fit_ : array, (n_samples, n_components)\n        Projection of the fitted data on the kernel principal components.\n        Only available when ``fit_inverse_transform`` is True.\n\n    X_fit_ : (n_samples, n_features)\n        The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n        a reference. This attribute is used for the calls to transform.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import KernelPCA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = KernelPCA(n_components=7, kernel='linear')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    References\n    ----------\n    Kernel PCA was introduced in:\n        Bernhard Schoelkopf, Alexander J. Smola,\n        and Klaus-Robert Mueller. 1999. Kernel principal\n        component analysis. In Advances in kernel methods,\n        MIT Press, Cambridge, MA, USA 327-352.\n    \"\"\"\n\n    def __init__(self, n_components=None, kernel=\"linear\",\n                 gamma=None, degree=3, coef0=1, kernel_params=None,\n                 alpha=1.0, fit_inverse_transform=False, eigen_solver='auto',\n                 tol=0, max_iter=None, remove_zero_eig=False,\n                 random_state=None, copy_X=True, n_jobs=None):\n        if fit_inverse_transform and kernel == 'precomputed':\n            raise ValueError(\n                \"Cannot fit_inverse_transform with a precomputed kernel.\")\n        self.n_components = n_components\n        self.kernel = kernel\n        self.kernel_params = kernel_params\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.alpha = alpha\n        self.fit_inverse_transform = fit_inverse_transform\n        self.eigen_solver = eigen_solver\n        self.remove_zero_eig = remove_zero_eig\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n        self.copy_X = copy_X\n\n    @property\n    def _pairwise(self):\n        return self.kernel == \"precomputed\"\n\n    def _get_kernel(self, X, Y=None):\n        if callable(self.kernel):\n            params = self.kernel_params or {}\n        else:\n            params = {\"gamma\": self.gamma,\n                      \"degree\": self.degree,\n                      \"coef0\": self.coef0}\n        return pairwise_kernels(X, Y, metric=self.kernel,\n                                filter_params=True, n_jobs=self.n_jobs,\n                                **params)\n\n    def _fit_transform(self, K):\n        \"\"\" Fit's using kernel K\"\"\"\n        # center kernel\n        K = self._centerer.fit_transform(K)\n\n        if self.n_components is None:\n            n_components = K.shape[0]\n        else:\n            n_components = min(K.shape[0], self.n_components)\n\n        # compute eigenvectors\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n(B)     tol : float, default=0\n        Convergence tolerance for arpack.\n        If 0, optimal value will be chosen by arpack.\n\n    max_iter : int, default=None\n        Maximum number of iterations for arpack.\n        If None, optimal value will be chosen by arpack.\n\n    remove_zero_eig : boolean, default=False\n        If True, then all components with zero eigenvalues are removed, so\n        that the number of components in the output may be < n_components\n        (and sometimes even zero due to numerical instability).\n        When n_components is None, this parameter is ignored and components\n        with zero eigenvalues are removed regardless.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``eigen_solver`` == 'arpack'.\n\n        .. versionadded:: 0.18\n\n    copy_X : boolean, default=True\n        If True, input X is copied and stored by the model in the `X_fit_`\n        attribute. If no further changes will be done to X, setting\n        `copy_X=False` saves memory by storing a reference.\n\n        .. versionadded:: 0.18\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    Attributes\n    ----------\n    lambdas_ : array, (n_components,)\n        Eigenvalues of the centered kernel matrix in decreasing order.\n        If `n_components` and `remove_zero_eig` are not set,\n        then all values are stored.\n\n    alphas_ : array, (n_samples, n_components)\n        Eigenvectors of the centered kernel matrix. If `n_components` and\n        `remove_zero_eig` are not set, then all components are stored.\n\n    dual_coef_ : array, (n_samples, n_features)\n        Inverse transform matrix. Only available when\n        ``fit_inverse_transform`` is True.\n\n    X_transformed_fit_ : array, (n_samples, n_components)\n        Projection of the fitted data on the kernel principal components.\n        Only available when ``fit_inverse_transform`` is True.\n\n    X_fit_ : (n_samples, n_features)\n        The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n        a reference. This attribute is used for the calls to transform.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import KernelPCA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = KernelPCA(n_components=7, kernel='linear')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    References\n    ----------\n    Kernel PCA was introduced in:\n        Bernhard Schoelkopf, Alexander J. Smola,\n        and Klaus-Robert Mueller. 1999. Kernel principal\n        component analysis. In Advances in kernel methods,\n        MIT Press, Cambridge, MA, USA 327-352.\n    \"\"\"\n\n    def __init__(self, n_components=None, kernel=\"linear\",\n                 gamma=None, degree=3, coef0=1, kernel_params=None,\n                 alpha=1.0, fit_inverse_transform=False, eigen_solver='auto',\n                 tol=0, max_iter=None, remove_zero_eig=False,\n                 random_state=None, copy_X=True, n_jobs=None):\n        if fit_inverse_transform and kernel == 'precomputed':\n            raise ValueError(\n                \"Cannot fit_inverse_transform with a precomputed kernel.\")\n        self.n_components = n_components\n        self.kernel = kernel\n        self.kernel_params = kernel_params\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.alpha = alpha\n        self.fit_inverse_transform = fit_inverse_transform\n        self.eigen_solver = eigen_solver\n        self.remove_zero_eig = remove_zero_eig\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n        self.copy_X = copy_X\n\n    @property\n    def _pairwise(self):\n        return self.kernel == \"precomputed\"\n\n    def _get_kernel(self, X, Y=None):\n        if callable(self.kernel):\n            params = self.kernel_params or {}\n        else:\n            params = {\"gamma\": self.gamma,\n                      \"degree\": self.degree,\n                      \"coef0\": self.coef0}\n        return pairwise_kernels(X, Y, metric=self.kernel,\n                                filter_params=True, n_jobs=self.n_jobs,\n                                **params)\n\n    def _fit_transform(self, K):\n        \"\"\" Fit's using kernel K\"\"\"\n        # center kernel\n        K = self._centerer.fit_transform(K)\n\n        if self.n_components is None:\n            n_components = K.shape[0]\n        else:\n            n_components = min(K.shape[0], self.n_components)\n\n        # compute eigenvectors\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n\n        return K\n\n    def _fit_inverse_transform(self, X_transformed, X):\n        if hasattr(X, \"tocsr\"):\n            raise NotImplementedError(\"Inverse transform not implemented for \"\n                                      \"sparse matrices!\")\n\n        n_samples = X_transformed.shape[0]\n        K = self._get_kernel(X_transformed)\n        K.flat[::n_samples + 1] += self.alpha\n        self.dual_coef_ = linalg.solve(K, X, sym_pos=True, overwrite_a=True)\n        self.X_transformed_fit_ = X_transformed\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = check_array(X, accept_sparse='csr', copy=self.copy_X)\n        self._centerer = KernelCenterer()\n        K = self._get_kernel(X)\n        self._fit_transform(K)\n\n        if self.fit_inverse_transform:\n            sqrt_lambdas = np.diag(np.sqrt(self.lambdas_))\n            X_transformed = np.dot(self.alphas_, sqrt_lambdas)\n            self._fit_inverse_transform(X_transformed, X)\n\n        self.X_fit_ = X\n        return self\n\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)", "answer": "A"}
{"uuid": "2117e53e-26c9-471b-9db9-5f0542680272", "issue_statement": "Pipeline should implement __len__\n#### Description\r\n\r\nWith the new indexing support `pipe[:len(pipe)]` raises an error.\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn import svm\r\nfrom sklearn.datasets import samples_generator\r\nfrom sklearn.feature_selection import SelectKBest\r\nfrom sklearn.feature_selection import f_regression\r\nfrom sklearn.pipeline import Pipeline\r\n\r\n# generate some data to play with\r\nX, y = samples_generator.make_classification(\r\n    n_informative=5, n_redundant=0, random_state=42)\r\n\r\nanova_filter = SelectKBest(f_regression, k=5)\r\nclf = svm.SVC(kernel='linear')\r\npipe = Pipeline([('anova', anova_filter), ('svc', clf)])\r\n\r\nlen(pipe)\r\n```\r\n\r\n#### Versions\r\n\r\n```\r\nSystem:\r\n    python: 3.6.7 | packaged by conda-forge | (default, Feb 19 2019, 18:37:23)  [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\r\nexecutable: /Users/krisz/.conda/envs/arrow36/bin/python\r\n   machine: Darwin-18.2.0-x86_64-i386-64bit\r\n\r\nBLAS:\r\n    macros: HAVE_CBLAS=None\r\n  lib_dirs: /Users/krisz/.conda/envs/arrow36/lib\r\ncblas_libs: openblas, openblas\r\n\r\nPython deps:\r\n       pip: 19.0.3\r\nsetuptools: 40.8.0\r\n   sklearn: 0.21.dev0\r\n     numpy: 1.16.2\r\n     scipy: 1.2.1\r\n    Cython: 0.29.6\r\n    pandas: 0.24.1\r\n```", "choices": "(A)     def __getitem__(self, ind):\n        \"\"\"Returns a sub-pipeline or a single esimtator in the pipeline\n\n        Indexing with an integer will return an estimator; using a slice\n        returns another Pipeline instance which copies a slice of this\n        Pipeline. This copy is shallow: modifying (or fitting) estimators in\n        the sub-pipeline will affect the larger pipeline and vice-versa.\n        However, replacing a value in `step` will not affect a copy.\n        \"\"\"\n        if isinstance(ind, slice):\n            if ind.step not in (1, None):\n                raise ValueError('Pipeline slicing only supports a step of 1')\n(B)             if trans is not None and trans != 'passthrough':\n                yield idx, name, trans\n\n    def __getitem__(self, ind):\n        \"\"\"Returns a sub-pipeline or a single esimtator in the pipeline\n\n        Indexing with an integer will return an estimator; using a slice\n        returns another Pipeline instance which copies a slice of this\n        Pipeline. This copy is shallow: modifying (or fitting) estimators in\n        the sub-pipeline will affect the larger pipeline and vice-versa.\n        However, replacing a value in `step` will not affect a copy.\n        \"\"\"\n(C)         Indexing with an integer will return an estimator; using a slice\n        returns another Pipeline instance which copies a slice of this\n        Pipeline. This copy is shallow: modifying (or fitting) estimators in\n        the sub-pipeline will affect the larger pipeline and vice-versa.\n        However, replacing a value in `step` will not affect a copy.\n        \"\"\"\n        if isinstance(ind, slice):\n            if ind.step not in (1, None):\n                raise ValueError('Pipeline slicing only supports a step of 1')\n            return self.__class__(self.steps[ind])\n        try:\n            name, est = self.steps[ind]\n(D)             stop -= 1\n\n        for idx, (name, trans) in enumerate(islice(self.steps, 0, stop)):\n            if trans is not None and trans != 'passthrough':\n                yield idx, name, trans\n\n    def __getitem__(self, ind):\n        \"\"\"Returns a sub-pipeline or a single esimtator in the pipeline\n\n        Indexing with an integer will return an estimator; using a slice\n        returns another Pipeline instance which copies a slice of this\n        Pipeline. This copy is shallow: modifying (or fitting) estimators in", "answer": "B"}
{"uuid": "43f0985b-7a66-4c84-8a61-faa452f3b9ae", "issue_statement": "Expose warm_start in Isolation forest\nIt seems to me that `sklearn.ensemble.IsolationForest` supports incremental addition of new trees with the `warm_start` parameter of its parent class, `sklearn.ensemble.BaseBagging`.\r\n\r\nEven though this parameter is not exposed in `__init__()` , it gets inherited from `BaseBagging` and one can use it by changing it to `True` after initialization. To make it work, you have to also increment `n_estimators` on every iteration. \r\n\r\nIt took me a while to notice that it actually works, and I had to inspect the source code of both `IsolationForest` and `BaseBagging`. Also, it looks to me that the behavior is in-line with `sklearn.ensemble.BaseForest` that is behind e.g. `sklearn.ensemble.RandomForestClassifier`.\r\n\r\nTo make it more easier to use, I'd suggest to:\r\n* expose `warm_start` in `IsolationForest.__init__()`, default `False`;\r\n* document it in the same way as it is documented for `RandomForestClassifier`, i.e. say:\r\n```py\r\n    warm_start : bool, optional (default=False)\r\n        When set to ``True``, reuse the solution of the previous call to fit\r\n        and add more estimators to the ensemble, otherwise, just fit a whole\r\n        new forest. See :term:`the Glossary <warm_start>`.\r\n```\r\n* add a test to make sure it works properly;\r\n* possibly also mention in the \"IsolationForest example\" documentation entry;", "choices": "(A)         dependent on the contamination parameter, in such a way that 0 becomes\n        its natural threshold to detect outliers.\n\n        .. versionadded:: 0.20\n           ``behaviour`` is added in 0.20 for back-compatibility purpose.\n\n        .. deprecated:: 0.20\n           ``behaviour='old'`` is deprecated in 0.20 and will not be possible\n           in 0.22.\n\n        .. deprecated:: 0.22\n           ``behaviour`` parameter will be deprecated in 0.22 and removed in\n           0.24.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n\n    Attributes\n    ----------\n    estimators_ : list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator.\n\n    max_samples_ : integer\n        The actual number of samples\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores.\n        We have the relation: ``decision_function = score_samples - offset_``.\n        Assuming behaviour == 'new', ``offset_`` is defined as follows.\n        When the contamination parameter is set to \"auto\", the offset is equal\n        to -0.5 as the scores of inliers are close to 0 and the scores of\n        outliers are close to -1. When a contamination parameter different\n        than \"auto\" is provided, the offset is defined in such a way we obtain\n        the expected number of outliers (samples with decision function < 0)\n        in training.\n        Assuming the behaviour parameter is set to 'old', we always have\n        ``offset_ = -0.5``, making the decision function independent from the\n        contamination parameter.\n\n    Notes\n    -----\n    The implementation is based on an ensemble of ExtraTreeRegressor. The\n    maximum depth of each tree is set to ``ceil(log_2(n))`` where\n    :math:`n` is the number of samples used to build the tree\n    (see (Liu et al., 2008) for more details).\n\n    References\n    ----------\n    .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n           Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n    .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n           anomaly detection.\" ACM Transactions on Knowledge Discovery from\n           Data (TKDD) 6.1 (2012): 3.\n\n    \"\"\"\n\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n(B)         When the contamination parameter is set to \"auto\", the offset is equal\n        to -0.5 as the scores of inliers are close to 0 and the scores of\n        outliers are close to -1. When a contamination parameter different\n        than \"auto\" is provided, the offset is defined in such a way we obtain\n        the expected number of outliers (samples with decision function < 0)\n        in training.\n        Assuming the behaviour parameter is set to 'old', we always have\n        ``offset_ = -0.5``, making the decision function independent from the\n        contamination parameter.\n\n    Notes\n    -----\n    The implementation is based on an ensemble of ExtraTreeRegressor. The\n    maximum depth of each tree is set to ``ceil(log_2(n))`` where\n    :math:`n` is the number of samples used to build the tree\n    (see (Liu et al., 2008) for more details).\n\n    References\n    ----------\n    .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n           Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n    .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n           anomaly detection.\" ACM Transactions on Knowledge Discovery from\n           Data (TKDD) 6.1 (2012): 3.\n\n    \"\"\"\n\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose)\n\n        self.behaviour = behaviour\n        self.contamination = contamination\n\n    def _set_oob_score(self, X, y):\n        raise NotImplementedError(\"OOB score not supported by iforest\")\n\n    def _parallel_args(self):\n        # ExtraTreeRegressor releases the GIL, so it's more efficient to use\n        # a thread-based backend rather than a process-based backend so as\n        # to avoid suffering from communication overhead and extra memory\n        # copies.\n        return _joblib_parallel_args(prefer='threads')\n\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Fit estimator.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            The input samples. Use ``dtype=np.float32`` for maximum\n            efficiency. Sparse matrices are also supported, use sparse\n            ``csc_matrix`` for maximum efficiency.\n\n        sample_weight : array-like, shape = [n_samples] or None\n            Sample weights. If None, then samples are equally weighted.\n\n        y : Ignored\n(C)     verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n\n    Attributes\n    ----------\n    estimators_ : list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    estimators_samples_ : list of arrays\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator.\n\n    max_samples_ : integer\n        The actual number of samples\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores.\n        We have the relation: ``decision_function = score_samples - offset_``.\n        Assuming behaviour == 'new', ``offset_`` is defined as follows.\n        When the contamination parameter is set to \"auto\", the offset is equal\n        to -0.5 as the scores of inliers are close to 0 and the scores of\n        outliers are close to -1. When a contamination parameter different\n        than \"auto\" is provided, the offset is defined in such a way we obtain\n        the expected number of outliers (samples with decision function < 0)\n        in training.\n        Assuming the behaviour parameter is set to 'old', we always have\n        ``offset_ = -0.5``, making the decision function independent from the\n        contamination parameter.\n\n    Notes\n    -----\n    The implementation is based on an ensemble of ExtraTreeRegressor. The\n    maximum depth of each tree is set to ``ceil(log_2(n))`` where\n    :math:`n` is the number of samples used to build the tree\n    (see (Liu et al., 2008) for more details).\n\n    References\n    ----------\n    .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n           Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n    .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n           anomaly detection.\" ACM Transactions on Knowledge Discovery from\n           Data (TKDD) 6.1 (2012): 3.\n\n    \"\"\"\n\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose)\n\n        self.behaviour = behaviour\n        self.contamination = contamination\n\n    def _set_oob_score(self, X, y):\n        raise NotImplementedError(\"OOB score not supported by iforest\")\n\n    def _parallel_args(self):\n(D)            Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n    .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n           anomaly detection.\" ACM Transactions on Knowledge Discovery from\n           Data (TKDD) 6.1 (2012): 3.\n\n    \"\"\"\n\n    def __init__(self,\n                 n_estimators=100,\n                 max_samples=\"auto\",\n                 contamination=\"legacy\",\n                 max_features=1.,\n                 bootstrap=False,\n                 n_jobs=None,\n                 behaviour='old',\n                 random_state=None,\n                 verbose=0):\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(\n                max_features=1,\n                splitter='random',\n                random_state=random_state),\n            # here above max_features has no links with self.max_features\n            bootstrap=bootstrap,\n            bootstrap_features=False,\n            n_estimators=n_estimators,\n            max_samples=max_samples,\n            max_features=max_features,\n            n_jobs=n_jobs,\n            random_state=random_state,\n            verbose=verbose)\n\n        self.behaviour = behaviour\n        self.contamination = contamination\n\n    def _set_oob_score(self, X, y):\n        raise NotImplementedError(\"OOB score not supported by iforest\")\n\n    def _parallel_args(self):\n        # ExtraTreeRegressor releases the GIL, so it's more efficient to use\n        # a thread-based backend rather than a process-based backend so as\n        # to avoid suffering from communication overhead and extra memory\n        # copies.\n        return _joblib_parallel_args(prefer='threads')\n\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Fit estimator.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            The input samples. Use ``dtype=np.float32`` for maximum\n            efficiency. Sparse matrices are also supported, use sparse\n            ``csc_matrix`` for maximum efficiency.\n\n        sample_weight : array-like, shape = [n_samples] or None\n            Sample weights. If None, then samples are equally weighted.\n\n        y : Ignored\n            not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n        \"\"\"\n        if self.contamination == \"legacy\":\n            warn('default contamination parameter 0.1 will change '\n                 'in version 0.22 to \"auto\". This will change the '\n                 'predict method behavior.',\n                 FutureWarning)\n            self._contamination = 0.1\n        else:\n            self._contamination = self.contamination\n\n        if self.behaviour == 'old':\n            warn('behaviour=\"old\" is deprecated and will be removed '\n                 'in version 0.22. Please use behaviour=\"new\", which '\n                 'makes the decision_function change to match '\n                 'other anomaly detection algorithm API.',", "answer": "C"}
{"uuid": "865c8de3-9a52-45e1-a3a9-ae35be9a41a7", "issue_statement": "Comparing string to array in _estimate_mi\nIn ``_estimate_mi`` there is ``discrete_features == 'auto'`` but discrete features can be an array of indices or a boolean mask.\r\nThis will error in future versions of numpy.\r\nAlso this means we never test this function with discrete features != 'auto', it seems?", "choices": "(A)     nn.set_params(algorithm='kd_tree')\n    nn.fit(c)\n    ind = nn.radius_neighbors(radius=radius, return_distance=False)\n    m_all = np.array([i.size for i in ind])\n\n    mi = (digamma(n_samples) + np.mean(digamma(k_all)) -\n          np.mean(digamma(label_counts)) -\n          np.mean(digamma(m_all + 1)))\n\n    return max(0, mi)\n\n\ndef _compute_mi(x, y, x_discrete, y_discrete, n_neighbors=3):\n    \"\"\"Compute mutual information between two variables.\n\n    This is a simple wrapper which selects a proper function to call based on\n    whether `x` and `y` are discrete or not.\n    \"\"\"\n    if x_discrete and y_discrete:\n        return mutual_info_score(x, y)\n    elif x_discrete and not y_discrete:\n        return _compute_mi_cd(y, x, n_neighbors)\n    elif not x_discrete and y_discrete:\n        return _compute_mi_cd(x, y, n_neighbors)\n    else:\n        return _compute_mi_cc(x, y, n_neighbors)\n\n\ndef _iterate_columns(X, columns=None):\n    \"\"\"Iterate over columns of a matrix.\n\n    Parameters\n    ----------\n    X : ndarray or csc_matrix, shape (n_samples, n_features)\n        Matrix over which to iterate.\n\n    columns : iterable or None, default None\n        Indices of columns to iterate over. If None, iterate over all columns.\n\n    Yields\n    ------\n    x : ndarray, shape (n_samples,)\n        Columns of `X` in dense format.\n    \"\"\"\n    if columns is None:\n        columns = range(X.shape[1])\n\n    if issparse(X):\n        for i in columns:\n            x = np.zeros(X.shape[0])\n            start_ptr, end_ptr = X.indptr[i], X.indptr[i + 1]\n            x[X.indices[start_ptr:end_ptr]] = X.data[start_ptr:end_ptr]\n            yield x\n    else:\n        for i in columns:\n            yield X[:, i]\n\n\ndef _estimate_mi(X, y, discrete_features='auto', discrete_target=False,\n                 n_neighbors=3, copy=True, random_state=None):\n    \"\"\"Estimate mutual information between the features and the target.\n\n    Parameters\n    ----------\n    X : array_like or sparse matrix, shape (n_samples, n_features)\n        Feature matrix.\n\n    y : array_like, shape (n_samples,)\n        Target vector.\n\n    discrete_features : {'auto', bool, array_like}, default 'auto'\n        If bool, then determines whether to consider all features discrete\n        or continuous. If array, then it should be either a boolean mask\n        with shape (n_features,) or array with indices of discrete features.\n        If 'auto', it is assigned to False for dense `X` and to True for\n        sparse `X`.\n\n    discrete_target : bool, default False\n        Whether to consider `y` as a discrete variable.\n\n    n_neighbors : int, default 3\n        Number of neighbors to use for MI estimation for continuous variables,\n        see [1]_ and [2]_. Higher values reduce variance of the estimation, but\n        could introduce a bias.\n\n    copy : bool, default True\n        Whether to make a copy of the given data. If set to False, the initial\n        data will be overwritten.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator for adding small noise\n        to continuous variables in order to remove repeated values.  If int,\n        random_state is the seed used by the random number generator; If\n        RandomState instance, random_state is the random number generator; If\n        None, the random number generator is the RandomState instance used by\n        `np.random`.\n\n    Returns\n    -------\n    mi : ndarray, shape (n_features,)\n        Estimated mutual information between each feature and the target.\n        A negative value will be replaced by 0.\n\n    References\n    ----------\n    .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n           information\". Phys. Rev. E 69, 2004.\n    .. [2] B. C. Ross \"Mutual Information between Discrete and Continuous\n           Data Sets\". PLoS ONE 9(2), 2014.\n    \"\"\"\n    X, y = check_X_y(X, y, accept_sparse='csc', y_numeric=not discrete_target)\n    n_samples, n_features = X.shape\n\n    if discrete_features == 'auto':\n        discrete_features = issparse(X)\n\n    if isinstance(discrete_features, bool):\n        discrete_mask = np.empty(n_features, dtype=bool)\n        discrete_mask.fill(discrete_features)\n    else:\n        discrete_features = np.asarray(discrete_features)\n        if discrete_features.dtype != 'bool':\n            discrete_mask = np.zeros(n_features, dtype=bool)\n            discrete_mask[discrete_features] = True\n        else:\n            discrete_mask = discrete_features\n\n    continuous_mask = ~discrete_mask\n    if np.any(continuous_mask) and issparse(X):\n        raise ValueError(\"Sparse matrix `X` can't have continuous features.\")\n\n    rng = check_random_state(random_state)\n    if np.any(continuous_mask):\n        if copy:\n            X = X.copy()\n\n        if not discrete_target:\n            X[:, continuous_mask] = scale(X[:, continuous_mask],\n                                          with_mean=False, copy=False)\n\n        # Add small noise to continuous features as advised in Kraskov et. al.\n        X = X.astype(float, **_astype_copy_false(X))\n        means = np.maximum(1, np.mean(np.abs(X[:, continuous_mask]), axis=0))\n        X[:, continuous_mask] += 1e-10 * means * rng.randn(\n                n_samples, np.sum(continuous_mask))\n\n    if not discrete_target:\n        y = scale(y, with_mean=False)\n        y += 1e-10 * np.maximum(1, np.mean(np.abs(y))) * rng.randn(n_samples)\n\n    mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n          x, discrete_feature in zip(_iterate_columns(X), discrete_mask)]\n\n    return np.array(mi)\n\n\ndef mutual_info_regression(X, y, discrete_features='auto', n_neighbors=3,\n                           copy=True, random_state=None):\n    \"\"\"Estimate mutual information for a continuous target variable.\n\n    Mutual information (MI) [1]_ between two random variables is a non-negative\n    value, which measures the dependency between the variables. It is equal\n    to zero if and only if two random variables are independent, and higher\n    values mean higher dependency.\n\n    The function relies on nonparametric methods based on entropy estimation\n    from k-nearest neighbors distances as described in [2]_ and [3]_. Both\n    methods are based on the idea originally proposed in [4]_.\n\n    It can be used for univariate features selection, read more in the\n    :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    X : array_like or sparse matrix, shape (n_samples, n_features)\n        Feature matrix.\n\n    y : array_like, shape (n_samples,)\n        Target vector.\n\n    discrete_features : {'auto', bool, array_like}, default 'auto'\n        If bool, then determines whether to consider all features discrete\n        or continuous. If array, then it should be either a boolean mask\n        with shape (n_features,) or array with indices of discrete features.\n        If 'auto', it is assigned to False for dense `X` and to True for\n        sparse `X`.\n\n    n_neighbors : int, default 3\n        Number of neighbors to use for MI estimation for continuous variables,\n        see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n        could introduce a bias.\n\n    copy : bool, default True\n        Whether to make a copy of the given data. If set to False, the initial\n        data will be overwritten.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator for adding small noise\n        to continuous variables in order to remove repeated values.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    mi : ndarray, shape (n_features,)\n        Estimated mutual information between each feature and the target.\n\n    Notes\n    -----\n    1. The term \"discrete features\" is used instead of naming them\n       \"categorical\", because it describes the essence more accurately.\n       For example, pixel intensities of an image are discrete features\n       (but hardly categorical) and you will get better results if mark them\n       as such. Also note, that treating a continuous variable as discrete and\n       vice versa will usually give incorrect results, so be attentive about that.\n    2. True mutual information can't be negative. If its estimate turns out\n       to be negative, it is replaced by zero.\n\n    References\n    ----------\n    .. [1] `Mutual Information <https://en.wikipedia.org/wiki/Mutual_information>`_\n           on Wikipedia.\n    .. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n           information\". Phys. Rev. E 69, 2004.\n    .. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n           Data Sets\". PLoS ONE 9(2), 2014.\n    .. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n           of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16\n    \"\"\"\n    return _estimate_mi(X, y, discrete_features, False, n_neighbors,\n                        copy, random_state)\n\n\ndef mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3,\n                        copy=True, random_state=None):\n    \"\"\"Estimate mutual information for a discrete target variable.\n\n    Mutual information (MI) [1]_ between two random variables is a non-negative\n    value, which measures the dependency between the variables. It is equal\n    to zero if and only if two random variables are independent, and higher\n    values mean higher dependency.\n\n    The function relies on nonparametric methods based on entropy estimation\n    from k-nearest neighbors distances as described in [2]_ and [3]_. Both\n    methods are based on the idea originally proposed in [4]_.\n\n    It can be used for univariate features selection, read more in the\n    :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n(B) from ..preprocessing import scale\nfrom ..utils import check_random_state\nfrom ..utils.fixes import _astype_copy_false\nfrom ..utils.validation import check_X_y\nfrom ..utils.multiclass import check_classification_targets\n\n\ndef _compute_mi_cc(x, y, n_neighbors):\n    \"\"\"Compute mutual information between two continuous variables.\n\n    Parameters\n    ----------\n    x, y : ndarray, shape (n_samples,)\n        Samples of two continuous random variables, must have an identical\n        shape.\n\n    n_neighbors : int\n        Number of nearest neighbors to search for each point, see [1]_.\n\n    Returns\n    -------\n    mi : float\n        Estimated mutual information. If it turned out to be negative it is\n        replace by 0.\n\n    Notes\n    -----\n    True mutual information can't be negative. If its estimate by a numerical\n    method is negative, it means (providing the method is adequate) that the\n    mutual information is close to 0 and replacing it by 0 is a reasonable\n    strategy.\n\n    References\n    ----------\n    .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n           information\". Phys. Rev. E 69, 2004.\n    \"\"\"\n    n_samples = x.size\n\n    x = x.reshape((-1, 1))\n    y = y.reshape((-1, 1))\n    xy = np.hstack((x, y))\n\n    # Here we rely on NearestNeighbors to select the fastest algorithm.\n    nn = NearestNeighbors(metric='chebyshev', n_neighbors=n_neighbors)\n\n    nn.fit(xy)\n    radius = nn.kneighbors()[0]\n    radius = np.nextafter(radius[:, -1], 0)\n\n    # Algorithm is selected explicitly to allow passing an array as radius\n    # later (not all algorithms support this).\n    nn.set_params(algorithm='kd_tree')\n\n    nn.fit(x)\n    ind = nn.radius_neighbors(radius=radius, return_distance=False)\n    nx = np.array([i.size for i in ind])\n\n    nn.fit(y)\n    ind = nn.radius_neighbors(radius=radius, return_distance=False)\n    ny = np.array([i.size for i in ind])\n\n    mi = (digamma(n_samples) + digamma(n_neighbors) -\n          np.mean(digamma(nx + 1)) - np.mean(digamma(ny + 1)))\n\n    return max(0, mi)\n\n\ndef _compute_mi_cd(c, d, n_neighbors):\n    \"\"\"Compute mutual information between continuous and discrete variables.\n\n    Parameters\n    ----------\n    c : ndarray, shape (n_samples,)\n        Samples of a continuous random variable.\n\n    d : ndarray, shape (n_samples,)\n        Samples of a discrete random variable.\n\n    n_neighbors : int\n        Number of nearest neighbors to search for each point, see [1]_.\n\n    Returns\n    -------\n    mi : float\n        Estimated mutual information. If it turned out to be negative it is\n        replace by 0.\n\n    Notes\n    -----\n    True mutual information can't be negative. If its estimate by a numerical\n    method is negative, it means (providing the method is adequate) that the\n    mutual information is close to 0 and replacing it by 0 is a reasonable\n    strategy.\n\n    References\n    ----------\n    .. [1] B. C. Ross \"Mutual Information between Discrete and Continuous\n       Data Sets\". PLoS ONE 9(2), 2014.\n    \"\"\"\n    n_samples = c.shape[0]\n    c = c.reshape((-1, 1))\n\n    radius = np.empty(n_samples)\n    label_counts = np.empty(n_samples)\n    k_all = np.empty(n_samples)\n    nn = NearestNeighbors()\n    for label in np.unique(d):\n        mask = d == label\n        count = np.sum(mask)\n        if count > 1:\n            k = min(n_neighbors, count - 1)\n            nn.set_params(n_neighbors=k)\n            nn.fit(c[mask])\n            r = nn.kneighbors()[0]\n            radius[mask] = np.nextafter(r[:, -1], 0)\n            k_all[mask] = k\n        label_counts[mask] = count\n\n    # Ignore points with unique labels.\n    mask = label_counts > 1\n    n_samples = np.sum(mask)\n    label_counts = label_counts[mask]\n    k_all = k_all[mask]\n    c = c[mask]\n    radius = radius[mask]\n\n    nn.set_params(algorithm='kd_tree')\n    nn.fit(c)\n    ind = nn.radius_neighbors(radius=radius, return_distance=False)\n    m_all = np.array([i.size for i in ind])\n\n    mi = (digamma(n_samples) + np.mean(digamma(k_all)) -\n          np.mean(digamma(label_counts)) -\n          np.mean(digamma(m_all + 1)))\n\n    return max(0, mi)\n\n\ndef _compute_mi(x, y, x_discrete, y_discrete, n_neighbors=3):\n    \"\"\"Compute mutual information between two variables.\n\n    This is a simple wrapper which selects a proper function to call based on\n    whether `x` and `y` are discrete or not.\n    \"\"\"\n    if x_discrete and y_discrete:\n        return mutual_info_score(x, y)\n    elif x_discrete and not y_discrete:\n        return _compute_mi_cd(y, x, n_neighbors)\n    elif not x_discrete and y_discrete:\n        return _compute_mi_cd(x, y, n_neighbors)\n    else:\n        return _compute_mi_cc(x, y, n_neighbors)\n\n\ndef _iterate_columns(X, columns=None):\n    \"\"\"Iterate over columns of a matrix.\n\n    Parameters\n    ----------\n    X : ndarray or csc_matrix, shape (n_samples, n_features)\n        Matrix over which to iterate.\n\n    columns : iterable or None, default None\n        Indices of columns to iterate over. If None, iterate over all columns.\n\n    Yields\n    ------\n    x : ndarray, shape (n_samples,)\n        Columns of `X` in dense format.\n    \"\"\"\n    if columns is None:\n        columns = range(X.shape[1])\n\n    if issparse(X):\n        for i in columns:\n            x = np.zeros(X.shape[0])\n            start_ptr, end_ptr = X.indptr[i], X.indptr[i + 1]\n            x[X.indices[start_ptr:end_ptr]] = X.data[start_ptr:end_ptr]\n            yield x\n    else:\n        for i in columns:\n            yield X[:, i]\n\n\ndef _estimate_mi(X, y, discrete_features='auto', discrete_target=False,\n                 n_neighbors=3, copy=True, random_state=None):\n    \"\"\"Estimate mutual information between the features and the target.\n\n    Parameters\n    ----------\n    X : array_like or sparse matrix, shape (n_samples, n_features)\n        Feature matrix.\n\n    y : array_like, shape (n_samples,)\n        Target vector.\n\n    discrete_features : {'auto', bool, array_like}, default 'auto'\n        If bool, then determines whether to consider all features discrete\n        or continuous. If array, then it should be either a boolean mask\n        with shape (n_features,) or array with indices of discrete features.\n        If 'auto', it is assigned to False for dense `X` and to True for\n        sparse `X`.\n\n    discrete_target : bool, default False\n        Whether to consider `y` as a discrete variable.\n\n    n_neighbors : int, default 3\n        Number of neighbors to use for MI estimation for continuous variables,\n        see [1]_ and [2]_. Higher values reduce variance of the estimation, but\n        could introduce a bias.\n\n    copy : bool, default True\n        Whether to make a copy of the given data. If set to False, the initial\n        data will be overwritten.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator for adding small noise\n        to continuous variables in order to remove repeated values.  If int,\n        random_state is the seed used by the random number generator; If\n        RandomState instance, random_state is the random number generator; If\n        None, the random number generator is the RandomState instance used by\n        `np.random`.\n\n    Returns\n    -------\n    mi : ndarray, shape (n_features,)\n        Estimated mutual information between each feature and the target.\n        A negative value will be replaced by 0.\n\n    References\n    ----------\n    .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n           information\". Phys. Rev. E 69, 2004.\n    .. [2] B. C. Ross \"Mutual Information between Discrete and Continuous\n           Data Sets\". PLoS ONE 9(2), 2014.\n    \"\"\"\n    X, y = check_X_y(X, y, accept_sparse='csc', y_numeric=not discrete_target)\n    n_samples, n_features = X.shape\n\n    if discrete_features == 'auto':\n        discrete_features = issparse(X)\n\n    if isinstance(discrete_features, bool):\n        discrete_mask = np.empty(n_features, dtype=bool)\n        discrete_mask.fill(discrete_features)\n    else:\n        discrete_features = np.asarray(discrete_features)\n        if discrete_features.dtype != 'bool':\n            discrete_mask = np.zeros(n_features, dtype=bool)\n            discrete_mask[discrete_features] = True\n        else:\n            discrete_mask = discrete_features\n(C) \n    return max(0, mi)\n\n\ndef _compute_mi_cd(c, d, n_neighbors):\n    \"\"\"Compute mutual information between continuous and discrete variables.\n\n    Parameters\n    ----------\n    c : ndarray, shape (n_samples,)\n        Samples of a continuous random variable.\n\n    d : ndarray, shape (n_samples,)\n        Samples of a discrete random variable.\n\n    n_neighbors : int\n        Number of nearest neighbors to search for each point, see [1]_.\n\n    Returns\n    -------\n    mi : float\n        Estimated mutual information. If it turned out to be negative it is\n        replace by 0.\n\n    Notes\n    -----\n    True mutual information can't be negative. If its estimate by a numerical\n    method is negative, it means (providing the method is adequate) that the\n    mutual information is close to 0 and replacing it by 0 is a reasonable\n    strategy.\n\n    References\n    ----------\n    .. [1] B. C. Ross \"Mutual Information between Discrete and Continuous\n       Data Sets\". PLoS ONE 9(2), 2014.\n    \"\"\"\n    n_samples = c.shape[0]\n    c = c.reshape((-1, 1))\n\n    radius = np.empty(n_samples)\n    label_counts = np.empty(n_samples)\n    k_all = np.empty(n_samples)\n    nn = NearestNeighbors()\n    for label in np.unique(d):\n        mask = d == label\n        count = np.sum(mask)\n        if count > 1:\n            k = min(n_neighbors, count - 1)\n            nn.set_params(n_neighbors=k)\n            nn.fit(c[mask])\n            r = nn.kneighbors()[0]\n            radius[mask] = np.nextafter(r[:, -1], 0)\n            k_all[mask] = k\n        label_counts[mask] = count\n\n    # Ignore points with unique labels.\n    mask = label_counts > 1\n    n_samples = np.sum(mask)\n    label_counts = label_counts[mask]\n    k_all = k_all[mask]\n    c = c[mask]\n    radius = radius[mask]\n\n    nn.set_params(algorithm='kd_tree')\n    nn.fit(c)\n    ind = nn.radius_neighbors(radius=radius, return_distance=False)\n    m_all = np.array([i.size for i in ind])\n\n    mi = (digamma(n_samples) + np.mean(digamma(k_all)) -\n          np.mean(digamma(label_counts)) -\n          np.mean(digamma(m_all + 1)))\n\n    return max(0, mi)\n\n\ndef _compute_mi(x, y, x_discrete, y_discrete, n_neighbors=3):\n    \"\"\"Compute mutual information between two variables.\n\n    This is a simple wrapper which selects a proper function to call based on\n    whether `x` and `y` are discrete or not.\n    \"\"\"\n    if x_discrete and y_discrete:\n        return mutual_info_score(x, y)\n    elif x_discrete and not y_discrete:\n        return _compute_mi_cd(y, x, n_neighbors)\n    elif not x_discrete and y_discrete:\n        return _compute_mi_cd(x, y, n_neighbors)\n    else:\n        return _compute_mi_cc(x, y, n_neighbors)\n\n\ndef _iterate_columns(X, columns=None):\n    \"\"\"Iterate over columns of a matrix.\n\n    Parameters\n    ----------\n    X : ndarray or csc_matrix, shape (n_samples, n_features)\n        Matrix over which to iterate.\n\n    columns : iterable or None, default None\n        Indices of columns to iterate over. If None, iterate over all columns.\n\n    Yields\n    ------\n    x : ndarray, shape (n_samples,)\n        Columns of `X` in dense format.\n    \"\"\"\n    if columns is None:\n        columns = range(X.shape[1])\n\n    if issparse(X):\n        for i in columns:\n            x = np.zeros(X.shape[0])\n            start_ptr, end_ptr = X.indptr[i], X.indptr[i + 1]\n            x[X.indices[start_ptr:end_ptr]] = X.data[start_ptr:end_ptr]\n            yield x\n    else:\n        for i in columns:\n            yield X[:, i]\n\n\ndef _estimate_mi(X, y, discrete_features='auto', discrete_target=False,\n                 n_neighbors=3, copy=True, random_state=None):\n    \"\"\"Estimate mutual information between the features and the target.\n\n    Parameters\n    ----------\n    X : array_like or sparse matrix, shape (n_samples, n_features)\n        Feature matrix.\n\n    y : array_like, shape (n_samples,)\n        Target vector.\n\n    discrete_features : {'auto', bool, array_like}, default 'auto'\n        If bool, then determines whether to consider all features discrete\n        or continuous. If array, then it should be either a boolean mask\n        with shape (n_features,) or array with indices of discrete features.\n        If 'auto', it is assigned to False for dense `X` and to True for\n        sparse `X`.\n\n    discrete_target : bool, default False\n        Whether to consider `y` as a discrete variable.\n\n    n_neighbors : int, default 3\n        Number of neighbors to use for MI estimation for continuous variables,\n        see [1]_ and [2]_. Higher values reduce variance of the estimation, but\n        could introduce a bias.\n\n    copy : bool, default True\n        Whether to make a copy of the given data. If set to False, the initial\n        data will be overwritten.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator for adding small noise\n        to continuous variables in order to remove repeated values.  If int,\n        random_state is the seed used by the random number generator; If\n        RandomState instance, random_state is the random number generator; If\n        None, the random number generator is the RandomState instance used by\n        `np.random`.\n\n    Returns\n    -------\n    mi : ndarray, shape (n_features,)\n        Estimated mutual information between each feature and the target.\n        A negative value will be replaced by 0.\n\n    References\n    ----------\n    .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n           information\". Phys. Rev. E 69, 2004.\n    .. [2] B. C. Ross \"Mutual Information between Discrete and Continuous\n           Data Sets\". PLoS ONE 9(2), 2014.\n    \"\"\"\n    X, y = check_X_y(X, y, accept_sparse='csc', y_numeric=not discrete_target)\n    n_samples, n_features = X.shape\n\n    if discrete_features == 'auto':\n        discrete_features = issparse(X)\n\n    if isinstance(discrete_features, bool):\n        discrete_mask = np.empty(n_features, dtype=bool)\n        discrete_mask.fill(discrete_features)\n    else:\n        discrete_features = np.asarray(discrete_features)\n        if discrete_features.dtype != 'bool':\n            discrete_mask = np.zeros(n_features, dtype=bool)\n            discrete_mask[discrete_features] = True\n        else:\n            discrete_mask = discrete_features\n\n    continuous_mask = ~discrete_mask\n    if np.any(continuous_mask) and issparse(X):\n        raise ValueError(\"Sparse matrix `X` can't have continuous features.\")\n\n    rng = check_random_state(random_state)\n    if np.any(continuous_mask):\n        if copy:\n            X = X.copy()\n\n        if not discrete_target:\n            X[:, continuous_mask] = scale(X[:, continuous_mask],\n                                          with_mean=False, copy=False)\n\n        # Add small noise to continuous features as advised in Kraskov et. al.\n        X = X.astype(float, **_astype_copy_false(X))\n        means = np.maximum(1, np.mean(np.abs(X[:, continuous_mask]), axis=0))\n        X[:, continuous_mask] += 1e-10 * means * rng.randn(\n                n_samples, np.sum(continuous_mask))\n\n    if not discrete_target:\n        y = scale(y, with_mean=False)\n        y += 1e-10 * np.maximum(1, np.mean(np.abs(y))) * rng.randn(n_samples)\n\n    mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n          x, discrete_feature in zip(_iterate_columns(X), discrete_mask)]\n\n    return np.array(mi)\n\n\ndef mutual_info_regression(X, y, discrete_features='auto', n_neighbors=3,\n                           copy=True, random_state=None):\n    \"\"\"Estimate mutual information for a continuous target variable.\n\n    Mutual information (MI) [1]_ between two random variables is a non-negative\n    value, which measures the dependency between the variables. It is equal\n    to zero if and only if two random variables are independent, and higher\n    values mean higher dependency.\n\n    The function relies on nonparametric methods based on entropy estimation\n    from k-nearest neighbors distances as described in [2]_ and [3]_. Both\n    methods are based on the idea originally proposed in [4]_.\n\n    It can be used for univariate features selection, read more in the\n    :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    X : array_like or sparse matrix, shape (n_samples, n_features)\n        Feature matrix.\n\n    y : array_like, shape (n_samples,)\n        Target vector.\n\n    discrete_features : {'auto', bool, array_like}, default 'auto'\n        If bool, then determines whether to consider all features discrete\n        or continuous. If array, then it should be either a boolean mask\n        with shape (n_features,) or array with indices of discrete features.\n        If 'auto', it is assigned to False for dense `X` and to True for\n        sparse `X`.\n\n    n_neighbors : int, default 3\n        Number of neighbors to use for MI estimation for continuous variables,\n        see [2]_ and [3]_. Higher values reduce variance of the estimation, but", "answer": "B"}
{"uuid": "039553d0-cd34-440c-bee0-5d5371aae46f", "issue_statement": "bug in print_changed_only in new repr: vector values\n```python\r\nimport sklearn\r\nimport numpy as np\r\nfrom sklearn.linear_model import LogisticRegressionCV\r\nsklearn.set_config(print_changed_only=True)\r\nprint(LogisticRegressionCV(Cs=np.array([0.1, 1])))\r\n```\r\n> ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n\r\nping @NicolasHug", "choices": "(A)     init_func = getattr(estimator.__init__, 'deprecated_original',\n                        estimator.__init__)\n    init_params = signature(init_func).parameters\n    init_params = {name: param.default for name, param in init_params.items()}\n    for k, v in params.items():\n        if (v != init_params[k] and\n                not (is_scalar_nan(init_params[k]) and is_scalar_nan(v))):\n(B)     init_params = signature(init_func).parameters\n    init_params = {name: param.default for name, param in init_params.items()}\n    for k, v in params.items():\n        if (v != init_params[k] and\n                not (is_scalar_nan(init_params[k]) and is_scalar_nan(v))):\n            filtered_params[k] = v\n    return filtered_params\n(C)                 not (is_scalar_nan(init_params[k]) and is_scalar_nan(v))):\n            filtered_params[k] = v\n    return filtered_params\n\n\nclass _EstimatorPrettyPrinter(pprint.PrettyPrinter):\n    \"\"\"Pretty Printer class for estimator objects.\n(D)     for k, v in params.items():\n        if (v != init_params[k] and\n                not (is_scalar_nan(init_params[k]) and is_scalar_nan(v))):\n            filtered_params[k] = v\n    return filtered_params\n\n", "answer": "B"}
{"uuid": "d3b12080-13b7-4356-bf24-79ec83eda154", "issue_statement": "Voting estimator will fail at fit if weights are passed and an estimator is None\nBecause we don't check for an estimator to be `None` in `sample_weight` support, `fit` is failing`.\r\n\r\n```python\r\n    X, y = load_iris(return_X_y=True)\r\n    voter = VotingClassifier(\r\n        estimators=[('lr', LogisticRegression()),\r\n                    ('rf', RandomForestClassifier())]\r\n    )\r\n    voter.fit(X, y, sample_weight=np.ones(y.shape))\r\n    voter.set_params(lr=None)\r\n    voter.fit(X, y, sample_weight=np.ones(y.shape))\r\n```\r\n\r\n```\r\nAttributeError: 'NoneType' object has no attribute 'fit'\r\n```", "choices": "(A)                              '; got %d weights, %d estimators'\n                             % (len(self.weights), len(self.estimators)))\n\n        if sample_weight is not None:\n            for name, step in self.estimators:\n                if not has_fit_parameter(step, 'sample_weight'):\n                    raise ValueError('Underlying estimator \\'%s\\' does not'\n                                     ' support sample weights.' % name)\n(B) \n        if sample_weight is not None:\n            for name, step in self.estimators:\n                if not has_fit_parameter(step, 'sample_weight'):\n                    raise ValueError('Underlying estimator \\'%s\\' does not'\n                                     ' support sample weights.' % name)\n\n        names, clfs = zip(*self.estimators)\n(C)                     raise ValueError('Underlying estimator \\'%s\\' does not'\n                                     ' support sample weights.' % name)\n\n        names, clfs = zip(*self.estimators)\n        self._validate_names(names)\n\n        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n        if n_isnone == len(self.estimators):\n(D)             for name, step in self.estimators:\n                if not has_fit_parameter(step, 'sample_weight'):\n                    raise ValueError('Underlying estimator \\'%s\\' does not'\n                                     ' support sample weights.' % name)\n\n        names, clfs = zip(*self.estimators)\n        self._validate_names(names)\n", "answer": "B"}
{"uuid": "43d8dea1-35e2-40cc-aaf0-11897d3fc697", "issue_statement": "IndexError thrown with LogisticRegressionCV and refit=False\n#### Description\r\nThe following error is thrown when trying to estimate a regularization parameter via cross-validation, *without* refitting.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nimport sys\r\nimport sklearn\r\nfrom sklearn.linear_model import LogisticRegressionCV\r\nimport numpy as np\r\n\r\nnp.random.seed(29)\r\nX = np.random.normal(size=(1000, 3))\r\nbeta = np.random.normal(size=3)\r\nintercept = np.random.normal(size=None)\r\ny = np.sign(intercept + X @ beta)\r\n\r\nLogisticRegressionCV(\r\ncv=5,\r\nsolver='saga', # same error with 'liblinear'\r\ntol=1e-2,\r\nrefit=False).fit(X, y)\r\n```\r\n\r\n\r\n#### Expected Results\r\nNo error is thrown. \r\n\r\n#### Actual Results\r\n```\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-3-81609fd8d2ca> in <module>\r\n----> 1 LogisticRegressionCV(refit=False).fit(X, y)\r\n\r\n~/.pyenv/versions/3.6.7/envs/jupyter/lib/python3.6/site-packages/sklearn/linear_model/logistic.py in fit(self, X, y, sample_weight)\r\n   2192                 else:\r\n   2193                     w = np.mean([coefs_paths[:, i, best_indices[i], :]\r\n-> 2194                                  for i in range(len(folds))], axis=0)\r\n   2195 \r\n   2196                 best_indices_C = best_indices % len(self.Cs_)\r\n\r\n~/.pyenv/versions/3.6.7/envs/jupyter/lib/python3.6/site-packages/sklearn/linear_model/logistic.py in <listcomp>(.0)\r\n   2192                 else:\r\n   2193                     w = np.mean([coefs_paths[:, i, best_indices[i], :]\r\n-> 2194                                  for i in range(len(folds))], axis=0)\r\n   2195 \r\n   2196                 best_indices_C = best_indices % len(self.Cs_)\r\n\r\nIndexError: too many indices for array\r\n```\r\n\r\n#### Versions\r\n```\r\nSystem:\r\n    python: 3.6.7 (default, May 13 2019, 16:14:45)  [GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)]\r\nexecutable: /Users/tsweetser/.pyenv/versions/3.6.7/envs/jupyter/bin/python\r\n   machine: Darwin-18.6.0-x86_64-i386-64bit\r\n\r\nBLAS:\r\n    macros: NO_ATLAS_INFO=3, HAVE_CBLAS=None\r\n  lib_dirs: \r\ncblas_libs: cblas\r\n\r\nPython deps:\r\n       pip: 19.1.1\r\nsetuptools: 39.0.1\r\n   sklearn: 0.21.2\r\n     numpy: 1.15.1\r\n     scipy: 1.1.0\r\n    Cython: 0.29.6\r\n    pandas: 0.24.2\r\n```", "choices": "(A)                 self.C_.append(np.mean(self.Cs_[best_indices_C]))\n\n                best_indices_l1 = best_indices // len(self.Cs_)\n                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))\n\n            if multi_class == 'multinomial':\n                self.C_ = np.tile(self.C_, n_classes)\n                self.l1_ratio_ = np.tile(self.l1_ratio_, n_classes)\n                self.coef_ = w[:, :X.shape[1]]\n                if self.fit_intercept:\n                    self.intercept_ = w[:, -1]\n            else:\n                self.coef_[index] = w[: X.shape[1]]\n                if self.fit_intercept:\n                    self.intercept_[index] = w[-1]\n\n        self.C_ = np.asarray(self.C_)\n        self.l1_ratio_ = np.asarray(self.l1_ratio_)\n        self.l1_ratios_ = np.asarray(l1_ratios_)\n        # if elasticnet was used, add the l1_ratios dimension to some\n        # attributes\n(B)                 else:\n                    w = np.mean([coefs_paths[:, i, best_indices[i], :]\n                                 for i in range(len(folds))], axis=0)\n\n                best_indices_C = best_indices % len(self.Cs_)\n                self.C_.append(np.mean(self.Cs_[best_indices_C]))\n\n                best_indices_l1 = best_indices // len(self.Cs_)\n                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))\n\n            if multi_class == 'multinomial':\n                self.C_ = np.tile(self.C_, n_classes)\n                self.l1_ratio_ = np.tile(self.l1_ratio_, n_classes)\n                self.coef_ = w[:, :X.shape[1]]\n                if self.fit_intercept:\n                    self.intercept_ = w[:, -1]\n            else:\n                self.coef_[index] = w[: X.shape[1]]\n                if self.fit_intercept:\n                    self.intercept_[index] = w[-1]\n\n(C)                     check_input=False, max_squared_sum=max_squared_sum,\n                    sample_weight=sample_weight,\n                    l1_ratio=l1_ratio_)\n                w = w[0]\n\n            else:\n                # Take the best scores across every fold and the average of\n                # all coefficients corresponding to the best scores.\n                best_indices = np.argmax(scores, axis=1)\n                if self.multi_class == 'ovr':\n                    w = np.mean([coefs_paths[i, best_indices[i], :]\n                                 for i in range(len(folds))], axis=0)\n                else:\n                    w = np.mean([coefs_paths[:, i, best_indices[i], :]\n                                 for i in range(len(folds))], axis=0)\n\n                best_indices_C = best_indices % len(self.Cs_)\n                self.C_.append(np.mean(self.Cs_[best_indices_C]))\n\n                best_indices_l1 = best_indices // len(self.Cs_)\n                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))\n(D)                 # Take the best scores across every fold and the average of\n                # all coefficients corresponding to the best scores.\n                best_indices = np.argmax(scores, axis=1)\n                if self.multi_class == 'ovr':\n                    w = np.mean([coefs_paths[i, best_indices[i], :]\n                                 for i in range(len(folds))], axis=0)\n                else:\n                    w = np.mean([coefs_paths[:, i, best_indices[i], :]\n                                 for i in range(len(folds))], axis=0)\n\n                best_indices_C = best_indices % len(self.Cs_)\n                self.C_.append(np.mean(self.Cs_[best_indices_C]))\n\n                best_indices_l1 = best_indices // len(self.Cs_)\n                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))\n\n            if multi_class == 'multinomial':\n                self.C_ = np.tile(self.C_, n_classes)\n                self.l1_ratio_ = np.tile(self.l1_ratio_, n_classes)\n                self.coef_ = w[:, :X.shape[1]]\n                if self.fit_intercept:", "answer": "D"}
{"uuid": "8bb72923-4237-4f94-bafe-0253ddfcb3cb", "issue_statement": "NCA fails in GridSearch due to too strict parameter checks\nNCA checks its parameters to have a specific type, which can easily fail in a GridSearch due to how param grid is made.\r\n\r\nHere is an example:\r\n```python\r\nimport numpy as np\r\n\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn.neighbors import NeighborhoodComponentsAnalysis\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\n\r\nX = np.random.random_sample((100, 10))\r\ny = np.random.randint(2, size=100)\r\n\r\nnca = NeighborhoodComponentsAnalysis()\r\nknn = KNeighborsClassifier()\r\n\r\npipe = Pipeline([('nca', nca),\r\n                 ('knn', knn)])\r\n                \r\nparams = {'nca__tol': [0.1, 0.5, 1],\r\n          'nca__n_components': np.arange(1, 10)}\r\n          \r\ngs = GridSearchCV(estimator=pipe, param_grid=params, error_score='raise')\r\ngs.fit(X,y)\r\n```\r\n\r\nThe issue is that for `tol`: 1 is not a float, and for  `n_components`: np.int64 is not int\r\n\r\nBefore proposing a fix for this specific situation, I'd like to have your general opinion about parameter checking.  \r\nI like this idea of common parameter checking tool introduced with the NCA PR. What do you think about extending it across the code-base (or at least for new or recent estimators) ?\r\n\r\nCurrently parameter checking is not always done or often partially done, and is quite redundant. For instance, here is the input validation of lda:\r\n```python\r\ndef _check_params(self):\r\n        \"\"\"Check model parameters.\"\"\"\r\n        if self.n_components <= 0:\r\n            raise ValueError(\"Invalid 'n_components' parameter: %r\"\r\n                             % self.n_components)\r\n\r\n        if self.total_samples <= 0:\r\n            raise ValueError(\"Invalid 'total_samples' parameter: %r\"\r\n                             % self.total_samples)\r\n\r\n        if self.learning_offset < 0:\r\n            raise ValueError(\"Invalid 'learning_offset' parameter: %r\"\r\n                             % self.learning_offset)\r\n\r\n        if self.learning_method not in (\"batch\", \"online\"):\r\n            raise ValueError(\"Invalid 'learning_method' parameter: %r\"\r\n                             % self.learning_method)\r\n```\r\nmost params aren't checked and for those who are there's a lot of duplicated code.\r\n\r\nA propose to be upgrade the new tool to be able to check open/closed intervals (currently only closed) and list membership.\r\n\r\nThe api would be something like that:\r\n```\r\ncheck_param(param, name, valid_options)\r\n```\r\nwhere valid_options would be a dict of `type: constraint`. e.g for the `beta_loss` param of `NMF`, it can be either a float or a string in a list, which would give\r\n```\r\nvalid_options = {numbers.Real: None,  # None for no constraint\r\n                 str: ['frobenius', 'kullback-leibler', 'itakura-saito']}\r\n```\r\nSometimes a parameter can only be positive or within a given interval, e.g. `l1_ratio` of `LogisticRegression` must be between 0 and 1, which would give\r\n```\r\nvalid_options = {numbers.Real: Interval(0, 1, closed='both')}\r\n```\r\npositivity of e.g. `max_iter` would be `numbers.Integral: Interval(left=1)`.", "choices": "(A) \n    tol : float, optional (default=1e-5)\n        Convergence tolerance for the optimization.\n\n    callback : callable, optional (default=None)\n        If not None, this function is called after every iteration of the\n        optimizer, taking as arguments the current solution (flattened\n        transformation matrix) and the number of iterations. This might be\n        useful in case one wants to examine or store the transformation\n        found after each iteration.\n\n    verbose : int, optional (default=0)\n        If 0, no progress messages will be printed.\n        If 1, progress messages will be printed to stdout.\n        If > 1, progress messages will be printed and the ``disp``\n        parameter of :func:`scipy.optimize.minimize` will be set to\n        ``verbose - 2``.\n\n    random_state : int or numpy.RandomState or None, optional (default=None)\n        A pseudo random number generator object or a seed for it if int. If\n        ``init='random'``, ``random_state`` is used to initialize the random\n        transformation. If ``init='pca'``, ``random_state`` is passed as an\n        argument to PCA when initializing the transformation.\n\n    Attributes\n    ----------\n    components_ : array, shape (n_components, n_features)\n        The linear transformation learned during fitting.\n\n    n_iter_ : int\n        Counts the number of iterations performed by the optimizer.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors.nca import NeighborhoodComponentsAnalysis\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = load_iris(return_X_y=True)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ... stratify=y, test_size=0.7, random_state=42)\n    >>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n    >>> nca.fit(X_train, y_train)\n    NeighborhoodComponentsAnalysis(...)\n    >>> knn = KNeighborsClassifier(n_neighbors=3)\n    >>> knn.fit(X_train, y_train)\n    KNeighborsClassifier(...)\n    >>> print(knn.score(X_test, y_test))\n    0.933333...\n    >>> knn.fit(nca.transform(X_train), y_train)\n    KNeighborsClassifier(...)\n    >>> print(knn.score(nca.transform(X_test), y_test))\n    0.961904...\n\n    References\n    ----------\n    .. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.\n           \"Neighbourhood Components Analysis\". Advances in Neural Information\n           Processing Systems. 17, 513-520, 2005.\n           http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf\n\n    .. [2] Wikipedia entry on Neighborhood Components Analysis\n           https://en.wikipedia.org/wiki/Neighbourhood_components_analysis\n\n    \"\"\"\n\n    def __init__(self, n_components=None, init='auto', warm_start=False,\n                 max_iter=50, tol=1e-5, callback=None, verbose=0,\n                 random_state=None):\n        self.n_components = n_components\n        self.init = init\n        self.warm_start = warm_start\n        self.max_iter = max_iter\n        self.tol = tol\n        self.callback = callback\n        self.verbose = verbose\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The training samples.\n\n        y : array-like, shape (n_samples,)\n            The corresponding training labels.\n\n        Returns\n        -------\n        self : object\n            returns a trained NeighborhoodComponentsAnalysis model.\n        \"\"\"\n\n        # Verify inputs X and y and NCA parameters, and transform a copy if\n        # needed\n        X, y, init = self._validate_params(X, y)\n\n        # Initialize the random generator\n        self.random_state_ = check_random_state(self.random_state)\n\n        # Measure the total training time\n        t_train = time.time()\n\n        # Compute a mask that stays fixed during optimization:\n        same_class_mask = y[:, np.newaxis] == y[np.newaxis, :]\n        # (n_samples, n_samples)\n\n        # Initialize the transformation\n        transformation = self._initialize(X, y, init)\n\n        # Create a dictionary of parameters to be passed to the optimizer\n        disp = self.verbose - 2 if self.verbose > 1 else -1\n        optimizer_params = {'method': 'L-BFGS-B',\n                            'fun': self._loss_grad_lbfgs,\n                            'args': (X, same_class_mask, -1.0),\n                            'jac': True,\n                            'x0': transformation,\n                            'tol': self.tol,\n                            'options': dict(maxiter=self.max_iter, disp=disp),\n                            'callback': self._callback\n                            }\n\n        # Call the optimizer\n        self.n_iter_ = 0\n        opt_result = minimize(**optimizer_params)\n\n        # Reshape the solution found by the optimizer\n        self.components_ = opt_result.x.reshape(-1, X.shape[1])\n\n        # Stop timer\n        t_train = time.time() - t_train\n        if self.verbose:\n            cls_name = self.__class__.__name__\n\n            # Warn the user if the algorithm did not converge\n            if not opt_result.success:\n                warn('[{}] NCA did not converge: {}'.format(\n                    cls_name, opt_result.message),\n                     ConvergenceWarning)\n\n            print('[{}] Training took {:8.2f}s.'.format(cls_name, t_train))\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Applies the learned transformation to the given data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Data samples.\n\n        Returns\n        -------\n        X_embedded: array, shape (n_samples, n_components)\n            The data samples transformed.\n\n        Raises\n        ------\n        NotFittedError\n            If :meth:`fit` has not been called before.\n        \"\"\"\n\n        check_is_fitted(self, ['components_'])\n        X = check_array(X)\n\n        return np.dot(X, self.components_.T)\n\n    def _validate_params(self, X, y):\n        \"\"\"Validate parameters as soon as :meth:`fit` is called.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The training samples.\n\n        y : array-like, shape (n_samples,)\n            The corresponding training labels.\n\n        Returns\n        -------\n        X : array, shape (n_samples, n_features)\n            The validated training samples.\n\n        y : array, shape (n_samples,)\n            The validated training labels, encoded to be integers in\n            the range(0, n_classes).\n\n        init : string or numpy array of shape (n_features_a, n_features_b)\n            The validated initialization of the linear transformation.\n\n        Raises\n        -------\n        TypeError\n            If a parameter is not an instance of the desired type.\n\n        ValueError\n            If a parameter's value violates its legal value range or if the\n            combination of two or more given parameters is incompatible.\n        \"\"\"\n\n        # Validate the inputs X and y, and converts y to numerical classes.\n        X, y = check_X_y(X, y, ensure_min_samples=2)\n        check_classification_targets(y)\n        y = LabelEncoder().fit_transform(y)\n\n        # Check the preferred dimensionality of the projected space\n        if self.n_components is not None:\n            check_scalar(self.n_components, 'n_components', int, 1)\n\n            if self.n_components > X.shape[1]:\n                raise ValueError('The preferred dimensionality of the '\n                                 'projected space `n_components` ({}) cannot '\n                                 'be greater than the given data '\n                                 'dimensionality ({})!'\n                                 .format(self.n_components, X.shape[1]))\n\n        # If warm_start is enabled, check that the inputs are consistent\n        check_scalar(self.warm_start, 'warm_start', bool)\n        if self.warm_start and hasattr(self, 'components_'):\n            if self.components_.shape[1] != X.shape[1]:\n                raise ValueError('The new inputs dimensionality ({}) does not '\n                                 'match the input dimensionality of the '\n                                 'previously learned transformation ({}).'\n                                 .format(X.shape[1],\n                                         self.components_.shape[1]))\n\n        check_scalar(self.max_iter, 'max_iter', int, 1)\n        check_scalar(self.tol, 'tol', float, 0.)\n        check_scalar(self.verbose, 'verbose', int, 0)\n\n        if self.callback is not None:\n            if not callable(self.callback):\n                raise ValueError('`callback` is not callable.')\n\n        # Check how the linear transformation should be initialized\n        init = self.init\n\n        if isinstance(init, np.ndarray):\n            init = check_array(init)\n\n            # Assert that init.shape[1] = X.shape[1]\n            if init.shape[1] != X.shape[1]:\n                raise ValueError(\n                    'The input dimensionality ({}) of the given '\n                    'linear transformation `init` must match the '\n                    'dimensionality of the given inputs `X` ({}).'\n                    .format(init.shape[1], X.shape[1]))\n\n            # Assert that init.shape[0] <= init.shape[1]\n            if init.shape[0] > init.shape[1]:\n                raise ValueError(\n                    'The output dimensionality ({}) of the given '\n                    'linear transformation `init` cannot be '\n                    'greater than its input dimensionality ({}).'\n                    .format(init.shape[0], init.shape[1]))\n\n            if self.n_components is not None:\n                # Assert that self.n_components = init.shape[0]\n                if self.n_components != init.shape[0]:\n                    raise ValueError('The preferred dimensionality of the '\n                                     'projected space `n_components` ({}) does'\n                                     ' not match the output dimensionality of '\n                                     'the given linear transformation '\n                                     '`init` ({})!'\n                                     .format(self.n_components,\n                                             init.shape[0]))\n        elif init in ['auto', 'pca', 'lda', 'identity', 'random']:\n            pass\n        else:\n            raise ValueError(\n                \"`init` must be 'auto', 'pca', 'lda', 'identity', 'random' \"\n                \"or a numpy array of shape (n_components, n_features).\")\n\n        return X, y, init\n\n    def _initialize(self, X, y, init):\n        \"\"\"Initialize the transformation.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The training samples.\n\n        y : array-like, shape (n_samples,)\n            The training labels.\n\n        init : string or numpy array of shape (n_features_a, n_features_b)\n            The validated initialization of the linear transformation.\n\n        Returns\n        -------\n        transformation : array, shape (n_components, n_features)\n            The initialized linear transformation.\n\n        \"\"\"\n\n        transformation = init\n        if self.warm_start and hasattr(self, 'components_'):\n            transformation = self.components_\n        elif isinstance(init, np.ndarray):\n            pass\n        else:\n            n_samples, n_features = X.shape\n            n_components = self.n_components or n_features\n            if init == 'auto':\n                n_classes = len(np.unique(y))\n                if n_components <= min(n_features, n_classes - 1):\n                    init = 'lda'\n                elif n_components < min(n_features, n_samples):\n                    init = 'pca'\n                else:\n                    init = 'identity'\n            if init == 'identity':\n(B)         \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The training samples.\n\n        y : array-like, shape (n_samples,)\n            The corresponding training labels.\n\n        Returns\n        -------\n        self : object\n            returns a trained NeighborhoodComponentsAnalysis model.\n        \"\"\"\n\n        # Verify inputs X and y and NCA parameters, and transform a copy if\n        # needed\n        X, y, init = self._validate_params(X, y)\n\n        # Initialize the random generator\n        self.random_state_ = check_random_state(self.random_state)\n\n        # Measure the total training time\n        t_train = time.time()\n\n        # Compute a mask that stays fixed during optimization:\n        same_class_mask = y[:, np.newaxis] == y[np.newaxis, :]\n        # (n_samples, n_samples)\n\n        # Initialize the transformation\n        transformation = self._initialize(X, y, init)\n\n        # Create a dictionary of parameters to be passed to the optimizer\n        disp = self.verbose - 2 if self.verbose > 1 else -1\n        optimizer_params = {'method': 'L-BFGS-B',\n                            'fun': self._loss_grad_lbfgs,\n                            'args': (X, same_class_mask, -1.0),\n                            'jac': True,\n                            'x0': transformation,\n                            'tol': self.tol,\n                            'options': dict(maxiter=self.max_iter, disp=disp),\n                            'callback': self._callback\n                            }\n\n        # Call the optimizer\n        self.n_iter_ = 0\n        opt_result = minimize(**optimizer_params)\n\n        # Reshape the solution found by the optimizer\n        self.components_ = opt_result.x.reshape(-1, X.shape[1])\n\n        # Stop timer\n        t_train = time.time() - t_train\n        if self.verbose:\n            cls_name = self.__class__.__name__\n\n            # Warn the user if the algorithm did not converge\n            if not opt_result.success:\n                warn('[{}] NCA did not converge: {}'.format(\n                    cls_name, opt_result.message),\n                     ConvergenceWarning)\n\n            print('[{}] Training took {:8.2f}s.'.format(cls_name, t_train))\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Applies the learned transformation to the given data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Data samples.\n\n        Returns\n        -------\n        X_embedded: array, shape (n_samples, n_components)\n            The data samples transformed.\n\n        Raises\n        ------\n        NotFittedError\n            If :meth:`fit` has not been called before.\n        \"\"\"\n\n        check_is_fitted(self, ['components_'])\n        X = check_array(X)\n\n        return np.dot(X, self.components_.T)\n\n    def _validate_params(self, X, y):\n        \"\"\"Validate parameters as soon as :meth:`fit` is called.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The training samples.\n\n        y : array-like, shape (n_samples,)\n            The corresponding training labels.\n\n        Returns\n        -------\n        X : array, shape (n_samples, n_features)\n            The validated training samples.\n\n        y : array, shape (n_samples,)\n            The validated training labels, encoded to be integers in\n            the range(0, n_classes).\n\n        init : string or numpy array of shape (n_features_a, n_features_b)\n            The validated initialization of the linear transformation.\n\n        Raises\n        -------\n        TypeError\n            If a parameter is not an instance of the desired type.\n\n        ValueError\n            If a parameter's value violates its legal value range or if the\n            combination of two or more given parameters is incompatible.\n        \"\"\"\n\n        # Validate the inputs X and y, and converts y to numerical classes.\n        X, y = check_X_y(X, y, ensure_min_samples=2)\n        check_classification_targets(y)\n        y = LabelEncoder().fit_transform(y)\n\n        # Check the preferred dimensionality of the projected space\n        if self.n_components is not None:\n            check_scalar(self.n_components, 'n_components', int, 1)\n\n            if self.n_components > X.shape[1]:\n                raise ValueError('The preferred dimensionality of the '\n                                 'projected space `n_components` ({}) cannot '\n                                 'be greater than the given data '\n                                 'dimensionality ({})!'\n                                 .format(self.n_components, X.shape[1]))\n\n        # If warm_start is enabled, check that the inputs are consistent\n        check_scalar(self.warm_start, 'warm_start', bool)\n        if self.warm_start and hasattr(self, 'components_'):\n            if self.components_.shape[1] != X.shape[1]:\n                raise ValueError('The new inputs dimensionality ({}) does not '\n                                 'match the input dimensionality of the '\n                                 'previously learned transformation ({}).'\n                                 .format(X.shape[1],\n                                         self.components_.shape[1]))\n\n        check_scalar(self.max_iter, 'max_iter', int, 1)\n        check_scalar(self.tol, 'tol', float, 0.)\n        check_scalar(self.verbose, 'verbose', int, 0)\n\n        if self.callback is not None:\n            if not callable(self.callback):\n                raise ValueError('`callback` is not callable.')\n\n        # Check how the linear transformation should be initialized\n        init = self.init\n\n        if isinstance(init, np.ndarray):\n            init = check_array(init)\n\n            # Assert that init.shape[1] = X.shape[1]\n            if init.shape[1] != X.shape[1]:\n                raise ValueError(\n                    'The input dimensionality ({}) of the given '\n                    'linear transformation `init` must match the '\n                    'dimensionality of the given inputs `X` ({}).'\n                    .format(init.shape[1], X.shape[1]))\n\n            # Assert that init.shape[0] <= init.shape[1]\n            if init.shape[0] > init.shape[1]:\n                raise ValueError(\n                    'The output dimensionality ({}) of the given '\n                    'linear transformation `init` cannot be '\n                    'greater than its input dimensionality ({}).'\n                    .format(init.shape[0], init.shape[1]))\n\n            if self.n_components is not None:\n                # Assert that self.n_components = init.shape[0]\n                if self.n_components != init.shape[0]:\n                    raise ValueError('The preferred dimensionality of the '\n                                     'projected space `n_components` ({}) does'\n                                     ' not match the output dimensionality of '\n                                     'the given linear transformation '\n                                     '`init` ({})!'\n                                     .format(self.n_components,\n                                             init.shape[0]))\n        elif init in ['auto', 'pca', 'lda', 'identity', 'random']:\n            pass\n        else:\n            raise ValueError(\n                \"`init` must be 'auto', 'pca', 'lda', 'identity', 'random' \"\n                \"or a numpy array of shape (n_components, n_features).\")\n\n        return X, y, init\n\n    def _initialize(self, X, y, init):\n        \"\"\"Initialize the transformation.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The training samples.\n\n        y : array-like, shape (n_samples,)\n            The training labels.\n\n        init : string or numpy array of shape (n_features_a, n_features_b)\n            The validated initialization of the linear transformation.\n\n        Returns\n        -------\n        transformation : array, shape (n_components, n_features)\n            The initialized linear transformation.\n\n        \"\"\"\n\n        transformation = init\n        if self.warm_start and hasattr(self, 'components_'):\n            transformation = self.components_\n        elif isinstance(init, np.ndarray):\n            pass\n        else:\n            n_samples, n_features = X.shape\n            n_components = self.n_components or n_features\n            if init == 'auto':\n                n_classes = len(np.unique(y))\n                if n_components <= min(n_features, n_classes - 1):\n                    init = 'lda'\n                elif n_components < min(n_features, n_samples):\n                    init = 'pca'\n                else:\n                    init = 'identity'\n            if init == 'identity':\n                transformation = np.eye(n_components, X.shape[1])\n            elif init == 'random':\n                transformation = self.random_state_.randn(n_components,\n                                                          X.shape[1])\n            elif init in {'pca', 'lda'}:\n                init_time = time.time()\n                if init == 'pca':\n                    pca = PCA(n_components=n_components,\n                              random_state=self.random_state_)\n                    if self.verbose:\n                        print('Finding principal components... ', end='')\n                        sys.stdout.flush()\n                    pca.fit(X)\n                    transformation = pca.components_\n                elif init == 'lda':\n                    from ..discriminant_analysis import (\n                        LinearDiscriminantAnalysis)\n                    lda = LinearDiscriminantAnalysis(n_components=n_components)\n                    if self.verbose:\n                        print('Finding most discriminative components... ',\n                              end='')\n                        sys.stdout.flush()\n                    lda.fit(X, y)\n                    transformation = lda.scalings_.T[:n_components]\n                if self.verbose:\n                    print('done in {:5.2f}s'.format(time.time() - init_time))\n        return transformation\n\n    def _callback(self, transformation):\n        \"\"\"Called after each iteration of the optimizer.\n\n        Parameters\n        ----------\n        transformation : array, shape=(n_components * n_features,)\n            The solution computed by the optimizer in this iteration.\n        \"\"\"\n        if self.callback is not None:\n            self.callback(transformation, self.n_iter_)\n\n        self.n_iter_ += 1\n\n    def _loss_grad_lbfgs(self, transformation, X, same_class_mask, sign=1.0):\n        \"\"\"Compute the loss and the loss gradient w.r.t. ``transformation``.\n\n        Parameters\n        ----------\n        transformation : array, shape (n_components * n_features,)\n            The raveled linear transformation on which to compute loss and\n            evaluate gradient.\n\n        X : array, shape (n_samples, n_features)\n            The training samples.\n\n        same_class_mask : array, shape (n_samples, n_samples)\n            A mask where ``mask[i, j] == 1`` if ``X[i]`` and ``X[j]`` belong\n            to the same class, and ``0`` otherwise.\n\n        Returns\n        -------\n        loss : float\n            The loss computed for the given transformation.\n\n        gradient : array, shape (n_components * n_features,)\n            The new (flattened) gradient of the loss.\n        \"\"\"\n\n        if self.n_iter_ == 0:\n            self.n_iter_ += 1\n            if self.verbose:\n                header_fields = ['Iteration', 'Objective Value', 'Time(s)']\n                header_fmt = '{:>10} {:>20} {:>10}'\n                header = header_fmt.format(*header_fields)\n                cls_name = self.__class__.__name__\n                print('[{}]'.format(cls_name))\n                print('[{}] {}\\n[{}] {}'.format(cls_name, header,\n                                                cls_name, '-' * len(header)))\n\n        t_funcall = time.time()\n\n(C) import numpy as np\nimport sys\nimport time\nfrom scipy.optimize import minimize\nfrom ..utils.extmath import softmax\nfrom ..metrics import pairwise_distances\nfrom ..base import BaseEstimator, TransformerMixin\nfrom ..preprocessing import LabelEncoder\nfrom ..decomposition import PCA\nfrom ..utils.multiclass import check_classification_targets\nfrom ..utils.random import check_random_state\nfrom ..utils.validation import (check_is_fitted, check_array, check_X_y,\n                                check_scalar)\nfrom ..exceptions import ConvergenceWarning\n\n\nclass NeighborhoodComponentsAnalysis(BaseEstimator, TransformerMixin):\n    \"\"\"Neighborhood Components Analysis\n\n    Neighborhood Component Analysis (NCA) is a machine learning algorithm for\n    metric learning. It learns a linear transformation in a supervised fashion\n    to improve the classification accuracy of a stochastic nearest neighbors\n    rule in the transformed space.\n\n    Read more in the :ref:`User Guide <nca>`.\n\n    Parameters\n    ----------\n    n_components : int, optional (default=None)\n        Preferred dimensionality of the projected space.\n        If None it will be set to ``n_features``.\n\n    init : string or numpy array, optional (default='auto')\n        Initialization of the linear transformation. Possible options are\n        'auto', 'pca', 'lda', 'identity', 'random', and a numpy array of shape\n        (n_features_a, n_features_b).\n\n        'auto'\n            Depending on ``n_components``, the most reasonable initialization\n            will be chosen. If ``n_components <= n_classes`` we use 'lda', as\n            it uses labels information. If not, but\n            ``n_components < min(n_features, n_samples)``, we use 'pca', as\n            it projects data in meaningful directions (those of higher\n            variance). Otherwise, we just use 'identity'.\n\n        'pca'\n            ``n_components`` principal components of the inputs passed\n            to :meth:`fit` will be used to initialize the transformation.\n            (See `decomposition.PCA`)\n\n        'lda'\n            ``min(n_components, n_classes)`` most discriminative\n            components of the inputs passed to :meth:`fit` will be used to\n            initialize the transformation. (If ``n_components > n_classes``,\n            the rest of the components will be zero.) (See\n            `discriminant_analysis.LinearDiscriminantAnalysis`)\n\n        'identity'\n            If ``n_components`` is strictly smaller than the\n            dimensionality of the inputs passed to :meth:`fit`, the identity\n            matrix will be truncated to the first ``n_components`` rows.\n\n        'random'\n            The initial transformation will be a random array of shape\n            `(n_components, n_features)`. Each value is sampled from the\n            standard normal distribution.\n\n        numpy array\n            n_features_b must match the dimensionality of the inputs passed to\n            :meth:`fit` and n_features_a must be less than or equal to that.\n            If ``n_components`` is not None, n_features_a must match it.\n\n    warm_start : bool, optional, (default=False)\n        If True and :meth:`fit` has been called before, the solution of the\n        previous call to :meth:`fit` is used as the initial linear\n        transformation (``n_components`` and ``init`` will be ignored).\n\n    max_iter : int, optional (default=50)\n        Maximum number of iterations in the optimization.\n\n    tol : float, optional (default=1e-5)\n        Convergence tolerance for the optimization.\n\n    callback : callable, optional (default=None)\n        If not None, this function is called after every iteration of the\n        optimizer, taking as arguments the current solution (flattened\n        transformation matrix) and the number of iterations. This might be\n        useful in case one wants to examine or store the transformation\n        found after each iteration.\n\n    verbose : int, optional (default=0)\n        If 0, no progress messages will be printed.\n        If 1, progress messages will be printed to stdout.\n        If > 1, progress messages will be printed and the ``disp``\n        parameter of :func:`scipy.optimize.minimize` will be set to\n        ``verbose - 2``.\n\n    random_state : int or numpy.RandomState or None, optional (default=None)\n        A pseudo random number generator object or a seed for it if int. If\n        ``init='random'``, ``random_state`` is used to initialize the random\n        transformation. If ``init='pca'``, ``random_state`` is passed as an\n        argument to PCA when initializing the transformation.\n\n    Attributes\n    ----------\n    components_ : array, shape (n_components, n_features)\n        The linear transformation learned during fitting.\n\n    n_iter_ : int\n        Counts the number of iterations performed by the optimizer.\n\n    Examples\n    --------\n    >>> from sklearn.neighbors.nca import NeighborhoodComponentsAnalysis\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = load_iris(return_X_y=True)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ... stratify=y, test_size=0.7, random_state=42)\n    >>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n    >>> nca.fit(X_train, y_train)\n    NeighborhoodComponentsAnalysis(...)\n    >>> knn = KNeighborsClassifier(n_neighbors=3)\n    >>> knn.fit(X_train, y_train)\n    KNeighborsClassifier(...)\n    >>> print(knn.score(X_test, y_test))\n    0.933333...\n    >>> knn.fit(nca.transform(X_train), y_train)\n    KNeighborsClassifier(...)\n    >>> print(knn.score(nca.transform(X_test), y_test))\n    0.961904...\n\n    References\n    ----------\n    .. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.\n           \"Neighbourhood Components Analysis\". Advances in Neural Information\n           Processing Systems. 17, 513-520, 2005.\n           http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf\n\n    .. [2] Wikipedia entry on Neighborhood Components Analysis\n           https://en.wikipedia.org/wiki/Neighbourhood_components_analysis\n\n    \"\"\"\n\n    def __init__(self, n_components=None, init='auto', warm_start=False,\n                 max_iter=50, tol=1e-5, callback=None, verbose=0,\n                 random_state=None):\n        self.n_components = n_components\n        self.init = init\n        self.warm_start = warm_start\n        self.max_iter = max_iter\n        self.tol = tol\n        self.callback = callback\n        self.verbose = verbose\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The training samples.\n\n        y : array-like, shape (n_samples,)\n            The corresponding training labels.\n\n        Returns\n        -------\n        self : object\n            returns a trained NeighborhoodComponentsAnalysis model.\n        \"\"\"\n\n        # Verify inputs X and y and NCA parameters, and transform a copy if\n        # needed\n        X, y, init = self._validate_params(X, y)\n\n        # Initialize the random generator\n        self.random_state_ = check_random_state(self.random_state)\n\n        # Measure the total training time\n        t_train = time.time()\n\n        # Compute a mask that stays fixed during optimization:\n        same_class_mask = y[:, np.newaxis] == y[np.newaxis, :]\n        # (n_samples, n_samples)\n\n        # Initialize the transformation\n        transformation = self._initialize(X, y, init)\n\n        # Create a dictionary of parameters to be passed to the optimizer\n        disp = self.verbose - 2 if self.verbose > 1 else -1\n        optimizer_params = {'method': 'L-BFGS-B',\n                            'fun': self._loss_grad_lbfgs,\n                            'args': (X, same_class_mask, -1.0),\n                            'jac': True,\n                            'x0': transformation,\n                            'tol': self.tol,\n                            'options': dict(maxiter=self.max_iter, disp=disp),\n                            'callback': self._callback\n                            }\n\n        # Call the optimizer\n        self.n_iter_ = 0\n        opt_result = minimize(**optimizer_params)\n\n        # Reshape the solution found by the optimizer\n        self.components_ = opt_result.x.reshape(-1, X.shape[1])\n\n        # Stop timer\n        t_train = time.time() - t_train\n        if self.verbose:\n            cls_name = self.__class__.__name__\n\n            # Warn the user if the algorithm did not converge\n            if not opt_result.success:\n                warn('[{}] NCA did not converge: {}'.format(\n                    cls_name, opt_result.message),\n                     ConvergenceWarning)\n\n            print('[{}] Training took {:8.2f}s.'.format(cls_name, t_train))\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Applies the learned transformation to the given data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Data samples.\n\n        Returns\n        -------\n        X_embedded: array, shape (n_samples, n_components)\n            The data samples transformed.\n\n        Raises\n        ------\n        NotFittedError\n            If :meth:`fit` has not been called before.\n        \"\"\"\n\n        check_is_fitted(self, ['components_'])\n        X = check_array(X)\n\n        return np.dot(X, self.components_.T)\n\n    def _validate_params(self, X, y):\n        \"\"\"Validate parameters as soon as :meth:`fit` is called.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The training samples.\n\n        y : array-like, shape (n_samples,)\n            The corresponding training labels.\n\n        Returns\n        -------\n        X : array, shape (n_samples, n_features)\n            The validated training samples.\n\n        y : array, shape (n_samples,)\n            The validated training labels, encoded to be integers in\n            the range(0, n_classes).\n\n        init : string or numpy array of shape (n_features_a, n_features_b)\n            The validated initialization of the linear transformation.\n\n        Raises\n        -------\n        TypeError\n            If a parameter is not an instance of the desired type.\n\n        ValueError\n            If a parameter's value violates its legal value range or if the\n            combination of two or more given parameters is incompatible.\n        \"\"\"\n\n        # Validate the inputs X and y, and converts y to numerical classes.\n        X, y = check_X_y(X, y, ensure_min_samples=2)\n        check_classification_targets(y)\n        y = LabelEncoder().fit_transform(y)\n\n        # Check the preferred dimensionality of the projected space\n        if self.n_components is not None:\n            check_scalar(self.n_components, 'n_components', int, 1)\n\n            if self.n_components > X.shape[1]:\n                raise ValueError('The preferred dimensionality of the '\n                                 'projected space `n_components` ({}) cannot '\n                                 'be greater than the given data '\n                                 'dimensionality ({})!'\n                                 .format(self.n_components, X.shape[1]))\n\n        # If warm_start is enabled, check that the inputs are consistent\n        check_scalar(self.warm_start, 'warm_start', bool)\n        if self.warm_start and hasattr(self, 'components_'):\n            if self.components_.shape[1] != X.shape[1]:\n                raise ValueError('The new inputs dimensionality ({}) does not '\n                                 'match the input dimensionality of the '\n                                 'previously learned transformation ({}).'\n                                 .format(X.shape[1],\n                                         self.components_.shape[1]))\n\n        check_scalar(self.max_iter, 'max_iter', int, 1)\n        check_scalar(self.tol, 'tol', float, 0.)\n        check_scalar(self.verbose, 'verbose', int, 0)\n\n        if self.callback is not None:\n            if not callable(self.callback):\n                raise ValueError('`callback` is not callable.')\n", "answer": "D"}
{"uuid": "57f8be80-cc1b-4a3d-9df8-2067cb85f9e6", "issue_statement": "ZeroDivisionError in _sparse_fit for SVM with empty support_vectors_\n#### Description\r\nWhen using sparse data, in the case where the support_vectors_ attribute is be empty, _fit_sparse gives a ZeroDivisionError\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\nimport numpy as np\r\nimport scipy\r\nimport sklearn\r\nfrom sklearn.svm import SVR\r\nx_train = np.array([[0, 1, 0, 0],\r\n[0, 0, 0, 1],\r\n[0, 0, 1, 0],\r\n[0, 0, 0, 1]])\r\ny_train = np.array([0.04, 0.04, 0.10, 0.16])\r\nmodel = SVR(C=316.227766017, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\r\n  \t    gamma=1.0, kernel='linear', max_iter=15000,\r\n  \t    shrinking=True, tol=0.001, verbose=False)\r\n# dense x_train has no error\r\nmodel.fit(x_train, y_train)\r\n\r\n# convert to sparse\r\nxtrain= scipy.sparse.csr_matrix(x_train)\r\nmodel.fit(xtrain, y_train)\r\n\r\n```\r\n#### Expected Results\r\nNo error is thrown and  `self.dual_coef_ = sp.csr_matrix([])`\r\n\r\n#### Actual Results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py\", line 209, in fit\r\n    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py\", line 302, in _sparse_fit\r\n    dual_coef_indices.size / n_class)\r\nZeroDivisionError: float division by zero\r\n```\r\n\r\n#### Versions\r\n```\r\n>>> sklearn.show_versions() \r\n\r\nSystem:\r\nexecutable: /usr/bin/python3\r\n    python: 3.5.2 (default, Nov 12 2018, 13:43:14)  [GCC 5.4.0 20160609]\r\n   machine: Linux-4.15.0-58-generic-x86_64-with-Ubuntu-16.04-xenial\r\n\r\nPython deps:\r\n     numpy: 1.17.0\r\n    Cython: None\r\n       pip: 19.2.1\r\n    pandas: 0.22.0\r\n   sklearn: 0.21.3\r\n     scipy: 1.3.0\r\nsetuptools: 40.4.3\r\n```", "choices": "(A)             (n_class, n_SV))\n\n    def predict(self, X):\n        \"\"\"Perform regression on samples in X.\n\n        For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples_test, n_samples_train).\n\n        Returns\n(B)         if hasattr(self, \"classes_\"):\n            n_class = len(self.classes_) - 1\n        else:  # regression\n            n_class = 1\n        n_SV = self.support_vectors_.shape[0]\n\n        dual_coef_indices = np.tile(np.arange(n_SV), n_class)\n        dual_coef_indptr = np.arange(0, dual_coef_indices.size + 1,\n                                     dual_coef_indices.size / n_class)\n        self.dual_coef_ = sp.csr_matrix(\n            (dual_coef_data, dual_coef_indices, dual_coef_indptr),\n            (n_class, n_SV))\n\n    def predict(self, X):\n(C)                                      dual_coef_indices.size / n_class)\n        self.dual_coef_ = sp.csr_matrix(\n            (dual_coef_data, dual_coef_indices, dual_coef_indptr),\n            (n_class, n_SV))\n\n    def predict(self, X):\n        \"\"\"Perform regression on samples in X.\n\n        For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            For kernel=\"precomputed\", the expected shape of X is\n(D)         n_SV = self.support_vectors_.shape[0]\n\n        dual_coef_indices = np.tile(np.arange(n_SV), n_class)\n        dual_coef_indptr = np.arange(0, dual_coef_indices.size + 1,\n                                     dual_coef_indices.size / n_class)\n        self.dual_coef_ = sp.csr_matrix(\n            (dual_coef_data, dual_coef_indices, dual_coef_indptr),\n            (n_class, n_SV))\n\n    def predict(self, X):\n        \"\"\"Perform regression on samples in X.\n\n        For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n", "answer": "D"}
{"uuid": "a55782d1-5cee-45e8-b317-fea602133eb8", "issue_statement": "RepeatedKFold and RepeatedStratifiedKFold do not show correct __repr__ string\n#### Description\r\n\r\n`RepeatedKFold` and `RepeatedStratifiedKFold` do not show correct \\_\\_repr\\_\\_ string.\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```python\r\n>>> from sklearn.model_selection import RepeatedKFold, RepeatedStratifiedKFold\r\n>>> repr(RepeatedKFold())\r\n>>> repr(RepeatedStratifiedKFold())\r\n```\r\n\r\n#### Expected Results\r\n\r\n```python\r\n>>> repr(RepeatedKFold())\r\nRepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\r\n>>> repr(RepeatedStratifiedKFold())\r\nRepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=None)\r\n```\r\n\r\n#### Actual Results\r\n\r\n```python\r\n>>> repr(RepeatedKFold())\r\n'<sklearn.model_selection._split.RepeatedKFold object at 0x0000016421AA4288>'\r\n>>> repr(RepeatedStratifiedKFold())\r\n'<sklearn.model_selection._split.RepeatedStratifiedKFold object at 0x0000016420E115C8>'\r\n```\r\n\r\n#### Versions\r\n```\r\nSystem:\r\n    python: 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\r\nexecutable: D:\\anaconda3\\envs\\xyz\\python.exe\r\n   machine: Windows-10-10.0.16299-SP0\r\n\r\nBLAS:\r\n    macros:\r\n  lib_dirs:\r\ncblas_libs: cblas\r\n\r\nPython deps:\r\n       pip: 19.2.2\r\nsetuptools: 41.0.1\r\n   sklearn: 0.21.2\r\n     numpy: 1.16.4\r\n     scipy: 1.3.1\r\n    Cython: None\r\n    pandas: 0.24.2\r\n```", "choices": "(A)     --------\n    StratifiedKFold\n        Takes group information into account to avoid building folds with\n        imbalanced class distributions (for binary or multiclass\n        classification tasks).\n\n    GroupKFold: K-fold iterator variant with non-overlapping groups.\n\n    RepeatedKFold: Repeats K-Fold n times.\n    \"\"\"\n\n    def __init__(self, n_splits=5, shuffle=False,\n                 random_state=None):\n        super().__init__(n_splits, shuffle, random_state)\n\n    def _iter_test_indices(self, X, y=None, groups=None):\n        n_samples = _num_samples(X)\n        indices = np.arange(n_samples)\n        if self.shuffle:\n            check_random_state(self.random_state).shuffle(indices)\n\n        n_splits = self.n_splits\n        fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n        fold_sizes[:n_samples % n_splits] += 1\n        current = 0\n        for fold_size in fold_sizes:\n            start, stop = current, current + fold_size\n            yield indices[start:stop]\n            current = stop\n\n\nclass GroupKFold(_BaseKFold):\n    \"\"\"K-fold iterator variant with non-overlapping groups.\n\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n\n    The folds are approximately balanced in the sense that the number of\n    distinct groups is approximately the same in each fold.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n        .. versionchanged:: 0.22\n            ``n_splits`` default value changed from 3 to 5.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import GroupKFold\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    >>> y = np.array([1, 2, 3, 4])\n    >>> groups = np.array([0, 0, 2, 2])\n    >>> group_kfold = GroupKFold(n_splits=2)\n    >>> group_kfold.get_n_splits(X, y, groups)\n    2\n    >>> print(group_kfold)\n    GroupKFold(n_splits=2)\n    >>> for train_index, test_index in group_kfold.split(X, y, groups):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...     print(X_train, X_test, y_train, y_test)\n    ...\n    TRAIN: [0 1] TEST: [2 3]\n    [[1 2]\n     [3 4]] [[5 6]\n     [7 8]] [1 2] [3 4]\n    TRAIN: [2 3] TEST: [0 1]\n    [[5 6]\n     [7 8]] [[1 2]\n     [3 4]] [3 4] [1 2]\n\n    See also\n    --------\n    LeaveOneGroupOut\n        For splitting the data according to explicit domain-specific\n        stratification of the dataset.\n    \"\"\"\n    def __init__(self, n_splits=5):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n\n    def _iter_test_indices(self, X, y, groups):\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, ensure_2d=False, dtype=None)\n\n        unique_groups, groups = np.unique(groups, return_inverse=True)\n        n_groups = len(unique_groups)\n\n        if self.n_splits > n_groups:\n            raise ValueError(\"Cannot have number of splits n_splits=%d greater\"\n                             \" than the number of groups: %d.\"\n                             % (self.n_splits, n_groups))\n\n        # Weight groups by their number of occurrences\n        n_samples_per_group = np.bincount(groups)\n\n        # Distribute the most frequent groups first\n        indices = np.argsort(n_samples_per_group)[::-1]\n        n_samples_per_group = n_samples_per_group[indices]\n\n        # Total weight of each fold\n        n_samples_per_fold = np.zeros(self.n_splits)\n\n        # Mapping from group index to fold index\n        group_to_fold = np.zeros(len(unique_groups))\n\n        # Distribute samples by adding the largest weight to the lightest fold\n        for group_index, weight in enumerate(n_samples_per_group):\n            lightest_fold = np.argmin(n_samples_per_fold)\n            n_samples_per_fold[lightest_fold] += weight\n            group_to_fold[indices[group_index]] = lightest_fold\n\n        indices = group_to_fold[groups]\n\n        for f in range(self.n_splits):\n            yield np.where(indices == f)[0]\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,), optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass StratifiedKFold(_BaseKFold):\n    \"\"\"Stratified K-Folds cross-validator\n\n    Provides train/test indices to split data in train/test sets.\n\n    This cross-validation object is a variation of KFold that returns\n    stratified folds. The folds are made by preserving the percentage of\n    samples for each class.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n        .. versionchanged:: 0.22\n            ``n_splits`` default value changed from 3 to 5.\n\n    shuffle : boolean, optional\n        Whether to shuffle each class's samples before splitting into batches.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``shuffle`` == True.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import StratifiedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> skf = StratifiedKFold(n_splits=2)\n    >>> skf.get_n_splits(X, y)\n    2\n    >>> print(skf)\n    StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n    >>> for train_index, test_index in skf.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [1 3] TEST: [0 2]\n    TRAIN: [0 2] TEST: [1 3]\n\n    Notes\n    -----\n    The implementation is designed to:\n\n    * Generate test sets such that all contain the same distribution of\n      classes, or as close as possible.\n    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n      ``y = [1, 0]`` should not change the indices generated.\n    * Preserve order dependencies in the dataset ordering, when\n      ``shuffle=False``: all samples from class k in some test set were\n      contiguous in y, or separated in y by samples from classes other than k.\n    * Generate test sets where the smallest and largest differ by at most one\n      sample.\n\n    .. versionchanged:: 0.22\n        The previous implementation did not follow the last constraint.\n\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n    \"\"\"\n\n    def __init__(self, n_splits=5, shuffle=False, random_state=None):\n        super().__init__(n_splits, shuffle, random_state)\n\n    def _make_test_folds(self, X, y=None):\n        rng = check_random_state(self.random_state)\n        y = np.asarray(y)\n        type_of_target_y = type_of_target(y)\n        allowed_target_types = ('binary', 'multiclass')\n        if type_of_target_y not in allowed_target_types:\n            raise ValueError(\n                'Supported target types are: {}. Got {!r} instead.'.format(\n                    allowed_target_types, type_of_target_y))\n\n        y = column_or_1d(y)\n\n        _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)\n        # y_inv encodes y according to lexicographic order. We invert y_idx to\n        # map the classes so that they are encoded by order of appearance:\n        # 0 represents the first label appearing in y, 1 the second, etc.\n        _, class_perm = np.unique(y_idx, return_inverse=True)\n        y_encoded = class_perm[y_inv]\n\n        n_classes = len(y_idx)\n        y_counts = np.bincount(y_encoded)\n        min_groups = np.min(y_counts)\n        if np.all(self.n_splits > y_counts):\n            raise ValueError(\"n_splits=%d cannot be greater than the\"\n                             \" number of members in each class.\"\n                             % (self.n_splits))\n        if self.n_splits > min_groups:\n            warnings.warn((\"The least populated class in y has only %d\"\n                           \" members, which is less than n_splits=%d.\"\n                           % (min_groups, self.n_splits)), UserWarning)\n\n        # Determine the optimal number of samples from each class in each fold,\n        # using round robin over the sorted y. (This can be done direct from\n        # counts, but that code is unreadable.)\n        y_order = np.sort(y_encoded)\n        allocation = np.asarray(\n            [np.bincount(y_order[i::self.n_splits], minlength=n_classes)\n             for i in range(self.n_splits)])\n\n        # To maintain the data order dependencies as best as possible within\n        # the stratification constraint, we assign samples from each class in\n        # blocks (and then mess that up when shuffle=True).\n        test_folds = np.empty(len(y), dtype='i')\n        for k in range(n_classes):\n            # since the kth column of allocation stores the number of samples\n            # of class k in each test set, this generates blocks of fold\n            # indices corresponding to the allocation for class k.\n            folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])\n            if self.shuffle:\n                rng.shuffle(folds_for_class)\n            test_folds[y_encoded == k] = folds_for_class\n        return test_folds\n\n    def _iter_test_masks(self, X, y=None, groups=None):\n        test_folds = self._make_test_folds(X, y)\n        for i in range(self.n_splits):\n            yield test_folds == i\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n            Stratification is done based on the y labels.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super().split(X, y, groups)\n\n\nclass TimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator\n\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals, in train/test sets.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n\n        .. versionchanged:: 0.22\n            ``n_splits`` default value changed from 3 to 5.\n\n    max_train_size : int, optional\n        Maximum size for a single training set.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import TimeSeriesSplit\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\n    >>> tscv = TimeSeriesSplit()\n    >>> print(tscv)\n    TimeSeriesSplit(max_train_size=None, n_splits=5)\n    >>> for train_index, test_index in tscv.split(X):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [0] TEST: [1]\n    TRAIN: [0 1] TEST: [2]\n    TRAIN: [0 1 2] TEST: [3]\n    TRAIN: [0 1 2 3] TEST: [4]\n    TRAIN: [0 1 2 3 4] TEST: [5]\n\n    Notes\n    -----\n    The training set has size ``i * n_samples // (n_splits + 1)\n    + n_samples % (n_splits + 1)`` in the ``i``th split,\n    with a test set of size ``n_samples//(n_splits + 1)``,\n    where ``n_samples`` is the number of samples.\n    \"\"\"\n    def __init__(self, n_splits=5, max_train_size=None):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_size = max_train_size\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,)\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        n_folds = n_splits + 1\n        if n_folds > n_samples:\n            raise ValueError(\n                (\"Cannot have number of folds ={0} greater\"\n                 \" than the number of samples: {1}.\").format(n_folds,\n                                                             n_samples))\n        indices = np.arange(n_samples)\n        test_size = (n_samples // n_folds)\n        test_starts = range(test_size + n_samples % n_folds,\n                            n_samples, test_size)\n        for test_start in test_starts:\n            if self.max_train_size and self.max_train_size < test_start:\n                yield (indices[test_start - self.max_train_size:test_start],\n                       indices[test_start:test_start + test_size])\n            else:\n                yield (indices[:test_start],\n                       indices[test_start:test_start + test_size])\n\n\nclass LeaveOneGroupOut(BaseCrossValidator):\n    \"\"\"Leave One Group Out cross-validator\n\n    Provides train/test indices to split data according to a third-party\n    provided group. This group information can be used to encode arbitrary\n    domain specific stratifications of the samples as integers.\n\n    For instance the groups could be the year of collection of the samples\n    and thus allow for cross-validation against time-based splits.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import LeaveOneGroupOut\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    >>> y = np.array([1, 2, 1, 2])\n    >>> groups = np.array([1, 1, 2, 2])\n    >>> logo = LeaveOneGroupOut()\n    >>> logo.get_n_splits(X, y, groups)\n    2\n    >>> logo.get_n_splits(groups=groups)  # 'groups' is always required\n    2\n    >>> print(logo)\n    LeaveOneGroupOut()\n    >>> for train_index, test_index in logo.split(X, y, groups):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    ...    print(X_train, X_test, y_train, y_test)\n    TRAIN: [2 3] TEST: [0 1]\n    [[5 6]\n     [7 8]] [[1 2]\n     [3 4]] [1 2] [1 2]\n    TRAIN: [0 1] TEST: [2 3]\n    [[1 2]\n     [3 4]] [[5 6]\n     [7 8]] [1 2] [1 2]\n\n    \"\"\"\n\n    def _iter_test_masks(self, X, y, groups):\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        # We make a copy of groups to avoid side-effects during iteration\n        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)\n        unique_groups = np.unique(groups)\n        if len(unique_groups) <= 1:\n            raise ValueError(\n                \"The groups parameter contains fewer than 2 unique groups \"\n                \"(%s). LeaveOneGroupOut expects at least 2.\" % unique_groups)\n        for i in unique_groups:\n            yield groups == i\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set. This 'groups' parameter must always be specified to\n            calculate the number of splits, though the other parameters can be\n            omitted.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, ensure_2d=False, dtype=None)\n        return len(np.unique(groups))\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples, optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass LeavePGroupsOut(BaseCrossValidator):\n    \"\"\"Leave P Group(s) Out cross-validator\n\n    Provides train/test indices to split data according to a third-party\n    provided group. This group information can be used to encode arbitrary\n    domain specific stratifications of the samples as integers.\n\n    For instance the groups could be the year of collection of the samples\n    and thus allow for cross-validation against time-based splits.\n\n    The difference between LeavePGroupsOut and LeaveOneGroupOut is that\n    the former builds the test sets with all the samples assigned to\n    ``p`` different values of the groups while the latter uses samples\n    all assigned the same groups.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_groups : int\n        Number of groups (``p``) to leave out in the test split.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import LeavePGroupsOut\n    >>> X = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> y = np.array([1, 2, 1])\n    >>> groups = np.array([1, 2, 3])\n    >>> lpgo = LeavePGroupsOut(n_groups=2)\n    >>> lpgo.get_n_splits(X, y, groups)\n    3\n    >>> lpgo.get_n_splits(groups=groups)  # 'groups' is always required\n    3\n    >>> print(lpgo)\n    LeavePGroupsOut(n_groups=2)\n    >>> for train_index, test_index in lpgo.split(X, y, groups):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    ...    print(X_train, X_test, y_train, y_test)\n    TRAIN: [2] TEST: [0 1]\n    [[5 6]] [[1 2]\n     [3 4]] [1] [1 2]\n    TRAIN: [1] TEST: [0 2]\n    [[3 4]] [[1 2]\n     [5 6]] [2] [1 1]\n    TRAIN: [0] TEST: [1 2]\n    [[1 2]] [[3 4]\n     [5 6]] [1] [2 1]\n\n    See also\n    --------\n    GroupKFold: K-fold iterator variant with non-overlapping groups.\n    \"\"\"\n\n    def __init__(self, n_groups):\n        self.n_groups = n_groups\n\n    def _iter_test_masks(self, X, y, groups):\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)\n        unique_groups = np.unique(groups)\n        if self.n_groups >= len(unique_groups):\n            raise ValueError(\n                \"The groups parameter contains fewer than (or equal to) \"\n                \"n_groups (%d) numbers of unique groups (%s). LeavePGroupsOut \"\n                \"expects that at least n_groups + 1 (%d) unique groups be \"\n                \"present\" % (self.n_groups, unique_groups, self.n_groups + 1))\n        combi = combinations(range(len(unique_groups)), self.n_groups)\n        for indices in combi:\n            test_index = np.zeros(_num_samples(X), dtype=np.bool)\n            for l in unique_groups[np.array(indices)]:\n                test_index[groups == l] = True\n            yield test_index\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set. This 'groups' parameter must always be specified to\n            calculate the number of splits, though the other parameters can be\n            omitted.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, ensure_2d=False, dtype=None)\n        return int(comb(len(np.unique(groups)), self.n_groups, exact=True))\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples, optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass _RepeatedSplits(metaclass=ABCMeta):\n    \"\"\"Repeated splits for an arbitrary randomized CV splitter.\n\n    Repeats splits for cross-validators n times with different randomization\n    in each repetition.\n\n    Parameters\n    ----------\n    cv : callable\n        Cross-validator class.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    **cvargs : additional params\n        Constructor parameters for cv. Must not contain random_state\n        and shuffle.\n    \"\"\"\n    def __init__(self, cv, n_repeats=10, random_state=None, **cvargs):\n        if not isinstance(n_repeats, numbers.Integral):\n            raise ValueError(\"Number of repetitions must be of Integral type.\")\n\n        if n_repeats <= 0:\n            raise ValueError(\"Number of repetitions must be greater than 0.\")\n\n        if any(key in cvargs for key in ('random_state', 'shuffle')):\n            raise ValueError(\n                \"cvargs must not contain random_state or shuffle.\")\n\n        self.cv = cv\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        self.cvargs = cvargs\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generates indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        n_repeats = self.n_repeats\n        rng = check_random_state(self.random_state)\n\n        for idx in range(n_repeats):\n            cv = self.cv(random_state=rng, shuffle=True,\n                         **self.cvargs)\n            for train_index, test_index in cv.split(X, y, groups):\n                yield train_index, test_index\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n            ``np.zeros(n_samples)`` may be used as a placeholder.\n\n        y : object\n            Always ignored, exists for compatibility.\n            ``np.zeros(n_samples)`` may be used as a placeholder.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        rng = check_random_state(self.random_state)\n        cv = self.cv(random_state=rng, shuffle=True,\n                     **self.cvargs)\n        return cv.get_n_splits(X, y, groups) * self.n_repeats\n\n\nclass RepeatedKFold(_RepeatedSplits):\n    \"\"\"Repeated K-Fold cross validator.\n\n    Repeats K-Fold n times with different randomization in each repetition.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import RepeatedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n    >>> for train_index, test_index in rkf.split(X):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [0 1] TEST: [2 3]\n    TRAIN: [2 3] TEST: [0 1]\n    TRAIN: [1 2] TEST: [0 3]\n    TRAIN: [0 3] TEST: [1 2]\n\n    Notes\n    -----\n    Randomized CV splitters may return different results for each call of\n    split. You can make the results identical by setting ``random_state``\n    to an integer.\n\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super().__init__(\n            KFold, n_repeats, random_state, n_splits=n_splits)\n\n\nclass RepeatedStratifiedKFold(_RepeatedSplits):\n    \"\"\"Repeated Stratified K-Fold cross validator.\n\n    Repeats Stratified K-Fold n times with different randomization in each\n    repetition.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : None, int or RandomState, default=None\n        Random state to be used to generate random state for each\n        repetition.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import RepeatedStratifiedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,\n    ...     random_state=36851234)\n    >>> for train_index, test_index in rskf.split(X, y):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [1 2] TEST: [0 3]\n    TRAIN: [0 3] TEST: [1 2]\n    TRAIN: [1 3] TEST: [0 2]\n    TRAIN: [0 2] TEST: [1 3]\n\n    Notes\n    -----\n    Randomized CV splitters may return different results for each call of\n    split. You can make the results identical by setting ``random_state``\n    to an integer.\n\n    See also\n    --------\n    RepeatedKFold: Repeats K-Fold n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super().__init__(\n            StratifiedKFold, n_repeats, random_state, n_splits=n_splits)\n\n\nclass BaseShuffleSplit(metaclass=ABCMeta):\n    \"\"\"Base class for ShuffleSplit and StratifiedShuffleSplit\"\"\"\n\n    def __init__(self, n_splits=10, test_size=None, train_size=None,\n                 random_state=None):\n        self.n_splits = n_splits\n        self.test_size = test_size\n        self.train_size = train_size\n        self.random_state = random_state\n        self._default_test_size = 0.1\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        X, y, groups = indexable(X, y, groups)\n        for train, test in self._iter_indices(X, y, groups):\n            yield train, test\n\n    @abstractmethod\n    def _iter_indices(self, X, y=None, groups=None):\n        \"\"\"Generate (train, test) indices\"\"\"\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return self.n_splits\n\n    def __repr__(self):\n        return _build_repr(self)\n\n\nclass ShuffleSplit(BaseShuffleSplit):\n    \"\"\"Random permutation cross-validator\n\n    Yields indices to split data into training and test sets.\n\n    Note: contrary to other cross-validation strategies, random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, default=None\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.1.\n\n    train_size : float, int, or None, default=None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import ShuffleSplit\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n    >>> y = np.array([1, 2, 1, 2, 1, 2])\n    >>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n    >>> rs.get_n_splits(X)\n    5\n    >>> print(rs)\n    ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n    >>> for train_index, test_index in rs.split(X):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    TRAIN: [1 3 0 4] TEST: [5 2]\n    TRAIN: [4 0 2 5] TEST: [1 3]\n    TRAIN: [1 2 4 0] TEST: [3 5]\n    TRAIN: [3 4 1 0] TEST: [5 2]\n    TRAIN: [3 5 1 0] TEST: [2 4]\n    >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,\n    ...                   random_state=0)\n    >>> for train_index, test_index in rs.split(X):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    TRAIN: [1 3 0] TEST: [5 2]\n    TRAIN: [4 0 2] TEST: [1 3]\n    TRAIN: [1 2 4] TEST: [3 5]\n    TRAIN: [3 4 1] TEST: [5 2]\n    TRAIN: [3 5 1] TEST: [2 4]\n    \"\"\"\n    def __init__(self, n_splits=10, test_size=None, train_size=None,\n                 random_state=None):\n        super().__init__(\n            n_splits=n_splits,\n(B)                      **self.cvargs)\n        return cv.get_n_splits(X, y, groups) * self.n_repeats\n\n\nclass RepeatedKFold(_RepeatedSplits):\n    \"\"\"Repeated K-Fold cross validator.\n\n    Repeats K-Fold n times with different randomization in each repetition.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import RepeatedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n    >>> for train_index, test_index in rkf.split(X):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [0 1] TEST: [2 3]\n    TRAIN: [2 3] TEST: [0 1]\n    TRAIN: [1 2] TEST: [0 3]\n    TRAIN: [0 3] TEST: [1 2]\n\n    Notes\n    -----\n    Randomized CV splitters may return different results for each call of\n    split. You can make the results identical by setting ``random_state``\n    to an integer.\n\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super().__init__(\n            KFold, n_repeats, random_state, n_splits=n_splits)\n\n\nclass RepeatedStratifiedKFold(_RepeatedSplits):\n    \"\"\"Repeated Stratified K-Fold cross validator.\n\n    Repeats Stratified K-Fold n times with different randomization in each\n    repetition.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : None, int or RandomState, default=None\n        Random state to be used to generate random state for each\n        repetition.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import RepeatedStratifiedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,\n    ...     random_state=36851234)\n    >>> for train_index, test_index in rskf.split(X, y):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [1 2] TEST: [0 3]\n    TRAIN: [0 3] TEST: [1 2]\n    TRAIN: [1 3] TEST: [0 2]\n    TRAIN: [0 2] TEST: [1 3]\n\n    Notes\n    -----\n    Randomized CV splitters may return different results for each call of\n    split. You can make the results identical by setting ``random_state``\n    to an integer.\n\n    See also\n    --------\n    RepeatedKFold: Repeats K-Fold n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super().__init__(\n            StratifiedKFold, n_repeats, random_state, n_splits=n_splits)\n\n\nclass BaseShuffleSplit(metaclass=ABCMeta):\n    \"\"\"Base class for ShuffleSplit and StratifiedShuffleSplit\"\"\"\n\n    def __init__(self, n_splits=10, test_size=None, train_size=None,\n                 random_state=None):\n        self.n_splits = n_splits\n        self.test_size = test_size\n        self.train_size = train_size\n        self.random_state = random_state\n        self._default_test_size = 0.1\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        X, y, groups = indexable(X, y, groups)\n        for train, test in self._iter_indices(X, y, groups):\n            yield train, test\n\n    @abstractmethod\n    def _iter_indices(self, X, y=None, groups=None):\n        \"\"\"Generate (train, test) indices\"\"\"\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return self.n_splits\n\n    def __repr__(self):\n        return _build_repr(self)\n\n\nclass ShuffleSplit(BaseShuffleSplit):\n    \"\"\"Random permutation cross-validator\n\n    Yields indices to split data into training and test sets.\n\n    Note: contrary to other cross-validation strategies, random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, default=None\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.1.\n\n    train_size : float, int, or None, default=None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import ShuffleSplit\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n    >>> y = np.array([1, 2, 1, 2, 1, 2])\n    >>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n    >>> rs.get_n_splits(X)\n    5\n    >>> print(rs)\n    ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n    >>> for train_index, test_index in rs.split(X):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    TRAIN: [1 3 0 4] TEST: [5 2]\n    TRAIN: [4 0 2 5] TEST: [1 3]\n    TRAIN: [1 2 4 0] TEST: [3 5]\n    TRAIN: [3 4 1 0] TEST: [5 2]\n    TRAIN: [3 5 1 0] TEST: [2 4]\n    >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,\n    ...                   random_state=0)\n    >>> for train_index, test_index in rs.split(X):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    TRAIN: [1 3 0] TEST: [5 2]\n    TRAIN: [4 0 2] TEST: [1 3]\n    TRAIN: [1 2 4] TEST: [3 5]\n    TRAIN: [3 4 1] TEST: [5 2]\n    TRAIN: [3 5 1] TEST: [2 4]\n    \"\"\"\n    def __init__(self, n_splits=10, test_size=None, train_size=None,\n                 random_state=None):\n        super().__init__(\n            n_splits=n_splits,\n            test_size=test_size,\n            train_size=train_size,\n            random_state=random_state)\n        self._default_test_size = 0.1\n\n    def _iter_indices(self, X, y=None, groups=None):\n        n_samples = _num_samples(X)\n        n_train, n_test = _validate_shuffle_split(\n            n_samples, self.test_size, self.train_size,\n            default_test_size=self._default_test_size)\n\n        rng = check_random_state(self.random_state)\n        for i in range(self.n_splits):\n            # random partition\n            permutation = rng.permutation(n_samples)\n            ind_test = permutation[:n_test]\n            ind_train = permutation[n_test:(n_test + n_train)]\n            yield ind_train, ind_test\n\n\nclass GroupShuffleSplit(ShuffleSplit):\n    '''Shuffle-Group(s)-Out cross-validation iterator\n\n    Provides randomized train/test indices to split data according to a\n    third-party provided group. This group information can be used to encode\n    arbitrary domain specific stratifications of the samples as integers.\n\n    For instance the groups could be the year of collection of the samples\n    and thus allow for cross-validation against time-based splits.\n\n    The difference between LeavePGroupsOut and GroupShuffleSplit is that\n    the former generates splits using all subsets of size ``p`` unique groups,\n    whereas GroupShuffleSplit generates a user-determined number of random\n    test splits, each with a user-determined fraction of unique groups.\n\n    For example, a less computationally intensive alternative to\n    ``LeavePGroupsOut(p=10)`` would be\n    ``GroupShuffleSplit(test_size=10, n_splits=100)``.\n\n    Note: The parameters ``test_size`` and ``train_size`` refer to groups, and\n    not to samples, as in ShuffleSplit.\n\n\n    Parameters\n    ----------\n    n_splits : int (default 5)\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, optional (default=None)\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of groups to include in the test split (rounded up). If int,\n        represents the absolute number of test groups. If None, the value is\n        set to the complement of the train size. By default, the value is set\n        to 0.2.\n        The default will change in version 0.21. It will remain 0.2 only\n        if ``train_size`` is unspecified, otherwise it will complement\n        the specified ``train_size``.\n\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the groups to include in the train split. If\n        int, represents the absolute number of train groups. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import GroupShuffleSplit\n    >>> X = np.ones(shape=(8, 2))\n    >>> y = np.ones(shape=(8, 1))\n    >>> groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n    >>> print(groups.shape)\n    (8,)\n    >>> gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n    >>> gss.get_n_splits()\n    2\n    >>> for train_idx, test_idx in gss.split(X, y, groups):\n    ...    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    TRAIN: [2 3 4 5 6 7] TEST: [0 1]\n    TRAIN: [0 1 5 6 7] TEST: [2 3 4]\n    '''\n\n    def __init__(self, n_splits=5, test_size=None, train_size=None,\n                 random_state=None):\n        super().__init__(\n            n_splits=n_splits,\n            test_size=test_size,\n            train_size=train_size,\n            random_state=random_state)\n        self._default_test_size = 0.2\n\n    def _iter_indices(self, X, y, groups):\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, ensure_2d=False, dtype=None)\n        classes, group_indices = np.unique(groups, return_inverse=True)\n        for group_train, group_test in super()._iter_indices(X=classes):\n            # these are the indices of classes in the partition\n            # invert them into data indices\n\n            train = np.flatnonzero(np.in1d(group_indices, group_train))\n            test = np.flatnonzero(np.in1d(group_indices, group_test))\n\n            yield train, test\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,), optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass StratifiedShuffleSplit(BaseShuffleSplit):\n    \"\"\"Stratified ShuffleSplit cross-validator\n\n    Provides train/test indices to split data in train/test sets.\n\n    This cross-validation object is a merge of StratifiedKFold and\n    ShuffleSplit, which returns stratified randomized folds. The folds\n    are made by preserving the percentage of samples for each class.\n\n    Note: like the ShuffleSplit strategy, stratified random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, optional (default=None)\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.1.\n\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import StratifiedShuffleSplit\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n    >>> sss.get_n_splits(X, y)\n    5\n    >>> print(sss)\n    StratifiedShuffleSplit(n_splits=5, random_state=0, ...)\n    >>> for train_index, test_index in sss.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [5 2 3] TEST: [4 1 0]\n    TRAIN: [5 1 4] TEST: [0 2 3]\n    TRAIN: [5 0 2] TEST: [4 3 1]\n    TRAIN: [4 1 0] TEST: [2 3 5]\n    TRAIN: [0 5 1] TEST: [3 4 2]\n    \"\"\"\n\n    def __init__(self, n_splits=10, test_size=None, train_size=None,\n                 random_state=None):\n        super().__init__(\n            n_splits=n_splits,\n            test_size=test_size,\n            train_size=train_size,\n            random_state=random_state)\n        self._default_test_size = 0.1\n\n    def _iter_indices(self, X, y, groups=None):\n        n_samples = _num_samples(X)\n        y = check_array(y, ensure_2d=False, dtype=None)\n        n_train, n_test = _validate_shuffle_split(\n            n_samples, self.test_size, self.train_size,\n            default_test_size=self._default_test_size)\n\n        if y.ndim == 2:\n            # for multi-label y, map each distinct row to a string repr\n            # using join because str(row) uses an ellipsis if len(row) > 1000\n            y = np.array([' '.join(row.astype('str')) for row in y])\n\n        classes, y_indices = np.unique(y, return_inverse=True)\n        n_classes = classes.shape[0]\n\n        class_counts = np.bincount(y_indices)\n        if np.min(class_counts) < 2:\n            raise ValueError(\"The least populated class in y has only 1\"\n                             \" member, which is too few. The minimum\"\n                             \" number of groups for any class cannot\"\n                             \" be less than 2.\")\n\n        if n_train < n_classes:\n            raise ValueError('The train_size = %d should be greater or '\n                             'equal to the number of classes = %d' %\n                             (n_train, n_classes))\n        if n_test < n_classes:\n            raise ValueError('The test_size = %d should be greater or '\n                             'equal to the number of classes = %d' %\n                             (n_test, n_classes))\n\n        # Find the sorted list of instances for each class:\n        # (np.unique above performs a sort, so code is O(n logn) already)\n        class_indices = np.split(np.argsort(y_indices, kind='mergesort'),\n                                 np.cumsum(class_counts)[:-1])\n\n        rng = check_random_state(self.random_state)\n\n        for _ in range(self.n_splits):\n            # if there are ties in the class-counts, we want\n            # to make sure to break them anew in each iteration\n            n_i = _approximate_mode(class_counts, n_train, rng)\n            class_counts_remaining = class_counts - n_i\n            t_i = _approximate_mode(class_counts_remaining, n_test, rng)\n\n            train = []\n            test = []\n\n            for i in range(n_classes):\n                permutation = rng.permutation(class_counts[i])\n                perm_indices_class_i = class_indices[i].take(permutation,\n                                                             mode='clip')\n\n                train.extend(perm_indices_class_i[:n_i[i]])\n                test.extend(perm_indices_class_i[n_i[i]:n_i[i] + t_i[i]])\n\n            train = rng.permutation(train)\n            test = rng.permutation(test)\n\n            yield train, test\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n            Stratification is done based on the y labels.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super().split(X, y, groups)\n\n\ndef _validate_shuffle_split(n_samples, test_size, train_size,\n                            default_test_size=None):\n    \"\"\"\n    Validation helper to check if the test/test sizes are meaningful wrt to the\n    size of the data (n_samples)\n    \"\"\"\n    if test_size is None and train_size is None:\n        test_size = default_test_size\n\n    test_size_type = np.asarray(test_size).dtype.kind\n    train_size_type = np.asarray(train_size).dtype.kind\n\n    if (test_size_type == 'i' and (test_size >= n_samples or test_size <= 0)\n       or test_size_type == 'f' and (test_size <= 0 or test_size >= 1)):\n        raise ValueError('test_size={0} should be either positive and smaller'\n                         ' than the number of samples {1} or a float in the '\n                         '(0, 1) range'.format(test_size, n_samples))\n\n    if (train_size_type == 'i' and (train_size >= n_samples or train_size <= 0)\n       or train_size_type == 'f' and (train_size <= 0 or train_size >= 1)):\n        raise ValueError('train_size={0} should be either positive and smaller'\n                         ' than the number of samples {1} or a float in the '\n                         '(0, 1) range'.format(train_size, n_samples))\n\n    if train_size is not None and train_size_type not in ('i', 'f'):\n        raise ValueError(\"Invalid value for train_size: {}\".format(train_size))\n    if test_size is not None and test_size_type not in ('i', 'f'):\n        raise ValueError(\"Invalid value for test_size: {}\".format(test_size))\n\n    if (train_size_type == 'f' and test_size_type == 'f' and\n            train_size + test_size > 1):\n        raise ValueError(\n            'The sum of test_size and train_size = {}, should be in the (0, 1)'\n            ' range. Reduce test_size and/or train_size.'\n            .format(train_size + test_size))\n\n    if test_size_type == 'f':\n        n_test = ceil(test_size * n_samples)\n    elif test_size_type == 'i':\n        n_test = float(test_size)\n\n    if train_size_type == 'f':\n        n_train = floor(train_size * n_samples)\n    elif train_size_type == 'i':\n        n_train = float(train_size)\n\n    if train_size is None:\n        n_train = n_samples - n_test\n    elif test_size is None:\n        n_test = n_samples - n_train\n\n    if n_train + n_test > n_samples:\n        raise ValueError('The sum of train_size and test_size = %d, '\n                         'should be smaller than the number of '\n                         'samples %d. Reduce test_size and/or '\n                         'train_size.' % (n_train + n_test, n_samples))\n\n    n_train, n_test = int(n_train), int(n_test)\n\n    if n_train == 0:\n        raise ValueError(\n            'With n_samples={}, test_size={} and train_size={}, the '\n            'resulting train set will be empty. Adjust any of the '\n            'aforementioned parameters.'.format(n_samples, test_size,\n                                                train_size)\n        )\n\n    return n_train, n_test\n\n\nclass PredefinedSplit(BaseCrossValidator):\n    \"\"\"Predefined split cross-validator\n\n    Provides train/test indices to split data into train/test sets using a\n    predefined scheme specified by the user with the ``test_fold`` parameter.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    test_fold : array-like, shape (n_samples,)\n        The entry ``test_fold[i]`` represents the index of the test set that\n        sample ``i`` belongs to. It is possible to exclude sample ``i`` from\n        any test set (i.e. include sample ``i`` in every training set) by\n        setting ``test_fold[i]`` equal to -1.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import PredefinedSplit\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> test_fold = [0, 1, -1, 1]\n    >>> ps = PredefinedSplit(test_fold)\n    >>> ps.get_n_splits()\n    2\n    >>> print(ps)\n    PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n    >>> for train_index, test_index in ps.split():\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [1 2 3] TEST: [0]\n    TRAIN: [0 2] TEST: [1 3]\n    \"\"\"\n\n    def __init__(self, test_fold):\n        self.test_fold = np.array(test_fold, dtype=np.int)\n        self.test_fold = column_or_1d(self.test_fold)\n        self.unique_folds = np.unique(self.test_fold)\n        self.unique_folds = self.unique_folds[self.unique_folds != -1]\n\n    def split(self, X=None, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        ind = np.arange(len(self.test_fold))\n        for test_index in self._iter_test_masks():\n            train_index = ind[np.logical_not(test_index)]\n            test_index = ind[test_index]\n            yield train_index, test_index\n\n    def _iter_test_masks(self):\n        \"\"\"Generates boolean masks corresponding to test sets.\"\"\"\n        for f in self.unique_folds:\n            test_index = np.where(self.test_fold == f)[0]\n            test_mask = np.zeros(len(self.test_fold), dtype=np.bool)\n            test_mask[test_index] = True\n            yield test_mask\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return len(self.unique_folds)\n\n\nclass _CVIterableWrapper(BaseCrossValidator):\n    \"\"\"Wrapper class for old style cv objects and iterables.\"\"\"\n    def __init__(self, cv):\n        self.cv = list(cv)\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return len(self.cv)\n\n    def split(self, X=None, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        for train, test in self.cv:\n            yield train, test\n\n\ndef check_cv(cv=5, y=None, classifier=False):\n    \"\"\"Input checker utility for building a cross-validator\n\n    Parameters\n    ----------\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if classifier is True and ``y`` is either\n        binary or multiclass, :class:`StratifiedKFold` is used. In all other\n        cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value changed from 3-fold to 5-fold.\n\n    y : array-like, optional\n        The target variable for supervised learning problems.\n\n    classifier : boolean, optional, default False\n        Whether the task is a classification task, in which case\n        stratified KFold will be used.\n\n    Returns\n    -------\n    checked_cv : a cross-validator instance.\n        The return value is a cross-validator which generates the train/test\n        splits via the ``split`` method.\n    \"\"\"\n    cv = 5 if cv is None else cv\n    if isinstance(cv, numbers.Integral):\n        if (classifier and (y is not None) and\n                (type_of_target(y) in ('binary', 'multiclass'))):\n            return StratifiedKFold(cv)\n        else:\n            return KFold(cv)\n\n    if not hasattr(cv, 'split') or isinstance(cv, str):\n        if not isinstance(cv, Iterable) or isinstance(cv, str):\n            raise ValueError(\"Expected cv as an integer, cross-validation \"\n                             \"object (from sklearn.model_selection) \"\n                             \"or an iterable. Got %s.\" % cv)\n        return _CVIterableWrapper(cv)\n\n    return cv  # New style cv objects are passed without any modification\n\n\ndef train_test_split(*arrays, **options):\n    \"\"\"Split arrays or matrices into random train and test subsets\n\n    Quick utility that wraps input validation and\n    ``next(ShuffleSplit().split(X, y))`` and application to input data\n    into a single call for splitting (and optionally subsampling) data in a\n    oneliner.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    *arrays : sequence of indexables with same length / shape[0]\n        Allowed inputs are lists, numpy arrays, scipy-sparse\n        matrices or pandas dataframes.\n\n    test_size : float, int or None, optional (default=None)\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.25.\n\n    train_size : float, int, or None, (default=None)\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    shuffle : boolean, optional (default=True)\n        Whether or not to shuffle the data before splitting. If shuffle=False\n        then stratify must be None.\n\n    stratify : array-like or None (default=None)\n        If not None, data is split in a stratified fashion, using this as\n        the class labels.\n\n    Returns\n    -------\n    splitting : list, length=2 * len(arrays)\n        List containing train-test split of inputs.\n\n        .. versionadded:: 0.16\n            If the input is sparse, the output will be a\n            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n            input type.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n    >>> X\n    array([[0, 1],\n           [2, 3],\n           [4, 5],\n           [6, 7],\n           [8, 9]])\n    >>> list(y)\n    [0, 1, 2, 3, 4]\n\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, test_size=0.33, random_state=42)\n    ...\n    >>> X_train\n    array([[4, 5],\n           [0, 1],\n           [6, 7]])\n    >>> y_train\n    [2, 0, 3]\n    >>> X_test\n    array([[2, 3],\n           [8, 9]])\n    >>> y_test\n    [1, 4]\n\n    >>> train_test_split(y, shuffle=False)\n    [[0, 1, 2], [3, 4]]\n\n    \"\"\"\n    n_arrays = len(arrays)\n    if n_arrays == 0:\n        raise ValueError(\"At least one array required as input\")\n    test_size = options.pop('test_size', None)\n    train_size = options.pop('train_size', None)\n    random_state = options.pop('random_state', None)\n    stratify = options.pop('stratify', None)\n    shuffle = options.pop('shuffle', True)\n\n    if options:\n        raise TypeError(\"Invalid parameters passed: %s\" % str(options))\n\n    arrays = indexable(*arrays)\n\n    n_samples = _num_samples(arrays[0])\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n                                              default_test_size=0.25)\n\n    if shuffle is False:\n        if stratify is not None:\n            raise ValueError(\n                \"Stratified train/test split is not implemented for \"\n                \"shuffle=False\")\n\n        train = np.arange(n_train)\n        test = np.arange(n_train, n_train + n_test)\n\n    else:\n        if stratify is not None:\n            CVClass = StratifiedShuffleSplit\n        else:\n            CVClass = ShuffleSplit\n\n        cv = CVClass(test_size=n_test,\n                     train_size=n_train,\n                     random_state=random_state)\n\n        train, test = next(cv.split(X=arrays[0], y=stratify))\n\n    return list(chain.from_iterable((safe_indexing(a, train),\n                                     safe_indexing(a, test)) for a in arrays))\n\n\n# Tell nose that train_test_split is not a test.\n# (Needed for external libraries that may use nose.)\ntrain_test_split.__test__ = False\n\n\ndef _build_repr(self):\n    # XXX This is copied from BaseEstimator's get_params\n    cls = self.__class__\n    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n    # Ignore varargs, kw and default values and pop self\n    init_signature = signature(init)\n    # Consider the constructor parameters excluding 'self'\n    if init is object.__init__:\n        args = []\n    else:\n        args = sorted([p.name for p in init_signature.parameters.values()\n                       if p.name != 'self' and p.kind != p.VAR_KEYWORD])\n    class_name = self.__class__.__name__\n    params = dict()\n    for key in args:\n        # We need deprecation warnings to always be on in order to\n        # catch deprecated param values.\n        # This is set in utils/__init__.py but it gets overwritten\n        # when running under python3 somehow.\n        warnings.simplefilter(\"always\", DeprecationWarning)\n        try:\n            with warnings.catch_warnings(record=True) as w:\n                value = getattr(self, key, None)\n            if len(w) and w[0].category == DeprecationWarning:\n                # if the parameter is deprecated, don't show it\n                continue\n        finally:\n            warnings.filters.pop(0)\n        params[key] = value\n\n    return '%s(%s)' % (class_name, _pprint(params, offset=len(class_name)))\n(C)         # counts, but that code is unreadable.)\n        y_order = np.sort(y_encoded)\n        allocation = np.asarray(\n            [np.bincount(y_order[i::self.n_splits], minlength=n_classes)\n             for i in range(self.n_splits)])\n\n        # To maintain the data order dependencies as best as possible within\n        # the stratification constraint, we assign samples from each class in\n        # blocks (and then mess that up when shuffle=True).\n        test_folds = np.empty(len(y), dtype='i')\n        for k in range(n_classes):\n            # since the kth column of allocation stores the number of samples\n            # of class k in each test set, this generates blocks of fold\n            # indices corresponding to the allocation for class k.\n            folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])\n            if self.shuffle:\n                rng.shuffle(folds_for_class)\n            test_folds[y_encoded == k] = folds_for_class\n        return test_folds\n\n    def _iter_test_masks(self, X, y=None, groups=None):\n        test_folds = self._make_test_folds(X, y)\n        for i in range(self.n_splits):\n            yield test_folds == i\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n            Stratification is done based on the y labels.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super().split(X, y, groups)\n\n\nclass TimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator\n\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals, in train/test sets.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n\n        .. versionchanged:: 0.22\n            ``n_splits`` default value changed from 3 to 5.\n\n    max_train_size : int, optional\n        Maximum size for a single training set.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import TimeSeriesSplit\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\n    >>> tscv = TimeSeriesSplit()\n    >>> print(tscv)\n    TimeSeriesSplit(max_train_size=None, n_splits=5)\n    >>> for train_index, test_index in tscv.split(X):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [0] TEST: [1]\n    TRAIN: [0 1] TEST: [2]\n    TRAIN: [0 1 2] TEST: [3]\n    TRAIN: [0 1 2 3] TEST: [4]\n    TRAIN: [0 1 2 3 4] TEST: [5]\n\n    Notes\n    -----\n    The training set has size ``i * n_samples // (n_splits + 1)\n    + n_samples % (n_splits + 1)`` in the ``i``th split,\n    with a test set of size ``n_samples//(n_splits + 1)``,\n    where ``n_samples`` is the number of samples.\n    \"\"\"\n    def __init__(self, n_splits=5, max_train_size=None):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_size = max_train_size\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,)\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        n_folds = n_splits + 1\n        if n_folds > n_samples:\n            raise ValueError(\n                (\"Cannot have number of folds ={0} greater\"\n                 \" than the number of samples: {1}.\").format(n_folds,\n                                                             n_samples))\n        indices = np.arange(n_samples)\n        test_size = (n_samples // n_folds)\n        test_starts = range(test_size + n_samples % n_folds,\n                            n_samples, test_size)\n        for test_start in test_starts:\n            if self.max_train_size and self.max_train_size < test_start:\n                yield (indices[test_start - self.max_train_size:test_start],\n                       indices[test_start:test_start + test_size])\n            else:\n                yield (indices[:test_start],\n                       indices[test_start:test_start + test_size])\n\n\nclass LeaveOneGroupOut(BaseCrossValidator):\n    \"\"\"Leave One Group Out cross-validator\n\n    Provides train/test indices to split data according to a third-party\n    provided group. This group information can be used to encode arbitrary\n    domain specific stratifications of the samples as integers.\n\n    For instance the groups could be the year of collection of the samples\n    and thus allow for cross-validation against time-based splits.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import LeaveOneGroupOut\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    >>> y = np.array([1, 2, 1, 2])\n    >>> groups = np.array([1, 1, 2, 2])\n    >>> logo = LeaveOneGroupOut()\n    >>> logo.get_n_splits(X, y, groups)\n    2\n    >>> logo.get_n_splits(groups=groups)  # 'groups' is always required\n    2\n    >>> print(logo)\n    LeaveOneGroupOut()\n    >>> for train_index, test_index in logo.split(X, y, groups):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    ...    print(X_train, X_test, y_train, y_test)\n    TRAIN: [2 3] TEST: [0 1]\n    [[5 6]\n     [7 8]] [[1 2]\n     [3 4]] [1 2] [1 2]\n    TRAIN: [0 1] TEST: [2 3]\n    [[1 2]\n     [3 4]] [[5 6]\n     [7 8]] [1 2] [1 2]\n\n    \"\"\"\n\n    def _iter_test_masks(self, X, y, groups):\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        # We make a copy of groups to avoid side-effects during iteration\n        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)\n        unique_groups = np.unique(groups)\n        if len(unique_groups) <= 1:\n            raise ValueError(\n                \"The groups parameter contains fewer than 2 unique groups \"\n                \"(%s). LeaveOneGroupOut expects at least 2.\" % unique_groups)\n        for i in unique_groups:\n            yield groups == i\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set. This 'groups' parameter must always be specified to\n            calculate the number of splits, though the other parameters can be\n            omitted.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, ensure_2d=False, dtype=None)\n        return len(np.unique(groups))\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples, optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass LeavePGroupsOut(BaseCrossValidator):\n    \"\"\"Leave P Group(s) Out cross-validator\n\n    Provides train/test indices to split data according to a third-party\n    provided group. This group information can be used to encode arbitrary\n    domain specific stratifications of the samples as integers.\n\n    For instance the groups could be the year of collection of the samples\n    and thus allow for cross-validation against time-based splits.\n\n    The difference between LeavePGroupsOut and LeaveOneGroupOut is that\n    the former builds the test sets with all the samples assigned to\n    ``p`` different values of the groups while the latter uses samples\n    all assigned the same groups.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_groups : int\n        Number of groups (``p``) to leave out in the test split.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import LeavePGroupsOut\n    >>> X = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> y = np.array([1, 2, 1])\n    >>> groups = np.array([1, 2, 3])\n    >>> lpgo = LeavePGroupsOut(n_groups=2)\n    >>> lpgo.get_n_splits(X, y, groups)\n    3\n    >>> lpgo.get_n_splits(groups=groups)  # 'groups' is always required\n    3\n    >>> print(lpgo)\n    LeavePGroupsOut(n_groups=2)\n    >>> for train_index, test_index in lpgo.split(X, y, groups):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    ...    print(X_train, X_test, y_train, y_test)\n    TRAIN: [2] TEST: [0 1]\n    [[5 6]] [[1 2]\n     [3 4]] [1] [1 2]\n    TRAIN: [1] TEST: [0 2]\n    [[3 4]] [[1 2]\n     [5 6]] [2] [1 1]\n    TRAIN: [0] TEST: [1 2]\n    [[1 2]] [[3 4]\n     [5 6]] [1] [2 1]\n\n    See also\n    --------\n    GroupKFold: K-fold iterator variant with non-overlapping groups.\n    \"\"\"\n\n    def __init__(self, n_groups):\n        self.n_groups = n_groups\n\n    def _iter_test_masks(self, X, y, groups):\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)\n        unique_groups = np.unique(groups)\n        if self.n_groups >= len(unique_groups):\n            raise ValueError(\n                \"The groups parameter contains fewer than (or equal to) \"\n                \"n_groups (%d) numbers of unique groups (%s). LeavePGroupsOut \"\n                \"expects that at least n_groups + 1 (%d) unique groups be \"\n                \"present\" % (self.n_groups, unique_groups, self.n_groups + 1))\n        combi = combinations(range(len(unique_groups)), self.n_groups)\n        for indices in combi:\n            test_index = np.zeros(_num_samples(X), dtype=np.bool)\n            for l in unique_groups[np.array(indices)]:\n                test_index[groups == l] = True\n            yield test_index\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set. This 'groups' parameter must always be specified to\n            calculate the number of splits, though the other parameters can be\n            omitted.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, ensure_2d=False, dtype=None)\n        return int(comb(len(np.unique(groups)), self.n_groups, exact=True))\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples, optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass _RepeatedSplits(metaclass=ABCMeta):\n    \"\"\"Repeated splits for an arbitrary randomized CV splitter.\n\n    Repeats splits for cross-validators n times with different randomization\n    in each repetition.\n\n    Parameters\n    ----------\n    cv : callable\n        Cross-validator class.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    **cvargs : additional params\n        Constructor parameters for cv. Must not contain random_state\n        and shuffle.\n    \"\"\"\n    def __init__(self, cv, n_repeats=10, random_state=None, **cvargs):\n        if not isinstance(n_repeats, numbers.Integral):\n            raise ValueError(\"Number of repetitions must be of Integral type.\")\n\n        if n_repeats <= 0:\n            raise ValueError(\"Number of repetitions must be greater than 0.\")\n\n        if any(key in cvargs for key in ('random_state', 'shuffle')):\n            raise ValueError(\n                \"cvargs must not contain random_state or shuffle.\")\n\n        self.cv = cv\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        self.cvargs = cvargs\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generates indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        n_repeats = self.n_repeats\n        rng = check_random_state(self.random_state)\n\n        for idx in range(n_repeats):\n            cv = self.cv(random_state=rng, shuffle=True,\n                         **self.cvargs)\n            for train_index, test_index in cv.split(X, y, groups):\n                yield train_index, test_index\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n            ``np.zeros(n_samples)`` may be used as a placeholder.\n\n        y : object\n            Always ignored, exists for compatibility.\n            ``np.zeros(n_samples)`` may be used as a placeholder.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        rng = check_random_state(self.random_state)\n        cv = self.cv(random_state=rng, shuffle=True,\n                     **self.cvargs)\n        return cv.get_n_splits(X, y, groups) * self.n_repeats\n\n\nclass RepeatedKFold(_RepeatedSplits):\n    \"\"\"Repeated K-Fold cross validator.\n\n    Repeats K-Fold n times with different randomization in each repetition.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import RepeatedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n    >>> for train_index, test_index in rkf.split(X):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [0 1] TEST: [2 3]\n    TRAIN: [2 3] TEST: [0 1]\n    TRAIN: [1 2] TEST: [0 3]\n    TRAIN: [0 3] TEST: [1 2]\n\n    Notes\n    -----\n    Randomized CV splitters may return different results for each call of\n    split. You can make the results identical by setting ``random_state``\n    to an integer.\n\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super().__init__(\n            KFold, n_repeats, random_state, n_splits=n_splits)\n\n\nclass RepeatedStratifiedKFold(_RepeatedSplits):\n    \"\"\"Repeated Stratified K-Fold cross validator.\n\n    Repeats Stratified K-Fold n times with different randomization in each\n    repetition.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : None, int or RandomState, default=None\n        Random state to be used to generate random state for each\n        repetition.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import RepeatedStratifiedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,\n    ...     random_state=36851234)\n    >>> for train_index, test_index in rskf.split(X, y):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [1 2] TEST: [0 3]\n    TRAIN: [0 3] TEST: [1 2]\n    TRAIN: [1 3] TEST: [0 2]\n    TRAIN: [0 2] TEST: [1 3]\n\n    Notes\n    -----\n    Randomized CV splitters may return different results for each call of\n    split. You can make the results identical by setting ``random_state``\n    to an integer.\n\n    See also\n    --------\n    RepeatedKFold: Repeats K-Fold n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super().__init__(\n            StratifiedKFold, n_repeats, random_state, n_splits=n_splits)\n\n\nclass BaseShuffleSplit(metaclass=ABCMeta):\n    \"\"\"Base class for ShuffleSplit and StratifiedShuffleSplit\"\"\"\n\n    def __init__(self, n_splits=10, test_size=None, train_size=None,\n                 random_state=None):\n        self.n_splits = n_splits\n        self.test_size = test_size\n        self.train_size = train_size\n        self.random_state = random_state\n        self._default_test_size = 0.1\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        X, y, groups = indexable(X, y, groups)\n        for train, test in self._iter_indices(X, y, groups):\n            yield train, test\n\n    @abstractmethod\n    def _iter_indices(self, X, y=None, groups=None):\n        \"\"\"Generate (train, test) indices\"\"\"\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return self.n_splits\n\n    def __repr__(self):\n        return _build_repr(self)\n\n\nclass ShuffleSplit(BaseShuffleSplit):\n    \"\"\"Random permutation cross-validator\n\n    Yields indices to split data into training and test sets.\n\n    Note: contrary to other cross-validation strategies, random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, default=None\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.1.\n\n    train_size : float, int, or None, default=None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import ShuffleSplit\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n    >>> y = np.array([1, 2, 1, 2, 1, 2])\n    >>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n    >>> rs.get_n_splits(X)\n    5\n    >>> print(rs)\n    ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n    >>> for train_index, test_index in rs.split(X):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    TRAIN: [1 3 0 4] TEST: [5 2]\n    TRAIN: [4 0 2 5] TEST: [1 3]\n    TRAIN: [1 2 4 0] TEST: [3 5]\n    TRAIN: [3 4 1 0] TEST: [5 2]\n    TRAIN: [3 5 1 0] TEST: [2 4]\n    >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,\n    ...                   random_state=0)\n    >>> for train_index, test_index in rs.split(X):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    TRAIN: [1 3 0] TEST: [5 2]\n    TRAIN: [4 0 2] TEST: [1 3]\n    TRAIN: [1 2 4] TEST: [3 5]\n    TRAIN: [3 4 1] TEST: [5 2]\n    TRAIN: [3 5 1] TEST: [2 4]\n    \"\"\"\n    def __init__(self, n_splits=10, test_size=None, train_size=None,\n                 random_state=None):\n        super().__init__(\n            n_splits=n_splits,\n            test_size=test_size,\n            train_size=train_size,\n            random_state=random_state)\n        self._default_test_size = 0.1\n\n    def _iter_indices(self, X, y=None, groups=None):\n        n_samples = _num_samples(X)\n        n_train, n_test = _validate_shuffle_split(\n            n_samples, self.test_size, self.train_size,\n            default_test_size=self._default_test_size)\n\n        rng = check_random_state(self.random_state)\n        for i in range(self.n_splits):\n            # random partition\n            permutation = rng.permutation(n_samples)\n            ind_test = permutation[:n_test]\n            ind_train = permutation[n_test:(n_test + n_train)]\n            yield ind_train, ind_test\n\n\nclass GroupShuffleSplit(ShuffleSplit):\n    '''Shuffle-Group(s)-Out cross-validation iterator\n\n    Provides randomized train/test indices to split data according to a\n    third-party provided group. This group information can be used to encode\n    arbitrary domain specific stratifications of the samples as integers.\n\n    For instance the groups could be the year of collection of the samples\n    and thus allow for cross-validation against time-based splits.\n\n    The difference between LeavePGroupsOut and GroupShuffleSplit is that\n    the former generates splits using all subsets of size ``p`` unique groups,\n    whereas GroupShuffleSplit generates a user-determined number of random\n    test splits, each with a user-determined fraction of unique groups.\n\n    For example, a less computationally intensive alternative to\n    ``LeavePGroupsOut(p=10)`` would be\n    ``GroupShuffleSplit(test_size=10, n_splits=100)``.\n\n    Note: The parameters ``test_size`` and ``train_size`` refer to groups, and\n    not to samples, as in ShuffleSplit.\n\n\n    Parameters\n    ----------\n    n_splits : int (default 5)\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, optional (default=None)\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of groups to include in the test split (rounded up). If int,\n        represents the absolute number of test groups. If None, the value is\n        set to the complement of the train size. By default, the value is set\n        to 0.2.\n        The default will change in version 0.21. It will remain 0.2 only\n        if ``train_size`` is unspecified, otherwise it will complement\n        the specified ``train_size``.\n\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the groups to include in the train split. If\n        int, represents the absolute number of train groups. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import GroupShuffleSplit\n    >>> X = np.ones(shape=(8, 2))\n    >>> y = np.ones(shape=(8, 1))\n    >>> groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n    >>> print(groups.shape)\n    (8,)\n    >>> gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n    >>> gss.get_n_splits()\n    2\n    >>> for train_idx, test_idx in gss.split(X, y, groups):\n    ...    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    TRAIN: [2 3 4 5 6 7] TEST: [0 1]\n    TRAIN: [0 1 5 6 7] TEST: [2 3 4]\n    '''\n\n    def __init__(self, n_splits=5, test_size=None, train_size=None,\n                 random_state=None):\n        super().__init__(\n            n_splits=n_splits,\n            test_size=test_size,\n            train_size=train_size,\n            random_state=random_state)\n        self._default_test_size = 0.2\n\n    def _iter_indices(self, X, y, groups):\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, ensure_2d=False, dtype=None)\n        classes, group_indices = np.unique(groups, return_inverse=True)\n        for group_train, group_test in super()._iter_indices(X=classes):\n            # these are the indices of classes in the partition\n            # invert them into data indices\n\n            train = np.flatnonzero(np.in1d(group_indices, group_train))\n            test = np.flatnonzero(np.in1d(group_indices, group_test))\n\n            yield train, test\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,), optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass StratifiedShuffleSplit(BaseShuffleSplit):\n    \"\"\"Stratified ShuffleSplit cross-validator\n\n    Provides train/test indices to split data in train/test sets.\n\n    This cross-validation object is a merge of StratifiedKFold and\n    ShuffleSplit, which returns stratified randomized folds. The folds\n    are made by preserving the percentage of samples for each class.\n\n    Note: like the ShuffleSplit strategy, stratified random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, optional (default=None)\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.1.\n\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import StratifiedShuffleSplit\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n    >>> sss.get_n_splits(X, y)\n    5\n    >>> print(sss)\n    StratifiedShuffleSplit(n_splits=5, random_state=0, ...)\n    >>> for train_index, test_index in sss.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [5 2 3] TEST: [4 1 0]\n    TRAIN: [5 1 4] TEST: [0 2 3]\n    TRAIN: [5 0 2] TEST: [4 3 1]\n    TRAIN: [4 1 0] TEST: [2 3 5]\n    TRAIN: [0 5 1] TEST: [3 4 2]\n    \"\"\"\n\n    def __init__(self, n_splits=10, test_size=None, train_size=None,\n                 random_state=None):\n        super().__init__(\n            n_splits=n_splits,\n            test_size=test_size,\n            train_size=train_size,\n            random_state=random_state)\n        self._default_test_size = 0.1\n\n    def _iter_indices(self, X, y, groups=None):\n        n_samples = _num_samples(X)\n        y = check_array(y, ensure_2d=False, dtype=None)\n        n_train, n_test = _validate_shuffle_split(\n            n_samples, self.test_size, self.train_size,\n            default_test_size=self._default_test_size)\n\n        if y.ndim == 2:\n            # for multi-label y, map each distinct row to a string repr\n            # using join because str(row) uses an ellipsis if len(row) > 1000\n            y = np.array([' '.join(row.astype('str')) for row in y])\n\n        classes, y_indices = np.unique(y, return_inverse=True)\n        n_classes = classes.shape[0]\n\n        class_counts = np.bincount(y_indices)\n        if np.min(class_counts) < 2:\n            raise ValueError(\"The least populated class in y has only 1\"\n                             \" member, which is too few. The minimum\"\n                             \" number of groups for any class cannot\"\n                             \" be less than 2.\")\n\n        if n_train < n_classes:\n            raise ValueError('The train_size = %d should be greater or '\n                             'equal to the number of classes = %d' %\n                             (n_train, n_classes))\n        if n_test < n_classes:\n            raise ValueError('The test_size = %d should be greater or '\n                             'equal to the number of classes = %d' %\n                             (n_test, n_classes))\n\n        # Find the sorted list of instances for each class:\n        # (np.unique above performs a sort, so code is O(n logn) already)\n        class_indices = np.split(np.argsort(y_indices, kind='mergesort'),\n                                 np.cumsum(class_counts)[:-1])\n\n        rng = check_random_state(self.random_state)\n\n(D)         Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples, optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass LeavePGroupsOut(BaseCrossValidator):\n    \"\"\"Leave P Group(s) Out cross-validator\n\n    Provides train/test indices to split data according to a third-party\n    provided group. This group information can be used to encode arbitrary\n    domain specific stratifications of the samples as integers.\n\n    For instance the groups could be the year of collection of the samples\n    and thus allow for cross-validation against time-based splits.\n\n    The difference between LeavePGroupsOut and LeaveOneGroupOut is that\n    the former builds the test sets with all the samples assigned to\n    ``p`` different values of the groups while the latter uses samples\n    all assigned the same groups.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_groups : int\n        Number of groups (``p``) to leave out in the test split.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import LeavePGroupsOut\n    >>> X = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> y = np.array([1, 2, 1])\n    >>> groups = np.array([1, 2, 3])\n    >>> lpgo = LeavePGroupsOut(n_groups=2)\n    >>> lpgo.get_n_splits(X, y, groups)\n    3\n    >>> lpgo.get_n_splits(groups=groups)  # 'groups' is always required\n    3\n    >>> print(lpgo)\n    LeavePGroupsOut(n_groups=2)\n    >>> for train_index, test_index in lpgo.split(X, y, groups):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    ...    print(X_train, X_test, y_train, y_test)\n    TRAIN: [2] TEST: [0 1]\n    [[5 6]] [[1 2]\n     [3 4]] [1] [1 2]\n    TRAIN: [1] TEST: [0 2]\n    [[3 4]] [[1 2]\n     [5 6]] [2] [1 1]\n    TRAIN: [0] TEST: [1 2]\n    [[1 2]] [[3 4]\n     [5 6]] [1] [2 1]\n\n    See also\n    --------\n    GroupKFold: K-fold iterator variant with non-overlapping groups.\n    \"\"\"\n\n    def __init__(self, n_groups):\n        self.n_groups = n_groups\n\n    def _iter_test_masks(self, X, y, groups):\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)\n        unique_groups = np.unique(groups)\n        if self.n_groups >= len(unique_groups):\n            raise ValueError(\n                \"The groups parameter contains fewer than (or equal to) \"\n                \"n_groups (%d) numbers of unique groups (%s). LeavePGroupsOut \"\n                \"expects that at least n_groups + 1 (%d) unique groups be \"\n                \"present\" % (self.n_groups, unique_groups, self.n_groups + 1))\n        combi = combinations(range(len(unique_groups)), self.n_groups)\n        for indices in combi:\n            test_index = np.zeros(_num_samples(X), dtype=np.bool)\n            for l in unique_groups[np.array(indices)]:\n                test_index[groups == l] = True\n            yield test_index\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set. This 'groups' parameter must always be specified to\n            calculate the number of splits, though the other parameters can be\n            omitted.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, ensure_2d=False, dtype=None)\n        return int(comb(len(np.unique(groups)), self.n_groups, exact=True))\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples, optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass _RepeatedSplits(metaclass=ABCMeta):\n    \"\"\"Repeated splits for an arbitrary randomized CV splitter.\n\n    Repeats splits for cross-validators n times with different randomization\n    in each repetition.\n\n    Parameters\n    ----------\n    cv : callable\n        Cross-validator class.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    **cvargs : additional params\n        Constructor parameters for cv. Must not contain random_state\n        and shuffle.\n    \"\"\"\n    def __init__(self, cv, n_repeats=10, random_state=None, **cvargs):\n        if not isinstance(n_repeats, numbers.Integral):\n            raise ValueError(\"Number of repetitions must be of Integral type.\")\n\n        if n_repeats <= 0:\n            raise ValueError(\"Number of repetitions must be greater than 0.\")\n\n        if any(key in cvargs for key in ('random_state', 'shuffle')):\n            raise ValueError(\n                \"cvargs must not contain random_state or shuffle.\")\n\n        self.cv = cv\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        self.cvargs = cvargs\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generates indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        n_repeats = self.n_repeats\n        rng = check_random_state(self.random_state)\n\n        for idx in range(n_repeats):\n            cv = self.cv(random_state=rng, shuffle=True,\n                         **self.cvargs)\n            for train_index, test_index in cv.split(X, y, groups):\n                yield train_index, test_index\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n            ``np.zeros(n_samples)`` may be used as a placeholder.\n\n        y : object\n            Always ignored, exists for compatibility.\n            ``np.zeros(n_samples)`` may be used as a placeholder.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        rng = check_random_state(self.random_state)\n        cv = self.cv(random_state=rng, shuffle=True,\n                     **self.cvargs)\n        return cv.get_n_splits(X, y, groups) * self.n_repeats\n\n\nclass RepeatedKFold(_RepeatedSplits):\n    \"\"\"Repeated K-Fold cross validator.\n\n    Repeats K-Fold n times with different randomization in each repetition.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import RepeatedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n    >>> for train_index, test_index in rkf.split(X):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [0 1] TEST: [2 3]\n    TRAIN: [2 3] TEST: [0 1]\n    TRAIN: [1 2] TEST: [0 3]\n    TRAIN: [0 3] TEST: [1 2]\n\n    Notes\n    -----\n    Randomized CV splitters may return different results for each call of\n    split. You can make the results identical by setting ``random_state``\n    to an integer.\n\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super().__init__(\n            KFold, n_repeats, random_state, n_splits=n_splits)\n\n\nclass RepeatedStratifiedKFold(_RepeatedSplits):\n    \"\"\"Repeated Stratified K-Fold cross validator.\n\n    Repeats Stratified K-Fold n times with different randomization in each\n    repetition.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : None, int or RandomState, default=None\n        Random state to be used to generate random state for each\n        repetition.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import RepeatedStratifiedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,\n    ...     random_state=36851234)\n    >>> for train_index, test_index in rskf.split(X, y):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [1 2] TEST: [0 3]\n    TRAIN: [0 3] TEST: [1 2]\n    TRAIN: [1 3] TEST: [0 2]\n    TRAIN: [0 2] TEST: [1 3]\n\n    Notes\n    -----\n    Randomized CV splitters may return different results for each call of\n    split. You can make the results identical by setting ``random_state``\n    to an integer.\n\n    See also\n    --------\n    RepeatedKFold: Repeats K-Fold n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super().__init__(\n            StratifiedKFold, n_repeats, random_state, n_splits=n_splits)\n\n\nclass BaseShuffleSplit(metaclass=ABCMeta):\n    \"\"\"Base class for ShuffleSplit and StratifiedShuffleSplit\"\"\"\n\n    def __init__(self, n_splits=10, test_size=None, train_size=None,\n                 random_state=None):\n        self.n_splits = n_splits\n        self.test_size = test_size\n        self.train_size = train_size\n        self.random_state = random_state\n        self._default_test_size = 0.1\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        X, y, groups = indexable(X, y, groups)\n        for train, test in self._iter_indices(X, y, groups):\n            yield train, test\n\n    @abstractmethod\n    def _iter_indices(self, X, y=None, groups=None):\n        \"\"\"Generate (train, test) indices\"\"\"\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return self.n_splits\n\n    def __repr__(self):\n        return _build_repr(self)\n\n\nclass ShuffleSplit(BaseShuffleSplit):\n    \"\"\"Random permutation cross-validator\n\n    Yields indices to split data into training and test sets.\n\n    Note: contrary to other cross-validation strategies, random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, default=None\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.1.\n\n    train_size : float, int, or None, default=None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import ShuffleSplit\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n    >>> y = np.array([1, 2, 1, 2, 1, 2])\n    >>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n    >>> rs.get_n_splits(X)\n    5\n    >>> print(rs)\n    ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n    >>> for train_index, test_index in rs.split(X):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    TRAIN: [1 3 0 4] TEST: [5 2]\n    TRAIN: [4 0 2 5] TEST: [1 3]\n    TRAIN: [1 2 4 0] TEST: [3 5]\n    TRAIN: [3 4 1 0] TEST: [5 2]\n    TRAIN: [3 5 1 0] TEST: [2 4]\n    >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,\n    ...                   random_state=0)\n    >>> for train_index, test_index in rs.split(X):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    TRAIN: [1 3 0] TEST: [5 2]\n    TRAIN: [4 0 2] TEST: [1 3]\n    TRAIN: [1 2 4] TEST: [3 5]\n    TRAIN: [3 4 1] TEST: [5 2]\n    TRAIN: [3 5 1] TEST: [2 4]\n    \"\"\"\n    def __init__(self, n_splits=10, test_size=None, train_size=None,\n                 random_state=None):\n        super().__init__(\n            n_splits=n_splits,\n            test_size=test_size,\n            train_size=train_size,\n            random_state=random_state)\n        self._default_test_size = 0.1\n\n    def _iter_indices(self, X, y=None, groups=None):\n        n_samples = _num_samples(X)\n        n_train, n_test = _validate_shuffle_split(\n            n_samples, self.test_size, self.train_size,\n            default_test_size=self._default_test_size)\n\n        rng = check_random_state(self.random_state)\n        for i in range(self.n_splits):\n            # random partition\n            permutation = rng.permutation(n_samples)\n            ind_test = permutation[:n_test]\n            ind_train = permutation[n_test:(n_test + n_train)]\n            yield ind_train, ind_test\n\n\nclass GroupShuffleSplit(ShuffleSplit):\n    '''Shuffle-Group(s)-Out cross-validation iterator\n\n    Provides randomized train/test indices to split data according to a\n    third-party provided group. This group information can be used to encode\n    arbitrary domain specific stratifications of the samples as integers.\n\n    For instance the groups could be the year of collection of the samples\n    and thus allow for cross-validation against time-based splits.\n\n    The difference between LeavePGroupsOut and GroupShuffleSplit is that\n    the former generates splits using all subsets of size ``p`` unique groups,\n    whereas GroupShuffleSplit generates a user-determined number of random\n    test splits, each with a user-determined fraction of unique groups.\n\n    For example, a less computationally intensive alternative to\n    ``LeavePGroupsOut(p=10)`` would be\n    ``GroupShuffleSplit(test_size=10, n_splits=100)``.\n\n    Note: The parameters ``test_size`` and ``train_size`` refer to groups, and\n    not to samples, as in ShuffleSplit.\n\n\n    Parameters\n    ----------\n    n_splits : int (default 5)\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, optional (default=None)\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of groups to include in the test split (rounded up). If int,\n        represents the absolute number of test groups. If None, the value is\n        set to the complement of the train size. By default, the value is set\n        to 0.2.\n        The default will change in version 0.21. It will remain 0.2 only\n        if ``train_size`` is unspecified, otherwise it will complement\n        the specified ``train_size``.\n\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the groups to include in the train split. If\n        int, represents the absolute number of train groups. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import GroupShuffleSplit\n    >>> X = np.ones(shape=(8, 2))\n    >>> y = np.ones(shape=(8, 1))\n    >>> groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n    >>> print(groups.shape)\n    (8,)\n    >>> gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n    >>> gss.get_n_splits()\n    2\n    >>> for train_idx, test_idx in gss.split(X, y, groups):\n    ...    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    TRAIN: [2 3 4 5 6 7] TEST: [0 1]\n    TRAIN: [0 1 5 6 7] TEST: [2 3 4]\n    '''\n\n    def __init__(self, n_splits=5, test_size=None, train_size=None,\n                 random_state=None):\n        super().__init__(\n            n_splits=n_splits,\n            test_size=test_size,\n            train_size=train_size,\n            random_state=random_state)\n        self._default_test_size = 0.2\n\n    def _iter_indices(self, X, y, groups):\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, ensure_2d=False, dtype=None)\n        classes, group_indices = np.unique(groups, return_inverse=True)\n        for group_train, group_test in super()._iter_indices(X=classes):\n            # these are the indices of classes in the partition\n            # invert them into data indices\n\n            train = np.flatnonzero(np.in1d(group_indices, group_train))\n            test = np.flatnonzero(np.in1d(group_indices, group_test))\n\n            yield train, test\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,), optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        return super().split(X, y, groups)\n\n\nclass StratifiedShuffleSplit(BaseShuffleSplit):\n    \"\"\"Stratified ShuffleSplit cross-validator\n\n    Provides train/test indices to split data in train/test sets.\n\n    This cross-validation object is a merge of StratifiedKFold and\n    ShuffleSplit, which returns stratified randomized folds. The folds\n    are made by preserving the percentage of samples for each class.\n\n    Note: like the ShuffleSplit strategy, stratified random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, optional (default=None)\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.1.\n\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import StratifiedShuffleSplit\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n    >>> sss.get_n_splits(X, y)\n    5\n    >>> print(sss)\n    StratifiedShuffleSplit(n_splits=5, random_state=0, ...)\n    >>> for train_index, test_index in sss.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [5 2 3] TEST: [4 1 0]\n    TRAIN: [5 1 4] TEST: [0 2 3]\n    TRAIN: [5 0 2] TEST: [4 3 1]\n    TRAIN: [4 1 0] TEST: [2 3 5]\n    TRAIN: [0 5 1] TEST: [3 4 2]\n    \"\"\"\n\n    def __init__(self, n_splits=10, test_size=None, train_size=None,\n                 random_state=None):\n        super().__init__(\n            n_splits=n_splits,\n            test_size=test_size,\n            train_size=train_size,\n            random_state=random_state)\n        self._default_test_size = 0.1\n\n    def _iter_indices(self, X, y, groups=None):\n        n_samples = _num_samples(X)\n        y = check_array(y, ensure_2d=False, dtype=None)\n        n_train, n_test = _validate_shuffle_split(\n            n_samples, self.test_size, self.train_size,\n            default_test_size=self._default_test_size)\n\n        if y.ndim == 2:\n            # for multi-label y, map each distinct row to a string repr\n            # using join because str(row) uses an ellipsis if len(row) > 1000\n            y = np.array([' '.join(row.astype('str')) for row in y])\n\n        classes, y_indices = np.unique(y, return_inverse=True)\n        n_classes = classes.shape[0]\n\n        class_counts = np.bincount(y_indices)\n        if np.min(class_counts) < 2:\n            raise ValueError(\"The least populated class in y has only 1\"\n                             \" member, which is too few. The minimum\"\n                             \" number of groups for any class cannot\"\n                             \" be less than 2.\")\n\n        if n_train < n_classes:\n            raise ValueError('The train_size = %d should be greater or '\n                             'equal to the number of classes = %d' %\n                             (n_train, n_classes))\n        if n_test < n_classes:\n            raise ValueError('The test_size = %d should be greater or '\n                             'equal to the number of classes = %d' %\n                             (n_test, n_classes))\n\n        # Find the sorted list of instances for each class:\n        # (np.unique above performs a sort, so code is O(n logn) already)\n        class_indices = np.split(np.argsort(y_indices, kind='mergesort'),\n                                 np.cumsum(class_counts)[:-1])\n\n        rng = check_random_state(self.random_state)\n\n        for _ in range(self.n_splits):\n            # if there are ties in the class-counts, we want\n            # to make sure to break them anew in each iteration\n            n_i = _approximate_mode(class_counts, n_train, rng)\n            class_counts_remaining = class_counts - n_i\n            t_i = _approximate_mode(class_counts_remaining, n_test, rng)\n\n            train = []\n            test = []\n\n            for i in range(n_classes):\n                permutation = rng.permutation(class_counts[i])\n                perm_indices_class_i = class_indices[i].take(permutation,\n                                                             mode='clip')\n\n                train.extend(perm_indices_class_i[:n_i[i]])\n                test.extend(perm_indices_class_i[n_i[i]:n_i[i] + t_i[i]])\n\n            train = rng.permutation(train)\n            test = rng.permutation(test)\n\n            yield train, test\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n            Stratification is done based on the y labels.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super().split(X, y, groups)\n\n\ndef _validate_shuffle_split(n_samples, test_size, train_size,\n                            default_test_size=None):\n    \"\"\"\n    Validation helper to check if the test/test sizes are meaningful wrt to the\n    size of the data (n_samples)\n    \"\"\"\n    if test_size is None and train_size is None:\n        test_size = default_test_size\n\n    test_size_type = np.asarray(test_size).dtype.kind\n    train_size_type = np.asarray(train_size).dtype.kind\n\n    if (test_size_type == 'i' and (test_size >= n_samples or test_size <= 0)\n       or test_size_type == 'f' and (test_size <= 0 or test_size >= 1)):\n        raise ValueError('test_size={0} should be either positive and smaller'\n                         ' than the number of samples {1} or a float in the '\n                         '(0, 1) range'.format(test_size, n_samples))\n\n    if (train_size_type == 'i' and (train_size >= n_samples or train_size <= 0)\n       or train_size_type == 'f' and (train_size <= 0 or train_size >= 1)):\n        raise ValueError('train_size={0} should be either positive and smaller'\n                         ' than the number of samples {1} or a float in the '\n                         '(0, 1) range'.format(train_size, n_samples))\n\n    if train_size is not None and train_size_type not in ('i', 'f'):\n        raise ValueError(\"Invalid value for train_size: {}\".format(train_size))\n    if test_size is not None and test_size_type not in ('i', 'f'):\n        raise ValueError(\"Invalid value for test_size: {}\".format(test_size))\n\n    if (train_size_type == 'f' and test_size_type == 'f' and\n            train_size + test_size > 1):\n        raise ValueError(\n            'The sum of test_size and train_size = {}, should be in the (0, 1)'\n            ' range. Reduce test_size and/or train_size.'\n            .format(train_size + test_size))\n\n    if test_size_type == 'f':\n        n_test = ceil(test_size * n_samples)\n    elif test_size_type == 'i':\n        n_test = float(test_size)\n\n    if train_size_type == 'f':\n        n_train = floor(train_size * n_samples)\n    elif train_size_type == 'i':\n        n_train = float(train_size)\n\n    if train_size is None:\n        n_train = n_samples - n_test\n    elif test_size is None:\n        n_test = n_samples - n_train\n\n    if n_train + n_test > n_samples:\n        raise ValueError('The sum of train_size and test_size = %d, '\n                         'should be smaller than the number of '\n                         'samples %d. Reduce test_size and/or '\n                         'train_size.' % (n_train + n_test, n_samples))\n\n    n_train, n_test = int(n_train), int(n_test)\n\n    if n_train == 0:\n        raise ValueError(\n            'With n_samples={}, test_size={} and train_size={}, the '\n            'resulting train set will be empty. Adjust any of the '\n            'aforementioned parameters.'.format(n_samples, test_size,\n                                                train_size)\n        )\n\n    return n_train, n_test\n\n\nclass PredefinedSplit(BaseCrossValidator):\n    \"\"\"Predefined split cross-validator\n\n    Provides train/test indices to split data into train/test sets using a\n    predefined scheme specified by the user with the ``test_fold`` parameter.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    test_fold : array-like, shape (n_samples,)\n        The entry ``test_fold[i]`` represents the index of the test set that\n        sample ``i`` belongs to. It is possible to exclude sample ``i`` from\n        any test set (i.e. include sample ``i`` in every training set) by\n        setting ``test_fold[i]`` equal to -1.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import PredefinedSplit\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> test_fold = [0, 1, -1, 1]\n    >>> ps = PredefinedSplit(test_fold)\n    >>> ps.get_n_splits()\n    2\n    >>> print(ps)\n    PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n    >>> for train_index, test_index in ps.split():\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [1 2 3] TEST: [0]\n    TRAIN: [0 2] TEST: [1 3]\n    \"\"\"\n\n    def __init__(self, test_fold):\n        self.test_fold = np.array(test_fold, dtype=np.int)\n        self.test_fold = column_or_1d(self.test_fold)\n        self.unique_folds = np.unique(self.test_fold)\n        self.unique_folds = self.unique_folds[self.unique_folds != -1]\n\n    def split(self, X=None, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        ind = np.arange(len(self.test_fold))\n        for test_index in self._iter_test_masks():\n            train_index = ind[np.logical_not(test_index)]\n            test_index = ind[test_index]\n            yield train_index, test_index\n\n    def _iter_test_masks(self):\n        \"\"\"Generates boolean masks corresponding to test sets.\"\"\"\n        for f in self.unique_folds:\n            test_index = np.where(self.test_fold == f)[0]\n            test_mask = np.zeros(len(self.test_fold), dtype=np.bool)\n            test_mask[test_index] = True\n            yield test_mask\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return len(self.unique_folds)\n\n\nclass _CVIterableWrapper(BaseCrossValidator):\n    \"\"\"Wrapper class for old style cv objects and iterables.\"\"\"\n    def __init__(self, cv):\n        self.cv = list(cv)\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns", "answer": "B"}
{"uuid": "20a9dadc-d18f-4d99-a7a1-fc80d3eefeaa", "issue_statement": "Return values of non converged affinity propagation clustering\nThe affinity propagation Documentation states: \r\n\"When the algorithm does not converge, it returns an empty array as cluster_center_indices and -1 as label for each training sample.\"\r\n\r\nExample:\r\n```python\r\nfrom sklearn.cluster import AffinityPropagation\r\nimport pandas as pd\r\n\r\ndata = pd.DataFrame([[1,0,0,0,0,0],[0,1,1,1,0,0],[0,0,1,0,0,1]])\r\naf = AffinityPropagation(affinity='euclidean', verbose=True, copy=False, max_iter=2).fit(data)\r\n\r\nprint(af.cluster_centers_indices_)\r\nprint(af.labels_)\r\n\r\n```\r\nI would expect that the clustering here (which does not converge) prints first an empty List and then [-1,-1,-1], however, I get [2] as cluster center and [0,0,0] as cluster labels. \r\nThe only way I currently know if the clustering fails is if I use the verbose option, however that is very unhandy. A hacky solution is to check if max_iter == n_iter_ but it could have converged exactly 15 iterations before max_iter (although unlikely).\r\nI am not sure if this is intended behavior and the documentation is wrong?\r\n\r\nFor my use-case within a bigger script, I would prefer to get back -1 values or have a property to check if it has converged, as otherwise, a user might not be aware that the clustering never converged.\r\n\r\n\r\n#### Versions\r\nSystem:\r\n    python: 3.6.7 | packaged by conda-forge | (default, Nov 21 2018, 02:32:25)  [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\r\nexecutable: /home/jenniferh/Programs/anaconda3/envs/TF_RDKit_1_19/bin/python\r\n   machine: Linux-4.15.0-52-generic-x86_64-with-debian-stretch-sid\r\nBLAS:\r\n    macros: SCIPY_MKL_H=None, HAVE_CBLAS=None\r\n  lib_dirs: /home/jenniferh/Programs/anaconda3/envs/TF_RDKit_1_19/lib\r\ncblas_libs: mkl_rt, pthread\r\nPython deps:\r\n    pip: 18.1\r\n   setuptools: 40.6.3\r\n   sklearn: 0.20.3\r\n   numpy: 1.15.4\r\n   scipy: 1.2.0\r\n   Cython: 0.29.2\r\n   pandas: 0.23.4", "choices": "(A)             unconverged = (np.sum((se == convergence_iter) + (se == 0))\n                           != n_samples)\n            if (not unconverged and (K > 0)) or (it == max_iter):\n                if verbose:\n                    print(\"Converged after %d iterations.\" % it)\n                break\n    else:\n        if verbose:\n            print(\"Did not converge\")\n\n    I = np.flatnonzero(E)\n    K = I.size  # Identify exemplars\n\n    if K > 0:\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)  # Identify clusters\n        # Refine the final set of exemplars and clusters and return results\n        for k in range(K):\n            ii = np.where(c == k)[0]\n            j = np.argmax(np.sum(S[ii[:, np.newaxis], ii], axis=0))\n            I[k] = ii[j]\n\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)\n        labels = I[c]\n        # Reduce labels to a sorted, gapless, list\n        cluster_centers_indices = np.unique(labels)\n        labels = np.searchsorted(cluster_centers_indices, labels)\n    else:\n        warnings.warn(\"Affinity propagation did not converge, this model \"\n                      \"will not have any cluster centers.\", ConvergenceWarning)\n        labels = np.array([-1] * n_samples)\n        cluster_centers_indices = []\n\n    if return_n_iter:\n        return cluster_centers_indices, labels, it + 1\n    else:\n        return cluster_centers_indices, labels\n\n\n###############################################################################\n\nclass AffinityPropagation(ClusterMixin, BaseEstimator):\n    \"\"\"Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n    damping : float, optional, default: 0.5\n        Damping factor (between 0.5 and 1) is the extent to\n        which the current value is maintained relative to\n        incoming values (weighted 1 - damping). This in order\n        to avoid numerical oscillations when updating these\n        values (messages).\n\n    max_iter : int, optional, default: 200\n        Maximum number of iterations.\n\n    convergence_iter : int, optional, default: 15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    copy : boolean, optional, default: True\n        Make a copy of input data.\n\n    preference : array-like, shape (n_samples,) or float, optional\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number\n        of exemplars, ie of clusters, is influenced by the input\n        preferences value. If the preferences are not passed as arguments,\n        they will be set to the median of the input similarities.\n\n    affinity : string, optional, default=``euclidean``\n        Which affinity to use. At the moment ``precomputed`` and\n        ``euclidean`` are supported. ``euclidean`` uses the\n        negative squared euclidean distance between points.\n\n    verbose : boolean, optional, default: False\n        Whether to be verbose.\n\n\n    Attributes\n    ----------\n    cluster_centers_indices_ : array, shape (n_clusters,)\n        Indices of cluster centers\n\n    cluster_centers_ : array, shape (n_clusters, n_features)\n        Cluster centers (if affinity != ``precomputed``).\n\n    labels_ : array, shape (n_samples,)\n        Labels of each point\n\n    affinity_matrix_ : array, shape (n_samples, n_samples)\n        Stores the affinity matrix used in ``fit``.\n\n    n_iter_ : int\n        Number of iterations taken to converge.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AffinityPropagation\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AffinityPropagation().fit(X)\n    >>> clustering\n    AffinityPropagation()\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    >>> clustering.predict([[0, 0], [4, 4]])\n    array([0, 1])\n    >>> clustering.cluster_centers_\n    array([[1, 2],\n           [4, 2]])\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\n    The algorithmic complexity of affinity propagation is quadratic\n    in the number of points.\n\n    When ``fit`` does not converge, ``cluster_centers_`` becomes an empty\n    array and all training samples will be labelled as ``-1``. In addition,\n    ``predict`` will then label every sample as ``-1``.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, ``fit`` will result in\n    a single cluster center and label ``0`` for every sample. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n    \"\"\"\n\n    def __init__(self, damping=.5, max_iter=200, convergence_iter=15,\n                 copy=True, preference=None, affinity='euclidean',\n                 verbose=False):\n\n        self.damping = damping\n        self.max_iter = max_iter\n        self.convergence_iter = convergence_iter\n        self.copy = copy\n        self.verbose = verbose\n        self.preference = preference\n        self.affinity = affinity\n\n    @property\n    def _pairwise(self):\n        return self.affinity == \"precomputed\"\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features), or \\\n            array-like, shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix\n            is provided, it will be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n        if self.affinity == \"precomputed\":\n            accept_sparse = False\n        else:\n            accept_sparse = 'csr'\n        X = check_array(X, accept_sparse=accept_sparse)\n        if self.affinity == \"precomputed\":\n            self.affinity_matrix_ = X\n        elif self.affinity == \"euclidean\":\n            self.affinity_matrix_ = -euclidean_distances(X, squared=True)\n        else:\n            raise ValueError(\"Affinity must be 'precomputed' or \"\n                             \"'euclidean'. Got %s instead\"\n                             % str(self.affinity))\n\n        self.cluster_centers_indices_, self.labels_, self.n_iter_ = \\\n            affinity_propagation(\n                self.affinity_matrix_, self.preference, max_iter=self.max_iter,\n                convergence_iter=self.convergence_iter, damping=self.damping,\n                copy=self.copy, verbose=self.verbose, return_n_iter=True)\n\n        if self.affinity != \"precomputed\":\n            self.cluster_centers_ = X[self.cluster_centers_indices_].copy()\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            New data to predict. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        labels : ndarray, shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        check_is_fitted(self)\n        if not hasattr(self, \"cluster_centers_\"):\n            raise ValueError(\"Predict method is not supported when \"\n                             \"affinity='precomputed'.\")\n\n        if self.cluster_centers_.shape[0] > 0:\n            return pairwise_distances_argmin(X, self.cluster_centers_)\n(B)         np.fill_diagonal(mask, 0)\n\n        return np.all(S[mask].flat == S[mask].flat[0])\n\n    return all_equal_preferences() and all_equal_similarities()\n\n\ndef affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,\n                         damping=0.5, copy=True, verbose=False,\n                         return_n_iter=False):\n    \"\"\"Perform Affinity Propagation Clustering of data\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n\n    S : array-like, shape (n_samples, n_samples)\n        Matrix of similarities between points\n\n    preference : array-like, shape (n_samples,) or float, optional\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number of\n        exemplars, i.e. of clusters, is influenced by the input preferences\n        value. If the preferences are not passed as arguments, they will be\n        set to the median of the input similarities (resulting in a moderate\n        number of clusters). For a smaller amount of clusters, this can be set\n        to the minimum value of the similarities.\n\n    convergence_iter : int, optional, default: 15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    max_iter : int, optional, default: 200\n        Maximum number of iterations\n\n    damping : float, optional, default: 0.5\n        Damping factor between 0.5 and 1.\n\n    copy : boolean, optional, default: True\n        If copy is False, the affinity matrix is modified inplace by the\n        algorithm, for memory efficiency\n\n    verbose : boolean, optional, default: False\n        The verbosity level\n\n    return_n_iter : bool, default False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n\n    cluster_centers_indices : array, shape (n_clusters,)\n        index of clusters centers\n\n    labels : array, shape (n_samples,)\n        cluster labels for each point\n\n    n_iter : int\n        number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\n    When the algorithm does not converge, it returns an empty array as\n    ``cluster_center_indices`` and ``-1`` as label for each training sample.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, a single cluster center\n    and label ``0`` for every sample will be returned. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n    \"\"\"\n    S = as_float_array(S, copy=copy)\n    n_samples = S.shape[0]\n\n    if S.shape[0] != S.shape[1]:\n        raise ValueError(\"S must be a square array (shape=%s)\" % repr(S.shape))\n\n    if preference is None:\n        preference = np.median(S)\n    if damping < 0.5 or damping >= 1:\n        raise ValueError('damping must be >= 0.5 and < 1')\n\n    preference = np.array(preference)\n\n    if (n_samples == 1 or\n            _equal_similarities_and_preferences(S, preference)):\n        # It makes no sense to run the algorithm in this case, so return 1 or\n        # n_samples clusters, depending on preferences\n        warnings.warn(\"All samples have mutually equal similarities. \"\n                      \"Returning arbitrary cluster center(s).\")\n        if preference.flat[0] >= S.flat[n_samples - 1]:\n            return ((np.arange(n_samples), np.arange(n_samples), 0)\n                    if return_n_iter\n                    else (np.arange(n_samples), np.arange(n_samples)))\n        else:\n            return ((np.array([0]), np.array([0] * n_samples), 0)\n                    if return_n_iter\n                    else (np.array([0]), np.array([0] * n_samples)))\n\n    random_state = np.random.RandomState(0)\n\n    # Place preference on the diagonal of S\n    S.flat[::(n_samples + 1)] = preference\n\n    A = np.zeros((n_samples, n_samples))\n    R = np.zeros((n_samples, n_samples))  # Initialize messages\n    # Intermediate results\n    tmp = np.zeros((n_samples, n_samples))\n\n    # Remove degeneracies\n    S += ((np.finfo(np.double).eps * S + np.finfo(np.double).tiny * 100) *\n          random_state.randn(n_samples, n_samples))\n\n    # Execute parallel affinity propagation updates\n    e = np.zeros((n_samples, convergence_iter))\n\n    ind = np.arange(n_samples)\n\n    for it in range(max_iter):\n        # tmp = A + S; compute responsibilities\n        np.add(A, S, tmp)\n        I = np.argmax(tmp, axis=1)\n        Y = tmp[ind, I]  # np.max(A + S, axis=1)\n        tmp[ind, I] = -np.inf\n        Y2 = np.max(tmp, axis=1)\n\n        # tmp = Rnew\n        np.subtract(S, Y[:, None], tmp)\n        tmp[ind, I] = S[ind, I] - Y2\n\n        # Damping\n        tmp *= 1 - damping\n        R *= damping\n        R += tmp\n\n        # tmp = Rp; compute availabilities\n        np.maximum(R, 0, tmp)\n        tmp.flat[::n_samples + 1] = R.flat[::n_samples + 1]\n\n        # tmp = -Anew\n        tmp -= np.sum(tmp, axis=0)\n        dA = np.diag(tmp).copy()\n        tmp.clip(0, np.inf, tmp)\n        tmp.flat[::n_samples + 1] = dA\n\n        # Damping\n        tmp *= 1 - damping\n        A *= damping\n        A -= tmp\n\n        # Check for convergence\n        E = (np.diag(A) + np.diag(R)) > 0\n        e[:, it % convergence_iter] = E\n        K = np.sum(E, axis=0)\n\n        if it >= convergence_iter:\n            se = np.sum(e, axis=1)\n            unconverged = (np.sum((se == convergence_iter) + (se == 0))\n                           != n_samples)\n            if (not unconverged and (K > 0)) or (it == max_iter):\n                if verbose:\n                    print(\"Converged after %d iterations.\" % it)\n                break\n    else:\n        if verbose:\n            print(\"Did not converge\")\n\n    I = np.flatnonzero(E)\n    K = I.size  # Identify exemplars\n\n    if K > 0:\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)  # Identify clusters\n        # Refine the final set of exemplars and clusters and return results\n        for k in range(K):\n            ii = np.where(c == k)[0]\n            j = np.argmax(np.sum(S[ii[:, np.newaxis], ii], axis=0))\n            I[k] = ii[j]\n\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)\n        labels = I[c]\n        # Reduce labels to a sorted, gapless, list\n        cluster_centers_indices = np.unique(labels)\n        labels = np.searchsorted(cluster_centers_indices, labels)\n    else:\n        warnings.warn(\"Affinity propagation did not converge, this model \"\n                      \"will not have any cluster centers.\", ConvergenceWarning)\n        labels = np.array([-1] * n_samples)\n        cluster_centers_indices = []\n\n    if return_n_iter:\n        return cluster_centers_indices, labels, it + 1\n    else:\n        return cluster_centers_indices, labels\n\n\n###############################################################################\n\nclass AffinityPropagation(ClusterMixin, BaseEstimator):\n    \"\"\"Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n    damping : float, optional, default: 0.5\n        Damping factor (between 0.5 and 1) is the extent to\n        which the current value is maintained relative to\n        incoming values (weighted 1 - damping). This in order\n        to avoid numerical oscillations when updating these\n        values (messages).\n(C)         cluster labels for each point\n\n    n_iter : int\n        number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\n    When the algorithm does not converge, it returns an empty array as\n    ``cluster_center_indices`` and ``-1`` as label for each training sample.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, a single cluster center\n    and label ``0`` for every sample will be returned. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n    \"\"\"\n    S = as_float_array(S, copy=copy)\n    n_samples = S.shape[0]\n\n    if S.shape[0] != S.shape[1]:\n        raise ValueError(\"S must be a square array (shape=%s)\" % repr(S.shape))\n\n    if preference is None:\n        preference = np.median(S)\n    if damping < 0.5 or damping >= 1:\n        raise ValueError('damping must be >= 0.5 and < 1')\n\n    preference = np.array(preference)\n\n    if (n_samples == 1 or\n            _equal_similarities_and_preferences(S, preference)):\n        # It makes no sense to run the algorithm in this case, so return 1 or\n        # n_samples clusters, depending on preferences\n        warnings.warn(\"All samples have mutually equal similarities. \"\n                      \"Returning arbitrary cluster center(s).\")\n        if preference.flat[0] >= S.flat[n_samples - 1]:\n            return ((np.arange(n_samples), np.arange(n_samples), 0)\n                    if return_n_iter\n                    else (np.arange(n_samples), np.arange(n_samples)))\n        else:\n            return ((np.array([0]), np.array([0] * n_samples), 0)\n                    if return_n_iter\n                    else (np.array([0]), np.array([0] * n_samples)))\n\n    random_state = np.random.RandomState(0)\n\n    # Place preference on the diagonal of S\n    S.flat[::(n_samples + 1)] = preference\n\n    A = np.zeros((n_samples, n_samples))\n    R = np.zeros((n_samples, n_samples))  # Initialize messages\n    # Intermediate results\n    tmp = np.zeros((n_samples, n_samples))\n\n    # Remove degeneracies\n    S += ((np.finfo(np.double).eps * S + np.finfo(np.double).tiny * 100) *\n          random_state.randn(n_samples, n_samples))\n\n    # Execute parallel affinity propagation updates\n    e = np.zeros((n_samples, convergence_iter))\n\n    ind = np.arange(n_samples)\n\n    for it in range(max_iter):\n        # tmp = A + S; compute responsibilities\n        np.add(A, S, tmp)\n        I = np.argmax(tmp, axis=1)\n        Y = tmp[ind, I]  # np.max(A + S, axis=1)\n        tmp[ind, I] = -np.inf\n        Y2 = np.max(tmp, axis=1)\n\n        # tmp = Rnew\n        np.subtract(S, Y[:, None], tmp)\n        tmp[ind, I] = S[ind, I] - Y2\n\n        # Damping\n        tmp *= 1 - damping\n        R *= damping\n        R += tmp\n\n        # tmp = Rp; compute availabilities\n        np.maximum(R, 0, tmp)\n        tmp.flat[::n_samples + 1] = R.flat[::n_samples + 1]\n\n        # tmp = -Anew\n        tmp -= np.sum(tmp, axis=0)\n        dA = np.diag(tmp).copy()\n        tmp.clip(0, np.inf, tmp)\n        tmp.flat[::n_samples + 1] = dA\n\n        # Damping\n        tmp *= 1 - damping\n        A *= damping\n        A -= tmp\n\n        # Check for convergence\n        E = (np.diag(A) + np.diag(R)) > 0\n        e[:, it % convergence_iter] = E\n        K = np.sum(E, axis=0)\n\n        if it >= convergence_iter:\n            se = np.sum(e, axis=1)\n            unconverged = (np.sum((se == convergence_iter) + (se == 0))\n                           != n_samples)\n            if (not unconverged and (K > 0)) or (it == max_iter):\n                if verbose:\n                    print(\"Converged after %d iterations.\" % it)\n                break\n    else:\n        if verbose:\n            print(\"Did not converge\")\n\n    I = np.flatnonzero(E)\n    K = I.size  # Identify exemplars\n\n    if K > 0:\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)  # Identify clusters\n        # Refine the final set of exemplars and clusters and return results\n        for k in range(K):\n            ii = np.where(c == k)[0]\n            j = np.argmax(np.sum(S[ii[:, np.newaxis], ii], axis=0))\n            I[k] = ii[j]\n\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)\n        labels = I[c]\n        # Reduce labels to a sorted, gapless, list\n        cluster_centers_indices = np.unique(labels)\n        labels = np.searchsorted(cluster_centers_indices, labels)\n    else:\n        warnings.warn(\"Affinity propagation did not converge, this model \"\n                      \"will not have any cluster centers.\", ConvergenceWarning)\n        labels = np.array([-1] * n_samples)\n        cluster_centers_indices = []\n\n    if return_n_iter:\n        return cluster_centers_indices, labels, it + 1\n    else:\n        return cluster_centers_indices, labels\n\n\n###############################################################################\n\nclass AffinityPropagation(ClusterMixin, BaseEstimator):\n    \"\"\"Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n    damping : float, optional, default: 0.5\n        Damping factor (between 0.5 and 1) is the extent to\n        which the current value is maintained relative to\n        incoming values (weighted 1 - damping). This in order\n        to avoid numerical oscillations when updating these\n        values (messages).\n\n    max_iter : int, optional, default: 200\n        Maximum number of iterations.\n\n    convergence_iter : int, optional, default: 15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    copy : boolean, optional, default: True\n        Make a copy of input data.\n\n    preference : array-like, shape (n_samples,) or float, optional\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number\n        of exemplars, ie of clusters, is influenced by the input\n        preferences value. If the preferences are not passed as arguments,\n        they will be set to the median of the input similarities.\n\n    affinity : string, optional, default=``euclidean``\n        Which affinity to use. At the moment ``precomputed`` and\n        ``euclidean`` are supported. ``euclidean`` uses the\n        negative squared euclidean distance between points.\n\n    verbose : boolean, optional, default: False\n        Whether to be verbose.\n\n\n    Attributes\n    ----------\n    cluster_centers_indices_ : array, shape (n_clusters,)\n        Indices of cluster centers\n\n    cluster_centers_ : array, shape (n_clusters, n_features)\n        Cluster centers (if affinity != ``precomputed``).\n\n    labels_ : array, shape (n_samples,)\n        Labels of each point\n\n    affinity_matrix_ : array, shape (n_samples, n_samples)\n        Stores the affinity matrix used in ``fit``.\n\n    n_iter_ : int\n        Number of iterations taken to converge.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AffinityPropagation\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AffinityPropagation().fit(X)\n    >>> clustering\n    AffinityPropagation()\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    >>> clustering.predict([[0, 0], [4, 4]])\n(D)     # Place preference on the diagonal of S\n    S.flat[::(n_samples + 1)] = preference\n\n    A = np.zeros((n_samples, n_samples))\n    R = np.zeros((n_samples, n_samples))  # Initialize messages\n    # Intermediate results\n    tmp = np.zeros((n_samples, n_samples))\n\n    # Remove degeneracies\n    S += ((np.finfo(np.double).eps * S + np.finfo(np.double).tiny * 100) *\n          random_state.randn(n_samples, n_samples))\n\n    # Execute parallel affinity propagation updates\n    e = np.zeros((n_samples, convergence_iter))\n\n    ind = np.arange(n_samples)\n\n    for it in range(max_iter):\n        # tmp = A + S; compute responsibilities\n        np.add(A, S, tmp)\n        I = np.argmax(tmp, axis=1)\n        Y = tmp[ind, I]  # np.max(A + S, axis=1)\n        tmp[ind, I] = -np.inf\n        Y2 = np.max(tmp, axis=1)\n\n        # tmp = Rnew\n        np.subtract(S, Y[:, None], tmp)\n        tmp[ind, I] = S[ind, I] - Y2\n\n        # Damping\n        tmp *= 1 - damping\n        R *= damping\n        R += tmp\n\n        # tmp = Rp; compute availabilities\n        np.maximum(R, 0, tmp)\n        tmp.flat[::n_samples + 1] = R.flat[::n_samples + 1]\n\n        # tmp = -Anew\n        tmp -= np.sum(tmp, axis=0)\n        dA = np.diag(tmp).copy()\n        tmp.clip(0, np.inf, tmp)\n        tmp.flat[::n_samples + 1] = dA\n\n        # Damping\n        tmp *= 1 - damping\n        A *= damping\n        A -= tmp\n\n        # Check for convergence\n        E = (np.diag(A) + np.diag(R)) > 0\n        e[:, it % convergence_iter] = E\n        K = np.sum(E, axis=0)\n\n        if it >= convergence_iter:\n            se = np.sum(e, axis=1)\n            unconverged = (np.sum((se == convergence_iter) + (se == 0))\n                           != n_samples)\n            if (not unconverged and (K > 0)) or (it == max_iter):\n                if verbose:\n                    print(\"Converged after %d iterations.\" % it)\n                break\n    else:\n        if verbose:\n            print(\"Did not converge\")\n\n    I = np.flatnonzero(E)\n    K = I.size  # Identify exemplars\n\n    if K > 0:\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)  # Identify clusters\n        # Refine the final set of exemplars and clusters and return results\n        for k in range(K):\n            ii = np.where(c == k)[0]\n            j = np.argmax(np.sum(S[ii[:, np.newaxis], ii], axis=0))\n            I[k] = ii[j]\n\n        c = np.argmax(S[:, I], axis=1)\n        c[I] = np.arange(K)\n        labels = I[c]\n        # Reduce labels to a sorted, gapless, list\n        cluster_centers_indices = np.unique(labels)\n        labels = np.searchsorted(cluster_centers_indices, labels)\n    else:\n        warnings.warn(\"Affinity propagation did not converge, this model \"\n                      \"will not have any cluster centers.\", ConvergenceWarning)\n        labels = np.array([-1] * n_samples)\n        cluster_centers_indices = []\n\n    if return_n_iter:\n        return cluster_centers_indices, labels, it + 1\n    else:\n        return cluster_centers_indices, labels\n\n\n###############################################################################\n\nclass AffinityPropagation(ClusterMixin, BaseEstimator):\n    \"\"\"Perform Affinity Propagation Clustering of data.\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n    damping : float, optional, default: 0.5\n        Damping factor (between 0.5 and 1) is the extent to\n        which the current value is maintained relative to\n        incoming values (weighted 1 - damping). This in order\n        to avoid numerical oscillations when updating these\n        values (messages).\n\n    max_iter : int, optional, default: 200\n        Maximum number of iterations.\n\n    convergence_iter : int, optional, default: 15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    copy : boolean, optional, default: True\n        Make a copy of input data.\n\n    preference : array-like, shape (n_samples,) or float, optional\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number\n        of exemplars, ie of clusters, is influenced by the input\n        preferences value. If the preferences are not passed as arguments,\n        they will be set to the median of the input similarities.\n\n    affinity : string, optional, default=``euclidean``\n        Which affinity to use. At the moment ``precomputed`` and\n        ``euclidean`` are supported. ``euclidean`` uses the\n        negative squared euclidean distance between points.\n\n    verbose : boolean, optional, default: False\n        Whether to be verbose.\n\n\n    Attributes\n    ----------\n    cluster_centers_indices_ : array, shape (n_clusters,)\n        Indices of cluster centers\n\n    cluster_centers_ : array, shape (n_clusters, n_features)\n        Cluster centers (if affinity != ``precomputed``).\n\n    labels_ : array, shape (n_samples,)\n        Labels of each point\n\n    affinity_matrix_ : array, shape (n_samples, n_samples)\n        Stores the affinity matrix used in ``fit``.\n\n    n_iter_ : int\n        Number of iterations taken to converge.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import AffinityPropagation\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n    ...               [4, 2], [4, 4], [4, 0]])\n    >>> clustering = AffinityPropagation().fit(X)\n    >>> clustering\n    AffinityPropagation()\n    >>> clustering.labels_\n    array([0, 0, 0, 1, 1, 1])\n    >>> clustering.predict([[0, 0], [4, 4]])\n    array([0, 1])\n    >>> clustering.cluster_centers_\n    array([[1, 2],\n           [4, 2]])\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py\n    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.\n\n    The algorithmic complexity of affinity propagation is quadratic\n    in the number of points.\n\n    When ``fit`` does not converge, ``cluster_centers_`` becomes an empty\n    array and all training samples will be labelled as ``-1``. In addition,\n    ``predict`` will then label every sample as ``-1``.\n\n    When all training samples have equal similarities and equal preferences,\n    the assignment of cluster centers and labels depends on the preference.\n    If the preference is smaller than the similarities, ``fit`` will result in\n    a single cluster center and label ``0`` for every sample. Otherwise, every\n    training sample becomes its own cluster center and is assigned a unique\n    label.\n\n    References\n    ----------\n\n    Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages\n    Between Data Points\", Science Feb. 2007\n    \"\"\"\n\n    def __init__(self, damping=.5, max_iter=200, convergence_iter=15,\n                 copy=True, preference=None, affinity='euclidean',\n                 verbose=False):\n\n        self.damping = damping\n        self.max_iter = max_iter\n        self.convergence_iter = convergence_iter\n        self.copy = copy\n        self.verbose = verbose\n        self.preference = preference\n        self.affinity = affinity\n\n    @property\n    def _pairwise(self):\n        return self.affinity == \"precomputed\"\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the clustering from features, or affinity matrix.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features), or \\\n            array-like, shape (n_samples, n_samples)\n            Training instances to cluster, or similarities / affinities between\n            instances if ``affinity='precomputed'``. If a sparse feature matrix", "answer": "A"}
{"uuid": "58383442-a71a-484f-aa75-4b93edbfdb3a", "issue_statement": "regression in input validation of clustering metrics\n```python\r\nfrom sklearn.metrics.cluster import mutual_info_score\r\nimport numpy as np\r\n\r\nx = np.random.choice(['a', 'b'], size=20).astype(object)\r\nmutual_info_score(x, x)\r\n```\r\nValueError: could not convert string to float: 'b'\r\n\r\nwhile\r\n```python\r\nx = np.random.choice(['a', 'b'], size=20)\r\nmutual_info_score(x, x)\r\n```\r\nworks with a warning?\r\n\r\nthis worked in 0.21.1 without a warning (as I think it should)\r\n\r\n\r\nEdit by @ogrisel: I removed the `.astype(object)` in the second code snippet.", "choices": "(A)         The predicted labels.\n    \"\"\"\n    labels_true = check_array(\n        labels_true, ensure_2d=False, ensure_min_samples=0\n    )\n    labels_pred = check_array(\n        labels_pred, ensure_2d=False, ensure_min_samples=0\n    )\n\n    # input checks\n(B)     labels_pred = check_array(\n        labels_pred, ensure_2d=False, ensure_min_samples=0\n    )\n\n    # input checks\n    if labels_true.ndim != 1:\n        raise ValueError(\n            \"labels_true must be 1D: shape is %r\" % (labels_true.shape,))\n    if labels_pred.ndim != 1:\n        raise ValueError(\n(C)         The true labels.\n\n    labels_pred : array-like of shape (n_samples,)\n        The predicted labels.\n    \"\"\"\n    labels_true = check_array(\n        labels_true, ensure_2d=False, ensure_min_samples=0\n    )\n    labels_pred = check_array(\n        labels_pred, ensure_2d=False, ensure_min_samples=0\n(D)         labels_true, ensure_2d=False, ensure_min_samples=0\n    )\n    labels_pred = check_array(\n        labels_pred, ensure_2d=False, ensure_min_samples=0\n    )\n\n    # input checks\n    if labels_true.ndim != 1:\n        raise ValueError(\n            \"labels_true must be 1D: shape is %r\" % (labels_true.shape,))", "answer": "A"}
{"uuid": "d815c3df-036f-4b89-9cd1-f73bab9c7a96", "issue_statement": "CalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```", "choices": "(A) \n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n(B)         self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n(C)             The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n(D)         X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)", "answer": "B"}
{"uuid": "ab80d0de-5783-45e7-9d4e-e0a72031d213", "issue_statement": "ColumnTransformer with pandas output can't handle transformers with no features\n### Describe the bug\r\n\r\nHi,\r\n\r\nColumnTransformer doesn't deal well with transformers that apply to 0 features (categorical_features in the example below) when using \"pandas\" as output. It seems steps with 0 features are not fitted, hence don't appear in `self._iter(fitted=True)` (_column_transformer.py l.856) and hence break the input to the `_add_prefix_for_feature_names_out` function (l.859).\r\n\r\n\r\n### Steps/Code to Reproduce\r\n\r\nHere is some code to reproduce the error. If you remove .set_output(transform=\"pandas\") on the line before last, all works fine. If you remove the (\"categorical\", ...) step, it works fine too.\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom lightgbm import LGBMClassifier\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.impute import SimpleImputer\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.preprocessing import RobustScaler\r\n\r\nX = pd.DataFrame(data=[[1.0, 2.0, 3.0, 4.0], [4, 2, 2, 5]],\r\n                 columns=[\"a\", \"b\", \"c\", \"d\"])\r\ny = np.array([0, 1])\r\ncategorical_features = []\r\nnumerical_features = [\"a\", \"b\", \"c\"]\r\nmodel_preprocessing = (\"preprocessing\",\r\n                       ColumnTransformer([\r\n                           ('categorical', 'passthrough', categorical_features),\r\n                           ('numerical', Pipeline([(\"scaler\", RobustScaler()),\r\n                                                   (\"imputer\", SimpleImputer(strategy=\"median\"))\r\n                                                   ]), numerical_features),\r\n                       ], remainder='drop'))\r\npipeline = Pipeline([model_preprocessing, (\"classifier\", LGBMClassifier())]).set_output(transform=\"pandas\")\r\npipeline.fit(X, y)\r\n```\r\n\r\n### Expected Results\r\n\r\nThe step with no features should be ignored.\r\n\r\n### Actual Results\r\n\r\nHere is the error message:\r\n```pytb\r\nTraceback (most recent call last):\r\n  File \"/home/philippe/workspace/script.py\", line 22, in <module>\r\n    pipeline.fit(X, y)\r\n  File \"/home/philippe/.anaconda3/envs/deleteme/lib/python3.9/site-packages/sklearn/pipeline.py\", line 402, in fit\r\n    Xt = self._fit(X, y, **fit_params_steps)\r\n  File \"/home/philippe/.anaconda3/envs/deleteme/lib/python3.9/site-packages/sklearn/pipeline.py\", line 360, in _fit\r\n    X, fitted_transformer = fit_transform_one_cached(\r\n  File \"/home/philippe/.anaconda3/envs/deleteme/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"/home/philippe/.anaconda3/envs/deleteme/lib/python3.9/site-packages/sklearn/pipeline.py\", line 894, in _fit_transform_one\r\n    res = transformer.fit_transform(X, y, **fit_params)\r\n  File \"/home/philippe/.anaconda3/envs/deleteme/lib/python3.9/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped\r\n    data_to_wrap = f(self, X, *args, **kwargs)\r\n  File \"/home/philippe/.anaconda3/envs/deleteme/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 750, in fit_transform\r\n    return self._hstack(list(Xs))\r\n  File \"/home/philippe/.anaconda3/envs/deleteme/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 862, in _hstack\r\n    output.columns = names_out\r\n  File \"/home/philippe/.anaconda3/envs/deleteme/lib/python3.9/site-packages/pandas/core/generic.py\", line 5596, in __setattr__\r\n    return object.__setattr__(self, name, value)\r\n  File \"pandas/_libs/properties.pyx\", line 70, in pandas._libs.properties.AxisProperty.__set__\r\n  File \"/home/philippe/.anaconda3/envs/deleteme/lib/python3.9/site-packages/pandas/core/generic.py\", line 769, in _set_axis\r\n    self._mgr.set_axis(axis, labels)\r\n  File \"/home/philippe/.anaconda3/envs/deleteme/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 214, in set_axis\r\n    self._validate_set_axis(axis, new_labels)\r\n  File \"/home/philippe/.anaconda3/envs/deleteme/lib/python3.9/site-packages/pandas/core/internals/base.py\", line 69, in _validate_set_axis\r\n    raise ValueError(\r\nValueError: Length mismatch: Expected axis has 3 elements, new values have 0 elements\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```", "choices": "(A)                     list(zip(transformer_names, feature_names_outs))\n                )\n                output.columns = names_out\n                return output\n\n            return np.hstack(Xs)\n\n    def _sk_visual_block_(self):\n        if isinstance(self.remainder, str) and self.remainder == \"drop\":\n(B)                 transformer_names = [\n                    t[0] for t in self._iter(fitted=True, replace_strings=True)\n                ]\n                feature_names_outs = [X.columns for X in Xs]\n                names_out = self._add_prefix_for_feature_names_out(\n                    list(zip(transformer_names, feature_names_outs))\n                )\n                output.columns = names_out\n                return output\n(C)                 feature_names_outs = [X.columns for X in Xs]\n                names_out = self._add_prefix_for_feature_names_out(\n                    list(zip(transformer_names, feature_names_outs))\n                )\n                output.columns = names_out\n                return output\n\n            return np.hstack(Xs)\n\n(D)                 if not self.verbose_feature_names_out:\n                    return output\n\n                transformer_names = [\n                    t[0] for t in self._iter(fitted=True, replace_strings=True)\n                ]\n                feature_names_outs = [X.columns for X in Xs]\n                names_out = self._add_prefix_for_feature_names_out(\n                    list(zip(transformer_names, feature_names_outs))", "answer": "B"}
{"uuid": "87f1f81e-e940-4e06-8f46-cdcb8728c94d", "issue_statement": "Support nullable pandas dtypes in `unique_labels`\n### Describe the workflow you want to enable\n\nI would like to be able to pass the nullable pandas dtypes (\"Int64\", \"Float64\", \"boolean\") into sklearn's `unique_labels` function. Because the dtypes become `object` dtype when converted to numpy arrays we get `ValueError: Mix type of y not allowed, got types {'binary', 'unknown'}`:\r\n\r\nRepro with sklearn 1.2.1\r\n```py \r\n    import pandas as pd\r\n    import pytest\r\n    from sklearn.utils.multiclass import unique_labels\r\n    \r\n    for dtype in [\"Int64\", \"Float64\", \"boolean\"]:\r\n        y_true = pd.Series([1, 0, 0, 1, 0, 1, 1, 0, 1], dtype=dtype)\r\n        y_predicted = pd.Series([0, 0, 1, 1, 0, 1, 1, 1, 1], dtype=\"int64\")\r\n\r\n        with pytest.raises(ValueError, match=\"Mix type of y not allowed, got types\"):\r\n            unique_labels(y_true, y_predicted)\r\n```\n\n### Describe your proposed solution\n\nWe should get the same behavior as when `int64`, `float64`, and `bool` dtypes are used, which is no error:  \r\n\r\n```python\r\n    import pandas as pd\r\n    from sklearn.utils.multiclass import unique_labels\r\n    \r\n    for dtype in [\"int64\", \"float64\", \"bool\"]:\r\n        y_true = pd.Series([1, 0, 0, 1, 0, 1, 1, 0, 1], dtype=dtype)\r\n        y_predicted = pd.Series([0, 0, 1, 1, 0, 1, 1, 1, 1], dtype=\"int64\")\r\n\r\n        unique_labels(y_true, y_predicted)\r\n```\n\n### Describe alternatives you've considered, if relevant\n\nOur current workaround is to convert the data to numpy arrays with the corresponding dtype that works prior to passing it into `unique_labels`.\n\n### Additional context\n\n_No response_", "choices": "(A)           dimensions are of size > 1.\n        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n          of two dimensions with at least two columns, and at most 2 unique\n          values.\n        * 'unknown': `y` is array-like but none of the above, such as a 3d\n          array, sequence of sequences, or an array of non-sequence objects.\n\n    Examples\n    --------\n    >>> from sklearn.utils.multiclass import type_of_target\n    >>> import numpy as np\n    >>> type_of_target([0.1, 0.6])\n    'continuous'\n    >>> type_of_target([1, -1, -1, 1])\n    'binary'\n    >>> type_of_target(['a', 'b', 'a'])\n    'binary'\n    >>> type_of_target([1.0, 2.0])\n    'binary'\n    >>> type_of_target([1, 0, 2])\n    'multiclass'\n    >>> type_of_target([1.0, 0.0, 3.0])\n    'multiclass'\n    >>> type_of_target(['a', 'b', 'c'])\n    'multiclass'\n    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n    'multiclass-multioutput'\n    >>> type_of_target([[1, 2]])\n    'multilabel-indicator'\n    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n    'continuous-multioutput'\n    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n    'multilabel-indicator'\n    \"\"\"\n    xp, is_array_api = get_namespace(y)\n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n\n    # The old sequence of sequences format\n    try:\n        if (\n            not hasattr(y[0], \"__array__\")\n            and isinstance(y[0], Sequence)\n            and not isinstance(y[0], str)\n        ):\n            raise ValueError(\n                \"You appear to be using a legacy multi-label data\"\n                \" representation. Sequence of sequences are no\"\n                \" longer supported; use a binary array or sparse\"\n                \" matrix instead - the MultiLabelBinarizer\"\n                \" transformer can convert to this format.\"\n            )\n    except IndexError:\n        pass\n\n    # Invalid inputs\n    if y.ndim not in (1, 2):\n        # Number of dimension greater than 2: [[[1, 2]]]\n        return \"unknown\"\n    if not min(y.shape):\n        # Empty ndarray: []/[[]]\n        if y.ndim == 1:\n            # 1-D empty array: []\n            return \"binary\"  # []\n        # 2-D empty array: [[]]\n        return \"unknown\"\n    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n        # [obj_1] and not [\"label_1\"]\n        return \"unknown\"\n\n    # Check if multioutput\n    if y.ndim == 2 and y.shape[1] > 1:\n        suffix = \"-multioutput\"  # [[1, 2], [1, 2]]\n    else:\n        suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n\n    # Check float and contains non-integer float values\n    if y.dtype.kind == \"f\":\n        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n        data = y.data if issparse(y) else y\n        if xp.any(data != data.astype(int)):\n            _assert_all_finite(data, input_name=input_name)\n            return \"continuous\" + suffix\n\n    # Check multiclass\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n        return \"multiclass\" + suffix\n    else:\n        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n\n\ndef _check_partial_fit_first_call(clf, classes=None):\n    \"\"\"Private helper function for factorizing common classes param logic.\n\n    Estimators that implement the ``partial_fit`` API need to be provided with\n    the list of possible classes at the first call to partial_fit.\n\n    Subsequent calls to partial_fit should check that ``classes`` is still\n    consistent with a previous value of ``clf.classes_`` when provided.\n\n    This function returns True if it detects that this was the first call to\n    ``partial_fit`` on ``clf``. In that case the ``classes_`` attribute is also\n    set on ``clf``.\n\n    \"\"\"\n    if getattr(clf, \"classes_\", None) is None and classes is None:\n        raise ValueError(\"classes must be passed on the first call to partial_fit.\")\n\n    elif classes is not None:\n        if getattr(clf, \"classes_\", None) is not None:\n            if not np.array_equal(clf.classes_, unique_labels(classes)):\n                raise ValueError(\n                    \"`classes=%r` is not the same as on last call \"\n                    \"to partial_fit, was: %r\" % (classes, clf.classes_)\n                )\n\n        else:\n            # This is the first call to partial_fit\n            clf.classes_ = unique_labels(classes)\n            return True\n\n    # classes is None and clf.classes_ has already previously been set:\n    # nothing to do\n    return False\n\n\ndef class_distribution(y, sample_weight=None):\n    \"\"\"Compute class priors from multioutput-multiclass target data.\n\n    Parameters\n    ----------\n    y : {array-like, sparse matrix} of size (n_samples, n_outputs)\n        The labels for each example.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    Returns\n    -------\n    classes : list of size n_outputs of ndarray of size (n_classes,)\n        List of classes for each column.\n\n    n_classes : list of int of size n_outputs\n        Number of classes in each column.\n\n    class_prior : list of size n_outputs of ndarray of size (n_classes,)\n        Class distribution of each column.\n    \"\"\"\n    classes = []\n    n_classes = []\n    class_prior = []\n\n    n_samples, n_outputs = y.shape\n    if sample_weight is not None:\n(B) \n    if is_array_api:\n        # array_api does not allow for mixed dtypes\n        unique_ys = xp.concat([_unique_labels(y) for y in ys])\n        return xp.unique_values(unique_ys)\n\n    ys_labels = set(chain.from_iterable((i for i in _unique_labels(y)) for y in ys))\n    # Check that we don't mix string type with number type\n    if len(set(isinstance(label, str) for label in ys_labels)) > 1:\n        raise ValueError(\"Mix of label input types (string and number)\")\n\n    return xp.asarray(sorted(ys_labels))\n\n\ndef _is_integral_float(y):\n    return y.dtype.kind == \"f\" and np.all(y.astype(int) == y)\n\n\ndef is_multilabel(y):\n    \"\"\"Check if ``y`` is in a multilabel format.\n\n    Parameters\n    ----------\n    y : ndarray of shape (n_samples,)\n        Target values.\n\n    Returns\n    -------\n    out : bool\n        Return ``True``, if ``y`` is in a multilabel format, else ```False``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.multiclass import is_multilabel\n    >>> is_multilabel([0, 1, 0, 1])\n    False\n    >>> is_multilabel([[1], [0, 2], []])\n    False\n    >>> is_multilabel(np.array([[1, 0], [0, 0]]))\n    True\n    >>> is_multilabel(np.array([[1], [0], [0]]))\n    False\n    >>> is_multilabel(np.array([[1, 0, 0]]))\n    True\n    \"\"\"\n    xp, is_array_api = get_namespace(y)\n    if hasattr(y, \"__array__\") or isinstance(y, Sequence) or is_array_api:\n        # DeprecationWarning will be replaced by ValueError, see NEP 34\n        # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n\n    if not (hasattr(y, \"shape\") and y.ndim == 2 and y.shape[1] > 1):\n        return False\n\n    if issparse(y):\n        if isinstance(y, (dok_matrix, lil_matrix)):\n            y = y.tocsr()\n        labels = xp.unique_values(y.data)\n        return (\n            len(y.data) == 0\n            or (labels.size == 1 or (labels.size == 2) and (0 in labels))\n            and (y.dtype.kind in \"biu\" or _is_integral_float(labels))  # bool, int, uint\n        )\n    else:\n        labels = xp.unique_values(y)\n\n        return len(labels) < 3 and (\n            y.dtype.kind in \"biu\" or _is_integral_float(labels)  # bool, int, uint\n        )\n\n\ndef check_classification_targets(y):\n    \"\"\"Ensure that target y is of a non-regression type.\n\n    Only the following target types (as defined in type_of_target) are allowed:\n        'binary', 'multiclass', 'multiclass-multioutput',\n        'multilabel-indicator', 'multilabel-sequences'\n\n    Parameters\n    ----------\n    y : array-like\n        Target values.\n    \"\"\"\n    y_type = type_of_target(y, input_name=\"y\")\n    if y_type not in [\n        \"binary\",\n        \"multiclass\",\n        \"multiclass-multioutput\",\n        \"multilabel-indicator\",\n        \"multilabel-sequences\",\n    ]:\n        raise ValueError(\"Unknown label type: %r\" % y_type)\n\n\ndef type_of_target(y, input_name=\"\"):\n    \"\"\"Determine the type of data indicated by the target.\n\n    Note that this type is the most specific type that can be inferred.\n    For example:\n\n        * ``binary`` is more specific but compatible with ``multiclass``.\n        * ``multiclass`` of integers is more specific but compatible with\n          ``continuous``.\n        * ``multilabel-indicator`` is more specific but compatible with\n          ``multiclass-multioutput``.\n\n    Parameters\n    ----------\n    y : {array-like, sparse matrix}\n        Target values. If a sparse matrix, `y` is expected to be a\n        CSR/CSC matrix.\n\n    input_name : str, default=\"\"\n        The data name used to construct the error message.\n\n        .. versionadded:: 1.1.0\n\n    Returns\n    -------\n    target_type : str\n        One of:\n\n        * 'continuous': `y` is an array-like of floats that are not all\n          integers, and is 1d or a column vector.\n        * 'continuous-multioutput': `y` is a 2d array of floats that are\n          not all integers, and both dimensions are of size > 1.\n        * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n          vector.\n        * 'multiclass': `y` contains more than two discrete values, is not a\n          sequence of sequences, and is 1d or a column vector.\n        * 'multiclass-multioutput': `y` is a 2d array that contains more\n          than two discrete values, is not a sequence of sequences, and both\n          dimensions are of size > 1.\n        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n          of two dimensions with at least two columns, and at most 2 unique\n          values.\n        * 'unknown': `y` is array-like but none of the above, such as a 3d\n          array, sequence of sequences, or an array of non-sequence objects.\n\n    Examples\n    --------\n    >>> from sklearn.utils.multiclass import type_of_target\n    >>> import numpy as np\n    >>> type_of_target([0.1, 0.6])\n    'continuous'\n    >>> type_of_target([1, -1, -1, 1])\n    'binary'\n    >>> type_of_target(['a', 'b', 'a'])\n    'binary'\n    >>> type_of_target([1.0, 2.0])\n    'binary'\n    >>> type_of_target([1, 0, 2])\n    'multiclass'\n    >>> type_of_target([1.0, 0.0, 3.0])\n    'multiclass'\n    >>> type_of_target(['a', 'b', 'c'])\n    'multiclass'\n    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n    'multiclass-multioutput'\n    >>> type_of_target([[1, 2]])\n    'multilabel-indicator'\n    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n    'continuous-multioutput'\n    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n    'multilabel-indicator'\n    \"\"\"\n    xp, is_array_api = get_namespace(y)\n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n(C)         \"multiclass\",\n        \"multiclass-multioutput\",\n        \"multilabel-indicator\",\n        \"multilabel-sequences\",\n    ]:\n        raise ValueError(\"Unknown label type: %r\" % y_type)\n\n\ndef type_of_target(y, input_name=\"\"):\n    \"\"\"Determine the type of data indicated by the target.\n\n    Note that this type is the most specific type that can be inferred.\n    For example:\n\n        * ``binary`` is more specific but compatible with ``multiclass``.\n        * ``multiclass`` of integers is more specific but compatible with\n          ``continuous``.\n        * ``multilabel-indicator`` is more specific but compatible with\n          ``multiclass-multioutput``.\n\n    Parameters\n    ----------\n    y : {array-like, sparse matrix}\n        Target values. If a sparse matrix, `y` is expected to be a\n        CSR/CSC matrix.\n\n    input_name : str, default=\"\"\n        The data name used to construct the error message.\n\n        .. versionadded:: 1.1.0\n\n    Returns\n    -------\n    target_type : str\n        One of:\n\n        * 'continuous': `y` is an array-like of floats that are not all\n          integers, and is 1d or a column vector.\n        * 'continuous-multioutput': `y` is a 2d array of floats that are\n          not all integers, and both dimensions are of size > 1.\n        * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n          vector.\n        * 'multiclass': `y` contains more than two discrete values, is not a\n          sequence of sequences, and is 1d or a column vector.\n        * 'multiclass-multioutput': `y` is a 2d array that contains more\n          than two discrete values, is not a sequence of sequences, and both\n          dimensions are of size > 1.\n        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n          of two dimensions with at least two columns, and at most 2 unique\n          values.\n        * 'unknown': `y` is array-like but none of the above, such as a 3d\n          array, sequence of sequences, or an array of non-sequence objects.\n\n    Examples\n    --------\n    >>> from sklearn.utils.multiclass import type_of_target\n    >>> import numpy as np\n    >>> type_of_target([0.1, 0.6])\n    'continuous'\n    >>> type_of_target([1, -1, -1, 1])\n    'binary'\n    >>> type_of_target(['a', 'b', 'a'])\n    'binary'\n    >>> type_of_target([1.0, 2.0])\n    'binary'\n    >>> type_of_target([1, 0, 2])\n    'multiclass'\n    >>> type_of_target([1.0, 0.0, 3.0])\n    'multiclass'\n    >>> type_of_target(['a', 'b', 'c'])\n    'multiclass'\n    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n    'multiclass-multioutput'\n    >>> type_of_target([[1, 2]])\n    'multilabel-indicator'\n    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n    'continuous-multioutput'\n    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n    'multilabel-indicator'\n    \"\"\"\n    xp, is_array_api = get_namespace(y)\n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n\n    # The old sequence of sequences format\n    try:\n        if (\n            not hasattr(y[0], \"__array__\")\n            and isinstance(y[0], Sequence)\n            and not isinstance(y[0], str)\n        ):\n            raise ValueError(\n                \"You appear to be using a legacy multi-label data\"\n                \" representation. Sequence of sequences are no\"\n                \" longer supported; use a binary array or sparse\"\n                \" matrix instead - the MultiLabelBinarizer\"\n                \" transformer can convert to this format.\"\n            )\n    except IndexError:\n        pass\n\n    # Invalid inputs\n    if y.ndim not in (1, 2):\n        # Number of dimension greater than 2: [[[1, 2]]]\n        return \"unknown\"\n    if not min(y.shape):\n        # Empty ndarray: []/[[]]\n        if y.ndim == 1:\n            # 1-D empty array: []\n            return \"binary\"  # []\n        # 2-D empty array: [[]]\n        return \"unknown\"\n    if not issparse(y) and y.dtype == object and not isinstance(y.flat[0], str):\n        # [obj_1] and not [\"label_1\"]\n        return \"unknown\"\n\n    # Check if multioutput\n    if y.ndim == 2 and y.shape[1] > 1:\n        suffix = \"-multioutput\"  # [[1, 2], [1, 2]]\n    else:\n        suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n\n    # Check float and contains non-integer float values\n    if y.dtype.kind == \"f\":\n        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n        data = y.data if issparse(y) else y\n        if xp.any(data != data.astype(int)):\n            _assert_all_finite(data, input_name=input_name)\n            return \"continuous\" + suffix\n\n    # Check multiclass\n    first_row = y[0] if not issparse(y) else y.getrow(0).data\n    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):\n        # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n        return \"multiclass\" + suffix\n    else:\n        return \"binary\"  # [1, 2] or [[\"a\"], [\"b\"]]\n\n\ndef _check_partial_fit_first_call(clf, classes=None):\n    \"\"\"Private helper function for factorizing common classes param logic.\n\n    Estimators that implement the ``partial_fit`` API need to be provided with\n    the list of possible classes at the first call to partial_fit.\n\n    Subsequent calls to partial_fit should check that ``classes`` is still\n    consistent with a previous value of ``clf.classes_`` when provided.\n\n    This function returns True if it detects that this was the first call to\n    ``partial_fit`` on ``clf``. In that case the ``classes_`` attribute is also\n    set on ``clf``.\n\n    \"\"\"\n    if getattr(clf, \"classes_\", None) is None and classes is None:\n        raise ValueError(\"classes must be passed on the first call to partial_fit.\")\n\n(D)     if hasattr(y, \"__array__\") or isinstance(y, Sequence) or is_array_api:\n        # DeprecationWarning will be replaced by ValueError, see NEP 34\n        # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n\n    if not (hasattr(y, \"shape\") and y.ndim == 2 and y.shape[1] > 1):\n        return False\n\n    if issparse(y):\n        if isinstance(y, (dok_matrix, lil_matrix)):\n            y = y.tocsr()\n        labels = xp.unique_values(y.data)\n        return (\n            len(y.data) == 0\n            or (labels.size == 1 or (labels.size == 2) and (0 in labels))\n            and (y.dtype.kind in \"biu\" or _is_integral_float(labels))  # bool, int, uint\n        )\n    else:\n        labels = xp.unique_values(y)\n\n        return len(labels) < 3 and (\n            y.dtype.kind in \"biu\" or _is_integral_float(labels)  # bool, int, uint\n        )\n\n\ndef check_classification_targets(y):\n    \"\"\"Ensure that target y is of a non-regression type.\n\n    Only the following target types (as defined in type_of_target) are allowed:\n        'binary', 'multiclass', 'multiclass-multioutput',\n        'multilabel-indicator', 'multilabel-sequences'\n\n    Parameters\n    ----------\n    y : array-like\n        Target values.\n    \"\"\"\n    y_type = type_of_target(y, input_name=\"y\")\n    if y_type not in [\n        \"binary\",\n        \"multiclass\",\n        \"multiclass-multioutput\",\n        \"multilabel-indicator\",\n        \"multilabel-sequences\",\n    ]:\n        raise ValueError(\"Unknown label type: %r\" % y_type)\n\n\ndef type_of_target(y, input_name=\"\"):\n    \"\"\"Determine the type of data indicated by the target.\n\n    Note that this type is the most specific type that can be inferred.\n    For example:\n\n        * ``binary`` is more specific but compatible with ``multiclass``.\n        * ``multiclass`` of integers is more specific but compatible with\n          ``continuous``.\n        * ``multilabel-indicator`` is more specific but compatible with\n          ``multiclass-multioutput``.\n\n    Parameters\n    ----------\n    y : {array-like, sparse matrix}\n        Target values. If a sparse matrix, `y` is expected to be a\n        CSR/CSC matrix.\n\n    input_name : str, default=\"\"\n        The data name used to construct the error message.\n\n        .. versionadded:: 1.1.0\n\n    Returns\n    -------\n    target_type : str\n        One of:\n\n        * 'continuous': `y` is an array-like of floats that are not all\n          integers, and is 1d or a column vector.\n        * 'continuous-multioutput': `y` is a 2d array of floats that are\n          not all integers, and both dimensions are of size > 1.\n        * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n          vector.\n        * 'multiclass': `y` contains more than two discrete values, is not a\n          sequence of sequences, and is 1d or a column vector.\n        * 'multiclass-multioutput': `y` is a 2d array that contains more\n          than two discrete values, is not a sequence of sequences, and both\n          dimensions are of size > 1.\n        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n          of two dimensions with at least two columns, and at most 2 unique\n          values.\n        * 'unknown': `y` is array-like but none of the above, such as a 3d\n          array, sequence of sequences, or an array of non-sequence objects.\n\n    Examples\n    --------\n    >>> from sklearn.utils.multiclass import type_of_target\n    >>> import numpy as np\n    >>> type_of_target([0.1, 0.6])\n    'continuous'\n    >>> type_of_target([1, -1, -1, 1])\n    'binary'\n    >>> type_of_target(['a', 'b', 'a'])\n    'binary'\n    >>> type_of_target([1.0, 2.0])\n    'binary'\n    >>> type_of_target([1, 0, 2])\n    'multiclass'\n    >>> type_of_target([1.0, 0.0, 3.0])\n    'multiclass'\n    >>> type_of_target(['a', 'b', 'c'])\n    'multiclass'\n    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n    'multiclass-multioutput'\n    >>> type_of_target([[1, 2]])\n    'multilabel-indicator'\n    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n    'continuous-multioutput'\n    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n    'multilabel-indicator'\n    \"\"\"\n    xp, is_array_api = get_namespace(y)\n    valid = (\n        (isinstance(y, Sequence) or issparse(y) or hasattr(y, \"__array__\"))\n        and not isinstance(y, str)\n        or is_array_api\n    )\n\n    if not valid:\n        raise ValueError(\n            \"Expected array-like (array or non-string sequence), got %r\" % y\n        )\n\n    sparse_pandas = y.__class__.__name__ in [\"SparseSeries\", \"SparseArray\"]\n    if sparse_pandas:\n        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n\n    if is_multilabel(y):\n        return \"multilabel-indicator\"\n\n    # DeprecationWarning will be replaced by ValueError, see NEP 34\n    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n    # We therefore catch both deprecation (NumPy < 1.24) warning and\n    # value error (NumPy >= 1.24).\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n        if not issparse(y):\n            try:\n                y = xp.asarray(y)\n            except (np.VisibleDeprecationWarning, ValueError):\n                # dtype=object should be provided explicitly for ragged arrays,\n                # see NEP 34\n                y = xp.asarray(y, dtype=object)\n\n    # The old sequence of sequences format\n    try:\n        if (\n            not hasattr(y[0], \"__array__\")\n            and isinstance(y[0], Sequence)\n            and not isinstance(y[0], str)\n        ):\n            raise ValueError(\n                \"You appear to be using a legacy multi-label data\"\n                \" representation. Sequence of sequences are no\"\n                \" longer supported; use a binary array or sparse\"\n                \" matrix instead - the MultiLabelBinarizer\"\n                \" transformer can convert to this format.\"\n            )\n    except IndexError:\n        pass\n\n    # Invalid inputs\n    if y.ndim not in (1, 2):\n        # Number of dimension greater than 2: [[[1, 2]]]\n        return \"unknown\"\n    if not min(y.shape):\n        # Empty ndarray: []/[[]]\n        if y.ndim == 1:\n            # 1-D empty array: []", "answer": "D"}
{"uuid": "8951d33a-8f79-4147-a890-0b9eceaa67dd", "issue_statement": "FeatureUnion not working when aggregating data and pandas transform output selected\n### Describe the bug\n\nI would like to use `pandas` transform output and use a custom transformer in a feature union which aggregates data. When I'm using this combination I got an error. When I use default `numpy` output it works fine.\n\n### Steps/Code to Reproduce\n\n```python\r\nimport pandas as pd\r\nfrom sklearn.base import BaseEstimator, TransformerMixin\r\nfrom sklearn import set_config\r\nfrom sklearn.pipeline import make_union\r\n\r\nindex = pd.date_range(start=\"2020-01-01\", end=\"2020-01-05\", inclusive=\"left\", freq=\"H\")\r\ndata = pd.DataFrame(index=index, data=[10] * len(index), columns=[\"value\"])\r\ndata[\"date\"] = index.date\r\n\r\n\r\nclass MyTransformer(BaseEstimator, TransformerMixin):\r\n    def fit(self, X: pd.DataFrame, y: pd.Series | None = None, **kwargs):\r\n        return self\r\n\r\n    def transform(self, X: pd.DataFrame, y: pd.Series | None = None) -> pd.DataFrame:\r\n        return X[\"value\"].groupby(X[\"date\"]).sum()\r\n\r\n\r\n# This works.\r\nset_config(transform_output=\"default\")\r\nprint(make_union(MyTransformer()).fit_transform(data))\r\n\r\n# This does not work.\r\nset_config(transform_output=\"pandas\")\r\nprint(make_union(MyTransformer()).fit_transform(data))\r\n```\n\n### Expected Results\n\nNo error is thrown when using `pandas` transform output.\n\n### Actual Results\n\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[5], line 25\r\n     23 # This does not work.\r\n     24 set_config(transform_output=\"pandas\")\r\n---> 25 print(make_union(MyTransformer()).fit_transform(data))\r\n\r\nFile ~/.local/share/virtualenvs/3e_VBrf2/lib/python3.10/site-packages/sklearn/utils/_set_output.py:150, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)\r\n    143 if isinstance(data_to_wrap, tuple):\r\n    144     # only wrap the first output for cross decomposition\r\n    145     return (\r\n    146         _wrap_data_with_container(method, data_to_wrap[0], X, self),\r\n    147         *data_to_wrap[1:],\r\n    148     )\r\n--> 150 return _wrap_data_with_container(method, data_to_wrap, X, self)\r\n\r\nFile ~/.local/share/virtualenvs/3e_VBrf2/lib/python3.10/site-packages/sklearn/utils/_set_output.py:130, in _wrap_data_with_container(method, data_to_wrap, original_input, estimator)\r\n    127     return data_to_wrap\r\n    129 # dense_config == \"pandas\"\r\n--> 130 return _wrap_in_pandas_container(\r\n    131     data_to_wrap=data_to_wrap,\r\n    132     index=getattr(original_input, \"index\", None),\r\n    133     columns=estimator.get_feature_names_out,\r\n    134 )\r\n\r\nFile ~/.local/share/virtualenvs/3e_VBrf2/lib/python3.10/site-packages/sklearn/utils/_set_output.py:59, in _wrap_in_pandas_container(data_to_wrap, columns, index)\r\n     57         data_to_wrap.columns = columns\r\n     58     if index is not None:\r\n---> 59         data_to_wrap.index = index\r\n     60     return data_to_wrap\r\n     62 return pd.DataFrame(data_to_wrap, index=index, columns=columns)\r\n\r\nFile ~/.local/share/virtualenvs/3e_VBrf2/lib/python3.10/site-packages/pandas/core/generic.py:5588, in NDFrame.__setattr__(self, name, value)\r\n   5586 try:\r\n   5587     object.__getattribute__(self, name)\r\n-> 5588     return object.__setattr__(self, name, value)\r\n   5589 except AttributeError:\r\n   5590     pass\r\n\r\nFile ~/.local/share/virtualenvs/3e_VBrf2/lib/python3.10/site-packages/pandas/_libs/properties.pyx:70, in pandas._libs.properties.AxisProperty.__set__()\r\n\r\nFile ~/.local/share/virtualenvs/3e_VBrf2/lib/python3.10/site-packages/pandas/core/generic.py:769, in NDFrame._set_axis(self, axis, labels)\r\n    767 def _set_axis(self, axis: int, labels: Index) -> None:\r\n    768     labels = ensure_index(labels)\r\n--> 769     self._mgr.set_axis(axis, labels)\r\n    770     self._clear_item_cache()\r\n\r\nFile ~/.local/share/virtualenvs/3e_VBrf2/lib/python3.10/site-packages/pandas/core/internals/managers.py:214, in BaseBlockManager.set_axis(self, axis, new_labels)\r\n    212 def set_axis(self, axis: int, new_labels: Index) -> None:\r\n    213     # Caller is responsible for ensuring we have an Index object.\r\n--> 214     self._validate_set_axis(axis, new_labels)\r\n    215     self.axes[axis] = new_labels\r\n\r\nFile ~/.local/share/virtualenvs/3e_VBrf2/lib/python3.10/site-packages/pandas/core/internals/base.py:69, in DataManager._validate_set_axis(self, axis, new_labels)\r\n     66     pass\r\n     68 elif new_len != old_len:\r\n---> 69     raise ValueError(\r\n     70         f\"Length mismatch: Expected axis has {old_len} elements, new \"\r\n     71         f\"values have {new_len} elements\"\r\n     72     )\r\n\r\nValueError: Length mismatch: Expected axis has 4 elements, new values have 96 elements\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.10.6 (main, Aug 30 2022, 05:11:14) [Clang 13.0.0 (clang-1300.0.29.30)]\r\nexecutable: /Users/macbookpro/.local/share/virtualenvs/3e_VBrf2/bin/python\r\n   machine: macOS-11.3-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.1\r\n          pip: 22.3.1\r\n   setuptools: 67.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.10.1\r\n       Cython: None\r\n       pandas: 1.4.4\r\n   matplotlib: 3.7.0\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /Users/macbookpro/.local/share/virtualenvs/3e_VBrf2/lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 4\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libomp\r\n       filepath: /Users/macbookpro/.local/share/virtualenvs/3e_VBrf2/lib/python3.10/site-packages/sklearn/.dylibs/libomp.dylib\r\n        version: None\r\n    num_threads: 8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /Users/macbookpro/.local/share/virtualenvs/3e_VBrf2/lib/python3.10/site-packages/scipy/.dylibs/libopenblas.0.dylib\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 4\n```", "choices": "(A)         try:\n            columns = columns()\n        except Exception:\n            columns = None\n\n    pd = check_pandas_support(\"Setting output container to 'pandas'\")\n\n    if isinstance(data_to_wrap, pd.DataFrame):\n        if columns is not None:\n            data_to_wrap.columns = columns\n        if index is not None:\n            data_to_wrap.index = index\n        return data_to_wrap\n\n    return pd.DataFrame(data_to_wrap, index=index, columns=columns)\n\n\ndef _get_output_config(method, estimator=None):\n    \"\"\"Get output config based on estimator and global configuration.\n\n    Parameters\n    ----------\n    method : {\"transform\"}\n        Estimator's method for which the output container is looked up.\n\n    estimator : estimator instance or None\n        Estimator to get the output configuration from. If `None`, check global\n(B)     dataframe : DataFrame\n        Container with column names or unchanged `output`.\n    \"\"\"\n    if issparse(data_to_wrap):\n        raise ValueError(\"Pandas output does not support sparse data.\")\n\n    if callable(columns):\n        try:\n            columns = columns()\n        except Exception:\n            columns = None\n\n    pd = check_pandas_support(\"Setting output container to 'pandas'\")\n\n    if isinstance(data_to_wrap, pd.DataFrame):\n        if columns is not None:\n            data_to_wrap.columns = columns\n        if index is not None:\n            data_to_wrap.index = index\n        return data_to_wrap\n\n    return pd.DataFrame(data_to_wrap, index=index, columns=columns)\n\n\ndef _get_output_config(method, estimator=None):\n    \"\"\"Get output config based on estimator and global configuration.\n\n(C)     columns : callable, ndarray, or None\n        The column names or a callable that returns the column names. The\n        callable is useful if the column names require some computation.\n        If `columns` is a callable that raises an error, `columns` will have\n        the same semantics as `None`. If `None` and `data_to_wrap` is already a\n        dataframe, then the column names are not changed. If `None` and\n        `data_to_wrap` is **not** a dataframe, then columns are\n        `range(n_features)`.\n\n    index : array-like, default=None\n        Index for data.\n\n    Returns\n    -------\n    dataframe : DataFrame\n        Container with column names or unchanged `output`.\n    \"\"\"\n    if issparse(data_to_wrap):\n        raise ValueError(\"Pandas output does not support sparse data.\")\n\n    if callable(columns):\n        try:\n            columns = columns()\n        except Exception:\n            columns = None\n\n    pd = check_pandas_support(\"Setting output container to 'pandas'\")\n(D)         `range(n_features)`.\n\n    index : array-like, default=None\n        Index for data.\n\n    Returns\n    -------\n    dataframe : DataFrame\n        Container with column names or unchanged `output`.\n    \"\"\"\n    if issparse(data_to_wrap):\n        raise ValueError(\"Pandas output does not support sparse data.\")\n\n    if callable(columns):\n        try:\n            columns = columns()\n        except Exception:\n            columns = None\n\n    pd = check_pandas_support(\"Setting output container to 'pandas'\")\n\n    if isinstance(data_to_wrap, pd.DataFrame):\n        if columns is not None:\n            data_to_wrap.columns = columns\n        if index is not None:\n            data_to_wrap.index = index\n        return data_to_wrap", "answer": "D"}
{"uuid": "6ad25d81-1e1e-42b4-860d-b1fb69162b50", "issue_statement": "inherited-members should support more than one class\n**Is your feature request related to a problem? Please describe.**\r\nI have two situations:\r\n- A class inherits from multiple other classes. I want to document members from some of the base classes but ignore some of the base classes\r\n- A module contains several class definitions that inherit from different classes that should all be ignored (e.g., classes that inherit from list or set or tuple). I want to ignore members from list, set, and tuple while documenting all other inherited members in classes in the module.\r\n\r\n**Describe the solution you'd like**\r\nThe :inherited-members: option to automodule should accept a list of classes. If any of these classes are encountered as base classes when instantiating autoclass documentation, they should be ignored.\r\n\r\n**Describe alternatives you've considered**\r\nThe alternative is to not use automodule, but instead manually enumerate several autoclass blocks for a module. This only addresses the second bullet in the problem description and not the first. It is also tedious for modules containing many class definitions.", "choices": "(A)     return {x.strip() for x in arg.split(',') if x.strip()}\n\n\ndef inherited_members_option(arg: Any) -> Union[object, Set[str]]:\n    \"\"\"Used to convert the :members: option to auto directives.\"\"\"\n    if arg in (None, True):\n        return 'object'\n    else:\n        return arg\n\n\ndef member_order_option(arg: Any) -> Optional[str]:\n    \"\"\"Used to convert the :members: option to auto directives.\"\"\"\n    if arg in (None, True):\n        return None\n    elif arg in ('alphabetical', 'bysource', 'groupwise'):\n        return arg\n    else:\n        raise ValueError(__('invalid value for member-order option: %s') % arg)\n\n\ndef class_doc_from_option(arg: Any) -> Optional[str]:\n    \"\"\"Used to convert the :class-doc-from: option to autoclass directives.\"\"\"\n    if arg in ('both', 'class', 'init'):\n        return arg\n    else:\n        raise ValueError(__('invalid value for class-doc-from option: %s') % arg)\n\n\nSUPPRESS = object()\n\n\ndef annotation_option(arg: Any) -> Any:\n    if arg in (None, True):\n        # suppress showing the representation of the object\n        return SUPPRESS\n    else:\n        return arg\n\n\ndef bool_option(arg: Any) -> bool:\n    \"\"\"Used to convert flag options to auto directives.  (Instead of\n    directives.flag(), which returns None).\n    \"\"\"\n    return True\n\n\ndef merge_special_members_option(options: Dict) -> None:\n    \"\"\"Merge :special-members: option to :members: option.\"\"\"\n    warnings.warn(\"merge_special_members_option() is deprecated.\",\n                  RemovedInSphinx50Warning, stacklevel=2)\n    if 'special-members' in options and options['special-members'] is not ALL:\n        if options.get('members') is ALL:\n            pass\n        elif options.get('members'):\n            for member in options['special-members']:\n                if member not in options['members']:\n                    options['members'].append(member)\n        else:\n            options['members'] = options['special-members']\n\n\ndef merge_members_option(options: Dict) -> None:\n    \"\"\"Merge :*-members: option to the :members: option.\"\"\"\n    if options.get('members') is ALL:\n        # merging is not needed when members: ALL\n        return\n\n    members = options.setdefault('members', [])\n    for key in {'private-members', 'special-members'}:\n        if key in options and options[key] not in (ALL, None):\n            for member in options[key]:\n                if member not in members:\n                    members.append(member)\n\n\n# Some useful event listener factories for autodoc-process-docstring.\n\ndef cut_lines(pre: int, post: int = 0, what: str = None) -> Callable:\n    \"\"\"Return a listener that removes the first *pre* and last *post*\n    lines of every docstring.  If *what* is a sequence of strings,\n    only docstrings of a type in *what* will be processed.\n\n    Use like this (e.g. in the ``setup()`` function of :file:`conf.py`)::\n\n       from sphinx.ext.autodoc import cut_lines\n       app.connect('autodoc-process-docstring', cut_lines(4, what=['module']))\n\n    This can (and should) be used in place of :confval:`automodule_skip_lines`.\n    \"\"\"\n    def process(app: Sphinx, what_: str, name: str, obj: Any, options: Any, lines: List[str]\n                ) -> None:\n        if what and what_ not in what:\n            return\n        del lines[:pre]\n        if post:\n            # remove one trailing blank line.\n            if lines and not lines[-1]:\n                lines.pop(-1)\n            del lines[-post:]\n        # make sure there is a blank line at the end\n        if lines and lines[-1]:\n            lines.append('')\n    return process\n\n\ndef between(marker: str, what: Sequence[str] = None, keepempty: bool = False,\n            exclude: bool = False) -> Callable:\n    \"\"\"Return a listener that either keeps, or if *exclude* is True excludes,\n    lines between lines that match the *marker* regular expression.  If no line\n    matches, the resulting docstring would be empty, so no change will be made\n    unless *keepempty* is true.\n\n    If *what* is a sequence of strings, only docstrings of a type in *what* will\n    be processed.\n    \"\"\"\n    marker_re = re.compile(marker)\n\n    def process(app: Sphinx, what_: str, name: str, obj: Any, options: Any, lines: List[str]\n                ) -> None:\n        if what and what_ not in what:\n            return\n        deleted = 0\n        delete = not exclude\n        orig_lines = lines[:]\n        for i, line in enumerate(orig_lines):\n            if delete:\n                lines.pop(i - deleted)\n                deleted += 1\n            if marker_re.match(line):\n                delete = not delete\n                if delete:\n                    lines.pop(i - deleted)\n                    deleted += 1\n        if not lines and not keepempty:\n            lines[:] = orig_lines\n        # make sure there is a blank line at the end\n        if lines and lines[-1]:\n            lines.append('')\n    return process\n\n\n# This class is used only in ``sphinx.ext.autodoc.directive``,\n# But we define this class here to keep compatibility (see #4538)\nclass Options(dict):\n    \"\"\"A dict/attribute hybrid that returns None on nonexisting keys.\"\"\"\n    def copy(self) -> \"Options\":\n        return Options(super().copy())\n\n    def __getattr__(self, name: str) -> Any:\n        try:\n            return self[name.replace('_', '-')]\n        except KeyError:\n            return None\n\n\nclass ObjectMember(tuple):\n    \"\"\"A member of object.\n\n    This is used for the result of `Documenter.get_object_members()` to\n    represent each member of the object.\n\n    .. Note::\n\n       An instance of this class behaves as a tuple of (name, object)\n       for compatibility to old Sphinx.  The behavior will be dropped\n       in the future.  Therefore extensions should not use the tuple\n       interface.\n    \"\"\"\n\n    def __new__(cls, name: str, obj: Any, **kwargs: Any) -> Any:\n        return super().__new__(cls, (name, obj))  # type: ignore\n\n    def __init__(self, name: str, obj: Any, docstring: Optional[str] = None,\n                 class_: Any = None, skipped: bool = False) -> None:\n        self.__name__ = name\n        self.object = obj\n        self.docstring = docstring\n        self.skipped = skipped\n        self.class_ = class_\n\n\nObjectMembers = Union[List[ObjectMember], List[Tuple[str, Any]]]\n\n\nclass Documenter:\n    \"\"\"\n    A Documenter knows how to autodocument a single object type.  When\n    registered with the AutoDirective, it will be used to document objects\n    of that type when needed by autodoc.\n\n    Its *objtype* attribute selects what auto directive it is assigned to\n    (the directive name is 'auto' + objtype), and what directive it generates\n    by default, though that can be overridden by an attribute called\n    *directivetype*.\n\n    A Documenter has an *option_spec* that works like a docutils directive's;\n    in fact, it will be used to parse an auto directive's options that matches\n    the Documenter.\n    \"\"\"\n    #: name by which the directive is called (auto...) and the default\n    #: generated directive name\n    objtype = 'object'\n    #: indentation by which to indent the directive content\n    content_indent = '   '\n    #: priority if multiple documenters return True from can_document_member\n    priority = 0\n    #: order if autodoc_member_order is set to 'groupwise'\n    member_order = 0\n    #: true if the generated content may contain titles\n    titles_allowed = False\n\n    option_spec: OptionSpec = {\n        'noindex': bool_option\n    }\n\n    def get_attr(self, obj: Any, name: str, *defargs: Any) -> Any:\n        \"\"\"getattr() override for types such as Zope interfaces.\"\"\"\n        return autodoc_attrgetter(self.env.app, obj, name, *defargs)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"Called to see if a member can be documented by this Documenter.\"\"\"\n        raise NotImplementedError('must be implemented in subclasses')\n\n    def __init__(self, directive: \"DocumenterBridge\", name: str, indent: str = '') -> None:\n        self.directive = directive\n        self.config: Config = directive.env.config\n        self.env: BuildEnvironment = directive.env\n        self.options = directive.genopt\n        self.name = name\n        self.indent = indent\n        # the module and object path within the module, and the fully\n        # qualified name (all set after resolve_name succeeds)\n        self.modname: str = None\n        self.module: ModuleType = None\n        self.objpath: List[str] = None\n        self.fullname: str = None\n        # extra signature items (arguments and return annotation,\n        # also set after resolve_name succeeds)\n        self.args: str = None\n        self.retann: str = None\n        # the object to document (set after import_object succeeds)\n        self.object: Any = None\n        self.object_name: str = None\n        # the parent/owner of the object to document\n        self.parent: Any = None\n        # the module analyzer to get at attribute docs, or None\n        self.analyzer: ModuleAnalyzer = None\n\n    @property\n    def documenters(self) -> Dict[str, Type[\"Documenter\"]]:\n        \"\"\"Returns registered Documenter classes\"\"\"\n        return self.env.app.registry.documenters\n\n    def add_line(self, line: str, source: str, *lineno: int) -> None:\n        \"\"\"Append one line of generated reST to the output.\"\"\"\n        if line.strip():  # not a blank line\n            self.directive.result.append(self.indent + line, source, *lineno)\n        else:\n            self.directive.result.append('', source, *lineno)\n\n    def resolve_name(self, modname: str, parents: Any, path: str, base: Any\n                     ) -> Tuple[str, List[str]]:\n        \"\"\"Resolve the module and name of the object to document given by the\n        arguments and the current module/class.\n\n        Must return a pair of the module name and a chain of attributes; for\n        example, it would return ``('zipfile', ['ZipFile', 'open'])`` for the\n        ``zipfile.ZipFile.open`` method.\n        \"\"\"\n        raise NotImplementedError('must be implemented in subclasses')\n\n    def parse_name(self) -> bool:\n        \"\"\"Determine what module to import and what attribute to document.\n\n        Returns True and sets *self.modname*, *self.objpath*, *self.fullname*,\n        *self.args* and *self.retann* if parsing and resolving was successful.\n        \"\"\"\n        # first, parse the definition -- auto directives for classes and\n        # functions can contain a signature which is then used instead of\n        # an autogenerated one\n        try:\n            matched = py_ext_sig_re.match(self.name)\n            explicit_modname, path, base, args, retann = matched.groups()\n        except AttributeError:\n            logger.warning(__('invalid signature for auto%s (%r)') % (self.objtype, self.name),\n                           type='autodoc')\n            return False\n\n        # support explicit module and class name separation via ::\n        if explicit_modname is not None:\n            modname = explicit_modname[:-2]\n            parents = path.rstrip('.').split('.') if path else []\n        else:\n            modname = None\n            parents = []\n\n        with mock(self.config.autodoc_mock_imports):\n            self.modname, self.objpath = self.resolve_name(modname, parents, path, base)\n\n        if not self.modname:\n            return False\n\n        self.args = args\n        self.retann = retann\n        self.fullname = ((self.modname or '') +\n                         ('.' + '.'.join(self.objpath) if self.objpath else ''))\n        return True\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Import the object given by *self.modname* and *self.objpath* and set\n        it as *self.object*.\n\n        Returns True if successful, False if an error occurred.\n        \"\"\"\n        with mock(self.config.autodoc_mock_imports):\n            try:\n                ret = import_object(self.modname, self.objpath, self.objtype,\n                                    attrgetter=self.get_attr,\n                                    warningiserror=self.config.autodoc_warningiserror)\n                self.module, self.parent, self.object_name, self.object = ret\n                if ismock(self.object):\n                    self.object = undecorate(self.object)\n                return True\n            except ImportError as exc:\n                if raiseerror:\n                    raise\n                else:\n                    logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                    self.env.note_reread()\n                    return False\n\n    def get_real_modname(self) -> str:\n        \"\"\"Get the real module name of an object to document.\n\n        It can differ from the name of the module through which the object was\n        imported.\n        \"\"\"\n        return self.get_attr(self.object, '__module__', None) or self.modname\n\n    def check_module(self) -> bool:\n        \"\"\"Check if *self.object* is really defined in the module given by\n        *self.modname*.\n        \"\"\"\n        if self.options.imported_members:\n            return True\n\n        subject = inspect.unpartial(self.object)\n        modname = self.get_attr(subject, '__module__', None)\n        if modname and modname != self.modname:\n            return False\n        return True\n\n    def format_args(self, **kwargs: Any) -> str:\n        \"\"\"Format the argument signature of *self.object*.\n\n        Should return None if the object does not have a signature.\n        \"\"\"\n        return None\n\n    def format_name(self) -> str:\n        \"\"\"Format the name of *self.object*.\n\n        This normally should be something that can be parsed by the generated\n        directive, but doesn't need to be (Sphinx will display it unparsed\n        then).\n        \"\"\"\n        # normally the name doesn't contain the module (except for module\n        # directives of course)\n        return '.'.join(self.objpath) or self.modname\n\n    def _call_format_args(self, **kwargs: Any) -> str:\n        if kwargs:\n            try:\n                return self.format_args(**kwargs)\n            except TypeError:\n                # avoid chaining exceptions, by putting nothing here\n                pass\n\n        # retry without arguments for old documenters\n        return self.format_args()\n\n    def format_signature(self, **kwargs: Any) -> str:\n        \"\"\"Format the signature (arguments and return annotation) of the object.\n\n        Let the user process it via the ``autodoc-process-signature`` event.\n        \"\"\"\n        if self.args is not None:\n            # signature given explicitly\n            args = \"(%s)\" % self.args\n            retann = self.retann\n        else:\n            # try to introspect the signature\n            try:\n                retann = None\n                args = self._call_format_args(**kwargs)\n                if args:\n                    matched = re.match(r'^(\\(.*\\))\\s+->\\s+(.*)$', args)\n                    if matched:\n                        args = matched.group(1)\n                        retann = matched.group(2)\n            except Exception as exc:\n                logger.warning(__('error while formatting arguments for %s: %s'),\n                               self.fullname, exc, type='autodoc')\n                args = None\n\n        result = self.env.events.emit_firstresult('autodoc-process-signature',\n                                                  self.objtype, self.fullname,\n                                                  self.object, self.options, args, retann)\n        if result:\n            args, retann = result\n\n        if args is not None:\n            return args + ((' -> %s' % retann) if retann else '')\n        else:\n            return ''\n\n    def add_directive_header(self, sig: str) -> None:\n        \"\"\"Add the directive header and options to the generated content.\"\"\"\n        domain = getattr(self, 'domain', 'py')\n        directive = getattr(self, 'directivetype', self.objtype)\n        name = self.format_name()\n        sourcename = self.get_sourcename()\n\n        # one signature per line, indented by column\n        prefix = '.. %s:%s:: ' % (domain, directive)\n        for i, sig_line in enumerate(sig.split(\"\\n\")):\n            self.add_line('%s%s%s' % (prefix, name, sig_line),\n                          sourcename)\n            if i == 0:\n                prefix = \" \" * len(prefix)\n\n        if self.options.noindex:\n            self.add_line('   :noindex:', sourcename)\n        if self.objpath:\n            # Be explicit about the module, this is necessary since .. class::\n            # etc. don't support a prepended module name\n            self.add_line('   :module: %s' % self.modname, sourcename)\n\n    def get_doc(self, ignore: int = None) -> Optional[List[List[str]]]:\n        \"\"\"Decode and return lines of the docstring(s) for the object.\n\n        When it returns None, autodoc-process-docstring will not be called for this\n        object.\n        \"\"\"\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n        docstring = getdoc(self.object, self.get_attr, self.config.autodoc_inherit_docstrings,\n                           self.parent, self.object_name)\n        if docstring:\n            tab_width = self.directive.state.document.settings.tab_width\n            return [prepare_docstring(docstring, ignore, tab_width)]\n        return []\n\n    def process_doc(self, docstrings: List[List[str]]) -> Iterator[str]:\n        \"\"\"Let the user process the docstrings before adding them.\"\"\"\n        for docstringlines in docstrings:\n            if self.env.app:\n                # let extensions preprocess docstrings\n                self.env.app.emit('autodoc-process-docstring',\n                                  self.objtype, self.fullname, self.object,\n                                  self.options, docstringlines)\n\n                if docstringlines and docstringlines[-1] != '':\n                    # append a blank line to the end of the docstring\n                    docstringlines.append('')\n\n            yield from docstringlines\n\n    def get_sourcename(self) -> str:\n        if (inspect.safe_getattr(self.object, '__module__', None) and\n                inspect.safe_getattr(self.object, '__qualname__', None)):\n            # Get the correct location of docstring from self.object\n            # to support inherited methods\n            fullname = '%s.%s' % (self.object.__module__, self.object.__qualname__)\n        else:\n            fullname = self.fullname\n\n        if self.analyzer:\n            return '%s:docstring of %s' % (self.analyzer.srcname, fullname)\n        else:\n            return 'docstring of %s' % fullname\n\n    def add_content(self, more_content: Optional[StringList], no_docstring: bool = False\n                    ) -> None:\n        \"\"\"Add content from docstrings, attribute documentation and user.\"\"\"\n        if no_docstring:\n            warnings.warn(\"The 'no_docstring' argument to %s.add_content() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        # set sourcename and add content from attribute documentation\n        sourcename = self.get_sourcename()\n        if self.analyzer:\n            attr_docs = self.analyzer.find_attr_docs()\n            if self.objpath:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if key in attr_docs:\n                    no_docstring = True\n                    # make a copy of docstring for attributes to avoid cache\n                    # the change of autodoc-process-docstring event.\n                    docstrings = [list(attr_docs[key])]\n\n                    for i, line in enumerate(self.process_doc(docstrings)):\n                        self.add_line(line, sourcename, i)\n\n        # add content from docstrings\n        if not no_docstring:\n            docstrings = self.get_doc()\n            if docstrings is None:\n                # Do not call autodoc-process-docstring on get_doc() returns None.\n                pass\n            else:\n                if not docstrings:\n                    # append at least a dummy docstring, so that the event\n                    # autodoc-process-docstring is fired and can add some\n                    # content if desired\n                    docstrings.append([])\n                for i, line in enumerate(self.process_doc(docstrings)):\n                    self.add_line(line, sourcename, i)\n\n        # add additional content (e.g. from document), if present\n        if more_content:\n            for line, src in zip(more_content.data, more_content.items):\n                self.add_line(line, src[0], src[1])\n\n    def get_object_members(self, want_all: bool) -> Tuple[bool, ObjectMembers]:\n        \"\"\"Return `(members_check_module, members)` where `members` is a\n        list of `(membername, member)` pairs of the members of *self.object*.\n\n        If *want_all* is True, return all members.  Else, only return those\n        members given by *self.options.members* (which may also be None).\n        \"\"\"\n        warnings.warn('The implementation of Documenter.get_object_members() will be '\n                      'removed from Sphinx-6.0.', RemovedInSphinx60Warning)\n        members = get_object_members(self.object, self.objpath, self.get_attr, self.analyzer)\n        if not want_all:\n            if not self.options.members:\n                return False, []  # type: ignore\n            # specific members given\n            selected = []\n            for name in self.options.members:  # type: str\n                if name in members:\n                    selected.append((name, members[name].value))\n                else:\n                    logger.warning(__('missing attribute %s in object %s') %\n                                   (name, self.fullname), type='autodoc')\n            return False, selected\n        elif self.options.inherited_members:\n            return False, [(m.name, m.value) for m in members.values()]\n        else:\n            return False, [(m.name, m.value) for m in members.values()\n                           if m.directly_defined]\n\n    def filter_members(self, members: ObjectMembers, want_all: bool\n                       ) -> List[Tuple[str, Any, bool]]:\n        \"\"\"Filter the given member list.\n\n        Members are skipped if\n\n        - they are private (except if given explicitly or the private-members\n          option is set)\n        - they are special methods (except if given explicitly or the\n          special-members option is set)\n        - they are undocumented (except if the undoc-members option is set)\n\n        The user can override the skipping decision by connecting to the\n        ``autodoc-skip-member`` event.\n        \"\"\"\n        def is_filtered_inherited_member(name: str, obj: Any) -> bool:\n            if inspect.isclass(self.object):\n                for cls in self.object.__mro__:\n                    if cls.__name__ == self.options.inherited_members and cls != self.object:\n                        # given member is a member of specified *super class*\n                        return True\n                    elif name in cls.__dict__:\n                        return False\n                    elif name in self.get_attr(cls, '__annotations__', {}):\n                        return False\n                    elif isinstance(obj, ObjectMember) and obj.class_ is cls:\n(B)         if explicit_modname is not None:\n            modname = explicit_modname[:-2]\n            parents = path.rstrip('.').split('.') if path else []\n        else:\n            modname = None\n            parents = []\n\n        with mock(self.config.autodoc_mock_imports):\n            self.modname, self.objpath = self.resolve_name(modname, parents, path, base)\n\n        if not self.modname:\n            return False\n\n        self.args = args\n        self.retann = retann\n        self.fullname = ((self.modname or '') +\n                         ('.' + '.'.join(self.objpath) if self.objpath else ''))\n        return True\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Import the object given by *self.modname* and *self.objpath* and set\n        it as *self.object*.\n\n        Returns True if successful, False if an error occurred.\n        \"\"\"\n        with mock(self.config.autodoc_mock_imports):\n            try:\n                ret = import_object(self.modname, self.objpath, self.objtype,\n                                    attrgetter=self.get_attr,\n                                    warningiserror=self.config.autodoc_warningiserror)\n                self.module, self.parent, self.object_name, self.object = ret\n                if ismock(self.object):\n                    self.object = undecorate(self.object)\n                return True\n            except ImportError as exc:\n                if raiseerror:\n                    raise\n                else:\n                    logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                    self.env.note_reread()\n                    return False\n\n    def get_real_modname(self) -> str:\n        \"\"\"Get the real module name of an object to document.\n\n        It can differ from the name of the module through which the object was\n        imported.\n        \"\"\"\n        return self.get_attr(self.object, '__module__', None) or self.modname\n\n    def check_module(self) -> bool:\n        \"\"\"Check if *self.object* is really defined in the module given by\n        *self.modname*.\n        \"\"\"\n        if self.options.imported_members:\n            return True\n\n        subject = inspect.unpartial(self.object)\n        modname = self.get_attr(subject, '__module__', None)\n        if modname and modname != self.modname:\n            return False\n        return True\n\n    def format_args(self, **kwargs: Any) -> str:\n        \"\"\"Format the argument signature of *self.object*.\n\n        Should return None if the object does not have a signature.\n        \"\"\"\n        return None\n\n    def format_name(self) -> str:\n        \"\"\"Format the name of *self.object*.\n\n        This normally should be something that can be parsed by the generated\n        directive, but doesn't need to be (Sphinx will display it unparsed\n        then).\n        \"\"\"\n        # normally the name doesn't contain the module (except for module\n        # directives of course)\n        return '.'.join(self.objpath) or self.modname\n\n    def _call_format_args(self, **kwargs: Any) -> str:\n        if kwargs:\n            try:\n                return self.format_args(**kwargs)\n            except TypeError:\n                # avoid chaining exceptions, by putting nothing here\n                pass\n\n        # retry without arguments for old documenters\n        return self.format_args()\n\n    def format_signature(self, **kwargs: Any) -> str:\n        \"\"\"Format the signature (arguments and return annotation) of the object.\n\n        Let the user process it via the ``autodoc-process-signature`` event.\n        \"\"\"\n        if self.args is not None:\n            # signature given explicitly\n            args = \"(%s)\" % self.args\n            retann = self.retann\n        else:\n            # try to introspect the signature\n            try:\n                retann = None\n                args = self._call_format_args(**kwargs)\n                if args:\n                    matched = re.match(r'^(\\(.*\\))\\s+->\\s+(.*)$', args)\n                    if matched:\n                        args = matched.group(1)\n                        retann = matched.group(2)\n            except Exception as exc:\n                logger.warning(__('error while formatting arguments for %s: %s'),\n                               self.fullname, exc, type='autodoc')\n                args = None\n\n        result = self.env.events.emit_firstresult('autodoc-process-signature',\n                                                  self.objtype, self.fullname,\n                                                  self.object, self.options, args, retann)\n        if result:\n            args, retann = result\n\n        if args is not None:\n            return args + ((' -> %s' % retann) if retann else '')\n        else:\n            return ''\n\n    def add_directive_header(self, sig: str) -> None:\n        \"\"\"Add the directive header and options to the generated content.\"\"\"\n        domain = getattr(self, 'domain', 'py')\n        directive = getattr(self, 'directivetype', self.objtype)\n        name = self.format_name()\n        sourcename = self.get_sourcename()\n\n        # one signature per line, indented by column\n        prefix = '.. %s:%s:: ' % (domain, directive)\n        for i, sig_line in enumerate(sig.split(\"\\n\")):\n            self.add_line('%s%s%s' % (prefix, name, sig_line),\n                          sourcename)\n            if i == 0:\n                prefix = \" \" * len(prefix)\n\n        if self.options.noindex:\n            self.add_line('   :noindex:', sourcename)\n        if self.objpath:\n            # Be explicit about the module, this is necessary since .. class::\n            # etc. don't support a prepended module name\n            self.add_line('   :module: %s' % self.modname, sourcename)\n\n    def get_doc(self, ignore: int = None) -> Optional[List[List[str]]]:\n        \"\"\"Decode and return lines of the docstring(s) for the object.\n\n        When it returns None, autodoc-process-docstring will not be called for this\n        object.\n        \"\"\"\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n        docstring = getdoc(self.object, self.get_attr, self.config.autodoc_inherit_docstrings,\n                           self.parent, self.object_name)\n        if docstring:\n            tab_width = self.directive.state.document.settings.tab_width\n            return [prepare_docstring(docstring, ignore, tab_width)]\n        return []\n\n    def process_doc(self, docstrings: List[List[str]]) -> Iterator[str]:\n        \"\"\"Let the user process the docstrings before adding them.\"\"\"\n        for docstringlines in docstrings:\n            if self.env.app:\n                # let extensions preprocess docstrings\n                self.env.app.emit('autodoc-process-docstring',\n                                  self.objtype, self.fullname, self.object,\n                                  self.options, docstringlines)\n\n                if docstringlines and docstringlines[-1] != '':\n                    # append a blank line to the end of the docstring\n                    docstringlines.append('')\n\n            yield from docstringlines\n\n    def get_sourcename(self) -> str:\n        if (inspect.safe_getattr(self.object, '__module__', None) and\n                inspect.safe_getattr(self.object, '__qualname__', None)):\n            # Get the correct location of docstring from self.object\n            # to support inherited methods\n            fullname = '%s.%s' % (self.object.__module__, self.object.__qualname__)\n        else:\n            fullname = self.fullname\n\n        if self.analyzer:\n            return '%s:docstring of %s' % (self.analyzer.srcname, fullname)\n        else:\n            return 'docstring of %s' % fullname\n\n    def add_content(self, more_content: Optional[StringList], no_docstring: bool = False\n                    ) -> None:\n        \"\"\"Add content from docstrings, attribute documentation and user.\"\"\"\n        if no_docstring:\n            warnings.warn(\"The 'no_docstring' argument to %s.add_content() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        # set sourcename and add content from attribute documentation\n        sourcename = self.get_sourcename()\n        if self.analyzer:\n            attr_docs = self.analyzer.find_attr_docs()\n            if self.objpath:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if key in attr_docs:\n                    no_docstring = True\n                    # make a copy of docstring for attributes to avoid cache\n                    # the change of autodoc-process-docstring event.\n                    docstrings = [list(attr_docs[key])]\n\n                    for i, line in enumerate(self.process_doc(docstrings)):\n                        self.add_line(line, sourcename, i)\n\n        # add content from docstrings\n        if not no_docstring:\n            docstrings = self.get_doc()\n            if docstrings is None:\n                # Do not call autodoc-process-docstring on get_doc() returns None.\n                pass\n            else:\n                if not docstrings:\n                    # append at least a dummy docstring, so that the event\n                    # autodoc-process-docstring is fired and can add some\n                    # content if desired\n                    docstrings.append([])\n                for i, line in enumerate(self.process_doc(docstrings)):\n                    self.add_line(line, sourcename, i)\n\n        # add additional content (e.g. from document), if present\n        if more_content:\n            for line, src in zip(more_content.data, more_content.items):\n                self.add_line(line, src[0], src[1])\n\n    def get_object_members(self, want_all: bool) -> Tuple[bool, ObjectMembers]:\n        \"\"\"Return `(members_check_module, members)` where `members` is a\n        list of `(membername, member)` pairs of the members of *self.object*.\n\n        If *want_all* is True, return all members.  Else, only return those\n        members given by *self.options.members* (which may also be None).\n        \"\"\"\n        warnings.warn('The implementation of Documenter.get_object_members() will be '\n                      'removed from Sphinx-6.0.', RemovedInSphinx60Warning)\n        members = get_object_members(self.object, self.objpath, self.get_attr, self.analyzer)\n        if not want_all:\n            if not self.options.members:\n                return False, []  # type: ignore\n            # specific members given\n            selected = []\n            for name in self.options.members:  # type: str\n                if name in members:\n                    selected.append((name, members[name].value))\n                else:\n                    logger.warning(__('missing attribute %s in object %s') %\n                                   (name, self.fullname), type='autodoc')\n            return False, selected\n        elif self.options.inherited_members:\n            return False, [(m.name, m.value) for m in members.values()]\n        else:\n            return False, [(m.name, m.value) for m in members.values()\n                           if m.directly_defined]\n\n    def filter_members(self, members: ObjectMembers, want_all: bool\n                       ) -> List[Tuple[str, Any, bool]]:\n        \"\"\"Filter the given member list.\n\n        Members are skipped if\n\n        - they are private (except if given explicitly or the private-members\n          option is set)\n        - they are special methods (except if given explicitly or the\n          special-members option is set)\n        - they are undocumented (except if the undoc-members option is set)\n\n        The user can override the skipping decision by connecting to the\n        ``autodoc-skip-member`` event.\n        \"\"\"\n        def is_filtered_inherited_member(name: str, obj: Any) -> bool:\n            if inspect.isclass(self.object):\n                for cls in self.object.__mro__:\n                    if cls.__name__ == self.options.inherited_members and cls != self.object:\n                        # given member is a member of specified *super class*\n                        return True\n                    elif name in cls.__dict__:\n                        return False\n                    elif name in self.get_attr(cls, '__annotations__', {}):\n                        return False\n                    elif isinstance(obj, ObjectMember) and obj.class_ is cls:\n                        return False\n\n            return False\n\n        ret = []\n\n        # search for members in source code too\n        namespace = '.'.join(self.objpath)  # will be empty for modules\n\n        if self.analyzer:\n            attr_docs = self.analyzer.find_attr_docs()\n        else:\n            attr_docs = {}\n\n        # process members and determine which to skip\n        for obj in members:\n            try:\n                membername, member = obj\n                # if isattr is True, the member is documented as an attribute\n                if member is INSTANCEATTR:\n                    isattr = True\n                elif (namespace, membername) in attr_docs:\n                    isattr = True\n                else:\n                    isattr = False\n\n                doc = getdoc(member, self.get_attr, self.config.autodoc_inherit_docstrings,\n                             self.object, membername)\n                if not isinstance(doc, str):\n                    # Ignore non-string __doc__\n                    doc = None\n\n                # if the member __doc__ is the same as self's __doc__, it's just\n                # inherited and therefore not the member's doc\n                cls = self.get_attr(member, '__class__', None)\n                if cls:\n                    cls_doc = self.get_attr(cls, '__doc__', None)\n                    if cls_doc == doc:\n                        doc = None\n\n                if isinstance(obj, ObjectMember) and obj.docstring:\n                    # hack for ClassDocumenter to inject docstring via ObjectMember\n                    doc = obj.docstring\n\n                doc, metadata = separate_metadata(doc)\n                has_doc = bool(doc)\n\n                if 'private' in metadata:\n                    # consider a member private if docstring has \"private\" metadata\n                    isprivate = True\n                elif 'public' in metadata:\n                    # consider a member public if docstring has \"public\" metadata\n                    isprivate = False\n                else:\n                    isprivate = membername.startswith('_')\n\n                keep = False\n                if ismock(member) and (namespace, membername) not in attr_docs:\n                    # mocked module or object\n                    pass\n                elif (self.options.exclude_members and\n                      membername in self.options.exclude_members):\n                    # remove members given by exclude-members\n                    keep = False\n                elif want_all and special_member_re.match(membername):\n                    # special __methods__\n                    if (self.options.special_members and\n                            membername in self.options.special_members):\n                        if membername == '__doc__':\n                            keep = False\n                        elif is_filtered_inherited_member(membername, obj):\n                            keep = False\n                        else:\n                            keep = has_doc or self.options.undoc_members\n                    else:\n                        keep = False\n                elif (namespace, membername) in attr_docs:\n                    if want_all and isprivate:\n                        if self.options.private_members is None:\n                            keep = False\n                        else:\n                            keep = membername in self.options.private_members\n                    else:\n                        # keep documented attributes\n                        keep = True\n                elif want_all and isprivate:\n                    if has_doc or self.options.undoc_members:\n                        if self.options.private_members is None:\n                            keep = False\n                        elif is_filtered_inherited_member(membername, obj):\n                            keep = False\n                        else:\n                            keep = membername in self.options.private_members\n                    else:\n                        keep = False\n                else:\n                    if (self.options.members is ALL and\n                            is_filtered_inherited_member(membername, obj)):\n                        keep = False\n                    else:\n                        # ignore undocumented members if :undoc-members: is not given\n                        keep = has_doc or self.options.undoc_members\n\n                if isinstance(obj, ObjectMember) and obj.skipped:\n                    # forcedly skipped member (ex. a module attribute not defined in __all__)\n                    keep = False\n\n                # give the user a chance to decide whether this member\n                # should be skipped\n                if self.env.app:\n                    # let extensions preprocess docstrings\n                    skip_user = self.env.app.emit_firstresult(\n                        'autodoc-skip-member', self.objtype, membername, member,\n                        not keep, self.options)\n                    if skip_user is not None:\n                        keep = not skip_user\n            except Exception as exc:\n                logger.warning(__('autodoc: failed to determine %s.%s (%r) to be documented, '\n                                  'the following exception was raised:\\n%s'),\n                               self.name, membername, member, exc, type='autodoc')\n                keep = False\n\n            if keep:\n                ret.append((membername, member, isattr))\n\n        return ret\n\n    def document_members(self, all_members: bool = False) -> None:\n        \"\"\"Generate reST for member documentation.\n\n        If *all_members* is True, document all members, else those given by\n        *self.options.members*.\n        \"\"\"\n        # set current namespace for finding members\n        self.env.temp_data['autodoc:module'] = self.modname\n        if self.objpath:\n            self.env.temp_data['autodoc:class'] = self.objpath[0]\n\n        want_all = (all_members or\n                    self.options.inherited_members or\n                    self.options.members is ALL)\n        # find out which members are documentable\n        members_check_module, members = self.get_object_members(want_all)\n\n        # document non-skipped members\n        memberdocumenters: List[Tuple[Documenter, bool]] = []\n        for (mname, member, isattr) in self.filter_members(members, want_all):\n            classes = [cls for cls in self.documenters.values()\n                       if cls.can_document_member(member, mname, isattr, self)]\n            if not classes:\n                # don't know how to document this member\n                continue\n            # prefer the documenter with the highest priority\n            classes.sort(key=lambda cls: cls.priority)\n            # give explicitly separated module name, so that members\n            # of inner classes can be documented\n            full_mname = self.modname + '::' + '.'.join(self.objpath + [mname])\n            documenter = classes[-1](self.directive, full_mname, self.indent)\n            memberdocumenters.append((documenter, isattr))\n\n        member_order = self.options.member_order or self.config.autodoc_member_order\n        memberdocumenters = self.sort_members(memberdocumenters, member_order)\n\n        for documenter, isattr in memberdocumenters:\n            documenter.generate(\n                all_members=True, real_modname=self.real_modname,\n                check_module=members_check_module and not isattr)\n\n        # reset current objects\n        self.env.temp_data['autodoc:module'] = None\n        self.env.temp_data['autodoc:class'] = None\n\n    def sort_members(self, documenters: List[Tuple[\"Documenter\", bool]],\n                     order: str) -> List[Tuple[\"Documenter\", bool]]:\n        \"\"\"Sort the given member list.\"\"\"\n        if order == 'groupwise':\n            # sort by group; alphabetically within groups\n            documenters.sort(key=lambda e: (e[0].member_order, e[0].name))\n        elif order == 'bysource':\n            if self.analyzer:\n                # sort by source order, by virtue of the module analyzer\n                tagorder = self.analyzer.tagorder\n\n                def keyfunc(entry: Tuple[Documenter, bool]) -> int:\n                    fullname = entry[0].name.split('::')[1]\n                    return tagorder.get(fullname, len(tagorder))\n                documenters.sort(key=keyfunc)\n            else:\n                # Assume that member discovery order matches source order.\n                # This is a reasonable assumption in Python 3.6 and up, where\n                # module.__dict__ is insertion-ordered.\n                pass\n        else:  # alphabetical\n            documenters.sort(key=lambda e: e[0].name)\n\n        return documenters\n\n    def generate(self, more_content: Optional[StringList] = None, real_modname: str = None,\n                 check_module: bool = False, all_members: bool = False) -> None:\n        \"\"\"Generate reST for the object given by *self.name*, and possibly for\n        its members.\n\n        If *more_content* is given, include that content. If *real_modname* is\n        given, use that module name to find attribute docs. If *check_module* is\n        True, only generate if the object is defined in the module name it is\n        imported from. If *all_members* is True, document all members.\n        \"\"\"\n        if not self.parse_name():\n            # need a module to import\n            logger.warning(\n                __('don\\'t know which module to import for autodocumenting '\n                   '%r (try placing a \"module\" or \"currentmodule\" directive '\n                   'in the document, or giving an explicit module name)') %\n                self.name, type='autodoc')\n            return\n\n        # now, import the module and get object to document\n        if not self.import_object():\n            return\n\n        # If there is no real module defined, figure out which to use.\n        # The real module is used in the module analyzer to look up the module\n        # where the attribute documentation would actually be found in.\n        # This is used for situations where you have a module that collects the\n        # functions and classes of internal submodules.\n        guess_modname = self.get_real_modname()\n        self.real_modname: str = real_modname or guess_modname\n\n        # try to also get a source code analyzer for attribute docs\n        try:\n            self.analyzer = ModuleAnalyzer.for_module(self.real_modname)\n            # parse right now, to get PycodeErrors on parsing (results will\n            # be cached anyway)\n            self.analyzer.find_attr_docs()\n        except PycodeError as exc:\n            logger.debug('[autodoc] module analyzer failed: %s', exc)\n            # no source file -- e.g. for builtin and C modules\n            self.analyzer = None\n            # at least add the module.__file__ as a dependency\n            if hasattr(self.module, '__file__') and self.module.__file__:\n                self.directive.record_dependencies.add(self.module.__file__)\n        else:\n            self.directive.record_dependencies.add(self.analyzer.srcname)\n\n        if self.real_modname != guess_modname:\n            # Add module to dependency list if target object is defined in other module.\n            try:\n                analyzer = ModuleAnalyzer.for_module(guess_modname)\n                self.directive.record_dependencies.add(analyzer.srcname)\n            except PycodeError:\n                pass\n\n        docstrings: List[str] = sum(self.get_doc() or [], [])\n        if ismock(self.object) and not docstrings:\n            logger.warning(__('A mocked object is detected: %r'),\n                           self.name, type='autodoc')\n\n        # check __module__ of object (for members not given explicitly)\n        if check_module:\n            if not self.check_module():\n                return\n\n        sourcename = self.get_sourcename()\n\n        # make sure that the result starts with an empty line.  This is\n        # necessary for some situations where another directive preprocesses\n        # reST and no starting newline is present\n        self.add_line('', sourcename)\n\n        # format the object's signature, if any\n        try:\n            sig = self.format_signature()\n        except Exception as exc:\n            logger.warning(__('error while formatting signature for %s: %s'),\n                           self.fullname, exc, type='autodoc')\n            return\n\n        # generate the directive header and options, if applicable\n        self.add_directive_header(sig)\n        self.add_line('', sourcename)\n\n        # e.g. the module directive doesn't have content\n        self.indent += self.content_indent\n\n        # add all content (from docstrings, attribute docs etc.)\n        self.add_content(more_content)\n\n        # document members, if possible\n        self.document_members(all_members)\n\n\nclass ModuleDocumenter(Documenter):\n(C)             # etc. don't support a prepended module name\n            self.add_line('   :module: %s' % self.modname, sourcename)\n\n    def get_doc(self, ignore: int = None) -> Optional[List[List[str]]]:\n        \"\"\"Decode and return lines of the docstring(s) for the object.\n\n        When it returns None, autodoc-process-docstring will not be called for this\n        object.\n        \"\"\"\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n        docstring = getdoc(self.object, self.get_attr, self.config.autodoc_inherit_docstrings,\n                           self.parent, self.object_name)\n        if docstring:\n            tab_width = self.directive.state.document.settings.tab_width\n            return [prepare_docstring(docstring, ignore, tab_width)]\n        return []\n\n    def process_doc(self, docstrings: List[List[str]]) -> Iterator[str]:\n        \"\"\"Let the user process the docstrings before adding them.\"\"\"\n        for docstringlines in docstrings:\n            if self.env.app:\n                # let extensions preprocess docstrings\n                self.env.app.emit('autodoc-process-docstring',\n                                  self.objtype, self.fullname, self.object,\n                                  self.options, docstringlines)\n\n                if docstringlines and docstringlines[-1] != '':\n                    # append a blank line to the end of the docstring\n                    docstringlines.append('')\n\n            yield from docstringlines\n\n    def get_sourcename(self) -> str:\n        if (inspect.safe_getattr(self.object, '__module__', None) and\n                inspect.safe_getattr(self.object, '__qualname__', None)):\n            # Get the correct location of docstring from self.object\n            # to support inherited methods\n            fullname = '%s.%s' % (self.object.__module__, self.object.__qualname__)\n        else:\n            fullname = self.fullname\n\n        if self.analyzer:\n            return '%s:docstring of %s' % (self.analyzer.srcname, fullname)\n        else:\n            return 'docstring of %s' % fullname\n\n    def add_content(self, more_content: Optional[StringList], no_docstring: bool = False\n                    ) -> None:\n        \"\"\"Add content from docstrings, attribute documentation and user.\"\"\"\n        if no_docstring:\n            warnings.warn(\"The 'no_docstring' argument to %s.add_content() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        # set sourcename and add content from attribute documentation\n        sourcename = self.get_sourcename()\n        if self.analyzer:\n            attr_docs = self.analyzer.find_attr_docs()\n            if self.objpath:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if key in attr_docs:\n                    no_docstring = True\n                    # make a copy of docstring for attributes to avoid cache\n                    # the change of autodoc-process-docstring event.\n                    docstrings = [list(attr_docs[key])]\n\n                    for i, line in enumerate(self.process_doc(docstrings)):\n                        self.add_line(line, sourcename, i)\n\n        # add content from docstrings\n        if not no_docstring:\n            docstrings = self.get_doc()\n            if docstrings is None:\n                # Do not call autodoc-process-docstring on get_doc() returns None.\n                pass\n            else:\n                if not docstrings:\n                    # append at least a dummy docstring, so that the event\n                    # autodoc-process-docstring is fired and can add some\n                    # content if desired\n                    docstrings.append([])\n                for i, line in enumerate(self.process_doc(docstrings)):\n                    self.add_line(line, sourcename, i)\n\n        # add additional content (e.g. from document), if present\n        if more_content:\n            for line, src in zip(more_content.data, more_content.items):\n                self.add_line(line, src[0], src[1])\n\n    def get_object_members(self, want_all: bool) -> Tuple[bool, ObjectMembers]:\n        \"\"\"Return `(members_check_module, members)` where `members` is a\n        list of `(membername, member)` pairs of the members of *self.object*.\n\n        If *want_all* is True, return all members.  Else, only return those\n        members given by *self.options.members* (which may also be None).\n        \"\"\"\n        warnings.warn('The implementation of Documenter.get_object_members() will be '\n                      'removed from Sphinx-6.0.', RemovedInSphinx60Warning)\n        members = get_object_members(self.object, self.objpath, self.get_attr, self.analyzer)\n        if not want_all:\n            if not self.options.members:\n                return False, []  # type: ignore\n            # specific members given\n            selected = []\n            for name in self.options.members:  # type: str\n                if name in members:\n                    selected.append((name, members[name].value))\n                else:\n                    logger.warning(__('missing attribute %s in object %s') %\n                                   (name, self.fullname), type='autodoc')\n            return False, selected\n        elif self.options.inherited_members:\n            return False, [(m.name, m.value) for m in members.values()]\n        else:\n            return False, [(m.name, m.value) for m in members.values()\n                           if m.directly_defined]\n\n    def filter_members(self, members: ObjectMembers, want_all: bool\n                       ) -> List[Tuple[str, Any, bool]]:\n        \"\"\"Filter the given member list.\n\n        Members are skipped if\n\n        - they are private (except if given explicitly or the private-members\n          option is set)\n        - they are special methods (except if given explicitly or the\n          special-members option is set)\n        - they are undocumented (except if the undoc-members option is set)\n\n        The user can override the skipping decision by connecting to the\n        ``autodoc-skip-member`` event.\n        \"\"\"\n        def is_filtered_inherited_member(name: str, obj: Any) -> bool:\n            if inspect.isclass(self.object):\n                for cls in self.object.__mro__:\n                    if cls.__name__ == self.options.inherited_members and cls != self.object:\n                        # given member is a member of specified *super class*\n                        return True\n                    elif name in cls.__dict__:\n                        return False\n                    elif name in self.get_attr(cls, '__annotations__', {}):\n                        return False\n                    elif isinstance(obj, ObjectMember) and obj.class_ is cls:\n                        return False\n\n            return False\n\n        ret = []\n\n        # search for members in source code too\n        namespace = '.'.join(self.objpath)  # will be empty for modules\n\n        if self.analyzer:\n            attr_docs = self.analyzer.find_attr_docs()\n        else:\n            attr_docs = {}\n\n        # process members and determine which to skip\n        for obj in members:\n            try:\n                membername, member = obj\n                # if isattr is True, the member is documented as an attribute\n                if member is INSTANCEATTR:\n                    isattr = True\n                elif (namespace, membername) in attr_docs:\n                    isattr = True\n                else:\n                    isattr = False\n\n                doc = getdoc(member, self.get_attr, self.config.autodoc_inherit_docstrings,\n                             self.object, membername)\n                if not isinstance(doc, str):\n                    # Ignore non-string __doc__\n                    doc = None\n\n                # if the member __doc__ is the same as self's __doc__, it's just\n                # inherited and therefore not the member's doc\n                cls = self.get_attr(member, '__class__', None)\n                if cls:\n                    cls_doc = self.get_attr(cls, '__doc__', None)\n                    if cls_doc == doc:\n                        doc = None\n\n                if isinstance(obj, ObjectMember) and obj.docstring:\n                    # hack for ClassDocumenter to inject docstring via ObjectMember\n                    doc = obj.docstring\n\n                doc, metadata = separate_metadata(doc)\n                has_doc = bool(doc)\n\n                if 'private' in metadata:\n                    # consider a member private if docstring has \"private\" metadata\n                    isprivate = True\n                elif 'public' in metadata:\n                    # consider a member public if docstring has \"public\" metadata\n                    isprivate = False\n                else:\n                    isprivate = membername.startswith('_')\n\n                keep = False\n                if ismock(member) and (namespace, membername) not in attr_docs:\n                    # mocked module or object\n                    pass\n                elif (self.options.exclude_members and\n                      membername in self.options.exclude_members):\n                    # remove members given by exclude-members\n                    keep = False\n                elif want_all and special_member_re.match(membername):\n                    # special __methods__\n                    if (self.options.special_members and\n                            membername in self.options.special_members):\n                        if membername == '__doc__':\n                            keep = False\n                        elif is_filtered_inherited_member(membername, obj):\n                            keep = False\n                        else:\n                            keep = has_doc or self.options.undoc_members\n                    else:\n                        keep = False\n                elif (namespace, membername) in attr_docs:\n                    if want_all and isprivate:\n                        if self.options.private_members is None:\n                            keep = False\n                        else:\n                            keep = membername in self.options.private_members\n                    else:\n                        # keep documented attributes\n                        keep = True\n                elif want_all and isprivate:\n                    if has_doc or self.options.undoc_members:\n                        if self.options.private_members is None:\n                            keep = False\n                        elif is_filtered_inherited_member(membername, obj):\n                            keep = False\n                        else:\n                            keep = membername in self.options.private_members\n                    else:\n                        keep = False\n                else:\n                    if (self.options.members is ALL and\n                            is_filtered_inherited_member(membername, obj)):\n                        keep = False\n                    else:\n                        # ignore undocumented members if :undoc-members: is not given\n                        keep = has_doc or self.options.undoc_members\n\n                if isinstance(obj, ObjectMember) and obj.skipped:\n                    # forcedly skipped member (ex. a module attribute not defined in __all__)\n                    keep = False\n\n                # give the user a chance to decide whether this member\n                # should be skipped\n                if self.env.app:\n                    # let extensions preprocess docstrings\n                    skip_user = self.env.app.emit_firstresult(\n                        'autodoc-skip-member', self.objtype, membername, member,\n                        not keep, self.options)\n                    if skip_user is not None:\n                        keep = not skip_user\n            except Exception as exc:\n                logger.warning(__('autodoc: failed to determine %s.%s (%r) to be documented, '\n                                  'the following exception was raised:\\n%s'),\n                               self.name, membername, member, exc, type='autodoc')\n                keep = False\n\n            if keep:\n                ret.append((membername, member, isattr))\n\n        return ret\n\n    def document_members(self, all_members: bool = False) -> None:\n        \"\"\"Generate reST for member documentation.\n\n        If *all_members* is True, document all members, else those given by\n        *self.options.members*.\n        \"\"\"\n        # set current namespace for finding members\n        self.env.temp_data['autodoc:module'] = self.modname\n        if self.objpath:\n            self.env.temp_data['autodoc:class'] = self.objpath[0]\n\n        want_all = (all_members or\n                    self.options.inherited_members or\n                    self.options.members is ALL)\n        # find out which members are documentable\n        members_check_module, members = self.get_object_members(want_all)\n\n        # document non-skipped members\n        memberdocumenters: List[Tuple[Documenter, bool]] = []\n        for (mname, member, isattr) in self.filter_members(members, want_all):\n            classes = [cls for cls in self.documenters.values()\n                       if cls.can_document_member(member, mname, isattr, self)]\n            if not classes:\n                # don't know how to document this member\n                continue\n            # prefer the documenter with the highest priority\n            classes.sort(key=lambda cls: cls.priority)\n            # give explicitly separated module name, so that members\n            # of inner classes can be documented\n            full_mname = self.modname + '::' + '.'.join(self.objpath + [mname])\n            documenter = classes[-1](self.directive, full_mname, self.indent)\n            memberdocumenters.append((documenter, isattr))\n\n        member_order = self.options.member_order or self.config.autodoc_member_order\n        memberdocumenters = self.sort_members(memberdocumenters, member_order)\n\n        for documenter, isattr in memberdocumenters:\n            documenter.generate(\n                all_members=True, real_modname=self.real_modname,\n                check_module=members_check_module and not isattr)\n\n        # reset current objects\n        self.env.temp_data['autodoc:module'] = None\n        self.env.temp_data['autodoc:class'] = None\n\n    def sort_members(self, documenters: List[Tuple[\"Documenter\", bool]],\n                     order: str) -> List[Tuple[\"Documenter\", bool]]:\n        \"\"\"Sort the given member list.\"\"\"\n        if order == 'groupwise':\n            # sort by group; alphabetically within groups\n            documenters.sort(key=lambda e: (e[0].member_order, e[0].name))\n        elif order == 'bysource':\n            if self.analyzer:\n                # sort by source order, by virtue of the module analyzer\n                tagorder = self.analyzer.tagorder\n\n                def keyfunc(entry: Tuple[Documenter, bool]) -> int:\n                    fullname = entry[0].name.split('::')[1]\n                    return tagorder.get(fullname, len(tagorder))\n                documenters.sort(key=keyfunc)\n            else:\n                # Assume that member discovery order matches source order.\n                # This is a reasonable assumption in Python 3.6 and up, where\n                # module.__dict__ is insertion-ordered.\n                pass\n        else:  # alphabetical\n            documenters.sort(key=lambda e: e[0].name)\n\n        return documenters\n\n    def generate(self, more_content: Optional[StringList] = None, real_modname: str = None,\n                 check_module: bool = False, all_members: bool = False) -> None:\n        \"\"\"Generate reST for the object given by *self.name*, and possibly for\n        its members.\n\n        If *more_content* is given, include that content. If *real_modname* is\n        given, use that module name to find attribute docs. If *check_module* is\n        True, only generate if the object is defined in the module name it is\n        imported from. If *all_members* is True, document all members.\n        \"\"\"\n        if not self.parse_name():\n            # need a module to import\n            logger.warning(\n                __('don\\'t know which module to import for autodocumenting '\n                   '%r (try placing a \"module\" or \"currentmodule\" directive '\n                   'in the document, or giving an explicit module name)') %\n                self.name, type='autodoc')\n            return\n\n        # now, import the module and get object to document\n        if not self.import_object():\n            return\n\n        # If there is no real module defined, figure out which to use.\n        # The real module is used in the module analyzer to look up the module\n        # where the attribute documentation would actually be found in.\n        # This is used for situations where you have a module that collects the\n        # functions and classes of internal submodules.\n        guess_modname = self.get_real_modname()\n        self.real_modname: str = real_modname or guess_modname\n\n        # try to also get a source code analyzer for attribute docs\n        try:\n            self.analyzer = ModuleAnalyzer.for_module(self.real_modname)\n            # parse right now, to get PycodeErrors on parsing (results will\n            # be cached anyway)\n            self.analyzer.find_attr_docs()\n        except PycodeError as exc:\n            logger.debug('[autodoc] module analyzer failed: %s', exc)\n            # no source file -- e.g. for builtin and C modules\n            self.analyzer = None\n            # at least add the module.__file__ as a dependency\n            if hasattr(self.module, '__file__') and self.module.__file__:\n                self.directive.record_dependencies.add(self.module.__file__)\n        else:\n            self.directive.record_dependencies.add(self.analyzer.srcname)\n\n        if self.real_modname != guess_modname:\n            # Add module to dependency list if target object is defined in other module.\n            try:\n                analyzer = ModuleAnalyzer.for_module(guess_modname)\n                self.directive.record_dependencies.add(analyzer.srcname)\n            except PycodeError:\n                pass\n\n        docstrings: List[str] = sum(self.get_doc() or [], [])\n        if ismock(self.object) and not docstrings:\n            logger.warning(__('A mocked object is detected: %r'),\n                           self.name, type='autodoc')\n\n        # check __module__ of object (for members not given explicitly)\n        if check_module:\n            if not self.check_module():\n                return\n\n        sourcename = self.get_sourcename()\n\n        # make sure that the result starts with an empty line.  This is\n        # necessary for some situations where another directive preprocesses\n        # reST and no starting newline is present\n        self.add_line('', sourcename)\n\n        # format the object's signature, if any\n        try:\n            sig = self.format_signature()\n        except Exception as exc:\n            logger.warning(__('error while formatting signature for %s: %s'),\n                           self.fullname, exc, type='autodoc')\n            return\n\n        # generate the directive header and options, if applicable\n        self.add_directive_header(sig)\n        self.add_line('', sourcename)\n\n        # e.g. the module directive doesn't have content\n        self.indent += self.content_indent\n\n        # add all content (from docstrings, attribute docs etc.)\n        self.add_content(more_content)\n\n        # document members, if possible\n        self.document_members(all_members)\n\n\nclass ModuleDocumenter(Documenter):\n    \"\"\"\n    Specialized Documenter subclass for modules.\n    \"\"\"\n    objtype = 'module'\n    content_indent = ''\n    titles_allowed = True\n\n    option_spec: OptionSpec = {\n        'members': members_option, 'undoc-members': bool_option,\n        'noindex': bool_option, 'inherited-members': inherited_members_option,\n        'show-inheritance': bool_option, 'synopsis': identity,\n        'platform': identity, 'deprecated': bool_option,\n        'member-order': member_order_option, 'exclude-members': exclude_members_option,\n        'private-members': members_option, 'special-members': members_option,\n        'imported-members': bool_option, 'ignore-module-all': bool_option,\n        'no-value': bool_option,\n    }\n\n    def __init__(self, *args: Any) -> None:\n        super().__init__(*args)\n        merge_members_option(self.options)\n        self.__all__: Optional[Sequence[str]] = None\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        # don't document submodules automatically\n        return False\n\n    def resolve_name(self, modname: str, parents: Any, path: str, base: Any\n                     ) -> Tuple[str, List[str]]:\n        if modname is not None:\n            logger.warning(__('\"::\" in automodule name doesn\\'t make sense'),\n                           type='autodoc')\n        return (path or '') + base, []\n\n    def parse_name(self) -> bool:\n        ret = super().parse_name()\n        if self.args or self.retann:\n            logger.warning(__('signature arguments or return annotation '\n                              'given for automodule %s') % self.fullname,\n                           type='autodoc')\n        return ret\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        ret = super().import_object(raiseerror)\n\n        try:\n            if not self.options.ignore_module_all:\n                self.__all__ = inspect.getall(self.object)\n        except ValueError as exc:\n            # invalid __all__ found.\n            logger.warning(__('__all__ should be a list of strings, not %r '\n                              '(in module %s) -- ignoring __all__') %\n                           (exc.args[0], self.fullname), type='autodoc')\n\n        return ret\n\n    def add_directive_header(self, sig: str) -> None:\n        Documenter.add_directive_header(self, sig)\n\n        sourcename = self.get_sourcename()\n\n        # add some module-specific options\n        if self.options.synopsis:\n            self.add_line('   :synopsis: ' + self.options.synopsis, sourcename)\n        if self.options.platform:\n            self.add_line('   :platform: ' + self.options.platform, sourcename)\n        if self.options.deprecated:\n            self.add_line('   :deprecated:', sourcename)\n\n    def get_module_members(self) -> Dict[str, ObjectMember]:\n        \"\"\"Get members of target module.\"\"\"\n        if self.analyzer:\n            attr_docs = self.analyzer.attr_docs\n        else:\n            attr_docs = {}\n\n        members: Dict[str, ObjectMember] = {}\n        for name in dir(self.object):\n            try:\n                value = safe_getattr(self.object, name, None)\n                if ismock(value):\n                    value = undecorate(value)\n                docstring = attr_docs.get(('', name), [])\n                members[name] = ObjectMember(name, value, docstring=\"\\n\".join(docstring))\n            except AttributeError:\n                continue\n\n        # annotation only member (ex. attr: int)\n        for name in inspect.getannotations(self.object):\n            if name not in members:\n                docstring = attr_docs.get(('', name), [])\n                members[name] = ObjectMember(name, INSTANCEATTR,\n                                             docstring=\"\\n\".join(docstring))\n\n        return members\n\n    def get_object_members(self, want_all: bool) -> Tuple[bool, ObjectMembers]:\n        members = self.get_module_members()\n        if want_all:\n            if self.__all__ is None:\n                # for implicit module members, check __module__ to avoid\n                # documenting imported objects\n                return True, list(members.values())\n            else:\n                for member in members.values():\n                    if member.__name__ not in self.__all__:\n                        member.skipped = True\n\n                return False, list(members.values())\n        else:\n            memberlist = self.options.members or []\n            ret = []\n            for name in memberlist:\n                if name in members:\n                    ret.append(members[name])\n                else:\n                    logger.warning(__('missing attribute mentioned in :members: option: '\n                                      'module %s, attribute %s') %\n                                   (safe_getattr(self.object, '__name__', '???'), name),\n                                   type='autodoc')\n            return False, ret\n\n    def sort_members(self, documenters: List[Tuple[\"Documenter\", bool]],\n                     order: str) -> List[Tuple[\"Documenter\", bool]]:\n        if order == 'bysource' and self.__all__:\n            # Sort alphabetically first (for members not listed on the __all__)\n            documenters.sort(key=lambda e: e[0].name)\n\n            # Sort by __all__\n            def keyfunc(entry: Tuple[Documenter, bool]) -> int:\n                name = entry[0].name.split('::')[1]\n                if self.__all__ and name in self.__all__:\n                    return self.__all__.index(name)\n                else:\n                    return len(self.__all__)\n            documenters.sort(key=keyfunc)\n\n            return documenters\n        else:\n            return super().sort_members(documenters, order)\n\n\nclass ModuleLevelDocumenter(Documenter):\n    \"\"\"\n(D)     def copy(self) -> \"Options\":\n        return Options(super().copy())\n\n    def __getattr__(self, name: str) -> Any:\n        try:\n            return self[name.replace('_', '-')]\n        except KeyError:\n            return None\n\n\nclass ObjectMember(tuple):\n    \"\"\"A member of object.\n\n    This is used for the result of `Documenter.get_object_members()` to\n    represent each member of the object.\n\n    .. Note::\n\n       An instance of this class behaves as a tuple of (name, object)\n       for compatibility to old Sphinx.  The behavior will be dropped\n       in the future.  Therefore extensions should not use the tuple\n       interface.\n    \"\"\"\n\n    def __new__(cls, name: str, obj: Any, **kwargs: Any) -> Any:\n        return super().__new__(cls, (name, obj))  # type: ignore\n\n    def __init__(self, name: str, obj: Any, docstring: Optional[str] = None,\n                 class_: Any = None, skipped: bool = False) -> None:\n        self.__name__ = name\n        self.object = obj\n        self.docstring = docstring\n        self.skipped = skipped\n        self.class_ = class_\n\n\nObjectMembers = Union[List[ObjectMember], List[Tuple[str, Any]]]\n\n\nclass Documenter:\n    \"\"\"\n    A Documenter knows how to autodocument a single object type.  When\n    registered with the AutoDirective, it will be used to document objects\n    of that type when needed by autodoc.\n\n    Its *objtype* attribute selects what auto directive it is assigned to\n    (the directive name is 'auto' + objtype), and what directive it generates\n    by default, though that can be overridden by an attribute called\n    *directivetype*.\n\n    A Documenter has an *option_spec* that works like a docutils directive's;\n    in fact, it will be used to parse an auto directive's options that matches\n    the Documenter.\n    \"\"\"\n    #: name by which the directive is called (auto...) and the default\n    #: generated directive name\n    objtype = 'object'\n    #: indentation by which to indent the directive content\n    content_indent = '   '\n    #: priority if multiple documenters return True from can_document_member\n    priority = 0\n    #: order if autodoc_member_order is set to 'groupwise'\n    member_order = 0\n    #: true if the generated content may contain titles\n    titles_allowed = False\n\n    option_spec: OptionSpec = {\n        'noindex': bool_option\n    }\n\n    def get_attr(self, obj: Any, name: str, *defargs: Any) -> Any:\n        \"\"\"getattr() override for types such as Zope interfaces.\"\"\"\n        return autodoc_attrgetter(self.env.app, obj, name, *defargs)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"Called to see if a member can be documented by this Documenter.\"\"\"\n        raise NotImplementedError('must be implemented in subclasses')\n\n    def __init__(self, directive: \"DocumenterBridge\", name: str, indent: str = '') -> None:\n        self.directive = directive\n        self.config: Config = directive.env.config\n        self.env: BuildEnvironment = directive.env\n        self.options = directive.genopt\n        self.name = name\n        self.indent = indent\n        # the module and object path within the module, and the fully\n        # qualified name (all set after resolve_name succeeds)\n        self.modname: str = None\n        self.module: ModuleType = None\n        self.objpath: List[str] = None\n        self.fullname: str = None\n        # extra signature items (arguments and return annotation,\n        # also set after resolve_name succeeds)\n        self.args: str = None\n        self.retann: str = None\n        # the object to document (set after import_object succeeds)\n        self.object: Any = None\n        self.object_name: str = None\n        # the parent/owner of the object to document\n        self.parent: Any = None\n        # the module analyzer to get at attribute docs, or None\n        self.analyzer: ModuleAnalyzer = None\n\n    @property\n    def documenters(self) -> Dict[str, Type[\"Documenter\"]]:\n        \"\"\"Returns registered Documenter classes\"\"\"\n        return self.env.app.registry.documenters\n\n    def add_line(self, line: str, source: str, *lineno: int) -> None:\n        \"\"\"Append one line of generated reST to the output.\"\"\"\n        if line.strip():  # not a blank line\n            self.directive.result.append(self.indent + line, source, *lineno)\n        else:\n            self.directive.result.append('', source, *lineno)\n\n    def resolve_name(self, modname: str, parents: Any, path: str, base: Any\n                     ) -> Tuple[str, List[str]]:\n        \"\"\"Resolve the module and name of the object to document given by the\n        arguments and the current module/class.\n\n        Must return a pair of the module name and a chain of attributes; for\n        example, it would return ``('zipfile', ['ZipFile', 'open'])`` for the\n        ``zipfile.ZipFile.open`` method.\n        \"\"\"\n        raise NotImplementedError('must be implemented in subclasses')\n\n    def parse_name(self) -> bool:\n        \"\"\"Determine what module to import and what attribute to document.\n\n        Returns True and sets *self.modname*, *self.objpath*, *self.fullname*,\n        *self.args* and *self.retann* if parsing and resolving was successful.\n        \"\"\"\n        # first, parse the definition -- auto directives for classes and\n        # functions can contain a signature which is then used instead of\n        # an autogenerated one\n        try:\n            matched = py_ext_sig_re.match(self.name)\n            explicit_modname, path, base, args, retann = matched.groups()\n        except AttributeError:\n            logger.warning(__('invalid signature for auto%s (%r)') % (self.objtype, self.name),\n                           type='autodoc')\n            return False\n\n        # support explicit module and class name separation via ::\n        if explicit_modname is not None:\n            modname = explicit_modname[:-2]\n            parents = path.rstrip('.').split('.') if path else []\n        else:\n            modname = None\n            parents = []\n\n        with mock(self.config.autodoc_mock_imports):\n            self.modname, self.objpath = self.resolve_name(modname, parents, path, base)\n\n        if not self.modname:\n            return False\n\n        self.args = args\n        self.retann = retann\n        self.fullname = ((self.modname or '') +\n                         ('.' + '.'.join(self.objpath) if self.objpath else ''))\n        return True\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Import the object given by *self.modname* and *self.objpath* and set\n        it as *self.object*.\n\n        Returns True if successful, False if an error occurred.\n        \"\"\"\n        with mock(self.config.autodoc_mock_imports):\n            try:\n                ret = import_object(self.modname, self.objpath, self.objtype,\n                                    attrgetter=self.get_attr,\n                                    warningiserror=self.config.autodoc_warningiserror)\n                self.module, self.parent, self.object_name, self.object = ret\n                if ismock(self.object):\n                    self.object = undecorate(self.object)\n                return True\n            except ImportError as exc:\n                if raiseerror:\n                    raise\n                else:\n                    logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                    self.env.note_reread()\n                    return False\n\n    def get_real_modname(self) -> str:\n        \"\"\"Get the real module name of an object to document.\n\n        It can differ from the name of the module through which the object was\n        imported.\n        \"\"\"\n        return self.get_attr(self.object, '__module__', None) or self.modname\n\n    def check_module(self) -> bool:\n        \"\"\"Check if *self.object* is really defined in the module given by\n        *self.modname*.\n        \"\"\"\n        if self.options.imported_members:\n            return True\n\n        subject = inspect.unpartial(self.object)\n        modname = self.get_attr(subject, '__module__', None)\n        if modname and modname != self.modname:\n            return False\n        return True\n\n    def format_args(self, **kwargs: Any) -> str:\n        \"\"\"Format the argument signature of *self.object*.\n\n        Should return None if the object does not have a signature.\n        \"\"\"\n        return None\n\n    def format_name(self) -> str:\n        \"\"\"Format the name of *self.object*.\n\n        This normally should be something that can be parsed by the generated\n        directive, but doesn't need to be (Sphinx will display it unparsed\n        then).\n        \"\"\"\n        # normally the name doesn't contain the module (except for module\n        # directives of course)\n        return '.'.join(self.objpath) or self.modname\n\n    def _call_format_args(self, **kwargs: Any) -> str:\n        if kwargs:\n            try:\n                return self.format_args(**kwargs)\n            except TypeError:\n                # avoid chaining exceptions, by putting nothing here\n                pass\n\n        # retry without arguments for old documenters\n        return self.format_args()\n\n    def format_signature(self, **kwargs: Any) -> str:\n        \"\"\"Format the signature (arguments and return annotation) of the object.\n\n        Let the user process it via the ``autodoc-process-signature`` event.\n        \"\"\"\n        if self.args is not None:\n            # signature given explicitly\n            args = \"(%s)\" % self.args\n            retann = self.retann\n        else:\n            # try to introspect the signature\n            try:\n                retann = None\n                args = self._call_format_args(**kwargs)\n                if args:\n                    matched = re.match(r'^(\\(.*\\))\\s+->\\s+(.*)$', args)\n                    if matched:\n                        args = matched.group(1)\n                        retann = matched.group(2)\n            except Exception as exc:\n                logger.warning(__('error while formatting arguments for %s: %s'),\n                               self.fullname, exc, type='autodoc')\n                args = None\n\n        result = self.env.events.emit_firstresult('autodoc-process-signature',\n                                                  self.objtype, self.fullname,\n                                                  self.object, self.options, args, retann)\n        if result:\n            args, retann = result\n\n        if args is not None:\n            return args + ((' -> %s' % retann) if retann else '')\n        else:\n            return ''\n\n    def add_directive_header(self, sig: str) -> None:\n        \"\"\"Add the directive header and options to the generated content.\"\"\"\n        domain = getattr(self, 'domain', 'py')\n        directive = getattr(self, 'directivetype', self.objtype)\n        name = self.format_name()\n        sourcename = self.get_sourcename()\n\n        # one signature per line, indented by column\n        prefix = '.. %s:%s:: ' % (domain, directive)\n        for i, sig_line in enumerate(sig.split(\"\\n\")):\n            self.add_line('%s%s%s' % (prefix, name, sig_line),\n                          sourcename)\n            if i == 0:\n                prefix = \" \" * len(prefix)\n\n        if self.options.noindex:\n            self.add_line('   :noindex:', sourcename)\n        if self.objpath:\n            # Be explicit about the module, this is necessary since .. class::\n            # etc. don't support a prepended module name\n            self.add_line('   :module: %s' % self.modname, sourcename)\n\n    def get_doc(self, ignore: int = None) -> Optional[List[List[str]]]:\n        \"\"\"Decode and return lines of the docstring(s) for the object.\n\n        When it returns None, autodoc-process-docstring will not be called for this\n        object.\n        \"\"\"\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n        docstring = getdoc(self.object, self.get_attr, self.config.autodoc_inherit_docstrings,\n                           self.parent, self.object_name)\n        if docstring:\n            tab_width = self.directive.state.document.settings.tab_width\n            return [prepare_docstring(docstring, ignore, tab_width)]\n        return []\n\n    def process_doc(self, docstrings: List[List[str]]) -> Iterator[str]:\n        \"\"\"Let the user process the docstrings before adding them.\"\"\"\n        for docstringlines in docstrings:\n            if self.env.app:\n                # let extensions preprocess docstrings\n                self.env.app.emit('autodoc-process-docstring',\n                                  self.objtype, self.fullname, self.object,\n                                  self.options, docstringlines)\n\n                if docstringlines and docstringlines[-1] != '':\n                    # append a blank line to the end of the docstring\n                    docstringlines.append('')\n\n            yield from docstringlines\n\n    def get_sourcename(self) -> str:\n        if (inspect.safe_getattr(self.object, '__module__', None) and\n                inspect.safe_getattr(self.object, '__qualname__', None)):\n            # Get the correct location of docstring from self.object\n            # to support inherited methods\n            fullname = '%s.%s' % (self.object.__module__, self.object.__qualname__)\n        else:\n            fullname = self.fullname\n\n        if self.analyzer:\n            return '%s:docstring of %s' % (self.analyzer.srcname, fullname)\n        else:\n            return 'docstring of %s' % fullname\n\n    def add_content(self, more_content: Optional[StringList], no_docstring: bool = False\n                    ) -> None:\n        \"\"\"Add content from docstrings, attribute documentation and user.\"\"\"\n        if no_docstring:\n            warnings.warn(\"The 'no_docstring' argument to %s.add_content() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        # set sourcename and add content from attribute documentation\n        sourcename = self.get_sourcename()\n        if self.analyzer:\n            attr_docs = self.analyzer.find_attr_docs()\n            if self.objpath:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if key in attr_docs:\n                    no_docstring = True\n                    # make a copy of docstring for attributes to avoid cache\n                    # the change of autodoc-process-docstring event.\n                    docstrings = [list(attr_docs[key])]\n\n                    for i, line in enumerate(self.process_doc(docstrings)):\n                        self.add_line(line, sourcename, i)\n\n        # add content from docstrings\n        if not no_docstring:\n            docstrings = self.get_doc()\n            if docstrings is None:\n                # Do not call autodoc-process-docstring on get_doc() returns None.\n                pass\n            else:\n                if not docstrings:\n                    # append at least a dummy docstring, so that the event\n                    # autodoc-process-docstring is fired and can add some\n                    # content if desired\n                    docstrings.append([])\n                for i, line in enumerate(self.process_doc(docstrings)):\n                    self.add_line(line, sourcename, i)\n\n        # add additional content (e.g. from document), if present\n        if more_content:\n            for line, src in zip(more_content.data, more_content.items):\n                self.add_line(line, src[0], src[1])\n\n    def get_object_members(self, want_all: bool) -> Tuple[bool, ObjectMembers]:\n        \"\"\"Return `(members_check_module, members)` where `members` is a\n        list of `(membername, member)` pairs of the members of *self.object*.\n\n        If *want_all* is True, return all members.  Else, only return those\n        members given by *self.options.members* (which may also be None).\n        \"\"\"\n        warnings.warn('The implementation of Documenter.get_object_members() will be '\n                      'removed from Sphinx-6.0.', RemovedInSphinx60Warning)\n        members = get_object_members(self.object, self.objpath, self.get_attr, self.analyzer)\n        if not want_all:\n            if not self.options.members:\n                return False, []  # type: ignore\n            # specific members given\n            selected = []\n            for name in self.options.members:  # type: str\n                if name in members:\n                    selected.append((name, members[name].value))\n                else:\n                    logger.warning(__('missing attribute %s in object %s') %\n                                   (name, self.fullname), type='autodoc')\n            return False, selected\n        elif self.options.inherited_members:\n            return False, [(m.name, m.value) for m in members.values()]\n        else:\n            return False, [(m.name, m.value) for m in members.values()\n                           if m.directly_defined]\n\n    def filter_members(self, members: ObjectMembers, want_all: bool\n                       ) -> List[Tuple[str, Any, bool]]:\n        \"\"\"Filter the given member list.\n\n        Members are skipped if\n\n        - they are private (except if given explicitly or the private-members\n          option is set)\n        - they are special methods (except if given explicitly or the\n          special-members option is set)\n        - they are undocumented (except if the undoc-members option is set)\n\n        The user can override the skipping decision by connecting to the\n        ``autodoc-skip-member`` event.\n        \"\"\"\n        def is_filtered_inherited_member(name: str, obj: Any) -> bool:\n            if inspect.isclass(self.object):\n                for cls in self.object.__mro__:\n                    if cls.__name__ == self.options.inherited_members and cls != self.object:\n                        # given member is a member of specified *super class*\n                        return True\n                    elif name in cls.__dict__:\n                        return False\n                    elif name in self.get_attr(cls, '__annotations__', {}):\n                        return False\n                    elif isinstance(obj, ObjectMember) and obj.class_ is cls:\n                        return False\n\n            return False\n\n        ret = []\n\n        # search for members in source code too\n        namespace = '.'.join(self.objpath)  # will be empty for modules\n\n        if self.analyzer:\n            attr_docs = self.analyzer.find_attr_docs()\n        else:\n            attr_docs = {}\n\n        # process members and determine which to skip\n        for obj in members:\n            try:\n                membername, member = obj\n                # if isattr is True, the member is documented as an attribute\n                if member is INSTANCEATTR:\n                    isattr = True\n                elif (namespace, membername) in attr_docs:\n                    isattr = True\n                else:\n                    isattr = False\n\n                doc = getdoc(member, self.get_attr, self.config.autodoc_inherit_docstrings,\n                             self.object, membername)\n                if not isinstance(doc, str):\n                    # Ignore non-string __doc__\n                    doc = None\n\n                # if the member __doc__ is the same as self's __doc__, it's just\n                # inherited and therefore not the member's doc\n                cls = self.get_attr(member, '__class__', None)\n                if cls:\n                    cls_doc = self.get_attr(cls, '__doc__', None)\n                    if cls_doc == doc:\n                        doc = None\n\n                if isinstance(obj, ObjectMember) and obj.docstring:\n                    # hack for ClassDocumenter to inject docstring via ObjectMember\n                    doc = obj.docstring\n\n                doc, metadata = separate_metadata(doc)\n                has_doc = bool(doc)\n\n                if 'private' in metadata:\n                    # consider a member private if docstring has \"private\" metadata\n                    isprivate = True\n                elif 'public' in metadata:\n                    # consider a member public if docstring has \"public\" metadata\n                    isprivate = False\n                else:\n                    isprivate = membername.startswith('_')\n\n                keep = False\n                if ismock(member) and (namespace, membername) not in attr_docs:\n                    # mocked module or object\n                    pass\n                elif (self.options.exclude_members and\n                      membername in self.options.exclude_members):\n                    # remove members given by exclude-members\n                    keep = False\n                elif want_all and special_member_re.match(membername):\n                    # special __methods__\n                    if (self.options.special_members and\n                            membername in self.options.special_members):\n                        if membername == '__doc__':\n                            keep = False\n                        elif is_filtered_inherited_member(membername, obj):\n                            keep = False\n                        else:\n                            keep = has_doc or self.options.undoc_members\n                    else:\n                        keep = False\n                elif (namespace, membername) in attr_docs:\n                    if want_all and isprivate:\n                        if self.options.private_members is None:\n                            keep = False\n                        else:\n                            keep = membername in self.options.private_members\n                    else:\n                        # keep documented attributes\n                        keep = True\n                elif want_all and isprivate:\n                    if has_doc or self.options.undoc_members:\n                        if self.options.private_members is None:\n                            keep = False\n                        elif is_filtered_inherited_member(membername, obj):\n                            keep = False\n                        else:\n                            keep = membername in self.options.private_members\n                    else:\n                        keep = False\n                else:\n                    if (self.options.members is ALL and\n                            is_filtered_inherited_member(membername, obj)):\n                        keep = False\n                    else:\n                        # ignore undocumented members if :undoc-members: is not given\n                        keep = has_doc or self.options.undoc_members\n\n                if isinstance(obj, ObjectMember) and obj.skipped:\n                    # forcedly skipped member (ex. a module attribute not defined in __all__)\n                    keep = False\n\n                # give the user a chance to decide whether this member\n                # should be skipped\n                if self.env.app:\n                    # let extensions preprocess docstrings\n                    skip_user = self.env.app.emit_firstresult(\n                        'autodoc-skip-member', self.objtype, membername, member,\n                        not keep, self.options)\n                    if skip_user is not None:\n                        keep = not skip_user\n            except Exception as exc:\n                logger.warning(__('autodoc: failed to determine %s.%s (%r) to be documented, '\n                                  'the following exception was raised:\\n%s'),\n                               self.name, membername, member, exc, type='autodoc')\n                keep = False\n\n            if keep:\n                ret.append((membername, member, isattr))\n\n        return ret\n\n    def document_members(self, all_members: bool = False) -> None:\n        \"\"\"Generate reST for member documentation.\n\n        If *all_members* is True, document all members, else those given by\n        *self.options.members*.\n        \"\"\"\n        # set current namespace for finding members\n        self.env.temp_data['autodoc:module'] = self.modname\n        if self.objpath:\n            self.env.temp_data['autodoc:class'] = self.objpath[0]\n\n        want_all = (all_members or\n                    self.options.inherited_members or\n                    self.options.members is ALL)\n        # find out which members are documentable\n        members_check_module, members = self.get_object_members(want_all)\n\n        # document non-skipped members\n        memberdocumenters: List[Tuple[Documenter, bool]] = []", "answer": "A"}
{"uuid": "07163acd-cf98-42f9-9c4b-023260a7eb76", "issue_statement": "Fix duplicated *args and **kwargs with autodoc_typehints\nFix duplicated *args and **kwargs with autodoc_typehints\r\n\r\n### Bugfix\r\n- Bugfix\r\n\r\n### Detail\r\nConsider this\r\n```python\r\nclass _ClassWithDocumentedInitAndStarArgs:\r\n    \"\"\"Class docstring.\"\"\"\r\n\r\n    def __init__(self, x: int, *args: int, **kwargs: int) -> None:\r\n        \"\"\"Init docstring.\r\n\r\n        :param x: Some integer\r\n        :param *args: Some integer\r\n        :param **kwargs: Some integer\r\n        \"\"\"\r\n```\r\nwhen using the autodoc extension and the setting `autodoc_typehints = \"description\"`.\r\n\r\nWIth sphinx 4.2.0, the current output is\r\n```\r\nClass docstring.\r\n\r\n   Parameters:\r\n      * **x** (*int*) --\r\n\r\n      * **args** (*int*) --\r\n\r\n      * **kwargs** (*int*) --\r\n\r\n   Return type:\r\n      None\r\n\r\n   __init__(x, *args, **kwargs)\r\n\r\n      Init docstring.\r\n\r\n      Parameters:\r\n         * **x** (*int*) -- Some integer\r\n\r\n         * ***args** --\r\n\r\n           Some integer\r\n\r\n         * ****kwargs** --\r\n\r\n           Some integer\r\n\r\n         * **args** (*int*) --\r\n\r\n         * **kwargs** (*int*) --\r\n\r\n      Return type:\r\n         None\r\n```\r\nwhere the *args and **kwargs are duplicated and incomplete.\r\n\r\nThe expected output is\r\n```\r\n  Class docstring.\r\n\r\n   Parameters:\r\n      * **x** (*int*) --\r\n\r\n      * ***args** (*int*) --\r\n\r\n      * ****kwargs** (*int*) --\r\n\r\n   Return type:\r\n      None\r\n\r\n   __init__(x, *args, **kwargs)\r\n\r\n      Init docstring.\r\n\r\n      Parameters:\r\n         * **x** (*int*) -- Some integer\r\n\r\n         * ***args** (*int*) --\r\n\r\n           Some integer\r\n\r\n         * ****kwargs** (*int*) --\r\n\r\n           Some integer\r\n\r\n      Return type:\r\n         None\r\n\r\n```", "choices": "(A)         parts = re.split(' +', field_name)\n        if parts[0] == 'param':\n            if len(parts) == 2:\n                # :param xxx:\n                arg = arguments.setdefault(parts[1], {})\n                arg['param'] = True\n            elif len(parts) > 2:\n                # :param xxx yyy:\n                name = ' '.join(parts[2:])\n                arg = arguments.setdefault(name, {})\n                arg['param'] = True\n                arg['type'] = True\n        elif parts[0] == 'type':\n            name = ' '.join(parts[1:])\n            arg = arguments.setdefault(name, {})\n            arg['type'] = True\n        elif parts[0] == 'rtype':\n            arguments['return'] = {'type': True}\n\n    for name, annotation in annotations.items():\n        if name == 'return':\n            continue\n\n        arg = arguments.get(name, {})\n        if not arg.get('type'):\n            field = nodes.field()\n            field += nodes.field_name('', 'type ' + name)\n            field += nodes.field_body('', nodes.paragraph('', annotation))\n            node += field\n        if not arg.get('param'):\n            field = nodes.field()\n            field += nodes.field_name('', 'param ' + name)\n            field += nodes.field_body('', nodes.paragraph('', ''))\n            node += field\n\n    if 'return' in annotations and 'return' not in arguments:\n        annotation = annotations['return']\n        if annotation == 'None' and suppress_rtype:\n            return\n\n        field = nodes.field()\n        field += nodes.field_name('', 'rtype')\n        field += nodes.field_body('', nodes.paragraph('', annotation))\n        node += field\n\n\ndef augment_descriptions_with_types(\n    node: nodes.field_list,\n    annotations: Dict[str, str],\n    force_rtype: bool\n) -> None:\n    fields = cast(Iterable[nodes.field], node)\n    has_description = set()  # type: Set[str]\n    has_type = set()  # type: Set[str]\n    for field in fields:\n        field_name = field[0].astext()\n        parts = re.split(' +', field_name)\n        if parts[0] == 'param':\n            if len(parts) == 2:\n                # :param xxx:\n                has_description.add(parts[1])\n            elif len(parts) > 2:\n                # :param xxx yyy:\n                name = ' '.join(parts[2:])\n                has_description.add(name)\n                has_type.add(name)\n        elif parts[0] == 'type':\n            name = ' '.join(parts[1:])\n            has_type.add(name)\n        elif parts[0] in ('return', 'returns'):\n            has_description.add('return')\n        elif parts[0] == 'rtype':\n            has_type.add('return')\n\n    # Add 'type' for parameters with a description but no declared type.\n    for name in annotations:\n        if name in ('return', 'returns'):\n            continue\n        if name in has_description and name not in has_type:\n(B)         field_lists = [n for n in contentnode if isinstance(n, nodes.field_list)]\n        if field_lists == []:\n            field_list = insert_field_list(contentnode)\n            field_lists.append(field_list)\n\n        for field_list in field_lists:\n            if app.config.autodoc_typehints_description_target == \"all\":\n                if objtype == 'class':\n                    modify_field_list(field_list, annotations[fullname], suppress_rtype=True)\n                else:\n                    modify_field_list(field_list, annotations[fullname])\n            elif app.config.autodoc_typehints_description_target == \"documented_params\":\n                augment_descriptions_with_types(\n                    field_list, annotations[fullname], force_rtype=True\n                )\n            else:\n                augment_descriptions_with_types(\n                    field_list, annotations[fullname], force_rtype=False\n                )\n\n\ndef insert_field_list(node: Element) -> nodes.field_list:\n    field_list = nodes.field_list()\n    desc = [n for n in node if isinstance(n, addnodes.desc)]\n    if desc:\n        # insert just before sub object descriptions (ex. methods, nested classes, etc.)\n        index = node.index(desc[0])\n        node.insert(index - 1, [field_list])\n    else:\n        node += field_list\n\n    return field_list\n\n\ndef modify_field_list(node: nodes.field_list, annotations: Dict[str, str],\n                      suppress_rtype: bool = False) -> None:\n    arguments: Dict[str, Dict[str, bool]] = {}\n    fields = cast(Iterable[nodes.field], node)\n    for field in fields:\n        field_name = field[0].astext()\n        parts = re.split(' +', field_name)\n        if parts[0] == 'param':\n            if len(parts) == 2:\n                # :param xxx:\n                arg = arguments.setdefault(parts[1], {})\n                arg['param'] = True\n            elif len(parts) > 2:\n                # :param xxx yyy:\n                name = ' '.join(parts[2:])\n                arg = arguments.setdefault(name, {})\n                arg['param'] = True\n                arg['type'] = True\n        elif parts[0] == 'type':\n            name = ' '.join(parts[1:])\n            arg = arguments.setdefault(name, {})\n            arg['type'] = True\n        elif parts[0] == 'rtype':\n            arguments['return'] = {'type': True}\n\n    for name, annotation in annotations.items():\n        if name == 'return':\n            continue\n\n        arg = arguments.get(name, {})\n        if not arg.get('type'):\n            field = nodes.field()\n            field += nodes.field_name('', 'type ' + name)\n            field += nodes.field_body('', nodes.paragraph('', annotation))\n            node += field\n        if not arg.get('param'):\n            field = nodes.field()\n            field += nodes.field_name('', 'param ' + name)\n            field += nodes.field_body('', nodes.paragraph('', ''))\n            node += field\n\n    if 'return' in annotations and 'return' not in arguments:\n        annotation = annotations['return']\n        if annotation == 'None' and suppress_rtype:\n            return\n(C) \ndef insert_field_list(node: Element) -> nodes.field_list:\n    field_list = nodes.field_list()\n    desc = [n for n in node if isinstance(n, addnodes.desc)]\n    if desc:\n        # insert just before sub object descriptions (ex. methods, nested classes, etc.)\n        index = node.index(desc[0])\n        node.insert(index - 1, [field_list])\n    else:\n        node += field_list\n\n    return field_list\n\n\ndef modify_field_list(node: nodes.field_list, annotations: Dict[str, str],\n                      suppress_rtype: bool = False) -> None:\n    arguments: Dict[str, Dict[str, bool]] = {}\n    fields = cast(Iterable[nodes.field], node)\n    for field in fields:\n        field_name = field[0].astext()\n        parts = re.split(' +', field_name)\n        if parts[0] == 'param':\n            if len(parts) == 2:\n                # :param xxx:\n                arg = arguments.setdefault(parts[1], {})\n                arg['param'] = True\n            elif len(parts) > 2:\n                # :param xxx yyy:\n                name = ' '.join(parts[2:])\n                arg = arguments.setdefault(name, {})\n                arg['param'] = True\n                arg['type'] = True\n        elif parts[0] == 'type':\n            name = ' '.join(parts[1:])\n            arg = arguments.setdefault(name, {})\n            arg['type'] = True\n        elif parts[0] == 'rtype':\n            arguments['return'] = {'type': True}\n\n    for name, annotation in annotations.items():\n        if name == 'return':\n            continue\n\n        arg = arguments.get(name, {})\n        if not arg.get('type'):\n            field = nodes.field()\n            field += nodes.field_name('', 'type ' + name)\n            field += nodes.field_body('', nodes.paragraph('', annotation))\n            node += field\n        if not arg.get('param'):\n            field = nodes.field()\n            field += nodes.field_name('', 'param ' + name)\n            field += nodes.field_body('', nodes.paragraph('', ''))\n            node += field\n\n    if 'return' in annotations and 'return' not in arguments:\n        annotation = annotations['return']\n        if annotation == 'None' and suppress_rtype:\n            return\n\n        field = nodes.field()\n        field += nodes.field_name('', 'rtype')\n        field += nodes.field_body('', nodes.paragraph('', annotation))\n        node += field\n\n\ndef augment_descriptions_with_types(\n    node: nodes.field_list,\n    annotations: Dict[str, str],\n    force_rtype: bool\n) -> None:\n    fields = cast(Iterable[nodes.field], node)\n    has_description = set()  # type: Set[str]\n    has_type = set()  # type: Set[str]\n    for field in fields:\n        field_name = field[0].astext()\n        parts = re.split(' +', field_name)\n        if parts[0] == 'param':\n            if len(parts) == 2:\n(D)         if name == 'return':\n            continue\n\n        arg = arguments.get(name, {})\n        if not arg.get('type'):\n            field = nodes.field()\n            field += nodes.field_name('', 'type ' + name)\n            field += nodes.field_body('', nodes.paragraph('', annotation))\n            node += field\n        if not arg.get('param'):\n            field = nodes.field()\n            field += nodes.field_name('', 'param ' + name)\n            field += nodes.field_body('', nodes.paragraph('', ''))\n            node += field\n\n    if 'return' in annotations and 'return' not in arguments:\n        annotation = annotations['return']\n        if annotation == 'None' and suppress_rtype:\n            return\n\n        field = nodes.field()\n        field += nodes.field_name('', 'rtype')\n        field += nodes.field_body('', nodes.paragraph('', annotation))\n        node += field\n\n\ndef augment_descriptions_with_types(\n    node: nodes.field_list,\n    annotations: Dict[str, str],\n    force_rtype: bool\n) -> None:\n    fields = cast(Iterable[nodes.field], node)\n    has_description = set()  # type: Set[str]\n    has_type = set()  # type: Set[str]\n    for field in fields:\n        field_name = field[0].astext()\n        parts = re.split(' +', field_name)\n        if parts[0] == 'param':\n            if len(parts) == 2:\n                # :param xxx:\n                has_description.add(parts[1])\n            elif len(parts) > 2:\n                # :param xxx yyy:\n                name = ' '.join(parts[2:])\n                has_description.add(name)\n                has_type.add(name)\n        elif parts[0] == 'type':\n            name = ' '.join(parts[1:])\n            has_type.add(name)\n        elif parts[0] in ('return', 'returns'):\n            has_description.add('return')\n        elif parts[0] == 'rtype':\n            has_type.add('return')\n\n    # Add 'type' for parameters with a description but no declared type.\n    for name in annotations:\n        if name in ('return', 'returns'):\n            continue\n        if name in has_description and name not in has_type:\n            field = nodes.field()\n            field += nodes.field_name('', 'type ' + name)\n            field += nodes.field_body('', nodes.paragraph('', annotations[name]))\n            node += field\n\n    # Add 'rtype' if 'return' is present and 'rtype' isn't.\n    if 'return' in annotations:\n        rtype = annotations['return']\n        if 'return' not in has_type and ('return' in has_description or\n                                         (force_rtype and rtype != \"None\")):\n            field = nodes.field()\n            field += nodes.field_name('', 'rtype')\n            field += nodes.field_body('', nodes.paragraph('', rtype))\n            node += field\n\n\ndef setup(app: Sphinx) -> Dict[str, Any]:\n    app.connect('autodoc-process-signature', record_typehints)\n    app.connect('object-description-transform', merge_typehints)\n", "answer": "D"}
{"uuid": "5d75d681-cf4a-4c21-b6bd-02117598ba2f", "issue_statement": "autosummary: The members variable for module template contains imported members\n**Describe the bug**\r\nautosummary: The members variable for module template contains imported members even if autosummary_imported_members is False.\r\n\r\n**To Reproduce**\r\n\r\n```\r\n# _templates/autosummary/module.rst\r\n{{ fullname | escape | underline }}\r\n\r\n.. automodule:: {{ fullname }}\r\n\r\n   .. autosummary::\r\n   {% for item in members %}\r\n      {{ item }}\r\n   {%- endfor %}\r\n\r\n```\r\n```\r\n# example.py\r\nimport os\r\n```\r\n```\r\n# index.rst\r\n.. autosummary::\r\n   :toctree: generated\r\n\r\n   example\r\n```\r\n```\r\n# conf.py\r\nautosummary_generate = True\r\nautosummary_imported_members = False\r\n```\r\n\r\nAs a result, I got following output:\r\n```\r\n# generated/example.rst\r\nexample\r\n=======\r\n\r\n.. automodule:: example\r\n\r\n   .. autosummary::\r\n\r\n      __builtins__\r\n      __cached__\r\n      __doc__\r\n      __file__\r\n      __loader__\r\n      __name__\r\n      __package__\r\n      __spec__\r\n      os\r\n```\r\n\r\n**Expected behavior**\r\nThe template variable `members` should not contain imported members when `autosummary_imported_members` is False.\r\n\r\n**Your project**\r\nNo\r\n\r\n**Screenshots**\r\nNo\r\n\r\n**Environment info**\r\n- OS: Mac\r\n- Python version: 3.8.2\r\n- Sphinx version: 3.1.0dev\r\n- Sphinx extensions:  sphinx.ext.autosummary\r\n- Extra tools: No\r\n\r\n**Additional context**\r\nNo", "choices": "(A) \"\"\"\n\nimport argparse\nimport locale\nimport os\nimport pkgutil\nimport pydoc\nimport re\nimport sys\nimport warnings\nfrom gettext import NullTranslations\nfrom os import path\nfrom typing import Any, Callable, Dict, List, NamedTuple, Set, Tuple, Union\n\nfrom jinja2 import TemplateNotFound\nfrom jinja2.sandbox import SandboxedEnvironment\n\nimport sphinx.locale\nfrom sphinx import __display_version__\nfrom sphinx import package_dir\nfrom sphinx.application import Sphinx\nfrom sphinx.builders import Builder\nfrom sphinx.config import Config\nfrom sphinx.deprecation import RemovedInSphinx40Warning, RemovedInSphinx50Warning\nfrom sphinx.ext.autodoc import Documenter\nfrom sphinx.ext.autosummary import import_by_name, get_documenter\nfrom sphinx.locale import __\nfrom sphinx.pycode import ModuleAnalyzer, PycodeError\nfrom sphinx.registry import SphinxComponentRegistry\nfrom sphinx.util import logging\nfrom sphinx.util import rst\nfrom sphinx.util import split_full_qualified_name\nfrom sphinx.util.inspect import safe_getattr\nfrom sphinx.util.osutil import ensuredir\nfrom sphinx.util.template import SphinxTemplateLoader\n\nif False:\n    # For type annotation\n    from typing import Type  # for python3.5.1\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass DummyApplication:\n    \"\"\"Dummy Application class for sphinx-autogen command.\"\"\"\n\n    def __init__(self, translator: NullTranslations) -> None:\n        self.config = Config()\n        self.registry = SphinxComponentRegistry()\n        self.messagelog = []  # type: List[str]\n        self.srcdir = \"/\"\n        self.translator = translator\n        self.verbosity = 0\n        self._warncount = 0\n        self.warningiserror = False\n\n        self.config.add('autosummary_context', {}, True, None)\n        self.config.init_values()\n\n    def emit_firstresult(self, *args: Any) -> None:\n        pass\n\n\nAutosummaryEntry = NamedTuple('AutosummaryEntry', [('name', str),\n                                                   ('path', str),\n                                                   ('template', str),\n                                                   ('recursive', bool)])\n\n\ndef setup_documenters(app: Any) -> None:\n    from sphinx.ext.autodoc import (\n        ModuleDocumenter, ClassDocumenter, ExceptionDocumenter, DataDocumenter,\n        FunctionDocumenter, MethodDocumenter, AttributeDocumenter,\n        InstanceAttributeDocumenter, DecoratorDocumenter, PropertyDocumenter,\n        SlotsAttributeDocumenter, DataDeclarationDocumenter,\n        SingledispatchFunctionDocumenter,\n    )\n    documenters = [\n        ModuleDocumenter, ClassDocumenter, ExceptionDocumenter, DataDocumenter,\n        FunctionDocumenter, MethodDocumenter, AttributeDocumenter,\n        InstanceAttributeDocumenter, DecoratorDocumenter, PropertyDocumenter,\n        SlotsAttributeDocumenter, DataDeclarationDocumenter,\n        SingledispatchFunctionDocumenter,\n    ]  # type: List[Type[Documenter]]\n    for documenter in documenters:\n        app.registry.add_documenter(documenter.objtype, documenter)\n\n\ndef _simple_info(msg: str) -> None:\n    print(msg)\n\n\ndef _simple_warn(msg: str) -> None:\n    print('WARNING: ' + msg, file=sys.stderr)\n\n\ndef _underline(title: str, line: str = '=') -> str:\n    if '\\n' in title:\n        raise ValueError('Can only underline single lines')\n    return title + '\\n' + line * len(title)\n\n\nclass AutosummaryRenderer:\n    \"\"\"A helper class for rendering.\"\"\"\n\n    def __init__(self, app: Union[Builder, Sphinx], template_dir: str = None) -> None:\n        if isinstance(app, Builder):\n            warnings.warn('The first argument for AutosummaryRenderer has been '\n                          'changed to Sphinx object',\n                          RemovedInSphinx50Warning, stacklevel=2)\n        if template_dir:\n            warnings.warn('template_dir argument for AutosummaryRenderer is deprecated.',\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        system_templates_path = [os.path.join(package_dir, 'ext', 'autosummary', 'templates')]\n        loader = SphinxTemplateLoader(app.srcdir, app.config.templates_path,\n                                      system_templates_path)\n\n        self.env = SandboxedEnvironment(loader=loader)\n        self.env.filters['escape'] = rst.escape\n        self.env.filters['e'] = rst.escape\n        self.env.filters['underline'] = _underline\n\n        if isinstance(app, (Sphinx, DummyApplication)):\n            if app.translator:\n                self.env.add_extension(\"jinja2.ext.i18n\")\n                self.env.install_gettext_translations(app.translator)  # type: ignore\n        elif isinstance(app, Builder):\n            if app.app.translator:\n                self.env.add_extension(\"jinja2.ext.i18n\")\n                self.env.install_gettext_translations(app.app.translator)  # type: ignore\n\n    def exists(self, template_name: str) -> bool:\n        \"\"\"Check if template file exists.\"\"\"\n        warnings.warn('AutosummaryRenderer.exists() is deprecated.',\n                      RemovedInSphinx50Warning, stacklevel=2)\n        try:\n            self.env.get_template(template_name)\n            return True\n        except TemplateNotFound:\n            return False\n\n    def render(self, template_name: str, context: Dict) -> str:\n        \"\"\"Render a template file.\"\"\"\n        try:\n            template = self.env.get_template(template_name)\n        except TemplateNotFound:\n            try:\n                # objtype is given as template_name\n                template = self.env.get_template('autosummary/%s.rst' % template_name)\n            except TemplateNotFound:\n                # fallback to base.rst\n                template = self.env.get_template('autosummary/base.rst')\n\n        return template.render(context)\n\n\n# -- Generating output ---------------------------------------------------------\n\n\ndef generate_autosummary_content(name: str, obj: Any, parent: Any,\n                                 template: AutosummaryRenderer, template_name: str,\n                                 imported_members: bool, app: Any,\n                                 recursive: bool, context: Dict) -> str:\n    doc = get_documenter(app, obj, parent)\n\n    def skip_member(obj: Any, name: str, objtype: str) -> bool:\n        try:\n            return app.emit_firstresult('autodoc-skip-member', objtype, name,\n                                        obj, False, {})\n        except Exception as exc:\n            logger.warning(__('autosummary: failed to determine %r to be documented, '\n                              'the following exception was raised:\\n%s'),\n                           name, exc, type='autosummary')\n            return False\n\n    def get_members(obj: Any, types: Set[str], include_public: List[str] = [],\n                    imported: bool = True) -> Tuple[List[str], List[str]]:\n        items = []  # type: List[str]\n        public = []  # type: List[str]\n        for name in dir(obj):\n            try:\n                value = safe_getattr(obj, name)\n            except AttributeError:\n                continue\n            documenter = get_documenter(app, value, obj)\n            if documenter.objtype in types:\n                # skip imported members if expected\n                if imported or getattr(value, '__module__', None) == obj.__name__:\n                    skipped = skip_member(value, name, documenter.objtype)\n                    if skipped is True:\n                        pass\n                    elif skipped is False:\n                        # show the member forcedly\n                        items.append(name)\n                        public.append(name)\n                    else:\n                        items.append(name)\n                        if name in include_public or not name.startswith('_'):\n                            # considers member as public\n                            public.append(name)\n        return public, items\n\n    def get_module_attrs(members: Any) -> Tuple[List[str], List[str]]:\n        \"\"\"Find module attributes with docstrings.\"\"\"\n        attrs, public = [], []\n        try:\n            analyzer = ModuleAnalyzer.for_module(name)\n            attr_docs = analyzer.find_attr_docs()\n            for namespace, attr_name in attr_docs:\n                if namespace == '' and attr_name in members:\n                    attrs.append(attr_name)\n                    if not attr_name.startswith('_'):\n                        public.append(attr_name)\n        except PycodeError:\n            pass    # give up if ModuleAnalyzer fails to parse code\n        return public, attrs\n\n    def get_modules(obj: Any) -> Tuple[List[str], List[str]]:\n        items = []  # type: List[str]\n        for _, modname, ispkg in pkgutil.iter_modules(obj.__path__):\n            fullname = name + '.' + modname\n            items.append(fullname)\n        public = [x for x in items if not x.split('.')[-1].startswith('_')]\n        return public, items\n\n    ns = {}  # type: Dict[str, Any]\n    ns.update(context)\n\n    if doc.objtype == 'module':\n        ns['members'] = dir(obj)\n        ns['functions'], ns['all_functions'] = \\\n            get_members(obj, {'function'}, imported=imported_members)\n        ns['classes'], ns['all_classes'] = \\\n            get_members(obj, {'class'}, imported=imported_members)\n        ns['exceptions'], ns['all_exceptions'] = \\\n            get_members(obj, {'exception'}, imported=imported_members)\n        ns['attributes'], ns['all_attributes'] = \\\n            get_module_attrs(ns['members'])\n        ispackage = hasattr(obj, '__path__')\n        if ispackage and recursive:\n            ns['modules'], ns['all_modules'] = get_modules(obj)\n    elif doc.objtype == 'class':\n        ns['members'] = dir(obj)\n        ns['inherited_members'] = \\\n            set(dir(obj)) - set(obj.__dict__.keys())\n        ns['methods'], ns['all_methods'] = \\\n            get_members(obj, {'method'}, ['__init__'])\n        ns['attributes'], ns['all_attributes'] = \\\n            get_members(obj, {'attribute', 'property'})\n\n    modname, qualname = split_full_qualified_name(name)\n    if doc.objtype in ('method', 'attribute', 'property'):\n        ns['class'] = qualname.rsplit(\".\", 1)[0]\n\n    if doc.objtype in ('class',):\n        shortname = qualname\n    else:\n        shortname = qualname.rsplit(\".\", 1)[-1]\n\n    ns['fullname'] = name\n    ns['module'] = modname\n    ns['objname'] = qualname\n    ns['name'] = shortname\n\n    ns['objtype'] = doc.objtype\n    ns['underline'] = len(name) * '='\n\n    if template_name:\n        return template.render(template_name, ns)\n    else:\n        return template.render(doc.objtype, ns)\n\n\ndef generate_autosummary_docs(sources: List[str], output_dir: str = None,\n                              suffix: str = '.rst', warn: Callable = None,\n                              info: Callable = None, base_path: str = None,\n                              builder: Builder = None, template_dir: str = None,\n                              imported_members: bool = False, app: Any = None,\n                              overwrite: bool = True) -> None:\n    if info:\n        warnings.warn('info argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx40Warning, stacklevel=2)\n        _info = info\n    else:\n        _info = logger.info\n(B)         \"\"\"Render a template file.\"\"\"\n        try:\n            template = self.env.get_template(template_name)\n        except TemplateNotFound:\n            try:\n                # objtype is given as template_name\n                template = self.env.get_template('autosummary/%s.rst' % template_name)\n            except TemplateNotFound:\n                # fallback to base.rst\n                template = self.env.get_template('autosummary/base.rst')\n\n        return template.render(context)\n\n\n# -- Generating output ---------------------------------------------------------\n\n\ndef generate_autosummary_content(name: str, obj: Any, parent: Any,\n                                 template: AutosummaryRenderer, template_name: str,\n                                 imported_members: bool, app: Any,\n                                 recursive: bool, context: Dict) -> str:\n    doc = get_documenter(app, obj, parent)\n\n    def skip_member(obj: Any, name: str, objtype: str) -> bool:\n        try:\n            return app.emit_firstresult('autodoc-skip-member', objtype, name,\n                                        obj, False, {})\n        except Exception as exc:\n            logger.warning(__('autosummary: failed to determine %r to be documented, '\n                              'the following exception was raised:\\n%s'),\n                           name, exc, type='autosummary')\n            return False\n\n    def get_members(obj: Any, types: Set[str], include_public: List[str] = [],\n                    imported: bool = True) -> Tuple[List[str], List[str]]:\n        items = []  # type: List[str]\n        public = []  # type: List[str]\n        for name in dir(obj):\n            try:\n                value = safe_getattr(obj, name)\n            except AttributeError:\n                continue\n            documenter = get_documenter(app, value, obj)\n            if documenter.objtype in types:\n                # skip imported members if expected\n                if imported or getattr(value, '__module__', None) == obj.__name__:\n                    skipped = skip_member(value, name, documenter.objtype)\n                    if skipped is True:\n                        pass\n                    elif skipped is False:\n                        # show the member forcedly\n                        items.append(name)\n                        public.append(name)\n                    else:\n                        items.append(name)\n                        if name in include_public or not name.startswith('_'):\n                            # considers member as public\n                            public.append(name)\n        return public, items\n\n    def get_module_attrs(members: Any) -> Tuple[List[str], List[str]]:\n        \"\"\"Find module attributes with docstrings.\"\"\"\n        attrs, public = [], []\n        try:\n            analyzer = ModuleAnalyzer.for_module(name)\n            attr_docs = analyzer.find_attr_docs()\n            for namespace, attr_name in attr_docs:\n                if namespace == '' and attr_name in members:\n                    attrs.append(attr_name)\n                    if not attr_name.startswith('_'):\n                        public.append(attr_name)\n        except PycodeError:\n            pass    # give up if ModuleAnalyzer fails to parse code\n        return public, attrs\n\n    def get_modules(obj: Any) -> Tuple[List[str], List[str]]:\n        items = []  # type: List[str]\n        for _, modname, ispkg in pkgutil.iter_modules(obj.__path__):\n            fullname = name + '.' + modname\n            items.append(fullname)\n        public = [x for x in items if not x.split('.')[-1].startswith('_')]\n        return public, items\n\n    ns = {}  # type: Dict[str, Any]\n    ns.update(context)\n\n    if doc.objtype == 'module':\n        ns['members'] = dir(obj)\n        ns['functions'], ns['all_functions'] = \\\n            get_members(obj, {'function'}, imported=imported_members)\n        ns['classes'], ns['all_classes'] = \\\n            get_members(obj, {'class'}, imported=imported_members)\n        ns['exceptions'], ns['all_exceptions'] = \\\n            get_members(obj, {'exception'}, imported=imported_members)\n        ns['attributes'], ns['all_attributes'] = \\\n            get_module_attrs(ns['members'])\n        ispackage = hasattr(obj, '__path__')\n        if ispackage and recursive:\n            ns['modules'], ns['all_modules'] = get_modules(obj)\n    elif doc.objtype == 'class':\n        ns['members'] = dir(obj)\n        ns['inherited_members'] = \\\n            set(dir(obj)) - set(obj.__dict__.keys())\n        ns['methods'], ns['all_methods'] = \\\n            get_members(obj, {'method'}, ['__init__'])\n        ns['attributes'], ns['all_attributes'] = \\\n            get_members(obj, {'attribute', 'property'})\n\n    modname, qualname = split_full_qualified_name(name)\n    if doc.objtype in ('method', 'attribute', 'property'):\n        ns['class'] = qualname.rsplit(\".\", 1)[0]\n\n    if doc.objtype in ('class',):\n        shortname = qualname\n    else:\n        shortname = qualname.rsplit(\".\", 1)[-1]\n\n    ns['fullname'] = name\n    ns['module'] = modname\n    ns['objname'] = qualname\n    ns['name'] = shortname\n\n    ns['objtype'] = doc.objtype\n    ns['underline'] = len(name) * '='\n\n    if template_name:\n        return template.render(template_name, ns)\n    else:\n        return template.render(doc.objtype, ns)\n\n\ndef generate_autosummary_docs(sources: List[str], output_dir: str = None,\n                              suffix: str = '.rst', warn: Callable = None,\n                              info: Callable = None, base_path: str = None,\n                              builder: Builder = None, template_dir: str = None,\n                              imported_members: bool = False, app: Any = None,\n                              overwrite: bool = True) -> None:\n    if info:\n        warnings.warn('info argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx40Warning, stacklevel=2)\n        _info = info\n    else:\n        _info = logger.info\n\n    if warn:\n        warnings.warn('warn argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx40Warning, stacklevel=2)\n        _warn = warn\n    else:\n        _warn = logger.warning\n\n    if builder:\n        warnings.warn('builder argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx50Warning, stacklevel=2)\n\n    if template_dir:\n        warnings.warn('template_dir argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx50Warning, stacklevel=2)\n\n    showed_sources = list(sorted(sources))\n    if len(showed_sources) > 20:\n        showed_sources = showed_sources[:10] + ['...'] + showed_sources[-10:]\n    _info(__('[autosummary] generating autosummary for: %s') %\n          ', '.join(showed_sources))\n\n    if output_dir:\n        _info(__('[autosummary] writing to %s') % output_dir)\n\n    if base_path is not None:\n        sources = [os.path.join(base_path, filename) for filename in sources]\n\n    template = AutosummaryRenderer(app)\n\n    # read\n    items = find_autosummary_in_files(sources)\n\n    # keep track of new files\n    new_files = []\n\n    # write\n    for entry in sorted(set(items), key=str):\n        if entry.path is None:\n            # The corresponding autosummary:: directive did not have\n            # a :toctree: option\n            continue\n\n        path = output_dir or os.path.abspath(entry.path)\n        ensuredir(path)\n\n        try:\n            name, obj, parent, mod_name = import_by_name(entry.name)\n        except ImportError as e:\n            _warn(__('[autosummary] failed to import %r: %s') % (entry.name, e))\n            continue\n\n        context = {}\n        if app:\n            context.update(app.config.autosummary_context)\n\n        content = generate_autosummary_content(name, obj, parent, template, entry.template,\n                                               imported_members, app, entry.recursive, context)\n\n        filename = os.path.join(path, name + suffix)\n        if os.path.isfile(filename):\n            with open(filename) as f:\n                old_content = f.read()\n\n            if content == old_content:\n                continue\n            elif overwrite:  # content has changed\n                with open(filename, 'w') as f:\n                    f.write(content)\n                new_files.append(filename)\n        else:\n            with open(filename, 'w') as f:\n                f.write(content)\n            new_files.append(filename)\n\n    # descend recursively to new files\n    if new_files:\n        generate_autosummary_docs(new_files, output_dir=output_dir,\n                                  suffix=suffix, warn=warn, info=info,\n                                  base_path=base_path,\n                                  imported_members=imported_members, app=app,\n                                  overwrite=overwrite)\n\n\n# -- Finding documented entries in files ---------------------------------------\n\ndef find_autosummary_in_files(filenames: List[str]) -> List[AutosummaryEntry]:\n    \"\"\"Find out what items are documented in source/*.rst.\n\n    See `find_autosummary_in_lines`.\n    \"\"\"\n    documented = []  # type: List[AutosummaryEntry]\n    for filename in filenames:\n        with open(filename, encoding='utf-8', errors='ignore') as f:\n            lines = f.read().splitlines()\n            documented.extend(find_autosummary_in_lines(lines, filename=filename))\n    return documented\n\n\ndef find_autosummary_in_docstring(name: str, module: str = None, filename: str = None\n                                  ) -> List[AutosummaryEntry]:\n    \"\"\"Find out what items are documented in the given object's docstring.\n\n    See `find_autosummary_in_lines`.\n    \"\"\"\n    if module:\n        warnings.warn('module argument for find_autosummary_in_docstring() is deprecated.',\n                      RemovedInSphinx50Warning, stacklevel=2)\n\n    try:\n        real_name, obj, parent, modname = import_by_name(name)\n        lines = pydoc.getdoc(obj).splitlines()\n        return find_autosummary_in_lines(lines, module=name, filename=filename)\n    except AttributeError:\n        pass\n    except ImportError as e:\n        print(\"Failed to import '%s': %s\" % (name, e))\n    except SystemExit:\n        print(\"Failed to import '%s'; the module executes module level \"\n              \"statement and it might call sys.exit().\" % name)\n    return []\n\n\ndef find_autosummary_in_lines(lines: List[str], module: str = None, filename: str = None\n                              ) -> List[AutosummaryEntry]:\n    \"\"\"Find out what items appear in autosummary:: directives in the\n    given lines.\n\n    Returns a list of (name, toctree, template) where *name* is a name\n    of an object and *toctree* the :toctree: path of the corresponding\n    autosummary directive (relative to the root of the file name), and\n    *template* the value of the :template: option. *toctree* and\n    *template* ``None`` if the directive does not have the\n    corresponding options set.\n    \"\"\"\n    autosummary_re = re.compile(r'^(\\s*)\\.\\.\\s+autosummary::\\s*')\n    automodule_re = re.compile(\n        r'^\\s*\\.\\.\\s+automodule::\\s*([A-Za-z0-9_.]+)\\s*$')\n    module_re = re.compile(\n        r'^\\s*\\.\\.\\s+(current)?module::\\s*([a-zA-Z0-9_.]+)\\s*$')\n    autosummary_item_re = re.compile(r'^\\s+(~?[_a-zA-Z][a-zA-Z0-9_.]*)\\s*.*?')\n    recursive_arg_re = re.compile(r'^\\s+:recursive:\\s*$')\n    toctree_arg_re = re.compile(r'^\\s+:toctree:\\s*(.*?)\\s*$')\n    template_arg_re = re.compile(r'^\\s+:template:\\s*(.*?)\\s*$')\n(C)         ModuleDocumenter, ClassDocumenter, ExceptionDocumenter, DataDocumenter,\n        FunctionDocumenter, MethodDocumenter, AttributeDocumenter,\n        InstanceAttributeDocumenter, DecoratorDocumenter, PropertyDocumenter,\n        SlotsAttributeDocumenter, DataDeclarationDocumenter,\n        SingledispatchFunctionDocumenter,\n    )\n    documenters = [\n        ModuleDocumenter, ClassDocumenter, ExceptionDocumenter, DataDocumenter,\n        FunctionDocumenter, MethodDocumenter, AttributeDocumenter,\n        InstanceAttributeDocumenter, DecoratorDocumenter, PropertyDocumenter,\n        SlotsAttributeDocumenter, DataDeclarationDocumenter,\n        SingledispatchFunctionDocumenter,\n    ]  # type: List[Type[Documenter]]\n    for documenter in documenters:\n        app.registry.add_documenter(documenter.objtype, documenter)\n\n\ndef _simple_info(msg: str) -> None:\n    print(msg)\n\n\ndef _simple_warn(msg: str) -> None:\n    print('WARNING: ' + msg, file=sys.stderr)\n\n\ndef _underline(title: str, line: str = '=') -> str:\n    if '\\n' in title:\n        raise ValueError('Can only underline single lines')\n    return title + '\\n' + line * len(title)\n\n\nclass AutosummaryRenderer:\n    \"\"\"A helper class for rendering.\"\"\"\n\n    def __init__(self, app: Union[Builder, Sphinx], template_dir: str = None) -> None:\n        if isinstance(app, Builder):\n            warnings.warn('The first argument for AutosummaryRenderer has been '\n                          'changed to Sphinx object',\n                          RemovedInSphinx50Warning, stacklevel=2)\n        if template_dir:\n            warnings.warn('template_dir argument for AutosummaryRenderer is deprecated.',\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        system_templates_path = [os.path.join(package_dir, 'ext', 'autosummary', 'templates')]\n        loader = SphinxTemplateLoader(app.srcdir, app.config.templates_path,\n                                      system_templates_path)\n\n        self.env = SandboxedEnvironment(loader=loader)\n        self.env.filters['escape'] = rst.escape\n        self.env.filters['e'] = rst.escape\n        self.env.filters['underline'] = _underline\n\n        if isinstance(app, (Sphinx, DummyApplication)):\n            if app.translator:\n                self.env.add_extension(\"jinja2.ext.i18n\")\n                self.env.install_gettext_translations(app.translator)  # type: ignore\n        elif isinstance(app, Builder):\n            if app.app.translator:\n                self.env.add_extension(\"jinja2.ext.i18n\")\n                self.env.install_gettext_translations(app.app.translator)  # type: ignore\n\n    def exists(self, template_name: str) -> bool:\n        \"\"\"Check if template file exists.\"\"\"\n        warnings.warn('AutosummaryRenderer.exists() is deprecated.',\n                      RemovedInSphinx50Warning, stacklevel=2)\n        try:\n            self.env.get_template(template_name)\n            return True\n        except TemplateNotFound:\n            return False\n\n    def render(self, template_name: str, context: Dict) -> str:\n        \"\"\"Render a template file.\"\"\"\n        try:\n            template = self.env.get_template(template_name)\n        except TemplateNotFound:\n            try:\n                # objtype is given as template_name\n                template = self.env.get_template('autosummary/%s.rst' % template_name)\n            except TemplateNotFound:\n                # fallback to base.rst\n                template = self.env.get_template('autosummary/base.rst')\n\n        return template.render(context)\n\n\n# -- Generating output ---------------------------------------------------------\n\n\ndef generate_autosummary_content(name: str, obj: Any, parent: Any,\n                                 template: AutosummaryRenderer, template_name: str,\n                                 imported_members: bool, app: Any,\n                                 recursive: bool, context: Dict) -> str:\n    doc = get_documenter(app, obj, parent)\n\n    def skip_member(obj: Any, name: str, objtype: str) -> bool:\n        try:\n            return app.emit_firstresult('autodoc-skip-member', objtype, name,\n                                        obj, False, {})\n        except Exception as exc:\n            logger.warning(__('autosummary: failed to determine %r to be documented, '\n                              'the following exception was raised:\\n%s'),\n                           name, exc, type='autosummary')\n            return False\n\n    def get_members(obj: Any, types: Set[str], include_public: List[str] = [],\n                    imported: bool = True) -> Tuple[List[str], List[str]]:\n        items = []  # type: List[str]\n        public = []  # type: List[str]\n        for name in dir(obj):\n            try:\n                value = safe_getattr(obj, name)\n            except AttributeError:\n                continue\n            documenter = get_documenter(app, value, obj)\n            if documenter.objtype in types:\n                # skip imported members if expected\n                if imported or getattr(value, '__module__', None) == obj.__name__:\n                    skipped = skip_member(value, name, documenter.objtype)\n                    if skipped is True:\n                        pass\n                    elif skipped is False:\n                        # show the member forcedly\n                        items.append(name)\n                        public.append(name)\n                    else:\n                        items.append(name)\n                        if name in include_public or not name.startswith('_'):\n                            # considers member as public\n                            public.append(name)\n        return public, items\n\n    def get_module_attrs(members: Any) -> Tuple[List[str], List[str]]:\n        \"\"\"Find module attributes with docstrings.\"\"\"\n        attrs, public = [], []\n        try:\n            analyzer = ModuleAnalyzer.for_module(name)\n            attr_docs = analyzer.find_attr_docs()\n            for namespace, attr_name in attr_docs:\n                if namespace == '' and attr_name in members:\n                    attrs.append(attr_name)\n                    if not attr_name.startswith('_'):\n                        public.append(attr_name)\n        except PycodeError:\n            pass    # give up if ModuleAnalyzer fails to parse code\n        return public, attrs\n\n    def get_modules(obj: Any) -> Tuple[List[str], List[str]]:\n        items = []  # type: List[str]\n        for _, modname, ispkg in pkgutil.iter_modules(obj.__path__):\n            fullname = name + '.' + modname\n            items.append(fullname)\n        public = [x for x in items if not x.split('.')[-1].startswith('_')]\n        return public, items\n\n    ns = {}  # type: Dict[str, Any]\n    ns.update(context)\n\n    if doc.objtype == 'module':\n        ns['members'] = dir(obj)\n        ns['functions'], ns['all_functions'] = \\\n            get_members(obj, {'function'}, imported=imported_members)\n        ns['classes'], ns['all_classes'] = \\\n            get_members(obj, {'class'}, imported=imported_members)\n        ns['exceptions'], ns['all_exceptions'] = \\\n            get_members(obj, {'exception'}, imported=imported_members)\n        ns['attributes'], ns['all_attributes'] = \\\n            get_module_attrs(ns['members'])\n        ispackage = hasattr(obj, '__path__')\n        if ispackage and recursive:\n            ns['modules'], ns['all_modules'] = get_modules(obj)\n    elif doc.objtype == 'class':\n        ns['members'] = dir(obj)\n        ns['inherited_members'] = \\\n            set(dir(obj)) - set(obj.__dict__.keys())\n        ns['methods'], ns['all_methods'] = \\\n            get_members(obj, {'method'}, ['__init__'])\n        ns['attributes'], ns['all_attributes'] = \\\n            get_members(obj, {'attribute', 'property'})\n\n    modname, qualname = split_full_qualified_name(name)\n    if doc.objtype in ('method', 'attribute', 'property'):\n        ns['class'] = qualname.rsplit(\".\", 1)[0]\n\n    if doc.objtype in ('class',):\n        shortname = qualname\n    else:\n        shortname = qualname.rsplit(\".\", 1)[-1]\n\n    ns['fullname'] = name\n    ns['module'] = modname\n    ns['objname'] = qualname\n    ns['name'] = shortname\n\n    ns['objtype'] = doc.objtype\n    ns['underline'] = len(name) * '='\n\n    if template_name:\n        return template.render(template_name, ns)\n    else:\n        return template.render(doc.objtype, ns)\n\n\ndef generate_autosummary_docs(sources: List[str], output_dir: str = None,\n                              suffix: str = '.rst', warn: Callable = None,\n                              info: Callable = None, base_path: str = None,\n                              builder: Builder = None, template_dir: str = None,\n                              imported_members: bool = False, app: Any = None,\n                              overwrite: bool = True) -> None:\n    if info:\n        warnings.warn('info argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx40Warning, stacklevel=2)\n        _info = info\n    else:\n        _info = logger.info\n\n    if warn:\n        warnings.warn('warn argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx40Warning, stacklevel=2)\n        _warn = warn\n    else:\n        _warn = logger.warning\n\n    if builder:\n        warnings.warn('builder argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx50Warning, stacklevel=2)\n\n    if template_dir:\n        warnings.warn('template_dir argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx50Warning, stacklevel=2)\n\n    showed_sources = list(sorted(sources))\n    if len(showed_sources) > 20:\n        showed_sources = showed_sources[:10] + ['...'] + showed_sources[-10:]\n    _info(__('[autosummary] generating autosummary for: %s') %\n          ', '.join(showed_sources))\n\n    if output_dir:\n        _info(__('[autosummary] writing to %s') % output_dir)\n\n    if base_path is not None:\n        sources = [os.path.join(base_path, filename) for filename in sources]\n\n    template = AutosummaryRenderer(app)\n\n    # read\n    items = find_autosummary_in_files(sources)\n\n    # keep track of new files\n    new_files = []\n\n    # write\n    for entry in sorted(set(items), key=str):\n        if entry.path is None:\n            # The corresponding autosummary:: directive did not have\n            # a :toctree: option\n            continue\n\n        path = output_dir or os.path.abspath(entry.path)\n        ensuredir(path)\n\n        try:\n            name, obj, parent, mod_name = import_by_name(entry.name)\n        except ImportError as e:\n            _warn(__('[autosummary] failed to import %r: %s') % (entry.name, e))\n            continue\n\n        context = {}\n        if app:\n            context.update(app.config.autosummary_context)\n\n        content = generate_autosummary_content(name, obj, parent, template, entry.template,\n                                               imported_members, app, entry.recursive, context)\n\n        filename = os.path.join(path, name + suffix)\n        if os.path.isfile(filename):\n            with open(filename) as f:\n                old_content = f.read()\n\n            if content == old_content:\n                continue\n            elif overwrite:  # content has changed\n                with open(filename, 'w') as f:\n                    f.write(content)\n                new_files.append(filename)\n        else:\n            with open(filename, 'w') as f:\n(D)             pass    # give up if ModuleAnalyzer fails to parse code\n        return public, attrs\n\n    def get_modules(obj: Any) -> Tuple[List[str], List[str]]:\n        items = []  # type: List[str]\n        for _, modname, ispkg in pkgutil.iter_modules(obj.__path__):\n            fullname = name + '.' + modname\n            items.append(fullname)\n        public = [x for x in items if not x.split('.')[-1].startswith('_')]\n        return public, items\n\n    ns = {}  # type: Dict[str, Any]\n    ns.update(context)\n\n    if doc.objtype == 'module':\n        ns['members'] = dir(obj)\n        ns['functions'], ns['all_functions'] = \\\n            get_members(obj, {'function'}, imported=imported_members)\n        ns['classes'], ns['all_classes'] = \\\n            get_members(obj, {'class'}, imported=imported_members)\n        ns['exceptions'], ns['all_exceptions'] = \\\n            get_members(obj, {'exception'}, imported=imported_members)\n        ns['attributes'], ns['all_attributes'] = \\\n            get_module_attrs(ns['members'])\n        ispackage = hasattr(obj, '__path__')\n        if ispackage and recursive:\n            ns['modules'], ns['all_modules'] = get_modules(obj)\n    elif doc.objtype == 'class':\n        ns['members'] = dir(obj)\n        ns['inherited_members'] = \\\n            set(dir(obj)) - set(obj.__dict__.keys())\n        ns['methods'], ns['all_methods'] = \\\n            get_members(obj, {'method'}, ['__init__'])\n        ns['attributes'], ns['all_attributes'] = \\\n            get_members(obj, {'attribute', 'property'})\n\n    modname, qualname = split_full_qualified_name(name)\n    if doc.objtype in ('method', 'attribute', 'property'):\n        ns['class'] = qualname.rsplit(\".\", 1)[0]\n\n    if doc.objtype in ('class',):\n        shortname = qualname\n    else:\n        shortname = qualname.rsplit(\".\", 1)[-1]\n\n    ns['fullname'] = name\n    ns['module'] = modname\n    ns['objname'] = qualname\n    ns['name'] = shortname\n\n    ns['objtype'] = doc.objtype\n    ns['underline'] = len(name) * '='\n\n    if template_name:\n        return template.render(template_name, ns)\n    else:\n        return template.render(doc.objtype, ns)\n\n\ndef generate_autosummary_docs(sources: List[str], output_dir: str = None,\n                              suffix: str = '.rst', warn: Callable = None,\n                              info: Callable = None, base_path: str = None,\n                              builder: Builder = None, template_dir: str = None,\n                              imported_members: bool = False, app: Any = None,\n                              overwrite: bool = True) -> None:\n    if info:\n        warnings.warn('info argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx40Warning, stacklevel=2)\n        _info = info\n    else:\n        _info = logger.info\n\n    if warn:\n        warnings.warn('warn argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx40Warning, stacklevel=2)\n        _warn = warn\n    else:\n        _warn = logger.warning\n\n    if builder:\n        warnings.warn('builder argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx50Warning, stacklevel=2)\n\n    if template_dir:\n        warnings.warn('template_dir argument for generate_autosummary_docs() is deprecated.',\n                      RemovedInSphinx50Warning, stacklevel=2)\n\n    showed_sources = list(sorted(sources))\n    if len(showed_sources) > 20:\n        showed_sources = showed_sources[:10] + ['...'] + showed_sources[-10:]\n    _info(__('[autosummary] generating autosummary for: %s') %\n          ', '.join(showed_sources))\n\n    if output_dir:\n        _info(__('[autosummary] writing to %s') % output_dir)\n\n    if base_path is not None:\n        sources = [os.path.join(base_path, filename) for filename in sources]\n\n    template = AutosummaryRenderer(app)\n\n    # read\n    items = find_autosummary_in_files(sources)\n\n    # keep track of new files\n    new_files = []\n\n    # write\n    for entry in sorted(set(items), key=str):\n        if entry.path is None:\n            # The corresponding autosummary:: directive did not have\n            # a :toctree: option\n            continue\n\n        path = output_dir or os.path.abspath(entry.path)\n        ensuredir(path)\n\n        try:\n            name, obj, parent, mod_name = import_by_name(entry.name)\n        except ImportError as e:\n            _warn(__('[autosummary] failed to import %r: %s') % (entry.name, e))\n            continue\n\n        context = {}\n        if app:\n            context.update(app.config.autosummary_context)\n\n        content = generate_autosummary_content(name, obj, parent, template, entry.template,\n                                               imported_members, app, entry.recursive, context)\n\n        filename = os.path.join(path, name + suffix)\n        if os.path.isfile(filename):\n            with open(filename) as f:\n                old_content = f.read()\n\n            if content == old_content:\n                continue\n            elif overwrite:  # content has changed\n                with open(filename, 'w') as f:\n                    f.write(content)\n                new_files.append(filename)\n        else:\n            with open(filename, 'w') as f:\n                f.write(content)\n            new_files.append(filename)\n\n    # descend recursively to new files\n    if new_files:\n        generate_autosummary_docs(new_files, output_dir=output_dir,\n                                  suffix=suffix, warn=warn, info=info,\n                                  base_path=base_path,\n                                  imported_members=imported_members, app=app,\n                                  overwrite=overwrite)\n\n\n# -- Finding documented entries in files ---------------------------------------\n\ndef find_autosummary_in_files(filenames: List[str]) -> List[AutosummaryEntry]:\n    \"\"\"Find out what items are documented in source/*.rst.\n\n    See `find_autosummary_in_lines`.\n    \"\"\"\n    documented = []  # type: List[AutosummaryEntry]\n    for filename in filenames:\n        with open(filename, encoding='utf-8', errors='ignore') as f:\n            lines = f.read().splitlines()\n            documented.extend(find_autosummary_in_lines(lines, filename=filename))\n    return documented\n\n\ndef find_autosummary_in_docstring(name: str, module: str = None, filename: str = None\n                                  ) -> List[AutosummaryEntry]:\n    \"\"\"Find out what items are documented in the given object's docstring.\n\n    See `find_autosummary_in_lines`.\n    \"\"\"\n    if module:\n        warnings.warn('module argument for find_autosummary_in_docstring() is deprecated.',\n                      RemovedInSphinx50Warning, stacklevel=2)\n\n    try:\n        real_name, obj, parent, modname = import_by_name(name)\n        lines = pydoc.getdoc(obj).splitlines()\n        return find_autosummary_in_lines(lines, module=name, filename=filename)\n    except AttributeError:\n        pass\n    except ImportError as e:\n        print(\"Failed to import '%s': %s\" % (name, e))\n    except SystemExit:\n        print(\"Failed to import '%s'; the module executes module level \"\n              \"statement and it might call sys.exit().\" % name)\n    return []\n\n\ndef find_autosummary_in_lines(lines: List[str], module: str = None, filename: str = None\n                              ) -> List[AutosummaryEntry]:\n    \"\"\"Find out what items appear in autosummary:: directives in the\n    given lines.\n\n    Returns a list of (name, toctree, template) where *name* is a name\n    of an object and *toctree* the :toctree: path of the corresponding\n    autosummary directive (relative to the root of the file name), and\n    *template* the value of the :template: option. *toctree* and\n    *template* ``None`` if the directive does not have the\n    corresponding options set.\n    \"\"\"\n    autosummary_re = re.compile(r'^(\\s*)\\.\\.\\s+autosummary::\\s*')\n    automodule_re = re.compile(\n        r'^\\s*\\.\\.\\s+automodule::\\s*([A-Za-z0-9_.]+)\\s*$')\n    module_re = re.compile(\n        r'^\\s*\\.\\.\\s+(current)?module::\\s*([a-zA-Z0-9_.]+)\\s*$')\n    autosummary_item_re = re.compile(r'^\\s+(~?[_a-zA-Z][a-zA-Z0-9_.]*)\\s*.*?')\n    recursive_arg_re = re.compile(r'^\\s+:recursive:\\s*$')\n    toctree_arg_re = re.compile(r'^\\s+:toctree:\\s*(.*?)\\s*$')\n    template_arg_re = re.compile(r'^\\s+:template:\\s*(.*?)\\s*$')\n\n    documented = []  # type: List[AutosummaryEntry]\n\n    recursive = False\n    toctree = None  # type: str\n    template = None\n    current_module = module\n    in_autosummary = False\n    base_indent = \"\"\n\n    for line in lines:\n        if in_autosummary:\n            m = recursive_arg_re.match(line)\n            if m:\n                recursive = True\n                continue\n\n            m = toctree_arg_re.match(line)\n            if m:\n                toctree = m.group(1)\n                if filename:\n                    toctree = os.path.join(os.path.dirname(filename),\n                                           toctree)\n                continue\n\n            m = template_arg_re.match(line)\n            if m:\n                template = m.group(1).strip()\n                continue\n\n            if line.strip().startswith(':'):\n                continue  # skip options\n\n            m = autosummary_item_re.match(line)\n            if m:\n                name = m.group(1).strip()\n                if name.startswith('~'):\n                    name = name[1:]\n                if current_module and \\\n                   not name.startswith(current_module + '.'):\n                    name = \"%s.%s\" % (current_module, name)\n                documented.append(AutosummaryEntry(name, toctree, template, recursive))\n                continue\n\n            if not line.strip() or line.startswith(base_indent + \" \"):\n                continue\n\n            in_autosummary = False\n\n        m = autosummary_re.match(line)\n        if m:\n            in_autosummary = True\n            base_indent = m.group(1)\n            recursive = False\n            toctree = None\n            template = None\n            continue\n\n        m = automodule_re.search(line)\n        if m:\n            current_module = m.group(1).strip()\n            # recurse into the automodule docstring\n            documented.extend(find_autosummary_in_docstring(\n                current_module, filename=filename))\n            continue\n\n        m = module_re.match(line)\n        if m:\n            current_module = m.group(2)\n            continue\n\n    return documented", "answer": "A"}
{"uuid": "bda90039-2e88-47a1-9961-f14316759416", "issue_statement": "overescaped trailing underscore on attribute with napoleon\n**Describe the bug**\r\nAttribute name `hello_` shows up as `hello\\_` in the html (visible backslash) with napoleon.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nempty `__init__.py`\r\n`a.py` contains\r\n```python\r\nclass A:\r\n    \"\"\"\r\n    Attributes\r\n    ----------\r\n    hello_: int\r\n        hi\r\n    \"\"\"\r\n    pass\r\n```\r\nrun `sphinx-quickstart`\r\nadd `'sphinx.ext.autodoc', 'sphinx.ext.napoleon'` to extensions in conf.py.\r\nadd `.. autoclass:: a.A` to index.rst\r\nPYTHONPATH=. make clean html\r\nopen _build/html/index.html in web browser and see the ugly backslash.\r\n\r\n**Expected behavior**\r\nNo backslash, a similar output to what I get for\r\n```rst\r\n    .. attribute:: hello_\r\n        :type: int\r\n\r\n        hi\r\n```\r\n(the type shows up differently as well, but that's not the point here)\r\nOlder versions like 2.4.3 look ok to me.\r\n\r\n**Environment info**\r\n- OS: Linux debian testing\r\n- Python version: 3.8.3\r\n- Sphinx version: 3.0.4\r\n- Sphinx extensions:  sphinx.ext.autodoc, sphinx.ext.napoleon\r\n- Extra tools:", "choices": "(A)         else:\n            min_indent = self._get_min_indent(lines)\n            return [line[min_indent:] for line in lines]\n\n    def _escape_args_and_kwargs(self, name: str) -> str:\n        if name.endswith('_'):\n            name = name[:-1] + r'\\_'\n(B)             name = name[:-1] + r'\\_'\n\n        if name[:2] == '**':\n            return r'\\*\\*' + name[2:]\n        elif name[:1] == '*':\n            return r'\\*' + name[1:]\n        else:\n(C)             return [line[min_indent:] for line in lines]\n\n    def _escape_args_and_kwargs(self, name: str) -> str:\n        if name.endswith('_'):\n            name = name[:-1] + r'\\_'\n\n        if name[:2] == '**':\n(D)     def _escape_args_and_kwargs(self, name: str) -> str:\n        if name.endswith('_'):\n            name = name[:-1] + r'\\_'\n\n        if name[:2] == '**':\n            return r'\\*\\*' + name[2:]\n        elif name[:1] == '*':", "answer": "C"}
{"uuid": "5f1057cf-97dd-4891-a9ae-24dc70f35e38", "issue_statement": "Two sections called Symbols in index\nWhen using index entries with the following leading characters: _@_, _\u00a3_, and _\u2190_ I get two sections called _Symbols_ in the HTML output, the first containing all _@_ entries before \u201dnormal\u201d words and the second containing _\u00a3_ and _\u2190_ entries after the \u201dnormal\u201d words.  Both have the same anchor in HTML so the links at the top of the index page contain two _Symbols_ links, one before the letters and one after, but both lead to the first section.", "choices": "(A)             if category_key:\n                # using specified category key to sort\n                key = category_key\n            lckey = unicodedata.normalize('NFD', key.lower())\n            if lckey.startswith('\\N{RIGHT-TO-LEFT MARK}'):\n                lckey = lckey[1:]\n            if lckey[0:1].isalpha() or lckey.startswith('_'):\n                lckey = chr(127) + lckey\n            # ensure a determinstic order *within* letters by also sorting on\n            # the entry itself\n            return (lckey, entry[0])\n        newlist = sorted(new.items(), key=keyfunc)\n\n        if group_entries:\n            # fixup entries: transform\n            #   func() (in module foo)\n            #   func() (in module bar)\n            # into\n            #   func()\n            #     (in module foo)\n            #     (in module bar)\n            oldkey = ''\n            oldsubitems = None  # type: Dict[str, List]\n            i = 0\n            while i < len(newlist):\n(B)         # sort the index entries for same keyword.\n        def keyfunc0(entry: Tuple[str, str]) -> Tuple[bool, str]:\n            main, uri = entry\n            return (not main, uri)  # show main entries at first\n\n        for indexentry in new.values():\n            indexentry[0].sort(key=keyfunc0)\n            for subentry in indexentry[1].values():\n                subentry[0].sort(key=keyfunc0)  # type: ignore\n\n        # sort the index entries; put all symbols at the front, even those\n        # following the letters in ASCII, this is where the chr(127) comes from\n        def keyfunc(entry: Tuple[str, List]) -> Tuple[str, str]:\n            key, (void, void, category_key) = entry\n            if category_key:\n                # using specified category key to sort\n                key = category_key\n            lckey = unicodedata.normalize('NFD', key.lower())\n            if lckey.startswith('\\N{RIGHT-TO-LEFT MARK}'):\n                lckey = lckey[1:]\n            if lckey[0:1].isalpha() or lckey.startswith('_'):\n                lckey = chr(127) + lckey\n            # ensure a determinstic order *within* letters by also sorting on\n            # the entry itself\n            return (lckey, entry[0])\n(C)             for subentry in indexentry[1].values():\n                subentry[0].sort(key=keyfunc0)  # type: ignore\n\n        # sort the index entries; put all symbols at the front, even those\n        # following the letters in ASCII, this is where the chr(127) comes from\n        def keyfunc(entry: Tuple[str, List]) -> Tuple[str, str]:\n            key, (void, void, category_key) = entry\n            if category_key:\n                # using specified category key to sort\n                key = category_key\n            lckey = unicodedata.normalize('NFD', key.lower())\n            if lckey.startswith('\\N{RIGHT-TO-LEFT MARK}'):\n                lckey = lckey[1:]\n            if lckey[0:1].isalpha() or lckey.startswith('_'):\n                lckey = chr(127) + lckey\n            # ensure a determinstic order *within* letters by also sorting on\n            # the entry itself\n            return (lckey, entry[0])\n        newlist = sorted(new.items(), key=keyfunc)\n\n        if group_entries:\n            # fixup entries: transform\n            #   func() (in module foo)\n            #   func() (in module bar)\n            # into\n(D)             if lckey[0:1].isalpha() or lckey.startswith('_'):\n                lckey = chr(127) + lckey\n            # ensure a determinstic order *within* letters by also sorting on\n            # the entry itself\n            return (lckey, entry[0])\n        newlist = sorted(new.items(), key=keyfunc)\n\n        if group_entries:\n            # fixup entries: transform\n            #   func() (in module foo)\n            #   func() (in module bar)\n            # into\n            #   func()\n            #     (in module foo)\n            #     (in module bar)\n            oldkey = ''\n            oldsubitems = None  # type: Dict[str, List]\n            i = 0\n            while i < len(newlist):\n                key, (targets, subitems, _key) = newlist[i]\n                # cannot move if it has subitems; structure gets too complex\n                if not subitems:\n                    m = _fixre.match(key)\n                    if m:\n                        if oldkey == m.group(1):", "answer": "C"}
{"uuid": "1e09439f-99d8-4f6e-9d74-6b9012301a2a", "issue_statement": "autodoc_typehints does not effect to overloaded callables\n**Describe the bug**\r\nautodoc_typehints does not effect to overloaded callables.\r\n\r\n**To Reproduce**\r\n\r\n```\r\n# in conf.py\r\nautodoc_typehints = 'none'\r\n```\r\n```\r\n# in index.rst\r\n.. automodule:: example\r\n   :members:\r\n   :undoc-members:\r\n```\r\n```\r\n# in example.py\r\nfrom typing import overload\r\n\r\n\r\n@overload\r\ndef foo(x: int) -> int:\r\n    ...\r\n\r\n\r\n@overload\r\ndef foo(x: float) -> float:\r\n    ...\r\n\r\n\r\ndef foo(x):\r\n    return x\r\n```\r\n\r\n**Expected behavior**\r\nAll typehints for overloaded callables are obeyed `autodoc_typehints` setting.\r\n\r\n**Your project**\r\nNo\r\n\r\n**Screenshots**\r\nNo\r\n\r\n**Environment info**\r\n- OS: Mac\r\n- Python version: 3.8.2\r\n- Sphinx version: 3.1.0dev\r\n- Sphinx extensions: sphinx.ext.autodoc\r\n- Extra tools: No\r\n\r\n**Additional context**\r\nNo", "choices": "(A)                 modname = self.env.temp_data.get('autodoc:module')\n            if not modname:\n                modname = self.env.ref_context.get('py:module')\n            # ... else, it stays None, which means invalid\n        return modname, parents + [base]\n\n\nclass DocstringSignatureMixin:\n    \"\"\"\n    Mixin for FunctionDocumenter and MethodDocumenter to provide the\n    feature of reading the signature from the docstring.\n    \"\"\"\n    _new_docstrings = None  # type: List[List[str]]\n    _signatures = None      # type: List[str]\n\n    def _find_signature(self, encoding: str = None) -> Tuple[str, str]:\n        if encoding is not None:\n            warnings.warn(\"The 'encoding' argument to autodoc.%s._find_signature() is \"\n                          \"deprecated.\" % self.__class__.__name__,\n                          RemovedInSphinx40Warning, stacklevel=2)\n\n        # candidates of the object name\n        valid_names = [self.objpath[-1]]  # type: ignore\n        if isinstance(self, ClassDocumenter):\n            valid_names.append('__init__')\n            if hasattr(self.object, '__mro__'):\n                valid_names.extend(cls.__name__ for cls in self.object.__mro__)\n\n        docstrings = self.get_doc()\n        self._new_docstrings = docstrings[:]\n        self._signatures = []\n        result = None\n        for i, doclines in enumerate(docstrings):\n            for j, line in enumerate(doclines):\n                if not line:\n                    # no lines in docstring, no match\n                    break\n\n                if line.endswith('\\\\'):\n                    multiline = True\n                    line = line.rstrip('\\\\').rstrip()\n                else:\n                    multiline = False\n\n                # match first line of docstring against signature RE\n                match = py_ext_sig_re.match(line)\n                if not match:\n                    continue\n                exmod, path, base, args, retann = match.groups()\n\n                # the base name must match ours\n                if base not in valid_names:\n                    continue\n\n                # re-prepare docstring to ignore more leading indentation\n                tab_width = self.directive.state.document.settings.tab_width  # type: ignore\n                self._new_docstrings[i] = prepare_docstring('\\n'.join(doclines[j + 1:]),\n                                                            tabsize=tab_width)\n\n                if result is None:\n                    # first signature\n                    result = args, retann\n                else:\n                    # subsequent signatures\n                    self._signatures.append(\"(%s) -> %s\" % (args, retann))\n\n                if multiline:\n                    # the signature have multiple signatures on docstring\n                    continue\n                else:\n                    # don't look any further\n                    break\n\n            if result:\n                # finish the loop when signature found\n                break\n\n        return result\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if encoding is not None:\n            warnings.warn(\"The 'encoding' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx40Warning, stacklevel=2)\n        if self._new_docstrings is not None:\n            return self._new_docstrings\n        return super().get_doc(None, ignore)  # type: ignore\n\n    def format_signature(self, **kwargs: Any) -> str:\n        if self.args is None and self.env.config.autodoc_docstring_signature:  # type: ignore\n            # only act if a signature is not explicitly given already, and if\n            # the feature is enabled\n            result = self._find_signature()\n            if result is not None:\n                self.args, self.retann = result\n        sig = super().format_signature(**kwargs)  # type: ignore\n        if self._signatures:\n            return \"\\n\".join([sig] + self._signatures)\n        else:\n            return sig\n\n\nclass DocstringStripSignatureMixin(DocstringSignatureMixin):\n    \"\"\"\n    Mixin for AttributeDocumenter to provide the\n    feature of stripping any function signature from the docstring.\n    \"\"\"\n    def format_signature(self, **kwargs: Any) -> str:\n        if self.args is None and self.env.config.autodoc_docstring_signature:  # type: ignore\n            # only act if a signature is not explicitly given already, and if\n            # the feature is enabled\n            result = self._find_signature()\n            if result is not None:\n                # Discarding _args is a only difference with\n                # DocstringSignatureMixin.format_signature.\n                # Documenter.format_signature use self.args value to format.\n                _args, self.retann = result\n        return super().format_signature(**kwargs)\n\n\nclass FunctionDocumenter(DocstringSignatureMixin, ModuleLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for functions.\n    \"\"\"\n    objtype = 'function'\n    member_order = 30\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        # supports functions, builtins and bound methods exported at the module level\n        return (inspect.isfunction(member) or inspect.isbuiltin(member) or\n                (inspect.isroutine(member) and isinstance(parent, ModuleDocumenter)))\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.env.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            self.env.app.emit('autodoc-before-process-signature', self.object, False)\n            sig = inspect.signature(self.object, follow_wrapped=True,\n                                    type_aliases=self.env.config.autodoc_type_aliases)\n            args = stringify_signature(sig, **kwargs)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a function signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n        except ValueError:\n            args = ''\n\n        if self.env.config.strip_signature_backslash:\n            # escape backslashes for reST\n            args = args.replace('\\\\', '\\\\\\\\')\n        return args\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def add_directive_header(self, sig: str) -> None:\n        sourcename = self.get_sourcename()\n        super().add_directive_header(sig)\n\n        if inspect.iscoroutinefunction(self.object):\n            self.add_line('   :async:', sourcename)\n\n    def format_signature(self, **kwargs: Any) -> str:\n        sigs = []\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.overloads:\n            # Use signatures for overloaded functions instead of the implementation function.\n            overloaded = True\n        else:\n            overloaded = False\n            sig = super().format_signature(**kwargs)\n            sigs.append(sig)\n\n        if inspect.is_singledispatch_function(self.object):\n            # append signature of singledispatch'ed functions\n            for typ, func in self.object.registry.items():\n                if typ is object:\n                    pass  # default implementation. skipped.\n                else:\n                    self.annotate_to_first_argument(func, typ)\n\n                    documenter = FunctionDocumenter(self.directive, '')\n                    documenter.object = func\n                    documenter.objpath = [None]\n                    sigs.append(documenter.format_signature())\n        if overloaded:\n            __globals__ = safe_getattr(self.object, '__globals__', {})\n            for overload in self.analyzer.overloads.get('.'.join(self.objpath)):\n                overload = evaluate_signature(overload, __globals__,\n                                              self.env.config.autodoc_type_aliases)\n\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def annotate_to_first_argument(self, func: Callable, typ: Type) -> None:\n        \"\"\"Annotate type hint to the first argument of function if needed.\"\"\"\n        try:\n            sig = inspect.signature(func, type_aliases=self.env.config.autodoc_type_aliases)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a function signature for %s: %s\"),\n                           self.fullname, exc)\n            return\n        except ValueError:\n            return\n\n        if len(sig.parameters) == 0:\n            return\n\n        params = list(sig.parameters.values())\n        if params[0].annotation is Parameter.empty:\n            params[0] = params[0].replace(annotation=typ)\n            try:\n                func.__signature__ = sig.replace(parameters=params)  # type: ignore\n            except TypeError:\n                # failed to update signature (ex. built-in or extension types)\n                return\n\n\nclass SingledispatchFunctionDocumenter(FunctionDocumenter):\n    \"\"\"\n    Used to be a specialized Documenter subclass for singledispatch'ed functions.\n\n    Retained for backwards compatibility, now does the same as the FunctionDocumenter\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        warnings.warn(\"%s is deprecated.\" % self.__class__.__name__,\n                      RemovedInSphinx50Warning, stacklevel=2)\n        super().__init__(*args, **kwargs)\n\n\nclass DecoratorDocumenter(FunctionDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for decorator functions.\n    \"\"\"\n    objtype = 'decorator'\n\n    # must be lower than FunctionDocumenter\n    priority = -1\n\n    def format_args(self, **kwargs: Any) -> Any:\n        args = super().format_args(**kwargs)\n        if ',' in args:\n            return args\n        else:\n            return None\n\n\n# Types which have confusing metaclass signatures it would be best not to show.\n# These are listed by name, rather than storing the objects themselves, to avoid\n# needing to import the modules.\n_METACLASS_CALL_BLACKLIST = [\n    'enum.EnumMeta.__call__',\n]\n\n\n# Types whose __new__ signature is a pass-thru.\n_CLASS_NEW_BLACKLIST = [\n    'typing.Generic.__new__',\n]\n\n\nclass ClassDocumenter(DocstringSignatureMixin, ModuleLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for classes.\n    \"\"\"\n    objtype = 'class'\n    member_order = 20\n    option_spec = {\n        'members': members_option, 'undoc-members': bool_option,\n        'noindex': bool_option, 'inherited-members': inherited_members_option,\n        'show-inheritance': bool_option, 'member-order': member_order_option,\n        'exclude-members': exclude_members_option,\n        'private-members': members_option, 'special-members': members_option,\n    }  # type: Dict[str, Callable]\n\n    _signature_class = None  # type: Any\n    _signature_method_name = None  # type: str\n\n    def __init__(self, *args: Any) -> None:\n        super().__init__(*args)\n        merge_members_option(self.options)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, type)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        ret = super().import_object(raiseerror)\n        # if the class is documented under another name, document it\n        # as data/attribute\n        if ret:\n            if hasattr(self.object, '__name__'):\n                self.doc_as_attr = (self.objpath[-1] != self.object.__name__)\n            else:\n                self.doc_as_attr = True\n        return ret\n\n    def _get_signature(self) -> Tuple[Optional[Any], Optional[str], Optional[Signature]]:\n        def get_user_defined_function_or_method(obj: Any, attr: str) -> Any:\n            \"\"\" Get the `attr` function or method from `obj`, if it is user-defined. \"\"\"\n            if inspect.is_builtin_class_method(obj, attr):\n                return None\n            attr = self.get_attr(obj, attr, None)\n            if not (inspect.ismethod(attr) or inspect.isfunction(attr)):\n                return None\n            return attr\n\n        # This sequence is copied from inspect._signature_from_callable.\n        # ValueError means that no signature could be found, so we keep going.\n\n        # First, let's see if it has an overloaded __call__ defined\n        # in its metaclass\n        call = get_user_defined_function_or_method(type(self.object), '__call__')\n\n        if call is not None:\n            if \"{0.__module__}.{0.__qualname__}\".format(call) in _METACLASS_CALL_BLACKLIST:\n                call = None\n\n        if call is not None:\n            self.env.app.emit('autodoc-before-process-signature', call, True)\n            try:\n                sig = inspect.signature(call, bound_method=True,\n                                        type_aliases=self.env.config.autodoc_type_aliases)\n                return type(self.object), '__call__', sig\n            except ValueError:\n                pass\n\n        # Now we check if the 'obj' class has a '__new__' method\n        new = get_user_defined_function_or_method(self.object, '__new__')\n\n        if new is not None:\n            if \"{0.__module__}.{0.__qualname__}\".format(new) in _CLASS_NEW_BLACKLIST:\n                new = None\n\n        if new is not None:\n            self.env.app.emit('autodoc-before-process-signature', new, True)\n            try:\n                sig = inspect.signature(new, bound_method=True,\n                                        type_aliases=self.env.config.autodoc_type_aliases)\n                return self.object, '__new__', sig\n            except ValueError:\n                pass\n\n        # Finally, we should have at least __init__ implemented\n        init = get_user_defined_function_or_method(self.object, '__init__')\n        if init is not None:\n            self.env.app.emit('autodoc-before-process-signature', init, True)\n            try:\n                sig = inspect.signature(init, bound_method=True,\n                                        type_aliases=self.env.config.autodoc_type_aliases)\n                return self.object, '__init__', sig\n            except ValueError:\n                pass\n\n        # None of the attributes are user-defined, so fall back to let inspect\n        # handle it.\n        # We don't know the exact method that inspect.signature will read\n        # the signature from, so just pass the object itself to our hook.\n        self.env.app.emit('autodoc-before-process-signature', self.object, False)\n        try:\n            sig = inspect.signature(self.object, bound_method=False,\n                                    type_aliases=self.env.config.autodoc_type_aliases)\n            return None, None, sig\n        except ValueError:\n            pass\n\n        # Still no signature: happens e.g. for old-style classes\n        # with __init__ in C and no `__text_signature__`.\n        return None, None, None\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.env.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            self._signature_class, self._signature_method_name, sig = self._get_signature()\n        except TypeError as exc:\n            # __signature__ attribute contained junk\n            logger.warning(__(\"Failed to get a constructor signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n\n        if sig is None:\n            return None\n\n        return stringify_signature(sig, show_return_annotation=False, **kwargs)\n\n    def format_signature(self, **kwargs: Any) -> str:\n        if self.doc_as_attr:\n            return ''\n\n        sig = super().format_signature()\n        sigs = []\n\n        overloads = self.get_overloaded_signatures()\n        if overloads:\n            # Use signatures for overloaded methods instead of the implementation method.\n            method = safe_getattr(self._signature_class, self._signature_method_name, None)\n            __globals__ = safe_getattr(method, '__globals__', {})\n            for overload in overloads:\n                overload = evaluate_signature(overload, __globals__,\n                                              self.env.config.autodoc_type_aliases)\n\n                parameters = list(overload.parameters.values())\n                overload = overload.replace(parameters=parameters[1:],\n                                            return_annotation=Parameter.empty)\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n        else:\n            sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def get_overloaded_signatures(self) -> List[Signature]:\n        if self._signature_class and self._signature_method_name:\n            for cls in self._signature_class.__mro__:\n                try:\n                    analyzer = ModuleAnalyzer.for_module(cls.__module__)\n                    analyzer.parse()\n                    qualname = '.'.join([cls.__qualname__, self._signature_method_name])\n                    if qualname in analyzer.overloads:\n                        return analyzer.overloads.get(qualname)\n                except PycodeError:\n                    pass\n\n        return []\n\n    def add_directive_header(self, sig: str) -> None:\n        sourcename = self.get_sourcename()\n\n        if self.doc_as_attr:\n            self.directivetype = 'attribute'\n        super().add_directive_header(sig)\n\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n        # add inheritance info, if wanted\n        if not self.doc_as_attr and self.options.show_inheritance:\n            sourcename = self.get_sourcename()\n            self.add_line('', sourcename)\n            if hasattr(self.object, '__bases__') and len(self.object.__bases__):\n                bases = [':class:`%s`' % b.__name__\n                         if b.__module__ in ('__builtin__', 'builtins')\n                         else ':class:`%s.%s`' % (b.__module__, b.__qualname__)\n                         for b in self.object.__bases__]\n                self.add_line('   ' + _('Bases: %s') % ', '.join(bases),\n                              sourcename)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if encoding is not None:\n            warnings.warn(\"The 'encoding' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx40Warning, stacklevel=2)\n        lines = getattr(self, '_new_docstrings', None)\n        if lines is not None:\n            return lines\n\n        content = self.env.config.autoclass_content\n\n        docstrings = []\n        attrdocstring = self.get_attr(self.object, '__doc__', None)\n        if attrdocstring:\n            docstrings.append(attrdocstring)\n\n        # for classes, what the \"docstring\" is can be controlled via a\n        # config value; the default is only the class docstring\n        if content in ('both', 'init'):\n            __init__ = self.get_attr(self.object, '__init__', None)\n            initdocstring = getdoc(__init__, self.get_attr,\n                                   self.env.config.autodoc_inherit_docstrings,\n                                   self.parent, self.object_name)\n            # for new-style classes, no __init__ means default __init__\n            if (initdocstring is not None and\n                (initdocstring == object.__init__.__doc__ or  # for pypy\n                 initdocstring.strip() == object.__init__.__doc__)):  # for !pypy\n                initdocstring = None\n            if not initdocstring:\n                # try __new__\n                __new__ = self.get_attr(self.object, '__new__', None)\n                initdocstring = getdoc(__new__, self.get_attr,\n                                       self.env.config.autodoc_inherit_docstrings,\n                                       self.parent, self.object_name)\n                # for new-style classes, no __new__ means default __new__\n                if (initdocstring is not None and\n                    (initdocstring == object.__new__.__doc__ or  # for pypy\n                     initdocstring.strip() == object.__new__.__doc__)):  # for !pypy\n                    initdocstring = None\n            if initdocstring:\n                if content == 'init':\n                    docstrings = [initdocstring]\n                else:\n                    docstrings.append(initdocstring)\n\n        tab_width = self.directive.state.document.settings.tab_width\n        return [prepare_docstring(docstring, ignore, tab_width) for docstring in docstrings]\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        if self.doc_as_attr:\n            classname = safe_getattr(self.object, '__qualname__', None)\n            if not classname:\n                classname = safe_getattr(self.object, '__name__', None)\n            if classname:\n                module = safe_getattr(self.object, '__module__', None)\n                parentmodule = safe_getattr(self.parent, '__module__', None)\n                if module and module != parentmodule:\n                    classname = str(module) + '.' + str(classname)\n                content = StringList([_('alias of :class:`%s`') % classname], source='')\n                super().add_content(content, no_docstring=True)\n        else:\n            super().add_content(more_content)\n\n    def document_members(self, all_members: bool = False) -> None:\n        if self.doc_as_attr:\n            return\n        super().document_members(all_members)\n\n    def generate(self, more_content: Any = None, real_modname: str = None,\n                 check_module: bool = False, all_members: bool = False) -> None:\n        # Do not pass real_modname and use the name from the __module__\n        # attribute of the class.\n        # If a class gets imported into the module real_modname\n        # the analyzer won't find the source of the class, if\n        # it looks in real_modname.\n        return super().generate(more_content=more_content,\n                                check_module=check_module,\n                                all_members=all_members)\n\n\nclass ExceptionDocumenter(ClassDocumenter):\n    \"\"\"\n    Specialized ClassDocumenter subclass for exceptions.\n    \"\"\"\n    objtype = 'exception'\n    member_order = 10\n\n    # needs a higher priority than ClassDocumenter\n    priority = 10\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, type) and issubclass(member, BaseException)\n\n\nclass DataDocumenter(ModuleLevelDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data items.\n    \"\"\"\n    objtype = 'data'\n    member_order = 40\n    priority = -10\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(parent, ModuleDocumenter) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain annotation for this data\n            try:\n                annotations = get_type_hints(self.parent)\n            except NameError:\n                # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n                annotations = safe_getattr(self.parent, '__annotations__', {})\n            except TypeError:\n                annotations = {}\n            except KeyError:\n                # a broken class found (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n                annotations = {}\n            except AttributeError:\n                # AttributeError is raised on 3.5.2 (fixed by 3.5.3)\n                annotations = {}\n\n            if self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n                    self.add_line('   :type: ' + self.analyzer.annotations[key],\n                                  sourcename)\n\n            try:\n                if self.object is UNINITIALIZED_ATTR:\n                    pass\n                else:\n                    objrepr = object_description(self.object)\n                    self.add_line('   :value: ' + objrepr, sourcename)\n            except ValueError:\n                pass\n        elif self.options.annotation is SUPPRESS:\n            pass\n        else:\n            self.add_line('   :annotation: %s' % self.options.annotation,\n                          sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n\nclass DataDeclarationDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data that cannot be imported\n    because they are declared without initial value (refs: PEP-526).\n    \"\"\"\n    objtype = 'datadecl'\n    directivetype = 'data'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only INSTANCEATTR members.\"\"\"\n        return (isinstance(parent, ModuleDocumenter) and\n                isattr and\n                member is INSTANCEATTR)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as a data\n        self.objtype = 'data'\n        self.object = UNINITIALIZED_ATTR\n        try:\n            # import module to obtain type annotation\n            self.parent = importlib.import_module(self.modname)\n        except ImportError:\n            pass\n\n        return True\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        \"\"\"Never try to get a docstring from the object.\"\"\"\n        super().add_content(more_content, no_docstring=True)\n\n(B)                                         type_aliases=self.env.config.autodoc_type_aliases)\n                return type(self.object), '__call__', sig\n            except ValueError:\n                pass\n\n        # Now we check if the 'obj' class has a '__new__' method\n        new = get_user_defined_function_or_method(self.object, '__new__')\n\n        if new is not None:\n            if \"{0.__module__}.{0.__qualname__}\".format(new) in _CLASS_NEW_BLACKLIST:\n                new = None\n\n        if new is not None:\n            self.env.app.emit('autodoc-before-process-signature', new, True)\n            try:\n                sig = inspect.signature(new, bound_method=True,\n                                        type_aliases=self.env.config.autodoc_type_aliases)\n                return self.object, '__new__', sig\n            except ValueError:\n                pass\n\n        # Finally, we should have at least __init__ implemented\n        init = get_user_defined_function_or_method(self.object, '__init__')\n        if init is not None:\n            self.env.app.emit('autodoc-before-process-signature', init, True)\n            try:\n                sig = inspect.signature(init, bound_method=True,\n                                        type_aliases=self.env.config.autodoc_type_aliases)\n                return self.object, '__init__', sig\n            except ValueError:\n                pass\n\n        # None of the attributes are user-defined, so fall back to let inspect\n        # handle it.\n        # We don't know the exact method that inspect.signature will read\n        # the signature from, so just pass the object itself to our hook.\n        self.env.app.emit('autodoc-before-process-signature', self.object, False)\n        try:\n            sig = inspect.signature(self.object, bound_method=False,\n                                    type_aliases=self.env.config.autodoc_type_aliases)\n            return None, None, sig\n        except ValueError:\n            pass\n\n        # Still no signature: happens e.g. for old-style classes\n        # with __init__ in C and no `__text_signature__`.\n        return None, None, None\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.env.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            self._signature_class, self._signature_method_name, sig = self._get_signature()\n        except TypeError as exc:\n            # __signature__ attribute contained junk\n            logger.warning(__(\"Failed to get a constructor signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n\n        if sig is None:\n            return None\n\n        return stringify_signature(sig, show_return_annotation=False, **kwargs)\n\n    def format_signature(self, **kwargs: Any) -> str:\n        if self.doc_as_attr:\n            return ''\n\n        sig = super().format_signature()\n        sigs = []\n\n        overloads = self.get_overloaded_signatures()\n        if overloads:\n            # Use signatures for overloaded methods instead of the implementation method.\n            method = safe_getattr(self._signature_class, self._signature_method_name, None)\n            __globals__ = safe_getattr(method, '__globals__', {})\n            for overload in overloads:\n                overload = evaluate_signature(overload, __globals__,\n                                              self.env.config.autodoc_type_aliases)\n\n                parameters = list(overload.parameters.values())\n                overload = overload.replace(parameters=parameters[1:],\n                                            return_annotation=Parameter.empty)\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n        else:\n            sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def get_overloaded_signatures(self) -> List[Signature]:\n        if self._signature_class and self._signature_method_name:\n            for cls in self._signature_class.__mro__:\n                try:\n                    analyzer = ModuleAnalyzer.for_module(cls.__module__)\n                    analyzer.parse()\n                    qualname = '.'.join([cls.__qualname__, self._signature_method_name])\n                    if qualname in analyzer.overloads:\n                        return analyzer.overloads.get(qualname)\n                except PycodeError:\n                    pass\n\n        return []\n\n    def add_directive_header(self, sig: str) -> None:\n        sourcename = self.get_sourcename()\n\n        if self.doc_as_attr:\n            self.directivetype = 'attribute'\n        super().add_directive_header(sig)\n\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n        # add inheritance info, if wanted\n        if not self.doc_as_attr and self.options.show_inheritance:\n            sourcename = self.get_sourcename()\n            self.add_line('', sourcename)\n            if hasattr(self.object, '__bases__') and len(self.object.__bases__):\n                bases = [':class:`%s`' % b.__name__\n                         if b.__module__ in ('__builtin__', 'builtins')\n                         else ':class:`%s.%s`' % (b.__module__, b.__qualname__)\n                         for b in self.object.__bases__]\n                self.add_line('   ' + _('Bases: %s') % ', '.join(bases),\n                              sourcename)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if encoding is not None:\n            warnings.warn(\"The 'encoding' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx40Warning, stacklevel=2)\n        lines = getattr(self, '_new_docstrings', None)\n        if lines is not None:\n            return lines\n\n        content = self.env.config.autoclass_content\n\n        docstrings = []\n        attrdocstring = self.get_attr(self.object, '__doc__', None)\n        if attrdocstring:\n            docstrings.append(attrdocstring)\n\n        # for classes, what the \"docstring\" is can be controlled via a\n        # config value; the default is only the class docstring\n        if content in ('both', 'init'):\n            __init__ = self.get_attr(self.object, '__init__', None)\n            initdocstring = getdoc(__init__, self.get_attr,\n                                   self.env.config.autodoc_inherit_docstrings,\n                                   self.parent, self.object_name)\n            # for new-style classes, no __init__ means default __init__\n            if (initdocstring is not None and\n                (initdocstring == object.__init__.__doc__ or  # for pypy\n                 initdocstring.strip() == object.__init__.__doc__)):  # for !pypy\n                initdocstring = None\n            if not initdocstring:\n                # try __new__\n                __new__ = self.get_attr(self.object, '__new__', None)\n                initdocstring = getdoc(__new__, self.get_attr,\n                                       self.env.config.autodoc_inherit_docstrings,\n                                       self.parent, self.object_name)\n                # for new-style classes, no __new__ means default __new__\n                if (initdocstring is not None and\n                    (initdocstring == object.__new__.__doc__ or  # for pypy\n                     initdocstring.strip() == object.__new__.__doc__)):  # for !pypy\n                    initdocstring = None\n            if initdocstring:\n                if content == 'init':\n                    docstrings = [initdocstring]\n                else:\n                    docstrings.append(initdocstring)\n\n        tab_width = self.directive.state.document.settings.tab_width\n        return [prepare_docstring(docstring, ignore, tab_width) for docstring in docstrings]\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        if self.doc_as_attr:\n            classname = safe_getattr(self.object, '__qualname__', None)\n            if not classname:\n                classname = safe_getattr(self.object, '__name__', None)\n            if classname:\n                module = safe_getattr(self.object, '__module__', None)\n                parentmodule = safe_getattr(self.parent, '__module__', None)\n                if module and module != parentmodule:\n                    classname = str(module) + '.' + str(classname)\n                content = StringList([_('alias of :class:`%s`') % classname], source='')\n                super().add_content(content, no_docstring=True)\n        else:\n            super().add_content(more_content)\n\n    def document_members(self, all_members: bool = False) -> None:\n        if self.doc_as_attr:\n            return\n        super().document_members(all_members)\n\n    def generate(self, more_content: Any = None, real_modname: str = None,\n                 check_module: bool = False, all_members: bool = False) -> None:\n        # Do not pass real_modname and use the name from the __module__\n        # attribute of the class.\n        # If a class gets imported into the module real_modname\n        # the analyzer won't find the source of the class, if\n        # it looks in real_modname.\n        return super().generate(more_content=more_content,\n                                check_module=check_module,\n                                all_members=all_members)\n\n\nclass ExceptionDocumenter(ClassDocumenter):\n    \"\"\"\n    Specialized ClassDocumenter subclass for exceptions.\n    \"\"\"\n    objtype = 'exception'\n    member_order = 10\n\n    # needs a higher priority than ClassDocumenter\n    priority = 10\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, type) and issubclass(member, BaseException)\n\n\nclass DataDocumenter(ModuleLevelDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data items.\n    \"\"\"\n    objtype = 'data'\n    member_order = 40\n    priority = -10\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(parent, ModuleDocumenter) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain annotation for this data\n            try:\n                annotations = get_type_hints(self.parent)\n            except NameError:\n                # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n                annotations = safe_getattr(self.parent, '__annotations__', {})\n            except TypeError:\n                annotations = {}\n            except KeyError:\n                # a broken class found (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n                annotations = {}\n            except AttributeError:\n                # AttributeError is raised on 3.5.2 (fixed by 3.5.3)\n                annotations = {}\n\n            if self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n                    self.add_line('   :type: ' + self.analyzer.annotations[key],\n                                  sourcename)\n\n            try:\n                if self.object is UNINITIALIZED_ATTR:\n                    pass\n                else:\n                    objrepr = object_description(self.object)\n                    self.add_line('   :value: ' + objrepr, sourcename)\n            except ValueError:\n                pass\n        elif self.options.annotation is SUPPRESS:\n            pass\n        else:\n            self.add_line('   :annotation: %s' % self.options.annotation,\n                          sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n\nclass DataDeclarationDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data that cannot be imported\n    because they are declared without initial value (refs: PEP-526).\n    \"\"\"\n    objtype = 'datadecl'\n    directivetype = 'data'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only INSTANCEATTR members.\"\"\"\n        return (isinstance(parent, ModuleDocumenter) and\n                isattr and\n                member is INSTANCEATTR)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as a data\n        self.objtype = 'data'\n        self.object = UNINITIALIZED_ATTR\n        try:\n            # import module to obtain type annotation\n            self.parent = importlib.import_module(self.modname)\n        except ImportError:\n            pass\n\n        return True\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        \"\"\"Never try to get a docstring from the object.\"\"\"\n        super().add_content(more_content, no_docstring=True)\n\n\nclass GenericAliasDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for GenericAliases.\n    \"\"\"\n\n    objtype = 'genericalias'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isgenericalias(member)\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        name = stringify_typehint(self.object)\n        content = StringList([_('alias of %s') % name], source='')\n        super().add_content(content)\n\n\nclass TypeVarDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for TypeVars.\n    \"\"\"\n\n    objtype = 'typevar'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, TypeVar) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        if self.object.__doc__ != TypeVar.__doc__:\n            return super().get_doc()\n        else:\n            return []\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        attrs = [repr(self.object.__name__)]\n        for constraint in self.object.__constraints__:\n            attrs.append(stringify_typehint(constraint))\n        if self.object.__covariant__:\n            attrs.append(\"covariant=True\")\n        if self.object.__contravariant__:\n            attrs.append(\"contravariant=True\")\n\n        content = StringList([_('alias of TypeVar(%s)') % \", \".join(attrs)], source='')\n        super().add_content(content)\n\n\nclass MethodDocumenter(DocstringSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for methods (normal, static and class).\n    \"\"\"\n    objtype = 'method'\n    directivetype = 'method'\n    member_order = 50\n    priority = 1  # must be more than FunctionDocumenter\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isroutine(member) and \\\n            not isinstance(parent, ModuleDocumenter)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        ret = super().import_object(raiseerror)\n        if not ret:\n            return ret\n\n        # to distinguish classmethod/staticmethod\n        obj = self.parent.__dict__.get(self.object_name)\n        if obj is None:\n            obj = self.object\n\n        if (inspect.isclassmethod(obj) or\n                inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name)):\n            # document class and static members before ordinary ones\n            self.member_order = self.member_order - 1\n\n        return ret\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.env.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            if self.object == object.__init__ and self.parent != object:\n                # Classes not having own __init__() method are shown as no arguments.\n                #\n                # Note: The signature of object.__init__() is (self, /, *args, **kwargs).\n                #       But it makes users confused.\n                args = '()'\n            else:\n                if inspect.isstaticmethod(self.object, cls=self.parent, name=self.object_name):\n                    self.env.app.emit('autodoc-before-process-signature', self.object, False)\n                    sig = inspect.signature(self.object, bound_method=False,\n                                            type_aliases=self.env.config.autodoc_type_aliases)\n                else:\n                    self.env.app.emit('autodoc-before-process-signature', self.object, True)\n                    sig = inspect.signature(self.object, bound_method=True,\n                                            follow_wrapped=True,\n                                            type_aliases=self.env.config.autodoc_type_aliases)\n                args = stringify_signature(sig, **kwargs)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n        except ValueError:\n            args = ''\n\n        if self.env.config.strip_signature_backslash:\n            # escape backslashes for reST\n            args = args.replace('\\\\', '\\\\\\\\')\n        return args\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n\n        sourcename = self.get_sourcename()\n        obj = self.parent.__dict__.get(self.object_name, self.object)\n        if inspect.isabstractmethod(obj):\n            self.add_line('   :abstractmethod:', sourcename)\n        if inspect.iscoroutinefunction(obj):\n            self.add_line('   :async:', sourcename)\n        if inspect.isclassmethod(obj):\n            self.add_line('   :classmethod:', sourcename)\n        if inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name):\n            self.add_line('   :staticmethod:', sourcename)\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def format_signature(self, **kwargs: Any) -> str:\n        sigs = []\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.overloads:\n            # Use signatures for overloaded methods instead of the implementation method.\n            overloaded = True\n        else:\n            overloaded = False\n            sig = super().format_signature(**kwargs)\n            sigs.append(sig)\n\n        meth = self.parent.__dict__.get(self.objpath[-1])\n        if inspect.is_singledispatch_method(meth):\n            # append signature of singledispatch'ed functions\n            for typ, func in meth.dispatcher.registry.items():\n                if typ is object:\n                    pass  # default implementation. skipped.\n                else:\n                    self.annotate_to_first_argument(func, typ)\n\n                    documenter = MethodDocumenter(self.directive, '')\n                    documenter.parent = self.parent\n                    documenter.object = func\n                    documenter.objpath = [None]\n                    sigs.append(documenter.format_signature())\n        if overloaded:\n            __globals__ = safe_getattr(self.object, '__globals__', {})\n            for overload in self.analyzer.overloads.get('.'.join(self.objpath)):\n                overload = evaluate_signature(overload, __globals__,\n                                              self.env.config.autodoc_type_aliases)\n\n                if not inspect.isstaticmethod(self.object, cls=self.parent,\n                                              name=self.object_name):\n                    parameters = list(overload.parameters.values())\n                    overload = overload.replace(parameters=parameters[1:])\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def annotate_to_first_argument(self, func: Callable, typ: Type) -> None:\n        \"\"\"Annotate type hint to the first argument of function if needed.\"\"\"\n        try:\n            sig = inspect.signature(func, type_aliases=self.env.config.autodoc_type_aliases)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return\n        except ValueError:\n            return\n        if len(sig.parameters) == 1:\n            return\n\n        params = list(sig.parameters.values())\n        if params[1].annotation is Parameter.empty:\n            params[1] = params[1].replace(annotation=typ)\n            try:\n                func.__signature__ = sig.replace(parameters=params)  # type: ignore\n            except TypeError:\n                # failed to update signature (ex. built-in or extension types)\n                return\n\n\nclass SingledispatchMethodDocumenter(MethodDocumenter):\n    \"\"\"\n    Used to be a specialized Documenter subclass for singledispatch'ed methods.\n\n    Retained for backwards compatibility, now does the same as the MethodDocumenter\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        warnings.warn(\"%s is deprecated.\" % self.__class__.__name__,\n                      RemovedInSphinx50Warning, stacklevel=2)\n        super().__init__(*args, **kwargs)\n\n\nclass AttributeDocumenter(DocstringStripSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for attributes.\n    \"\"\"\n    objtype = 'attribute'\n    member_order = 60\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    # must be higher than the MethodDocumenter, else it will recognize\n    # some non-data descriptors as methods\n    priority = 10\n\n    @staticmethod\n    def is_function_or_method(obj: Any) -> bool:\n        return inspect.isfunction(obj) or inspect.isbuiltin(obj) or inspect.ismethod(obj)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        if inspect.isattributedescriptor(member):\n            return True\n        elif (not isinstance(parent, ModuleDocumenter) and\n              not inspect.isroutine(member) and\n              not isinstance(member, type)):\n            return True\n        else:\n            return False\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def isinstanceattribute(self) -> bool:\n        \"\"\"Check the subject is an instance attribute.\"\"\"\n        try:\n            analyzer = ModuleAnalyzer.for_module(self.modname)\n            attr_docs = analyzer.find_attr_docs()\n            if self.objpath:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if key in attr_docs:\n                    return True\n\n            return False\n        except PycodeError:\n            return False\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        try:\n            ret = super().import_object(raiseerror=True)\n            if inspect.isenumattribute(self.object):\n                self.object = self.object.value\n            if inspect.isattributedescriptor(self.object):\n                self._datadescriptor = True\n            else:\n                # if it's not a data descriptor\n                self._datadescriptor = False\n        except ImportError as exc:\n            if self.isinstanceattribute():\n                self.object = INSTANCEATTR\n                self._datadescriptor = False\n                ret = True\n            elif raiseerror:\n                raise\n            else:\n                logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                self.env.note_reread()\n                ret = False\n\n        return ret\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain type annotation for this attribute\n            try:\n                annotations = get_type_hints(self.parent)\n            except NameError:\n                # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n                annotations = safe_getattr(self.parent, '__annotations__', {})\n            except TypeError:\n                annotations = {}\n            except KeyError:\n                # a broken class found (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n                annotations = {}\n            except AttributeError:\n                # AttributeError is raised on 3.5.2 (fixed by 3.5.3)\n                annotations = {}\n\n            if self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n(C)                     (initdocstring == object.__new__.__doc__ or  # for pypy\n                     initdocstring.strip() == object.__new__.__doc__)):  # for !pypy\n                    initdocstring = None\n            if initdocstring:\n                if content == 'init':\n                    docstrings = [initdocstring]\n                else:\n                    docstrings.append(initdocstring)\n\n        tab_width = self.directive.state.document.settings.tab_width\n        return [prepare_docstring(docstring, ignore, tab_width) for docstring in docstrings]\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        if self.doc_as_attr:\n            classname = safe_getattr(self.object, '__qualname__', None)\n            if not classname:\n                classname = safe_getattr(self.object, '__name__', None)\n            if classname:\n                module = safe_getattr(self.object, '__module__', None)\n                parentmodule = safe_getattr(self.parent, '__module__', None)\n                if module and module != parentmodule:\n                    classname = str(module) + '.' + str(classname)\n                content = StringList([_('alias of :class:`%s`') % classname], source='')\n                super().add_content(content, no_docstring=True)\n        else:\n            super().add_content(more_content)\n\n    def document_members(self, all_members: bool = False) -> None:\n        if self.doc_as_attr:\n            return\n        super().document_members(all_members)\n\n    def generate(self, more_content: Any = None, real_modname: str = None,\n                 check_module: bool = False, all_members: bool = False) -> None:\n        # Do not pass real_modname and use the name from the __module__\n        # attribute of the class.\n        # If a class gets imported into the module real_modname\n        # the analyzer won't find the source of the class, if\n        # it looks in real_modname.\n        return super().generate(more_content=more_content,\n                                check_module=check_module,\n                                all_members=all_members)\n\n\nclass ExceptionDocumenter(ClassDocumenter):\n    \"\"\"\n    Specialized ClassDocumenter subclass for exceptions.\n    \"\"\"\n    objtype = 'exception'\n    member_order = 10\n\n    # needs a higher priority than ClassDocumenter\n    priority = 10\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, type) and issubclass(member, BaseException)\n\n\nclass DataDocumenter(ModuleLevelDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data items.\n    \"\"\"\n    objtype = 'data'\n    member_order = 40\n    priority = -10\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(parent, ModuleDocumenter) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain annotation for this data\n            try:\n                annotations = get_type_hints(self.parent)\n            except NameError:\n                # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n                annotations = safe_getattr(self.parent, '__annotations__', {})\n            except TypeError:\n                annotations = {}\n            except KeyError:\n                # a broken class found (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n                annotations = {}\n            except AttributeError:\n                # AttributeError is raised on 3.5.2 (fixed by 3.5.3)\n                annotations = {}\n\n            if self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n                    self.add_line('   :type: ' + self.analyzer.annotations[key],\n                                  sourcename)\n\n            try:\n                if self.object is UNINITIALIZED_ATTR:\n                    pass\n                else:\n                    objrepr = object_description(self.object)\n                    self.add_line('   :value: ' + objrepr, sourcename)\n            except ValueError:\n                pass\n        elif self.options.annotation is SUPPRESS:\n            pass\n        else:\n            self.add_line('   :annotation: %s' % self.options.annotation,\n                          sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n\nclass DataDeclarationDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data that cannot be imported\n    because they are declared without initial value (refs: PEP-526).\n    \"\"\"\n    objtype = 'datadecl'\n    directivetype = 'data'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only INSTANCEATTR members.\"\"\"\n        return (isinstance(parent, ModuleDocumenter) and\n                isattr and\n                member is INSTANCEATTR)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as a data\n        self.objtype = 'data'\n        self.object = UNINITIALIZED_ATTR\n        try:\n            # import module to obtain type annotation\n            self.parent = importlib.import_module(self.modname)\n        except ImportError:\n            pass\n\n        return True\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        \"\"\"Never try to get a docstring from the object.\"\"\"\n        super().add_content(more_content, no_docstring=True)\n\n\nclass GenericAliasDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for GenericAliases.\n    \"\"\"\n\n    objtype = 'genericalias'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isgenericalias(member)\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        name = stringify_typehint(self.object)\n        content = StringList([_('alias of %s') % name], source='')\n        super().add_content(content)\n\n\nclass TypeVarDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for TypeVars.\n    \"\"\"\n\n    objtype = 'typevar'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, TypeVar) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        if self.object.__doc__ != TypeVar.__doc__:\n            return super().get_doc()\n        else:\n            return []\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        attrs = [repr(self.object.__name__)]\n        for constraint in self.object.__constraints__:\n            attrs.append(stringify_typehint(constraint))\n        if self.object.__covariant__:\n            attrs.append(\"covariant=True\")\n        if self.object.__contravariant__:\n            attrs.append(\"contravariant=True\")\n\n        content = StringList([_('alias of TypeVar(%s)') % \", \".join(attrs)], source='')\n        super().add_content(content)\n\n\nclass MethodDocumenter(DocstringSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for methods (normal, static and class).\n    \"\"\"\n    objtype = 'method'\n    directivetype = 'method'\n    member_order = 50\n    priority = 1  # must be more than FunctionDocumenter\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isroutine(member) and \\\n            not isinstance(parent, ModuleDocumenter)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        ret = super().import_object(raiseerror)\n        if not ret:\n            return ret\n\n        # to distinguish classmethod/staticmethod\n        obj = self.parent.__dict__.get(self.object_name)\n        if obj is None:\n            obj = self.object\n\n        if (inspect.isclassmethod(obj) or\n                inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name)):\n            # document class and static members before ordinary ones\n            self.member_order = self.member_order - 1\n\n        return ret\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.env.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            if self.object == object.__init__ and self.parent != object:\n                # Classes not having own __init__() method are shown as no arguments.\n                #\n                # Note: The signature of object.__init__() is (self, /, *args, **kwargs).\n                #       But it makes users confused.\n                args = '()'\n            else:\n                if inspect.isstaticmethod(self.object, cls=self.parent, name=self.object_name):\n                    self.env.app.emit('autodoc-before-process-signature', self.object, False)\n                    sig = inspect.signature(self.object, bound_method=False,\n                                            type_aliases=self.env.config.autodoc_type_aliases)\n                else:\n                    self.env.app.emit('autodoc-before-process-signature', self.object, True)\n                    sig = inspect.signature(self.object, bound_method=True,\n                                            follow_wrapped=True,\n                                            type_aliases=self.env.config.autodoc_type_aliases)\n                args = stringify_signature(sig, **kwargs)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n        except ValueError:\n            args = ''\n\n        if self.env.config.strip_signature_backslash:\n            # escape backslashes for reST\n            args = args.replace('\\\\', '\\\\\\\\')\n        return args\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n\n        sourcename = self.get_sourcename()\n        obj = self.parent.__dict__.get(self.object_name, self.object)\n        if inspect.isabstractmethod(obj):\n            self.add_line('   :abstractmethod:', sourcename)\n        if inspect.iscoroutinefunction(obj):\n            self.add_line('   :async:', sourcename)\n        if inspect.isclassmethod(obj):\n            self.add_line('   :classmethod:', sourcename)\n        if inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name):\n            self.add_line('   :staticmethod:', sourcename)\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def format_signature(self, **kwargs: Any) -> str:\n        sigs = []\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.overloads:\n            # Use signatures for overloaded methods instead of the implementation method.\n            overloaded = True\n        else:\n            overloaded = False\n            sig = super().format_signature(**kwargs)\n            sigs.append(sig)\n\n        meth = self.parent.__dict__.get(self.objpath[-1])\n        if inspect.is_singledispatch_method(meth):\n            # append signature of singledispatch'ed functions\n            for typ, func in meth.dispatcher.registry.items():\n                if typ is object:\n                    pass  # default implementation. skipped.\n                else:\n                    self.annotate_to_first_argument(func, typ)\n\n                    documenter = MethodDocumenter(self.directive, '')\n                    documenter.parent = self.parent\n                    documenter.object = func\n                    documenter.objpath = [None]\n                    sigs.append(documenter.format_signature())\n        if overloaded:\n            __globals__ = safe_getattr(self.object, '__globals__', {})\n            for overload in self.analyzer.overloads.get('.'.join(self.objpath)):\n                overload = evaluate_signature(overload, __globals__,\n                                              self.env.config.autodoc_type_aliases)\n\n                if not inspect.isstaticmethod(self.object, cls=self.parent,\n                                              name=self.object_name):\n                    parameters = list(overload.parameters.values())\n                    overload = overload.replace(parameters=parameters[1:])\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def annotate_to_first_argument(self, func: Callable, typ: Type) -> None:\n        \"\"\"Annotate type hint to the first argument of function if needed.\"\"\"\n        try:\n            sig = inspect.signature(func, type_aliases=self.env.config.autodoc_type_aliases)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return\n        except ValueError:\n            return\n        if len(sig.parameters) == 1:\n            return\n\n        params = list(sig.parameters.values())\n        if params[1].annotation is Parameter.empty:\n            params[1] = params[1].replace(annotation=typ)\n            try:\n                func.__signature__ = sig.replace(parameters=params)  # type: ignore\n            except TypeError:\n                # failed to update signature (ex. built-in or extension types)\n                return\n\n\nclass SingledispatchMethodDocumenter(MethodDocumenter):\n    \"\"\"\n    Used to be a specialized Documenter subclass for singledispatch'ed methods.\n\n    Retained for backwards compatibility, now does the same as the MethodDocumenter\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        warnings.warn(\"%s is deprecated.\" % self.__class__.__name__,\n                      RemovedInSphinx50Warning, stacklevel=2)\n        super().__init__(*args, **kwargs)\n\n\nclass AttributeDocumenter(DocstringStripSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for attributes.\n    \"\"\"\n    objtype = 'attribute'\n    member_order = 60\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    # must be higher than the MethodDocumenter, else it will recognize\n    # some non-data descriptors as methods\n    priority = 10\n\n    @staticmethod\n    def is_function_or_method(obj: Any) -> bool:\n        return inspect.isfunction(obj) or inspect.isbuiltin(obj) or inspect.ismethod(obj)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        if inspect.isattributedescriptor(member):\n            return True\n        elif (not isinstance(parent, ModuleDocumenter) and\n              not inspect.isroutine(member) and\n              not isinstance(member, type)):\n            return True\n        else:\n            return False\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def isinstanceattribute(self) -> bool:\n        \"\"\"Check the subject is an instance attribute.\"\"\"\n        try:\n            analyzer = ModuleAnalyzer.for_module(self.modname)\n            attr_docs = analyzer.find_attr_docs()\n            if self.objpath:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if key in attr_docs:\n                    return True\n\n            return False\n        except PycodeError:\n            return False\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        try:\n            ret = super().import_object(raiseerror=True)\n            if inspect.isenumattribute(self.object):\n                self.object = self.object.value\n            if inspect.isattributedescriptor(self.object):\n                self._datadescriptor = True\n            else:\n                # if it's not a data descriptor\n                self._datadescriptor = False\n        except ImportError as exc:\n            if self.isinstanceattribute():\n                self.object = INSTANCEATTR\n                self._datadescriptor = False\n                ret = True\n            elif raiseerror:\n                raise\n            else:\n                logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                self.env.note_reread()\n                ret = False\n\n        return ret\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain type annotation for this attribute\n            try:\n                annotations = get_type_hints(self.parent)\n            except NameError:\n                # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n                annotations = safe_getattr(self.parent, '__annotations__', {})\n            except TypeError:\n                annotations = {}\n            except KeyError:\n                # a broken class found (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n                annotations = {}\n            except AttributeError:\n                # AttributeError is raised on 3.5.2 (fixed by 3.5.3)\n                annotations = {}\n\n            if self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n                    self.add_line('   :type: ' + self.analyzer.annotations[key],\n                                  sourcename)\n\n            # data descriptors do not have useful values\n            if not self._datadescriptor:\n                try:\n                    if self.object is INSTANCEATTR:\n                        pass\n                    else:\n                        objrepr = object_description(self.object)\n                        self.add_line('   :value: ' + objrepr, sourcename)\n                except ValueError:\n                    pass\n        elif self.options.annotation is SUPPRESS:\n            pass\n        else:\n            self.add_line('   :annotation: %s' % self.options.annotation, sourcename)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        try:\n            # Disable `autodoc_inherit_docstring` temporarily to avoid to obtain\n            # a docstring from the value which descriptor returns unexpectedly.\n            # ref: https://github.com/sphinx-doc/sphinx/issues/7805\n            orig = self.env.config.autodoc_inherit_docstrings\n            self.env.config.autodoc_inherit_docstrings = False  # type: ignore\n            return super().get_doc(encoding, ignore)\n        finally:\n            self.env.config.autodoc_inherit_docstrings = orig  # type: ignore\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        if not self._datadescriptor:\n            # if it's not a data descriptor, its docstring is very probably the\n            # wrong thing to display\n            no_docstring = True\n        super().add_content(more_content, no_docstring)\n\n\nclass PropertyDocumenter(DocstringStripSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for properties.\n    \"\"\"\n    objtype = 'property'\n    directivetype = 'method'\n    member_order = 60\n\n    # before AttributeDocumenter\n    priority = AttributeDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isproperty(member) and isinstance(parent, ClassDocumenter)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if inspect.isabstractmethod(self.object):\n            self.add_line('   :abstractmethod:', sourcename)\n        self.add_line('   :property:', sourcename)\n\n\nclass InstanceAttributeDocumenter(AttributeDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for attributes that cannot be imported\n    because they are instance attributes (e.g. assigned in __init__).\n    \"\"\"\n    objtype = 'instanceattribute'\n    directivetype = 'attribute'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only INSTANCEATTR members.\"\"\"\n        return (not isinstance(parent, ModuleDocumenter) and\n                isattr and\n                member is INSTANCEATTR)\n\n    def import_parent(self) -> Any:\n        try:\n            parent = importlib.import_module(self.modname)\n            for name in self.objpath[:-1]:\n                parent = self.get_attr(parent, name)\n\n            return parent\n        except (ImportError, AttributeError):\n            return None\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as an attribute\n        self.objtype = 'attribute'\n        self.object = INSTANCEATTR\n        self.parent = self.import_parent()\n        self._datadescriptor = False\n        return True\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        \"\"\"Never try to get a docstring from the object.\"\"\"\n        super().add_content(more_content, no_docstring=True)\n\n\nclass SlotsAttributeDocumenter(AttributeDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for attributes that cannot be imported\n    because they are attributes in __slots__.\n    \"\"\"\n    objtype = 'slotsattribute'\n    directivetype = 'attribute'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only SLOTSATTR members.\"\"\"\n        return member is SLOTSATTR\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as an attribute\n        self.objtype = 'attribute'\n        self._datadescriptor = True\n\n        with mock(self.env.config.autodoc_mock_imports):\n            try:\n                ret = import_object(self.modname, self.objpath[:-1], 'class',\n                                    attrgetter=self.get_attr,\n                                    warningiserror=self.env.config.autodoc_warningiserror)\n                self.module, _, _, self.parent = ret\n                return True\n            except ImportError as exc:\n                if raiseerror:\n                    raise\n                else:\n                    logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                    self.env.note_reread()\n                    return False\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        \"\"\"Decode and return lines of the docstring(s) for the object.\"\"\"\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n        name = self.objpath[-1]\n        __slots__ = safe_getattr(self.parent, '__slots__', [])\n        if isinstance(__slots__, dict) and isinstance(__slots__.get(name), str):\n            docstring = prepare_docstring(__slots__[name])\n            return [docstring]\n        else:\n(D) \n    def format_signature(self, **kwargs: Any) -> str:\n        sigs = []\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.overloads:\n            # Use signatures for overloaded functions instead of the implementation function.\n            overloaded = True\n        else:\n            overloaded = False\n            sig = super().format_signature(**kwargs)\n            sigs.append(sig)\n\n        if inspect.is_singledispatch_function(self.object):\n            # append signature of singledispatch'ed functions\n            for typ, func in self.object.registry.items():\n                if typ is object:\n                    pass  # default implementation. skipped.\n                else:\n                    self.annotate_to_first_argument(func, typ)\n\n                    documenter = FunctionDocumenter(self.directive, '')\n                    documenter.object = func\n                    documenter.objpath = [None]\n                    sigs.append(documenter.format_signature())\n        if overloaded:\n            __globals__ = safe_getattr(self.object, '__globals__', {})\n            for overload in self.analyzer.overloads.get('.'.join(self.objpath)):\n                overload = evaluate_signature(overload, __globals__,\n                                              self.env.config.autodoc_type_aliases)\n\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def annotate_to_first_argument(self, func: Callable, typ: Type) -> None:\n        \"\"\"Annotate type hint to the first argument of function if needed.\"\"\"\n        try:\n            sig = inspect.signature(func, type_aliases=self.env.config.autodoc_type_aliases)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a function signature for %s: %s\"),\n                           self.fullname, exc)\n            return\n        except ValueError:\n            return\n\n        if len(sig.parameters) == 0:\n            return\n\n        params = list(sig.parameters.values())\n        if params[0].annotation is Parameter.empty:\n            params[0] = params[0].replace(annotation=typ)\n            try:\n                func.__signature__ = sig.replace(parameters=params)  # type: ignore\n            except TypeError:\n                # failed to update signature (ex. built-in or extension types)\n                return\n\n\nclass SingledispatchFunctionDocumenter(FunctionDocumenter):\n    \"\"\"\n    Used to be a specialized Documenter subclass for singledispatch'ed functions.\n\n    Retained for backwards compatibility, now does the same as the FunctionDocumenter\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        warnings.warn(\"%s is deprecated.\" % self.__class__.__name__,\n                      RemovedInSphinx50Warning, stacklevel=2)\n        super().__init__(*args, **kwargs)\n\n\nclass DecoratorDocumenter(FunctionDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for decorator functions.\n    \"\"\"\n    objtype = 'decorator'\n\n    # must be lower than FunctionDocumenter\n    priority = -1\n\n    def format_args(self, **kwargs: Any) -> Any:\n        args = super().format_args(**kwargs)\n        if ',' in args:\n            return args\n        else:\n            return None\n\n\n# Types which have confusing metaclass signatures it would be best not to show.\n# These are listed by name, rather than storing the objects themselves, to avoid\n# needing to import the modules.\n_METACLASS_CALL_BLACKLIST = [\n    'enum.EnumMeta.__call__',\n]\n\n\n# Types whose __new__ signature is a pass-thru.\n_CLASS_NEW_BLACKLIST = [\n    'typing.Generic.__new__',\n]\n\n\nclass ClassDocumenter(DocstringSignatureMixin, ModuleLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for classes.\n    \"\"\"\n    objtype = 'class'\n    member_order = 20\n    option_spec = {\n        'members': members_option, 'undoc-members': bool_option,\n        'noindex': bool_option, 'inherited-members': inherited_members_option,\n        'show-inheritance': bool_option, 'member-order': member_order_option,\n        'exclude-members': exclude_members_option,\n        'private-members': members_option, 'special-members': members_option,\n    }  # type: Dict[str, Callable]\n\n    _signature_class = None  # type: Any\n    _signature_method_name = None  # type: str\n\n    def __init__(self, *args: Any) -> None:\n        super().__init__(*args)\n        merge_members_option(self.options)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, type)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        ret = super().import_object(raiseerror)\n        # if the class is documented under another name, document it\n        # as data/attribute\n        if ret:\n            if hasattr(self.object, '__name__'):\n                self.doc_as_attr = (self.objpath[-1] != self.object.__name__)\n            else:\n                self.doc_as_attr = True\n        return ret\n\n    def _get_signature(self) -> Tuple[Optional[Any], Optional[str], Optional[Signature]]:\n        def get_user_defined_function_or_method(obj: Any, attr: str) -> Any:\n            \"\"\" Get the `attr` function or method from `obj`, if it is user-defined. \"\"\"\n            if inspect.is_builtin_class_method(obj, attr):\n                return None\n            attr = self.get_attr(obj, attr, None)\n            if not (inspect.ismethod(attr) or inspect.isfunction(attr)):\n                return None\n            return attr\n\n        # This sequence is copied from inspect._signature_from_callable.\n        # ValueError means that no signature could be found, so we keep going.\n\n        # First, let's see if it has an overloaded __call__ defined\n        # in its metaclass\n        call = get_user_defined_function_or_method(type(self.object), '__call__')\n\n        if call is not None:\n            if \"{0.__module__}.{0.__qualname__}\".format(call) in _METACLASS_CALL_BLACKLIST:\n                call = None\n\n        if call is not None:\n            self.env.app.emit('autodoc-before-process-signature', call, True)\n            try:\n                sig = inspect.signature(call, bound_method=True,\n                                        type_aliases=self.env.config.autodoc_type_aliases)\n                return type(self.object), '__call__', sig\n            except ValueError:\n                pass\n\n        # Now we check if the 'obj' class has a '__new__' method\n        new = get_user_defined_function_or_method(self.object, '__new__')\n\n        if new is not None:\n            if \"{0.__module__}.{0.__qualname__}\".format(new) in _CLASS_NEW_BLACKLIST:\n                new = None\n\n        if new is not None:\n            self.env.app.emit('autodoc-before-process-signature', new, True)\n            try:\n                sig = inspect.signature(new, bound_method=True,\n                                        type_aliases=self.env.config.autodoc_type_aliases)\n                return self.object, '__new__', sig\n            except ValueError:\n                pass\n\n        # Finally, we should have at least __init__ implemented\n        init = get_user_defined_function_or_method(self.object, '__init__')\n        if init is not None:\n            self.env.app.emit('autodoc-before-process-signature', init, True)\n            try:\n                sig = inspect.signature(init, bound_method=True,\n                                        type_aliases=self.env.config.autodoc_type_aliases)\n                return self.object, '__init__', sig\n            except ValueError:\n                pass\n\n        # None of the attributes are user-defined, so fall back to let inspect\n        # handle it.\n        # We don't know the exact method that inspect.signature will read\n        # the signature from, so just pass the object itself to our hook.\n        self.env.app.emit('autodoc-before-process-signature', self.object, False)\n        try:\n            sig = inspect.signature(self.object, bound_method=False,\n                                    type_aliases=self.env.config.autodoc_type_aliases)\n            return None, None, sig\n        except ValueError:\n            pass\n\n        # Still no signature: happens e.g. for old-style classes\n        # with __init__ in C and no `__text_signature__`.\n        return None, None, None\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.env.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            self._signature_class, self._signature_method_name, sig = self._get_signature()\n        except TypeError as exc:\n            # __signature__ attribute contained junk\n            logger.warning(__(\"Failed to get a constructor signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n\n        if sig is None:\n            return None\n\n        return stringify_signature(sig, show_return_annotation=False, **kwargs)\n\n    def format_signature(self, **kwargs: Any) -> str:\n        if self.doc_as_attr:\n            return ''\n\n        sig = super().format_signature()\n        sigs = []\n\n        overloads = self.get_overloaded_signatures()\n        if overloads:\n            # Use signatures for overloaded methods instead of the implementation method.\n            method = safe_getattr(self._signature_class, self._signature_method_name, None)\n            __globals__ = safe_getattr(method, '__globals__', {})\n            for overload in overloads:\n                overload = evaluate_signature(overload, __globals__,\n                                              self.env.config.autodoc_type_aliases)\n\n                parameters = list(overload.parameters.values())\n                overload = overload.replace(parameters=parameters[1:],\n                                            return_annotation=Parameter.empty)\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n        else:\n            sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def get_overloaded_signatures(self) -> List[Signature]:\n        if self._signature_class and self._signature_method_name:\n            for cls in self._signature_class.__mro__:\n                try:\n                    analyzer = ModuleAnalyzer.for_module(cls.__module__)\n                    analyzer.parse()\n                    qualname = '.'.join([cls.__qualname__, self._signature_method_name])\n                    if qualname in analyzer.overloads:\n                        return analyzer.overloads.get(qualname)\n                except PycodeError:\n                    pass\n\n        return []\n\n    def add_directive_header(self, sig: str) -> None:\n        sourcename = self.get_sourcename()\n\n        if self.doc_as_attr:\n            self.directivetype = 'attribute'\n        super().add_directive_header(sig)\n\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n        # add inheritance info, if wanted\n        if not self.doc_as_attr and self.options.show_inheritance:\n            sourcename = self.get_sourcename()\n            self.add_line('', sourcename)\n            if hasattr(self.object, '__bases__') and len(self.object.__bases__):\n                bases = [':class:`%s`' % b.__name__\n                         if b.__module__ in ('__builtin__', 'builtins')\n                         else ':class:`%s.%s`' % (b.__module__, b.__qualname__)\n                         for b in self.object.__bases__]\n                self.add_line('   ' + _('Bases: %s') % ', '.join(bases),\n                              sourcename)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if encoding is not None:\n            warnings.warn(\"The 'encoding' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx40Warning, stacklevel=2)\n        lines = getattr(self, '_new_docstrings', None)\n        if lines is not None:\n            return lines\n\n        content = self.env.config.autoclass_content\n\n        docstrings = []\n        attrdocstring = self.get_attr(self.object, '__doc__', None)\n        if attrdocstring:\n            docstrings.append(attrdocstring)\n\n        # for classes, what the \"docstring\" is can be controlled via a\n        # config value; the default is only the class docstring\n        if content in ('both', 'init'):\n            __init__ = self.get_attr(self.object, '__init__', None)\n            initdocstring = getdoc(__init__, self.get_attr,\n                                   self.env.config.autodoc_inherit_docstrings,\n                                   self.parent, self.object_name)\n            # for new-style classes, no __init__ means default __init__\n            if (initdocstring is not None and\n                (initdocstring == object.__init__.__doc__ or  # for pypy\n                 initdocstring.strip() == object.__init__.__doc__)):  # for !pypy\n                initdocstring = None\n            if not initdocstring:\n                # try __new__\n                __new__ = self.get_attr(self.object, '__new__', None)\n                initdocstring = getdoc(__new__, self.get_attr,\n                                       self.env.config.autodoc_inherit_docstrings,\n                                       self.parent, self.object_name)\n                # for new-style classes, no __new__ means default __new__\n                if (initdocstring is not None and\n                    (initdocstring == object.__new__.__doc__ or  # for pypy\n                     initdocstring.strip() == object.__new__.__doc__)):  # for !pypy\n                    initdocstring = None\n            if initdocstring:\n                if content == 'init':\n                    docstrings = [initdocstring]\n                else:\n                    docstrings.append(initdocstring)\n\n        tab_width = self.directive.state.document.settings.tab_width\n        return [prepare_docstring(docstring, ignore, tab_width) for docstring in docstrings]\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        if self.doc_as_attr:\n            classname = safe_getattr(self.object, '__qualname__', None)\n            if not classname:\n                classname = safe_getattr(self.object, '__name__', None)\n            if classname:\n                module = safe_getattr(self.object, '__module__', None)\n                parentmodule = safe_getattr(self.parent, '__module__', None)\n                if module and module != parentmodule:\n                    classname = str(module) + '.' + str(classname)\n                content = StringList([_('alias of :class:`%s`') % classname], source='')\n                super().add_content(content, no_docstring=True)\n        else:\n            super().add_content(more_content)\n\n    def document_members(self, all_members: bool = False) -> None:\n        if self.doc_as_attr:\n            return\n        super().document_members(all_members)\n\n    def generate(self, more_content: Any = None, real_modname: str = None,\n                 check_module: bool = False, all_members: bool = False) -> None:\n        # Do not pass real_modname and use the name from the __module__\n        # attribute of the class.\n        # If a class gets imported into the module real_modname\n        # the analyzer won't find the source of the class, if\n        # it looks in real_modname.\n        return super().generate(more_content=more_content,\n                                check_module=check_module,\n                                all_members=all_members)\n\n\nclass ExceptionDocumenter(ClassDocumenter):\n    \"\"\"\n    Specialized ClassDocumenter subclass for exceptions.\n    \"\"\"\n    objtype = 'exception'\n    member_order = 10\n\n    # needs a higher priority than ClassDocumenter\n    priority = 10\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, type) and issubclass(member, BaseException)\n\n\nclass DataDocumenter(ModuleLevelDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data items.\n    \"\"\"\n    objtype = 'data'\n    member_order = 40\n    priority = -10\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(parent, ModuleDocumenter) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain annotation for this data\n            try:\n                annotations = get_type_hints(self.parent)\n            except NameError:\n                # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n                annotations = safe_getattr(self.parent, '__annotations__', {})\n            except TypeError:\n                annotations = {}\n            except KeyError:\n                # a broken class found (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n                annotations = {}\n            except AttributeError:\n                # AttributeError is raised on 3.5.2 (fixed by 3.5.3)\n                annotations = {}\n\n            if self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n                    self.add_line('   :type: ' + self.analyzer.annotations[key],\n                                  sourcename)\n\n            try:\n                if self.object is UNINITIALIZED_ATTR:\n                    pass\n                else:\n                    objrepr = object_description(self.object)\n                    self.add_line('   :value: ' + objrepr, sourcename)\n            except ValueError:\n                pass\n        elif self.options.annotation is SUPPRESS:\n            pass\n        else:\n            self.add_line('   :annotation: %s' % self.options.annotation,\n                          sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n\nclass DataDeclarationDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data that cannot be imported\n    because they are declared without initial value (refs: PEP-526).\n    \"\"\"\n    objtype = 'datadecl'\n    directivetype = 'data'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only INSTANCEATTR members.\"\"\"\n        return (isinstance(parent, ModuleDocumenter) and\n                isattr and\n                member is INSTANCEATTR)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as a data\n        self.objtype = 'data'\n        self.object = UNINITIALIZED_ATTR\n        try:\n            # import module to obtain type annotation\n            self.parent = importlib.import_module(self.modname)\n        except ImportError:\n            pass\n\n        return True\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        \"\"\"Never try to get a docstring from the object.\"\"\"\n        super().add_content(more_content, no_docstring=True)\n\n\nclass GenericAliasDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for GenericAliases.\n    \"\"\"\n\n    objtype = 'genericalias'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isgenericalias(member)\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        name = stringify_typehint(self.object)\n        content = StringList([_('alias of %s') % name], source='')\n        super().add_content(content)\n\n\nclass TypeVarDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for TypeVars.\n    \"\"\"\n\n    objtype = 'typevar'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, TypeVar) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        if self.object.__doc__ != TypeVar.__doc__:\n            return super().get_doc()\n        else:\n            return []\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        attrs = [repr(self.object.__name__)]\n        for constraint in self.object.__constraints__:\n            attrs.append(stringify_typehint(constraint))\n        if self.object.__covariant__:\n            attrs.append(\"covariant=True\")\n        if self.object.__contravariant__:\n            attrs.append(\"contravariant=True\")\n\n        content = StringList([_('alias of TypeVar(%s)') % \", \".join(attrs)], source='')\n        super().add_content(content)\n\n\nclass MethodDocumenter(DocstringSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for methods (normal, static and class).\n    \"\"\"\n    objtype = 'method'\n    directivetype = 'method'\n    member_order = 50\n    priority = 1  # must be more than FunctionDocumenter\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isroutine(member) and \\\n            not isinstance(parent, ModuleDocumenter)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        ret = super().import_object(raiseerror)\n        if not ret:\n            return ret\n\n        # to distinguish classmethod/staticmethod\n        obj = self.parent.__dict__.get(self.object_name)\n        if obj is None:\n            obj = self.object\n\n        if (inspect.isclassmethod(obj) or\n                inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name)):\n            # document class and static members before ordinary ones\n            self.member_order = self.member_order - 1\n\n        return ret\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.env.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            if self.object == object.__init__ and self.parent != object:\n                # Classes not having own __init__() method are shown as no arguments.\n                #\n                # Note: The signature of object.__init__() is (self, /, *args, **kwargs).\n                #       But it makes users confused.\n                args = '()'\n            else:\n                if inspect.isstaticmethod(self.object, cls=self.parent, name=self.object_name):\n                    self.env.app.emit('autodoc-before-process-signature', self.object, False)\n                    sig = inspect.signature(self.object, bound_method=False,\n                                            type_aliases=self.env.config.autodoc_type_aliases)\n                else:\n                    self.env.app.emit('autodoc-before-process-signature', self.object, True)\n                    sig = inspect.signature(self.object, bound_method=True,\n                                            follow_wrapped=True,\n                                            type_aliases=self.env.config.autodoc_type_aliases)\n                args = stringify_signature(sig, **kwargs)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n        except ValueError:\n            args = ''\n\n        if self.env.config.strip_signature_backslash:\n            # escape backslashes for reST\n            args = args.replace('\\\\', '\\\\\\\\')\n        return args\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n\n        sourcename = self.get_sourcename()\n        obj = self.parent.__dict__.get(self.object_name, self.object)\n        if inspect.isabstractmethod(obj):\n            self.add_line('   :abstractmethod:', sourcename)\n        if inspect.iscoroutinefunction(obj):\n            self.add_line('   :async:', sourcename)\n        if inspect.isclassmethod(obj):\n            self.add_line('   :classmethod:', sourcename)\n        if inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name):\n            self.add_line('   :staticmethod:', sourcename)\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def format_signature(self, **kwargs: Any) -> str:\n        sigs = []\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.overloads:\n            # Use signatures for overloaded methods instead of the implementation method.\n            overloaded = True\n        else:\n            overloaded = False\n            sig = super().format_signature(**kwargs)\n            sigs.append(sig)\n", "answer": "D"}
{"uuid": "14e3a864-52ce-42e9-a17f-a637f26befe4", "issue_statement": "autodoc_type_aliases does not effect to variables and attributes\n**Describe the bug**\r\nautodoc_type_aliases does not effect to variables and attributes\r\n\r\n**To Reproduce**\r\n\r\n```\r\n# example.py\r\nfrom __future__ import annotations\r\n\r\n\r\n#: blah blah blah\r\nvar: String\r\n\r\n\r\nclass MyString:\r\n    \"mystring\"\r\n\r\n    #: blah blah blah\r\n    var: String\r\n```\r\n```\r\n# index.rst\r\n.. automodule:: example\r\n   :members:\r\n   :undoc-members:\r\n```\r\n```\r\n# conf.py\r\nautodoc_type_aliases = {\r\n    'String': 'example.MyString'\r\n}\r\n```\r\n\r\n**Expected behavior**\r\n`autodoc_type_aliases` should be applied to `example.var` and `example.MyString.var`.\r\n\r\n**Your project**\r\nN/A\r\n\r\n**Screenshots**\r\nN/A\r\n\r\n**Environment info**\r\n- OS: Mac\r\n- Python version: 3.9.0\r\n- Sphinx version: HEAD of 3.x branch\r\n- Sphinx extensions: sphinx.ext.autodoc\r\n- Extra tools: Nothing\r\n\r\n**Additional context**\r\nN/A", "choices": "(A)             docstrings.append(attrdocstring)\n\n        # for classes, what the \"docstring\" is can be controlled via a\n        # config value; the default is only the class docstring\n        if content in ('both', 'init'):\n            __init__ = self.get_attr(self.object, '__init__', None)\n            initdocstring = getdoc(__init__, self.get_attr,\n                                   self.config.autodoc_inherit_docstrings,\n                                   self.parent, self.object_name)\n            # for new-style classes, no __init__ means default __init__\n            if (initdocstring is not None and\n                (initdocstring == object.__init__.__doc__ or  # for pypy\n                 initdocstring.strip() == object.__init__.__doc__)):  # for !pypy\n                initdocstring = None\n            if not initdocstring:\n                # try __new__\n                __new__ = self.get_attr(self.object, '__new__', None)\n                initdocstring = getdoc(__new__, self.get_attr,\n                                       self.config.autodoc_inherit_docstrings,\n                                       self.parent, self.object_name)\n                # for new-style classes, no __new__ means default __new__\n                if (initdocstring is not None and\n                    (initdocstring == object.__new__.__doc__ or  # for pypy\n                     initdocstring.strip() == object.__new__.__doc__)):  # for !pypy\n                    initdocstring = None\n            if initdocstring:\n                if content == 'init':\n                    docstrings = [initdocstring]\n                else:\n                    docstrings.append(initdocstring)\n\n        tab_width = self.directive.state.document.settings.tab_width\n        return [prepare_docstring(docstring, ignore, tab_width) for docstring in docstrings]\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        if self.doc_as_attr:\n            classname = safe_getattr(self.object, '__qualname__', None)\n            if not classname:\n                classname = safe_getattr(self.object, '__name__', None)\n            if classname:\n                module = safe_getattr(self.object, '__module__', None)\n                parentmodule = safe_getattr(self.parent, '__module__', None)\n                if module and module != parentmodule:\n                    classname = str(module) + '.' + str(classname)\n                content = StringList([_('alias of :class:`%s`') % classname], source='')\n                super().add_content(content, no_docstring=True)\n        else:\n            super().add_content(more_content)\n\n    def document_members(self, all_members: bool = False) -> None:\n        if self.doc_as_attr:\n            return\n        super().document_members(all_members)\n\n    def generate(self, more_content: Any = None, real_modname: str = None,\n                 check_module: bool = False, all_members: bool = False) -> None:\n        # Do not pass real_modname and use the name from the __module__\n        # attribute of the class.\n        # If a class gets imported into the module real_modname\n        # the analyzer won't find the source of the class, if\n        # it looks in real_modname.\n        return super().generate(more_content=more_content,\n                                check_module=check_module,\n                                all_members=all_members)\n\n\nclass ExceptionDocumenter(ClassDocumenter):\n    \"\"\"\n    Specialized ClassDocumenter subclass for exceptions.\n    \"\"\"\n    objtype = 'exception'\n    member_order = 10\n\n    # needs a higher priority than ClassDocumenter\n    priority = 10\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, type) and issubclass(member, BaseException)\n\n\nclass DataDocumenter(ModuleLevelDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data items.\n    \"\"\"\n    objtype = 'data'\n    member_order = 40\n    priority = -10\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(parent, ModuleDocumenter) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain annotation for this data\n            try:\n                annotations = get_type_hints(self.parent)\n            except NameError:\n                # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n                annotations = safe_getattr(self.parent, '__annotations__', {})\n            except TypeError:\n                annotations = {}\n            except KeyError:\n                # a broken class found (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n                annotations = {}\n            except AttributeError:\n                # AttributeError is raised on 3.5.2 (fixed by 3.5.3)\n                annotations = {}\n\n            if self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n                    self.add_line('   :type: ' + self.analyzer.annotations[key],\n                                  sourcename)\n\n            try:\n                if self.object is UNINITIALIZED_ATTR:\n                    pass\n                else:\n                    objrepr = object_description(self.object)\n                    self.add_line('   :value: ' + objrepr, sourcename)\n            except ValueError:\n                pass\n        elif self.options.annotation is SUPPRESS:\n            pass\n        else:\n            self.add_line('   :annotation: %s' % self.options.annotation,\n                          sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n\nclass DataDeclarationDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data that cannot be imported\n    because they are declared without initial value (refs: PEP-526).\n    \"\"\"\n    objtype = 'datadecl'\n    directivetype = 'data'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only INSTANCEATTR members.\"\"\"\n        return (isinstance(parent, ModuleDocumenter) and\n                isattr and\n                member is INSTANCEATTR)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as a data\n        self.objtype = 'data'\n        self.object = UNINITIALIZED_ATTR\n        try:\n            # import module to obtain type annotation\n            self.parent = importlib.import_module(self.modname)\n        except ImportError:\n            pass\n\n        return True\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        \"\"\"Never try to get a docstring from the object.\"\"\"\n        super().add_content(more_content, no_docstring=True)\n\n\nclass GenericAliasDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for GenericAliases.\n    \"\"\"\n\n    objtype = 'genericalias'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isgenericalias(member)\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        name = stringify_typehint(self.object)\n        content = StringList([_('alias of %s') % name], source='')\n        super().add_content(content)\n\n\nclass TypeVarDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for TypeVars.\n    \"\"\"\n\n    objtype = 'typevar'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, TypeVar) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        if self.object.__doc__ != TypeVar.__doc__:\n            return super().get_doc()\n        else:\n            return []\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        attrs = [repr(self.object.__name__)]\n        for constraint in self.object.__constraints__:\n            attrs.append(stringify_typehint(constraint))\n        if self.object.__covariant__:\n            attrs.append(\"covariant=True\")\n        if self.object.__contravariant__:\n            attrs.append(\"contravariant=True\")\n\n        content = StringList([_('alias of TypeVar(%s)') % \", \".join(attrs)], source='')\n        super().add_content(content)\n\n\nclass MethodDocumenter(DocstringSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for methods (normal, static and class).\n    \"\"\"\n    objtype = 'method'\n    directivetype = 'method'\n    member_order = 50\n    priority = 1  # must be more than FunctionDocumenter\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isroutine(member) and \\\n            not isinstance(parent, ModuleDocumenter)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        ret = super().import_object(raiseerror)\n        if not ret:\n            return ret\n\n        # to distinguish classmethod/staticmethod\n        obj = self.parent.__dict__.get(self.object_name)\n        if obj is None:\n            obj = self.object\n\n        if (inspect.isclassmethod(obj) or\n                inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name)):\n            # document class and static members before ordinary ones\n            self.member_order = self.member_order - 1\n\n        return ret\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            if self.object == object.__init__ and self.parent != object:\n                # Classes not having own __init__() method are shown as no arguments.\n                #\n                # Note: The signature of object.__init__() is (self, /, *args, **kwargs).\n                #       But it makes users confused.\n                args = '()'\n            else:\n                if inspect.isstaticmethod(self.object, cls=self.parent, name=self.object_name):\n                    self.env.app.emit('autodoc-before-process-signature', self.object, False)\n                    sig = inspect.signature(self.object, bound_method=False,\n                                            type_aliases=self.config.autodoc_type_aliases)\n                else:\n                    self.env.app.emit('autodoc-before-process-signature', self.object, True)\n                    sig = inspect.signature(self.object, bound_method=True,\n                                            type_aliases=self.config.autodoc_type_aliases)\n                args = stringify_signature(sig, **kwargs)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n        except ValueError:\n            args = ''\n\n        if self.config.strip_signature_backslash:\n            # escape backslashes for reST\n            args = args.replace('\\\\', '\\\\\\\\')\n        return args\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n\n        sourcename = self.get_sourcename()\n        obj = self.parent.__dict__.get(self.object_name, self.object)\n        if inspect.isabstractmethod(obj):\n            self.add_line('   :abstractmethod:', sourcename)\n        if inspect.iscoroutinefunction(obj):\n            self.add_line('   :async:', sourcename)\n        if inspect.isclassmethod(obj):\n            self.add_line('   :classmethod:', sourcename)\n        if inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name):\n            self.add_line('   :staticmethod:', sourcename)\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def format_signature(self, **kwargs: Any) -> str:\n        sigs = []\n        if (self.analyzer and\n                '.'.join(self.objpath) in self.analyzer.overloads and\n                self.config.autodoc_typehints == 'signature'):\n            # Use signatures for overloaded methods instead of the implementation method.\n            overloaded = True\n        else:\n            overloaded = False\n            sig = super().format_signature(**kwargs)\n            sigs.append(sig)\n\n        meth = self.parent.__dict__.get(self.objpath[-1])\n        if inspect.is_singledispatch_method(meth):\n            # append signature of singledispatch'ed functions\n            for typ, func in meth.dispatcher.registry.items():\n                if typ is object:\n                    pass  # default implementation. skipped.\n                else:\n                    self.annotate_to_first_argument(func, typ)\n\n                    documenter = MethodDocumenter(self.directive, '')\n                    documenter.parent = self.parent\n                    documenter.object = func\n                    documenter.objpath = [None]\n                    sigs.append(documenter.format_signature())\n        if overloaded:\n            __globals__ = safe_getattr(self.object, '__globals__', {})\n            for overload in self.analyzer.overloads.get('.'.join(self.objpath)):\n                overload = evaluate_signature(overload, __globals__,\n                                              self.config.autodoc_type_aliases)\n\n                if not inspect.isstaticmethod(self.object, cls=self.parent,\n                                              name=self.object_name):\n                    parameters = list(overload.parameters.values())\n                    overload = overload.replace(parameters=parameters[1:])\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def annotate_to_first_argument(self, func: Callable, typ: Type) -> None:\n        \"\"\"Annotate type hint to the first argument of function if needed.\"\"\"\n        try:\n            sig = inspect.signature(func, type_aliases=self.config.autodoc_type_aliases)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return\n        except ValueError:\n            return\n        if len(sig.parameters) == 1:\n            return\n\n        params = list(sig.parameters.values())\n        if params[1].annotation is Parameter.empty:\n            params[1] = params[1].replace(annotation=typ)\n            try:\n                func.__signature__ = sig.replace(parameters=params)  # type: ignore\n            except TypeError:\n                # failed to update signature (ex. built-in or extension types)\n                return\n\n(B)         self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        name = stringify_typehint(self.object)\n        content = StringList([_('alias of %s') % name], source='')\n        super().add_content(content)\n\n\nclass TypeVarDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for TypeVars.\n    \"\"\"\n\n    objtype = 'typevar'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, TypeVar) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        if self.object.__doc__ != TypeVar.__doc__:\n            return super().get_doc()\n        else:\n            return []\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        attrs = [repr(self.object.__name__)]\n        for constraint in self.object.__constraints__:\n            attrs.append(stringify_typehint(constraint))\n        if self.object.__covariant__:\n            attrs.append(\"covariant=True\")\n        if self.object.__contravariant__:\n            attrs.append(\"contravariant=True\")\n\n        content = StringList([_('alias of TypeVar(%s)') % \", \".join(attrs)], source='')\n        super().add_content(content)\n\n\nclass MethodDocumenter(DocstringSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for methods (normal, static and class).\n    \"\"\"\n    objtype = 'method'\n    directivetype = 'method'\n    member_order = 50\n    priority = 1  # must be more than FunctionDocumenter\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isroutine(member) and \\\n            not isinstance(parent, ModuleDocumenter)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        ret = super().import_object(raiseerror)\n        if not ret:\n            return ret\n\n        # to distinguish classmethod/staticmethod\n        obj = self.parent.__dict__.get(self.object_name)\n        if obj is None:\n            obj = self.object\n\n        if (inspect.isclassmethod(obj) or\n                inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name)):\n            # document class and static members before ordinary ones\n            self.member_order = self.member_order - 1\n\n        return ret\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            if self.object == object.__init__ and self.parent != object:\n                # Classes not having own __init__() method are shown as no arguments.\n                #\n                # Note: The signature of object.__init__() is (self, /, *args, **kwargs).\n                #       But it makes users confused.\n                args = '()'\n            else:\n                if inspect.isstaticmethod(self.object, cls=self.parent, name=self.object_name):\n                    self.env.app.emit('autodoc-before-process-signature', self.object, False)\n                    sig = inspect.signature(self.object, bound_method=False,\n                                            type_aliases=self.config.autodoc_type_aliases)\n                else:\n                    self.env.app.emit('autodoc-before-process-signature', self.object, True)\n                    sig = inspect.signature(self.object, bound_method=True,\n                                            type_aliases=self.config.autodoc_type_aliases)\n                args = stringify_signature(sig, **kwargs)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n        except ValueError:\n            args = ''\n\n        if self.config.strip_signature_backslash:\n            # escape backslashes for reST\n            args = args.replace('\\\\', '\\\\\\\\')\n        return args\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n\n        sourcename = self.get_sourcename()\n        obj = self.parent.__dict__.get(self.object_name, self.object)\n        if inspect.isabstractmethod(obj):\n            self.add_line('   :abstractmethod:', sourcename)\n        if inspect.iscoroutinefunction(obj):\n            self.add_line('   :async:', sourcename)\n        if inspect.isclassmethod(obj):\n            self.add_line('   :classmethod:', sourcename)\n        if inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name):\n            self.add_line('   :staticmethod:', sourcename)\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def format_signature(self, **kwargs: Any) -> str:\n        sigs = []\n        if (self.analyzer and\n                '.'.join(self.objpath) in self.analyzer.overloads and\n                self.config.autodoc_typehints == 'signature'):\n            # Use signatures for overloaded methods instead of the implementation method.\n            overloaded = True\n        else:\n            overloaded = False\n            sig = super().format_signature(**kwargs)\n            sigs.append(sig)\n\n        meth = self.parent.__dict__.get(self.objpath[-1])\n        if inspect.is_singledispatch_method(meth):\n            # append signature of singledispatch'ed functions\n            for typ, func in meth.dispatcher.registry.items():\n                if typ is object:\n                    pass  # default implementation. skipped.\n                else:\n                    self.annotate_to_first_argument(func, typ)\n\n                    documenter = MethodDocumenter(self.directive, '')\n                    documenter.parent = self.parent\n                    documenter.object = func\n                    documenter.objpath = [None]\n                    sigs.append(documenter.format_signature())\n        if overloaded:\n            __globals__ = safe_getattr(self.object, '__globals__', {})\n            for overload in self.analyzer.overloads.get('.'.join(self.objpath)):\n                overload = evaluate_signature(overload, __globals__,\n                                              self.config.autodoc_type_aliases)\n\n                if not inspect.isstaticmethod(self.object, cls=self.parent,\n                                              name=self.object_name):\n                    parameters = list(overload.parameters.values())\n                    overload = overload.replace(parameters=parameters[1:])\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def annotate_to_first_argument(self, func: Callable, typ: Type) -> None:\n        \"\"\"Annotate type hint to the first argument of function if needed.\"\"\"\n        try:\n            sig = inspect.signature(func, type_aliases=self.config.autodoc_type_aliases)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return\n        except ValueError:\n            return\n        if len(sig.parameters) == 1:\n            return\n\n        params = list(sig.parameters.values())\n        if params[1].annotation is Parameter.empty:\n            params[1] = params[1].replace(annotation=typ)\n            try:\n                func.__signature__ = sig.replace(parameters=params)  # type: ignore\n            except TypeError:\n                # failed to update signature (ex. built-in or extension types)\n                return\n\n\nclass SingledispatchMethodDocumenter(MethodDocumenter):\n    \"\"\"\n    Used to be a specialized Documenter subclass for singledispatch'ed methods.\n\n    Retained for backwards compatibility, now does the same as the MethodDocumenter\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        warnings.warn(\"%s is deprecated.\" % self.__class__.__name__,\n                      RemovedInSphinx50Warning, stacklevel=2)\n        super().__init__(*args, **kwargs)\n\n\nclass AttributeDocumenter(DocstringStripSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for attributes.\n    \"\"\"\n    objtype = 'attribute'\n    member_order = 60\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    # must be higher than the MethodDocumenter, else it will recognize\n    # some non-data descriptors as methods\n    priority = 10\n\n    @staticmethod\n    def is_function_or_method(obj: Any) -> bool:\n        return inspect.isfunction(obj) or inspect.isbuiltin(obj) or inspect.ismethod(obj)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        if inspect.isattributedescriptor(member):\n            return True\n        elif (not isinstance(parent, ModuleDocumenter) and\n              not inspect.isroutine(member) and\n              not isinstance(member, type)):\n            return True\n        else:\n            return False\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def isinstanceattribute(self) -> bool:\n        \"\"\"Check the subject is an instance attribute.\"\"\"\n        try:\n            analyzer = ModuleAnalyzer.for_module(self.modname)\n            attr_docs = analyzer.find_attr_docs()\n            if self.objpath:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if key in attr_docs:\n                    return True\n\n            return False\n        except PycodeError:\n            return False\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        try:\n            ret = super().import_object(raiseerror=True)\n            if inspect.isenumattribute(self.object):\n                self.object = self.object.value\n            if inspect.isattributedescriptor(self.object):\n                self._datadescriptor = True\n            else:\n                # if it's not a data descriptor\n                self._datadescriptor = False\n        except ImportError as exc:\n            if self.isinstanceattribute():\n                self.object = INSTANCEATTR\n                self._datadescriptor = False\n                ret = True\n            elif raiseerror:\n                raise\n            else:\n                logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                self.env.note_reread()\n                ret = False\n\n        return ret\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain type annotation for this attribute\n            try:\n                annotations = get_type_hints(self.parent)\n            except NameError:\n                # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n                annotations = safe_getattr(self.parent, '__annotations__', {})\n            except TypeError:\n                annotations = {}\n            except KeyError:\n                # a broken class found (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n                annotations = {}\n            except AttributeError:\n                # AttributeError is raised on 3.5.2 (fixed by 3.5.3)\n                annotations = {}\n\n            if self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n                    self.add_line('   :type: ' + self.analyzer.annotations[key],\n                                  sourcename)\n\n            # data descriptors do not have useful values\n            if not self._datadescriptor:\n                try:\n                    if self.object is INSTANCEATTR:\n                        pass\n                    else:\n                        objrepr = object_description(self.object)\n                        self.add_line('   :value: ' + objrepr, sourcename)\n                except ValueError:\n                    pass\n        elif self.options.annotation is SUPPRESS:\n            pass\n        else:\n            self.add_line('   :annotation: %s' % self.options.annotation, sourcename)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        try:\n            # Disable `autodoc_inherit_docstring` temporarily to avoid to obtain\n            # a docstring from the value which descriptor returns unexpectedly.\n            # ref: https://github.com/sphinx-doc/sphinx/issues/7805\n            orig = self.config.autodoc_inherit_docstrings\n            self.config.autodoc_inherit_docstrings = False  # type: ignore\n            return super().get_doc(encoding, ignore)\n        finally:\n            self.config.autodoc_inherit_docstrings = orig  # type: ignore\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        if not self._datadescriptor:\n            # if it's not a data descriptor, its docstring is very probably the\n            # wrong thing to display\n            no_docstring = True\n        super().add_content(more_content, no_docstring)\n\n\nclass PropertyDocumenter(DocstringStripSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for properties.\n    \"\"\"\n    objtype = 'property'\n    directivetype = 'method'\n    member_order = 60\n\n    # before AttributeDocumenter\n    priority = AttributeDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isproperty(member) and isinstance(parent, ClassDocumenter)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if inspect.isabstractmethod(self.object):\n            self.add_line('   :abstractmethod:', sourcename)\n        self.add_line('   :property:', sourcename)\n\n\nclass InstanceAttributeDocumenter(AttributeDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for attributes that cannot be imported\n    because they are instance attributes (e.g. assigned in __init__).\n    \"\"\"\n    objtype = 'instanceattribute'\n    directivetype = 'attribute'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only INSTANCEATTR members.\"\"\"\n        return (not isinstance(parent, ModuleDocumenter) and\n                isattr and\n                member is INSTANCEATTR)\n(C)                                             type_aliases=self.config.autodoc_type_aliases)\n                else:\n                    self.env.app.emit('autodoc-before-process-signature', self.object, True)\n                    sig = inspect.signature(self.object, bound_method=True,\n                                            type_aliases=self.config.autodoc_type_aliases)\n                args = stringify_signature(sig, **kwargs)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n        except ValueError:\n            args = ''\n\n        if self.config.strip_signature_backslash:\n            # escape backslashes for reST\n            args = args.replace('\\\\', '\\\\\\\\')\n        return args\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n\n        sourcename = self.get_sourcename()\n        obj = self.parent.__dict__.get(self.object_name, self.object)\n        if inspect.isabstractmethod(obj):\n            self.add_line('   :abstractmethod:', sourcename)\n        if inspect.iscoroutinefunction(obj):\n            self.add_line('   :async:', sourcename)\n        if inspect.isclassmethod(obj):\n            self.add_line('   :classmethod:', sourcename)\n        if inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name):\n            self.add_line('   :staticmethod:', sourcename)\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def format_signature(self, **kwargs: Any) -> str:\n        sigs = []\n        if (self.analyzer and\n                '.'.join(self.objpath) in self.analyzer.overloads and\n                self.config.autodoc_typehints == 'signature'):\n            # Use signatures for overloaded methods instead of the implementation method.\n            overloaded = True\n        else:\n            overloaded = False\n            sig = super().format_signature(**kwargs)\n            sigs.append(sig)\n\n        meth = self.parent.__dict__.get(self.objpath[-1])\n        if inspect.is_singledispatch_method(meth):\n            # append signature of singledispatch'ed functions\n            for typ, func in meth.dispatcher.registry.items():\n                if typ is object:\n                    pass  # default implementation. skipped.\n                else:\n                    self.annotate_to_first_argument(func, typ)\n\n                    documenter = MethodDocumenter(self.directive, '')\n                    documenter.parent = self.parent\n                    documenter.object = func\n                    documenter.objpath = [None]\n                    sigs.append(documenter.format_signature())\n        if overloaded:\n            __globals__ = safe_getattr(self.object, '__globals__', {})\n            for overload in self.analyzer.overloads.get('.'.join(self.objpath)):\n                overload = evaluate_signature(overload, __globals__,\n                                              self.config.autodoc_type_aliases)\n\n                if not inspect.isstaticmethod(self.object, cls=self.parent,\n                                              name=self.object_name):\n                    parameters = list(overload.parameters.values())\n                    overload = overload.replace(parameters=parameters[1:])\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def annotate_to_first_argument(self, func: Callable, typ: Type) -> None:\n        \"\"\"Annotate type hint to the first argument of function if needed.\"\"\"\n        try:\n            sig = inspect.signature(func, type_aliases=self.config.autodoc_type_aliases)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return\n        except ValueError:\n            return\n        if len(sig.parameters) == 1:\n            return\n\n        params = list(sig.parameters.values())\n        if params[1].annotation is Parameter.empty:\n            params[1] = params[1].replace(annotation=typ)\n            try:\n                func.__signature__ = sig.replace(parameters=params)  # type: ignore\n            except TypeError:\n                # failed to update signature (ex. built-in or extension types)\n                return\n\n\nclass SingledispatchMethodDocumenter(MethodDocumenter):\n    \"\"\"\n    Used to be a specialized Documenter subclass for singledispatch'ed methods.\n\n    Retained for backwards compatibility, now does the same as the MethodDocumenter\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        warnings.warn(\"%s is deprecated.\" % self.__class__.__name__,\n                      RemovedInSphinx50Warning, stacklevel=2)\n        super().__init__(*args, **kwargs)\n\n\nclass AttributeDocumenter(DocstringStripSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for attributes.\n    \"\"\"\n    objtype = 'attribute'\n    member_order = 60\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    # must be higher than the MethodDocumenter, else it will recognize\n    # some non-data descriptors as methods\n    priority = 10\n\n    @staticmethod\n    def is_function_or_method(obj: Any) -> bool:\n        return inspect.isfunction(obj) or inspect.isbuiltin(obj) or inspect.ismethod(obj)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        if inspect.isattributedescriptor(member):\n            return True\n        elif (not isinstance(parent, ModuleDocumenter) and\n              not inspect.isroutine(member) and\n              not isinstance(member, type)):\n            return True\n        else:\n            return False\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def isinstanceattribute(self) -> bool:\n        \"\"\"Check the subject is an instance attribute.\"\"\"\n        try:\n            analyzer = ModuleAnalyzer.for_module(self.modname)\n            attr_docs = analyzer.find_attr_docs()\n            if self.objpath:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if key in attr_docs:\n                    return True\n\n            return False\n        except PycodeError:\n            return False\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        try:\n            ret = super().import_object(raiseerror=True)\n            if inspect.isenumattribute(self.object):\n                self.object = self.object.value\n            if inspect.isattributedescriptor(self.object):\n                self._datadescriptor = True\n            else:\n                # if it's not a data descriptor\n                self._datadescriptor = False\n        except ImportError as exc:\n            if self.isinstanceattribute():\n                self.object = INSTANCEATTR\n                self._datadescriptor = False\n                ret = True\n            elif raiseerror:\n                raise\n            else:\n                logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                self.env.note_reread()\n                ret = False\n\n        return ret\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain type annotation for this attribute\n            try:\n                annotations = get_type_hints(self.parent)\n            except NameError:\n                # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n                annotations = safe_getattr(self.parent, '__annotations__', {})\n            except TypeError:\n                annotations = {}\n            except KeyError:\n                # a broken class found (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n                annotations = {}\n            except AttributeError:\n                # AttributeError is raised on 3.5.2 (fixed by 3.5.3)\n                annotations = {}\n\n            if self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n                    self.add_line('   :type: ' + self.analyzer.annotations[key],\n                                  sourcename)\n\n            # data descriptors do not have useful values\n            if not self._datadescriptor:\n                try:\n                    if self.object is INSTANCEATTR:\n                        pass\n                    else:\n                        objrepr = object_description(self.object)\n                        self.add_line('   :value: ' + objrepr, sourcename)\n                except ValueError:\n                    pass\n        elif self.options.annotation is SUPPRESS:\n            pass\n        else:\n            self.add_line('   :annotation: %s' % self.options.annotation, sourcename)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        try:\n            # Disable `autodoc_inherit_docstring` temporarily to avoid to obtain\n            # a docstring from the value which descriptor returns unexpectedly.\n            # ref: https://github.com/sphinx-doc/sphinx/issues/7805\n            orig = self.config.autodoc_inherit_docstrings\n            self.config.autodoc_inherit_docstrings = False  # type: ignore\n            return super().get_doc(encoding, ignore)\n        finally:\n            self.config.autodoc_inherit_docstrings = orig  # type: ignore\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        if not self._datadescriptor:\n            # if it's not a data descriptor, its docstring is very probably the\n            # wrong thing to display\n            no_docstring = True\n        super().add_content(more_content, no_docstring)\n\n\nclass PropertyDocumenter(DocstringStripSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for properties.\n    \"\"\"\n    objtype = 'property'\n    directivetype = 'method'\n    member_order = 60\n\n    # before AttributeDocumenter\n    priority = AttributeDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isproperty(member) and isinstance(parent, ClassDocumenter)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if inspect.isabstractmethod(self.object):\n            self.add_line('   :abstractmethod:', sourcename)\n        self.add_line('   :property:', sourcename)\n\n\nclass InstanceAttributeDocumenter(AttributeDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for attributes that cannot be imported\n    because they are instance attributes (e.g. assigned in __init__).\n    \"\"\"\n    objtype = 'instanceattribute'\n    directivetype = 'attribute'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only INSTANCEATTR members.\"\"\"\n        return (not isinstance(parent, ModuleDocumenter) and\n                isattr and\n                member is INSTANCEATTR)\n\n    def import_parent(self) -> Any:\n        try:\n            parent = importlib.import_module(self.modname)\n            for name in self.objpath[:-1]:\n                parent = self.get_attr(parent, name)\n\n            return parent\n        except (ImportError, AttributeError):\n            return None\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as an attribute\n        self.objtype = 'attribute'\n        self.object = INSTANCEATTR\n        self.parent = self.import_parent()\n        self._datadescriptor = False\n        return True\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        \"\"\"Never try to get a docstring from the object.\"\"\"\n        super().add_content(more_content, no_docstring=True)\n\n\nclass SlotsAttributeDocumenter(AttributeDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for attributes that cannot be imported\n    because they are attributes in __slots__.\n    \"\"\"\n    objtype = 'slotsattribute'\n    directivetype = 'attribute'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only SLOTSATTR members.\"\"\"\n        return member is SLOTSATTR\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as an attribute\n        self.objtype = 'attribute'\n        self._datadescriptor = True\n\n        with mock(self.config.autodoc_mock_imports):\n            try:\n                ret = import_object(self.modname, self.objpath[:-1], 'class',\n                                    attrgetter=self.get_attr,\n                                    warningiserror=self.config.autodoc_warningiserror)\n                self.module, _, _, self.parent = ret\n                return True\n            except ImportError as exc:\n                if raiseerror:\n                    raise\n                else:\n                    logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                    self.env.note_reread()\n                    return False\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        \"\"\"Decode and return lines of the docstring(s) for the object.\"\"\"\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n        name = self.objpath[-1]\n        __slots__ = inspect.getslots(self.parent)\n        if __slots__ and isinstance(__slots__.get(name, None), str):\n            docstring = prepare_docstring(__slots__[name])\n            return [docstring]\n        else:\n            return []\n\n\ndef get_documenters(app: Sphinx) -> Dict[str, \"Type[Documenter]\"]:\n    \"\"\"Returns registered Documenter classes\"\"\"\n    warnings.warn(\"get_documenters() is deprecated.\", RemovedInSphinx50Warning, stacklevel=2)\n    return app.registry.documenters\n\n\ndef autodoc_attrgetter(app: Sphinx, obj: Any, name: str, *defargs: Any) -> Any:\n    \"\"\"Alternative getattr() for types\"\"\"\n    for typ, func in app.registry.autodoc_attrgettrs.items():\n        if isinstance(obj, typ):\n            return func(obj, name, *defargs)\n\n    return safe_getattr(obj, name, *defargs)\n\n\ndef migrate_autodoc_member_order(app: Sphinx, config: Config) -> None:\n    if config.autodoc_member_order == 'alphabetic':\n        # RemovedInSphinx50Warning\n        logger.warning(__('autodoc_member_order now accepts \"alphabetical\" '\n                          'instead of \"alphabetic\". Please update your setting.'))\n        config.autodoc_member_order = 'alphabetical'  # type: ignore\n(D)         if not self.options.annotation:\n            # obtain annotation for this data\n            try:\n                annotations = get_type_hints(self.parent)\n            except NameError:\n                # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n                annotations = safe_getattr(self.parent, '__annotations__', {})\n            except TypeError:\n                annotations = {}\n            except KeyError:\n                # a broken class found (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n                annotations = {}\n            except AttributeError:\n                # AttributeError is raised on 3.5.2 (fixed by 3.5.3)\n                annotations = {}\n\n            if self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n                    self.add_line('   :type: ' + self.analyzer.annotations[key],\n                                  sourcename)\n\n            try:\n                if self.object is UNINITIALIZED_ATTR:\n                    pass\n                else:\n                    objrepr = object_description(self.object)\n                    self.add_line('   :value: ' + objrepr, sourcename)\n            except ValueError:\n                pass\n        elif self.options.annotation is SUPPRESS:\n            pass\n        else:\n            self.add_line('   :annotation: %s' % self.options.annotation,\n                          sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n\nclass DataDeclarationDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data that cannot be imported\n    because they are declared without initial value (refs: PEP-526).\n    \"\"\"\n    objtype = 'datadecl'\n    directivetype = 'data'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only INSTANCEATTR members.\"\"\"\n        return (isinstance(parent, ModuleDocumenter) and\n                isattr and\n                member is INSTANCEATTR)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as a data\n        self.objtype = 'data'\n        self.object = UNINITIALIZED_ATTR\n        try:\n            # import module to obtain type annotation\n            self.parent = importlib.import_module(self.modname)\n        except ImportError:\n            pass\n\n        return True\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        \"\"\"Never try to get a docstring from the object.\"\"\"\n        super().add_content(more_content, no_docstring=True)\n\n\nclass GenericAliasDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for GenericAliases.\n    \"\"\"\n\n    objtype = 'genericalias'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isgenericalias(member)\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        name = stringify_typehint(self.object)\n        content = StringList([_('alias of %s') % name], source='')\n        super().add_content(content)\n\n\nclass TypeVarDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for TypeVars.\n    \"\"\"\n\n    objtype = 'typevar'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, TypeVar) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options = Options(self.options)\n        self.options['annotation'] = SUPPRESS\n        super().add_directive_header(sig)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        if self.object.__doc__ != TypeVar.__doc__:\n            return super().get_doc()\n        else:\n            return []\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        attrs = [repr(self.object.__name__)]\n        for constraint in self.object.__constraints__:\n            attrs.append(stringify_typehint(constraint))\n        if self.object.__covariant__:\n            attrs.append(\"covariant=True\")\n        if self.object.__contravariant__:\n            attrs.append(\"contravariant=True\")\n\n        content = StringList([_('alias of TypeVar(%s)') % \", \".join(attrs)], source='')\n        super().add_content(content)\n\n\nclass MethodDocumenter(DocstringSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for methods (normal, static and class).\n    \"\"\"\n    objtype = 'method'\n    directivetype = 'method'\n    member_order = 50\n    priority = 1  # must be more than FunctionDocumenter\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isroutine(member) and \\\n            not isinstance(parent, ModuleDocumenter)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        ret = super().import_object(raiseerror)\n        if not ret:\n            return ret\n\n        # to distinguish classmethod/staticmethod\n        obj = self.parent.__dict__.get(self.object_name)\n        if obj is None:\n            obj = self.object\n\n        if (inspect.isclassmethod(obj) or\n                inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name)):\n            # document class and static members before ordinary ones\n            self.member_order = self.member_order - 1\n\n        return ret\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            if self.object == object.__init__ and self.parent != object:\n                # Classes not having own __init__() method are shown as no arguments.\n                #\n                # Note: The signature of object.__init__() is (self, /, *args, **kwargs).\n                #       But it makes users confused.\n                args = '()'\n            else:\n                if inspect.isstaticmethod(self.object, cls=self.parent, name=self.object_name):\n                    self.env.app.emit('autodoc-before-process-signature', self.object, False)\n                    sig = inspect.signature(self.object, bound_method=False,\n                                            type_aliases=self.config.autodoc_type_aliases)\n                else:\n                    self.env.app.emit('autodoc-before-process-signature', self.object, True)\n                    sig = inspect.signature(self.object, bound_method=True,\n                                            type_aliases=self.config.autodoc_type_aliases)\n                args = stringify_signature(sig, **kwargs)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n        except ValueError:\n            args = ''\n\n        if self.config.strip_signature_backslash:\n            # escape backslashes for reST\n            args = args.replace('\\\\', '\\\\\\\\')\n        return args\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n\n        sourcename = self.get_sourcename()\n        obj = self.parent.__dict__.get(self.object_name, self.object)\n        if inspect.isabstractmethod(obj):\n            self.add_line('   :abstractmethod:', sourcename)\n        if inspect.iscoroutinefunction(obj):\n            self.add_line('   :async:', sourcename)\n        if inspect.isclassmethod(obj):\n            self.add_line('   :classmethod:', sourcename)\n        if inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name):\n            self.add_line('   :staticmethod:', sourcename)\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def format_signature(self, **kwargs: Any) -> str:\n        sigs = []\n        if (self.analyzer and\n                '.'.join(self.objpath) in self.analyzer.overloads and\n                self.config.autodoc_typehints == 'signature'):\n            # Use signatures for overloaded methods instead of the implementation method.\n            overloaded = True\n        else:\n            overloaded = False\n            sig = super().format_signature(**kwargs)\n            sigs.append(sig)\n\n        meth = self.parent.__dict__.get(self.objpath[-1])\n        if inspect.is_singledispatch_method(meth):\n            # append signature of singledispatch'ed functions\n            for typ, func in meth.dispatcher.registry.items():\n                if typ is object:\n                    pass  # default implementation. skipped.\n                else:\n                    self.annotate_to_first_argument(func, typ)\n\n                    documenter = MethodDocumenter(self.directive, '')\n                    documenter.parent = self.parent\n                    documenter.object = func\n                    documenter.objpath = [None]\n                    sigs.append(documenter.format_signature())\n        if overloaded:\n            __globals__ = safe_getattr(self.object, '__globals__', {})\n            for overload in self.analyzer.overloads.get('.'.join(self.objpath)):\n                overload = evaluate_signature(overload, __globals__,\n                                              self.config.autodoc_type_aliases)\n\n                if not inspect.isstaticmethod(self.object, cls=self.parent,\n                                              name=self.object_name):\n                    parameters = list(overload.parameters.values())\n                    overload = overload.replace(parameters=parameters[1:])\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def annotate_to_first_argument(self, func: Callable, typ: Type) -> None:\n        \"\"\"Annotate type hint to the first argument of function if needed.\"\"\"\n        try:\n            sig = inspect.signature(func, type_aliases=self.config.autodoc_type_aliases)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return\n        except ValueError:\n            return\n        if len(sig.parameters) == 1:\n            return\n\n        params = list(sig.parameters.values())\n        if params[1].annotation is Parameter.empty:\n            params[1] = params[1].replace(annotation=typ)\n            try:\n                func.__signature__ = sig.replace(parameters=params)  # type: ignore\n            except TypeError:\n                # failed to update signature (ex. built-in or extension types)\n                return\n\n\nclass SingledispatchMethodDocumenter(MethodDocumenter):\n    \"\"\"\n    Used to be a specialized Documenter subclass for singledispatch'ed methods.\n\n    Retained for backwards compatibility, now does the same as the MethodDocumenter\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        warnings.warn(\"%s is deprecated.\" % self.__class__.__name__,\n                      RemovedInSphinx50Warning, stacklevel=2)\n        super().__init__(*args, **kwargs)\n\n\nclass AttributeDocumenter(DocstringStripSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for attributes.\n    \"\"\"\n    objtype = 'attribute'\n    member_order = 60\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    # must be higher than the MethodDocumenter, else it will recognize\n    # some non-data descriptors as methods\n    priority = 10\n\n    @staticmethod\n    def is_function_or_method(obj: Any) -> bool:\n        return inspect.isfunction(obj) or inspect.isbuiltin(obj) or inspect.ismethod(obj)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        if inspect.isattributedescriptor(member):\n            return True\n        elif (not isinstance(parent, ModuleDocumenter) and\n              not inspect.isroutine(member) and\n              not isinstance(member, type)):\n            return True\n        else:\n            return False\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def isinstanceattribute(self) -> bool:\n        \"\"\"Check the subject is an instance attribute.\"\"\"\n        try:\n            analyzer = ModuleAnalyzer.for_module(self.modname)\n            attr_docs = analyzer.find_attr_docs()\n            if self.objpath:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if key in attr_docs:\n                    return True\n\n            return False\n        except PycodeError:\n            return False\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        try:\n            ret = super().import_object(raiseerror=True)\n            if inspect.isenumattribute(self.object):\n                self.object = self.object.value\n            if inspect.isattributedescriptor(self.object):\n                self._datadescriptor = True\n            else:\n                # if it's not a data descriptor\n                self._datadescriptor = False\n        except ImportError as exc:\n            if self.isinstanceattribute():\n                self.object = INSTANCEATTR\n                self._datadescriptor = False\n                ret = True\n            elif raiseerror:\n                raise\n            else:\n                logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                self.env.note_reread()\n                ret = False\n\n        return ret\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain type annotation for this attribute\n            try:\n                annotations = get_type_hints(self.parent)\n            except NameError:\n                # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n                annotations = safe_getattr(self.parent, '__annotations__', {})\n            except TypeError:\n                annotations = {}", "answer": "D"}
{"uuid": "261e2b12-6c9c-4d23-a0ef-de93edf0fa07", "issue_statement": "v3.3 upgrade started generating \"WARNING: no number is assigned for table\" warnings\nWe've updated to Sphinx 3.3 in our documentation, and suddenly the following warning started popping up in our builds when we build either `singlehtml` or `latex`.:\r\n\r\n`WARNING: no number is assigned for table:`\r\n\r\nI looked through the changelog but it didn't seem like there was anything related to `numref` that was changed, but perhaps I missed something? Could anyone point me to a change in the numref logic so I can figure out where these warnings are coming from?", "choices": "(A)             return contnode\n\n        try:\n            if node['refexplicit']:\n                title = contnode.astext()\n            else:\n                title = env.config.numfig_format.get(figtype, '')\n\n            if figname is None and '{name}' in title:\n(B)             logger.warning(__(\"no number is assigned for %s: %s\"), figtype, labelid,\n                           location=node)\n            return contnode\n\n        try:\n            if node['refexplicit']:\n                title = contnode.astext()\n            else:\n                title = env.config.numfig_format.get(figtype, '')\n(C)             if fignumber is None:\n                return contnode\n        except ValueError:\n            logger.warning(__(\"no number is assigned for %s: %s\"), figtype, labelid,\n                           location=node)\n            return contnode\n\n        try:\n            if node['refexplicit']:\n(D) \n        try:\n            fignumber = self.get_fignumber(env, builder, figtype, docname, target_node)\n            if fignumber is None:\n                return contnode\n        except ValueError:\n            logger.warning(__(\"no number is assigned for %s: %s\"), figtype, labelid,\n                           location=node)\n            return contnode", "answer": "C"}
{"uuid": "e5bbac01-16b5-407b-9b9e-221f60141e4b", "issue_statement": "Sphinx 3.2 complains about option:: syntax that earlier versions accepted\nSphinx 3.2 complains about use of the option:: directive that earlier versions accepted without complaint.\r\n\r\nThe QEMU documentation includes this:\r\n```\r\n.. option:: [enable=]PATTERN\r\n\r\n   Immediately enable events matching *PATTERN*\r\n```\r\n\r\nas part of the documentation of the command line options of one of its programs. Earlier versions of Sphinx were fine with this, but Sphinx 3.2 complains:\r\n\r\n```\r\nWarning, treated as error:\r\n../../docs/qemu-option-trace.rst.inc:4:Malformed option description '[enable=]PATTERN', should look like \"opt\", \"-opt args\", \"--opt args\", \"/opt args\" or \"+opt args\"\r\n```\r\n\r\nSphinx ideally shouldn't change in ways that break the building of documentation that worked in older versions, because this makes it unworkably difficult to have documentation that builds with whatever the Linux distro's sphinx-build is.\r\n\r\nThe error message suggests that Sphinx has a very restrictive idea of what option syntax is; it would be better if it just accepted any string, because not all programs and OSes have option syntax that matches the limited list the error message indicates.", "choices": "(A) \n\n# RE for option descriptions\noption_desc_re = re.compile(r'((?:/|--|-|\\+)?[^\\s=[]+)(=?\\s*.*)')\n# RE for grammar tokens\ntoken_re = re.compile(r'`(\\w+)`', re.U)\n\n\nclass GenericObject(ObjectDescription):\n    \"\"\"\n    A generic x-ref directive registered with Sphinx.add_object_type().\n    \"\"\"\n    indextemplate = ''\n    parse_node = None  # type: Callable[[GenericObject, BuildEnvironment, str, desc_signature], str]  # NOQA\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> str:\n        if self.parse_node:\n            name = self.parse_node(self.env, sig, signode)\n        else:\n            signode.clear()\n            signode += addnodes.desc_name(sig, sig)\n            # normalize whitespace like XRefRole does\n            name = ws_re.sub(' ', sig)\n        return name\n\n    def add_target_and_index(self, name: str, sig: str, signode: desc_signature) -> None:\n        node_id = make_id(self.env, self.state.document, self.objtype, name)\n        signode['ids'].append(node_id)\n\n        # Assign old styled node_id not to break old hyperlinks (if possible)\n        # Note: Will be removed in Sphinx-5.0 (RemovedInSphinx50Warning)\n        old_node_id = self.make_old_id(name)\n        if old_node_id not in self.state.document.ids and old_node_id not in signode['ids']:\n            signode['ids'].append(old_node_id)\n\n        self.state.document.note_explicit_target(signode)\n\n        if self.indextemplate:\n            colon = self.indextemplate.find(':')\n            if colon != -1:\n                indextype = self.indextemplate[:colon].strip()\n                indexentry = self.indextemplate[colon + 1:].strip() % (name,)\n            else:\n                indextype = 'single'\n                indexentry = self.indextemplate % (name,)\n            self.indexnode['entries'].append((indextype, indexentry, node_id, '', None))\n\n        std = cast(StandardDomain, self.env.get_domain('std'))\n        std.note_object(self.objtype, name, node_id, location=signode)\n\n    def make_old_id(self, name: str) -> str:\n        \"\"\"Generate old styled node_id for generic objects.\n\n        .. note:: Old Styled node_id was used until Sphinx-3.0.\n                  This will be removed in Sphinx-5.0.\n        \"\"\"\n        return self.objtype + '-' + name\n\n\nclass EnvVar(GenericObject):\n    indextemplate = _('environment variable; %s')\n\n\nclass EnvVarXRefRole(XRefRole):\n    \"\"\"\n    Cross-referencing role for environment variables (adds an index entry).\n    \"\"\"\n\n    def result_nodes(self, document: nodes.document, env: \"BuildEnvironment\", node: Element,\n                     is_ref: bool) -> Tuple[List[Node], List[system_message]]:\n        if not is_ref:\n            return [node], []\n        varname = node['reftarget']\n        tgtid = 'index-%s' % env.new_serialno('index')\n        indexnode = addnodes.index()\n        indexnode['entries'] = [\n            ('single', varname, tgtid, '', None),\n            ('single', _('environment variable; %s') % varname, tgtid, '', None)\n        ]\n        targetnode = nodes.target('', '', ids=[tgtid])\n        document.note_explicit_target(targetnode)\n        return [indexnode, targetnode, node], []\n\n\nclass Target(SphinxDirective):\n    \"\"\"\n    Generic target for user-defined cross-reference types.\n    \"\"\"\n    indextemplate = ''\n\n    has_content = False\n    required_arguments = 1\n    optional_arguments = 0\n    final_argument_whitespace = True\n    option_spec = {}  # type: Dict\n\n    def run(self) -> List[Node]:\n        # normalize whitespace in fullname like XRefRole does\n        fullname = ws_re.sub(' ', self.arguments[0].strip())\n        node_id = make_id(self.env, self.state.document, self.name, fullname)\n        node = nodes.target('', '', ids=[node_id])\n        self.set_source_info(node)\n\n        # Assign old styled node_id not to break old hyperlinks (if possible)\n        # Note: Will be removed in Sphinx-5.0 (RemovedInSphinx50Warning)\n        old_node_id = self.make_old_id(fullname)\n        if old_node_id not in self.state.document.ids and old_node_id not in node['ids']:\n            node['ids'].append(old_node_id)\n\n        self.state.document.note_explicit_target(node)\n        ret = [node]  # type: List[Node]\n        if self.indextemplate:\n            indexentry = self.indextemplate % (fullname,)\n            indextype = 'single'\n            colon = indexentry.find(':')\n            if colon != -1:\n                indextype = indexentry[:colon].strip()\n                indexentry = indexentry[colon + 1:].strip()\n            inode = addnodes.index(entries=[(indextype, indexentry, node_id, '', None)])\n            ret.insert(0, inode)\n        name = self.name\n        if ':' in self.name:\n            _, name = self.name.split(':', 1)\n\n        std = cast(StandardDomain, self.env.get_domain('std'))\n        std.note_object(name, fullname, node_id, location=node)\n\n        return ret\n\n    def make_old_id(self, name: str) -> str:\n        \"\"\"Generate old styled node_id for targets.\n\n        .. note:: Old Styled node_id was used until Sphinx-3.0.\n                  This will be removed in Sphinx-5.0.\n        \"\"\"\n        return self.name + '-' + name\n\n\nclass Cmdoption(ObjectDescription):\n    \"\"\"\n    Description of a command-line option (.. option).\n    \"\"\"\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> str:\n        \"\"\"Transform an option description into RST nodes.\"\"\"\n        count = 0\n        firstname = ''\n        for potential_option in sig.split(', '):\n            potential_option = potential_option.strip()\n            m = option_desc_re.match(potential_option)\n            if not m:\n                logger.warning(__('Malformed option description %r, should '\n                                  'look like \"opt\", \"-opt args\", \"--opt args\", '\n                                  '\"/opt args\" or \"+opt args\"'), potential_option,\n                               location=signode)\n                continue\n            optname, args = m.groups()\n            if count:\n                signode += addnodes.desc_addname(', ', ', ')\n            signode += addnodes.desc_name(optname, optname)\n            signode += addnodes.desc_addname(args, args)\n            if not count:\n                firstname = optname\n                signode['allnames'] = [optname]\n            else:\n(B) \nclass Target(SphinxDirective):\n    \"\"\"\n    Generic target for user-defined cross-reference types.\n    \"\"\"\n    indextemplate = ''\n\n    has_content = False\n    required_arguments = 1\n    optional_arguments = 0\n    final_argument_whitespace = True\n    option_spec = {}  # type: Dict\n\n    def run(self) -> List[Node]:\n        # normalize whitespace in fullname like XRefRole does\n        fullname = ws_re.sub(' ', self.arguments[0].strip())\n        node_id = make_id(self.env, self.state.document, self.name, fullname)\n        node = nodes.target('', '', ids=[node_id])\n        self.set_source_info(node)\n\n        # Assign old styled node_id not to break old hyperlinks (if possible)\n        # Note: Will be removed in Sphinx-5.0 (RemovedInSphinx50Warning)\n        old_node_id = self.make_old_id(fullname)\n        if old_node_id not in self.state.document.ids and old_node_id not in node['ids']:\n            node['ids'].append(old_node_id)\n\n        self.state.document.note_explicit_target(node)\n        ret = [node]  # type: List[Node]\n        if self.indextemplate:\n            indexentry = self.indextemplate % (fullname,)\n            indextype = 'single'\n            colon = indexentry.find(':')\n            if colon != -1:\n                indextype = indexentry[:colon].strip()\n                indexentry = indexentry[colon + 1:].strip()\n            inode = addnodes.index(entries=[(indextype, indexentry, node_id, '', None)])\n            ret.insert(0, inode)\n        name = self.name\n        if ':' in self.name:\n            _, name = self.name.split(':', 1)\n\n        std = cast(StandardDomain, self.env.get_domain('std'))\n        std.note_object(name, fullname, node_id, location=node)\n\n        return ret\n\n    def make_old_id(self, name: str) -> str:\n        \"\"\"Generate old styled node_id for targets.\n\n        .. note:: Old Styled node_id was used until Sphinx-3.0.\n                  This will be removed in Sphinx-5.0.\n        \"\"\"\n        return self.name + '-' + name\n\n\nclass Cmdoption(ObjectDescription):\n    \"\"\"\n    Description of a command-line option (.. option).\n    \"\"\"\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> str:\n        \"\"\"Transform an option description into RST nodes.\"\"\"\n        count = 0\n        firstname = ''\n        for potential_option in sig.split(', '):\n            potential_option = potential_option.strip()\n            m = option_desc_re.match(potential_option)\n            if not m:\n                logger.warning(__('Malformed option description %r, should '\n                                  'look like \"opt\", \"-opt args\", \"--opt args\", '\n                                  '\"/opt args\" or \"+opt args\"'), potential_option,\n                               location=signode)\n                continue\n            optname, args = m.groups()\n            if count:\n                signode += addnodes.desc_addname(', ', ', ')\n            signode += addnodes.desc_name(optname, optname)\n            signode += addnodes.desc_addname(args, args)\n            if not count:\n                firstname = optname\n                signode['allnames'] = [optname]\n            else:\n                signode['allnames'].append(optname)\n            count += 1\n        if not firstname:\n            raise ValueError\n        return firstname\n\n    def add_target_and_index(self, firstname: str, sig: str, signode: desc_signature) -> None:\n        currprogram = self.env.ref_context.get('std:program')\n        for optname in signode.get('allnames', []):\n            prefixes = ['cmdoption']\n            if currprogram:\n                prefixes.append(currprogram)\n            if not optname.startswith(('-', '/')):\n                prefixes.append('arg')\n            prefix = '-'.join(prefixes)\n            node_id = make_id(self.env, self.state.document, prefix, optname)\n            signode['ids'].append(node_id)\n\n            old_node_id = self.make_old_id(prefix, optname)\n            if old_node_id not in self.state.document.ids and \\\n               old_node_id not in signode['ids']:\n                signode['ids'].append(old_node_id)\n\n        self.state.document.note_explicit_target(signode)\n\n        domain = cast(StandardDomain, self.env.get_domain('std'))\n        for optname in signode.get('allnames', []):\n            domain.add_program_option(currprogram, optname,\n                                      self.env.docname, signode['ids'][0])\n\n        # create an index entry\n        if currprogram:\n            descr = _('%s command line option') % currprogram\n        else:\n            descr = _('command line option')\n        for option in sig.split(', '):\n            entry = '; '.join([descr, option])\n            self.indexnode['entries'].append(('pair', entry, signode['ids'][0], '', None))\n\n    def make_old_id(self, prefix: str, optname: str) -> str:\n        \"\"\"Generate old styled node_id for cmdoption.\n\n        .. note:: Old Styled node_id was used until Sphinx-3.0.\n                  This will be removed in Sphinx-5.0.\n        \"\"\"\n        return nodes.make_id(prefix + '-' + optname)\n\n\nclass Program(SphinxDirective):\n    \"\"\"\n    Directive to name the program for which options are documented.\n    \"\"\"\n\n    has_content = False\n    required_arguments = 1\n    optional_arguments = 0\n    final_argument_whitespace = True\n    option_spec = {}  # type: Dict\n\n    def run(self) -> List[Node]:\n        program = ws_re.sub('-', self.arguments[0].strip())\n        if program == 'None':\n            self.env.ref_context.pop('std:program', None)\n        else:\n            self.env.ref_context['std:program'] = program\n        return []\n\n\nclass OptionXRefRole(XRefRole):\n    def process_link(self, env: \"BuildEnvironment\", refnode: Element, has_explicit_title: bool,\n                     title: str, target: str) -> Tuple[str, str]:\n        refnode['std:program'] = env.ref_context.get('std:program')\n        return title, target\n\n\ndef split_term_classifiers(line: str) -> List[Optional[str]]:\n    # split line into a term and classifiers. if no classifier, None is used..\n    parts = re.split(' +: +', line) + [None]\n    return parts\n\n\ndef make_glossary_term(env: \"BuildEnvironment\", textnodes: Iterable[Node], index_key: str,\n                       source: str, lineno: int, node_id: str = None,\n(C) \"\"\"\n    sphinx.domains.std\n    ~~~~~~~~~~~~~~~~~~\n\n    The standard domain.\n\n    :copyright: Copyright 2007-2020 by the Sphinx team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\nimport unicodedata\nimport warnings\nfrom copy import copy\nfrom typing import Any, Callable, Dict, Iterable, Iterator, List, Optional, Tuple, Union, cast\n\nfrom docutils import nodes\nfrom docutils.nodes import Element, Node, system_message\nfrom docutils.parsers.rst import Directive, directives\nfrom docutils.statemachine import StringList\n\nfrom sphinx import addnodes\nfrom sphinx.addnodes import desc_signature, pending_xref\nfrom sphinx.deprecation import RemovedInSphinx40Warning, RemovedInSphinx50Warning\nfrom sphinx.directives import ObjectDescription\nfrom sphinx.domains import Domain, ObjType\nfrom sphinx.locale import _, __\nfrom sphinx.roles import XRefRole\nfrom sphinx.util import docname_join, logging, ws_re\nfrom sphinx.util.docutils import SphinxDirective\nfrom sphinx.util.nodes import clean_astext, make_id, make_refnode\nfrom sphinx.util.typing import RoleFunction\n\nif False:\n    # For type annotation\n    from typing import Type  # for python3.5.1\n\n    from sphinx.application import Sphinx\n    from sphinx.builders import Builder\n    from sphinx.environment import BuildEnvironment\n\nlogger = logging.getLogger(__name__)\n\n\n# RE for option descriptions\noption_desc_re = re.compile(r'((?:/|--|-|\\+)?[^\\s=[]+)(=?\\s*.*)')\n# RE for grammar tokens\ntoken_re = re.compile(r'`(\\w+)`', re.U)\n\n\nclass GenericObject(ObjectDescription):\n    \"\"\"\n    A generic x-ref directive registered with Sphinx.add_object_type().\n    \"\"\"\n    indextemplate = ''\n    parse_node = None  # type: Callable[[GenericObject, BuildEnvironment, str, desc_signature], str]  # NOQA\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> str:\n        if self.parse_node:\n            name = self.parse_node(self.env, sig, signode)\n        else:\n            signode.clear()\n            signode += addnodes.desc_name(sig, sig)\n            # normalize whitespace like XRefRole does\n            name = ws_re.sub(' ', sig)\n        return name\n\n    def add_target_and_index(self, name: str, sig: str, signode: desc_signature) -> None:\n        node_id = make_id(self.env, self.state.document, self.objtype, name)\n        signode['ids'].append(node_id)\n\n        # Assign old styled node_id not to break old hyperlinks (if possible)\n        # Note: Will be removed in Sphinx-5.0 (RemovedInSphinx50Warning)\n        old_node_id = self.make_old_id(name)\n        if old_node_id not in self.state.document.ids and old_node_id not in signode['ids']:\n            signode['ids'].append(old_node_id)\n\n        self.state.document.note_explicit_target(signode)\n\n        if self.indextemplate:\n            colon = self.indextemplate.find(':')\n            if colon != -1:\n                indextype = self.indextemplate[:colon].strip()\n                indexentry = self.indextemplate[colon + 1:].strip() % (name,)\n            else:\n                indextype = 'single'\n                indexentry = self.indextemplate % (name,)\n            self.indexnode['entries'].append((indextype, indexentry, node_id, '', None))\n\n        std = cast(StandardDomain, self.env.get_domain('std'))\n        std.note_object(self.objtype, name, node_id, location=signode)\n\n    def make_old_id(self, name: str) -> str:\n        \"\"\"Generate old styled node_id for generic objects.\n\n        .. note:: Old Styled node_id was used until Sphinx-3.0.\n                  This will be removed in Sphinx-5.0.\n        \"\"\"\n        return self.objtype + '-' + name\n\n\nclass EnvVar(GenericObject):\n    indextemplate = _('environment variable; %s')\n\n\nclass EnvVarXRefRole(XRefRole):\n    \"\"\"\n    Cross-referencing role for environment variables (adds an index entry).\n    \"\"\"\n\n    def result_nodes(self, document: nodes.document, env: \"BuildEnvironment\", node: Element,\n                     is_ref: bool) -> Tuple[List[Node], List[system_message]]:\n        if not is_ref:\n            return [node], []\n        varname = node['reftarget']\n        tgtid = 'index-%s' % env.new_serialno('index')\n        indexnode = addnodes.index()\n        indexnode['entries'] = [\n            ('single', varname, tgtid, '', None),\n            ('single', _('environment variable; %s') % varname, tgtid, '', None)\n        ]\n        targetnode = nodes.target('', '', ids=[tgtid])\n        document.note_explicit_target(targetnode)\n        return [indexnode, targetnode, node], []\n\n\nclass Target(SphinxDirective):\n    \"\"\"\n    Generic target for user-defined cross-reference types.\n    \"\"\"\n    indextemplate = ''\n\n    has_content = False\n    required_arguments = 1\n    optional_arguments = 0\n    final_argument_whitespace = True\n    option_spec = {}  # type: Dict\n\n    def run(self) -> List[Node]:\n        # normalize whitespace in fullname like XRefRole does\n        fullname = ws_re.sub(' ', self.arguments[0].strip())\n        node_id = make_id(self.env, self.state.document, self.name, fullname)\n        node = nodes.target('', '', ids=[node_id])\n        self.set_source_info(node)\n\n        # Assign old styled node_id not to break old hyperlinks (if possible)\n        # Note: Will be removed in Sphinx-5.0 (RemovedInSphinx50Warning)\n        old_node_id = self.make_old_id(fullname)\n        if old_node_id not in self.state.document.ids and old_node_id not in node['ids']:\n            node['ids'].append(old_node_id)\n\n        self.state.document.note_explicit_target(node)\n        ret = [node]  # type: List[Node]\n        if self.indextemplate:\n            indexentry = self.indextemplate % (fullname,)\n            indextype = 'single'\n            colon = indexentry.find(':')\n            if colon != -1:\n                indextype = indexentry[:colon].strip()\n                indexentry = indexentry[colon + 1:].strip()\n            inode = addnodes.index(entries=[(indextype, indexentry, node_id, '', None)])\n            ret.insert(0, inode)\n        name = self.name\n        if ':' in self.name:\n            _, name = self.name.split(':', 1)\n(D)             else:\n                indextype = 'single'\n                indexentry = self.indextemplate % (name,)\n            self.indexnode['entries'].append((indextype, indexentry, node_id, '', None))\n\n        std = cast(StandardDomain, self.env.get_domain('std'))\n        std.note_object(self.objtype, name, node_id, location=signode)\n\n    def make_old_id(self, name: str) -> str:\n        \"\"\"Generate old styled node_id for generic objects.\n\n        .. note:: Old Styled node_id was used until Sphinx-3.0.\n                  This will be removed in Sphinx-5.0.\n        \"\"\"\n        return self.objtype + '-' + name\n\n\nclass EnvVar(GenericObject):\n    indextemplate = _('environment variable; %s')\n\n\nclass EnvVarXRefRole(XRefRole):\n    \"\"\"\n    Cross-referencing role for environment variables (adds an index entry).\n    \"\"\"\n\n    def result_nodes(self, document: nodes.document, env: \"BuildEnvironment\", node: Element,\n                     is_ref: bool) -> Tuple[List[Node], List[system_message]]:\n        if not is_ref:\n            return [node], []\n        varname = node['reftarget']\n        tgtid = 'index-%s' % env.new_serialno('index')\n        indexnode = addnodes.index()\n        indexnode['entries'] = [\n            ('single', varname, tgtid, '', None),\n            ('single', _('environment variable; %s') % varname, tgtid, '', None)\n        ]\n        targetnode = nodes.target('', '', ids=[tgtid])\n        document.note_explicit_target(targetnode)\n        return [indexnode, targetnode, node], []\n\n\nclass Target(SphinxDirective):\n    \"\"\"\n    Generic target for user-defined cross-reference types.\n    \"\"\"\n    indextemplate = ''\n\n    has_content = False\n    required_arguments = 1\n    optional_arguments = 0\n    final_argument_whitespace = True\n    option_spec = {}  # type: Dict\n\n    def run(self) -> List[Node]:\n        # normalize whitespace in fullname like XRefRole does\n        fullname = ws_re.sub(' ', self.arguments[0].strip())\n        node_id = make_id(self.env, self.state.document, self.name, fullname)\n        node = nodes.target('', '', ids=[node_id])\n        self.set_source_info(node)\n\n        # Assign old styled node_id not to break old hyperlinks (if possible)\n        # Note: Will be removed in Sphinx-5.0 (RemovedInSphinx50Warning)\n        old_node_id = self.make_old_id(fullname)\n        if old_node_id not in self.state.document.ids and old_node_id not in node['ids']:\n            node['ids'].append(old_node_id)\n\n        self.state.document.note_explicit_target(node)\n        ret = [node]  # type: List[Node]\n        if self.indextemplate:\n            indexentry = self.indextemplate % (fullname,)\n            indextype = 'single'\n            colon = indexentry.find(':')\n            if colon != -1:\n                indextype = indexentry[:colon].strip()\n                indexentry = indexentry[colon + 1:].strip()\n            inode = addnodes.index(entries=[(indextype, indexentry, node_id, '', None)])\n            ret.insert(0, inode)\n        name = self.name\n        if ':' in self.name:\n            _, name = self.name.split(':', 1)\n\n        std = cast(StandardDomain, self.env.get_domain('std'))\n        std.note_object(name, fullname, node_id, location=node)\n\n        return ret\n\n    def make_old_id(self, name: str) -> str:\n        \"\"\"Generate old styled node_id for targets.\n\n        .. note:: Old Styled node_id was used until Sphinx-3.0.\n                  This will be removed in Sphinx-5.0.\n        \"\"\"\n        return self.name + '-' + name\n\n\nclass Cmdoption(ObjectDescription):\n    \"\"\"\n    Description of a command-line option (.. option).\n    \"\"\"\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> str:\n        \"\"\"Transform an option description into RST nodes.\"\"\"\n        count = 0\n        firstname = ''\n        for potential_option in sig.split(', '):\n            potential_option = potential_option.strip()\n            m = option_desc_re.match(potential_option)\n            if not m:\n                logger.warning(__('Malformed option description %r, should '\n                                  'look like \"opt\", \"-opt args\", \"--opt args\", '\n                                  '\"/opt args\" or \"+opt args\"'), potential_option,\n                               location=signode)\n                continue\n            optname, args = m.groups()\n            if count:\n                signode += addnodes.desc_addname(', ', ', ')\n            signode += addnodes.desc_name(optname, optname)\n            signode += addnodes.desc_addname(args, args)\n            if not count:\n                firstname = optname\n                signode['allnames'] = [optname]\n            else:\n                signode['allnames'].append(optname)\n            count += 1\n        if not firstname:\n            raise ValueError\n        return firstname\n\n    def add_target_and_index(self, firstname: str, sig: str, signode: desc_signature) -> None:\n        currprogram = self.env.ref_context.get('std:program')\n        for optname in signode.get('allnames', []):\n            prefixes = ['cmdoption']\n            if currprogram:\n                prefixes.append(currprogram)\n            if not optname.startswith(('-', '/')):\n                prefixes.append('arg')\n            prefix = '-'.join(prefixes)\n            node_id = make_id(self.env, self.state.document, prefix, optname)\n            signode['ids'].append(node_id)\n\n            old_node_id = self.make_old_id(prefix, optname)\n            if old_node_id not in self.state.document.ids and \\\n               old_node_id not in signode['ids']:\n                signode['ids'].append(old_node_id)\n\n        self.state.document.note_explicit_target(signode)\n\n        domain = cast(StandardDomain, self.env.get_domain('std'))\n        for optname in signode.get('allnames', []):\n            domain.add_program_option(currprogram, optname,\n                                      self.env.docname, signode['ids'][0])\n\n        # create an index entry\n        if currprogram:\n            descr = _('%s command line option') % currprogram\n        else:\n            descr = _('command line option')\n        for option in sig.split(', '):\n            entry = '; '.join([descr, option])\n            self.indexnode['entries'].append(('pair', entry, signode['ids'][0], '', None))\n\n    def make_old_id(self, prefix: str, optname: str) -> str:\n        \"\"\"Generate old styled node_id for cmdoption.\n", "answer": "A"}
{"uuid": "50c42c31-4f08-4628-b8fd-d3d7b24cf6d6", "issue_statement": "autodoc: empty __all__ attribute is ignored\n**Describe the bug**\r\nautodoc: empty `__all__` attribute is ignored\r\n\r\n**To Reproduce**\r\n```\r\n# example.py\r\n__all__ = []\r\n\r\n\r\ndef foo():\r\n    \"docstring\"\r\n\r\n\r\ndef bar():\r\n    \"docstring\"\r\n\r\n\r\ndef baz():\r\n    \"docstring\"\r\n```\r\n```\r\n# index.rst\r\n.. automodule:: example\r\n   :members:\r\n```\r\n\r\nAll foo, bar, and baz are shown.\r\n\r\n**Expected behavior**\r\nNo entries should be shown because `__all__` is empty.\r\n\r\n**Your project**\r\nNo\r\n\r\n**Screenshots**\r\nNo\r\n\r\n**Environment info**\r\n- OS: Mac\r\n- Python version: 3.9.1\r\n- Sphinx version: HEAD of 3.x\r\n- Sphinx extensions: sphinx.ext.autodoc\r\n- Extra tools: No\r\n\r\n**Additional context**\r\nNo", "choices": "(A)                 # for implicit module members, check __module__ to avoid\n                # documenting imported objects\n                return True, list(members.values())\n            else:\n                for member in members.values():\n                    if member.__name__ not in self.__all__:\n                        member.skipped = True\n(B)         return members\n\n    def get_object_members(self, want_all: bool) -> Tuple[bool, ObjectMembers]:\n        members = self.get_module_members()\n        if want_all:\n            if not self.__all__:\n                # for implicit module members, check __module__ to avoid\n(C)         if want_all:\n            if not self.__all__:\n                # for implicit module members, check __module__ to avoid\n                # documenting imported objects\n                return True, list(members.values())\n            else:\n                for member in members.values():\n(D)     def get_object_members(self, want_all: bool) -> Tuple[bool, ObjectMembers]:\n        members = self.get_module_members()\n        if want_all:\n            if not self.__all__:\n                # for implicit module members, check __module__ to avoid\n                # documenting imported objects\n                return True, list(members.values())", "answer": "D"}
{"uuid": "42602a37-4a5b-498f-b1e0-eefa6f961222", "issue_statement": "autodoc isn't able to resolve struct.Struct type annotations\n**Describe the bug**\r\nIf `struct.Struct` is declared in any type annotations, I get `class reference target not found: Struct`\r\n\r\n**To Reproduce**\r\nSimple `index.rst`\r\n```\r\nHello World\r\n===========\r\n\r\ncode docs\r\n=========\r\n\r\n.. automodule:: helloworld.helloworld\r\n```\r\n\r\nSimple `helloworld.py`\r\n```\r\nimport struct\r\nimport pathlib\r\n\r\ndef consume_struct(_: struct.Struct) -> None:\r\n    pass\r\n\r\ndef make_struct() -> struct.Struct:\r\n    mystruct = struct.Struct('HH')\r\n    return mystruct\r\n\r\ndef make_path() -> pathlib.Path:\r\n    return pathlib.Path()\r\n```\r\n\r\nCommand line:\r\n```\r\npython3 -m sphinx -b html docs/ doc-out -nvWT\r\n```\r\n\r\n**Expected behavior**\r\nIf you comment out the 2 functions that have `Struct` type annotations, you'll see that `pathlib.Path` resolves fine and shows up in the resulting documentation. I'd expect that `Struct` would also resolve correctly.\r\n\r\n**Your project**\r\nn/a\r\n\r\n**Screenshots**\r\nn/a\r\n\r\n**Environment info**\r\n- OS: Ubuntu 18.04, 20.04\r\n- Python version: 3.8.2\r\n- Sphinx version: 3.2.1\r\n- Sphinx extensions:  'sphinx.ext.autodoc',\r\n              'sphinx.ext.autosectionlabel',\r\n              'sphinx.ext.intersphinx',\r\n              'sphinx.ext.doctest',\r\n              'sphinx.ext.todo'\r\n- Extra tools: \r\n\r\n**Additional context**\r\n\r\n\r\n- [e.g. URL or Ticket]", "choices": "(A) \n\ndef restify(cls: Optional[\"Type\"]) -> str:\n    \"\"\"Convert python class to a reST reference.\"\"\"\n    from sphinx.util import inspect  # lazy loading\n\n    if cls is None or cls is NoneType:\n        return ':obj:`None`'\n    elif cls is Ellipsis:\n        return '...'\n    elif inspect.isNewType(cls):\n        return ':class:`%s`' % cls.__name__\n    elif cls.__module__ in ('__builtin__', 'builtins'):\n        return ':class:`%s`' % cls.__name__\n    else:\n        if sys.version_info >= (3, 7):  # py37+\n            return _restify_py37(cls)\n        else:\n            return _restify_py36(cls)\n\n\ndef _restify_py37(cls: Optional[\"Type\"]) -> str:\n    \"\"\"Convert python class to a reST reference.\"\"\"\n    from sphinx.util import inspect  # lazy loading\n\n    if (inspect.isgenericalias(cls) and\n            cls.__module__ == 'typing' and cls.__origin__ is Union):\n        # Union\n        if len(cls.__args__) > 1 and cls.__args__[-1] is NoneType:\n            if len(cls.__args__) > 2:\n                args = ', '.join(restify(a) for a in cls.__args__[:-1])\n                return ':obj:`Optional`\\\\ [:obj:`Union`\\\\ [%s]]' % args\n            else:\n                return ':obj:`Optional`\\\\ [%s]' % restify(cls.__args__[0])\n        else:\n            args = ', '.join(restify(a) for a in cls.__args__)\n            return ':obj:`Union`\\\\ [%s]' % args\n    elif inspect.isgenericalias(cls):\n        if getattr(cls, '_name', None):\n            if cls.__module__ == 'typing':\n                text = ':class:`%s`' % cls._name\n            else:\n                text = ':class:`%s.%s`' % (cls.__module__, cls._name)\n        else:\n            text = restify(cls.__origin__)\n\n        if not hasattr(cls, '__args__'):\n            pass\n        elif all(is_system_TypeVar(a) for a in cls.__args__):\n            # Suppress arguments if all system defined TypeVars (ex. Dict[KT, VT])\n            pass\n        elif cls.__module__ == 'typing' and cls._name == 'Callable':\n            args = ', '.join(restify(a) for a in cls.__args__[:-1])\n            text += r\"\\ [[%s], %s]\" % (args, restify(cls.__args__[-1]))\n        elif cls.__args__:\n            text += r\"\\ [%s]\" % \", \".join(restify(a) for a in cls.__args__)\n\n        return text\n    elif hasattr(cls, '__qualname__'):\n        if cls.__module__ == 'typing':\n            return ':class:`%s`' % cls.__qualname__\n        else:\n            return ':class:`%s.%s`' % (cls.__module__, cls.__qualname__)\n    elif hasattr(cls, '_name'):\n        # SpecialForm\n        if cls.__module__ == 'typing':\n            return ':obj:`%s`' % cls._name\n        else:\n            return ':obj:`%s.%s`' % (cls.__module__, cls._name)\n    elif isinstance(cls, ForwardRef):\n        return ':class:`%s`' % cls.__forward_arg__\n    else:\n        # not a class (ex. TypeVar)\n        return ':obj:`%s.%s`' % (cls.__module__, cls.__name__)\n\n\ndef _restify_py36(cls: Optional[\"Type\"]) -> str:\n    module = getattr(cls, '__module__', None)\n    if module == 'typing':\n        if getattr(cls, '_name', None):\n            qualname = cls._name\n        elif getattr(cls, '__qualname__', None):\n            qualname = cls.__qualname__\n        elif getattr(cls, '__forward_arg__', None):\n            qualname = cls.__forward_arg__\n        elif getattr(cls, '__origin__', None):\n            qualname = stringify(cls.__origin__)  # ex. Union\n        else:\n            qualname = repr(cls).replace('typing.', '')\n    elif hasattr(cls, '__qualname__'):\n        qualname = '%s.%s' % (module, cls.__qualname__)\n    else:\n        qualname = repr(cls)\n\n    if (isinstance(cls, typing.TupleMeta) and  # type: ignore\n            not hasattr(cls, '__tuple_params__')):  # for Python 3.6\n        params = cls.__args__\n        if params:\n            param_str = ', '.join(restify(p) for p in params)\n            return ':class:`%s`\\\\ [%s]' % (qualname, param_str)\n        else:\n            return ':class:`%s`' % qualname\n    elif isinstance(cls, typing.GenericMeta):\n        params = None\n        if hasattr(cls, '__args__'):\n            # for Python 3.5.2+\n            if cls.__args__ is None or len(cls.__args__) <= 2:  # type: ignore  # NOQA\n                params = cls.__args__  # type: ignore\n            elif cls.__origin__ == Generator:  # type: ignore\n                params = cls.__args__  # type: ignore\n            else:  # typing.Callable\n                args = ', '.join(restify(arg) for arg in cls.__args__[:-1])  # type: ignore\n                result = restify(cls.__args__[-1])  # type: ignore\n                return ':class:`%s`\\\\ [[%s], %s]' % (qualname, args, result)\n        elif hasattr(cls, '__parameters__'):\n            # for Python 3.5.0 and 3.5.1\n            params = cls.__parameters__  # type: ignore\n\n        if params:\n            param_str = ', '.join(restify(p) for p in params)\n            return ':class:`%s`\\\\ [%s]' % (qualname, param_str)\n        else:\n            return ':class:`%s`' % qualname\n    elif (hasattr(typing, 'UnionMeta') and\n            isinstance(cls, typing.UnionMeta) and  # type: ignore\n            hasattr(cls, '__union_params__')):  # for Python 3.5\n        params = cls.__union_params__\n        if params is not None:\n            if len(params) == 2 and params[1] is NoneType:\n                return ':obj:`Optional`\\\\ [%s]' % restify(params[0])\n            else:\n                param_str = ', '.join(restify(p) for p in params)\n                return ':obj:`%s`\\\\ [%s]' % (qualname, param_str)\n        else:\n            return ':obj:`%s`' % qualname\n    elif (hasattr(cls, '__origin__') and\n          cls.__origin__ is typing.Union):  # for Python 3.5.2+\n        params = cls.__args__\n        if params is not None:\n            if len(params) > 1 and params[-1] is NoneType:\n                if len(params) > 2:\n                    param_str = \", \".join(restify(p) for p in params[:-1])\n                    return ':obj:`Optional`\\\\ [:obj:`Union`\\\\ [%s]]' % param_str\n                else:\n                    return ':obj:`Optional`\\\\ [%s]' % restify(params[0])\n            else:\n                param_str = ', '.join(restify(p) for p in params)\n                return ':obj:`Union`\\\\ [%s]' % param_str\n        else:\n            return ':obj:`Union`'\n    elif (isinstance(cls, typing.CallableMeta) and  # type: ignore\n          getattr(cls, '__args__', None) is not None and\n          hasattr(cls, '__result__')):  # for Python 3.5\n        # Skipped in the case of plain typing.Callable\n        args = cls.__args__\n        if args is None:\n            return qualname\n        elif args is Ellipsis:\n            args_str = '...'\n        else:\n            formatted_args = (restify(a) for a in args)  # type: ignore\n            args_str = '[%s]' % ', '.join(formatted_args)\n\n        return ':class:`%s`\\\\ [%s, %s]' % (qualname, args_str, stringify(cls.__result__))\n    elif (isinstance(cls, typing.TupleMeta) and  # type: ignore\n          hasattr(cls, '__tuple_params__') and\n          hasattr(cls, '__tuple_use_ellipsis__')):  # for Python 3.5\n        params = cls.__tuple_params__\n        if params is not None:\n            param_strings = [restify(p) for p in params]\n            if cls.__tuple_use_ellipsis__:\n                param_strings.append('...')\n            return ':class:`%s`\\\\ [%s]' % (qualname, ', '.join(param_strings))\n        else:\n            return ':class:`%s`' % qualname\n    elif hasattr(cls, '__qualname__'):\n        if cls.__module__ == 'typing':\n            return ':class:`%s`' % cls.__qualname__\n        else:\n            return ':class:`%s.%s`' % (cls.__module__, cls.__qualname__)\n    elif hasattr(cls, '_name'):\n        # SpecialForm\n        if cls.__module__ == 'typing':\n            return ':obj:`%s`' % cls._name\n        else:\n            return ':obj:`%s.%s`' % (cls.__module__, cls._name)\n    elif hasattr(cls, '__name__'):\n        # not a class (ex. TypeVar)\n        return ':obj:`%s.%s`' % (cls.__module__, cls.__name__)\n    else:\n        # others (ex. Any)\n        if cls.__module__ == 'typing':\n            return ':obj:`%s`' % qualname\n        else:\n            return ':obj:`%s.%s`' % (cls.__module__, qualname)\n\n\ndef stringify(annotation: Any) -> str:\n    \"\"\"Stringify type annotation object.\"\"\"\n    from sphinx.util import inspect  # lazy loading\n\n    if isinstance(annotation, str):\n        if annotation.startswith(\"'\") and annotation.endswith(\"'\"):\n            # might be a double Forward-ref'ed type.  Go unquoting.\n            return annotation[1:-1]\n        else:\n            return annotation\n    elif isinstance(annotation, TypeVar):\n        return annotation.__name__\n    elif inspect.isNewType(annotation):\n        # Could not get the module where it defiend\n        return annotation.__name__\n    elif not annotation:\n        return repr(annotation)\n    elif annotation is NoneType:\n        return 'None'\n    elif (getattr(annotation, '__module__', None) == 'builtins' and\n          hasattr(annotation, '__qualname__')):\n        return annotation.__qualname__\n    elif annotation is Ellipsis:\n        return '...'\n\n    if sys.version_info >= (3, 7):  # py37+\n        return _stringify_py37(annotation)\n    else:\n        return _stringify_py36(annotation)\n\n\ndef _stringify_py37(annotation: Any) -> str:\n    \"\"\"stringify() for py37+.\"\"\"\n    module = getattr(annotation, '__module__', None)\n    if module == 'typing':\n        if getattr(annotation, '_name', None):\n            qualname = annotation._name\n        elif getattr(annotation, '__qualname__', None):\n            qualname = annotation.__qualname__\n        elif getattr(annotation, '__forward_arg__', None):\n            qualname = annotation.__forward_arg__\n        else:\n            qualname = stringify(annotation.__origin__)  # ex. Union\n    elif hasattr(annotation, '__qualname__'):\n        qualname = '%s.%s' % (module, annotation.__qualname__)\n    elif hasattr(annotation, '__origin__'):\n        # instantiated generic provided by a user\n        qualname = stringify(annotation.__origin__)\n    else:\n        # we weren't able to extract the base type, appending arguments would\n        # only make them appear twice\n        return repr(annotation)\n\n    if getattr(annotation, '__args__', None):\n        if not isinstance(annotation.__args__, (list, tuple)):\n            # broken __args__ found\n            pass\n        elif qualname == 'Union':\n            if len(annotation.__args__) > 1 and annotation.__args__[-1] is NoneType:\n                if len(annotation.__args__) > 2:\n                    args = ', '.join(stringify(a) for a in annotation.__args__[:-1])\n                    return 'Optional[Union[%s]]' % args\n                else:\n                    return 'Optional[%s]' % stringify(annotation.__args__[0])\n            else:\n                args = ', '.join(stringify(a) for a in annotation.__args__)\n                return 'Union[%s]' % args\n        elif qualname == 'Callable':\n            args = ', '.join(stringify(a) for a in annotation.__args__[:-1])\n            returns = stringify(annotation.__args__[-1])\n            return '%s[[%s], %s]' % (qualname, args, returns)\n        elif str(annotation).startswith('typing.Annotated'):  # for py39+\n            return stringify(annotation.__args__[0])\n        elif all(is_system_TypeVar(a) for a in annotation.__args__):\n            # Suppress arguments if all system defined TypeVars (ex. Dict[KT, VT])\n            return qualname\n        else:\n            args = ', '.join(stringify(a) for a in annotation.__args__)\n            return '%s[%s]' % (qualname, args)\n\n    return qualname\n\n\ndef _stringify_py36(annotation: Any) -> str:\n    \"\"\"stringify() for py35 and py36.\"\"\"\n    module = getattr(annotation, '__module__', None)\n    if module == 'typing':\n        if getattr(annotation, '_name', None):\n            qualname = annotation._name\n        elif getattr(annotation, '__qualname__', None):\n            qualname = annotation.__qualname__\n        elif getattr(annotation, '__forward_arg__', None):\n            qualname = annotation.__forward_arg__\n        elif getattr(annotation, '__origin__', None):\n            qualname = stringify(annotation.__origin__)  # ex. Union\n        else:\n            qualname = repr(annotation).replace('typing.', '')\n    elif hasattr(annotation, '__qualname__'):\n        qualname = '%s.%s' % (module, annotation.__qualname__)\n    else:\n        qualname = repr(annotation)\n\n    if (isinstance(annotation, typing.TupleMeta) and  # type: ignore\n            not hasattr(annotation, '__tuple_params__')):  # for Python 3.6\n        params = annotation.__args__\n        if params:\n            param_str = ', '.join(stringify(p) for p in params)\n            return '%s[%s]' % (qualname, param_str)\n        else:\n            return qualname\n    elif isinstance(annotation, typing.GenericMeta):\n(B) \nimport sys\nimport typing\nfrom typing import Any, Callable, Dict, Generator, List, Optional, Tuple, TypeVar, Union\n\nfrom docutils import nodes\nfrom docutils.parsers.rst.states import Inliner\n\nif sys.version_info > (3, 7):\n    from typing import ForwardRef\nelse:\n    from typing import _ForwardRef  # type: ignore\n\n    class ForwardRef:\n        \"\"\"A pseudo ForwardRef class for py35 and py36.\"\"\"\n        def __init__(self, arg: Any, is_argument: bool = True) -> None:\n            self.arg = arg\n\n        def _evaluate(self, globalns: Dict, localns: Dict) -> Any:\n            ref = _ForwardRef(self.arg)\n            return ref._eval_type(globalns, localns)\n\nif False:\n    # For type annotation\n    from typing import Type  # NOQA # for python3.5.1\n\n\n# An entry of Directive.option_spec\nDirectiveOption = Callable[[str], Any]\n\n# Text like nodes which are initialized with text and rawsource\nTextlikeNode = Union[nodes.Text, nodes.TextElement]\n\n# type of None\nNoneType = type(None)\n\n# path matcher\nPathMatcher = Callable[[str], bool]\n\n# common role functions\nRoleFunction = Callable[[str, str, str, int, Inliner, Dict[str, Any], List[str]],\n                        Tuple[List[nodes.Node], List[nodes.system_message]]]\n\n# title getter functions for enumerable nodes (see sphinx.domains.std)\nTitleGetter = Callable[[nodes.Node], str]\n\n# inventory data on memory\nInventory = Dict[str, Dict[str, Tuple[str, str, str, str]]]\n\n\ndef get_type_hints(obj: Any, globalns: Dict = None, localns: Dict = None) -> Dict[str, Any]:\n    \"\"\"Return a dictionary containing type hints for a function, method, module or class object.\n\n    This is a simple wrapper of `typing.get_type_hints()` that does not raise an error on\n    runtime.\n    \"\"\"\n    from sphinx.util.inspect import safe_getattr  # lazy loading\n\n    try:\n        return typing.get_type_hints(obj, globalns, localns)\n    except NameError:\n        # Failed to evaluate ForwardRef (maybe TYPE_CHECKING)\n        return safe_getattr(obj, '__annotations__', {})\n    except TypeError:\n        return {}\n    except KeyError:\n        # a broken class found (refs: https://github.com/sphinx-doc/sphinx/issues/8084)\n        return {}\n    except AttributeError:\n        # AttributeError is raised on 3.5.2 (fixed by 3.5.3)\n        return {}\n\n\ndef is_system_TypeVar(typ: Any) -> bool:\n    \"\"\"Check *typ* is system defined TypeVar.\"\"\"\n    modname = getattr(typ, '__module__', '')\n    return modname == 'typing' and isinstance(typ, TypeVar)\n\n\ndef restify(cls: Optional[\"Type\"]) -> str:\n    \"\"\"Convert python class to a reST reference.\"\"\"\n    from sphinx.util import inspect  # lazy loading\n\n    if cls is None or cls is NoneType:\n        return ':obj:`None`'\n    elif cls is Ellipsis:\n        return '...'\n    elif inspect.isNewType(cls):\n        return ':class:`%s`' % cls.__name__\n    elif cls.__module__ in ('__builtin__', 'builtins'):\n        return ':class:`%s`' % cls.__name__\n    else:\n        if sys.version_info >= (3, 7):  # py37+\n            return _restify_py37(cls)\n        else:\n            return _restify_py36(cls)\n\n\ndef _restify_py37(cls: Optional[\"Type\"]) -> str:\n    \"\"\"Convert python class to a reST reference.\"\"\"\n    from sphinx.util import inspect  # lazy loading\n\n    if (inspect.isgenericalias(cls) and\n            cls.__module__ == 'typing' and cls.__origin__ is Union):\n        # Union\n        if len(cls.__args__) > 1 and cls.__args__[-1] is NoneType:\n            if len(cls.__args__) > 2:\n                args = ', '.join(restify(a) for a in cls.__args__[:-1])\n                return ':obj:`Optional`\\\\ [:obj:`Union`\\\\ [%s]]' % args\n            else:\n                return ':obj:`Optional`\\\\ [%s]' % restify(cls.__args__[0])\n        else:\n            args = ', '.join(restify(a) for a in cls.__args__)\n            return ':obj:`Union`\\\\ [%s]' % args\n    elif inspect.isgenericalias(cls):\n        if getattr(cls, '_name', None):\n            if cls.__module__ == 'typing':\n                text = ':class:`%s`' % cls._name\n            else:\n                text = ':class:`%s.%s`' % (cls.__module__, cls._name)\n        else:\n            text = restify(cls.__origin__)\n\n        if not hasattr(cls, '__args__'):\n            pass\n        elif all(is_system_TypeVar(a) for a in cls.__args__):\n            # Suppress arguments if all system defined TypeVars (ex. Dict[KT, VT])\n            pass\n        elif cls.__module__ == 'typing' and cls._name == 'Callable':\n            args = ', '.join(restify(a) for a in cls.__args__[:-1])\n            text += r\"\\ [[%s], %s]\" % (args, restify(cls.__args__[-1]))\n        elif cls.__args__:\n            text += r\"\\ [%s]\" % \", \".join(restify(a) for a in cls.__args__)\n\n        return text\n    elif hasattr(cls, '__qualname__'):\n        if cls.__module__ == 'typing':\n            return ':class:`%s`' % cls.__qualname__\n        else:\n            return ':class:`%s.%s`' % (cls.__module__, cls.__qualname__)\n    elif hasattr(cls, '_name'):\n        # SpecialForm\n        if cls.__module__ == 'typing':\n            return ':obj:`%s`' % cls._name\n        else:\n            return ':obj:`%s.%s`' % (cls.__module__, cls._name)\n    elif isinstance(cls, ForwardRef):\n        return ':class:`%s`' % cls.__forward_arg__\n    else:\n        # not a class (ex. TypeVar)\n        return ':obj:`%s.%s`' % (cls.__module__, cls.__name__)\n\n\ndef _restify_py36(cls: Optional[\"Type\"]) -> str:\n    module = getattr(cls, '__module__', None)\n    if module == 'typing':\n        if getattr(cls, '_name', None):\n            qualname = cls._name\n        elif getattr(cls, '__qualname__', None):\n            qualname = cls.__qualname__\n        elif getattr(cls, '__forward_arg__', None):\n            qualname = cls.__forward_arg__\n        elif getattr(cls, '__origin__', None):\n            qualname = stringify(cls.__origin__)  # ex. Union\n        else:\n            qualname = repr(cls).replace('typing.', '')\n    elif hasattr(cls, '__qualname__'):\n        qualname = '%s.%s' % (module, cls.__qualname__)\n    else:\n        qualname = repr(cls)\n\n    if (isinstance(cls, typing.TupleMeta) and  # type: ignore\n            not hasattr(cls, '__tuple_params__')):  # for Python 3.6\n        params = cls.__args__\n        if params:\n            param_str = ', '.join(restify(p) for p in params)\n            return ':class:`%s`\\\\ [%s]' % (qualname, param_str)\n        else:\n            return ':class:`%s`' % qualname\n    elif isinstance(cls, typing.GenericMeta):\n        params = None\n        if hasattr(cls, '__args__'):\n            # for Python 3.5.2+\n            if cls.__args__ is None or len(cls.__args__) <= 2:  # type: ignore  # NOQA\n                params = cls.__args__  # type: ignore\n            elif cls.__origin__ == Generator:  # type: ignore\n                params = cls.__args__  # type: ignore\n            else:  # typing.Callable\n                args = ', '.join(restify(arg) for arg in cls.__args__[:-1])  # type: ignore\n                result = restify(cls.__args__[-1])  # type: ignore\n                return ':class:`%s`\\\\ [[%s], %s]' % (qualname, args, result)\n        elif hasattr(cls, '__parameters__'):\n            # for Python 3.5.0 and 3.5.1\n            params = cls.__parameters__  # type: ignore\n\n        if params:\n            param_str = ', '.join(restify(p) for p in params)\n            return ':class:`%s`\\\\ [%s]' % (qualname, param_str)\n        else:\n            return ':class:`%s`' % qualname\n    elif (hasattr(typing, 'UnionMeta') and\n            isinstance(cls, typing.UnionMeta) and  # type: ignore\n            hasattr(cls, '__union_params__')):  # for Python 3.5\n        params = cls.__union_params__\n        if params is not None:\n            if len(params) == 2 and params[1] is NoneType:\n                return ':obj:`Optional`\\\\ [%s]' % restify(params[0])\n            else:\n                param_str = ', '.join(restify(p) for p in params)\n                return ':obj:`%s`\\\\ [%s]' % (qualname, param_str)\n        else:\n            return ':obj:`%s`' % qualname\n    elif (hasattr(cls, '__origin__') and\n          cls.__origin__ is typing.Union):  # for Python 3.5.2+\n        params = cls.__args__\n        if params is not None:\n            if len(params) > 1 and params[-1] is NoneType:\n                if len(params) > 2:\n                    param_str = \", \".join(restify(p) for p in params[:-1])\n                    return ':obj:`Optional`\\\\ [:obj:`Union`\\\\ [%s]]' % param_str\n                else:\n                    return ':obj:`Optional`\\\\ [%s]' % restify(params[0])\n            else:\n                param_str = ', '.join(restify(p) for p in params)\n                return ':obj:`Union`\\\\ [%s]' % param_str\n        else:\n            return ':obj:`Union`'\n    elif (isinstance(cls, typing.CallableMeta) and  # type: ignore\n          getattr(cls, '__args__', None) is not None and\n          hasattr(cls, '__result__')):  # for Python 3.5\n        # Skipped in the case of plain typing.Callable\n        args = cls.__args__\n        if args is None:\n            return qualname\n        elif args is Ellipsis:\n            args_str = '...'\n        else:\n            formatted_args = (restify(a) for a in args)  # type: ignore\n            args_str = '[%s]' % ', '.join(formatted_args)\n\n        return ':class:`%s`\\\\ [%s, %s]' % (qualname, args_str, stringify(cls.__result__))\n    elif (isinstance(cls, typing.TupleMeta) and  # type: ignore\n          hasattr(cls, '__tuple_params__') and\n          hasattr(cls, '__tuple_use_ellipsis__')):  # for Python 3.5\n        params = cls.__tuple_params__\n        if params is not None:\n            param_strings = [restify(p) for p in params]\n            if cls.__tuple_use_ellipsis__:\n                param_strings.append('...')\n            return ':class:`%s`\\\\ [%s]' % (qualname, ', '.join(param_strings))\n        else:\n            return ':class:`%s`' % qualname\n    elif hasattr(cls, '__qualname__'):\n        if cls.__module__ == 'typing':\n            return ':class:`%s`' % cls.__qualname__\n        else:\n            return ':class:`%s.%s`' % (cls.__module__, cls.__qualname__)\n    elif hasattr(cls, '_name'):\n        # SpecialForm\n        if cls.__module__ == 'typing':\n            return ':obj:`%s`' % cls._name\n        else:\n            return ':obj:`%s.%s`' % (cls.__module__, cls._name)\n    elif hasattr(cls, '__name__'):\n        # not a class (ex. TypeVar)\n        return ':obj:`%s.%s`' % (cls.__module__, cls.__name__)\n    else:\n        # others (ex. Any)\n        if cls.__module__ == 'typing':\n            return ':obj:`%s`' % qualname\n        else:\n            return ':obj:`%s.%s`' % (cls.__module__, qualname)\n\n\ndef stringify(annotation: Any) -> str:\n    \"\"\"Stringify type annotation object.\"\"\"\n    from sphinx.util import inspect  # lazy loading\n\n    if isinstance(annotation, str):\n        if annotation.startswith(\"'\") and annotation.endswith(\"'\"):\n            # might be a double Forward-ref'ed type.  Go unquoting.\n            return annotation[1:-1]\n        else:\n            return annotation\n    elif isinstance(annotation, TypeVar):\n        return annotation.__name__\n    elif inspect.isNewType(annotation):\n        # Could not get the module where it defiend\n        return annotation.__name__\n    elif not annotation:\n        return repr(annotation)\n    elif annotation is NoneType:\n        return 'None'\n    elif (getattr(annotation, '__module__', None) == 'builtins' and\n          hasattr(annotation, '__qualname__')):\n        return annotation.__qualname__\n    elif annotation is Ellipsis:\n        return '...'\n\n    if sys.version_info >= (3, 7):  # py37+\n        return _stringify_py37(annotation)\n    else:\n        return _stringify_py36(annotation)\n\n\ndef _stringify_py37(annotation: Any) -> str:\n    \"\"\"stringify() for py37+.\"\"\"\n    module = getattr(annotation, '__module__', None)", "answer": "C"}
{"uuid": "8eb24038-4b5b-4473-986f-04d614901da1", "issue_statement": "napoleon_use_param should also affect \"other parameters\" section\nSubject: napoleon_use_param should also affect \"other parameters\" section\r\n\r\n### Problem\r\nCurrently, napoleon always renders the Other parameters section as if napoleon_use_param was False, see source\r\n```\r\n    def _parse_other_parameters_section(self, section):\r\n        # type: (unicode) -> List[unicode]\r\n        return self._format_fields(_('Other Parameters'), self._consume_fields())\r\n\r\n    def _parse_parameters_section(self, section):\r\n        # type: (unicode) -> List[unicode]\r\n        fields = self._consume_fields()\r\n        if self._config.napoleon_use_param:\r\n            return self._format_docutils_params(fields)\r\n        else:\r\n            return self._format_fields(_('Parameters'), fields)\r\n```\r\nwhereas it would make sense that this section should follow the same formatting rules as the Parameters section.\r\n\r\n#### Procedure to reproduce the problem\r\n```\r\nIn [5]: print(str(sphinx.ext.napoleon.NumpyDocstring(\"\"\"\\ \r\n   ...: Parameters \r\n   ...: ---------- \r\n   ...: x : int \r\n   ...:  \r\n   ...: Other parameters \r\n   ...: ---------------- \r\n   ...: y: float \r\n   ...: \"\"\")))                                                                                                                                                                                      \r\n:param x:\r\n:type x: int\r\n\r\n:Other Parameters: **y** (*float*)\r\n```\r\n\r\nNote the difference in rendering.\r\n\r\n#### Error logs / results\r\nSee above.\r\n\r\n#### Expected results\r\n```\r\n:param x:\r\n:type x: int\r\n\r\n:Other Parameters:  // Or some other kind of heading.\r\n:param: y\r\n:type y: float\r\n```\r\n\r\nAlternatively another separate config value could be introduced, but that seems a bit overkill.\r\n\r\n### Reproducible project / your project\r\nN/A\r\n\r\n### Environment info\r\n- OS: Linux\r\n- Python version: 3.7\r\n- Sphinx version: 1.8.1", "choices": "(A)         return self._parse_generic_section(_('Notes'), use_admonition)\n\n    def _parse_other_parameters_section(self, section: str) -> List[str]:\n        return self._format_fields(_('Other Parameters'), self._consume_fields())\n\n    def _parse_parameters_section(self, section: str) -> List[str]:\n        if self._config.napoleon_use_param:\n            # Allow to declare multiple parameters at once (ex: x, y: int)\n            fields = self._consume_fields(multiple=True)\n            return self._format_docutils_params(fields)\n        else:\n            fields = self._consume_fields()\n            return self._format_fields(_('Parameters'), fields)\n(B) \n    def _parse_parameters_section(self, section: str) -> List[str]:\n        if self._config.napoleon_use_param:\n            # Allow to declare multiple parameters at once (ex: x, y: int)\n            fields = self._consume_fields(multiple=True)\n            return self._format_docutils_params(fields)\n        else:\n            fields = self._consume_fields()\n            return self._format_fields(_('Parameters'), fields)\n\n    def _parse_raises_section(self, section: str) -> List[str]:\n        fields = self._consume_fields(parse_type=False, prefer_type=True)\n        lines = []  # type: List[str]\n(C)             # Allow to declare multiple parameters at once (ex: x, y: int)\n            fields = self._consume_fields(multiple=True)\n            return self._format_docutils_params(fields)\n        else:\n            fields = self._consume_fields()\n            return self._format_fields(_('Parameters'), fields)\n\n    def _parse_raises_section(self, section: str) -> List[str]:\n        fields = self._consume_fields(parse_type=False, prefer_type=True)\n        lines = []  # type: List[str]\n        for _name, _type, _desc in fields:\n            m = self._name_rgx.match(_type)\n            if m and m.group('name'):\n(D)         return lines\n\n    def _parse_notes_section(self, section: str) -> List[str]:\n        use_admonition = self._config.napoleon_use_admonition_for_notes\n        return self._parse_generic_section(_('Notes'), use_admonition)\n\n    def _parse_other_parameters_section(self, section: str) -> List[str]:\n        return self._format_fields(_('Other Parameters'), self._consume_fields())\n\n    def _parse_parameters_section(self, section: str) -> List[str]:\n        if self._config.napoleon_use_param:\n            # Allow to declare multiple parameters at once (ex: x, y: int)\n            fields = self._consume_fields(multiple=True)", "answer": "A"}
{"uuid": "1eb379a0-d3b2-4e5c-bccf-0f6a2fc30b74", "issue_statement": "viewcode creates pages for epub even if `viewcode_enable_epub=False` on `make html epub`\n**Describe the bug**\r\nviewcode creates pages for epub even if `viewcode_enable_epub=False` on `make html epub`\r\n\r\n**To Reproduce**\r\n```\r\n$ make html epub\r\n```\r\n\r\n**Expected behavior**\r\nmodule pages should not be created for epub by default.\r\n\r\n**Your project**\r\nNo\r\n\r\n**Screenshots**\r\nNo\r\n\r\n**Environment info**\r\n- OS: Mac\r\n- Python version: 3.9.1\r\n- Sphinx version: HEAD of 3.x\r\n- Sphinx extensions:  sphinx.ext.viewcode\r\n- Extra tools: No\r\n\r\n**Additional context**\r\nNo", "choices": "(A)     highlighter = app.builder.highlighter  # type: ignore\n    urito = app.builder.get_relative_uri\n\n    modnames = set(env._viewcode_modules)  # type: ignore\n\n    for modname, entry in status_iterator(\n            sorted(env._viewcode_modules.items()),  # type: ignore\n            __('highlighting module code... '), \"blue\",\n            len(env._viewcode_modules),  # type: ignore\n            app.verbosity, lambda x: x[0]):\n(B) \n    modnames = set(env._viewcode_modules)  # type: ignore\n\n    for modname, entry in status_iterator(\n            sorted(env._viewcode_modules.items()),  # type: ignore\n            __('highlighting module code... '), \"blue\",\n            len(env._viewcode_modules),  # type: ignore\n            app.verbosity, lambda x: x[0]):\n        if not entry:\n            continue\n(C) \n\ndef collect_pages(app: Sphinx) -> Generator[Tuple[str, Dict[str, Any], str], None, None]:\n    env = app.builder.env\n    if not hasattr(env, '_viewcode_modules'):\n        return\n    highlighter = app.builder.highlighter  # type: ignore\n    urito = app.builder.get_relative_uri\n\n    modnames = set(env._viewcode_modules)  # type: ignore\n(D)     env = app.builder.env\n    if not hasattr(env, '_viewcode_modules'):\n        return\n    highlighter = app.builder.highlighter  # type: ignore\n    urito = app.builder.get_relative_uri\n\n    modnames = set(env._viewcode_modules)  # type: ignore\n\n    for modname, entry in status_iterator(\n            sorted(env._viewcode_modules.items()),  # type: ignore", "answer": "D"}
{"uuid": "64bf5b8d-73b1-4161-b88d-e492238973fe", "issue_statement": "autodoc: The annotation only member in superclass is treated as \"undocumented\"\n**Describe the bug**\r\nautodoc: The annotation only member in superclass is treated as \"undocumented\".\r\n\r\n**To Reproduce**\r\n\r\n```\r\n# example.py\r\nclass Foo:\r\n    \"\"\"docstring\"\"\"\r\n    attr1: int  #: docstring\r\n\r\n\r\nclass Bar(Foo):\r\n    \"\"\"docstring\"\"\"\r\n    attr2: str  #: docstring\r\n```\r\n```\r\n# index.rst\r\n.. autoclass:: example.Bar\r\n   :members:\r\n   :inherited-members:\r\n```\r\n\r\n`Bar.attr1` is not documented. It will be shown if I give `:undoc-members:` option to the autoclass directive call. It seems the attribute is treated as undocumented.\r\n\r\n**Expected behavior**\r\nIt should be shown.\r\n\r\n**Your project**\r\nNo\r\n\r\n**Screenshots**\r\nNo\r\n\r\n**Environment info**\r\n- OS: Mac\r\n- Python version: 3.9.1\r\n- Sphinx version: HEAD of 3.x\r\n- Sphinx extensions: sphinx.ext.autodoc\r\n- Extra tools: No\r\n\r\n**Additional context**\r\nNo", "choices": "(A)     except (TypeError, ValueError):\n        pass\n\n    # other members\n    for name in dir(subject):\n        try:\n            value = attrgetter(subject, name)\n            if ismock(value):\n                value = undecorate(value)\n\n            unmangled = unmangle(subject, name)\n            if unmangled and unmangled not in members:\n                if name in obj_dict:\n                    members[unmangled] = ObjectMember(unmangled, value, class_=subject)\n                else:\n                    members[unmangled] = ObjectMember(unmangled, value)\n        except AttributeError:\n            continue\n\n    try:\n        for cls in getmro(subject):\n            # annotation only member (ex. attr: int)\n            for name in getannotations(cls):\n                name = unmangle(cls, name)\n                if name and name not in members:\n                    members[name] = ObjectMember(name, INSTANCEATTR, class_=cls)\n\n            # append instance attributes (cf. self.attr1) if analyzer knows\n            try:\n                modname = safe_getattr(cls, '__module__')\n                qualname = safe_getattr(cls, '__qualname__')\n                analyzer = ModuleAnalyzer.for_module(modname)\n                analyzer.analyze()\n                for (ns, name), docstring in analyzer.attr_docs.items():\n                    if ns == qualname and name not in members:\n(B) \n            unmangled = unmangle(subject, name)\n            if unmangled and unmangled not in members:\n                if name in obj_dict:\n                    members[unmangled] = ObjectMember(unmangled, value, class_=subject)\n                else:\n                    members[unmangled] = ObjectMember(unmangled, value)\n        except AttributeError:\n            continue\n\n    try:\n        for cls in getmro(subject):\n            # annotation only member (ex. attr: int)\n            for name in getannotations(cls):\n                name = unmangle(cls, name)\n                if name and name not in members:\n                    members[name] = ObjectMember(name, INSTANCEATTR, class_=cls)\n\n            # append instance attributes (cf. self.attr1) if analyzer knows\n            try:\n                modname = safe_getattr(cls, '__module__')\n                qualname = safe_getattr(cls, '__qualname__')\n                analyzer = ModuleAnalyzer.for_module(modname)\n                analyzer.analyze()\n                for (ns, name), docstring in analyzer.attr_docs.items():\n                    if ns == qualname and name not in members:\n                        members[name] = ObjectMember(name, INSTANCEATTR, class_=cls,\n                                                     docstring='\\n'.join(docstring))\n            except (AttributeError, PycodeError):\n                pass\n    except AttributeError:\n        pass\n\n    return members\n\n(C)             # append instance attributes (cf. self.attr1) if analyzer knows\n            try:\n                modname = safe_getattr(cls, '__module__')\n                qualname = safe_getattr(cls, '__qualname__')\n                analyzer = ModuleAnalyzer.for_module(modname)\n                analyzer.analyze()\n                for (ns, name), docstring in analyzer.attr_docs.items():\n                    if ns == qualname and name not in members:\n                        members[name] = ObjectMember(name, INSTANCEATTR, class_=cls,\n                                                     docstring='\\n'.join(docstring))\n            except (AttributeError, PycodeError):\n                pass\n    except AttributeError:\n        pass\n\n    return members\n\n\nfrom sphinx.ext.autodoc.mock import (MockFinder, MockLoader, _MockModule, _MockObject,  # NOQA\n                                     mock)\n\ndeprecated_alias('sphinx.ext.autodoc.importer',\n                 {\n                     '_MockModule': _MockModule,\n                     '_MockObject': _MockObject,\n                     'MockFinder': MockFinder,\n                     'MockLoader': MockLoader,\n                     'mock': mock,\n                 },\n                 RemovedInSphinx40Warning,\n                 {\n                     '_MockModule': 'sphinx.ext.autodoc.mock._MockModule',\n                     '_MockObject': 'sphinx.ext.autodoc.mock._MockObject',\n                     'MockFinder': 'sphinx.ext.autodoc.mock.MockFinder',\n                     'MockLoader': 'sphinx.ext.autodoc.mock.MockLoader',\n(D) \n    try:\n        for cls in getmro(subject):\n            # annotation only member (ex. attr: int)\n            for name in getannotations(cls):\n                name = unmangle(cls, name)\n                if name and name not in members:\n                    members[name] = ObjectMember(name, INSTANCEATTR, class_=cls)\n\n            # append instance attributes (cf. self.attr1) if analyzer knows\n            try:\n                modname = safe_getattr(cls, '__module__')\n                qualname = safe_getattr(cls, '__qualname__')\n                analyzer = ModuleAnalyzer.for_module(modname)\n                analyzer.analyze()\n                for (ns, name), docstring in analyzer.attr_docs.items():\n                    if ns == qualname and name not in members:\n                        members[name] = ObjectMember(name, INSTANCEATTR, class_=cls,\n                                                     docstring='\\n'.join(docstring))\n            except (AttributeError, PycodeError):\n                pass\n    except AttributeError:\n        pass\n\n    return members\n\n\nfrom sphinx.ext.autodoc.mock import (MockFinder, MockLoader, _MockModule, _MockObject,  # NOQA\n                                     mock)\n\ndeprecated_alias('sphinx.ext.autodoc.importer',\n                 {\n                     '_MockModule': _MockModule,\n                     '_MockObject': _MockObject,\n                     'MockFinder': MockFinder,", "answer": "D"}
{"uuid": "00dc96c8-2806-4fd0-a9b8-e5fa55a3a446", "issue_statement": "ccode(sinc(x)) doesn't work\n```\nIn [30]: ccode(sinc(x))\nOut[30]: '// Not supported in C:\\n// sinc\\nsinc(x)'\n```\n\nI don't think `math.h` has `sinc`, but it could print\n\n```\nIn [38]: ccode(Piecewise((sin(theta)/theta, Ne(theta, 0)), (1, True)))\nOut[38]: '((Ne(theta, 0)) ? (\\n   sin(theta)/theta\\n)\\n: (\\n   1\\n))'\n```", "choices": "(A)         op = expr.rel_op\n        rhs_code = self._print(expr.rhs)\n        return \"{0} {1} {2};\".format(lhs_code, op, rhs_code)\n\n    def _print_For(self, expr):\n        target = self._print(expr.target)\n        if isinstance(expr.iterable, Range):\n            start, stop, step = expr.iterable.args\n        else:\n            raise NotImplementedError(\"Only iterable currently supported is Range\")\n        body = self._print(expr.body)\n        return ('for ({target} = {start}; {target} < {stop}; {target} += '\n                '{step}) {{\\n{body}\\n}}').format(target=target, start=start,\n                stop=stop, step=step, body=body)\n\n    def _print_sign(self, func):\n        return '((({0}) > 0) - (({0}) < 0))'.format(self._print(func.args[0]))\n\n    def indent_code(self, code):\n        \"\"\"Accepts a string of code or a list of code lines\"\"\"\n(B) \n        name = super(CCodePrinter, self)._print_Symbol(expr)\n\n        if expr in self._dereference:\n            return '(*{0})'.format(name)\n        else:\n            return name\n\n    def _print_AugmentedAssignment(self, expr):\n        lhs_code = self._print(expr.lhs)\n        op = expr.rel_op\n        rhs_code = self._print(expr.rhs)\n        return \"{0} {1} {2};\".format(lhs_code, op, rhs_code)\n\n    def _print_For(self, expr):\n        target = self._print(expr.target)\n        if isinstance(expr.iterable, Range):\n            start, stop, step = expr.iterable.args\n        else:\n            raise NotImplementedError(\"Only iterable currently supported is Range\")\n(C)         else:\n            return name\n\n    def _print_AugmentedAssignment(self, expr):\n        lhs_code = self._print(expr.lhs)\n        op = expr.rel_op\n        rhs_code = self._print(expr.rhs)\n        return \"{0} {1} {2};\".format(lhs_code, op, rhs_code)\n\n    def _print_For(self, expr):\n        target = self._print(expr.target)\n        if isinstance(expr.iterable, Range):\n            start, stop, step = expr.iterable.args\n        else:\n            raise NotImplementedError(\"Only iterable currently supported is Range\")\n        body = self._print(expr.body)\n        return ('for ({target} = {start}; {target} < {stop}; {target} += '\n                '{step}) {{\\n{body}\\n}}').format(target=target, start=start,\n                stop=stop, step=step, body=body)\n\n(D)         target = self._print(expr.target)\n        if isinstance(expr.iterable, Range):\n            start, stop, step = expr.iterable.args\n        else:\n            raise NotImplementedError(\"Only iterable currently supported is Range\")\n        body = self._print(expr.body)\n        return ('for ({target} = {start}; {target} < {stop}; {target} += '\n                '{step}) {{\\n{body}\\n}}').format(target=target, start=start,\n                stop=stop, step=step, body=body)\n\n    def _print_sign(self, func):\n        return '((({0}) > 0) - (({0}) < 0))'.format(self._print(func.args[0]))\n\n    def indent_code(self, code):\n        \"\"\"Accepts a string of code or a list of code lines\"\"\"\n\n        if isinstance(code, string_types):\n            code_lines = self.indent_code(code.splitlines(True))\n            return ''.join(code_lines)\n", "answer": "C"}
{"uuid": "28471a88-bb02-40b5-b891-8a38c5278eae", "issue_statement": "simplifying exponential -> trig identities\n```\r\nf = 1 / 2 * (-I*exp(I*k) + I*exp(-I*k))\r\ntrigsimp(f)\r\n```\r\n\r\nIdeally, this would yield `sin(k)`. Is there a way to do this?\r\n\r\nAs a corollary, it would be awesome if \r\n\r\n```\r\nf = 1 / 2 / k* (-I*exp(I*k) + I*exp(-I*k))\r\ntrigsimp(f)\r\n```\r\n\r\ncould yield `sinc(k)`. Thank you for your consideration!", "choices": "(A)             return True\n\n\nclass cos(TrigonometricFunction):\n    \"\"\"\n    The cosine function.\n\n    Returns the cosine of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import cos, pi\n    >>> from sympy.abc import x\n    >>> cos(x**2).diff(x)\n    -2*x*sin(x**2)\n    >>> cos(1).diff(x)\n    0\n    >>> cos(pi)\n    -1\n    >>> cos(pi/2)\n    0\n    >>> cos(2*pi/3)\n    -1/2\n    >>> cos(pi/12)\n    sqrt(2)/4 + sqrt(6)/4\n\n    See Also\n    ========\n\n    sin, csc, sec, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Cos\n    \"\"\"\n\n    def period(self, symbol=None):\n        return self._period(2*pi, symbol)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return -sin(self.args[0])\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @classmethod\n    def eval(cls, arg):\n        from sympy.functions.special.polynomials import chebyshevt\n        from sympy.calculus.util import AccumBounds\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Zero:\n                return S.One\n            elif arg is S.Infinity or arg is S.NegativeInfinity:\n                # In this case it is better to return AccumBounds(-1, 1)\n                # rather than returning S.NaN, since AccumBounds(-1, 1)\n                # preserves the information that sin(oo) is between\n                # -1 and 1, where S.NaN does not do that.\n                return AccumBounds(-1, 1)\n\n        if isinstance(arg, AccumBounds):\n            return sin(arg + S.Pi/2)\n\n        if arg.could_extract_minus_sign():\n            return cls(-arg)\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return cosh(i_coeff)\n\n        pi_coeff = _pi_coeff(arg)\n        if pi_coeff is not None:\n            if pi_coeff.is_integer:\n                return (S.NegativeOne)**pi_coeff\n\n            if (2*pi_coeff).is_integer:\n                if pi_coeff.is_even:\n                    return (S.NegativeOne)**(pi_coeff/2)\n                elif pi_coeff.is_even is False:\n                    return S.Zero\n\n            if not pi_coeff.is_Rational:\n                narg = pi_coeff*S.Pi\n                if narg != arg:\n                    return cls(narg)\n                return None\n\n            # cosine formula #####################\n            # https://github.com/sympy/sympy/issues/6048\n            # explicit calculations are preformed for\n            # cos(k pi/n) for n = 8,10,12,15,20,24,30,40,60,120\n            # Some other exact values like cos(k pi/240) can be\n            # calculated using a partial-fraction decomposition\n            # by calling cos( X ).rewrite(sqrt)\n            cst_table_some = {\n                3: S.Half,\n                5: (sqrt(5) + 1)/4,\n            }\n            if pi_coeff.is_Rational:\n                q = pi_coeff.q\n                p = pi_coeff.p % (2*q)\n                if p > q:\n                    narg = (pi_coeff - 1)*S.Pi\n                    return -cls(narg)\n                if 2*p > q:\n                    narg = (1 - pi_coeff)*S.Pi\n                    return -cls(narg)\n\n                # If nested sqrt's are worse than un-evaluation\n                # you can require q to be in (1, 2, 3, 4, 6, 12)\n                # q <= 12, q=15, q=20, q=24, q=30, q=40, q=60, q=120 return\n                # expressions with 2 or fewer sqrt nestings.\n                table2 = {\n                    12: (3, 4),\n                    20: (4, 5),\n                    30: (5, 6),\n                    15: (6, 10),\n                    24: (6, 8),\n                    40: (8, 10),\n                    60: (20, 30),\n                    120: (40, 60)\n                    }\n                if q in table2:\n                    a, b = p*S.Pi/table2[q][0], p*S.Pi/table2[q][1]\n                    nvala, nvalb = cls(a), cls(b)\n                    if None == nvala or None == nvalb:\n                        return None\n                    return nvala*nvalb + cls(S.Pi/2 - a)*cls(S.Pi/2 - b)\n\n                if q > 12:\n                    return None\n\n                if q in cst_table_some:\n                    cts = cst_table_some[pi_coeff.q]\n                    return chebyshevt(pi_coeff.p, cts).expand()\n\n                if 0 == q % 2:\n                    narg = (pi_coeff*2)*S.Pi\n                    nval = cls(narg)\n                    if None == nval:\n                        return None\n                    x = (2*pi_coeff + 1)/2\n                    sign_cos = (-1)**((-1 if x < 0 else 1)*int(abs(x)))\n                    return sign_cos*sqrt( (1 + nval)/2 )\n            return None\n\n        if arg.is_Add:\n            x, m = _peeloff_pi(arg)\n            if m:\n                return cos(m)*cos(x) - sin(m)*sin(x)\n\n        if isinstance(arg, acos):\n            return arg.args[0]\n\n        if isinstance(arg, atan):\n            x = arg.args[0]\n            return 1 / sqrt(1 + x**2)\n\n        if isinstance(arg, atan2):\n            y, x = arg.args\n            return x / sqrt(x**2 + y**2)\n\n        if isinstance(arg, asin):\n            x = arg.args[0]\n            return sqrt(1 - x ** 2)\n\n        if isinstance(arg, acot):\n            x = arg.args[0]\n            return 1 / sqrt(1 + 1 / x**2)\n\n        if isinstance(arg, acsc):\n            x = arg.args[0]\n            return sqrt(1 - 1 / x**2)\n\n        if isinstance(arg, asec):\n            x = arg.args[0]\n            return 1 / x\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        if n < 0 or n % 2 == 1:\n            return S.Zero\n        else:\n            x = sympify(x)\n\n            if len(previous_terms) > 2:\n                p = previous_terms[-2]\n                return -p * x**2 / (n*(n - 1))\n            else:\n                return (-1)**(n//2)*x**(n)/factorial(n)\n\n    def _eval_rewrite_as_exp(self, arg):\n        I = S.ImaginaryUnit\n        if isinstance(arg, TrigonometricFunction) or isinstance(arg, HyperbolicFunction):\n            arg = arg.func(arg.args[0]).rewrite(exp)\n        return (exp(arg*I) + exp(-arg*I)) / 2\n\n    def _eval_rewrite_as_Pow(self, arg):\n        if isinstance(arg, log):\n            I = S.ImaginaryUnit\n            x = arg.args[0]\n            return x**I/2 + x**-I/2\n\n    def _eval_rewrite_as_sin(self, arg):\n        return sin(arg + S.Pi / 2, evaluate=False)\n\n    def _eval_rewrite_as_tan(self, arg):\n        tan_half = tan(S.Half*arg)**2\n        return (1 - tan_half)/(1 + tan_half)\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return sin(arg)*cos(arg)/sin(arg)\n\n    def _eval_rewrite_as_cot(self, arg):\n        cot_half = cot(S.Half*arg)**2\n        return (cot_half - 1)/(cot_half + 1)\n\n    def _eval_rewrite_as_pow(self, arg):\n        return self._eval_rewrite_as_sqrt(arg)\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        from sympy.functions.special.polynomials import chebyshevt\n\n        def migcdex(x):\n            # recursive calcuation of gcd and linear combination\n            # for a sequence of integers.\n            # Given  (x1, x2, x3)\n            # Returns (y1, y1, y3, g)\n            # such that g is the gcd and x1*y1+x2*y2+x3*y3 - g = 0\n            # Note, that this is only one such linear combination.\n            if len(x) == 1:\n                return (1, x[0])\n            if len(x) == 2:\n                return igcdex(x[0], x[-1])\n            g = migcdex(x[1:])\n            u, v, h = igcdex(x[0], g[-1])\n            return tuple([u] + [v*i for i in g[0:-1] ] + [h])\n\n        def ipartfrac(r, factors=None):\n            from sympy.ntheory import factorint\n            if isinstance(r, int):\n                return r\n            if not isinstance(r, Rational):\n                raise TypeError(\"r is not rational\")\n            n = r.q\n            if 2 > r.q*r.q:\n                return r.q\n\n            if None == factors:\n                a = [n//x**y for x, y in factorint(r.q).items()]\n            else:\n                a = [n//x for x in factors]\n            if len(a) == 1:\n                return [ r ]\n            h = migcdex(a)\n            ans = [ r.p*Rational(i*j, r.q) for i, j in zip(h[:-1], a) ]\n            assert r == sum(ans)\n            return ans\n        pi_coeff = _pi_coeff(arg)\n        if pi_coeff is None:\n            return None\n\n        if pi_coeff.is_integer:\n            # it was unevaluated\n            return self.func(pi_coeff*S.Pi)\n\n        if not pi_coeff.is_Rational:\n            return None\n\n        def _cospi257():\n            \"\"\" Express cos(pi/257) explicitly as a function of radicals\n                Based upon the equations in\n                http://math.stackexchange.com/questions/516142/how-does-cos2-pi-257-look-like-in-real-radicals\n                See also http://www.susqu.edu/brakke/constructions/257-gon.m.txt\n            \"\"\"\n            def f1(a, b):\n                return (a + sqrt(a**2 + b))/2, (a - sqrt(a**2 + b))/2\n\n            def f2(a, b):\n                return (a - sqrt(a**2 + b))/2\n\n            t1, t2 = f1(-1, 256)\n            z1, z3 = f1(t1, 64)\n            z2, z4 = f1(t2, 64)\n            y1, y5 = f1(z1, 4*(5 + t1 + 2*z1))\n            y6, y2 = f1(z2, 4*(5 + t2 + 2*z2))\n            y3, y7 = f1(z3, 4*(5 + t1 + 2*z3))\n            y8, y4 = f1(z4, 4*(5 + t2 + 2*z4))\n            x1, x9 = f1(y1, -4*(t1 + y1 + y3 + 2*y6))\n            x2, x10 = f1(y2, -4*(t2 + y2 + y4 + 2*y7))\n            x3, x11 = f1(y3, -4*(t1 + y3 + y5 + 2*y8))\n            x4, x12 = f1(y4, -4*(t2 + y4 + y6 + 2*y1))\n            x5, x13 = f1(y5, -4*(t1 + y5 + y7 + 2*y2))\n            x6, x14 = f1(y6, -4*(t2 + y6 + y8 + 2*y3))\n            x15, x7 = f1(y7, -4*(t1 + y7 + y1 + 2*y4))\n            x8, x16 = f1(y8, -4*(t2 + y8 + y2 + 2*y5))\n            v1 = f2(x1, -4*(x1 + x2 + x3 + x6))\n            v2 = f2(x2, -4*(x2 + x3 + x4 + x7))\n            v3 = f2(x8, -4*(x8 + x9 + x10 + x13))\n            v4 = f2(x9, -4*(x9 + x10 + x11 + x14))\n            v5 = f2(x10, -4*(x10 + x11 + x12 + x15))\n            v6 = f2(x16, -4*(x16 + x1 + x2 + x5))\n            u1 = -f2(-v1, -4*(v2 + v3))\n            u2 = -f2(-v4, -4*(v5 + v6))\n            w1 = -2*f2(-u1, -4*u2)\n            return sqrt(sqrt(2)*sqrt(w1 + 4)/8 + S.Half)\n\n        cst_table_some = {\n            3: S.Half,\n            5: (sqrt(5) + 1)/4,\n            17: sqrt((15 + sqrt(17))/32 + sqrt(2)*(sqrt(17 - sqrt(17)) +\n                sqrt(sqrt(2)*(-8*sqrt(17 + sqrt(17)) - (1 - sqrt(17))\n                *sqrt(17 - sqrt(17))) + 6*sqrt(17) + 34))/32),\n            257: _cospi257()\n            # 65537 is the only other known Fermat prime and the very\n            # large expression is intentionally omitted from SymPy; see\n            # http://www.susqu.edu/brakke/constructions/65537-gon.m.txt\n        }\n\n        def _fermatCoords(n):\n            # if n can be factored in terms of Fermat primes with\n            # multiplicity of each being 1, return those primes, else\n            # False\n            primes = []\n            for p_i in cst_table_some:\n                quotient, remainder = divmod(n, p_i)\n                if remainder == 0:\n                    n = quotient\n                    primes.append(p_i)\n                    if n == 1:\n                        return tuple(primes)\n            return False\n\n        if pi_coeff.q in cst_table_some:\n            rv = chebyshevt(pi_coeff.p, cst_table_some[pi_coeff.q])\n            if pi_coeff.q < 257:\n                rv = rv.expand()\n            return rv\n\n        if not pi_coeff.q % 2:  # recursively remove factors of 2\n            pico2 = pi_coeff*2\n            nval = cos(pico2*S.Pi).rewrite(sqrt)\n            x = (pico2 + 1)/2\n            sign_cos = -1 if int(x) % 2 else 1\n            return sign_cos*sqrt( (1 + nval)/2 )\n\n        FC = _fermatCoords(pi_coeff.q)\n        if FC:\n            decomp = ipartfrac(pi_coeff, FC)\n            X = [(x[1], x[0]*S.Pi) for x in zip(decomp, numbered_symbols('z'))]\n            pcls = cos(sum([x[0] for x in X]))._eval_expand_trig().subs(X)\n            return pcls.rewrite(sqrt)\n        else:\n            decomp = ipartfrac(pi_coeff)\n            X = [(x[1], x[0]*S.Pi) for x in zip(decomp, numbered_symbols('z'))]\n            pcls = cos(sum([x[0] for x in X]))._eval_expand_trig().subs(X)\n            return pcls\n\n    def _eval_rewrite_as_sec(self, arg):\n        return 1/sec(arg)\n\n    def _eval_rewrite_as_csc(self, arg):\n        return 1 / sec(arg)._eval_rewrite_as_csc(arg)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        re, im = self._as_real_imag(deep=deep, **hints)\n        return (cos(re)*cosh(im), -sin(re)*sinh(im))\n\n    def _eval_expand_trig(self, **hints):\n        from sympy.functions.special.polynomials import chebyshevt\n        arg = self.args[0]\n        x = None\n        if arg.is_Add:  # TODO: Do this more efficiently for more than two terms\n            x, y = arg.as_two_terms()\n            sx = sin(x, evaluate=False)._eval_expand_trig()\n            sy = sin(y, evaluate=False)._eval_expand_trig()\n            cx = cos(x, evaluate=False)._eval_expand_trig()\n            cy = cos(y, evaluate=False)._eval_expand_trig()\n            return cx*cy - sx*sy\n        else:\n            coeff, terms = arg.as_coeff_Mul(rational=True)\n            if coeff.is_Integer:\n                return chebyshevt(coeff, cos(terms))\n            pi_coeff = _pi_coeff(arg)\n            if pi_coeff is not None:\n                if pi_coeff.is_Rational:\n                    return self.rewrite(sqrt)\n        return cos(arg)\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return S.One\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        return self.args[0].is_real\n\n    def _eval_is_finite(self):\n        arg = self.args[0]\n\n        if arg.is_real:\n            return True\n\n\nclass tan(TrigonometricFunction):\n    \"\"\"\n    The tangent function.\n\n    Returns the tangent of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import tan, pi\n    >>> from sympy.abc import x\n    >>> tan(x**2).diff(x)\n    2*x*(tan(x**2)**2 + 1)\n    >>> tan(1).diff(x)\n    0\n    >>> tan(pi/8).expand()\n    -1 + sqrt(2)\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Tan\n    \"\"\"\n\n    def period(self, symbol=None):\n        return self._period(pi, symbol)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return S.One + self**2\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return atan\n\n    @classmethod\n    def eval(cls, arg):\n        from sympy.calculus.util import AccumBounds\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Zero:\n                return S.Zero\n            elif arg is S.Infinity or arg is S.NegativeInfinity:\n                return AccumBounds(S.NegativeInfinity, S.Infinity)\n\n        if isinstance(arg, AccumBounds):\n            min, max = arg.min, arg.max\n            d = floor(min/S.Pi)\n            if min is not S.NegativeInfinity:\n                min = min - d*S.Pi\n            if max is not S.Infinity:\n                max = max - d*S.Pi\n            if AccumBounds(min, max).intersection(FiniteSet(S.Pi/2, 3*S.Pi/2)):\n                return AccumBounds(S.NegativeInfinity, S.Infinity)\n            else:\n                return AccumBounds(tan(min), tan(max))\n\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return S.ImaginaryUnit * tanh(i_coeff)\n\n        pi_coeff = _pi_coeff(arg, 2)\n        if pi_coeff is not None:\n            if pi_coeff.is_integer:\n                return S.Zero\n\n            if not pi_coeff.is_Rational:\n                narg = pi_coeff*S.Pi\n                if narg != arg:\n                    return cls(narg)\n                return None\n\n            if pi_coeff.is_Rational:\n                if not pi_coeff.q % 2:\n                    narg = pi_coeff*S.Pi*2\n                    cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                    if not isinstance(cresult, cos) \\\n                            and not isinstance(sresult, cos):\n                        if sresult == 0:\n                            return S.ComplexInfinity\n                        return (1 - cresult)/sresult\n                table2 = {\n                    12: (3, 4),\n                    20: (4, 5),\n                    30: (5, 6),\n                    15: (6, 10),\n                    24: (6, 8),\n                    40: (8, 10),\n                    60: (20, 30),\n                    120: (40, 60)\n                    }\n                q = pi_coeff.q\n                p = pi_coeff.p % q\n                if q in table2:\n                    nvala, nvalb = cls(p*S.Pi/table2[q][0]), cls(p*S.Pi/table2[q][1])\n                    if None == nvala or None == nvalb:\n                        return None\n                    return (nvala - nvalb)/(1 + nvala*nvalb)\n                narg = ((pi_coeff + S.Half) % 1 - S.Half)*S.Pi\n                # see cos() to specify which expressions should  be\n                # expanded automatically in terms of radicals\n                cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                if not isinstance(cresult, cos) \\\n                        and not isinstance(sresult, cos):\n                    if cresult == 0:\n                        return S.ComplexInfinity\n                    return (sresult/cresult)\n                if narg != arg:\n                    return cls(narg)\n\n        if arg.is_Add:\n            x, m = _peeloff_pi(arg)\n            if m:\n                tanm = tan(m)\n                if tanm is S.ComplexInfinity:\n                    return -cot(x)\n                else: # tanm == 0\n                    return tan(x)\n\n        if isinstance(arg, atan):\n            return arg.args[0]\n\n        if isinstance(arg, atan2):\n            y, x = arg.args\n            return y/x\n\n        if isinstance(arg, asin):\n            x = arg.args[0]\n            return x / sqrt(1 - x**2)\n\n        if isinstance(arg, acos):\n            x = arg.args[0]\n            return sqrt(1 - x**2) / x\n\n        if isinstance(arg, acot):\n            x = arg.args[0]\n            return 1 / x\n\n        if isinstance(arg, acsc):\n            x = arg.args[0]\n            return 1 / (sqrt(1 - 1 / x**2) * x)\n\n        if isinstance(arg, asec):\n            x = arg.args[0]\n            return sqrt(1 - 1 / x**2) * x\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        from sympy import bernoulli\n        if n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n\n            a, b = ((n - 1)//2), 2**(n + 1)\n\n            B = bernoulli(n + 1)\n            F = factorial(n + 1)\n\n            return (-1)**a * b*(b - 1) * B/F * x**n\n\n    def _eval_nseries(self, x, n, logx):\n        i = self.args[0].limit(x, 0)*2/S.Pi\n        if i and i.is_Integer:\n            return self.rewrite(cos)._eval_nseries(x, n=n, logx=logx)\n        return Function._eval_nseries(self, x, n=n, logx=logx)\n\n    def _eval_rewrite_as_Pow(self, arg):\n        if isinstance(arg, log):\n            I = S.ImaginaryUnit\n            x = arg.args[0]\n            return I*(x**-I - x**I)/(x**-I + x**I)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        re, im = self._as_real_imag(deep=deep, **hints)\n        if im:\n            denom = cos(2*re) + cosh(2*im)\n            return (sin(2*re)/denom, sinh(2*im)/denom)\n        else:\n            return (self.func(re), S.Zero)\n\n    def _eval_expand_trig(self, **hints):\n        from sympy import im, re\n        arg = self.args[0]\n        x = None\n        if arg.is_Add:\n            from sympy import symmetric_poly\n            n = len(arg.args)\n            TX = []\n            for x in arg.args:\n                tx = tan(x, evaluate=False)._eval_expand_trig()\n                TX.append(tx)\n\n            Yg = numbered_symbols('Y')\n            Y = [ next(Yg) for i in range(n) ]\n\n            p = [0, 0]\n            for i in range(n + 1):\n                p[1 - i % 2] += symmetric_poly(i, Y)*(-1)**((i % 4)//2)\n            return (p[0]/p[1]).subs(list(zip(Y, TX)))\n\n        else:\n            coeff, terms = arg.as_coeff_Mul(rational=True)\n            if coeff.is_Integer and coeff > 1:\n                I = S.ImaginaryUnit\n                z = Symbol('dummy', real=True)\n                P = ((1 + I*z)**coeff).expand()\n                return (im(P)/re(P)).subs([(z, tan(terms))])\n        return tan(arg)\n\n    def _eval_rewrite_as_exp(self, arg):\n        I = S.ImaginaryUnit\n        if isinstance(arg, TrigonometricFunction) or isinstance(arg, HyperbolicFunction):\n            arg = arg.func(arg.args[0]).rewrite(exp)\n        neg_exp, pos_exp = exp(-arg*I), exp(arg*I)\n        return I*(neg_exp - pos_exp)/(neg_exp + pos_exp)\n\n    def _eval_rewrite_as_sin(self, x):\n        return 2*sin(x)**2/sin(2*x)\n\n    def _eval_rewrite_as_cos(self, x):\n        return cos(x - S.Pi / 2, evaluate=False) / cos(x)\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return sin(arg)/cos(arg)\n\n    def _eval_rewrite_as_cot(self, arg):\n        return 1/cot(arg)\n\n    def _eval_rewrite_as_sec(self, arg):\n        sin_in_sec_form = sin(arg)._eval_rewrite_as_sec(arg)\n        cos_in_sec_form = cos(arg)._eval_rewrite_as_sec(arg)\n        return sin_in_sec_form / cos_in_sec_form\n\n    def _eval_rewrite_as_csc(self, arg):\n        sin_in_csc_form = sin(arg)._eval_rewrite_as_csc(arg)\n        cos_in_csc_form = cos(arg)._eval_rewrite_as_csc(arg)\n        return sin_in_csc_form / cos_in_csc_form\n\n    def _eval_rewrite_as_pow(self, arg):\n        y = self.rewrite(cos).rewrite(pow)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        y = self.rewrite(cos).rewrite(sqrt)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return arg\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        return self.args[0].is_real\n\n    def _eval_is_finite(self):\n        arg = self.args[0]\n\n        if arg.is_imaginary:\n            return True\n\n\nclass cot(TrigonometricFunction):\n    \"\"\"\n    The cotangent function.\n\n    Returns the cotangent of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import cot, pi\n    >>> from sympy.abc import x\n    >>> cot(x**2).diff(x)\n    2*x*(-cot(x**2)**2 - 1)\n    >>> cot(1).diff(x)\n    0\n    >>> cot(pi/12)\n    sqrt(3) + 2\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Cot\n    \"\"\"\n\n    def period(self, symbol=None):\n        return self._period(pi, symbol)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return S.NegativeOne - self**2\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return acot\n\n    @classmethod\n    def eval(cls, arg):\n        from sympy.calculus.util import AccumBounds\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            if arg is S.Zero:\n                return S.ComplexInfinity\n\n        if isinstance(arg, AccumBounds):\n            return -tan(arg + S.Pi/2)\n\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return -S.ImaginaryUnit * coth(i_coeff)\n\n        pi_coeff = _pi_coeff(arg, 2)\n        if pi_coeff is not None:\n            if pi_coeff.is_integer:\n                return S.ComplexInfinity\n\n            if not pi_coeff.is_Rational:\n                narg = pi_coeff*S.Pi\n                if narg != arg:\n                    return cls(narg)\n                return None\n\n            if pi_coeff.is_Rational:\n                if pi_coeff.q > 2 and not pi_coeff.q % 2:\n                    narg = pi_coeff*S.Pi*2\n                    cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                    if not isinstance(cresult, cos) \\\n                            and not isinstance(sresult, cos):\n                        return (1 + cresult)/sresult\n                table2 = {\n                    12: (3, 4),\n                    20: (4, 5),\n                    30: (5, 6),\n                    15: (6, 10),\n                    24: (6, 8),\n                    40: (8, 10),\n                    60: (20, 30),\n                    120: (40, 60)\n                    }\n                q = pi_coeff.q\n                p = pi_coeff.p % q\n                if q in table2:\n                    nvala, nvalb = cls(p*S.Pi/table2[q][0]), cls(p*S.Pi/table2[q][1])\n                    if None == nvala or None == nvalb:\n                        return None\n                    return (1 + nvala*nvalb)/(nvalb - nvala)\n                narg = (((pi_coeff + S.Half) % 1) - S.Half)*S.Pi\n                # see cos() to specify which expressions should be\n                # expanded automatically in terms of radicals\n                cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                if not isinstance(cresult, cos) \\\n                        and not isinstance(sresult, cos):\n                    if sresult == 0:\n                        return S.ComplexInfinity\n                    return cresult / sresult\n                if narg != arg:\n                    return cls(narg)\n\n        if arg.is_Add:\n            x, m = _peeloff_pi(arg)\n            if m:\n                cotm = cot(m)\n                if cotm is S.ComplexInfinity:\n                    return cot(x)\n                else: # cotm == 0\n                    return -tan(x)\n\n        if isinstance(arg, acot):\n            return arg.args[0]\n\n        if isinstance(arg, atan):\n            x = arg.args[0]\n            return 1 / x\n\n        if isinstance(arg, atan2):\n            y, x = arg.args\n            return x/y\n\n        if isinstance(arg, asin):\n            x = arg.args[0]\n            return sqrt(1 - x**2) / x\n\n        if isinstance(arg, acos):\n            x = arg.args[0]\n            return x / sqrt(1 - x**2)\n\n        if isinstance(arg, acsc):\n            x = arg.args[0]\n            return sqrt(1 - 1 / x**2) * x\n\n        if isinstance(arg, asec):\n            x = arg.args[0]\n            return 1 / (sqrt(1 - 1 / x**2) * x)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        from sympy import bernoulli\n        if n == 0:\n            return 1 / sympify(x)\n        elif n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n\n            B = bernoulli(n + 1)\n            F = factorial(n + 1)\n\n            return (-1)**((n + 1)//2) * 2**(n + 1) * B/F * x**n\n\n    def _eval_nseries(self, x, n, logx):\n        i = self.args[0].limit(x, 0)/S.Pi\n        if i and i.is_Integer:\n            return self.rewrite(cos)._eval_nseries(x, n=n, logx=logx)\n        return self.rewrite(tan)._eval_nseries(x, n=n, logx=logx)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        re, im = self._as_real_imag(deep=deep, **hints)\n        if im:\n            denom = cos(2*re) - cosh(2*im)\n            return (-sin(2*re)/denom, -sinh(2*im)/denom)\n        else:\n            return (self.func(re), S.Zero)\n\n    def _eval_rewrite_as_exp(self, arg):\n        I = S.ImaginaryUnit\n        if isinstance(arg, TrigonometricFunction) or isinstance(arg, HyperbolicFunction):\n            arg = arg.func(arg.args[0]).rewrite(exp)\n        neg_exp, pos_exp = exp(-arg*I), exp(arg*I)\n        return I*(pos_exp + neg_exp)/(pos_exp - neg_exp)\n\n    def _eval_rewrite_as_Pow(self, arg):\n        if isinstance(arg, log):\n            I = S.ImaginaryUnit\n            x = arg.args[0]\n            return -I*(x**-I + x**I)/(x**-I - x**I)\n\n    def _eval_rewrite_as_sin(self, x):\n        return 2*sin(2*x)/sin(x)**2\n\n    def _eval_rewrite_as_cos(self, x):\n        return cos(x) / cos(x - S.Pi / 2, evaluate=False)\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return cos(arg)/sin(arg)\n\n    def _eval_rewrite_as_tan(self, arg):\n        return 1/tan(arg)\n\n    def _eval_rewrite_as_sec(self, arg):\n        cos_in_sec_form = cos(arg)._eval_rewrite_as_sec(arg)\n        sin_in_sec_form = sin(arg)._eval_rewrite_as_sec(arg)\n        return cos_in_sec_form / sin_in_sec_form\n\n    def _eval_rewrite_as_csc(self, arg):\n        cos_in_csc_form = cos(arg)._eval_rewrite_as_csc(arg)\n        sin_in_csc_form = sin(arg)._eval_rewrite_as_csc(arg)\n        return cos_in_csc_form / sin_in_csc_form\n\n    def _eval_rewrite_as_pow(self, arg):\n        y = self.rewrite(cos).rewrite(pow)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        y = self.rewrite(cos).rewrite(sqrt)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return 1/arg\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        return self.args[0].is_real\n\n    def _eval_expand_trig(self, **hints):\n        from sympy import im, re\n        arg = self.args[0]\n        x = None\n        if arg.is_Add:\n            from sympy import symmetric_poly\n            n = len(arg.args)\n            CX = []\n            for x in arg.args:\n                cx = cot(x, evaluate=False)._eval_expand_trig()\n                CX.append(cx)\n\n            Yg = numbered_symbols('Y')\n            Y = [ next(Yg) for i in range(n) ]\n\n            p = [0, 0]\n            for i in range(n, -1, -1):\n                p[(n - i) % 2] += symmetric_poly(i, Y)*(-1)**(((n - i) % 4)//2)\n            return (p[0]/p[1]).subs(list(zip(Y, CX)))\n        else:\n            coeff, terms = arg.as_coeff_Mul(rational=True)\n            if coeff.is_Integer and coeff > 1:\n                I = S.ImaginaryUnit\n                z = Symbol('dummy', real=True)\n                P = ((z + I)**coeff).expand()\n                return (re(P)/im(P)).subs([(z, cot(terms))])\n        return cot(arg)\n\n    def _eval_is_finite(self):\n        arg = self.args[0]\n        if arg.is_imaginary:\n            return True\n\n    def _eval_subs(self, old, new):\n        if self == old:\n            return new\n        arg = self.args[0]\n        argnew = arg.subs(old, new)\n        if arg != argnew and (argnew/S.Pi).is_integer:\n            return S.ComplexInfinity\n        return cot(argnew)\n\n\nclass ReciprocalTrigonometricFunction(TrigonometricFunction):\n    \"\"\"Base class for reciprocal functions of trigonometric functions. \"\"\"\n\n    _reciprocal_of = None       # mandatory, to be defined in subclass\n\n    # _is_even and _is_odd are used for correct evaluation of csc(-x), sec(-x)\n    # TODO refactor into TrigonometricFunction common parts of\n    # trigonometric functions eval() like even/odd, func(x+2*k*pi), etc.\n    _is_even = None  # optional, to be defined in subclass\n    _is_odd = None   # optional, to be defined in subclass\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.could_extract_minus_sign():\n            if cls._is_even:\n                return cls(-arg)\n            if cls._is_odd:\n                return -cls(-arg)\n\n        pi_coeff = _pi_coeff(arg)\n        if (pi_coeff is not None\n            and not (2*pi_coeff).is_integer\n            and pi_coeff.is_Rational):\n                q = pi_coeff.q\n                p = pi_coeff.p % (2*q)\n                if p > q:\n                    narg = (pi_coeff - 1)*S.Pi\n                    return -cls(narg)\n                if 2*p > q:\n                    narg = (1 - pi_coeff)*S.Pi\n                    if cls._is_odd:\n                        return cls(narg)\n                    elif cls._is_even:\n                        return -cls(narg)\n\n        t = cls._reciprocal_of.eval(arg)\n        if hasattr(arg, 'inverse') and arg.inverse() == cls:\n            return arg.args[0]\n        return 1/t if t != None else t\n\n    def _call_reciprocal(self, method_name, *args, **kwargs):\n        # Calls method_name on _reciprocal_of\n        o = self._reciprocal_of(self.args[0])\n        return getattr(o, method_name)(*args, **kwargs)\n\n    def _calculate_reciprocal(self, method_name, *args, **kwargs):\n        # If calling method_name on _reciprocal_of returns a value != None\n        # then return the reciprocal of that value\n        t = self._call_reciprocal(method_name, *args, **kwargs)\n        return 1/t if t != None else t\n\n    def _rewrite_reciprocal(self, method_name, arg):\n        # Special handling for rewrite functions. If reciprocal rewrite returns\n        # unmodified expression, then return None\n        t = self._call_reciprocal(method_name, arg)\n        if t != None and t != self._reciprocal_of(arg):\n            return 1/t\n\n    def _period(self, symbol):\n        f = self.args[0]\n        return self._reciprocal_of(f).period(symbol)\n\n    def fdiff(self, argindex=1):\n        return -self._calculate_reciprocal(\"fdiff\", argindex)/self**2\n\n    def _eval_rewrite_as_exp(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_exp\", arg)\n\n    def _eval_rewrite_as_Pow(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_Pow\", arg)\n\n    def _eval_rewrite_as_sin(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_sin\", arg)\n\n    def _eval_rewrite_as_cos(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_cos\", arg)\n\n    def _eval_rewrite_as_tan(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_tan\", arg)\n\n    def _eval_rewrite_as_pow(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_pow\", arg)\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_sqrt\", arg)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        return (1/self._reciprocal_of(self.args[0])).as_real_imag(deep,\n                                                                  **hints)\n\n    def _eval_expand_trig(self, **hints):\n        return self._calculate_reciprocal(\"_eval_expand_trig\", **hints)\n\n    def _eval_is_real(self):\n        return self._reciprocal_of(self.args[0])._eval_is_real()\n\n    def _eval_as_leading_term(self, x):\n        return (1/self._reciprocal_of(self.args[0]))._eval_as_leading_term(x)\n\n    def _eval_is_finite(self):\n        return (1/self._reciprocal_of(self.args[0])).is_finite\n\n    def _eval_nseries(self, x, n, logx):\n        return (1/self._reciprocal_of(self.args[0]))._eval_nseries(x, n, logx)\n\n\nclass sec(ReciprocalTrigonometricFunction):\n    \"\"\"\n    The secant function.\n\n    Returns the secant of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import sec\n    >>> from sympy.abc import x\n    >>> sec(x**2).diff(x)\n    2*x*tan(x**2)*sec(x**2)\n    >>> sec(1).diff(x)\n    0\n\n    See Also\n    ========\n\n    sin, csc, cos, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Sec\n    \"\"\"\n\n    _reciprocal_of = cos\n    _is_even = True\n\n    def period(self, symbol=None):\n        return self._period(symbol)\n\n    def _eval_rewrite_as_cot(self, arg):\n        cot_half_sq = cot(arg/2)**2\n        return (cot_half_sq + 1)/(cot_half_sq - 1)\n\n    def _eval_rewrite_as_cos(self, arg):\n        return (1/cos(arg))\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return sin(arg)/(cos(arg)*sin(arg))\n\n    def _eval_rewrite_as_sin(self, arg):\n        return (1 / cos(arg)._eval_rewrite_as_sin(arg))\n\n    def _eval_rewrite_as_tan(self, arg):\n        return (1 / cos(arg)._eval_rewrite_as_tan(arg))\n\n    def _eval_rewrite_as_csc(self, arg):\n        return csc(pi / 2 - arg, evaluate=False)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return tan(self.args[0])*sec(self.args[0])\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        # Reference Formula:\n        # http://functions.wolfram.com/ElementaryFunctions/Sec/06/01/02/01/\n        from sympy.functions.combinatorial.numbers import euler\n        if n < 0 or n % 2 == 1:\n            return S.Zero\n        else:\n            x = sympify(x)\n            k = n//2\n            return (-1)**k*euler(2*k)/factorial(2*k)*x**(2*k)\n\n\nclass csc(ReciprocalTrigonometricFunction):\n    \"\"\"\n    The cosecant function.\n\n    Returns the cosecant of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import csc\n    >>> from sympy.abc import x\n    >>> csc(x**2).diff(x)\n    -2*x*cot(x**2)*csc(x**2)\n    >>> csc(1).diff(x)\n    0\n\n    See Also\n    ========\n\n    sin, cos, sec, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Csc\n    \"\"\"\n\n    _reciprocal_of = sin\n    _is_odd = True\n\n    def period(self, symbol=None):\n        return self._period(symbol)\n\n    def _eval_rewrite_as_sin(self, arg):\n        return (1/sin(arg))\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return cos(arg)/(sin(arg)*cos(arg))\n\n    def _eval_rewrite_as_cot(self, arg):\n        cot_half = cot(arg/2)\n        return (1 + cot_half**2)/(2*cot_half)\n\n    def _eval_rewrite_as_cos(self, arg):\n        return (1 / sin(arg)._eval_rewrite_as_cos(arg))\n\n    def _eval_rewrite_as_sec(self, arg):\n        return sec(pi / 2 - arg, evaluate=False)\n\n    def _eval_rewrite_as_tan(self, arg):\n        return (1 / sin(arg)._eval_rewrite_as_tan(arg))\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return -cot(self.args[0])*csc(self.args[0])\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        from sympy import bernoulli\n        if n == 0:\n            return 1/sympify(x)\n        elif n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n            k = n//2 + 1\n            return ((-1)**(k - 1)*2*(2**(2*k - 1) - 1)*\n                    bernoulli(2*k)*x**(2*k - 1)/factorial(2*k))\n\n\nclass sinc(TrigonometricFunction):\n    r\"\"\"Represents unnormalized sinc function\n\n    Examples\n    ========\n\n    >>> from sympy import sinc, oo, jn, Product, Symbol\n    >>> from sympy.abc import x\n    >>> sinc(x)\n    sinc(x)\n\n    * Automated Evaluation\n\n    >>> sinc(0)\n    1\n    >>> sinc(oo)\n    0\n\n    * Differentiation\n\n    >>> sinc(x).diff()\n    (x*cos(x) - sin(x))/x**2\n\n    * Series Expansion\n\n    >>> sinc(x).series()\n    1 - x**2/6 + x**4/120 + O(x**6)\n\n    * As zero'th order spherical Bessel Function\n\n    >>> sinc(x).rewrite(jn)\n    jn(0, x)\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Sinc_function\n\n    \"\"\"\n\n    def fdiff(self, argindex=1):\n        x = self.args[0]\n        if argindex == 1:\n            return (x*cos(x) - sin(x)) / x**2\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_zero:\n            return S.One\n        if arg.is_Number:\n            if arg in [S.Infinity, -S.Infinity]:\n                return S.Zero\n            elif arg is S.NaN:\n                return S.NaN\n\n        if arg is S.ComplexInfinity:\n            return S.NaN\n\n        if arg.could_extract_minus_sign():\n            return cls(-arg)\n\n        pi_coeff = _pi_coeff(arg)\n        if pi_coeff is not None:\n            if pi_coeff.is_integer:\n                if fuzzy_not(arg.is_zero):\n                    return S.Zero\n            elif (2*pi_coeff).is_integer:\n                return S.NegativeOne**(pi_coeff - S.Half) / arg\n\n    def _eval_nseries(self, x, n, logx):\n        x = self.args[0]\n        return (sin(x)/x)._eval_nseries(x, n, logx)\n\n    def _eval_rewrite_as_jn(self, arg):\n        from sympy.functions.special.bessel import jn\n        return jn(0, arg)\n\n    def _eval_rewrite_as_sin(self, arg):\n        return sin(arg) / arg\n\n\n###############################################################################\n########################### TRIGONOMETRIC INVERSES ############################\n###############################################################################\n\n\nclass InverseTrigonometricFunction(Function):\n    \"\"\"Base class for inverse trigonometric functions.\"\"\"\n\n    pass\n\n\nclass asin(InverseTrigonometricFunction):\n    \"\"\"\n    The inverse sine function.\n\n    Returns the arcsine of x in radians.\n\n    Notes\n    =====\n\n    asin(x) will evaluate automatically in the cases oo, -oo, 0, 1,\n    -1 and for some instances when the result is a rational multiple\n    of pi (see the eval class method).\n\n    Examples\n    ========\n\n    >>> from sympy import asin, oo, pi\n    >>> asin(1)\n    pi/2\n    >>> asin(-1)\n    -pi/2\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan, cot\n    acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Inverse_trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.23\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/ArcSin\n    \"\"\"\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return 1/sqrt(1 - self.args[0]**2)\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def _eval_is_rational(self):\n        s = self.func(*self.args)\n        if s.func == self.func:\n            if s.args[0].is_rational:\n                return False\n        else:\n            return s.is_rational\n\n    def _eval_is_positive(self):\n        if self.args[0].is_positive:\n            return (self.args[0] - 1).is_negative\n        if self.args[0].is_negative:\n            return not (self.args[0] + 1).is_positive\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Infinity:\n                return S.NegativeInfinity * S.ImaginaryUnit\n            elif arg is S.NegativeInfinity:\n                return S.Infinity * S.ImaginaryUnit\n            elif arg is S.Zero:\n                return S.Zero\n            elif arg is S.One:\n                return S.Pi / 2\n            elif arg is S.NegativeOne:\n                return -S.Pi / 2\n\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        if arg.is_number:\n            cst_table = {\n                sqrt(3)/2: 3,\n                -sqrt(3)/2: -3,\n                sqrt(2)/2: 4,\n                -sqrt(2)/2: -4,\n                1/sqrt(2): 4,\n                -1/sqrt(2): -4,\n                sqrt((5 - sqrt(5))/8): 5,\n                -sqrt((5 - sqrt(5))/8): -5,\n                S.Half: 6,\n                -S.Half: -6,\n                sqrt(2 - sqrt(2))/2: 8,\n                -sqrt(2 - sqrt(2))/2: -8,\n                (sqrt(5) - 1)/4: 10,\n                (1 - sqrt(5))/4: -10,\n                (sqrt(3) - 1)/sqrt(2**3): 12,\n                (1 - sqrt(3))/sqrt(2**3): -12,\n                (sqrt(5) + 1)/4: S(10)/3,\n                -(sqrt(5) + 1)/4: -S(10)/3\n            }\n\n            if arg in cst_table:\n                return S.Pi / cst_table[arg]\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return S.ImaginaryUnit * asinh(i_coeff)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        if n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n            if len(previous_terms) >= 2 and n > 2:\n                p = previous_terms[-2]\n                return p * (n - 2)**2/(n*(n - 1)) * x**2\n            else:\n                k = (n - 1) // 2\n                R = RisingFactorial(S.Half, k)\n                F = factorial(k)\n                return R / F * x**n / n\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return arg\n        else:\n            return self.func(arg)\n\n    def _eval_rewrite_as_acos(self, x):\n        return S.Pi/2 - acos(x)\n\n    def _eval_rewrite_as_atan(self, x):\n        return 2*atan(x/(1 + sqrt(1 - x**2)))\n\n    def _eval_rewrite_as_log(self, x):\n        return -S.ImaginaryUnit*log(S.ImaginaryUnit*x + sqrt(1 - x**2))\n\n    def _eval_rewrite_as_acot(self, arg):\n        return 2*acot((1 + sqrt(1 - arg**2))/arg)\n\n    def _eval_rewrite_as_asec(self, arg):\n        return S.Pi/2 - asec(1/arg)\n\n    def _eval_rewrite_as_acsc(self, arg):\n        return acsc(1/arg)\n\n    def _eval_is_real(self):\n        x = self.args[0]\n        return x.is_real and (1 - abs(x)).is_nonnegative\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return sin\n\n\nclass acos(InverseTrigonometricFunction):\n    \"\"\"\n    The inverse cosine function.\n\n    Returns the arc cosine of x (measured in radians).\n\n    Notes\n    =====\n\n    ``acos(x)`` will evaluate automatically in the cases\n    ``oo``, ``-oo``, ``0``, ``1``, ``-1``.\n\n    ``acos(zoo)`` evaluates to ``zoo``\n    (see note in :py:class`sympy.functions.elementary.trigonometric.asec`)\n\n    Examples\n    ========\n\n    >>> from sympy import acos, oo, pi\n    >>> acos(1)\n    0\n    >>> acos(0)\n    pi/2\n    >>> acos(oo)\n    oo*I\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan, cot\n    asin, acsc, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Inverse_trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.23\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/ArcCos\n    \"\"\"\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return -1/sqrt(1 - self.args[0]**2)\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def _eval_is_rational(self):\n        s = self.func(*self.args)\n        if s.func == self.func:\n            if s.args[0].is_rational:\n                return False\n        else:\n            return s.is_rational\n\n    def _eval_is_positive(self):\n        x = self.args[0]\n        return (1 - abs(x)).is_nonnegative\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Infinity:\n                return S.Infinity * S.ImaginaryUnit\n            elif arg is S.NegativeInfinity:\n                return S.NegativeInfinity * S.ImaginaryUnit\n            elif arg is S.Zero:\n                return S.Pi / 2\n            elif arg is S.One:\n                return S.Zero\n            elif arg is S.NegativeOne:\n                return S.Pi\n\n        if arg is S.ComplexInfinity:\n            return S.ComplexInfinity\n\n        if arg.is_number:\n            cst_table = {\n                S.Half: S.Pi/3,\n                -S.Half: 2*S.Pi/3,\n                sqrt(2)/2: S.Pi/4,\n                -sqrt(2)/2: 3*S.Pi/4,\n                1/sqrt(2): S.Pi/4,\n                -1/sqrt(2): 3*S.Pi/4,\n                sqrt(3)/2: S.Pi/6,\n                -sqrt(3)/2: 5*S.Pi/6,\n            }\n\n            if arg in cst_table:\n                return cst_table[arg]\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        if n == 0:\n            return S.Pi / 2\n        elif n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n            if len(previous_terms) >= 2 and n > 2:\n                p = previous_terms[-2]\n                return p * (n - 2)**2/(n*(n - 1)) * x**2\n            else:\n                k = (n - 1) // 2\n                R = RisingFactorial(S.Half, k)\n                F = factorial(k)\n                return -R / F * x**n / n\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return arg\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        x = self.args[0]\n        return x.is_real and (1 - abs(x)).is_nonnegative\n\n    def _eval_rewrite_as_log(self, x):\n        return S.Pi/2 + S.ImaginaryUnit * \\\n            log(S.ImaginaryUnit * x + sqrt(1 - x**2))\n\n    def _eval_rewrite_as_asin(self, x):\n        return S.Pi/2 - asin(x)\n\n    def _eval_rewrite_as_atan(self, x):\n        return atan(sqrt(1 - x**2)/x) + (S.Pi/2)*(1 - x*sqrt(1/x**2))\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return cos\n\n    def _eval_rewrite_as_acot(self, arg):\n        return S.Pi/2 - 2*acot((1 + sqrt(1 - arg**2))/arg)\n\n    def _eval_rewrite_as_asec(self, arg):\n        return asec(1/arg)\n\n    def _eval_rewrite_as_acsc(self, arg):\n        return S.Pi/2 - acsc(1/arg)\n\n    def _eval_conjugate(self):\n        z = self.args[0]\n        r = self.func(self.args[0].conjugate())\n        if z.is_real is False:\n            return r\n        elif z.is_real and (z + 1).is_nonnegative and (z - 1).is_nonpositive:\n            return r\n\n\nclass atan(InverseTrigonometricFunction):\n    \"\"\"\n    The inverse tangent function.\n\n    Returns the arc tangent of x (measured in radians).\n\n    Notes\n    =====\n\n    atan(x) will evaluate automatically in the cases\n    oo, -oo, 0, 1, -1.\n\n    Examples\n    ========\n\n    >>> from sympy import atan, oo, pi\n    >>> atan(0)\n    0\n    >>> atan(1)\n    pi/4\n    >>> atan(oo)\n    pi/2\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan, cot\n    asin, acsc, acos, asec, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Inverse_trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.23\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/ArcTan\n    \"\"\"\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return 1/(1 + self.args[0]**2)\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def _eval_is_rational(self):\n        s = self.func(*self.args)\n        if s.func == self.func:\n            if s.args[0].is_rational:\n                return False\n        else:\n            return s.is_rational\n\n    def _eval_is_positive(self):\n        return self.args[0].is_positive\n\n    def _eval_is_nonnegative(self):\n        return self.args[0].is_nonnegative\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Infinity:\n                return S.Pi / 2\n            elif arg is S.NegativeInfinity:\n                return -S.Pi / 2\n            elif arg is S.Zero:\n                return S.Zero\n            elif arg is S.One:\n                return S.Pi / 4\n            elif arg is S.NegativeOne:\n                return -S.Pi / 4\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        if arg.is_number:\n            cst_table = {\n                sqrt(3)/3: 6,\n                -sqrt(3)/3: -6,\n                1/sqrt(3): 6,\n                -1/sqrt(3): -6,\n                sqrt(3): 3,\n                -sqrt(3): -3,\n                (1 + sqrt(2)): S(8)/3,\n                -(1 + sqrt(2)): S(8)/3,\n                (sqrt(2) - 1): 8,\n                (1 - sqrt(2)): -8,\n                sqrt((5 + 2*sqrt(5))): S(5)/2,\n                -sqrt((5 + 2*sqrt(5))): -S(5)/2,\n                (2 - sqrt(3)): 12,\n                -(2 - sqrt(3)): -12\n            }\n\n            if arg in cst_table:\n                return S.Pi / cst_table[arg]\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return S.ImaginaryUnit * atanh(i_coeff)\n(B)     See Also\n    ========\n\n    sin, csc, cos, sec, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Tan\n    \"\"\"\n\n    def period(self, symbol=None):\n        return self._period(pi, symbol)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return S.One + self**2\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return atan\n\n    @classmethod\n    def eval(cls, arg):\n        from sympy.calculus.util import AccumBounds\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Zero:\n                return S.Zero\n            elif arg is S.Infinity or arg is S.NegativeInfinity:\n                return AccumBounds(S.NegativeInfinity, S.Infinity)\n\n        if isinstance(arg, AccumBounds):\n            min, max = arg.min, arg.max\n            d = floor(min/S.Pi)\n            if min is not S.NegativeInfinity:\n                min = min - d*S.Pi\n            if max is not S.Infinity:\n                max = max - d*S.Pi\n            if AccumBounds(min, max).intersection(FiniteSet(S.Pi/2, 3*S.Pi/2)):\n                return AccumBounds(S.NegativeInfinity, S.Infinity)\n            else:\n                return AccumBounds(tan(min), tan(max))\n\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return S.ImaginaryUnit * tanh(i_coeff)\n\n        pi_coeff = _pi_coeff(arg, 2)\n        if pi_coeff is not None:\n            if pi_coeff.is_integer:\n                return S.Zero\n\n            if not pi_coeff.is_Rational:\n                narg = pi_coeff*S.Pi\n                if narg != arg:\n                    return cls(narg)\n                return None\n\n            if pi_coeff.is_Rational:\n                if not pi_coeff.q % 2:\n                    narg = pi_coeff*S.Pi*2\n                    cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                    if not isinstance(cresult, cos) \\\n                            and not isinstance(sresult, cos):\n                        if sresult == 0:\n                            return S.ComplexInfinity\n                        return (1 - cresult)/sresult\n                table2 = {\n                    12: (3, 4),\n                    20: (4, 5),\n                    30: (5, 6),\n                    15: (6, 10),\n                    24: (6, 8),\n                    40: (8, 10),\n                    60: (20, 30),\n                    120: (40, 60)\n                    }\n                q = pi_coeff.q\n                p = pi_coeff.p % q\n                if q in table2:\n                    nvala, nvalb = cls(p*S.Pi/table2[q][0]), cls(p*S.Pi/table2[q][1])\n                    if None == nvala or None == nvalb:\n                        return None\n                    return (nvala - nvalb)/(1 + nvala*nvalb)\n                narg = ((pi_coeff + S.Half) % 1 - S.Half)*S.Pi\n                # see cos() to specify which expressions should  be\n                # expanded automatically in terms of radicals\n                cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                if not isinstance(cresult, cos) \\\n                        and not isinstance(sresult, cos):\n                    if cresult == 0:\n                        return S.ComplexInfinity\n                    return (sresult/cresult)\n                if narg != arg:\n                    return cls(narg)\n\n        if arg.is_Add:\n            x, m = _peeloff_pi(arg)\n            if m:\n                tanm = tan(m)\n                if tanm is S.ComplexInfinity:\n                    return -cot(x)\n                else: # tanm == 0\n                    return tan(x)\n\n        if isinstance(arg, atan):\n            return arg.args[0]\n\n        if isinstance(arg, atan2):\n            y, x = arg.args\n            return y/x\n\n        if isinstance(arg, asin):\n            x = arg.args[0]\n            return x / sqrt(1 - x**2)\n\n        if isinstance(arg, acos):\n            x = arg.args[0]\n            return sqrt(1 - x**2) / x\n\n        if isinstance(arg, acot):\n            x = arg.args[0]\n            return 1 / x\n\n        if isinstance(arg, acsc):\n            x = arg.args[0]\n            return 1 / (sqrt(1 - 1 / x**2) * x)\n\n        if isinstance(arg, asec):\n            x = arg.args[0]\n            return sqrt(1 - 1 / x**2) * x\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        from sympy import bernoulli\n        if n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n\n            a, b = ((n - 1)//2), 2**(n + 1)\n\n            B = bernoulli(n + 1)\n            F = factorial(n + 1)\n\n            return (-1)**a * b*(b - 1) * B/F * x**n\n\n    def _eval_nseries(self, x, n, logx):\n        i = self.args[0].limit(x, 0)*2/S.Pi\n        if i and i.is_Integer:\n            return self.rewrite(cos)._eval_nseries(x, n=n, logx=logx)\n        return Function._eval_nseries(self, x, n=n, logx=logx)\n\n    def _eval_rewrite_as_Pow(self, arg):\n        if isinstance(arg, log):\n            I = S.ImaginaryUnit\n            x = arg.args[0]\n            return I*(x**-I - x**I)/(x**-I + x**I)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        re, im = self._as_real_imag(deep=deep, **hints)\n        if im:\n            denom = cos(2*re) + cosh(2*im)\n            return (sin(2*re)/denom, sinh(2*im)/denom)\n        else:\n            return (self.func(re), S.Zero)\n\n    def _eval_expand_trig(self, **hints):\n        from sympy import im, re\n        arg = self.args[0]\n        x = None\n        if arg.is_Add:\n            from sympy import symmetric_poly\n            n = len(arg.args)\n            TX = []\n            for x in arg.args:\n                tx = tan(x, evaluate=False)._eval_expand_trig()\n                TX.append(tx)\n\n            Yg = numbered_symbols('Y')\n            Y = [ next(Yg) for i in range(n) ]\n\n            p = [0, 0]\n            for i in range(n + 1):\n                p[1 - i % 2] += symmetric_poly(i, Y)*(-1)**((i % 4)//2)\n            return (p[0]/p[1]).subs(list(zip(Y, TX)))\n\n        else:\n            coeff, terms = arg.as_coeff_Mul(rational=True)\n            if coeff.is_Integer and coeff > 1:\n                I = S.ImaginaryUnit\n                z = Symbol('dummy', real=True)\n                P = ((1 + I*z)**coeff).expand()\n                return (im(P)/re(P)).subs([(z, tan(terms))])\n        return tan(arg)\n\n    def _eval_rewrite_as_exp(self, arg):\n        I = S.ImaginaryUnit\n        if isinstance(arg, TrigonometricFunction) or isinstance(arg, HyperbolicFunction):\n            arg = arg.func(arg.args[0]).rewrite(exp)\n        neg_exp, pos_exp = exp(-arg*I), exp(arg*I)\n        return I*(neg_exp - pos_exp)/(neg_exp + pos_exp)\n\n    def _eval_rewrite_as_sin(self, x):\n        return 2*sin(x)**2/sin(2*x)\n\n    def _eval_rewrite_as_cos(self, x):\n        return cos(x - S.Pi / 2, evaluate=False) / cos(x)\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return sin(arg)/cos(arg)\n\n    def _eval_rewrite_as_cot(self, arg):\n        return 1/cot(arg)\n\n    def _eval_rewrite_as_sec(self, arg):\n        sin_in_sec_form = sin(arg)._eval_rewrite_as_sec(arg)\n        cos_in_sec_form = cos(arg)._eval_rewrite_as_sec(arg)\n        return sin_in_sec_form / cos_in_sec_form\n\n    def _eval_rewrite_as_csc(self, arg):\n        sin_in_csc_form = sin(arg)._eval_rewrite_as_csc(arg)\n        cos_in_csc_form = cos(arg)._eval_rewrite_as_csc(arg)\n        return sin_in_csc_form / cos_in_csc_form\n\n    def _eval_rewrite_as_pow(self, arg):\n        y = self.rewrite(cos).rewrite(pow)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        y = self.rewrite(cos).rewrite(sqrt)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return arg\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        return self.args[0].is_real\n\n    def _eval_is_finite(self):\n        arg = self.args[0]\n\n        if arg.is_imaginary:\n            return True\n\n\nclass cot(TrigonometricFunction):\n    \"\"\"\n    The cotangent function.\n\n    Returns the cotangent of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import cot, pi\n    >>> from sympy.abc import x\n    >>> cot(x**2).diff(x)\n    2*x*(-cot(x**2)**2 - 1)\n    >>> cot(1).diff(x)\n    0\n    >>> cot(pi/12)\n    sqrt(3) + 2\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Cot\n    \"\"\"\n\n    def period(self, symbol=None):\n        return self._period(pi, symbol)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return S.NegativeOne - self**2\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return acot\n\n    @classmethod\n    def eval(cls, arg):\n        from sympy.calculus.util import AccumBounds\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            if arg is S.Zero:\n                return S.ComplexInfinity\n\n        if isinstance(arg, AccumBounds):\n            return -tan(arg + S.Pi/2)\n\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return -S.ImaginaryUnit * coth(i_coeff)\n\n        pi_coeff = _pi_coeff(arg, 2)\n        if pi_coeff is not None:\n            if pi_coeff.is_integer:\n                return S.ComplexInfinity\n\n            if not pi_coeff.is_Rational:\n                narg = pi_coeff*S.Pi\n                if narg != arg:\n                    return cls(narg)\n                return None\n\n            if pi_coeff.is_Rational:\n                if pi_coeff.q > 2 and not pi_coeff.q % 2:\n                    narg = pi_coeff*S.Pi*2\n                    cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                    if not isinstance(cresult, cos) \\\n                            and not isinstance(sresult, cos):\n                        return (1 + cresult)/sresult\n                table2 = {\n                    12: (3, 4),\n                    20: (4, 5),\n                    30: (5, 6),\n                    15: (6, 10),\n                    24: (6, 8),\n                    40: (8, 10),\n                    60: (20, 30),\n                    120: (40, 60)\n                    }\n                q = pi_coeff.q\n                p = pi_coeff.p % q\n                if q in table2:\n                    nvala, nvalb = cls(p*S.Pi/table2[q][0]), cls(p*S.Pi/table2[q][1])\n                    if None == nvala or None == nvalb:\n                        return None\n                    return (1 + nvala*nvalb)/(nvalb - nvala)\n                narg = (((pi_coeff + S.Half) % 1) - S.Half)*S.Pi\n                # see cos() to specify which expressions should be\n                # expanded automatically in terms of radicals\n                cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                if not isinstance(cresult, cos) \\\n                        and not isinstance(sresult, cos):\n                    if sresult == 0:\n                        return S.ComplexInfinity\n                    return cresult / sresult\n                if narg != arg:\n                    return cls(narg)\n\n        if arg.is_Add:\n            x, m = _peeloff_pi(arg)\n            if m:\n                cotm = cot(m)\n                if cotm is S.ComplexInfinity:\n                    return cot(x)\n                else: # cotm == 0\n                    return -tan(x)\n\n        if isinstance(arg, acot):\n            return arg.args[0]\n\n        if isinstance(arg, atan):\n            x = arg.args[0]\n            return 1 / x\n\n        if isinstance(arg, atan2):\n            y, x = arg.args\n            return x/y\n\n        if isinstance(arg, asin):\n            x = arg.args[0]\n            return sqrt(1 - x**2) / x\n\n        if isinstance(arg, acos):\n            x = arg.args[0]\n            return x / sqrt(1 - x**2)\n\n        if isinstance(arg, acsc):\n            x = arg.args[0]\n            return sqrt(1 - 1 / x**2) * x\n\n        if isinstance(arg, asec):\n            x = arg.args[0]\n            return 1 / (sqrt(1 - 1 / x**2) * x)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        from sympy import bernoulli\n        if n == 0:\n            return 1 / sympify(x)\n        elif n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n\n            B = bernoulli(n + 1)\n            F = factorial(n + 1)\n\n            return (-1)**((n + 1)//2) * 2**(n + 1) * B/F * x**n\n\n    def _eval_nseries(self, x, n, logx):\n        i = self.args[0].limit(x, 0)/S.Pi\n        if i and i.is_Integer:\n            return self.rewrite(cos)._eval_nseries(x, n=n, logx=logx)\n        return self.rewrite(tan)._eval_nseries(x, n=n, logx=logx)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        re, im = self._as_real_imag(deep=deep, **hints)\n        if im:\n            denom = cos(2*re) - cosh(2*im)\n            return (-sin(2*re)/denom, -sinh(2*im)/denom)\n        else:\n            return (self.func(re), S.Zero)\n\n    def _eval_rewrite_as_exp(self, arg):\n        I = S.ImaginaryUnit\n        if isinstance(arg, TrigonometricFunction) or isinstance(arg, HyperbolicFunction):\n            arg = arg.func(arg.args[0]).rewrite(exp)\n        neg_exp, pos_exp = exp(-arg*I), exp(arg*I)\n        return I*(pos_exp + neg_exp)/(pos_exp - neg_exp)\n\n    def _eval_rewrite_as_Pow(self, arg):\n        if isinstance(arg, log):\n            I = S.ImaginaryUnit\n            x = arg.args[0]\n            return -I*(x**-I + x**I)/(x**-I - x**I)\n\n    def _eval_rewrite_as_sin(self, x):\n        return 2*sin(2*x)/sin(x)**2\n\n    def _eval_rewrite_as_cos(self, x):\n        return cos(x) / cos(x - S.Pi / 2, evaluate=False)\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return cos(arg)/sin(arg)\n\n    def _eval_rewrite_as_tan(self, arg):\n        return 1/tan(arg)\n\n    def _eval_rewrite_as_sec(self, arg):\n        cos_in_sec_form = cos(arg)._eval_rewrite_as_sec(arg)\n        sin_in_sec_form = sin(arg)._eval_rewrite_as_sec(arg)\n        return cos_in_sec_form / sin_in_sec_form\n\n    def _eval_rewrite_as_csc(self, arg):\n        cos_in_csc_form = cos(arg)._eval_rewrite_as_csc(arg)\n        sin_in_csc_form = sin(arg)._eval_rewrite_as_csc(arg)\n        return cos_in_csc_form / sin_in_csc_form\n\n    def _eval_rewrite_as_pow(self, arg):\n        y = self.rewrite(cos).rewrite(pow)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        y = self.rewrite(cos).rewrite(sqrt)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return 1/arg\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        return self.args[0].is_real\n\n    def _eval_expand_trig(self, **hints):\n        from sympy import im, re\n        arg = self.args[0]\n        x = None\n        if arg.is_Add:\n            from sympy import symmetric_poly\n            n = len(arg.args)\n            CX = []\n            for x in arg.args:\n                cx = cot(x, evaluate=False)._eval_expand_trig()\n                CX.append(cx)\n\n            Yg = numbered_symbols('Y')\n            Y = [ next(Yg) for i in range(n) ]\n\n            p = [0, 0]\n            for i in range(n, -1, -1):\n                p[(n - i) % 2] += symmetric_poly(i, Y)*(-1)**(((n - i) % 4)//2)\n            return (p[0]/p[1]).subs(list(zip(Y, CX)))\n        else:\n            coeff, terms = arg.as_coeff_Mul(rational=True)\n            if coeff.is_Integer and coeff > 1:\n                I = S.ImaginaryUnit\n                z = Symbol('dummy', real=True)\n                P = ((z + I)**coeff).expand()\n                return (re(P)/im(P)).subs([(z, cot(terms))])\n        return cot(arg)\n\n    def _eval_is_finite(self):\n        arg = self.args[0]\n        if arg.is_imaginary:\n            return True\n\n    def _eval_subs(self, old, new):\n        if self == old:\n            return new\n        arg = self.args[0]\n        argnew = arg.subs(old, new)\n        if arg != argnew and (argnew/S.Pi).is_integer:\n            return S.ComplexInfinity\n        return cot(argnew)\n\n\nclass ReciprocalTrigonometricFunction(TrigonometricFunction):\n    \"\"\"Base class for reciprocal functions of trigonometric functions. \"\"\"\n\n    _reciprocal_of = None       # mandatory, to be defined in subclass\n\n    # _is_even and _is_odd are used for correct evaluation of csc(-x), sec(-x)\n    # TODO refactor into TrigonometricFunction common parts of\n    # trigonometric functions eval() like even/odd, func(x+2*k*pi), etc.\n    _is_even = None  # optional, to be defined in subclass\n    _is_odd = None   # optional, to be defined in subclass\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.could_extract_minus_sign():\n            if cls._is_even:\n                return cls(-arg)\n            if cls._is_odd:\n                return -cls(-arg)\n\n        pi_coeff = _pi_coeff(arg)\n        if (pi_coeff is not None\n            and not (2*pi_coeff).is_integer\n            and pi_coeff.is_Rational):\n                q = pi_coeff.q\n                p = pi_coeff.p % (2*q)\n                if p > q:\n                    narg = (pi_coeff - 1)*S.Pi\n                    return -cls(narg)\n                if 2*p > q:\n                    narg = (1 - pi_coeff)*S.Pi\n                    if cls._is_odd:\n                        return cls(narg)\n                    elif cls._is_even:\n                        return -cls(narg)\n\n        t = cls._reciprocal_of.eval(arg)\n        if hasattr(arg, 'inverse') and arg.inverse() == cls:\n            return arg.args[0]\n        return 1/t if t != None else t\n\n    def _call_reciprocal(self, method_name, *args, **kwargs):\n        # Calls method_name on _reciprocal_of\n        o = self._reciprocal_of(self.args[0])\n        return getattr(o, method_name)(*args, **kwargs)\n\n    def _calculate_reciprocal(self, method_name, *args, **kwargs):\n        # If calling method_name on _reciprocal_of returns a value != None\n        # then return the reciprocal of that value\n        t = self._call_reciprocal(method_name, *args, **kwargs)\n        return 1/t if t != None else t\n\n    def _rewrite_reciprocal(self, method_name, arg):\n        # Special handling for rewrite functions. If reciprocal rewrite returns\n        # unmodified expression, then return None\n        t = self._call_reciprocal(method_name, arg)\n        if t != None and t != self._reciprocal_of(arg):\n            return 1/t\n\n    def _period(self, symbol):\n        f = self.args[0]\n        return self._reciprocal_of(f).period(symbol)\n\n    def fdiff(self, argindex=1):\n        return -self._calculate_reciprocal(\"fdiff\", argindex)/self**2\n\n    def _eval_rewrite_as_exp(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_exp\", arg)\n\n    def _eval_rewrite_as_Pow(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_Pow\", arg)\n\n    def _eval_rewrite_as_sin(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_sin\", arg)\n\n    def _eval_rewrite_as_cos(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_cos\", arg)\n\n    def _eval_rewrite_as_tan(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_tan\", arg)\n\n    def _eval_rewrite_as_pow(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_pow\", arg)\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_sqrt\", arg)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        return (1/self._reciprocal_of(self.args[0])).as_real_imag(deep,\n                                                                  **hints)\n\n    def _eval_expand_trig(self, **hints):\n        return self._calculate_reciprocal(\"_eval_expand_trig\", **hints)\n\n    def _eval_is_real(self):\n        return self._reciprocal_of(self.args[0])._eval_is_real()\n\n    def _eval_as_leading_term(self, x):\n        return (1/self._reciprocal_of(self.args[0]))._eval_as_leading_term(x)\n\n    def _eval_is_finite(self):\n        return (1/self._reciprocal_of(self.args[0])).is_finite\n\n    def _eval_nseries(self, x, n, logx):\n        return (1/self._reciprocal_of(self.args[0]))._eval_nseries(x, n, logx)\n\n\nclass sec(ReciprocalTrigonometricFunction):\n    \"\"\"\n    The secant function.\n\n    Returns the secant of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import sec\n    >>> from sympy.abc import x\n    >>> sec(x**2).diff(x)\n    2*x*tan(x**2)*sec(x**2)\n    >>> sec(1).diff(x)\n    0\n\n    See Also\n    ========\n\n    sin, csc, cos, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Sec\n    \"\"\"\n\n    _reciprocal_of = cos\n    _is_even = True\n\n    def period(self, symbol=None):\n        return self._period(symbol)\n\n    def _eval_rewrite_as_cot(self, arg):\n        cot_half_sq = cot(arg/2)**2\n        return (cot_half_sq + 1)/(cot_half_sq - 1)\n\n    def _eval_rewrite_as_cos(self, arg):\n        return (1/cos(arg))\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return sin(arg)/(cos(arg)*sin(arg))\n\n    def _eval_rewrite_as_sin(self, arg):\n        return (1 / cos(arg)._eval_rewrite_as_sin(arg))\n\n    def _eval_rewrite_as_tan(self, arg):\n        return (1 / cos(arg)._eval_rewrite_as_tan(arg))\n\n    def _eval_rewrite_as_csc(self, arg):\n        return csc(pi / 2 - arg, evaluate=False)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return tan(self.args[0])*sec(self.args[0])\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        # Reference Formula:\n        # http://functions.wolfram.com/ElementaryFunctions/Sec/06/01/02/01/\n        from sympy.functions.combinatorial.numbers import euler\n        if n < 0 or n % 2 == 1:\n            return S.Zero\n        else:\n            x = sympify(x)\n            k = n//2\n            return (-1)**k*euler(2*k)/factorial(2*k)*x**(2*k)\n\n\nclass csc(ReciprocalTrigonometricFunction):\n    \"\"\"\n    The cosecant function.\n\n    Returns the cosecant of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import csc\n    >>> from sympy.abc import x\n    >>> csc(x**2).diff(x)\n    -2*x*cot(x**2)*csc(x**2)\n    >>> csc(1).diff(x)\n    0\n\n    See Also\n    ========\n\n    sin, cos, sec, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Csc\n    \"\"\"\n\n    _reciprocal_of = sin\n    _is_odd = True\n\n    def period(self, symbol=None):\n        return self._period(symbol)\n\n    def _eval_rewrite_as_sin(self, arg):\n        return (1/sin(arg))\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return cos(arg)/(sin(arg)*cos(arg))\n\n    def _eval_rewrite_as_cot(self, arg):\n        cot_half = cot(arg/2)\n        return (1 + cot_half**2)/(2*cot_half)\n\n    def _eval_rewrite_as_cos(self, arg):\n        return (1 / sin(arg)._eval_rewrite_as_cos(arg))\n\n    def _eval_rewrite_as_sec(self, arg):\n        return sec(pi / 2 - arg, evaluate=False)\n\n    def _eval_rewrite_as_tan(self, arg):\n        return (1 / sin(arg)._eval_rewrite_as_tan(arg))\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return -cot(self.args[0])*csc(self.args[0])\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        from sympy import bernoulli\n        if n == 0:\n            return 1/sympify(x)\n        elif n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n            k = n//2 + 1\n            return ((-1)**(k - 1)*2*(2**(2*k - 1) - 1)*\n                    bernoulli(2*k)*x**(2*k - 1)/factorial(2*k))\n\n\nclass sinc(TrigonometricFunction):\n    r\"\"\"Represents unnormalized sinc function\n\n    Examples\n    ========\n\n    >>> from sympy import sinc, oo, jn, Product, Symbol\n    >>> from sympy.abc import x\n    >>> sinc(x)\n    sinc(x)\n\n    * Automated Evaluation\n\n    >>> sinc(0)\n    1\n    >>> sinc(oo)\n    0\n\n    * Differentiation\n\n    >>> sinc(x).diff()\n    (x*cos(x) - sin(x))/x**2\n\n    * Series Expansion\n\n    >>> sinc(x).series()\n    1 - x**2/6 + x**4/120 + O(x**6)\n\n    * As zero'th order spherical Bessel Function\n\n    >>> sinc(x).rewrite(jn)\n    jn(0, x)\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Sinc_function\n\n    \"\"\"\n\n    def fdiff(self, argindex=1):\n        x = self.args[0]\n        if argindex == 1:\n            return (x*cos(x) - sin(x)) / x**2\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_zero:\n            return S.One\n        if arg.is_Number:\n            if arg in [S.Infinity, -S.Infinity]:\n                return S.Zero\n            elif arg is S.NaN:\n                return S.NaN\n\n        if arg is S.ComplexInfinity:\n            return S.NaN\n\n        if arg.could_extract_minus_sign():\n            return cls(-arg)\n\n        pi_coeff = _pi_coeff(arg)\n        if pi_coeff is not None:\n            if pi_coeff.is_integer:\n                if fuzzy_not(arg.is_zero):\n                    return S.Zero\n            elif (2*pi_coeff).is_integer:\n                return S.NegativeOne**(pi_coeff - S.Half) / arg\n\n    def _eval_nseries(self, x, n, logx):\n        x = self.args[0]\n        return (sin(x)/x)._eval_nseries(x, n, logx)\n\n    def _eval_rewrite_as_jn(self, arg):\n        from sympy.functions.special.bessel import jn\n        return jn(0, arg)\n\n    def _eval_rewrite_as_sin(self, arg):\n        return sin(arg) / arg\n\n\n###############################################################################\n########################### TRIGONOMETRIC INVERSES ############################\n###############################################################################\n\n\nclass InverseTrigonometricFunction(Function):\n    \"\"\"Base class for inverse trigonometric functions.\"\"\"\n\n    pass\n\n\nclass asin(InverseTrigonometricFunction):\n    \"\"\"\n    The inverse sine function.\n\n    Returns the arcsine of x in radians.\n\n    Notes\n    =====\n\n    asin(x) will evaluate automatically in the cases oo, -oo, 0, 1,\n    -1 and for some instances when the result is a rational multiple\n    of pi (see the eval class method).\n\n    Examples\n    ========\n\n    >>> from sympy import asin, oo, pi\n    >>> asin(1)\n    pi/2\n    >>> asin(-1)\n    -pi/2\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan, cot\n    acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Inverse_trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.23\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/ArcSin\n    \"\"\"\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return 1/sqrt(1 - self.args[0]**2)\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def _eval_is_rational(self):\n        s = self.func(*self.args)\n        if s.func == self.func:\n            if s.args[0].is_rational:\n                return False\n        else:\n            return s.is_rational\n\n    def _eval_is_positive(self):\n        if self.args[0].is_positive:\n            return (self.args[0] - 1).is_negative\n        if self.args[0].is_negative:\n            return not (self.args[0] + 1).is_positive\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Infinity:\n                return S.NegativeInfinity * S.ImaginaryUnit\n            elif arg is S.NegativeInfinity:\n                return S.Infinity * S.ImaginaryUnit\n            elif arg is S.Zero:\n                return S.Zero\n            elif arg is S.One:\n                return S.Pi / 2\n            elif arg is S.NegativeOne:\n                return -S.Pi / 2\n\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        if arg.is_number:\n            cst_table = {\n                sqrt(3)/2: 3,\n                -sqrt(3)/2: -3,\n                sqrt(2)/2: 4,\n                -sqrt(2)/2: -4,\n                1/sqrt(2): 4,\n                -1/sqrt(2): -4,\n                sqrt((5 - sqrt(5))/8): 5,\n                -sqrt((5 - sqrt(5))/8): -5,\n                S.Half: 6,\n                -S.Half: -6,\n                sqrt(2 - sqrt(2))/2: 8,\n                -sqrt(2 - sqrt(2))/2: -8,\n                (sqrt(5) - 1)/4: 10,\n                (1 - sqrt(5))/4: -10,\n                (sqrt(3) - 1)/sqrt(2**3): 12,\n                (1 - sqrt(3))/sqrt(2**3): -12,\n                (sqrt(5) + 1)/4: S(10)/3,\n                -(sqrt(5) + 1)/4: -S(10)/3\n            }\n\n            if arg in cst_table:\n                return S.Pi / cst_table[arg]\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return S.ImaginaryUnit * asinh(i_coeff)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        if n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n            if len(previous_terms) >= 2 and n > 2:\n                p = previous_terms[-2]\n                return p * (n - 2)**2/(n*(n - 1)) * x**2\n            else:\n                k = (n - 1) // 2\n                R = RisingFactorial(S.Half, k)\n                F = factorial(k)\n                return R / F * x**n / n\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return arg\n        else:\n            return self.func(arg)\n\n    def _eval_rewrite_as_acos(self, x):\n        return S.Pi/2 - acos(x)\n\n    def _eval_rewrite_as_atan(self, x):\n        return 2*atan(x/(1 + sqrt(1 - x**2)))\n\n    def _eval_rewrite_as_log(self, x):\n        return -S.ImaginaryUnit*log(S.ImaginaryUnit*x + sqrt(1 - x**2))\n\n    def _eval_rewrite_as_acot(self, arg):\n        return 2*acot((1 + sqrt(1 - arg**2))/arg)\n\n    def _eval_rewrite_as_asec(self, arg):\n        return S.Pi/2 - asec(1/arg)\n\n    def _eval_rewrite_as_acsc(self, arg):\n        return acsc(1/arg)\n\n    def _eval_is_real(self):\n        x = self.args[0]\n        return x.is_real and (1 - abs(x)).is_nonnegative\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return sin\n\n\nclass acos(InverseTrigonometricFunction):\n    \"\"\"\n    The inverse cosine function.\n\n    Returns the arc cosine of x (measured in radians).\n\n    Notes\n    =====\n\n    ``acos(x)`` will evaluate automatically in the cases\n    ``oo``, ``-oo``, ``0``, ``1``, ``-1``.\n\n    ``acos(zoo)`` evaluates to ``zoo``\n    (see note in :py:class`sympy.functions.elementary.trigonometric.asec`)\n\n    Examples\n    ========\n\n    >>> from sympy import acos, oo, pi\n    >>> acos(1)\n    0\n    >>> acos(0)\n    pi/2\n    >>> acos(oo)\n    oo*I\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan, cot\n    asin, acsc, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Inverse_trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.23\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/ArcCos\n    \"\"\"\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return -1/sqrt(1 - self.args[0]**2)\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def _eval_is_rational(self):\n        s = self.func(*self.args)\n        if s.func == self.func:\n            if s.args[0].is_rational:\n                return False\n        else:\n            return s.is_rational\n\n    def _eval_is_positive(self):\n        x = self.args[0]\n        return (1 - abs(x)).is_nonnegative\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Infinity:\n                return S.Infinity * S.ImaginaryUnit\n            elif arg is S.NegativeInfinity:\n                return S.NegativeInfinity * S.ImaginaryUnit\n            elif arg is S.Zero:\n                return S.Pi / 2\n            elif arg is S.One:\n                return S.Zero\n            elif arg is S.NegativeOne:\n                return S.Pi\n\n        if arg is S.ComplexInfinity:\n            return S.ComplexInfinity\n\n        if arg.is_number:\n            cst_table = {\n                S.Half: S.Pi/3,\n                -S.Half: 2*S.Pi/3,\n                sqrt(2)/2: S.Pi/4,\n                -sqrt(2)/2: 3*S.Pi/4,\n                1/sqrt(2): S.Pi/4,\n                -1/sqrt(2): 3*S.Pi/4,\n                sqrt(3)/2: S.Pi/6,\n                -sqrt(3)/2: 5*S.Pi/6,\n            }\n\n            if arg in cst_table:\n                return cst_table[arg]\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        if n == 0:\n            return S.Pi / 2\n        elif n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n            if len(previous_terms) >= 2 and n > 2:\n                p = previous_terms[-2]\n                return p * (n - 2)**2/(n*(n - 1)) * x**2\n            else:\n                k = (n - 1) // 2\n                R = RisingFactorial(S.Half, k)\n                F = factorial(k)\n                return -R / F * x**n / n\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return arg\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        x = self.args[0]\n        return x.is_real and (1 - abs(x)).is_nonnegative\n\n    def _eval_rewrite_as_log(self, x):\n        return S.Pi/2 + S.ImaginaryUnit * \\\n            log(S.ImaginaryUnit * x + sqrt(1 - x**2))\n\n    def _eval_rewrite_as_asin(self, x):\n        return S.Pi/2 - asin(x)\n\n    def _eval_rewrite_as_atan(self, x):\n        return atan(sqrt(1 - x**2)/x) + (S.Pi/2)*(1 - x*sqrt(1/x**2))\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return cos\n\n    def _eval_rewrite_as_acot(self, arg):\n        return S.Pi/2 - 2*acot((1 + sqrt(1 - arg**2))/arg)\n\n    def _eval_rewrite_as_asec(self, arg):\n        return asec(1/arg)\n\n    def _eval_rewrite_as_acsc(self, arg):\n        return S.Pi/2 - acsc(1/arg)\n\n    def _eval_conjugate(self):\n        z = self.args[0]\n        r = self.func(self.args[0].conjugate())\n        if z.is_real is False:\n            return r\n        elif z.is_real and (z + 1).is_nonnegative and (z - 1).is_nonpositive:\n            return r\n\n\nclass atan(InverseTrigonometricFunction):\n    \"\"\"\n    The inverse tangent function.\n\n    Returns the arc tangent of x (measured in radians).\n\n    Notes\n    =====\n\n    atan(x) will evaluate automatically in the cases\n    oo, -oo, 0, 1, -1.\n\n    Examples\n    ========\n\n    >>> from sympy import atan, oo, pi\n    >>> atan(0)\n    0\n    >>> atan(1)\n    pi/4\n    >>> atan(oo)\n    pi/2\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan, cot\n    asin, acsc, acos, asec, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Inverse_trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.23\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/ArcTan\n    \"\"\"\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return 1/(1 + self.args[0]**2)\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def _eval_is_rational(self):\n        s = self.func(*self.args)\n        if s.func == self.func:\n            if s.args[0].is_rational:\n                return False\n        else:\n            return s.is_rational\n\n    def _eval_is_positive(self):\n        return self.args[0].is_positive\n\n    def _eval_is_nonnegative(self):\n        return self.args[0].is_nonnegative\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Infinity:\n                return S.Pi / 2\n            elif arg is S.NegativeInfinity:\n                return -S.Pi / 2\n            elif arg is S.Zero:\n                return S.Zero\n            elif arg is S.One:\n                return S.Pi / 4\n            elif arg is S.NegativeOne:\n                return -S.Pi / 4\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        if arg.is_number:\n            cst_table = {\n                sqrt(3)/3: 6,\n                -sqrt(3)/3: -6,\n                1/sqrt(3): 6,\n                -1/sqrt(3): -6,\n                sqrt(3): 3,\n                -sqrt(3): -3,\n                (1 + sqrt(2)): S(8)/3,\n                -(1 + sqrt(2)): S(8)/3,\n                (sqrt(2) - 1): 8,\n                (1 - sqrt(2)): -8,\n                sqrt((5 + 2*sqrt(5))): S(5)/2,\n                -sqrt((5 + 2*sqrt(5))): -S(5)/2,\n                (2 - sqrt(3)): 12,\n                -(2 - sqrt(3)): -12\n            }\n\n            if arg in cst_table:\n                return S.Pi / cst_table[arg]\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return S.ImaginaryUnit * atanh(i_coeff)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        if n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n            return (-1)**((n - 1)//2) * x**n / n\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return arg\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        return self.args[0].is_real\n\n    def _eval_rewrite_as_log(self, x):\n        return S.ImaginaryUnit/2 * (log(S(1) - S.ImaginaryUnit * x)\n            - log(S(1) + S.ImaginaryUnit * x))\n\n    def _eval_aseries(self, n, args0, x, logx):\n        if args0[0] == S.Infinity:\n            return (S.Pi/2 - atan(1/self.args[0]))._eval_nseries(x, n, logx)\n        elif args0[0] == S.NegativeInfinity:\n            return (-S.Pi/2 - atan(1/self.args[0]))._eval_nseries(x, n, logx)\n        else:\n            return super(atan, self)._eval_aseries(n, args0, x, logx)\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return tan\n\n    def _eval_rewrite_as_asin(self, arg):\n        return sqrt(arg**2)/arg*(S.Pi/2 - asin(1/sqrt(1 + arg**2)))\n\n    def _eval_rewrite_as_acos(self, arg):\n        return sqrt(arg**2)/arg*acos(1/sqrt(1 + arg**2))\n\n    def _eval_rewrite_as_acot(self, arg):\n        return acot(1/arg)\n\n    def _eval_rewrite_as_asec(self, arg):\n        return sqrt(arg**2)/arg*asec(sqrt(1 + arg**2))\n\n    def _eval_rewrite_as_acsc(self, arg):\n        return sqrt(arg**2)/arg*(S.Pi/2 - acsc(sqrt(1 + arg**2)))\n\n\nclass acot(InverseTrigonometricFunction):\n    \"\"\"\n    The inverse cotangent function.\n\n    Returns the arc cotangent of x (measured in radians).\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan, cot\n    asin, acsc, acos, asec, atan, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Inverse_trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.23\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/ArcCot\n    \"\"\"\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return -1 / (1 + self.args[0]**2)\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def _eval_is_rational(self):\n        s = self.func(*self.args)\n        if s.func == self.func:\n            if s.args[0].is_rational:\n                return False\n        else:\n            return s.is_rational\n\n    def _eval_is_positive(self):\n        return self.args[0].is_real\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Infinity:\n                return S.Zero\n            elif arg is S.NegativeInfinity:\n                return S.Zero\n            elif arg is S.Zero:\n                return S.Pi/ 2\n            elif arg is S.One:\n                return S.Pi / 4\n            elif arg is S.NegativeOne:\n                return -S.Pi / 4\n\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        if arg.is_number:\n            cst_table = {\n                sqrt(3)/3: 3,\n                -sqrt(3)/3: -3,\n                1/sqrt(3): 3,\n                -1/sqrt(3): -3,\n                sqrt(3): 6,\n                -sqrt(3): -6,\n                (1 + sqrt(2)): 8,\n                -(1 + sqrt(2)): -8,\n                (1 - sqrt(2)): -S(8)/3,\n                (sqrt(2) - 1): S(8)/3,\n                sqrt(5 + 2*sqrt(5)): 10,\n                -sqrt(5 + 2*sqrt(5)): -10,\n                (2 + sqrt(3)): 12,\n                -(2 + sqrt(3)): -12,\n                (2 - sqrt(3)): S(12)/5,\n                -(2 - sqrt(3)): -S(12)/5,\n            }\n\n            if arg in cst_table:\n                return S.Pi / cst_table[arg]\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return -S.ImaginaryUnit * acoth(i_coeff)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        if n == 0:\n            return S.Pi / 2  # FIX THIS\n        elif n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n            return (-1)**((n + 1)//2) * x**n / n\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return arg\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        return self.args[0].is_real\n\n    def _eval_aseries(self, n, args0, x, logx):\n        if args0[0] == S.Infinity:\n            return (S.Pi/2 - acot(1/self.args[0]))._eval_nseries(x, n, logx)\n        elif args0[0] == S.NegativeInfinity:\n            return (3*S.Pi/2 - acot(1/self.args[0]))._eval_nseries(x, n, logx)\n        else:\n            return super(atan, self)._eval_aseries(n, args0, x, logx)\n\n    def _eval_rewrite_as_log(self, x):\n        return S.ImaginaryUnit/2 * (log(1 - S.ImaginaryUnit/x)\n            - log(1 + S.ImaginaryUnit/x))\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return cot\n\n    def _eval_rewrite_as_asin(self, arg):\n        return (arg*sqrt(1/arg**2)*\n                (S.Pi/2 - asin(sqrt(-arg**2)/sqrt(-arg**2 - 1))))\n\n    def _eval_rewrite_as_acos(self, arg):\n        return arg*sqrt(1/arg**2)*acos(sqrt(-arg**2)/sqrt(-arg**2 - 1))\n\n    def _eval_rewrite_as_atan(self, arg):\n        return atan(1/arg)\n\n    def _eval_rewrite_as_asec(self, arg):\n        return arg*sqrt(1/arg**2)*asec(sqrt((1 + arg**2)/arg**2))\n\n    def _eval_rewrite_as_acsc(self, arg):\n        return arg*sqrt(1/arg**2)*(S.Pi/2 - acsc(sqrt((1 + arg**2)/arg**2)))\n\n\nclass asec(InverseTrigonometricFunction):\n    r\"\"\"\n    The inverse secant function.\n\n    Returns the arc secant of x (measured in radians).\n\n    Notes\n    =====\n\n    ``asec(x)`` will evaluate automatically in the cases\n    ``oo``, ``-oo``, ``0``, ``1``, ``-1``.\n\n    ``asec(x)`` has branch cut in the interval [-1, 1]. For complex arguments,\n    it can be defined [4]_ as\n\n    .. math::\n        sec^{-1}(z) = -i*(log(\\sqrt{1 - z^2} + 1) / z)\n\n    At ``x = 0``, for positive branch cut, the limit evaluates to ``zoo``. For\n    negative branch cut, the limit\n\n    .. math::\n        \\lim_{z \\to 0}-i*(log(-\\sqrt{1 - z^2} + 1) / z)\n\n    simplifies to :math:`-i*log(z/2 + O(z^3))` which ultimately evaluates to\n    ``zoo``.\n\n    As ``asex(x)`` = ``asec(1/x)``, a similar argument can be given for\n    ``acos(x)``.\n\n    Examples\n    ========\n\n    >>> from sympy import asec, oo, pi\n    >>> asec(1)\n    0\n    >>> asec(-1)\n    pi\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan, cot\n    asin, acsc, acos, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Inverse_trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.23\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/ArcSec\n    .. [4] http://refrence.wolfram.com/language/ref/ArcSec.html\n    \"\"\"\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_zero:\n            return S.ComplexInfinity\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.One:\n                return S.Zero\n            elif arg is S.NegativeOne:\n                return S.Pi\n        if arg in [S.Infinity, S.NegativeInfinity, S.ComplexInfinity]:\n            return S.Pi/2\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return 1/(self.args[0]**2*sqrt(1 - 1/self.args[0]**2))\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return sec\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n        if Order(1,x).contains(arg):\n            return log(arg)\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        x = self.args[0]\n        if x.is_real is False:\n            return False\n        return (x - 1).is_nonnegative or (-x - 1).is_nonnegative\n\n    def _eval_rewrite_as_log(self, arg):\n        return S.Pi/2 + S.ImaginaryUnit*log(S.ImaginaryUnit/arg + sqrt(1 - 1/arg**2))\n\n    def _eval_rewrite_as_asin(self, arg):\n        return S.Pi/2 - asin(1/arg)\n\n    def _eval_rewrite_as_acos(self, arg):\n        return acos(1/arg)\n\n    def _eval_rewrite_as_atan(self, arg):\n        return sqrt(arg**2)/arg*(-S.Pi/2 + 2*atan(arg + sqrt(arg**2 - 1)))\n\n    def _eval_rewrite_as_acot(self, arg):\n        return sqrt(arg**2)/arg*(-S.Pi/2 + 2*acot(arg - sqrt(arg**2 - 1)))\n\n    def _eval_rewrite_as_acsc(self, arg):\n        return S.Pi/2 - acsc(arg)\n\n\nclass acsc(InverseTrigonometricFunction):\n    \"\"\"\n    The inverse cosecant function.\n\n    Returns the arc cosecant of x (measured in radians).\n\n    Notes\n    =====\n\n    acsc(x) will evaluate automatically in the cases\n    oo, -oo, 0, 1, -1.\n\n    Examples\n    ========\n\n    >>> from sympy import acsc, oo, pi\n    >>> acsc(1)\n    pi/2\n    >>> acsc(-1)\n    -pi/2\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan, cot\n    asin, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Inverse_trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.23\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/ArcCsc\n    \"\"\"\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.One:\n                return S.Pi/2\n            elif arg is S.NegativeOne:\n                return -S.Pi/2\n        if arg in [S.Infinity, S.NegativeInfinity, S.ComplexInfinity]:\n            return S.Zero\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return -1/(self.args[0]**2*sqrt(1 - 1/self.args[0]**2))\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return csc\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n        if Order(1,x).contains(arg):\n            return log(arg)\n        else:\n            return self.func(arg)\n\n    def _eval_rewrite_as_log(self, arg):\n        return -S.ImaginaryUnit*log(S.ImaginaryUnit/arg + sqrt(1 - 1/arg**2))\n\n    def _eval_rewrite_as_asin(self, arg):\n        return asin(1/arg)\n\n    def _eval_rewrite_as_acos(self, arg):\n        return S.Pi/2 - acos(1/arg)\n\n    def _eval_rewrite_as_atan(self, arg):\n        return sqrt(arg**2)/arg*(S.Pi/2 - atan(sqrt(arg**2 - 1)))\n\n    def _eval_rewrite_as_acot(self, arg):\n        return sqrt(arg**2)/arg*(S.Pi/2 - acot(1/sqrt(arg**2 - 1)))\n\n    def _eval_rewrite_as_asec(self, arg):\n        return S.Pi/2 - asec(arg)\n\n\nclass atan2(InverseTrigonometricFunction):\n    r\"\"\"\n    The function ``atan2(y, x)`` computes `\\operatorname{atan}(y/x)` taking\n    two arguments `y` and `x`.  Signs of both `y` and `x` are considered to\n    determine the appropriate quadrant of `\\operatorname{atan}(y/x)`.\n    The range is `(-\\pi, \\pi]`. The complete definition reads as follows:\n\n    .. math::\n\n        \\operatorname{atan2}(y, x) =\n        \\begin{cases}\n          \\arctan\\left(\\frac y x\\right) & \\qquad x > 0 \\\\\n          \\arctan\\left(\\frac y x\\right) + \\pi& \\qquad y \\ge 0 , x < 0 \\\\\n          \\arctan\\left(\\frac y x\\right) - \\pi& \\qquad y < 0 , x < 0 \\\\\n          +\\frac{\\pi}{2} & \\qquad y > 0 , x = 0 \\\\\n          -\\frac{\\pi}{2} & \\qquad y < 0 , x = 0 \\\\\n          \\text{undefined} & \\qquad y = 0, x = 0\n        \\end{cases}\n\n    Attention: Note the role reversal of both arguments. The `y`-coordinate\n    is the first argument and the `x`-coordinate the second.\n\n    Examples\n    ========\n\n    Going counter-clock wise around the origin we find the\n    following angles:\n\n    >>> from sympy import atan2\n    >>> atan2(0, 1)\n    0\n    >>> atan2(1, 1)\n    pi/4\n    >>> atan2(1, 0)\n    pi/2\n    >>> atan2(1, -1)\n    3*pi/4\n    >>> atan2(0, -1)\n    pi\n    >>> atan2(-1, -1)\n    -3*pi/4\n    >>> atan2(-1, 0)\n    -pi/2\n    >>> atan2(-1, 1)\n    -pi/4\n\n    which are all correct. Compare this to the results of the ordinary\n    `\\operatorname{atan}` function for the point `(x, y) = (-1, 1)`\n\n    >>> from sympy import atan, S\n(C) from sympy.sets.sets import FiniteSet\nfrom sympy.utilities.iterables import numbered_symbols\nfrom sympy.core.compatibility import range\n\n###############################################################################\n########################## TRIGONOMETRIC FUNCTIONS ############################\n###############################################################################\n\n\nclass TrigonometricFunction(Function):\n    \"\"\"Base class for trigonometric functions. \"\"\"\n\n    unbranched = True\n\n    def _eval_is_rational(self):\n        s = self.func(*self.args)\n        if s.func == self.func:\n            if s.args[0].is_rational and fuzzy_not(s.args[0].is_zero):\n                return False\n        else:\n            return s.is_rational\n\n    def _eval_is_algebraic(self):\n        s = self.func(*self.args)\n        if s.func == self.func:\n            if fuzzy_not(self.args[0].is_zero) and self.args[0].is_algebraic:\n                return False\n            pi_coeff = _pi_coeff(self.args[0])\n            if pi_coeff is not None and pi_coeff.is_rational:\n                return True\n        else:\n            return s.is_algebraic\n\n    def _eval_expand_complex(self, deep=True, **hints):\n        re_part, im_part = self.as_real_imag(deep=deep, **hints)\n        return re_part + im_part*S.ImaginaryUnit\n\n    def _as_real_imag(self, deep=True, **hints):\n        if self.args[0].is_real:\n            if deep:\n                hints['complex'] = False\n                return (self.args[0].expand(deep, **hints), S.Zero)\n            else:\n                return (self.args[0], S.Zero)\n        if deep:\n            re, im = self.args[0].expand(deep, **hints).as_real_imag()\n        else:\n            re, im = self.args[0].as_real_imag()\n        return (re, im)\n\n    def _period(self, general_period, symbol=None):\n        f = self.args[0]\n        if symbol is None:\n            symbol = tuple(f.free_symbols)[0]\n\n        if not f.has(symbol):\n            return S.Zero\n\n        if f == symbol:\n            return general_period\n\n        if symbol in f.free_symbols:\n            p, q = Wild('p'), Wild('q')\n            if f.is_Mul:\n                g, h = f.as_independent(symbol)\n                if h == symbol:\n                    return general_period/abs(g)\n\n            if f.is_Add:\n                a, h = f.as_independent(symbol)\n                g, h = h.as_independent(symbol, as_Add=False)\n                if h == symbol:\n                    return general_period/abs(g)\n\n        raise NotImplementedError(\"Use the periodicity function instead.\")\n\n\ndef _peeloff_pi(arg):\n    \"\"\"\n    Split ARG into two parts, a \"rest\" and a multiple of pi/2.\n    This assumes ARG to be an Add.\n    The multiple of pi returned in the second position is always a Rational.\n\n    Examples\n    ========\n\n    >>> from sympy.functions.elementary.trigonometric import _peeloff_pi as peel\n    >>> from sympy import pi\n    >>> from sympy.abc import x, y\n    >>> peel(x + pi/2)\n    (x, pi/2)\n    >>> peel(x + 2*pi/3 + pi*y)\n    (x + pi*y + pi/6, pi/2)\n    \"\"\"\n    for a in Add.make_args(arg):\n        if a is S.Pi:\n            K = S.One\n            break\n        elif a.is_Mul:\n            K, p = a.as_two_terms()\n            if p is S.Pi and K.is_Rational:\n                break\n    else:\n        return arg, S.Zero\n\n    m1 = (K % S.Half) * S.Pi\n    m2 = K*S.Pi - m1\n    return arg - m2, m2\n\n\ndef _pi_coeff(arg, cycles=1):\n    \"\"\"\n    When arg is a Number times pi (e.g. 3*pi/2) then return the Number\n    normalized to be in the range [0, 2], else None.\n\n    When an even multiple of pi is encountered, if it is multiplying\n    something with known parity then the multiple is returned as 0 otherwise\n    as 2.\n\n    Examples\n    ========\n\n    >>> from sympy.functions.elementary.trigonometric import _pi_coeff as coeff\n    >>> from sympy import pi, Dummy\n    >>> from sympy.abc import x, y\n    >>> coeff(3*x*pi)\n    3*x\n    >>> coeff(11*pi/7)\n    11/7\n    >>> coeff(-11*pi/7)\n    3/7\n    >>> coeff(4*pi)\n    0\n    >>> coeff(5*pi)\n    1\n    >>> coeff(5.0*pi)\n    1\n    >>> coeff(5.5*pi)\n    3/2\n    >>> coeff(2 + pi)\n\n    >>> coeff(2*Dummy(integer=True)*pi)\n    2\n    >>> coeff(2*Dummy(even=True)*pi)\n    0\n    \"\"\"\n    arg = sympify(arg)\n    if arg is S.Pi:\n        return S.One\n    elif not arg:\n        return S.Zero\n    elif arg.is_Mul:\n        cx = arg.coeff(S.Pi)\n        if cx:\n            c, x = cx.as_coeff_Mul()  # pi is not included as coeff\n            if c.is_Float:\n                # recast exact binary fractions to Rationals\n                f = abs(c) % 1\n                if f != 0:\n                    p = -int(round(log(f, 2).evalf()))\n                    m = 2**p\n                    cm = c*m\n                    i = int(cm)\n                    if i == cm:\n                        c = Rational(i, m)\n                        cx = c*x\n                else:\n                    c = Rational(int(c))\n                    cx = c*x\n            if x.is_integer:\n                c2 = c % 2\n                if c2 == 1:\n                    return x\n                elif not c2:\n                    if x.is_even is not None:  # known parity\n                        return S.Zero\n                    return S(2)\n                else:\n                    return c2*x\n            return cx\n\n\nclass sin(TrigonometricFunction):\n    \"\"\"\n    The sine function.\n\n    Returns the sine of x (measured in radians).\n\n    Notes\n    =====\n\n    This function will evaluate automatically in the\n    case x/pi is some rational number [4]_.  For example,\n    if x is a multiple of pi, pi/2, pi/3, pi/4 and pi/6.\n\n    Examples\n    ========\n\n    >>> from sympy import sin, pi\n    >>> from sympy.abc import x\n    >>> sin(x**2).diff(x)\n    2*x*cos(x**2)\n    >>> sin(1).diff(x)\n    0\n    >>> sin(pi)\n    0\n    >>> sin(pi/2)\n    1\n    >>> sin(pi/6)\n    1/2\n    >>> sin(pi/12)\n    -sqrt(2)/4 + sqrt(6)/4\n\n\n    See Also\n    ========\n\n    csc, cos, sec, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Sin\n    .. [4] http://mathworld.wolfram.com/TrigonometryAngles.html\n    \"\"\"\n\n    def period(self, symbol=None):\n        return self._period(2*pi, symbol)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return cos(self.args[0])\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @classmethod\n    def eval(cls, arg):\n        from sympy.calculus import AccumBounds\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Zero:\n                return S.Zero\n            elif arg is S.Infinity or arg is S.NegativeInfinity:\n                return AccumBounds(-1, 1)\n\n        if isinstance(arg, AccumBounds):\n            min, max = arg.min, arg.max\n            d = floor(min/(2*S.Pi))\n            if min is not S.NegativeInfinity:\n                min = min - d*2*S.Pi\n            if max is not S.Infinity:\n                max = max - d*2*S.Pi\n            if AccumBounds(min, max).intersection(FiniteSet(S.Pi/2, 5*S.Pi/2)) \\\n                    is not S.EmptySet and \\\n                    AccumBounds(min, max).intersection(FiniteSet(3*S.Pi/2,\n                        7*S.Pi/2)) is not S.EmptySet:\n                return AccumBounds(-1, 1)\n            elif AccumBounds(min, max).intersection(FiniteSet(S.Pi/2, 5*S.Pi/2)) \\\n                    is not S.EmptySet:\n                return AccumBounds(Min(sin(min), sin(max)), 1)\n            elif AccumBounds(min, max).intersection(FiniteSet(3*S.Pi/2, 8*S.Pi/2)) \\\n                        is not S.EmptySet:\n                return AccumBounds(-1, Max(sin(min), sin(max)))\n            else:\n                return AccumBounds(Min(sin(min), sin(max)),\n                                Max(sin(min), sin(max)))\n\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return S.ImaginaryUnit * sinh(i_coeff)\n\n        pi_coeff = _pi_coeff(arg)\n        if pi_coeff is not None:\n            if pi_coeff.is_integer:\n                return S.Zero\n\n            if (2*pi_coeff).is_integer:\n                if pi_coeff.is_even:\n                    return S.Zero\n                elif pi_coeff.is_even is False:\n                    return S.NegativeOne**(pi_coeff - S.Half)\n\n            if not pi_coeff.is_Rational:\n                narg = pi_coeff*S.Pi\n                if narg != arg:\n                    return cls(narg)\n                return None\n\n            # https://github.com/sympy/sympy/issues/6048\n            # transform a sine to a cosine, to avoid redundant code\n            if pi_coeff.is_Rational:\n                x = pi_coeff % 2\n                if x > 1:\n                    return -cls((x % 1)*S.Pi)\n                if 2*x > 1:\n                    return cls((1 - x)*S.Pi)\n                narg = ((pi_coeff + Rational(3, 2)) % 2)*S.Pi\n                result = cos(narg)\n                if not isinstance(result, cos):\n                    return result\n                if pi_coeff*S.Pi != arg:\n                    return cls(pi_coeff*S.Pi)\n                return None\n\n        if arg.is_Add:\n            x, m = _peeloff_pi(arg)\n            if m:\n                return sin(m)*cos(x) + cos(m)*sin(x)\n\n        if isinstance(arg, asin):\n            return arg.args[0]\n\n        if isinstance(arg, atan):\n            x = arg.args[0]\n            return x / sqrt(1 + x**2)\n\n        if isinstance(arg, atan2):\n            y, x = arg.args\n            return y / sqrt(x**2 + y**2)\n\n        if isinstance(arg, acos):\n            x = arg.args[0]\n            return sqrt(1 - x**2)\n\n        if isinstance(arg, acot):\n            x = arg.args[0]\n            return 1 / (sqrt(1 + 1 / x**2) * x)\n\n        if isinstance(arg, acsc):\n            x = arg.args[0]\n            return 1 / x\n\n        if isinstance(arg, asec):\n            x = arg.args[0]\n            return sqrt(1 - 1 / x**2)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        if n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n\n            if len(previous_terms) > 2:\n                p = previous_terms[-2]\n                return -p * x**2 / (n*(n - 1))\n            else:\n                return (-1)**(n//2) * x**(n)/factorial(n)\n\n    def _eval_rewrite_as_exp(self, arg):\n        I = S.ImaginaryUnit\n        if isinstance(arg, TrigonometricFunction) or isinstance(arg, HyperbolicFunction):\n            arg = arg.func(arg.args[0]).rewrite(exp)\n        return (exp(arg*I) - exp(-arg*I)) / (2*I)\n\n    def _eval_rewrite_as_Pow(self, arg):\n        if isinstance(arg, log):\n            I = S.ImaginaryUnit\n            x = arg.args[0]\n            return I*x**-I / 2 - I*x**I /2\n\n    def _eval_rewrite_as_cos(self, arg):\n        return cos(arg - S.Pi / 2, evaluate=False)\n\n    def _eval_rewrite_as_tan(self, arg):\n        tan_half = tan(S.Half*arg)\n        return 2*tan_half/(1 + tan_half**2)\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return sin(arg)*cos(arg)/cos(arg)\n\n    def _eval_rewrite_as_cot(self, arg):\n        cot_half = cot(S.Half*arg)\n        return 2*cot_half/(1 + cot_half**2)\n\n    def _eval_rewrite_as_pow(self, arg):\n        return self.rewrite(cos).rewrite(pow)\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        return self.rewrite(cos).rewrite(sqrt)\n\n    def _eval_rewrite_as_csc(self, arg):\n        return 1/csc(arg)\n\n    def _eval_rewrite_as_sec(self, arg):\n        return 1 / sec(arg - S.Pi / 2, evaluate=False)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        re, im = self._as_real_imag(deep=deep, **hints)\n        return (sin(re)*cosh(im), cos(re)*sinh(im))\n\n    def _eval_expand_trig(self, **hints):\n        from sympy import expand_mul\n        from sympy.functions.special.polynomials import chebyshevt, chebyshevu\n        arg = self.args[0]\n        x = None\n        if arg.is_Add:  # TODO, implement more if deep stuff here\n            # TODO: Do this more efficiently for more than two terms\n            x, y = arg.as_two_terms()\n            sx = sin(x, evaluate=False)._eval_expand_trig()\n            sy = sin(y, evaluate=False)._eval_expand_trig()\n            cx = cos(x, evaluate=False)._eval_expand_trig()\n            cy = cos(y, evaluate=False)._eval_expand_trig()\n            return sx*cy + sy*cx\n        else:\n            n, x = arg.as_coeff_Mul(rational=True)\n            if n.is_Integer:  # n will be positive because of .eval\n                # canonicalization\n\n                # See http://mathworld.wolfram.com/Multiple-AngleFormulas.html\n                if n.is_odd:\n                    return (-1)**((n - 1)/2)*chebyshevt(n, sin(x))\n                else:\n                    return expand_mul((-1)**(n/2 - 1)*cos(x)*chebyshevu(n -\n                        1, sin(x)), deep=False)\n            pi_coeff = _pi_coeff(arg)\n            if pi_coeff is not None:\n                if pi_coeff.is_Rational:\n                    return self.rewrite(sqrt)\n        return sin(arg)\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return arg\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        return self.args[0].is_real\n\n    def _eval_is_finite(self):\n        arg = self.args[0]\n        if arg.is_real:\n            return True\n\n\nclass cos(TrigonometricFunction):\n    \"\"\"\n    The cosine function.\n\n    Returns the cosine of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import cos, pi\n    >>> from sympy.abc import x\n    >>> cos(x**2).diff(x)\n    -2*x*sin(x**2)\n    >>> cos(1).diff(x)\n    0\n    >>> cos(pi)\n    -1\n    >>> cos(pi/2)\n    0\n    >>> cos(2*pi/3)\n    -1/2\n    >>> cos(pi/12)\n    sqrt(2)/4 + sqrt(6)/4\n\n    See Also\n    ========\n\n    sin, csc, sec, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Cos\n    \"\"\"\n\n    def period(self, symbol=None):\n        return self._period(2*pi, symbol)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return -sin(self.args[0])\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @classmethod\n    def eval(cls, arg):\n        from sympy.functions.special.polynomials import chebyshevt\n        from sympy.calculus.util import AccumBounds\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Zero:\n                return S.One\n            elif arg is S.Infinity or arg is S.NegativeInfinity:\n                # In this case it is better to return AccumBounds(-1, 1)\n                # rather than returning S.NaN, since AccumBounds(-1, 1)\n                # preserves the information that sin(oo) is between\n                # -1 and 1, where S.NaN does not do that.\n                return AccumBounds(-1, 1)\n\n        if isinstance(arg, AccumBounds):\n            return sin(arg + S.Pi/2)\n\n        if arg.could_extract_minus_sign():\n            return cls(-arg)\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return cosh(i_coeff)\n\n        pi_coeff = _pi_coeff(arg)\n        if pi_coeff is not None:\n            if pi_coeff.is_integer:\n                return (S.NegativeOne)**pi_coeff\n\n            if (2*pi_coeff).is_integer:\n                if pi_coeff.is_even:\n                    return (S.NegativeOne)**(pi_coeff/2)\n                elif pi_coeff.is_even is False:\n                    return S.Zero\n\n            if not pi_coeff.is_Rational:\n                narg = pi_coeff*S.Pi\n                if narg != arg:\n                    return cls(narg)\n                return None\n\n            # cosine formula #####################\n            # https://github.com/sympy/sympy/issues/6048\n            # explicit calculations are preformed for\n            # cos(k pi/n) for n = 8,10,12,15,20,24,30,40,60,120\n            # Some other exact values like cos(k pi/240) can be\n            # calculated using a partial-fraction decomposition\n            # by calling cos( X ).rewrite(sqrt)\n            cst_table_some = {\n                3: S.Half,\n                5: (sqrt(5) + 1)/4,\n            }\n            if pi_coeff.is_Rational:\n                q = pi_coeff.q\n                p = pi_coeff.p % (2*q)\n                if p > q:\n                    narg = (pi_coeff - 1)*S.Pi\n                    return -cls(narg)\n                if 2*p > q:\n                    narg = (1 - pi_coeff)*S.Pi\n                    return -cls(narg)\n\n                # If nested sqrt's are worse than un-evaluation\n                # you can require q to be in (1, 2, 3, 4, 6, 12)\n                # q <= 12, q=15, q=20, q=24, q=30, q=40, q=60, q=120 return\n                # expressions with 2 or fewer sqrt nestings.\n                table2 = {\n                    12: (3, 4),\n                    20: (4, 5),\n                    30: (5, 6),\n                    15: (6, 10),\n                    24: (6, 8),\n                    40: (8, 10),\n                    60: (20, 30),\n                    120: (40, 60)\n                    }\n                if q in table2:\n                    a, b = p*S.Pi/table2[q][0], p*S.Pi/table2[q][1]\n                    nvala, nvalb = cls(a), cls(b)\n                    if None == nvala or None == nvalb:\n                        return None\n                    return nvala*nvalb + cls(S.Pi/2 - a)*cls(S.Pi/2 - b)\n\n                if q > 12:\n                    return None\n\n                if q in cst_table_some:\n                    cts = cst_table_some[pi_coeff.q]\n                    return chebyshevt(pi_coeff.p, cts).expand()\n\n                if 0 == q % 2:\n                    narg = (pi_coeff*2)*S.Pi\n                    nval = cls(narg)\n                    if None == nval:\n                        return None\n                    x = (2*pi_coeff + 1)/2\n                    sign_cos = (-1)**((-1 if x < 0 else 1)*int(abs(x)))\n                    return sign_cos*sqrt( (1 + nval)/2 )\n            return None\n\n        if arg.is_Add:\n            x, m = _peeloff_pi(arg)\n            if m:\n                return cos(m)*cos(x) - sin(m)*sin(x)\n\n        if isinstance(arg, acos):\n            return arg.args[0]\n\n        if isinstance(arg, atan):\n            x = arg.args[0]\n            return 1 / sqrt(1 + x**2)\n\n        if isinstance(arg, atan2):\n            y, x = arg.args\n            return x / sqrt(x**2 + y**2)\n\n        if isinstance(arg, asin):\n            x = arg.args[0]\n            return sqrt(1 - x ** 2)\n\n        if isinstance(arg, acot):\n            x = arg.args[0]\n            return 1 / sqrt(1 + 1 / x**2)\n\n        if isinstance(arg, acsc):\n            x = arg.args[0]\n            return sqrt(1 - 1 / x**2)\n\n        if isinstance(arg, asec):\n            x = arg.args[0]\n            return 1 / x\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        if n < 0 or n % 2 == 1:\n            return S.Zero\n        else:\n            x = sympify(x)\n\n            if len(previous_terms) > 2:\n                p = previous_terms[-2]\n                return -p * x**2 / (n*(n - 1))\n            else:\n                return (-1)**(n//2)*x**(n)/factorial(n)\n\n    def _eval_rewrite_as_exp(self, arg):\n        I = S.ImaginaryUnit\n        if isinstance(arg, TrigonometricFunction) or isinstance(arg, HyperbolicFunction):\n            arg = arg.func(arg.args[0]).rewrite(exp)\n        return (exp(arg*I) + exp(-arg*I)) / 2\n\n    def _eval_rewrite_as_Pow(self, arg):\n        if isinstance(arg, log):\n            I = S.ImaginaryUnit\n            x = arg.args[0]\n            return x**I/2 + x**-I/2\n\n    def _eval_rewrite_as_sin(self, arg):\n        return sin(arg + S.Pi / 2, evaluate=False)\n\n    def _eval_rewrite_as_tan(self, arg):\n        tan_half = tan(S.Half*arg)**2\n        return (1 - tan_half)/(1 + tan_half)\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return sin(arg)*cos(arg)/sin(arg)\n\n    def _eval_rewrite_as_cot(self, arg):\n        cot_half = cot(S.Half*arg)**2\n        return (cot_half - 1)/(cot_half + 1)\n\n    def _eval_rewrite_as_pow(self, arg):\n        return self._eval_rewrite_as_sqrt(arg)\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        from sympy.functions.special.polynomials import chebyshevt\n\n        def migcdex(x):\n            # recursive calcuation of gcd and linear combination\n            # for a sequence of integers.\n            # Given  (x1, x2, x3)\n            # Returns (y1, y1, y3, g)\n            # such that g is the gcd and x1*y1+x2*y2+x3*y3 - g = 0\n            # Note, that this is only one such linear combination.\n            if len(x) == 1:\n                return (1, x[0])\n            if len(x) == 2:\n                return igcdex(x[0], x[-1])\n            g = migcdex(x[1:])\n            u, v, h = igcdex(x[0], g[-1])\n            return tuple([u] + [v*i for i in g[0:-1] ] + [h])\n\n        def ipartfrac(r, factors=None):\n            from sympy.ntheory import factorint\n            if isinstance(r, int):\n                return r\n            if not isinstance(r, Rational):\n                raise TypeError(\"r is not rational\")\n            n = r.q\n            if 2 > r.q*r.q:\n                return r.q\n\n            if None == factors:\n                a = [n//x**y for x, y in factorint(r.q).items()]\n            else:\n                a = [n//x for x in factors]\n            if len(a) == 1:\n                return [ r ]\n            h = migcdex(a)\n            ans = [ r.p*Rational(i*j, r.q) for i, j in zip(h[:-1], a) ]\n            assert r == sum(ans)\n            return ans\n        pi_coeff = _pi_coeff(arg)\n        if pi_coeff is None:\n            return None\n\n        if pi_coeff.is_integer:\n            # it was unevaluated\n            return self.func(pi_coeff*S.Pi)\n\n        if not pi_coeff.is_Rational:\n            return None\n\n        def _cospi257():\n            \"\"\" Express cos(pi/257) explicitly as a function of radicals\n                Based upon the equations in\n                http://math.stackexchange.com/questions/516142/how-does-cos2-pi-257-look-like-in-real-radicals\n                See also http://www.susqu.edu/brakke/constructions/257-gon.m.txt\n            \"\"\"\n            def f1(a, b):\n                return (a + sqrt(a**2 + b))/2, (a - sqrt(a**2 + b))/2\n\n            def f2(a, b):\n                return (a - sqrt(a**2 + b))/2\n\n            t1, t2 = f1(-1, 256)\n            z1, z3 = f1(t1, 64)\n            z2, z4 = f1(t2, 64)\n            y1, y5 = f1(z1, 4*(5 + t1 + 2*z1))\n            y6, y2 = f1(z2, 4*(5 + t2 + 2*z2))\n            y3, y7 = f1(z3, 4*(5 + t1 + 2*z3))\n            y8, y4 = f1(z4, 4*(5 + t2 + 2*z4))\n            x1, x9 = f1(y1, -4*(t1 + y1 + y3 + 2*y6))\n            x2, x10 = f1(y2, -4*(t2 + y2 + y4 + 2*y7))\n            x3, x11 = f1(y3, -4*(t1 + y3 + y5 + 2*y8))\n            x4, x12 = f1(y4, -4*(t2 + y4 + y6 + 2*y1))\n            x5, x13 = f1(y5, -4*(t1 + y5 + y7 + 2*y2))\n            x6, x14 = f1(y6, -4*(t2 + y6 + y8 + 2*y3))\n            x15, x7 = f1(y7, -4*(t1 + y7 + y1 + 2*y4))\n            x8, x16 = f1(y8, -4*(t2 + y8 + y2 + 2*y5))\n            v1 = f2(x1, -4*(x1 + x2 + x3 + x6))\n            v2 = f2(x2, -4*(x2 + x3 + x4 + x7))\n            v3 = f2(x8, -4*(x8 + x9 + x10 + x13))\n            v4 = f2(x9, -4*(x9 + x10 + x11 + x14))\n            v5 = f2(x10, -4*(x10 + x11 + x12 + x15))\n            v6 = f2(x16, -4*(x16 + x1 + x2 + x5))\n            u1 = -f2(-v1, -4*(v2 + v3))\n            u2 = -f2(-v4, -4*(v5 + v6))\n            w1 = -2*f2(-u1, -4*u2)\n            return sqrt(sqrt(2)*sqrt(w1 + 4)/8 + S.Half)\n\n        cst_table_some = {\n            3: S.Half,\n            5: (sqrt(5) + 1)/4,\n            17: sqrt((15 + sqrt(17))/32 + sqrt(2)*(sqrt(17 - sqrt(17)) +\n                sqrt(sqrt(2)*(-8*sqrt(17 + sqrt(17)) - (1 - sqrt(17))\n                *sqrt(17 - sqrt(17))) + 6*sqrt(17) + 34))/32),\n            257: _cospi257()\n            # 65537 is the only other known Fermat prime and the very\n            # large expression is intentionally omitted from SymPy; see\n            # http://www.susqu.edu/brakke/constructions/65537-gon.m.txt\n        }\n\n        def _fermatCoords(n):\n            # if n can be factored in terms of Fermat primes with\n            # multiplicity of each being 1, return those primes, else\n            # False\n            primes = []\n            for p_i in cst_table_some:\n                quotient, remainder = divmod(n, p_i)\n                if remainder == 0:\n                    n = quotient\n                    primes.append(p_i)\n                    if n == 1:\n                        return tuple(primes)\n            return False\n\n        if pi_coeff.q in cst_table_some:\n            rv = chebyshevt(pi_coeff.p, cst_table_some[pi_coeff.q])\n            if pi_coeff.q < 257:\n                rv = rv.expand()\n            return rv\n\n        if not pi_coeff.q % 2:  # recursively remove factors of 2\n            pico2 = pi_coeff*2\n            nval = cos(pico2*S.Pi).rewrite(sqrt)\n            x = (pico2 + 1)/2\n            sign_cos = -1 if int(x) % 2 else 1\n            return sign_cos*sqrt( (1 + nval)/2 )\n\n        FC = _fermatCoords(pi_coeff.q)\n        if FC:\n            decomp = ipartfrac(pi_coeff, FC)\n            X = [(x[1], x[0]*S.Pi) for x in zip(decomp, numbered_symbols('z'))]\n            pcls = cos(sum([x[0] for x in X]))._eval_expand_trig().subs(X)\n            return pcls.rewrite(sqrt)\n        else:\n            decomp = ipartfrac(pi_coeff)\n            X = [(x[1], x[0]*S.Pi) for x in zip(decomp, numbered_symbols('z'))]\n            pcls = cos(sum([x[0] for x in X]))._eval_expand_trig().subs(X)\n            return pcls\n\n    def _eval_rewrite_as_sec(self, arg):\n        return 1/sec(arg)\n\n    def _eval_rewrite_as_csc(self, arg):\n        return 1 / sec(arg)._eval_rewrite_as_csc(arg)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        re, im = self._as_real_imag(deep=deep, **hints)\n        return (cos(re)*cosh(im), -sin(re)*sinh(im))\n\n    def _eval_expand_trig(self, **hints):\n        from sympy.functions.special.polynomials import chebyshevt\n        arg = self.args[0]\n        x = None\n        if arg.is_Add:  # TODO: Do this more efficiently for more than two terms\n            x, y = arg.as_two_terms()\n            sx = sin(x, evaluate=False)._eval_expand_trig()\n            sy = sin(y, evaluate=False)._eval_expand_trig()\n            cx = cos(x, evaluate=False)._eval_expand_trig()\n            cy = cos(y, evaluate=False)._eval_expand_trig()\n            return cx*cy - sx*sy\n        else:\n            coeff, terms = arg.as_coeff_Mul(rational=True)\n            if coeff.is_Integer:\n                return chebyshevt(coeff, cos(terms))\n            pi_coeff = _pi_coeff(arg)\n            if pi_coeff is not None:\n                if pi_coeff.is_Rational:\n                    return self.rewrite(sqrt)\n        return cos(arg)\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return S.One\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        return self.args[0].is_real\n\n    def _eval_is_finite(self):\n        arg = self.args[0]\n\n        if arg.is_real:\n            return True\n\n\nclass tan(TrigonometricFunction):\n    \"\"\"\n    The tangent function.\n\n    Returns the tangent of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import tan, pi\n    >>> from sympy.abc import x\n    >>> tan(x**2).diff(x)\n    2*x*(tan(x**2)**2 + 1)\n    >>> tan(1).diff(x)\n    0\n    >>> tan(pi/8).expand()\n    -1 + sqrt(2)\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Tan\n    \"\"\"\n\n    def period(self, symbol=None):\n        return self._period(pi, symbol)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return S.One + self**2\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return atan\n\n    @classmethod\n    def eval(cls, arg):\n        from sympy.calculus.util import AccumBounds\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            elif arg is S.Zero:\n                return S.Zero\n            elif arg is S.Infinity or arg is S.NegativeInfinity:\n                return AccumBounds(S.NegativeInfinity, S.Infinity)\n\n        if isinstance(arg, AccumBounds):\n            min, max = arg.min, arg.max\n            d = floor(min/S.Pi)\n            if min is not S.NegativeInfinity:\n                min = min - d*S.Pi\n            if max is not S.Infinity:\n                max = max - d*S.Pi\n            if AccumBounds(min, max).intersection(FiniteSet(S.Pi/2, 3*S.Pi/2)):\n                return AccumBounds(S.NegativeInfinity, S.Infinity)\n            else:\n                return AccumBounds(tan(min), tan(max))\n\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return S.ImaginaryUnit * tanh(i_coeff)\n\n        pi_coeff = _pi_coeff(arg, 2)\n        if pi_coeff is not None:\n            if pi_coeff.is_integer:\n                return S.Zero\n\n            if not pi_coeff.is_Rational:\n                narg = pi_coeff*S.Pi\n                if narg != arg:\n                    return cls(narg)\n                return None\n\n            if pi_coeff.is_Rational:\n                if not pi_coeff.q % 2:\n                    narg = pi_coeff*S.Pi*2\n                    cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                    if not isinstance(cresult, cos) \\\n                            and not isinstance(sresult, cos):\n                        if sresult == 0:\n                            return S.ComplexInfinity\n                        return (1 - cresult)/sresult\n                table2 = {\n                    12: (3, 4),\n                    20: (4, 5),\n                    30: (5, 6),\n                    15: (6, 10),\n                    24: (6, 8),\n                    40: (8, 10),\n                    60: (20, 30),\n                    120: (40, 60)\n                    }\n                q = pi_coeff.q\n                p = pi_coeff.p % q\n                if q in table2:\n                    nvala, nvalb = cls(p*S.Pi/table2[q][0]), cls(p*S.Pi/table2[q][1])\n                    if None == nvala or None == nvalb:\n                        return None\n                    return (nvala - nvalb)/(1 + nvala*nvalb)\n                narg = ((pi_coeff + S.Half) % 1 - S.Half)*S.Pi\n                # see cos() to specify which expressions should  be\n                # expanded automatically in terms of radicals\n                cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                if not isinstance(cresult, cos) \\\n                        and not isinstance(sresult, cos):\n                    if cresult == 0:\n                        return S.ComplexInfinity\n                    return (sresult/cresult)\n                if narg != arg:\n                    return cls(narg)\n\n        if arg.is_Add:\n            x, m = _peeloff_pi(arg)\n            if m:\n                tanm = tan(m)\n                if tanm is S.ComplexInfinity:\n                    return -cot(x)\n                else: # tanm == 0\n                    return tan(x)\n\n        if isinstance(arg, atan):\n            return arg.args[0]\n\n        if isinstance(arg, atan2):\n            y, x = arg.args\n            return y/x\n\n        if isinstance(arg, asin):\n            x = arg.args[0]\n            return x / sqrt(1 - x**2)\n\n        if isinstance(arg, acos):\n            x = arg.args[0]\n            return sqrt(1 - x**2) / x\n\n        if isinstance(arg, acot):\n            x = arg.args[0]\n            return 1 / x\n\n        if isinstance(arg, acsc):\n            x = arg.args[0]\n            return 1 / (sqrt(1 - 1 / x**2) * x)\n\n        if isinstance(arg, asec):\n            x = arg.args[0]\n            return sqrt(1 - 1 / x**2) * x\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        from sympy import bernoulli\n        if n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n\n            a, b = ((n - 1)//2), 2**(n + 1)\n\n            B = bernoulli(n + 1)\n            F = factorial(n + 1)\n\n            return (-1)**a * b*(b - 1) * B/F * x**n\n\n    def _eval_nseries(self, x, n, logx):\n        i = self.args[0].limit(x, 0)*2/S.Pi\n        if i and i.is_Integer:\n            return self.rewrite(cos)._eval_nseries(x, n=n, logx=logx)\n        return Function._eval_nseries(self, x, n=n, logx=logx)\n\n    def _eval_rewrite_as_Pow(self, arg):\n        if isinstance(arg, log):\n            I = S.ImaginaryUnit\n            x = arg.args[0]\n            return I*(x**-I - x**I)/(x**-I + x**I)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        re, im = self._as_real_imag(deep=deep, **hints)\n        if im:\n            denom = cos(2*re) + cosh(2*im)\n            return (sin(2*re)/denom, sinh(2*im)/denom)\n        else:\n            return (self.func(re), S.Zero)\n\n    def _eval_expand_trig(self, **hints):\n        from sympy import im, re\n        arg = self.args[0]\n        x = None\n        if arg.is_Add:\n            from sympy import symmetric_poly\n            n = len(arg.args)\n            TX = []\n            for x in arg.args:\n                tx = tan(x, evaluate=False)._eval_expand_trig()\n                TX.append(tx)\n\n            Yg = numbered_symbols('Y')\n            Y = [ next(Yg) for i in range(n) ]\n\n            p = [0, 0]\n            for i in range(n + 1):\n                p[1 - i % 2] += symmetric_poly(i, Y)*(-1)**((i % 4)//2)\n            return (p[0]/p[1]).subs(list(zip(Y, TX)))\n\n        else:\n            coeff, terms = arg.as_coeff_Mul(rational=True)\n            if coeff.is_Integer and coeff > 1:\n                I = S.ImaginaryUnit\n                z = Symbol('dummy', real=True)\n                P = ((1 + I*z)**coeff).expand()\n                return (im(P)/re(P)).subs([(z, tan(terms))])\n        return tan(arg)\n\n    def _eval_rewrite_as_exp(self, arg):\n        I = S.ImaginaryUnit\n        if isinstance(arg, TrigonometricFunction) or isinstance(arg, HyperbolicFunction):\n            arg = arg.func(arg.args[0]).rewrite(exp)\n        neg_exp, pos_exp = exp(-arg*I), exp(arg*I)\n        return I*(neg_exp - pos_exp)/(neg_exp + pos_exp)\n\n    def _eval_rewrite_as_sin(self, x):\n        return 2*sin(x)**2/sin(2*x)\n\n    def _eval_rewrite_as_cos(self, x):\n        return cos(x - S.Pi / 2, evaluate=False) / cos(x)\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return sin(arg)/cos(arg)\n\n    def _eval_rewrite_as_cot(self, arg):\n        return 1/cot(arg)\n\n    def _eval_rewrite_as_sec(self, arg):\n        sin_in_sec_form = sin(arg)._eval_rewrite_as_sec(arg)\n        cos_in_sec_form = cos(arg)._eval_rewrite_as_sec(arg)\n        return sin_in_sec_form / cos_in_sec_form\n\n    def _eval_rewrite_as_csc(self, arg):\n        sin_in_csc_form = sin(arg)._eval_rewrite_as_csc(arg)\n        cos_in_csc_form = cos(arg)._eval_rewrite_as_csc(arg)\n        return sin_in_csc_form / cos_in_csc_form\n\n    def _eval_rewrite_as_pow(self, arg):\n        y = self.rewrite(cos).rewrite(pow)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        y = self.rewrite(cos).rewrite(sqrt)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return arg\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        return self.args[0].is_real\n\n    def _eval_is_finite(self):\n        arg = self.args[0]\n\n        if arg.is_imaginary:\n            return True\n\n\nclass cot(TrigonometricFunction):\n    \"\"\"\n    The cotangent function.\n\n    Returns the cotangent of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import cot, pi\n    >>> from sympy.abc import x\n    >>> cot(x**2).diff(x)\n    2*x*(-cot(x**2)**2 - 1)\n    >>> cot(1).diff(x)\n    0\n    >>> cot(pi/12)\n    sqrt(3) + 2\n\n    See Also\n    ========\n\n    sin, csc, cos, sec, tan\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Cot\n    \"\"\"\n\n    def period(self, symbol=None):\n        return self._period(pi, symbol)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return S.NegativeOne - self**2\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    def inverse(self, argindex=1):\n        \"\"\"\n        Returns the inverse of this function.\n        \"\"\"\n        return acot\n\n    @classmethod\n    def eval(cls, arg):\n        from sympy.calculus.util import AccumBounds\n        if arg.is_Number:\n            if arg is S.NaN:\n                return S.NaN\n            if arg is S.Zero:\n                return S.ComplexInfinity\n\n        if isinstance(arg, AccumBounds):\n            return -tan(arg + S.Pi/2)\n\n        if arg.could_extract_minus_sign():\n            return -cls(-arg)\n\n        i_coeff = arg.as_coefficient(S.ImaginaryUnit)\n        if i_coeff is not None:\n            return -S.ImaginaryUnit * coth(i_coeff)\n\n        pi_coeff = _pi_coeff(arg, 2)\n        if pi_coeff is not None:\n            if pi_coeff.is_integer:\n                return S.ComplexInfinity\n\n            if not pi_coeff.is_Rational:\n                narg = pi_coeff*S.Pi\n                if narg != arg:\n                    return cls(narg)\n                return None\n\n            if pi_coeff.is_Rational:\n                if pi_coeff.q > 2 and not pi_coeff.q % 2:\n                    narg = pi_coeff*S.Pi*2\n                    cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                    if not isinstance(cresult, cos) \\\n                            and not isinstance(sresult, cos):\n                        return (1 + cresult)/sresult\n                table2 = {\n                    12: (3, 4),\n                    20: (4, 5),\n                    30: (5, 6),\n                    15: (6, 10),\n                    24: (6, 8),\n                    40: (8, 10),\n                    60: (20, 30),\n                    120: (40, 60)\n                    }\n                q = pi_coeff.q\n                p = pi_coeff.p % q\n                if q in table2:\n                    nvala, nvalb = cls(p*S.Pi/table2[q][0]), cls(p*S.Pi/table2[q][1])\n                    if None == nvala or None == nvalb:\n                        return None\n                    return (1 + nvala*nvalb)/(nvalb - nvala)\n                narg = (((pi_coeff + S.Half) % 1) - S.Half)*S.Pi\n                # see cos() to specify which expressions should be\n                # expanded automatically in terms of radicals\n                cresult, sresult = cos(narg), cos(narg - S.Pi/2)\n                if not isinstance(cresult, cos) \\\n                        and not isinstance(sresult, cos):\n                    if sresult == 0:\n                        return S.ComplexInfinity\n                    return cresult / sresult\n                if narg != arg:\n                    return cls(narg)\n\n        if arg.is_Add:\n            x, m = _peeloff_pi(arg)\n            if m:\n                cotm = cot(m)\n                if cotm is S.ComplexInfinity:\n                    return cot(x)\n                else: # cotm == 0\n                    return -tan(x)\n\n        if isinstance(arg, acot):\n            return arg.args[0]\n\n        if isinstance(arg, atan):\n            x = arg.args[0]\n            return 1 / x\n\n        if isinstance(arg, atan2):\n            y, x = arg.args\n            return x/y\n\n        if isinstance(arg, asin):\n            x = arg.args[0]\n            return sqrt(1 - x**2) / x\n\n        if isinstance(arg, acos):\n            x = arg.args[0]\n            return x / sqrt(1 - x**2)\n\n        if isinstance(arg, acsc):\n            x = arg.args[0]\n            return sqrt(1 - 1 / x**2) * x\n\n        if isinstance(arg, asec):\n            x = arg.args[0]\n            return 1 / (sqrt(1 - 1 / x**2) * x)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        from sympy import bernoulli\n        if n == 0:\n            return 1 / sympify(x)\n        elif n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n\n            B = bernoulli(n + 1)\n            F = factorial(n + 1)\n\n            return (-1)**((n + 1)//2) * 2**(n + 1) * B/F * x**n\n\n    def _eval_nseries(self, x, n, logx):\n        i = self.args[0].limit(x, 0)/S.Pi\n        if i and i.is_Integer:\n            return self.rewrite(cos)._eval_nseries(x, n=n, logx=logx)\n        return self.rewrite(tan)._eval_nseries(x, n=n, logx=logx)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        re, im = self._as_real_imag(deep=deep, **hints)\n        if im:\n            denom = cos(2*re) - cosh(2*im)\n            return (-sin(2*re)/denom, -sinh(2*im)/denom)\n        else:\n            return (self.func(re), S.Zero)\n\n    def _eval_rewrite_as_exp(self, arg):\n        I = S.ImaginaryUnit\n        if isinstance(arg, TrigonometricFunction) or isinstance(arg, HyperbolicFunction):\n            arg = arg.func(arg.args[0]).rewrite(exp)\n        neg_exp, pos_exp = exp(-arg*I), exp(arg*I)\n        return I*(pos_exp + neg_exp)/(pos_exp - neg_exp)\n\n    def _eval_rewrite_as_Pow(self, arg):\n        if isinstance(arg, log):\n            I = S.ImaginaryUnit\n            x = arg.args[0]\n            return -I*(x**-I + x**I)/(x**-I - x**I)\n\n    def _eval_rewrite_as_sin(self, x):\n        return 2*sin(2*x)/sin(x)**2\n\n    def _eval_rewrite_as_cos(self, x):\n        return cos(x) / cos(x - S.Pi / 2, evaluate=False)\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return cos(arg)/sin(arg)\n\n    def _eval_rewrite_as_tan(self, arg):\n        return 1/tan(arg)\n\n    def _eval_rewrite_as_sec(self, arg):\n        cos_in_sec_form = cos(arg)._eval_rewrite_as_sec(arg)\n        sin_in_sec_form = sin(arg)._eval_rewrite_as_sec(arg)\n        return cos_in_sec_form / sin_in_sec_form\n\n    def _eval_rewrite_as_csc(self, arg):\n        cos_in_csc_form = cos(arg)._eval_rewrite_as_csc(arg)\n        sin_in_csc_form = sin(arg)._eval_rewrite_as_csc(arg)\n        return cos_in_csc_form / sin_in_csc_form\n\n    def _eval_rewrite_as_pow(self, arg):\n        y = self.rewrite(cos).rewrite(pow)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        y = self.rewrite(cos).rewrite(sqrt)\n        if y.has(cos):\n            return None\n        return y\n\n    def _eval_as_leading_term(self, x):\n        from sympy import Order\n        arg = self.args[0].as_leading_term(x)\n\n        if x in arg.free_symbols and Order(1, x).contains(arg):\n            return 1/arg\n        else:\n            return self.func(arg)\n\n    def _eval_is_real(self):\n        return self.args[0].is_real\n\n    def _eval_expand_trig(self, **hints):\n        from sympy import im, re\n        arg = self.args[0]\n        x = None\n        if arg.is_Add:\n            from sympy import symmetric_poly\n            n = len(arg.args)\n            CX = []\n            for x in arg.args:\n                cx = cot(x, evaluate=False)._eval_expand_trig()\n                CX.append(cx)\n\n            Yg = numbered_symbols('Y')\n            Y = [ next(Yg) for i in range(n) ]\n\n            p = [0, 0]\n            for i in range(n, -1, -1):\n                p[(n - i) % 2] += symmetric_poly(i, Y)*(-1)**(((n - i) % 4)//2)\n            return (p[0]/p[1]).subs(list(zip(Y, CX)))\n        else:\n            coeff, terms = arg.as_coeff_Mul(rational=True)\n            if coeff.is_Integer and coeff > 1:\n                I = S.ImaginaryUnit\n                z = Symbol('dummy', real=True)\n                P = ((z + I)**coeff).expand()\n                return (re(P)/im(P)).subs([(z, cot(terms))])\n        return cot(arg)\n\n    def _eval_is_finite(self):\n        arg = self.args[0]\n        if arg.is_imaginary:\n            return True\n\n    def _eval_subs(self, old, new):\n        if self == old:\n            return new\n        arg = self.args[0]\n        argnew = arg.subs(old, new)\n        if arg != argnew and (argnew/S.Pi).is_integer:\n            return S.ComplexInfinity\n        return cot(argnew)\n\n\nclass ReciprocalTrigonometricFunction(TrigonometricFunction):\n    \"\"\"Base class for reciprocal functions of trigonometric functions. \"\"\"\n\n    _reciprocal_of = None       # mandatory, to be defined in subclass\n\n    # _is_even and _is_odd are used for correct evaluation of csc(-x), sec(-x)\n    # TODO refactor into TrigonometricFunction common parts of\n    # trigonometric functions eval() like even/odd, func(x+2*k*pi), etc.\n    _is_even = None  # optional, to be defined in subclass\n    _is_odd = None   # optional, to be defined in subclass\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.could_extract_minus_sign():\n            if cls._is_even:\n                return cls(-arg)\n            if cls._is_odd:\n                return -cls(-arg)\n\n        pi_coeff = _pi_coeff(arg)\n        if (pi_coeff is not None\n            and not (2*pi_coeff).is_integer\n            and pi_coeff.is_Rational):\n                q = pi_coeff.q\n                p = pi_coeff.p % (2*q)\n                if p > q:\n                    narg = (pi_coeff - 1)*S.Pi\n                    return -cls(narg)\n                if 2*p > q:\n                    narg = (1 - pi_coeff)*S.Pi\n                    if cls._is_odd:\n                        return cls(narg)\n                    elif cls._is_even:\n                        return -cls(narg)\n\n        t = cls._reciprocal_of.eval(arg)\n        if hasattr(arg, 'inverse') and arg.inverse() == cls:\n            return arg.args[0]\n        return 1/t if t != None else t\n\n    def _call_reciprocal(self, method_name, *args, **kwargs):\n        # Calls method_name on _reciprocal_of\n        o = self._reciprocal_of(self.args[0])\n        return getattr(o, method_name)(*args, **kwargs)\n\n    def _calculate_reciprocal(self, method_name, *args, **kwargs):\n        # If calling method_name on _reciprocal_of returns a value != None\n        # then return the reciprocal of that value\n        t = self._call_reciprocal(method_name, *args, **kwargs)\n        return 1/t if t != None else t\n\n    def _rewrite_reciprocal(self, method_name, arg):\n        # Special handling for rewrite functions. If reciprocal rewrite returns\n        # unmodified expression, then return None\n        t = self._call_reciprocal(method_name, arg)\n        if t != None and t != self._reciprocal_of(arg):\n            return 1/t\n\n    def _period(self, symbol):\n        f = self.args[0]\n        return self._reciprocal_of(f).period(symbol)\n\n    def fdiff(self, argindex=1):\n        return -self._calculate_reciprocal(\"fdiff\", argindex)/self**2\n\n    def _eval_rewrite_as_exp(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_exp\", arg)\n\n    def _eval_rewrite_as_Pow(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_Pow\", arg)\n\n    def _eval_rewrite_as_sin(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_sin\", arg)\n\n    def _eval_rewrite_as_cos(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_cos\", arg)\n\n    def _eval_rewrite_as_tan(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_tan\", arg)\n\n    def _eval_rewrite_as_pow(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_pow\", arg)\n\n    def _eval_rewrite_as_sqrt(self, arg):\n        return self._rewrite_reciprocal(\"_eval_rewrite_as_sqrt\", arg)\n\n    def _eval_conjugate(self):\n        return self.func(self.args[0].conjugate())\n\n    def as_real_imag(self, deep=True, **hints):\n        return (1/self._reciprocal_of(self.args[0])).as_real_imag(deep,\n                                                                  **hints)\n\n    def _eval_expand_trig(self, **hints):\n        return self._calculate_reciprocal(\"_eval_expand_trig\", **hints)\n\n    def _eval_is_real(self):\n        return self._reciprocal_of(self.args[0])._eval_is_real()\n\n    def _eval_as_leading_term(self, x):\n        return (1/self._reciprocal_of(self.args[0]))._eval_as_leading_term(x)\n\n    def _eval_is_finite(self):\n        return (1/self._reciprocal_of(self.args[0])).is_finite\n\n    def _eval_nseries(self, x, n, logx):\n        return (1/self._reciprocal_of(self.args[0]))._eval_nseries(x, n, logx)\n\n\nclass sec(ReciprocalTrigonometricFunction):\n    \"\"\"\n    The secant function.\n\n    Returns the secant of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import sec\n    >>> from sympy.abc import x\n    >>> sec(x**2).diff(x)\n    2*x*tan(x**2)*sec(x**2)\n    >>> sec(1).diff(x)\n    0\n\n    See Also\n    ========\n\n    sin, csc, cos, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Sec\n    \"\"\"\n\n    _reciprocal_of = cos\n    _is_even = True\n\n    def period(self, symbol=None):\n        return self._period(symbol)\n\n    def _eval_rewrite_as_cot(self, arg):\n        cot_half_sq = cot(arg/2)**2\n        return (cot_half_sq + 1)/(cot_half_sq - 1)\n\n    def _eval_rewrite_as_cos(self, arg):\n        return (1/cos(arg))\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return sin(arg)/(cos(arg)*sin(arg))\n\n    def _eval_rewrite_as_sin(self, arg):\n        return (1 / cos(arg)._eval_rewrite_as_sin(arg))\n\n    def _eval_rewrite_as_tan(self, arg):\n        return (1 / cos(arg)._eval_rewrite_as_tan(arg))\n\n    def _eval_rewrite_as_csc(self, arg):\n        return csc(pi / 2 - arg, evaluate=False)\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return tan(self.args[0])*sec(self.args[0])\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        # Reference Formula:\n        # http://functions.wolfram.com/ElementaryFunctions/Sec/06/01/02/01/\n        from sympy.functions.combinatorial.numbers import euler\n        if n < 0 or n % 2 == 1:\n            return S.Zero\n        else:\n            x = sympify(x)\n            k = n//2\n            return (-1)**k*euler(2*k)/factorial(2*k)*x**(2*k)\n\n\nclass csc(ReciprocalTrigonometricFunction):\n    \"\"\"\n    The cosecant function.\n\n    Returns the cosecant of x (measured in radians).\n\n    Notes\n    =====\n\n    See :func:`sin` for notes about automatic evaluation.\n\n    Examples\n    ========\n\n    >>> from sympy import csc\n    >>> from sympy.abc import x\n    >>> csc(x**2).diff(x)\n    -2*x*cot(x**2)*csc(x**2)\n    >>> csc(1).diff(x)\n    0\n\n    See Also\n    ========\n\n    sin, cos, sec, tan, cot\n    asin, acsc, acos, asec, atan, acot, atan2\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Trigonometric_functions\n    .. [2] http://dlmf.nist.gov/4.14\n    .. [3] http://functions.wolfram.com/ElementaryFunctions/Csc\n    \"\"\"\n\n    _reciprocal_of = sin\n    _is_odd = True\n\n    def period(self, symbol=None):\n        return self._period(symbol)\n\n    def _eval_rewrite_as_sin(self, arg):\n        return (1/sin(arg))\n\n    def _eval_rewrite_as_sincos(self, arg):\n        return cos(arg)/(sin(arg)*cos(arg))\n\n    def _eval_rewrite_as_cot(self, arg):\n        cot_half = cot(arg/2)\n        return (1 + cot_half**2)/(2*cot_half)\n\n    def _eval_rewrite_as_cos(self, arg):\n        return (1 / sin(arg)._eval_rewrite_as_cos(arg))\n\n    def _eval_rewrite_as_sec(self, arg):\n        return sec(pi / 2 - arg, evaluate=False)\n\n    def _eval_rewrite_as_tan(self, arg):\n        return (1 / sin(arg)._eval_rewrite_as_tan(arg))\n\n    def fdiff(self, argindex=1):\n        if argindex == 1:\n            return -cot(self.args[0])*csc(self.args[0])\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @staticmethod\n    @cacheit\n    def taylor_term(n, x, *previous_terms):\n        from sympy import bernoulli\n        if n == 0:\n            return 1/sympify(x)\n        elif n < 0 or n % 2 == 0:\n            return S.Zero\n        else:\n            x = sympify(x)\n            k = n//2 + 1\n            return ((-1)**(k - 1)*2*(2**(2*k - 1) - 1)*\n                    bernoulli(2*k)*x**(2*k - 1)/factorial(2*k))\n\n\nclass sinc(TrigonometricFunction):\n    r\"\"\"Represents unnormalized sinc function\n\n    Examples\n    ========\n\n    >>> from sympy import sinc, oo, jn, Product, Symbol\n    >>> from sympy.abc import x\n    >>> sinc(x)\n    sinc(x)\n\n    * Automated Evaluation\n\n    >>> sinc(0)\n    1\n    >>> sinc(oo)\n    0\n\n    * Differentiation\n\n    >>> sinc(x).diff()\n    (x*cos(x) - sin(x))/x**2\n\n    * Series Expansion\n\n    >>> sinc(x).series()\n    1 - x**2/6 + x**4/120 + O(x**6)\n\n    * As zero'th order spherical Bessel Function\n\n    >>> sinc(x).rewrite(jn)\n    jn(0, x)\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Sinc_function\n\n    \"\"\"\n\n    def fdiff(self, argindex=1):\n        x = self.args[0]\n        if argindex == 1:\n            return (x*cos(x) - sin(x)) / x**2\n        else:\n            raise ArgumentIndexError(self, argindex)\n\n    @classmethod\n    def eval(cls, arg):\n        if arg.is_zero:\n            return S.One\n        if arg.is_Number:\n            if arg in [S.Infinity, -S.Infinity]:\n                return S.Zero\n            elif arg is S.NaN:\n                return S.NaN\n\n        if arg is S.ComplexInfinity:\n            return S.NaN\n\n        if arg.could_extract_minus_sign():", "answer": "C"}
{"uuid": "de83f1d3-a030-4f8e-bced-cc950c9bc853", "issue_statement": "LaTeX printer inconsistent with pretty printer\nThe LaTeX printer should always give the same output as the pretty printer, unless better output is possible from LaTeX. In some cases it is inconsistent. For instance:\n\n``` py\nIn [9]: var('x', positive=True)\nOut[9]: x\n\nIn [10]: latex(exp(-x)*log(x))\nOut[10]: '\\\\frac{1}{e^{x}} \\\\log{\\\\left (x \\\\right )}'\n\nIn [11]: pprint(exp(-x)*log(x))\n -x\n\u212f  \u22c5log(x)\n```\n\n(I also don't think the assumptions should affect printing). \n\n``` py\nIn [14]: var('x y')\nOut[14]: (x, y)\n\nIn [15]: latex(1/(x + y)/2)\nOut[15]: '\\\\frac{1}{2 x + 2 y}'\n\nIn [16]: pprint(1/(x + y)/2)\n    1\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n2\u22c5(x + y)\n```", "choices": "(A)             any([expr.has(x) for x in (Integral, Piecewise, Product, Sum)])):\n            return True\n\n        return False\n\n\n    def _needs_add_brackets(self, expr):\n        \"\"\"\n        Returns True if the expression needs to be wrapped in brackets when\n        printed as part of an Add, False otherwise.  This is False for most\n        things.\n        \"\"\"\n(B)             return True\n        elif expr.is_Relational:\n            return True\n        elif expr.is_Mul:\n            if not first and _coeff_isneg(expr):\n                return True\n        if any([expr.has(x) for x in (Mod,)]):\n            return True\n        if (not last and\n            any([expr.has(x) for x in (Integral, Piecewise, Product, Sum)])):\n            return True\n\n(C)         elif expr.is_Mul:\n            if not first and _coeff_isneg(expr):\n                return True\n        if any([expr.has(x) for x in (Mod,)]):\n            return True\n        if (not last and\n            any([expr.has(x) for x in (Integral, Piecewise, Product, Sum)])):\n            return True\n\n        return False\n\n\n(D)         if any([expr.has(x) for x in (Mod,)]):\n            return True\n        if (not last and\n            any([expr.has(x) for x in (Integral, Piecewise, Product, Sum)])):\n            return True\n\n        return False\n\n\n    def _needs_add_brackets(self, expr):\n        \"\"\"\n        Returns True if the expression needs to be wrapped in brackets when", "answer": "C"}
{"uuid": "c45bdec7-2bc1-4e8a-a374-61abfd99b2f0", "issue_statement": "matematica code printer does not handle floats and derivatives correctly\nIn its current state the mathematica code printer does not handle Derivative(func(vars), deriver) \r\ne.g. Derivative(f(t), t) yields Derivative(f(t), t) instead of D[f[t],t]\r\n\r\nAlso floats with exponents are not handled correctly e.g. 1.0e-4 is not converted to 1.0*^-4\r\n\r\nThis has an easy fix by adding the following lines to MCodePrinter:\r\n\r\n\r\ndef _print_Derivative(self, expr):\r\n        return \"D[%s]\" % (self.stringify(expr.args, \", \"))\r\n\r\ndef _print_Float(self, expr):\r\n        res =str(expr)\r\n        return res.replace('e','*^')", "choices": "(A)     r\"\"\"Converts an expr to a string of the Wolfram Mathematica code\n\n    Examples\n    ========\n\n    >>> from sympy import mathematica_code as mcode, symbols, sin\n    >>> x = symbols('x')\n    >>> mcode(sin(x).series(x).removeO())\n    '(1/120)*x^5 - 1/6*x^3 + x'\n(B)             args = expr.args\n        return \"Hold[Integrate[\" + ', '.join(self.doprint(a) for a in args) + \"]]\"\n\n    def _print_Sum(self, expr):\n        return \"Hold[Sum[\" + ', '.join(self.doprint(a) for a in expr.args) + \"]]\"\n\n\ndef mathematica_code(expr, **settings):\n    r\"\"\"Converts an expr to a string of the Wolfram Mathematica code\n(C) \ndef mathematica_code(expr, **settings):\n    r\"\"\"Converts an expr to a string of the Wolfram Mathematica code\n\n    Examples\n    ========\n\n    >>> from sympy import mathematica_code as mcode, symbols, sin\n    >>> x = symbols('x')\n(D)     def _print_Sum(self, expr):\n        return \"Hold[Sum[\" + ', '.join(self.doprint(a) for a in expr.args) + \"]]\"\n\n\ndef mathematica_code(expr, **settings):\n    r\"\"\"Converts an expr to a string of the Wolfram Mathematica code\n\n    Examples\n    ========", "answer": "D"}
{"uuid": "6aa2d66c-0c47-412c-85e5-ef21671bd5cf", "issue_statement": "Wrong result with apart\n```\r\nPython 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00) \r\nType \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\nIPython 5.1.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython's features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python's own help system.\r\nobject?   -> Details about 'object', use 'object??' for extra details.\r\n\r\nIn [1]: from sympy import symbols\r\n\r\nIn [2]: a = symbols('a', real=True)\r\n\r\nIn [3]: t = symbols('t', real=True, negative=False)\r\n\r\nIn [4]: bug = a * (-t + (-t + 1) * (2 * t - 1)) / (2 * t - 1)\r\n\r\nIn [5]: bug.subs(a, 1)\r\nOut[5]: (-t + (-t + 1)*(2*t - 1))/(2*t - 1)\r\n\r\nIn [6]: bug.subs(a, 1).apart()\r\nOut[6]: -t + 1/2 - 1/(2*(2*t - 1))\r\n\r\nIn [7]: bug.subs(a, 1).apart(t)\r\nOut[7]: -t + 1/2 - 1/(2*(2*t - 1))\r\n\r\nIn [8]: bug.apart(t)\r\nOut[8]: -a*t\r\n\r\nIn [9]: import sympy; sympy.__version__\r\nOut[9]: '1.0'\r\n```\nWrong result with apart\n```\r\nPython 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00) \r\nType \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\nIPython 5.1.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython's features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python's own help system.\r\nobject?   -> Details about 'object', use 'object??' for extra details.\r\n\r\nIn [1]: from sympy import symbols\r\n\r\nIn [2]: a = symbols('a', real=True)\r\n\r\nIn [3]: t = symbols('t', real=True, negative=False)\r\n\r\nIn [4]: bug = a * (-t + (-t + 1) * (2 * t - 1)) / (2 * t - 1)\r\n\r\nIn [5]: bug.subs(a, 1)\r\nOut[5]: (-t + (-t + 1)*(2*t - 1))/(2*t - 1)\r\n\r\nIn [6]: bug.subs(a, 1).apart()\r\nOut[6]: -t + 1/2 - 1/(2*(2*t - 1))\r\n\r\nIn [7]: bug.subs(a, 1).apart(t)\r\nOut[7]: -t + 1/2 - 1/(2*(2*t - 1))\r\n\r\nIn [8]: bug.apart(t)\r\nOut[8]: -a*t\r\n\r\nIn [9]: import sympy; sympy.__version__\r\nOut[9]: '1.0'\r\n```", "choices": "(A) \n    def from_FractionField(K1, a, K0):\n        \"\"\"Convert a rational function to ``dtype``. \"\"\"\n        denom = K0.denom(a)\n\n        if denom.is_ground:\n            return K1.from_PolynomialRing(K0.numer(a)/denom, K0.field.ring.to_domain())\n        else:\n            return None\n\n(B)             return a.set_ring(K1.ring)\n        except (CoercionFailed, GeneratorsError):\n            return None\n\n    def from_FractionField(K1, a, K0):\n        \"\"\"Convert a rational function to ``dtype``. \"\"\"\n        denom = K0.denom(a)\n\n        if denom.is_ground:\n            return K1.from_PolynomialRing(K0.numer(a)/denom, K0.field.ring.to_domain())\n(C)         if denom.is_ground:\n            return K1.from_PolynomialRing(K0.numer(a)/denom, K0.field.ring.to_domain())\n        else:\n            return None\n\n    def get_field(self):\n        \"\"\"Returns a field associated with `self`. \"\"\"\n        return self.ring.to_field().to_domain()\n\n    def is_positive(self, a):\n(D)         denom = K0.denom(a)\n\n        if denom.is_ground:\n            return K1.from_PolynomialRing(K0.numer(a)/denom, K0.field.ring.to_domain())\n        else:\n            return None\n\n    def get_field(self):\n        \"\"\"Returns a field associated with `self`. \"\"\"\n        return self.ring.to_field().to_domain()", "answer": "A"}
{"uuid": "bef363f8-9a68-4191-9474-374bd13bc3fb", "issue_statement": "is_upper() raises IndexError for tall matrices\nThe function Matrix.is_upper raises an IndexError for a 4x2 matrix of zeros.\r\n```\r\n>>> sympy.zeros(4,2).is_upper\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"sympy/matrices/matrices.py\", line 1112, in is_upper\r\n    for i in range(1, self.rows)\r\n  File \"sympy/matrices/matrices.py\", line 1113, in <genexpr>\r\n    for j in range(i))\r\n  File \"sympy/matrices/dense.py\", line 119, in __getitem__\r\n    return self.extract(i, j)\r\n  File \"sympy/matrices/matrices.py\", line 352, in extract\r\n    colsList = [a2idx(k, self.cols) for k in colsList]\r\n  File \"sympy/matrices/matrices.py\", line 5261, in a2idx\r\n    raise IndexError(\"Index out of range: a[%s]\" % (j,))\r\nIndexError: Index out of range: a[2]\r\n```\r\nThe code for is_upper() is\r\n```\r\n        return all(self[i, j].is_zero\r\n                   for i in range(1, self.rows)\r\n                   for j in range(i))\r\n```\r\nFor a 4x2 matrix, is_upper iterates over the indices:\r\n```\r\n>>> A = sympy.zeros(4, 2)\r\n>>> print tuple([i, j] for i in range(1, A.rows) for j in range(i))\r\n([1, 0], [2, 0], [2, 1], [3, 0], [3, 1], [3, 2])\r\n```\r\nThe attempt to index the (3,2) entry appears to be the source of the error.", "choices": "(A) \n        Examples\n        ========\n\n        >>> from sympy import Matrix, ones\n        >>> m = Matrix(3, 3, range(9))\n        >>> m\n        Matrix([\n        [0, 1, 2],\n        [3, 4, 5],\n        [6, 7, 8]])\n        >>> m.tolist()\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n        >>> ones(3, 0).tolist()\n        [[], [], []]\n\n        When there are no rows then it will not be possible to tell how\n        many columns were in the original matrix:\n\n        >>> ones(0, 3).tolist()\n        []\n\n        \"\"\"\n        if not self.rows:\n            return []\n        if not self.cols:\n            return [[] for i in range(self.rows)]\n        return self._eval_tolist()\n\n    def vec(self):\n        \"\"\"Return the Matrix converted into a one column matrix by stacking columns\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m=Matrix([[1, 3], [2, 4]])\n        >>> m\n        Matrix([\n        [1, 3],\n        [2, 4]])\n        >>> m.vec()\n        Matrix([\n        [1],\n        [2],\n        [3],\n        [4]])\n\n        See Also\n        ========\n\n        vech\n        \"\"\"\n        return self._eval_vec()\n\n\nclass MatrixProperties(MatrixRequired):\n    \"\"\"Provides basic properties of a matrix.\"\"\"\n\n    def _eval_atoms(self, *types):\n        result = set()\n        for i in self:\n            result.update(i.atoms(*types))\n        return result\n\n    def _eval_free_symbols(self):\n        return set().union(*(i.free_symbols for i in self))\n\n    def _eval_has(self, *patterns):\n        return any(a.has(*patterns) for a in self)\n\n    def _eval_is_anti_symmetric(self, simpfunc):\n        if not all(simpfunc(self[i, j] + self[j, i]).is_zero for i in range(self.rows) for j in range(self.cols)):\n            return False\n        return True\n\n    def _eval_is_diagonal(self):\n        for i in range(self.rows):\n            for j in range(self.cols):\n                if i != j and self[i, j]:\n                    return False\n        return True\n\n    def _eval_is_hermetian(self, simpfunc):\n        mat = self._new(self.rows, self.cols, lambda i, j: simpfunc(self[i, j] - self[j, i].conjugate()))\n        return mat.is_zero\n\n    def _eval_is_Identity(self):\n        def dirac(i, j):\n            if i == j:\n                return 1\n            return 0\n\n        return all(self[i, j] == dirac(i, j) for i in range(self.rows) for j in\n                   range(self.cols))\n\n    def _eval_is_lower_hessenberg(self):\n        return all(self[i, j].is_zero\n                   for i in range(self.rows)\n                   for j in range(i + 2, self.cols))\n\n    def _eval_is_lower(self):\n        return all(self[i, j].is_zero\n                   for i in range(self.rows)\n                   for j in range(i + 1, self.cols))\n\n    def _eval_is_symbolic(self):\n        return self.has(Symbol)\n\n    def _eval_is_symmetric(self, simpfunc):\n        mat = self._new(self.rows, self.cols, lambda i, j: simpfunc(self[i, j] - self[j, i]))\n        return mat.is_zero\n\n    def _eval_is_zero(self):\n        if any(i.is_zero == False for i in self):\n            return False\n        if any(i.is_zero == None for i in self):\n            return None\n        return True\n\n    def _eval_is_upper_hessenberg(self):\n        return all(self[i, j].is_zero\n                   for i in range(2, self.rows)\n                   for j in range(i - 1))\n\n    def _eval_values(self):\n        return [i for i in self if not i.is_zero]\n\n    def atoms(self, *types):\n        \"\"\"Returns the atoms that form the current object.\n\n        Examples\n        ========\n\n        >>> from sympy.abc import x, y\n        >>> from sympy.matrices import Matrix\n        >>> Matrix([[x]])\n        Matrix([[x]])\n        >>> _.atoms()\n        {x}\n        \"\"\"\n\n        types = tuple(t if isinstance(t, type) else type(t) for t in types)\n        if not types:\n            types = (Atom,)\n        return self._eval_atoms(*types)\n\n    @property\n    def free_symbols(self):\n        \"\"\"Returns the free symbols within the matrix.\n\n        Examples\n        ========\n\n        >>> from sympy.abc import x\n        >>> from sympy.matrices import Matrix\n        >>> Matrix([[x], [1]]).free_symbols\n        {x}\n        \"\"\"\n        return self._eval_free_symbols()\n\n    def has(self, *patterns):\n        \"\"\"Test whether any subexpression matches any of the patterns.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix, SparseMatrix, Float\n        >>> from sympy.abc import x, y\n        >>> A = Matrix(((1, x), (0.2, 3)))\n        >>> B = SparseMatrix(((1, x), (0.2, 3)))\n        >>> A.has(x)\n        True\n        >>> A.has(y)\n        False\n        >>> A.has(Float)\n        True\n        >>> B.has(x)\n        True\n        >>> B.has(y)\n        False\n        >>> B.has(Float)\n        True\n        \"\"\"\n        return self._eval_has(*patterns)\n\n    def is_anti_symmetric(self, simplify=True):\n        \"\"\"Check if matrix M is an antisymmetric matrix,\n        that is, M is a square matrix with all M[i, j] == -M[j, i].\n\n        When ``simplify=True`` (default), the sum M[i, j] + M[j, i] is\n        simplified before testing to see if it is zero. By default,\n        the SymPy simplify function is used. To use a custom function\n        set simplify to a function that accepts a single argument which\n        returns a simplified expression. To skip simplification, set\n        simplify to False but note that although this will be faster,\n        it may induce false negatives.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix, symbols\n        >>> m = Matrix(2, 2, [0, 1, -1, 0])\n        >>> m\n        Matrix([\n        [ 0, 1],\n        [-1, 0]])\n        >>> m.is_anti_symmetric()\n        True\n        >>> x, y = symbols('x y')\n        >>> m = Matrix(2, 3, [0, 0, x, -y, 0, 0])\n        >>> m\n        Matrix([\n        [ 0, 0, x],\n        [-y, 0, 0]])\n        >>> m.is_anti_symmetric()\n        False\n\n        >>> from sympy.abc import x, y\n        >>> m = Matrix(3, 3, [0, x**2 + 2*x + 1, y,\n        ...                   -(x + 1)**2 , 0, x*y,\n        ...                   -y, -x*y, 0])\n\n        Simplification of matrix elements is done by default so even\n        though two elements which should be equal and opposite wouldn't\n        pass an equality test, the matrix is still reported as\n        anti-symmetric:\n\n        >>> m[0, 1] == -m[1, 0]\n        False\n        >>> m.is_anti_symmetric()\n        True\n\n        If 'simplify=False' is used for the case when a Matrix is already\n        simplified, this will speed things up. Here, we see that without\n        simplification the matrix does not appear anti-symmetric:\n\n        >>> m.is_anti_symmetric(simplify=False)\n        False\n\n        But if the matrix were already expanded, then it would appear\n        anti-symmetric and simplification in the is_anti_symmetric routine\n        is not needed:\n\n        >>> m = m.expand()\n        >>> m.is_anti_symmetric(simplify=False)\n        True\n        \"\"\"\n        # accept custom simplification\n        simpfunc = simplify\n        if not isinstance(simplify, FunctionType):\n            simpfunc = _simplify if simplify else lambda x: x\n\n        if not self.is_square:\n            return False\n        return self._eval_is_anti_symmetric(simpfunc)\n\n    def is_diagonal(self):\n        \"\"\"Check if matrix is diagonal,\n        that is matrix in which the entries outside the main diagonal are all zero.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix, diag\n        >>> m = Matrix(2, 2, [1, 0, 0, 2])\n        >>> m\n        Matrix([\n        [1, 0],\n        [0, 2]])\n        >>> m.is_diagonal()\n        True\n\n        >>> m = Matrix(2, 2, [1, 1, 0, 2])\n        >>> m\n        Matrix([\n        [1, 1],\n        [0, 2]])\n        >>> m.is_diagonal()\n        False\n\n        >>> m = diag(1, 2, 3)\n        >>> m\n        Matrix([\n        [1, 0, 0],\n        [0, 2, 0],\n        [0, 0, 3]])\n        >>> m.is_diagonal()\n        True\n\n        See Also\n        ========\n\n        is_lower\n        is_upper\n        is_diagonalizable\n        diagonalize\n        \"\"\"\n        return self._eval_is_diagonal()\n\n    @property\n    def is_hermitian(self, simplify=True):\n        \"\"\"Checks if the matrix is Hermitian.\n\n        In a Hermitian matrix element i,j is the complex conjugate of\n        element j,i.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> from sympy import I\n        >>> from sympy.abc import x\n        >>> a = Matrix([[1, I], [-I, 1]])\n        >>> a\n        Matrix([\n        [ 1, I],\n        [-I, 1]])\n        >>> a.is_hermitian\n        True\n        >>> a[0, 0] = 2*I\n        >>> a.is_hermitian\n        False\n        >>> a[0, 0] = x\n        >>> a.is_hermitian\n        >>> a[0, 1] = a[1, 0]*I\n        >>> a.is_hermitian\n        False\n        \"\"\"\n        if not self.is_square:\n            return False\n\n        simpfunc = simplify\n        if not isinstance(simplify, FunctionType):\n            simpfunc = _simplify if simplify else lambda x: x\n\n        return self._eval_is_hermetian(simpfunc)\n\n    @property\n    def is_Identity(self):\n        if not self.is_square:\n            return False\n        return self._eval_is_Identity()\n\n    @property\n    def is_lower_hessenberg(self):\n        r\"\"\"Checks if the matrix is in the lower-Hessenberg form.\n\n        The lower hessenberg matrix has zero entries\n        above the first superdiagonal.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> a = Matrix([[1, 2, 0, 0], [5, 2, 3, 0], [3, 4, 3, 7], [5, 6, 1, 1]])\n        >>> a\n        Matrix([\n        [1, 2, 0, 0],\n        [5, 2, 3, 0],\n        [3, 4, 3, 7],\n        [5, 6, 1, 1]])\n        >>> a.is_lower_hessenberg\n        True\n\n        See Also\n        ========\n\n        is_upper_hessenberg\n        is_lower\n        \"\"\"\n        return self._eval_is_lower_hessenberg()\n\n    @property\n    def is_lower(self):\n        \"\"\"Check if matrix is a lower triangular matrix. True can be returned\n        even if the matrix is not square.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, [1, 0, 0, 1])\n        >>> m\n        Matrix([\n        [1, 0],\n        [0, 1]])\n        >>> m.is_lower\n        True\n\n        >>> m = Matrix(4, 3, [0, 0, 0, 2, 0, 0, 1, 4 , 0, 6, 6, 5])\n        >>> m\n        Matrix([\n        [0, 0, 0],\n        [2, 0, 0],\n        [1, 4, 0],\n        [6, 6, 5]])\n        >>> m.is_lower\n        True\n\n        >>> from sympy.abc import x, y\n        >>> m = Matrix(2, 2, [x**2 + y, y**2 + x, 0, x + y])\n        >>> m\n        Matrix([\n        [x**2 + y, x + y**2],\n        [       0,    x + y]])\n        >>> m.is_lower\n        False\n\n        See Also\n        ========\n\n        is_upper\n        is_diagonal\n        is_lower_hessenberg\n        \"\"\"\n        return self._eval_is_lower()\n\n    @property\n    def is_square(self):\n        \"\"\"Checks if a matrix is square.\n\n        A matrix is square if the number of rows equals the number of columns.\n        The empty matrix is square by definition, since the number of rows and\n        the number of columns are both zero.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> a = Matrix([[1, 2, 3], [4, 5, 6]])\n        >>> b = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        >>> c = Matrix([])\n        >>> a.is_square\n        False\n        >>> b.is_square\n        True\n        >>> c.is_square\n        True\n        \"\"\"\n        return self.rows == self.cols\n\n    def is_symbolic(self):\n        \"\"\"Checks if any elements contain Symbols.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> from sympy.abc import x, y\n        >>> M = Matrix([[x, y], [1, 0]])\n        >>> M.is_symbolic()\n        True\n\n        \"\"\"\n        return self._eval_is_symbolic()\n\n    def is_symmetric(self, simplify=True):\n        \"\"\"Check if matrix is symmetric matrix,\n        that is square matrix and is equal to its transpose.\n\n        By default, simplifications occur before testing symmetry.\n        They can be skipped using 'simplify=False'; while speeding things a bit,\n        this may however induce false negatives.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, [0, 1, 1, 2])\n        >>> m\n        Matrix([\n        [0, 1],\n        [1, 2]])\n        >>> m.is_symmetric()\n        True\n\n        >>> m = Matrix(2, 2, [0, 1, 2, 0])\n(B)         But if the matrix were already expanded, then it would appear\n        anti-symmetric and simplification in the is_anti_symmetric routine\n        is not needed:\n\n        >>> m = m.expand()\n        >>> m.is_anti_symmetric(simplify=False)\n        True\n        \"\"\"\n        # accept custom simplification\n        simpfunc = simplify\n        if not isinstance(simplify, FunctionType):\n            simpfunc = _simplify if simplify else lambda x: x\n\n        if not self.is_square:\n            return False\n        return self._eval_is_anti_symmetric(simpfunc)\n\n    def is_diagonal(self):\n        \"\"\"Check if matrix is diagonal,\n        that is matrix in which the entries outside the main diagonal are all zero.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix, diag\n        >>> m = Matrix(2, 2, [1, 0, 0, 2])\n        >>> m\n        Matrix([\n        [1, 0],\n        [0, 2]])\n        >>> m.is_diagonal()\n        True\n\n        >>> m = Matrix(2, 2, [1, 1, 0, 2])\n        >>> m\n        Matrix([\n        [1, 1],\n        [0, 2]])\n        >>> m.is_diagonal()\n        False\n\n        >>> m = diag(1, 2, 3)\n        >>> m\n        Matrix([\n        [1, 0, 0],\n        [0, 2, 0],\n        [0, 0, 3]])\n        >>> m.is_diagonal()\n        True\n\n        See Also\n        ========\n\n        is_lower\n        is_upper\n        is_diagonalizable\n        diagonalize\n        \"\"\"\n        return self._eval_is_diagonal()\n\n    @property\n    def is_hermitian(self, simplify=True):\n        \"\"\"Checks if the matrix is Hermitian.\n\n        In a Hermitian matrix element i,j is the complex conjugate of\n        element j,i.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> from sympy import I\n        >>> from sympy.abc import x\n        >>> a = Matrix([[1, I], [-I, 1]])\n        >>> a\n        Matrix([\n        [ 1, I],\n        [-I, 1]])\n        >>> a.is_hermitian\n        True\n        >>> a[0, 0] = 2*I\n        >>> a.is_hermitian\n        False\n        >>> a[0, 0] = x\n        >>> a.is_hermitian\n        >>> a[0, 1] = a[1, 0]*I\n        >>> a.is_hermitian\n        False\n        \"\"\"\n        if not self.is_square:\n            return False\n\n        simpfunc = simplify\n        if not isinstance(simplify, FunctionType):\n            simpfunc = _simplify if simplify else lambda x: x\n\n        return self._eval_is_hermetian(simpfunc)\n\n    @property\n    def is_Identity(self):\n        if not self.is_square:\n            return False\n        return self._eval_is_Identity()\n\n    @property\n    def is_lower_hessenberg(self):\n        r\"\"\"Checks if the matrix is in the lower-Hessenberg form.\n\n        The lower hessenberg matrix has zero entries\n        above the first superdiagonal.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> a = Matrix([[1, 2, 0, 0], [5, 2, 3, 0], [3, 4, 3, 7], [5, 6, 1, 1]])\n        >>> a\n        Matrix([\n        [1, 2, 0, 0],\n        [5, 2, 3, 0],\n        [3, 4, 3, 7],\n        [5, 6, 1, 1]])\n        >>> a.is_lower_hessenberg\n        True\n\n        See Also\n        ========\n\n        is_upper_hessenberg\n        is_lower\n        \"\"\"\n        return self._eval_is_lower_hessenberg()\n\n    @property\n    def is_lower(self):\n        \"\"\"Check if matrix is a lower triangular matrix. True can be returned\n        even if the matrix is not square.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, [1, 0, 0, 1])\n        >>> m\n        Matrix([\n        [1, 0],\n        [0, 1]])\n        >>> m.is_lower\n        True\n\n        >>> m = Matrix(4, 3, [0, 0, 0, 2, 0, 0, 1, 4 , 0, 6, 6, 5])\n        >>> m\n        Matrix([\n        [0, 0, 0],\n        [2, 0, 0],\n        [1, 4, 0],\n        [6, 6, 5]])\n        >>> m.is_lower\n        True\n\n        >>> from sympy.abc import x, y\n        >>> m = Matrix(2, 2, [x**2 + y, y**2 + x, 0, x + y])\n        >>> m\n        Matrix([\n        [x**2 + y, x + y**2],\n        [       0,    x + y]])\n        >>> m.is_lower\n        False\n\n        See Also\n        ========\n\n        is_upper\n        is_diagonal\n        is_lower_hessenberg\n        \"\"\"\n        return self._eval_is_lower()\n\n    @property\n    def is_square(self):\n        \"\"\"Checks if a matrix is square.\n\n        A matrix is square if the number of rows equals the number of columns.\n        The empty matrix is square by definition, since the number of rows and\n        the number of columns are both zero.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> a = Matrix([[1, 2, 3], [4, 5, 6]])\n        >>> b = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        >>> c = Matrix([])\n        >>> a.is_square\n        False\n        >>> b.is_square\n        True\n        >>> c.is_square\n        True\n        \"\"\"\n        return self.rows == self.cols\n\n    def is_symbolic(self):\n        \"\"\"Checks if any elements contain Symbols.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> from sympy.abc import x, y\n        >>> M = Matrix([[x, y], [1, 0]])\n        >>> M.is_symbolic()\n        True\n\n        \"\"\"\n        return self._eval_is_symbolic()\n\n    def is_symmetric(self, simplify=True):\n        \"\"\"Check if matrix is symmetric matrix,\n        that is square matrix and is equal to its transpose.\n\n        By default, simplifications occur before testing symmetry.\n        They can be skipped using 'simplify=False'; while speeding things a bit,\n        this may however induce false negatives.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, [0, 1, 1, 2])\n        >>> m\n        Matrix([\n        [0, 1],\n        [1, 2]])\n        >>> m.is_symmetric()\n        True\n\n        >>> m = Matrix(2, 2, [0, 1, 2, 0])\n        >>> m\n        Matrix([\n        [0, 1],\n        [2, 0]])\n        >>> m.is_symmetric()\n        False\n\n        >>> m = Matrix(2, 3, [0, 0, 0, 0, 0, 0])\n        >>> m\n        Matrix([\n        [0, 0, 0],\n        [0, 0, 0]])\n        >>> m.is_symmetric()\n        False\n\n        >>> from sympy.abc import x, y\n        >>> m = Matrix(3, 3, [1, x**2 + 2*x + 1, y, (x + 1)**2 , 2, 0, y, 0, 3])\n        >>> m\n        Matrix([\n        [         1, x**2 + 2*x + 1, y],\n        [(x + 1)**2,              2, 0],\n        [         y,              0, 3]])\n        >>> m.is_symmetric()\n        True\n\n        If the matrix is already simplified, you may speed-up is_symmetric()\n        test by using 'simplify=False'.\n\n        >>> bool(m.is_symmetric(simplify=False))\n        False\n        >>> m1 = m.expand()\n        >>> m1.is_symmetric(simplify=False)\n        True\n        \"\"\"\n        simpfunc = simplify\n        if not isinstance(simplify, FunctionType):\n            simpfunc = _simplify if simplify else lambda x: x\n\n        if not self.is_square:\n            return False\n\n        return self._eval_is_symmetric(simpfunc)\n\n    @property\n    def is_upper_hessenberg(self):\n        \"\"\"Checks if the matrix is the upper-Hessenberg form.\n\n        The upper hessenberg matrix has zero entries\n        below the first subdiagonal.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> a = Matrix([[1, 4, 2, 3], [3, 4, 1, 7], [0, 2, 3, 4], [0, 0, 1, 3]])\n        >>> a\n        Matrix([\n        [1, 4, 2, 3],\n        [3, 4, 1, 7],\n        [0, 2, 3, 4],\n        [0, 0, 1, 3]])\n        >>> a.is_upper_hessenberg\n        True\n\n        See Also\n        ========\n\n        is_lower_hessenberg\n        is_upper\n        \"\"\"\n        return self._eval_is_upper_hessenberg()\n\n    @property\n    def is_upper(self):\n        \"\"\"Check if matrix is an upper triangular matrix. True can be returned\n        even if the matrix is not square.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, [1, 0, 0, 1])\n        >>> m\n        Matrix([\n        [1, 0],\n        [0, 1]])\n        >>> m.is_upper\n        True\n\n        >>> m = Matrix(4, 3, [5, 1, 9, 0, 4 , 6, 0, 0, 5, 0, 0, 0])\n        >>> m\n        Matrix([\n        [5, 1, 9],\n        [0, 4, 6],\n        [0, 0, 5],\n        [0, 0, 0]])\n        >>> m.is_upper\n        True\n\n        >>> m = Matrix(2, 3, [4, 2, 5, 6, 1, 1])\n        >>> m\n        Matrix([\n        [4, 2, 5],\n        [6, 1, 1]])\n        >>> m.is_upper\n        False\n\n        See Also\n        ========\n\n        is_lower\n        is_diagonal\n        is_upper_hessenberg\n        \"\"\"\n        return all(self[i, j].is_zero\n                   for i in range(1, self.rows)\n                   for j in range(i))\n\n    @property\n    def is_zero(self):\n        \"\"\"Checks if a matrix is a zero matrix.\n\n        A matrix is zero if every element is zero.  A matrix need not be square\n        to be considered zero.  The empty matrix is zero by the principle of\n        vacuous truth.  For a matrix that may or may not be zero (e.g.\n        contains a symbol), this will be None\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix, zeros\n        >>> from sympy.abc import x\n        >>> a = Matrix([[0, 0], [0, 0]])\n        >>> b = zeros(3, 4)\n        >>> c = Matrix([[0, 1], [0, 0]])\n        >>> d = Matrix([])\n        >>> e = Matrix([[x, 0], [0, 0]])\n        >>> a.is_zero\n        True\n        >>> b.is_zero\n        True\n        >>> c.is_zero\n        False\n        >>> d.is_zero\n        True\n        >>> e.is_zero\n        \"\"\"\n        return self._eval_is_zero()\n\n    def values(self):\n        \"\"\"Return non-zero values of self.\"\"\"\n        return self._eval_values()\n\n\nclass MatrixOperations(MatrixRequired):\n    \"\"\"Provides basic matrix shape and elementwise\n    operations.  Should not be instantiated directly.\"\"\"\n\n    def _eval_adjoint(self):\n        return self.transpose().conjugate()\n\n    def _eval_applyfunc(self, f):\n        out = self._new(self.rows, self.cols, [f(x) for x in self])\n        return out\n\n    def _eval_as_real_imag(self):\n        from sympy.functions.elementary.complexes import re, im\n\n        return (self.applyfunc(re), self.applyfunc(im))\n\n    def _eval_conjugate(self):\n        return self.applyfunc(lambda x: x.conjugate())\n\n    def _eval_trace(self):\n        return sum(self[i, i] for i in range(self.rows))\n\n    def _eval_transpose(self):\n        return self._new(self.cols, self.rows, lambda i, j: self[j, i])\n\n    def adjoint(self):\n        \"\"\"Conjugate transpose or Hermitian conjugation.\"\"\"\n        return self._eval_adjoint()\n\n    def applyfunc(self, f):\n        \"\"\"Apply a function to each element of the matrix.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, lambda i, j: i*2+j)\n        >>> m\n        Matrix([\n        [0, 1],\n        [2, 3]])\n        >>> m.applyfunc(lambda i: 2*i)\n        Matrix([\n        [0, 2],\n        [4, 6]])\n\n        \"\"\"\n        if not callable(f):\n            raise TypeError(\"`f` must be callable.\")\n\n        return self._eval_applyfunc(f)\n\n    def as_real_imag(self):\n        \"\"\"Returns a tuple containing the (real, imaginary) part of matrix.\"\"\"\n        return self._eval_as_real_imag()\n\n    def conjugate(self):\n        \"\"\"Return the by-element conjugation.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> from sympy import I\n        >>> a = SparseMatrix(((1, 2 + I), (3, 4), (I, -I)))\n        >>> a\n        Matrix([\n        [1, 2 + I],\n        [3,     4],\n        [I,    -I]])\n        >>> a.C\n        Matrix([\n        [ 1, 2 - I],\n        [ 3,     4],\n        [-I,     I]])\n\n        See Also\n        ========\n\n        transpose: Matrix transposition\n        H: Hermite conjugation\n        D: Dirac conjugation\n        \"\"\"\n        return self._eval_conjugate()\n\n    def doit(self, **kwargs):\n(C)     def _eval_is_upper_hessenberg(self):\n        return all(self[i, j].is_zero\n                   for i in range(2, self.rows)\n                   for j in range(i - 1))\n\n    def _eval_values(self):\n        return [i for i in self if not i.is_zero]\n\n    def atoms(self, *types):\n        \"\"\"Returns the atoms that form the current object.\n\n        Examples\n        ========\n\n        >>> from sympy.abc import x, y\n        >>> from sympy.matrices import Matrix\n        >>> Matrix([[x]])\n        Matrix([[x]])\n        >>> _.atoms()\n        {x}\n        \"\"\"\n\n        types = tuple(t if isinstance(t, type) else type(t) for t in types)\n        if not types:\n            types = (Atom,)\n        return self._eval_atoms(*types)\n\n    @property\n    def free_symbols(self):\n        \"\"\"Returns the free symbols within the matrix.\n\n        Examples\n        ========\n\n        >>> from sympy.abc import x\n        >>> from sympy.matrices import Matrix\n        >>> Matrix([[x], [1]]).free_symbols\n        {x}\n        \"\"\"\n        return self._eval_free_symbols()\n\n    def has(self, *patterns):\n        \"\"\"Test whether any subexpression matches any of the patterns.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix, SparseMatrix, Float\n        >>> from sympy.abc import x, y\n        >>> A = Matrix(((1, x), (0.2, 3)))\n        >>> B = SparseMatrix(((1, x), (0.2, 3)))\n        >>> A.has(x)\n        True\n        >>> A.has(y)\n        False\n        >>> A.has(Float)\n        True\n        >>> B.has(x)\n        True\n        >>> B.has(y)\n        False\n        >>> B.has(Float)\n        True\n        \"\"\"\n        return self._eval_has(*patterns)\n\n    def is_anti_symmetric(self, simplify=True):\n        \"\"\"Check if matrix M is an antisymmetric matrix,\n        that is, M is a square matrix with all M[i, j] == -M[j, i].\n\n        When ``simplify=True`` (default), the sum M[i, j] + M[j, i] is\n        simplified before testing to see if it is zero. By default,\n        the SymPy simplify function is used. To use a custom function\n        set simplify to a function that accepts a single argument which\n        returns a simplified expression. To skip simplification, set\n        simplify to False but note that although this will be faster,\n        it may induce false negatives.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix, symbols\n        >>> m = Matrix(2, 2, [0, 1, -1, 0])\n        >>> m\n        Matrix([\n        [ 0, 1],\n        [-1, 0]])\n        >>> m.is_anti_symmetric()\n        True\n        >>> x, y = symbols('x y')\n        >>> m = Matrix(2, 3, [0, 0, x, -y, 0, 0])\n        >>> m\n        Matrix([\n        [ 0, 0, x],\n        [-y, 0, 0]])\n        >>> m.is_anti_symmetric()\n        False\n\n        >>> from sympy.abc import x, y\n        >>> m = Matrix(3, 3, [0, x**2 + 2*x + 1, y,\n        ...                   -(x + 1)**2 , 0, x*y,\n        ...                   -y, -x*y, 0])\n\n        Simplification of matrix elements is done by default so even\n        though two elements which should be equal and opposite wouldn't\n        pass an equality test, the matrix is still reported as\n        anti-symmetric:\n\n        >>> m[0, 1] == -m[1, 0]\n        False\n        >>> m.is_anti_symmetric()\n        True\n\n        If 'simplify=False' is used for the case when a Matrix is already\n        simplified, this will speed things up. Here, we see that without\n        simplification the matrix does not appear anti-symmetric:\n\n        >>> m.is_anti_symmetric(simplify=False)\n        False\n\n        But if the matrix were already expanded, then it would appear\n        anti-symmetric and simplification in the is_anti_symmetric routine\n        is not needed:\n\n        >>> m = m.expand()\n        >>> m.is_anti_symmetric(simplify=False)\n        True\n        \"\"\"\n        # accept custom simplification\n        simpfunc = simplify\n        if not isinstance(simplify, FunctionType):\n            simpfunc = _simplify if simplify else lambda x: x\n\n        if not self.is_square:\n            return False\n        return self._eval_is_anti_symmetric(simpfunc)\n\n    def is_diagonal(self):\n        \"\"\"Check if matrix is diagonal,\n        that is matrix in which the entries outside the main diagonal are all zero.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix, diag\n        >>> m = Matrix(2, 2, [1, 0, 0, 2])\n        >>> m\n        Matrix([\n        [1, 0],\n        [0, 2]])\n        >>> m.is_diagonal()\n        True\n\n        >>> m = Matrix(2, 2, [1, 1, 0, 2])\n        >>> m\n        Matrix([\n        [1, 1],\n        [0, 2]])\n        >>> m.is_diagonal()\n        False\n\n        >>> m = diag(1, 2, 3)\n        >>> m\n        Matrix([\n        [1, 0, 0],\n        [0, 2, 0],\n        [0, 0, 3]])\n        >>> m.is_diagonal()\n        True\n\n        See Also\n        ========\n\n        is_lower\n        is_upper\n        is_diagonalizable\n        diagonalize\n        \"\"\"\n        return self._eval_is_diagonal()\n\n    @property\n    def is_hermitian(self, simplify=True):\n        \"\"\"Checks if the matrix is Hermitian.\n\n        In a Hermitian matrix element i,j is the complex conjugate of\n        element j,i.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> from sympy import I\n        >>> from sympy.abc import x\n        >>> a = Matrix([[1, I], [-I, 1]])\n        >>> a\n        Matrix([\n        [ 1, I],\n        [-I, 1]])\n        >>> a.is_hermitian\n        True\n        >>> a[0, 0] = 2*I\n        >>> a.is_hermitian\n        False\n        >>> a[0, 0] = x\n        >>> a.is_hermitian\n        >>> a[0, 1] = a[1, 0]*I\n        >>> a.is_hermitian\n        False\n        \"\"\"\n        if not self.is_square:\n            return False\n\n        simpfunc = simplify\n        if not isinstance(simplify, FunctionType):\n            simpfunc = _simplify if simplify else lambda x: x\n\n        return self._eval_is_hermetian(simpfunc)\n\n    @property\n    def is_Identity(self):\n        if not self.is_square:\n            return False\n        return self._eval_is_Identity()\n\n    @property\n    def is_lower_hessenberg(self):\n        r\"\"\"Checks if the matrix is in the lower-Hessenberg form.\n\n        The lower hessenberg matrix has zero entries\n        above the first superdiagonal.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> a = Matrix([[1, 2, 0, 0], [5, 2, 3, 0], [3, 4, 3, 7], [5, 6, 1, 1]])\n        >>> a\n        Matrix([\n        [1, 2, 0, 0],\n        [5, 2, 3, 0],\n        [3, 4, 3, 7],\n        [5, 6, 1, 1]])\n        >>> a.is_lower_hessenberg\n        True\n\n        See Also\n        ========\n\n        is_upper_hessenberg\n        is_lower\n        \"\"\"\n        return self._eval_is_lower_hessenberg()\n\n    @property\n    def is_lower(self):\n        \"\"\"Check if matrix is a lower triangular matrix. True can be returned\n        even if the matrix is not square.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, [1, 0, 0, 1])\n        >>> m\n        Matrix([\n        [1, 0],\n        [0, 1]])\n        >>> m.is_lower\n        True\n\n        >>> m = Matrix(4, 3, [0, 0, 0, 2, 0, 0, 1, 4 , 0, 6, 6, 5])\n        >>> m\n        Matrix([\n        [0, 0, 0],\n        [2, 0, 0],\n        [1, 4, 0],\n        [6, 6, 5]])\n        >>> m.is_lower\n        True\n\n        >>> from sympy.abc import x, y\n        >>> m = Matrix(2, 2, [x**2 + y, y**2 + x, 0, x + y])\n        >>> m\n        Matrix([\n        [x**2 + y, x + y**2],\n        [       0,    x + y]])\n        >>> m.is_lower\n        False\n\n        See Also\n        ========\n\n        is_upper\n        is_diagonal\n        is_lower_hessenberg\n        \"\"\"\n        return self._eval_is_lower()\n\n    @property\n    def is_square(self):\n        \"\"\"Checks if a matrix is square.\n\n        A matrix is square if the number of rows equals the number of columns.\n        The empty matrix is square by definition, since the number of rows and\n        the number of columns are both zero.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> a = Matrix([[1, 2, 3], [4, 5, 6]])\n        >>> b = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        >>> c = Matrix([])\n        >>> a.is_square\n        False\n        >>> b.is_square\n        True\n        >>> c.is_square\n        True\n        \"\"\"\n        return self.rows == self.cols\n\n    def is_symbolic(self):\n        \"\"\"Checks if any elements contain Symbols.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> from sympy.abc import x, y\n        >>> M = Matrix([[x, y], [1, 0]])\n        >>> M.is_symbolic()\n        True\n\n        \"\"\"\n        return self._eval_is_symbolic()\n\n    def is_symmetric(self, simplify=True):\n        \"\"\"Check if matrix is symmetric matrix,\n        that is square matrix and is equal to its transpose.\n\n        By default, simplifications occur before testing symmetry.\n        They can be skipped using 'simplify=False'; while speeding things a bit,\n        this may however induce false negatives.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, [0, 1, 1, 2])\n        >>> m\n        Matrix([\n        [0, 1],\n        [1, 2]])\n        >>> m.is_symmetric()\n        True\n\n        >>> m = Matrix(2, 2, [0, 1, 2, 0])\n        >>> m\n        Matrix([\n        [0, 1],\n        [2, 0]])\n        >>> m.is_symmetric()\n        False\n\n        >>> m = Matrix(2, 3, [0, 0, 0, 0, 0, 0])\n        >>> m\n        Matrix([\n        [0, 0, 0],\n        [0, 0, 0]])\n        >>> m.is_symmetric()\n        False\n\n        >>> from sympy.abc import x, y\n        >>> m = Matrix(3, 3, [1, x**2 + 2*x + 1, y, (x + 1)**2 , 2, 0, y, 0, 3])\n        >>> m\n        Matrix([\n        [         1, x**2 + 2*x + 1, y],\n        [(x + 1)**2,              2, 0],\n        [         y,              0, 3]])\n        >>> m.is_symmetric()\n        True\n\n        If the matrix is already simplified, you may speed-up is_symmetric()\n        test by using 'simplify=False'.\n\n        >>> bool(m.is_symmetric(simplify=False))\n        False\n        >>> m1 = m.expand()\n        >>> m1.is_symmetric(simplify=False)\n        True\n        \"\"\"\n        simpfunc = simplify\n        if not isinstance(simplify, FunctionType):\n            simpfunc = _simplify if simplify else lambda x: x\n\n        if not self.is_square:\n            return False\n\n        return self._eval_is_symmetric(simpfunc)\n\n    @property\n    def is_upper_hessenberg(self):\n        \"\"\"Checks if the matrix is the upper-Hessenberg form.\n\n        The upper hessenberg matrix has zero entries\n        below the first subdiagonal.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> a = Matrix([[1, 4, 2, 3], [3, 4, 1, 7], [0, 2, 3, 4], [0, 0, 1, 3]])\n        >>> a\n        Matrix([\n        [1, 4, 2, 3],\n        [3, 4, 1, 7],\n        [0, 2, 3, 4],\n        [0, 0, 1, 3]])\n        >>> a.is_upper_hessenberg\n        True\n\n        See Also\n        ========\n\n        is_lower_hessenberg\n        is_upper\n        \"\"\"\n        return self._eval_is_upper_hessenberg()\n\n    @property\n    def is_upper(self):\n        \"\"\"Check if matrix is an upper triangular matrix. True can be returned\n        even if the matrix is not square.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, [1, 0, 0, 1])\n        >>> m\n        Matrix([\n        [1, 0],\n        [0, 1]])\n        >>> m.is_upper\n        True\n\n        >>> m = Matrix(4, 3, [5, 1, 9, 0, 4 , 6, 0, 0, 5, 0, 0, 0])\n        >>> m\n        Matrix([\n        [5, 1, 9],\n        [0, 4, 6],\n        [0, 0, 5],\n        [0, 0, 0]])\n        >>> m.is_upper\n        True\n\n        >>> m = Matrix(2, 3, [4, 2, 5, 6, 1, 1])\n        >>> m\n        Matrix([\n        [4, 2, 5],\n        [6, 1, 1]])\n        >>> m.is_upper\n        False\n\n        See Also\n        ========\n\n        is_lower\n        is_diagonal\n        is_upper_hessenberg\n        \"\"\"\n        return all(self[i, j].is_zero\n                   for i in range(1, self.rows)\n                   for j in range(i))\n\n    @property\n    def is_zero(self):\n(D)         [5, 2, 3, 0],\n        [3, 4, 3, 7],\n        [5, 6, 1, 1]])\n        >>> a.is_lower_hessenberg\n        True\n\n        See Also\n        ========\n\n        is_upper_hessenberg\n        is_lower\n        \"\"\"\n        return self._eval_is_lower_hessenberg()\n\n    @property\n    def is_lower(self):\n        \"\"\"Check if matrix is a lower triangular matrix. True can be returned\n        even if the matrix is not square.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, [1, 0, 0, 1])\n        >>> m\n        Matrix([\n        [1, 0],\n        [0, 1]])\n        >>> m.is_lower\n        True\n\n        >>> m = Matrix(4, 3, [0, 0, 0, 2, 0, 0, 1, 4 , 0, 6, 6, 5])\n        >>> m\n        Matrix([\n        [0, 0, 0],\n        [2, 0, 0],\n        [1, 4, 0],\n        [6, 6, 5]])\n        >>> m.is_lower\n        True\n\n        >>> from sympy.abc import x, y\n        >>> m = Matrix(2, 2, [x**2 + y, y**2 + x, 0, x + y])\n        >>> m\n        Matrix([\n        [x**2 + y, x + y**2],\n        [       0,    x + y]])\n        >>> m.is_lower\n        False\n\n        See Also\n        ========\n\n        is_upper\n        is_diagonal\n        is_lower_hessenberg\n        \"\"\"\n        return self._eval_is_lower()\n\n    @property\n    def is_square(self):\n        \"\"\"Checks if a matrix is square.\n\n        A matrix is square if the number of rows equals the number of columns.\n        The empty matrix is square by definition, since the number of rows and\n        the number of columns are both zero.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> a = Matrix([[1, 2, 3], [4, 5, 6]])\n        >>> b = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        >>> c = Matrix([])\n        >>> a.is_square\n        False\n        >>> b.is_square\n        True\n        >>> c.is_square\n        True\n        \"\"\"\n        return self.rows == self.cols\n\n    def is_symbolic(self):\n        \"\"\"Checks if any elements contain Symbols.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> from sympy.abc import x, y\n        >>> M = Matrix([[x, y], [1, 0]])\n        >>> M.is_symbolic()\n        True\n\n        \"\"\"\n        return self._eval_is_symbolic()\n\n    def is_symmetric(self, simplify=True):\n        \"\"\"Check if matrix is symmetric matrix,\n        that is square matrix and is equal to its transpose.\n\n        By default, simplifications occur before testing symmetry.\n        They can be skipped using 'simplify=False'; while speeding things a bit,\n        this may however induce false negatives.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, [0, 1, 1, 2])\n        >>> m\n        Matrix([\n        [0, 1],\n        [1, 2]])\n        >>> m.is_symmetric()\n        True\n\n        >>> m = Matrix(2, 2, [0, 1, 2, 0])\n        >>> m\n        Matrix([\n        [0, 1],\n        [2, 0]])\n        >>> m.is_symmetric()\n        False\n\n        >>> m = Matrix(2, 3, [0, 0, 0, 0, 0, 0])\n        >>> m\n        Matrix([\n        [0, 0, 0],\n        [0, 0, 0]])\n        >>> m.is_symmetric()\n        False\n\n        >>> from sympy.abc import x, y\n        >>> m = Matrix(3, 3, [1, x**2 + 2*x + 1, y, (x + 1)**2 , 2, 0, y, 0, 3])\n        >>> m\n        Matrix([\n        [         1, x**2 + 2*x + 1, y],\n        [(x + 1)**2,              2, 0],\n        [         y,              0, 3]])\n        >>> m.is_symmetric()\n        True\n\n        If the matrix is already simplified, you may speed-up is_symmetric()\n        test by using 'simplify=False'.\n\n        >>> bool(m.is_symmetric(simplify=False))\n        False\n        >>> m1 = m.expand()\n        >>> m1.is_symmetric(simplify=False)\n        True\n        \"\"\"\n        simpfunc = simplify\n        if not isinstance(simplify, FunctionType):\n            simpfunc = _simplify if simplify else lambda x: x\n\n        if not self.is_square:\n            return False\n\n        return self._eval_is_symmetric(simpfunc)\n\n    @property\n    def is_upper_hessenberg(self):\n        \"\"\"Checks if the matrix is the upper-Hessenberg form.\n\n        The upper hessenberg matrix has zero entries\n        below the first subdiagonal.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> a = Matrix([[1, 4, 2, 3], [3, 4, 1, 7], [0, 2, 3, 4], [0, 0, 1, 3]])\n        >>> a\n        Matrix([\n        [1, 4, 2, 3],\n        [3, 4, 1, 7],\n        [0, 2, 3, 4],\n        [0, 0, 1, 3]])\n        >>> a.is_upper_hessenberg\n        True\n\n        See Also\n        ========\n\n        is_lower_hessenberg\n        is_upper\n        \"\"\"\n        return self._eval_is_upper_hessenberg()\n\n    @property\n    def is_upper(self):\n        \"\"\"Check if matrix is an upper triangular matrix. True can be returned\n        even if the matrix is not square.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, [1, 0, 0, 1])\n        >>> m\n        Matrix([\n        [1, 0],\n        [0, 1]])\n        >>> m.is_upper\n        True\n\n        >>> m = Matrix(4, 3, [5, 1, 9, 0, 4 , 6, 0, 0, 5, 0, 0, 0])\n        >>> m\n        Matrix([\n        [5, 1, 9],\n        [0, 4, 6],\n        [0, 0, 5],\n        [0, 0, 0]])\n        >>> m.is_upper\n        True\n\n        >>> m = Matrix(2, 3, [4, 2, 5, 6, 1, 1])\n        >>> m\n        Matrix([\n        [4, 2, 5],\n        [6, 1, 1]])\n        >>> m.is_upper\n        False\n\n        See Also\n        ========\n\n        is_lower\n        is_diagonal\n        is_upper_hessenberg\n        \"\"\"\n        return all(self[i, j].is_zero\n                   for i in range(1, self.rows)\n                   for j in range(i))\n\n    @property\n    def is_zero(self):\n        \"\"\"Checks if a matrix is a zero matrix.\n\n        A matrix is zero if every element is zero.  A matrix need not be square\n        to be considered zero.  The empty matrix is zero by the principle of\n        vacuous truth.  For a matrix that may or may not be zero (e.g.\n        contains a symbol), this will be None\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix, zeros\n        >>> from sympy.abc import x\n        >>> a = Matrix([[0, 0], [0, 0]])\n        >>> b = zeros(3, 4)\n        >>> c = Matrix([[0, 1], [0, 0]])\n        >>> d = Matrix([])\n        >>> e = Matrix([[x, 0], [0, 0]])\n        >>> a.is_zero\n        True\n        >>> b.is_zero\n        True\n        >>> c.is_zero\n        False\n        >>> d.is_zero\n        True\n        >>> e.is_zero\n        \"\"\"\n        return self._eval_is_zero()\n\n    def values(self):\n        \"\"\"Return non-zero values of self.\"\"\"\n        return self._eval_values()\n\n\nclass MatrixOperations(MatrixRequired):\n    \"\"\"Provides basic matrix shape and elementwise\n    operations.  Should not be instantiated directly.\"\"\"\n\n    def _eval_adjoint(self):\n        return self.transpose().conjugate()\n\n    def _eval_applyfunc(self, f):\n        out = self._new(self.rows, self.cols, [f(x) for x in self])\n        return out\n\n    def _eval_as_real_imag(self):\n        from sympy.functions.elementary.complexes import re, im\n\n        return (self.applyfunc(re), self.applyfunc(im))\n\n    def _eval_conjugate(self):\n        return self.applyfunc(lambda x: x.conjugate())\n\n    def _eval_trace(self):\n        return sum(self[i, i] for i in range(self.rows))\n\n    def _eval_transpose(self):\n        return self._new(self.cols, self.rows, lambda i, j: self[j, i])\n\n    def adjoint(self):\n        \"\"\"Conjugate transpose or Hermitian conjugation.\"\"\"\n        return self._eval_adjoint()\n\n    def applyfunc(self, f):\n        \"\"\"Apply a function to each element of the matrix.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix\n        >>> m = Matrix(2, 2, lambda i, j: i*2+j)\n        >>> m\n        Matrix([\n        [0, 1],\n        [2, 3]])\n        >>> m.applyfunc(lambda i: 2*i)\n        Matrix([\n        [0, 2],\n        [4, 6]])\n\n        \"\"\"\n        if not callable(f):\n            raise TypeError(\"`f` must be callable.\")\n\n        return self._eval_applyfunc(f)\n\n    def as_real_imag(self):\n        \"\"\"Returns a tuple containing the (real, imaginary) part of matrix.\"\"\"\n        return self._eval_as_real_imag()\n\n    def conjugate(self):\n        \"\"\"Return the by-element conjugation.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> from sympy import I\n        >>> a = SparseMatrix(((1, 2 + I), (3, 4), (I, -I)))\n        >>> a\n        Matrix([\n        [1, 2 + I],\n        [3,     4],\n        [I,    -I]])\n        >>> a.C\n        Matrix([\n        [ 1, 2 - I],\n        [ 3,     4],\n        [-I,     I]])\n\n        See Also\n        ========\n\n        transpose: Matrix transposition\n        H: Hermite conjugation\n        D: Dirac conjugation\n        \"\"\"\n        return self._eval_conjugate()\n\n    def doit(self, **kwargs):\n        return self.applyfunc(lambda x: x.doit())\n\n    def evalf(self, prec=None, **options):\n        \"\"\"Apply evalf() to each element of self.\"\"\"\n        return self.applyfunc(lambda i: i.evalf(prec, **options))\n\n    def expand(self, deep=True, modulus=None, power_base=True, power_exp=True,\n               mul=True, log=True, multinomial=True, basic=True, **hints):\n        \"\"\"Apply core.function.expand to each entry of the matrix.\n\n        Examples\n        ========\n\n        >>> from sympy.abc import x\n        >>> from sympy.matrices import Matrix\n        >>> Matrix(1, 1, [x*(x+1)])\n        Matrix([[x*(x + 1)]])\n        >>> _.expand()\n        Matrix([[x**2 + x]])\n\n        \"\"\"\n        return self.applyfunc(lambda x: x.expand(\n            deep, modulus, power_base, power_exp, mul, log, multinomial, basic,\n            **hints))\n\n    @property\n    def H(self):\n        \"\"\"Return Hermite conjugate.\n\n        Examples\n        ========\n\n        >>> from sympy import Matrix, I\n        >>> m = Matrix((0, 1 + I, 2, 3))\n        >>> m\n        Matrix([\n        [    0],\n        [1 + I],\n        [    2],\n        [    3]])\n        >>> m.H\n        Matrix([[0, 1 - I, 2, 3]])\n\n        See Also\n        ========\n\n        conjugate: By-element conjugation\n        D: Dirac conjugation\n        \"\"\"\n        return self.T.C\n\n    def refine(self, assumptions=True):\n        \"\"\"Apply refine to each element of the matrix.\n\n        Examples\n        ========\n\n        >>> from sympy import Symbol, Matrix, Abs, sqrt, Q\n        >>> x = Symbol('x')\n        >>> Matrix([[Abs(x)**2, sqrt(x**2)],[sqrt(x**2), Abs(x)**2]])\n        Matrix([\n        [ Abs(x)**2, sqrt(x**2)],\n        [sqrt(x**2),  Abs(x)**2]])\n        >>> _.refine(Q.real(x))\n        Matrix([\n        [  x**2, Abs(x)],\n        [Abs(x),   x**2]])\n\n        \"\"\"\n        return self.applyfunc(lambda x: refine(x, assumptions))\n\n    def replace(self, F, G, map=False):\n        \"\"\"Replaces Function F in Matrix entries with Function G.\n\n        Examples\n        ========\n\n        >>> from sympy import symbols, Function, Matrix\n        >>> F, G = symbols('F, G', cls=Function)\n        >>> M = Matrix(2, 2, lambda i, j: F(i+j)) ; M\n        Matrix([\n        [F(0), F(1)],\n        [F(1), F(2)]])\n        >>> N = M.replace(F,G)\n        >>> N\n        Matrix([\n        [G(0), G(1)],\n        [G(1), G(2)]])\n        \"\"\"\n        return self.applyfunc(lambda x: x.replace(F, G, map))\n\n    def simplify(self, ratio=1.7, measure=count_ops):\n        \"\"\"Apply simplify to each element of the matrix.\n\n        Examples\n        ========\n\n        >>> from sympy.abc import x, y\n        >>> from sympy import sin, cos\n        >>> from sympy.matrices import SparseMatrix\n        >>> SparseMatrix(1, 1, [x*sin(y)**2 + x*cos(y)**2])\n        Matrix([[x*sin(y)**2 + x*cos(y)**2]])\n        >>> _.simplify()\n        Matrix([[x]])\n        \"\"\"\n        return self.applyfunc(lambda x: x.simplify(ratio, measure))\n\n    def subs(self, *args, **kwargs):  # should mirror core.basic.subs\n        \"\"\"Return a new matrix with subs applied to each entry.\n\n        Examples\n        ========\n\n        >>> from sympy.abc import x, y\n        >>> from sympy.matrices import SparseMatrix, Matrix\n        >>> SparseMatrix(1, 1, [x])\n        Matrix([[x]])\n        >>> _.subs(x, y)\n        Matrix([[y]])", "answer": "C"}
{"uuid": "a6ad689e-6c84-47a9-8dae-d04813bc322f", "issue_statement": "`Permutation` constructor fails with non-disjoint cycles\nCalling `Permutation([[0,1],[0,1]])` raises a `ValueError` instead of constructing the identity permutation.  If the cycles passed in are non-disjoint, they should be applied in left-to-right order and the resulting permutation should be returned.\r\n\r\nThis should be easy to compute.  I don't see a reason why non-disjoint cycles should be forbidden.", "choices": "(A)         # unless a cycle notation has been provided. A 0 will be added\n        # for convenience in case one wants to enter permutations where\n        # counting starts from 1.\n\n        temp = flatten(args)\n        if has_dups(temp):\n            if is_cycle:\n                raise ValueError('there were repeated elements; to resolve '\n(B)             if is_cycle:\n                raise ValueError('there were repeated elements; to resolve '\n                'cycles use Cycle%s.' % ''.join([str(tuple(c)) for c in args]))\n            else:\n                raise ValueError('there were repeated elements.')\n        temp = set(temp)\n\n        if not is_cycle and \\\n(C)         temp = flatten(args)\n        if has_dups(temp):\n            if is_cycle:\n                raise ValueError('there were repeated elements; to resolve '\n                'cycles use Cycle%s.' % ''.join([str(tuple(c)) for c in args]))\n            else:\n                raise ValueError('there were repeated elements.')\n        temp = set(temp)\n(D)         # counting starts from 1.\n\n        temp = flatten(args)\n        if has_dups(temp):\n            if is_cycle:\n                raise ValueError('there were repeated elements; to resolve '\n                'cycles use Cycle%s.' % ''.join([str(tuple(c)) for c in args]))\n            else:", "answer": "D"}
{"uuid": "78310573-c3a4-4729-8972-71f1ef0997b3", "issue_statement": "Behavior of Matrix hstack and vstack changed in sympy 1.1\nIn sympy 1.0:\r\n```\r\nimport sympy as sy\r\nM1 = sy.Matrix.zeros(0, 0)\r\nM2 = sy.Matrix.zeros(0, 1)\r\nM3 = sy.Matrix.zeros(0, 2)\r\nM4 = sy.Matrix.zeros(0, 3)\r\nsy.Matrix.hstack(M1, M2, M3, M4).shape\r\n```\r\nreturns \r\n`(0, 6)`\r\n\r\nNow, same in sympy 1.1:\r\n```\r\nimport sympy as sy\r\nM1 = sy.Matrix.zeros(0, 0)\r\nM2 = sy.Matrix.zeros(0, 1)\r\nM3 = sy.Matrix.zeros(0, 2)\r\nM4 = sy.Matrix.zeros(0, 3)\r\nsy.Matrix.hstack(M1, M2, M3, M4).shape\r\n```\r\nreturns\r\n`(0, 3)\r\n`\r\nwhereas:\r\n```\r\nimport sympy as sy\r\nM1 = sy.Matrix.zeros(1, 0)\r\nM2 = sy.Matrix.zeros(1, 1)\r\nM3 = sy.Matrix.zeros(1, 2)\r\nM4 = sy.Matrix.zeros(1, 3)\r\nsy.Matrix.hstack(M1, M2, M3, M4).shape\r\n```\r\nreturns\r\n`(1, 6)\r\n`", "choices": "(A)         [0, 0],\n        [1, 1]])\n\n\n        To replace row r you assign to position r*m where m\n        is the number of columns:\n\n        >>> M = SparseMatrix(4, 4, {})\n        >>> m = M.cols\n        >>> M[3*m] = ones(1, m)*2; M\n        Matrix([\n        [0, 0, 0, 0],\n        [0, 0, 0, 0],\n        [0, 0, 0, 0],\n        [2, 2, 2, 2]])\n\n        And to replace column c you can assign to position c:\n\n        >>> M[2] = ones(m, 1)*4; M\n        Matrix([\n        [0, 0, 4, 0],\n        [0, 0, 4, 0],\n        [0, 0, 4, 0],\n        [2, 2, 4, 2]])\n        \"\"\"\n        rv = self._setitem(key, value)\n        if rv is not None:\n            i, j, value = rv\n            if value:\n                self._smat[(i, j)] = value\n            elif (i, j) in self._smat:\n                del self._smat[(i, j)]\n\n    def as_mutable(self):\n        return self.copy()\n\n    __hash__ = None\n\n    def col_del(self, k):\n        \"\"\"Delete the given column of the matrix.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> M = SparseMatrix([[0, 0], [0, 1]])\n        >>> M\n        Matrix([\n        [0, 0],\n        [0, 1]])\n        >>> M.col_del(0)\n        >>> M\n        Matrix([\n        [0],\n        [1]])\n\n        See Also\n        ========\n\n        row_del\n        \"\"\"\n        newD = {}\n        k = a2idx(k, self.cols)\n        for (i, j) in self._smat:\n            if j == k:\n                pass\n            elif j > k:\n                newD[i, j - 1] = self._smat[i, j]\n            else:\n                newD[i, j] = self._smat[i, j]\n        self._smat = newD\n        self.cols -= 1\n\n    def col_join(self, other):\n        \"\"\"Returns B augmented beneath A (row-wise joining)::\n\n            [A]\n            [B]\n\n        Examples\n        ========\n\n        >>> from sympy import SparseMatrix, Matrix, ones\n        >>> A = SparseMatrix(ones(3))\n        >>> A\n        Matrix([\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]])\n        >>> B = SparseMatrix.eye(3)\n        >>> B\n        Matrix([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]])\n        >>> C = A.col_join(B); C\n        Matrix([\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]])\n        >>> C == A.col_join(Matrix(B))\n        True\n\n        Joining along columns is the same as appending rows at the end\n        of the matrix:\n\n        >>> C == A.row_insert(A.rows, Matrix(B))\n        True\n        \"\"\"\n        if not self:\n            return type(self)(other)\n        A, B = self, other\n        if not A.cols == B.cols:\n            raise ShapeError()\n        A = A.copy()\n        if not isinstance(B, SparseMatrix):\n            k = 0\n            b = B._mat\n            for i in range(B.rows):\n                for j in range(B.cols):\n                    v = b[k]\n                    if v:\n                        A._smat[(i + A.rows, j)] = v\n                    k += 1\n        else:\n            for (i, j), v in B._smat.items():\n                A._smat[i + A.rows, j] = v\n        A.rows += B.rows\n        return A\n\n    def col_op(self, j, f):\n        \"\"\"In-place operation on col j using two-arg functor whose args are\n        interpreted as (self[i, j], i) for i in range(self.rows).\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> M = SparseMatrix.eye(3)*2\n        >>> M[1, 0] = -1\n        >>> M.col_op(1, lambda v, i: v + 2*M[i, 0]); M\n        Matrix([\n        [ 2, 4, 0],\n        [-1, 0, 0],\n        [ 0, 0, 2]])\n        \"\"\"\n        for i in range(self.rows):\n            v = self._smat.get((i, j), S.Zero)\n            fv = f(v, i)\n            if fv:\n                self._smat[(i, j)] = fv\n            elif v:\n                self._smat.pop((i, j))\n\n    def col_swap(self, i, j):\n        \"\"\"Swap, in place, columns i and j.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> S = SparseMatrix.eye(3); S[2, 1] = 2\n        >>> S.col_swap(1, 0); S\n        Matrix([\n        [0, 1, 0],\n        [1, 0, 0],\n        [2, 0, 1]])\n        \"\"\"\n        if i > j:\n            i, j = j, i\n        rows = self.col_list()\n        temp = []\n        for ii, jj, v in rows:\n            if jj == i:\n                self._smat.pop((ii, jj))\n                temp.append((ii, v))\n            elif jj == j:\n                self._smat.pop((ii, jj))\n                self._smat[ii, i] = v\n            elif jj > j:\n                break\n        for k, v in temp:\n            self._smat[k, j] = v\n\n    def copyin_list(self, key, value):\n        if not is_sequence(value):\n            raise TypeError(\"`value` must be of type list or tuple.\")\n        self.copyin_matrix(key, Matrix(value))\n\n    def copyin_matrix(self, key, value):\n        # include this here because it's not part of BaseMatrix\n        rlo, rhi, clo, chi = self.key2bounds(key)\n        shape = value.shape\n        dr, dc = rhi - rlo, chi - clo\n        if shape != (dr, dc):\n            raise ShapeError(\n                \"The Matrix `value` doesn't have the same dimensions \"\n                \"as the in sub-Matrix given by `key`.\")\n        if not isinstance(value, SparseMatrix):\n            for i in range(value.rows):\n                for j in range(value.cols):\n                    self[i + rlo, j + clo] = value[i, j]\n        else:\n            if (rhi - rlo)*(chi - clo) < len(self):\n                for i in range(rlo, rhi):\n                    for j in range(clo, chi):\n                        self._smat.pop((i, j), None)\n            else:\n                for i, j, v in self.row_list():\n                    if rlo <= i < rhi and clo <= j < chi:\n                        self._smat.pop((i, j), None)\n            for k, v in value._smat.items():\n                i, j = k\n                self[i + rlo, j + clo] = value[i, j]\n\n(B)         [1]])\n\n        See Also\n        ========\n\n        row_del\n        \"\"\"\n        newD = {}\n        k = a2idx(k, self.cols)\n        for (i, j) in self._smat:\n            if j == k:\n                pass\n            elif j > k:\n                newD[i, j - 1] = self._smat[i, j]\n            else:\n                newD[i, j] = self._smat[i, j]\n        self._smat = newD\n        self.cols -= 1\n\n    def col_join(self, other):\n        \"\"\"Returns B augmented beneath A (row-wise joining)::\n\n            [A]\n            [B]\n\n        Examples\n        ========\n\n        >>> from sympy import SparseMatrix, Matrix, ones\n        >>> A = SparseMatrix(ones(3))\n        >>> A\n        Matrix([\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]])\n        >>> B = SparseMatrix.eye(3)\n        >>> B\n        Matrix([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]])\n        >>> C = A.col_join(B); C\n        Matrix([\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]])\n        >>> C == A.col_join(Matrix(B))\n        True\n\n        Joining along columns is the same as appending rows at the end\n        of the matrix:\n\n        >>> C == A.row_insert(A.rows, Matrix(B))\n        True\n        \"\"\"\n        if not self:\n            return type(self)(other)\n        A, B = self, other\n        if not A.cols == B.cols:\n            raise ShapeError()\n        A = A.copy()\n        if not isinstance(B, SparseMatrix):\n            k = 0\n            b = B._mat\n            for i in range(B.rows):\n                for j in range(B.cols):\n                    v = b[k]\n                    if v:\n                        A._smat[(i + A.rows, j)] = v\n                    k += 1\n        else:\n            for (i, j), v in B._smat.items():\n                A._smat[i + A.rows, j] = v\n        A.rows += B.rows\n        return A\n\n    def col_op(self, j, f):\n        \"\"\"In-place operation on col j using two-arg functor whose args are\n        interpreted as (self[i, j], i) for i in range(self.rows).\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> M = SparseMatrix.eye(3)*2\n        >>> M[1, 0] = -1\n        >>> M.col_op(1, lambda v, i: v + 2*M[i, 0]); M\n        Matrix([\n        [ 2, 4, 0],\n        [-1, 0, 0],\n        [ 0, 0, 2]])\n        \"\"\"\n        for i in range(self.rows):\n            v = self._smat.get((i, j), S.Zero)\n            fv = f(v, i)\n            if fv:\n                self._smat[(i, j)] = fv\n            elif v:\n                self._smat.pop((i, j))\n\n    def col_swap(self, i, j):\n        \"\"\"Swap, in place, columns i and j.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> S = SparseMatrix.eye(3); S[2, 1] = 2\n        >>> S.col_swap(1, 0); S\n        Matrix([\n        [0, 1, 0],\n        [1, 0, 0],\n        [2, 0, 1]])\n        \"\"\"\n        if i > j:\n            i, j = j, i\n        rows = self.col_list()\n        temp = []\n        for ii, jj, v in rows:\n            if jj == i:\n                self._smat.pop((ii, jj))\n                temp.append((ii, v))\n            elif jj == j:\n                self._smat.pop((ii, jj))\n                self._smat[ii, i] = v\n            elif jj > j:\n                break\n        for k, v in temp:\n            self._smat[k, j] = v\n\n    def copyin_list(self, key, value):\n        if not is_sequence(value):\n            raise TypeError(\"`value` must be of type list or tuple.\")\n        self.copyin_matrix(key, Matrix(value))\n\n    def copyin_matrix(self, key, value):\n        # include this here because it's not part of BaseMatrix\n        rlo, rhi, clo, chi = self.key2bounds(key)\n        shape = value.shape\n        dr, dc = rhi - rlo, chi - clo\n        if shape != (dr, dc):\n            raise ShapeError(\n                \"The Matrix `value` doesn't have the same dimensions \"\n                \"as the in sub-Matrix given by `key`.\")\n        if not isinstance(value, SparseMatrix):\n            for i in range(value.rows):\n                for j in range(value.cols):\n                    self[i + rlo, j + clo] = value[i, j]\n        else:\n            if (rhi - rlo)*(chi - clo) < len(self):\n                for i in range(rlo, rhi):\n                    for j in range(clo, chi):\n                        self._smat.pop((i, j), None)\n            else:\n                for i, j, v in self.row_list():\n                    if rlo <= i < rhi and clo <= j < chi:\n                        self._smat.pop((i, j), None)\n            for k, v in value._smat.items():\n                i, j = k\n                self[i + rlo, j + clo] = value[i, j]\n\n    def fill(self, value):\n        \"\"\"Fill self with the given value.\n\n        Notes\n        =====\n\n        Unless many values are going to be deleted (i.e. set to zero)\n        this will create a matrix that is slower than a dense matrix in\n        operations.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> M = SparseMatrix.zeros(3); M\n        Matrix([\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n        >>> M.fill(1); M\n        Matrix([\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]])\n        \"\"\"\n        if not value:\n            self._smat = {}\n        else:\n            v = self._sympify(value)\n            self._smat = dict([((i, j), v)\n                for i in range(self.rows) for j in range(self.cols)])\n\n    def row_del(self, k):\n        \"\"\"Delete the given row of the matrix.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> M = SparseMatrix([[0, 0], [0, 1]])\n        >>> M\n        Matrix([\n        [0, 0],\n        [0, 1]])\n        >>> M.row_del(0)\n        >>> M\n        Matrix([[0, 1]])\n\n        See Also\n        ========\n\n        col_del\n        \"\"\"\n        newD = {}\n(C)         >>> S = SparseMatrix.eye(3); S[2, 1] = 2\n        >>> S.col_swap(1, 0); S\n        Matrix([\n        [0, 1, 0],\n        [1, 0, 0],\n        [2, 0, 1]])\n        \"\"\"\n        if i > j:\n            i, j = j, i\n        rows = self.col_list()\n        temp = []\n        for ii, jj, v in rows:\n            if jj == i:\n                self._smat.pop((ii, jj))\n                temp.append((ii, v))\n            elif jj == j:\n                self._smat.pop((ii, jj))\n                self._smat[ii, i] = v\n            elif jj > j:\n                break\n        for k, v in temp:\n            self._smat[k, j] = v\n\n    def copyin_list(self, key, value):\n        if not is_sequence(value):\n            raise TypeError(\"`value` must be of type list or tuple.\")\n        self.copyin_matrix(key, Matrix(value))\n\n    def copyin_matrix(self, key, value):\n        # include this here because it's not part of BaseMatrix\n        rlo, rhi, clo, chi = self.key2bounds(key)\n        shape = value.shape\n        dr, dc = rhi - rlo, chi - clo\n        if shape != (dr, dc):\n            raise ShapeError(\n                \"The Matrix `value` doesn't have the same dimensions \"\n                \"as the in sub-Matrix given by `key`.\")\n        if not isinstance(value, SparseMatrix):\n            for i in range(value.rows):\n                for j in range(value.cols):\n                    self[i + rlo, j + clo] = value[i, j]\n        else:\n            if (rhi - rlo)*(chi - clo) < len(self):\n                for i in range(rlo, rhi):\n                    for j in range(clo, chi):\n                        self._smat.pop((i, j), None)\n            else:\n                for i, j, v in self.row_list():\n                    if rlo <= i < rhi and clo <= j < chi:\n                        self._smat.pop((i, j), None)\n            for k, v in value._smat.items():\n                i, j = k\n                self[i + rlo, j + clo] = value[i, j]\n\n    def fill(self, value):\n        \"\"\"Fill self with the given value.\n\n        Notes\n        =====\n\n        Unless many values are going to be deleted (i.e. set to zero)\n        this will create a matrix that is slower than a dense matrix in\n        operations.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> M = SparseMatrix.zeros(3); M\n        Matrix([\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n        >>> M.fill(1); M\n        Matrix([\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]])\n        \"\"\"\n        if not value:\n            self._smat = {}\n        else:\n            v = self._sympify(value)\n            self._smat = dict([((i, j), v)\n                for i in range(self.rows) for j in range(self.cols)])\n\n    def row_del(self, k):\n        \"\"\"Delete the given row of the matrix.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> M = SparseMatrix([[0, 0], [0, 1]])\n        >>> M\n        Matrix([\n        [0, 0],\n        [0, 1]])\n        >>> M.row_del(0)\n        >>> M\n        Matrix([[0, 1]])\n\n        See Also\n        ========\n\n        col_del\n        \"\"\"\n        newD = {}\n        k = a2idx(k, self.rows)\n        for (i, j) in self._smat:\n            if i == k:\n                pass\n            elif i > k:\n                newD[i - 1, j] = self._smat[i, j]\n            else:\n                newD[i, j] = self._smat[i, j]\n        self._smat = newD\n        self.rows -= 1\n\n    def row_join(self, other):\n        \"\"\"Returns B appended after A (column-wise augmenting)::\n\n            [A B]\n\n        Examples\n        ========\n\n        >>> from sympy import SparseMatrix, Matrix\n        >>> A = SparseMatrix(((1, 0, 1), (0, 1, 0), (1, 1, 0)))\n        >>> A\n        Matrix([\n        [1, 0, 1],\n        [0, 1, 0],\n        [1, 1, 0]])\n        >>> B = SparseMatrix(((1, 0, 0), (0, 1, 0), (0, 0, 1)))\n        >>> B\n        Matrix([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]])\n        >>> C = A.row_join(B); C\n        Matrix([\n        [1, 0, 1, 1, 0, 0],\n        [0, 1, 0, 0, 1, 0],\n        [1, 1, 0, 0, 0, 1]])\n        >>> C == A.row_join(Matrix(B))\n        True\n\n        Joining at row ends is the same as appending columns at the end\n        of the matrix:\n\n        >>> C == A.col_insert(A.cols, B)\n        True\n        \"\"\"\n        if not self:\n            return type(self)(other)\n        A, B = self, other\n        if not A.rows == B.rows:\n            raise ShapeError()\n        A = A.copy()\n        if not isinstance(B, SparseMatrix):\n            k = 0\n            b = B._mat\n            for i in range(B.rows):\n                for j in range(B.cols):\n                    v = b[k]\n                    if v:\n                        A._smat[(i, j + A.cols)] = v\n                    k += 1\n        else:\n            for (i, j), v in B._smat.items():\n                A._smat[(i, j + A.cols)] = v\n        A.cols += B.cols\n        return A\n\n    def row_op(self, i, f):\n        \"\"\"In-place operation on row ``i`` using two-arg functor whose args are\n        interpreted as ``(self[i, j], j)``.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> M = SparseMatrix.eye(3)*2\n        >>> M[0, 1] = -1\n        >>> M.row_op(1, lambda v, j: v + 2*M[0, j]); M\n        Matrix([\n        [2, -1, 0],\n        [4,  0, 0],\n        [0,  0, 2]])\n\n        See Also\n        ========\n        row\n        zip_row_op\n        col_op\n\n        \"\"\"\n        for j in range(self.cols):\n            v = self._smat.get((i, j), S.Zero)\n            fv = f(v, j)\n            if fv:\n                self._smat[(i, j)] = fv\n            elif v:\n                self._smat.pop((i, j))\n\n    def row_swap(self, i, j):\n        \"\"\"Swap, in place, columns i and j.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> S = SparseMatrix.eye(3); S[2, 1] = 2\n        >>> S.row_swap(1, 0); S\n        Matrix([\n        [0, 1, 0],\n        [1, 0, 0],\n(D)         >>> C == A.row_insert(A.rows, Matrix(B))\n        True\n        \"\"\"\n        if not self:\n            return type(self)(other)\n        A, B = self, other\n        if not A.cols == B.cols:\n            raise ShapeError()\n        A = A.copy()\n        if not isinstance(B, SparseMatrix):\n            k = 0\n            b = B._mat\n            for i in range(B.rows):\n                for j in range(B.cols):\n                    v = b[k]\n                    if v:\n                        A._smat[(i + A.rows, j)] = v\n                    k += 1\n        else:\n            for (i, j), v in B._smat.items():\n                A._smat[i + A.rows, j] = v\n        A.rows += B.rows\n        return A\n\n    def col_op(self, j, f):\n        \"\"\"In-place operation on col j using two-arg functor whose args are\n        interpreted as (self[i, j], i) for i in range(self.rows).\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> M = SparseMatrix.eye(3)*2\n        >>> M[1, 0] = -1\n        >>> M.col_op(1, lambda v, i: v + 2*M[i, 0]); M\n        Matrix([\n        [ 2, 4, 0],\n        [-1, 0, 0],\n        [ 0, 0, 2]])\n        \"\"\"\n        for i in range(self.rows):\n            v = self._smat.get((i, j), S.Zero)\n            fv = f(v, i)\n            if fv:\n                self._smat[(i, j)] = fv\n            elif v:\n                self._smat.pop((i, j))\n\n    def col_swap(self, i, j):\n        \"\"\"Swap, in place, columns i and j.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> S = SparseMatrix.eye(3); S[2, 1] = 2\n        >>> S.col_swap(1, 0); S\n        Matrix([\n        [0, 1, 0],\n        [1, 0, 0],\n        [2, 0, 1]])\n        \"\"\"\n        if i > j:\n            i, j = j, i\n        rows = self.col_list()\n        temp = []\n        for ii, jj, v in rows:\n            if jj == i:\n                self._smat.pop((ii, jj))\n                temp.append((ii, v))\n            elif jj == j:\n                self._smat.pop((ii, jj))\n                self._smat[ii, i] = v\n            elif jj > j:\n                break\n        for k, v in temp:\n            self._smat[k, j] = v\n\n    def copyin_list(self, key, value):\n        if not is_sequence(value):\n            raise TypeError(\"`value` must be of type list or tuple.\")\n        self.copyin_matrix(key, Matrix(value))\n\n    def copyin_matrix(self, key, value):\n        # include this here because it's not part of BaseMatrix\n        rlo, rhi, clo, chi = self.key2bounds(key)\n        shape = value.shape\n        dr, dc = rhi - rlo, chi - clo\n        if shape != (dr, dc):\n            raise ShapeError(\n                \"The Matrix `value` doesn't have the same dimensions \"\n                \"as the in sub-Matrix given by `key`.\")\n        if not isinstance(value, SparseMatrix):\n            for i in range(value.rows):\n                for j in range(value.cols):\n                    self[i + rlo, j + clo] = value[i, j]\n        else:\n            if (rhi - rlo)*(chi - clo) < len(self):\n                for i in range(rlo, rhi):\n                    for j in range(clo, chi):\n                        self._smat.pop((i, j), None)\n            else:\n                for i, j, v in self.row_list():\n                    if rlo <= i < rhi and clo <= j < chi:\n                        self._smat.pop((i, j), None)\n            for k, v in value._smat.items():\n                i, j = k\n                self[i + rlo, j + clo] = value[i, j]\n\n    def fill(self, value):\n        \"\"\"Fill self with the given value.\n\n        Notes\n        =====\n\n        Unless many values are going to be deleted (i.e. set to zero)\n        this will create a matrix that is slower than a dense matrix in\n        operations.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> M = SparseMatrix.zeros(3); M\n        Matrix([\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n        >>> M.fill(1); M\n        Matrix([\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]])\n        \"\"\"\n        if not value:\n            self._smat = {}\n        else:\n            v = self._sympify(value)\n            self._smat = dict([((i, j), v)\n                for i in range(self.rows) for j in range(self.cols)])\n\n    def row_del(self, k):\n        \"\"\"Delete the given row of the matrix.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import SparseMatrix\n        >>> M = SparseMatrix([[0, 0], [0, 1]])\n        >>> M\n        Matrix([\n        [0, 0],\n        [0, 1]])\n        >>> M.row_del(0)\n        >>> M\n        Matrix([[0, 1]])\n\n        See Also\n        ========\n\n        col_del\n        \"\"\"\n        newD = {}\n        k = a2idx(k, self.rows)\n        for (i, j) in self._smat:\n            if i == k:\n                pass\n            elif i > k:\n                newD[i - 1, j] = self._smat[i, j]\n            else:\n                newD[i, j] = self._smat[i, j]\n        self._smat = newD\n        self.rows -= 1\n\n    def row_join(self, other):\n        \"\"\"Returns B appended after A (column-wise augmenting)::\n\n            [A B]\n\n        Examples\n        ========\n\n        >>> from sympy import SparseMatrix, Matrix\n        >>> A = SparseMatrix(((1, 0, 1), (0, 1, 0), (1, 1, 0)))\n        >>> A\n        Matrix([\n        [1, 0, 1],\n        [0, 1, 0],\n        [1, 1, 0]])\n        >>> B = SparseMatrix(((1, 0, 0), (0, 1, 0), (0, 0, 1)))\n        >>> B\n        Matrix([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]])\n        >>> C = A.row_join(B); C\n        Matrix([\n        [1, 0, 1, 1, 0, 0],\n        [0, 1, 0, 0, 1, 0],\n        [1, 1, 0, 0, 0, 1]])\n        >>> C == A.row_join(Matrix(B))\n        True\n\n        Joining at row ends is the same as appending columns at the end\n        of the matrix:\n\n        >>> C == A.col_insert(A.cols, B)\n        True\n        \"\"\"\n        if not self:\n            return type(self)(other)\n        A, B = self, other\n        if not A.rows == B.rows:\n            raise ShapeError()\n        A = A.copy()\n        if not isinstance(B, SparseMatrix):\n            k = 0\n            b = B._mat", "answer": "D"}
{"uuid": "d5dd088e-f85b-4bd4-a2c6-5033b95afbb5", "issue_statement": "decompose() function in intpoly returns a list of arbitrary order\nThe decompose() function, with separate=True, returns `list(poly_dict.values())`, which is ordered arbitrarily.  \r\n\r\nWhat is this used for? It should be sorted somehow, or returning a set (in which case, why not just use the returned dictionary and have the caller take the values). This is causing test failures for me after some changes to the core. \r\n\r\nCC @ArifAhmed1995 @certik", "choices": "(A)     expr : Polynomial(SymPy expression)\n\n    Optional Parameters :\n\n    separate : If True then simply return a list of the constituent monomials\n               If not then break up the polynomial into constituent homogeneous\n               polynomials.\n    Examples\n    ========\n    >>> from sympy.abc import x, y\n    >>> from sympy.integrals.intpoly import decompose\n    >>> decompose(x**2 + x*y + x + y + x**3*y**2 + y**5)\n    {1: x + y, 2: x**2 + x*y, 5: x**3*y**2 + y**5}\n    >>> decompose(x**2 + x*y + x + y + x**3*y**2 + y**5, True)\n    [x, y, x**2, y**5, x*y, x**3*y**2]\n    \"\"\"\n    expr = S(expr)\n    poly_dict = {}\n\n    if isinstance(expr, Expr) and not expr.is_number:\n        if expr.is_Symbol:\n            poly_dict[1] = expr\n        elif expr.is_Add:\n            symbols = expr.atoms(Symbol)\n            degrees = [(sum(degree_list(monom, *symbols)), monom)\n                       for monom in expr.args]\n            if separate:\n                return [monom[1] for monom in degrees]\n            else:\n                for monom in degrees:\n                    degree, term = monom\n                    if poly_dict.get(degree):\n                        poly_dict[degree] += term\n                    else:\n                        poly_dict[degree] = term\n        elif expr.is_Pow:\n            _, degree = expr.args\n            poly_dict[degree] = expr\n        else:  # Now expr can only be of `Mul` type\n            degree = 0\n            for term in expr.args:\n                term_type = len(term.args)\n                if term_type == 0 and term.is_Symbol:\n                    degree += 1\n(B)         elif expr.is_Add:\n            symbols = expr.atoms(Symbol)\n            degrees = [(sum(degree_list(monom, *symbols)), monom)\n                       for monom in expr.args]\n            if separate:\n                return [monom[1] for monom in degrees]\n            else:\n                for monom in degrees:\n                    degree, term = monom\n                    if poly_dict.get(degree):\n                        poly_dict[degree] += term\n                    else:\n                        poly_dict[degree] = term\n        elif expr.is_Pow:\n            _, degree = expr.args\n            poly_dict[degree] = expr\n        else:  # Now expr can only be of `Mul` type\n            degree = 0\n            for term in expr.args:\n                term_type = len(term.args)\n                if term_type == 0 and term.is_Symbol:\n                    degree += 1\n                elif term_type == 2:\n                    degree += term.args[1]\n            poly_dict[degree] = expr\n    else:\n        poly_dict[0] = expr\n\n    if separate:\n        return list(poly_dict.values())\n    return poly_dict\n\n\ndef clockwise_sort(poly):\n    \"\"\"Returns the same polygon with points sorted in clockwise order.\n\n    Note that it's necessary for input points to be sorted in some order\n    (clockwise or anti-clockwise) for the algorithm to work. As a convention\n    algorithm has been implemented keeping clockwise orientation in mind.\n\n    Parameters\n    ==========\n    poly: 2-Polytope\n\n(C)     >>> decompose(x**2 + x*y + x + y + x**3*y**2 + y**5)\n    {1: x + y, 2: x**2 + x*y, 5: x**3*y**2 + y**5}\n    >>> decompose(x**2 + x*y + x + y + x**3*y**2 + y**5, True)\n    [x, y, x**2, y**5, x*y, x**3*y**2]\n    \"\"\"\n    expr = S(expr)\n    poly_dict = {}\n\n    if isinstance(expr, Expr) and not expr.is_number:\n        if expr.is_Symbol:\n            poly_dict[1] = expr\n        elif expr.is_Add:\n            symbols = expr.atoms(Symbol)\n            degrees = [(sum(degree_list(monom, *symbols)), monom)\n                       for monom in expr.args]\n            if separate:\n                return [monom[1] for monom in degrees]\n            else:\n                for monom in degrees:\n                    degree, term = monom\n                    if poly_dict.get(degree):\n                        poly_dict[degree] += term\n                    else:\n                        poly_dict[degree] = term\n        elif expr.is_Pow:\n            _, degree = expr.args\n            poly_dict[degree] = expr\n        else:  # Now expr can only be of `Mul` type\n            degree = 0\n            for term in expr.args:\n                term_type = len(term.args)\n                if term_type == 0 and term.is_Symbol:\n                    degree += 1\n                elif term_type == 2:\n                    degree += term.args[1]\n            poly_dict[degree] = expr\n    else:\n        poly_dict[0] = expr\n\n    if separate:\n        return list(poly_dict.values())\n    return poly_dict\n\n\n(D)                     else:\n                        poly_dict[degree] = term\n        elif expr.is_Pow:\n            _, degree = expr.args\n            poly_dict[degree] = expr\n        else:  # Now expr can only be of `Mul` type\n            degree = 0\n            for term in expr.args:\n                term_type = len(term.args)\n                if term_type == 0 and term.is_Symbol:\n                    degree += 1\n                elif term_type == 2:\n                    degree += term.args[1]\n            poly_dict[degree] = expr\n    else:\n        poly_dict[0] = expr\n\n    if separate:\n        return list(poly_dict.values())\n    return poly_dict\n\n\ndef clockwise_sort(poly):\n    \"\"\"Returns the same polygon with points sorted in clockwise order.\n\n    Note that it's necessary for input points to be sorted in some order\n    (clockwise or anti-clockwise) for the algorithm to work. As a convention\n    algorithm has been implemented keeping clockwise orientation in mind.\n\n    Parameters\n    ==========\n    poly: 2-Polytope\n\n    Examples\n    ========\n    >>> from sympy.integrals.intpoly import clockwise_sort\n    >>> from sympy.geometry.point import Point\n    >>> from sympy.geometry.polygon import Polygon\n    >>> clockwise_sort(Polygon(Point(0, 0), Point(1, 0), Point(1, 1)))\n    Triangle(Point2D(1, 1), Point2D(1, 0), Point2D(0, 0))\n\n    \"\"\"\n    n = len(poly.vertices)\n    vertices = list(poly.vertices)", "answer": "C"}
{"uuid": "3c8c620c-5ebf-40f7-8512-efedb285e02e", "issue_statement": "Exponent doesn't fully simplify\nSay I have code like this:\n\n```\nimport sympy\nfrom sympy import *\nx=Symbol('x')\nexpr1 = S(1)/2*x**2.5\nexpr2 = S(1)*x**(S(5)/2)/2\nres = expr1-expr2\nres= simplify(res.evalf(5))\nprint res\n```\n\nThe output is\n`-0.5*x**2.5 + 0.5*x**2.5`\nHow do I simplify it to 0?", "choices": "(A)                         args.append(a)\n                    else:\n                        args.append(newa)\n                if not _aresame(tuple(args), tail_args):\n                    tail = self.func(*args)\n                return self.func(x, tail)\n\n        # this is the same as above, but there were no pure-number args to\n        # deal with\n        args = []\n        for a in self.args:\n            newa = a._eval_evalf(prec)\n            if newa is None:\n                args.append(a)\n            else:\n                args.append(newa)\n        if not _aresame(tuple(args), self.args):\n            return self.func(*args)\n(B)         args = []\n        for a in self.args:\n            newa = a._eval_evalf(prec)\n            if newa is None:\n                args.append(a)\n            else:\n                args.append(newa)\n        if not _aresame(tuple(args), self.args):\n            return self.func(*args)\n        return self\n\n    @classmethod\n    def make_args(cls, expr):\n        \"\"\"\n        Return a sequence of elements `args` such that cls(*args) == expr\n\n        >>> from sympy import Symbol, Mul, Add\n        >>> x, y = map(Symbol, 'xy')\n(C)                     # are dealing with and all other _eval_evalf routines should\n                    # be doing the same thing (i.e. taking binary prec and\n                    # finding the evalf-able args)\n                    newa = a._eval_evalf(prec)\n                    if newa is None:\n                        args.append(a)\n                    else:\n                        args.append(newa)\n                if not _aresame(tuple(args), tail_args):\n                    tail = self.func(*args)\n                return self.func(x, tail)\n\n        # this is the same as above, but there were no pure-number args to\n        # deal with\n        args = []\n        for a in self.args:\n            newa = a._eval_evalf(prec)\n            if newa is None:\n(D)                 return self.func(x, tail)\n\n        # this is the same as above, but there were no pure-number args to\n        # deal with\n        args = []\n        for a in self.args:\n            newa = a._eval_evalf(prec)\n            if newa is None:\n                args.append(a)\n            else:\n                args.append(newa)\n        if not _aresame(tuple(args), self.args):\n            return self.func(*args)\n        return self\n\n    @classmethod\n    def make_args(cls, expr):\n        \"\"\"", "answer": "A"}
{"uuid": "503b66f6-c9b5-42d3-84c7-37c191159e1d", "issue_statement": "Mod(x**2, x) is not (always) 0\nWhen the base is not an integer, `x**2 % x` is not 0. The base is not tested to be an integer in Mod's eval logic:\r\n\r\n```\r\nif (p == q or p == -q or\r\n        p.is_Pow and p.exp.is_Integer and p.base == q or\r\n        p.is_integer and q == 1):\r\n    return S.Zero\r\n```\r\n\r\nso\r\n\r\n```\r\n>>> Mod(x**2, x)\r\n0\r\n```\r\nbut\r\n```\r\n>>> x = S(1.5)\r\n>>> Mod(x**2, x)\r\n0.75\r\n```", "choices": "(A)             if p.is_infinite or q.is_infinite or p is nan or q is nan:\n                return nan\n            if (p == q or p == -q or\n                    p.is_Pow and p.exp.is_Integer and p.base == q or\n                    p.is_integer and q == 1):\n                return S.Zero\n\n            if q.is_Number:\n(B)                     p.is_integer and q == 1):\n                return S.Zero\n\n            if q.is_Number:\n                if p.is_Number:\n                    return (p % q)\n                if q == 2:\n                    if p.is_even:\n(C)             if (p == q or p == -q or\n                    p.is_Pow and p.exp.is_Integer and p.base == q or\n                    p.is_integer and q == 1):\n                return S.Zero\n\n            if q.is_Number:\n                if p.is_Number:\n                    return (p % q)\n(D)             \"\"\"\n\n            if p.is_infinite or q.is_infinite or p is nan or q is nan:\n                return nan\n            if (p == q or p == -q or\n                    p.is_Pow and p.exp.is_Integer and p.base == q or\n                    p.is_integer and q == 1):\n                return S.Zero", "answer": "A"}
{"uuid": "c9f6eb65-3650-4700-ab40-a4a4dc6427cd", "issue_statement": "bell(n).limit(n, oo) should be oo rather than bell(oo)\n`bell(n).limit(n,oo)` should take the value infinity, but the current output is `bell(oo)`. As the Bell numbers represent the number of partitions of a set, it seems natural that `bell(oo)` should be able to be evaluated rather than be returned unevaluated. This issue is also in line with the recent fixes to the corresponding limit for the Fibonacci numbers and Lucas numbers.\n\n```\nfrom sympy import *\nn = symbols('n')\nbell(n).limit(n,oo)\n\nOutput:\nbell(oo)\n```\n\nI'm new to Sympy, so I'd appreciate the opportunity to fix this bug myself if that's alright.", "choices": "(A)             return S.One\n        elif (n == 0) or (k == 0):\n            return S.Zero\n        s = S.Zero\n        a = S.One\n        for m in range(1, n - k + 2):\n            s += a * bell._bell_incomplete_poly(\n                n - m, k - 1, symbols) * symbols[m - 1]\n            a = a * (n - m) / m\n        return expand_mul(s)\n\n    @classmethod\n    def eval(cls, n, k_sym=None, symbols=None):\n        if n.is_Integer and n.is_nonnegative:\n            if k_sym is None:\n(B)         a = S.One\n        for m in range(1, n - k + 2):\n            s += a * bell._bell_incomplete_poly(\n                n - m, k - 1, symbols) * symbols[m - 1]\n            a = a * (n - m) / m\n        return expand_mul(s)\n\n    @classmethod\n    def eval(cls, n, k_sym=None, symbols=None):\n        if n.is_Integer and n.is_nonnegative:\n            if k_sym is None:\n                return Integer(cls._bell(int(n)))\n            elif symbols is None:\n                return cls._bell_poly(int(n)).subs(_sym, k_sym)\n            else:\n(C)             a = a * (n - m) / m\n        return expand_mul(s)\n\n    @classmethod\n    def eval(cls, n, k_sym=None, symbols=None):\n        if n.is_Integer and n.is_nonnegative:\n            if k_sym is None:\n                return Integer(cls._bell(int(n)))\n            elif symbols is None:\n                return cls._bell_poly(int(n)).subs(_sym, k_sym)\n            else:\n                r = cls._bell_incomplete_poly(int(n), int(k_sym), symbols)\n                return r\n\n    def _eval_rewrite_as_Sum(self, n, k_sym=None, symbols=None):\n(D)             B_{0,k} = 0; for k>=1\n\n        \"\"\"\n        if (n == 0) and (k == 0):\n            return S.One\n        elif (n == 0) or (k == 0):\n            return S.Zero\n        s = S.Zero\n        a = S.One\n        for m in range(1, n - k + 2):\n            s += a * bell._bell_incomplete_poly(\n                n - m, k - 1, symbols) * symbols[m - 1]\n            a = a * (n - m) / m\n        return expand_mul(s)\n", "answer": "A"}
{"uuid": "d70e16a8-550c-4d60-b737-65a7b58e0948", "issue_statement": "Python 2->3 pickle fails with float-containing expressions\nDumping a pickled sympy expression containing a float in Python 2, then loading it in Python 3 generates an error.\r\n\r\nHere is a minimum working example, verified with sympy git commit 3546ac7 (master at time of writing), Python 2.7 and Python 3.6:\r\n\r\n```python\r\npython2 -c 'import pickle; import sympy; x = sympy.symbols(\"x\"); print pickle.dumps(x + 1.0, 2)' | python3 -c 'import pickle; import sys; print(pickle.loads(sys.stdin.buffer.read()))'\r\n```\r\n\r\nand the result:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Users/alex/git/VU/sympy/sympy/core/numbers.py\", line 1045, in __new__\r\n    num[1] = long(num[1], 16)\r\nValueError: invalid literal for int() with base 16: '1L'\r\n```", "choices": "(A)                 num[1] = long(num[1], 16)\n                _mpf_ = tuple(num)\n            else:\n                if len(num) == 4:\n                    # handle normalization hack\n                    return Float._new(num, precision)\n                else:\n                    return (S.NegativeOne**num[0]*num[1]*S(2)**num[2]).evalf(precision)\n        else:\n            try:\n                _mpf_ = num._as_mpf_val(precision)\n(B)                 # it's a hexadecimal (coming from a pickled object)\n                # assume that it is in standard form\n                num = list(num)\n                num[1] = long(num[1], 16)\n                _mpf_ = tuple(num)\n            else:\n                if len(num) == 4:\n                    # handle normalization hack\n                    return Float._new(num, precision)\n                else:\n                    return (S.NegativeOne**num[0]*num[1]*S(2)**num[2]).evalf(precision)\n(C)                 raise ValueError(\"unexpected decimal value %s\" % str(num))\n        elif isinstance(num, tuple) and len(num) in (3, 4):\n            if type(num[1]) is str:\n                # it's a hexadecimal (coming from a pickled object)\n                # assume that it is in standard form\n                num = list(num)\n                num[1] = long(num[1], 16)\n                _mpf_ = tuple(num)\n            else:\n                if len(num) == 4:\n                    # handle normalization hack\n(D)                 if len(num) == 4:\n                    # handle normalization hack\n                    return Float._new(num, precision)\n                else:\n                    return (S.NegativeOne**num[0]*num[1]*S(2)**num[2]).evalf(precision)\n        else:\n            try:\n                _mpf_ = num._as_mpf_val(precision)\n            except (NotImplementedError, AttributeError):\n                _mpf_ = mpmath.mpf(num, prec=precision)._mpf_\n", "answer": "B"}
{"uuid": "f09fca3d-9efb-4542-a41f-500eae82b9e6", "issue_statement": ".subs on coth(log(tan(x))) errors for certain integral values\n    >>> from sympy import *\r\n    >>> x = Symbol('x')\r\n    >>> e = coth(log(tan(x)))\r\n    >>> print(e.subs(x, 2))\r\n    ...\r\n    File \"C:\\Users\\E\\Desktop\\sympy-master\\sympy\\functions\\elementary\\hyperbolic.py\", line 590, in eval\r\n        if cotm is S.ComplexInfinity:\r\n    NameError: name 'cotm' is not defined\r\n\r\nFails for 2, 3, 5, 6, 8, 9, 11, 12, 13, 15, 18, ... etc.", "choices": "(A)                     cothm = coth(m)\n                    if cotm is S.ComplexInfinity:\n                        return coth(x)\n                    else: # cothm == 0\n                        return tanh(x)\n\n            if arg.func == asinh:\n(B)                         return coth(x)\n                    else: # cothm == 0\n                        return tanh(x)\n\n            if arg.func == asinh:\n                x = arg.args[0]\n                return sqrt(1 + x**2)/x\n(C)                 x, m = _peeloff_ipi(arg)\n                if m:\n                    cothm = coth(m)\n                    if cotm is S.ComplexInfinity:\n                        return coth(x)\n                    else: # cothm == 0\n                        return tanh(x)\n(D) \n            if arg.is_Add:\n                x, m = _peeloff_ipi(arg)\n                if m:\n                    cothm = coth(m)\n                    if cotm is S.ComplexInfinity:\n                        return coth(x)", "answer": "C"}
{"uuid": "5d6de4f9-776d-4d01-81bd-111b92d88fe2", "issue_statement": "Matrix.col_insert() no longer seems to work correctly.\nExample:\r\n\r\n```\r\nIn [28]: import sympy as sm\r\n\r\nIn [29]: M = sm.eye(6)\r\n\r\nIn [30]: M\r\nOut[30]: \r\n\u23a11  0  0  0  0  0\u23a4\r\n\u23a2                \u23a5\r\n\u23a20  1  0  0  0  0\u23a5\r\n\u23a2                \u23a5\r\n\u23a20  0  1  0  0  0\u23a5\r\n\u23a2                \u23a5\r\n\u23a20  0  0  1  0  0\u23a5\r\n\u23a2                \u23a5\r\n\u23a20  0  0  0  1  0\u23a5\r\n\u23a2                \u23a5\r\n\u23a30  0  0  0  0  1\u23a6\r\n\r\nIn [31]: V = 2 * sm.ones(6, 2)\r\n\r\nIn [32]: V\r\nOut[32]: \r\n\u23a12  2\u23a4\r\n\u23a2    \u23a5\r\n\u23a22  2\u23a5\r\n\u23a2    \u23a5\r\n\u23a22  2\u23a5\r\n\u23a2    \u23a5\r\n\u23a22  2\u23a5\r\n\u23a2    \u23a5\r\n\u23a22  2\u23a5\r\n\u23a2    \u23a5\r\n\u23a32  2\u23a6\r\n\r\nIn [33]: M.col_insert(3, V)\r\nOut[33]: \r\n\u23a11  0  0  2  2  1  0  0\u23a4\r\n\u23a2                      \u23a5\r\n\u23a20  1  0  2  2  0  1  0\u23a5\r\n\u23a2                      \u23a5\r\n\u23a20  0  1  2  2  0  0  1\u23a5\r\n\u23a2                      \u23a5\r\n\u23a20  0  0  2  2  0  0  0\u23a5\r\n\u23a2                      \u23a5\r\n\u23a20  0  0  2  2  0  0  0\u23a5\r\n\u23a2                      \u23a5\r\n\u23a30  0  0  2  2  0  0  0\u23a6\r\nIn [34]: sm.__version__\r\nOut[34]: '1.1.1'\r\n```\r\n\r\nThe 3 x 3 identify matrix to the right of the columns of twos is shifted from the bottom three rows to the top three rows.\r\n\r\n@siefkenj Do you think this has to do with your matrix refactor?", "choices": "(A)                 return other[i, j - pos]\n            return self[i, j - pos - other.cols]\n\n        return self._new(self.rows, self.cols + other.cols,\n                         lambda i, j: entry(i, j))\n\n    def _eval_col_join(self, other):\n(B)         def entry(i, j):\n            if j < pos:\n                return self[i, j]\n            elif pos <= j < pos + other.cols:\n                return other[i, j - pos]\n            return self[i, j - pos - other.cols]\n\n(C)                 return self[i, j]\n            elif pos <= j < pos + other.cols:\n                return other[i, j - pos]\n            return self[i, j - pos - other.cols]\n\n        return self._new(self.rows, self.cols + other.cols,\n                         lambda i, j: entry(i, j))\n(D) \n        return self._new(self.rows, self.cols + other.cols,\n                         lambda i, j: entry(i, j))\n\n    def _eval_col_join(self, other):\n        rows = self.rows\n", "answer": "C"}
{"uuid": "5da23375-286a-4ef6-bb84-302fb2af52eb", "issue_statement": "@ (__matmul__) should fail if one argument is not a matrix\n```\r\n>>> A = Matrix([[1, 2], [3, 4]])\r\n>>> B = Matrix([[2, 3], [1, 2]])\r\n>>> A@B\r\nMatrix([\r\n[ 4,  7],\r\n[10, 17]])\r\n>>> 2@B\r\nMatrix([\r\n[4, 6],\r\n[2, 4]])\r\n```\r\n\r\nRight now `@` (`__matmul__`) just copies `__mul__`, but it should actually only work if the multiplication is actually a matrix multiplication. \r\n\r\nThis is also how NumPy works\r\n\r\n```\r\n>>> import numpy as np\r\n>>> a = np.array([[1, 2], [3, 4]])\r\n>>> 2*a\r\narray([[2, 4],\r\n       [6, 8]])\r\n>>> 2@a\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nValueError: Scalar operands are not allowed, use '*' instead\r\n```", "choices": "(A) \n        return NotImplemented\n\n    def __neg__(self):\n        return self._eval_scalar_mul(-1)\n\n    @call_highest_priority('__rpow__')\n    def __pow__(self, num):\n        if not self.rows == self.cols:\n            raise NonSquareMatrixError()\n        try:\n            a = self\n            num = sympify(num)\n            if num.is_Number and num % 1 == 0:\n                if a.rows == 1:\n                    return a._new([[a[0]**num]])\n                if num == 0:\n                    return self._new(self.rows, self.cols, lambda i, j: int(i == j))\n                if num < 0:\n                    num = -num\n                    a = a.inv()\n                # When certain conditions are met,\n                # Jordan block algorithm is faster than\n                # computation by recursion.\n                elif a.rows == 2 and num > 100000:\n                    try:\n                        return a._matrix_pow_by_jordan_blocks(num)\n                    except (AttributeError, MatrixError):\n                        pass\n                return a._eval_pow_by_recursion(num)\n            elif isinstance(num, (Expr, float)):\n                return a._matrix_pow_by_jordan_blocks(num)\n            else:\n                raise TypeError(\n                    \"Only SymPy expressions or integers are supported as exponent for matrices\")\n        except AttributeError:\n            raise TypeError(\"Don't know how to raise {} to {}\".format(self.__class__, num))\n\n    @call_highest_priority('__add__')\n    def __radd__(self, other):\n        return self + other\n\n    @call_highest_priority('__matmul__')\n    def __rmatmul__(self, other):\n        return self.__rmul__(other)\n\n    @call_highest_priority('__mul__')\n    def __rmul__(self, other):\n        other = _matrixify(other)\n        # matrix-like objects can have shapes.  This is\n        # our first sanity check.\n        if hasattr(other, 'shape') and len(other.shape) == 2:\n            if self.shape[0] != other.shape[1]:\n                raise ShapeError(\"Matrix size mismatch.\")\n\n        # honest sympy matrices defer to their class's routine\n        if getattr(other, 'is_Matrix', False):\n            return other._new(other.as_mutable() * self)\n        # Matrix-like objects can be passed to CommonMatrix routines directly.\n        if getattr(other, 'is_MatrixLike', False):\n            return MatrixArithmetic._eval_matrix_rmul(self, other)\n\n        # if 'other' is not iterable then scalar multiplication.\n        if not isinstance(other, collections.Iterable):\n            try:\n                return self._eval_scalar_rmul(other)\n            except TypeError:\n                pass\n\n        return NotImplemented\n\n    @call_highest_priority('__sub__')\n    def __rsub__(self, a):\n        return (-self) + a\n\n    @call_highest_priority('__rsub__')\n    def __sub__(self, a):\n        return self + (-a)\n\n    @call_highest_priority('__rtruediv__')\n    def __truediv__(self, other):\n        return self.__div__(other)\n\n    def multiply_elementwise(self, other):\n        \"\"\"Return the Hadamard product (elementwise product) of A and B\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> A = Matrix([[0, 1, 2], [3, 4, 5]])\n        >>> B = Matrix([[1, 10, 100], [100, 10, 1]])\n        >>> A.multiply_elementwise(B)\n        Matrix([\n        [  0, 10, 200],\n        [300, 40,   5]])\n\n        See Also\n        ========\n\n        cross\n        dot\n        multiply\n        \"\"\"\n        if self.shape != other.shape:\n            raise ShapeError(\"Matrix shapes must agree {} != {}\".format(self.shape, other.shape))\n\n(B) \n        See Also\n        ========\n\n        matrix_multiply_elementwise\n        \"\"\"\n        other = _matrixify(other)\n        # matrix-like objects can have shapes.  This is\n        # our first sanity check.\n        if hasattr(other, 'shape') and len(other.shape) == 2:\n            if self.shape[1] != other.shape[0]:\n                raise ShapeError(\"Matrix size mismatch: %s * %s.\" % (\n                    self.shape, other.shape))\n\n        # honest sympy matrices defer to their class's routine\n        if getattr(other, 'is_Matrix', False):\n            return self._eval_matrix_mul(other)\n        # Matrix-like objects can be passed to CommonMatrix routines directly.\n        if getattr(other, 'is_MatrixLike', False):\n            return MatrixArithmetic._eval_matrix_mul(self, other)\n\n        # if 'other' is not iterable then scalar multiplication.\n        if not isinstance(other, collections.Iterable):\n            try:\n                return self._eval_scalar_mul(other)\n            except TypeError:\n                pass\n\n        return NotImplemented\n\n    def __neg__(self):\n        return self._eval_scalar_mul(-1)\n\n    @call_highest_priority('__rpow__')\n    def __pow__(self, num):\n        if not self.rows == self.cols:\n            raise NonSquareMatrixError()\n        try:\n            a = self\n            num = sympify(num)\n            if num.is_Number and num % 1 == 0:\n                if a.rows == 1:\n                    return a._new([[a[0]**num]])\n                if num == 0:\n                    return self._new(self.rows, self.cols, lambda i, j: int(i == j))\n                if num < 0:\n                    num = -num\n                    a = a.inv()\n                # When certain conditions are met,\n                # Jordan block algorithm is faster than\n                # computation by recursion.\n                elif a.rows == 2 and num > 100000:\n                    try:\n                        return a._matrix_pow_by_jordan_blocks(num)\n                    except (AttributeError, MatrixError):\n                        pass\n                return a._eval_pow_by_recursion(num)\n            elif isinstance(num, (Expr, float)):\n                return a._matrix_pow_by_jordan_blocks(num)\n            else:\n                raise TypeError(\n                    \"Only SymPy expressions or integers are supported as exponent for matrices\")\n        except AttributeError:\n            raise TypeError(\"Don't know how to raise {} to {}\".format(self.__class__, num))\n\n    @call_highest_priority('__add__')\n    def __radd__(self, other):\n        return self + other\n\n    @call_highest_priority('__matmul__')\n    def __rmatmul__(self, other):\n        return self.__rmul__(other)\n\n    @call_highest_priority('__mul__')\n    def __rmul__(self, other):\n        other = _matrixify(other)\n        # matrix-like objects can have shapes.  This is\n        # our first sanity check.\n        if hasattr(other, 'shape') and len(other.shape) == 2:\n            if self.shape[0] != other.shape[1]:\n                raise ShapeError(\"Matrix size mismatch.\")\n\n        # honest sympy matrices defer to their class's routine\n        if getattr(other, 'is_Matrix', False):\n            return other._new(other.as_mutable() * self)\n        # Matrix-like objects can be passed to CommonMatrix routines directly.\n        if getattr(other, 'is_MatrixLike', False):\n            return MatrixArithmetic._eval_matrix_rmul(self, other)\n\n        # if 'other' is not iterable then scalar multiplication.\n        if not isinstance(other, collections.Iterable):\n            try:\n                return self._eval_scalar_rmul(other)\n            except TypeError:\n                pass\n\n        return NotImplemented\n\n    @call_highest_priority('__sub__')\n    def __rsub__(self, a):\n        return (-self) + a\n\n    @call_highest_priority('__rsub__')\n    def __sub__(self, a):\n        return self + (-a)\n\n    @call_highest_priority('__rtruediv__')\n(C) \n    @call_highest_priority('__rmatmul__')\n    def __matmul__(self, other):\n        return self.__mul__(other)\n\n    @call_highest_priority('__rmul__')\n    def __mul__(self, other):\n        \"\"\"Return self*other where other is either a scalar or a matrix\n        of compatible dimensions.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> A = Matrix([[1, 2, 3], [4, 5, 6]])\n        >>> 2*A == A*2 == Matrix([[2, 4, 6], [8, 10, 12]])\n        True\n        >>> B = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        >>> A*B\n        Matrix([\n        [30, 36, 42],\n        [66, 81, 96]])\n        >>> B*A\n        Traceback (most recent call last):\n        ...\n        ShapeError: Matrices size mismatch.\n        >>>\n\n        See Also\n        ========\n\n        matrix_multiply_elementwise\n        \"\"\"\n        other = _matrixify(other)\n        # matrix-like objects can have shapes.  This is\n        # our first sanity check.\n        if hasattr(other, 'shape') and len(other.shape) == 2:\n            if self.shape[1] != other.shape[0]:\n                raise ShapeError(\"Matrix size mismatch: %s * %s.\" % (\n                    self.shape, other.shape))\n\n        # honest sympy matrices defer to their class's routine\n        if getattr(other, 'is_Matrix', False):\n            return self._eval_matrix_mul(other)\n        # Matrix-like objects can be passed to CommonMatrix routines directly.\n        if getattr(other, 'is_MatrixLike', False):\n            return MatrixArithmetic._eval_matrix_mul(self, other)\n\n        # if 'other' is not iterable then scalar multiplication.\n        if not isinstance(other, collections.Iterable):\n            try:\n                return self._eval_scalar_mul(other)\n            except TypeError:\n                pass\n\n        return NotImplemented\n\n    def __neg__(self):\n        return self._eval_scalar_mul(-1)\n\n    @call_highest_priority('__rpow__')\n    def __pow__(self, num):\n        if not self.rows == self.cols:\n            raise NonSquareMatrixError()\n        try:\n            a = self\n            num = sympify(num)\n            if num.is_Number and num % 1 == 0:\n                if a.rows == 1:\n                    return a._new([[a[0]**num]])\n                if num == 0:\n                    return self._new(self.rows, self.cols, lambda i, j: int(i == j))\n                if num < 0:\n                    num = -num\n                    a = a.inv()\n                # When certain conditions are met,\n                # Jordan block algorithm is faster than\n                # computation by recursion.\n                elif a.rows == 2 and num > 100000:\n                    try:\n                        return a._matrix_pow_by_jordan_blocks(num)\n                    except (AttributeError, MatrixError):\n                        pass\n                return a._eval_pow_by_recursion(num)\n            elif isinstance(num, (Expr, float)):\n                return a._matrix_pow_by_jordan_blocks(num)\n            else:\n                raise TypeError(\n                    \"Only SymPy expressions or integers are supported as exponent for matrices\")\n        except AttributeError:\n            raise TypeError(\"Don't know how to raise {} to {}\".format(self.__class__, num))\n\n    @call_highest_priority('__add__')\n    def __radd__(self, other):\n        return self + other\n\n    @call_highest_priority('__matmul__')\n    def __rmatmul__(self, other):\n        return self.__rmul__(other)\n\n    @call_highest_priority('__mul__')\n    def __rmul__(self, other):\n        other = _matrixify(other)\n        # matrix-like objects can have shapes.  This is\n        # our first sanity check.\n        if hasattr(other, 'shape') and len(other.shape) == 2:\n            if self.shape[0] != other.shape[1]:\n(D)     @call_highest_priority('__radd__')\n    def __add__(self, other):\n        \"\"\"Return self + other, raising ShapeError if shapes don't match.\"\"\"\n        other = _matrixify(other)\n        # matrix-like objects can have shapes.  This is\n        # our first sanity check.\n        if hasattr(other, 'shape'):\n            if self.shape != other.shape:\n                raise ShapeError(\"Matrix size mismatch: %s + %s\" % (\n                    self.shape, other.shape))\n\n        # honest sympy matrices defer to their class's routine\n        if getattr(other, 'is_Matrix', False):\n            # call the highest-priority class's _eval_add\n            a, b = self, other\n            if a.__class__ != classof(a, b):\n                b, a = a, b\n            return a._eval_add(b)\n        # Matrix-like objects can be passed to CommonMatrix routines directly.\n        if getattr(other, 'is_MatrixLike', False):\n            return MatrixArithmetic._eval_add(self, other)\n\n        raise TypeError('cannot add %s and %s' % (type(self), type(other)))\n\n    @call_highest_priority('__rdiv__')\n    def __div__(self, other):\n        return self * (S.One / other)\n\n    @call_highest_priority('__rmatmul__')\n    def __matmul__(self, other):\n        return self.__mul__(other)\n\n    @call_highest_priority('__rmul__')\n    def __mul__(self, other):\n        \"\"\"Return self*other where other is either a scalar or a matrix\n        of compatible dimensions.\n\n        Examples\n        ========\n\n        >>> from sympy.matrices import Matrix\n        >>> A = Matrix([[1, 2, 3], [4, 5, 6]])\n        >>> 2*A == A*2 == Matrix([[2, 4, 6], [8, 10, 12]])\n        True\n        >>> B = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        >>> A*B\n        Matrix([\n        [30, 36, 42],\n        [66, 81, 96]])\n        >>> B*A\n        Traceback (most recent call last):\n        ...\n        ShapeError: Matrices size mismatch.\n        >>>\n\n        See Also\n        ========\n\n        matrix_multiply_elementwise\n        \"\"\"\n        other = _matrixify(other)\n        # matrix-like objects can have shapes.  This is\n        # our first sanity check.\n        if hasattr(other, 'shape') and len(other.shape) == 2:\n            if self.shape[1] != other.shape[0]:\n                raise ShapeError(\"Matrix size mismatch: %s * %s.\" % (\n                    self.shape, other.shape))\n\n        # honest sympy matrices defer to their class's routine\n        if getattr(other, 'is_Matrix', False):\n            return self._eval_matrix_mul(other)\n        # Matrix-like objects can be passed to CommonMatrix routines directly.\n        if getattr(other, 'is_MatrixLike', False):\n            return MatrixArithmetic._eval_matrix_mul(self, other)\n\n        # if 'other' is not iterable then scalar multiplication.\n        if not isinstance(other, collections.Iterable):\n            try:\n                return self._eval_scalar_mul(other)\n            except TypeError:\n                pass\n\n        return NotImplemented\n\n    def __neg__(self):\n        return self._eval_scalar_mul(-1)\n\n    @call_highest_priority('__rpow__')\n    def __pow__(self, num):\n        if not self.rows == self.cols:\n            raise NonSquareMatrixError()\n        try:\n            a = self\n            num = sympify(num)\n            if num.is_Number and num % 1 == 0:\n                if a.rows == 1:\n                    return a._new([[a[0]**num]])\n                if num == 0:\n                    return self._new(self.rows, self.cols, lambda i, j: int(i == j))\n                if num < 0:\n                    num = -num\n                    a = a.inv()\n                # When certain conditions are met,\n                # Jordan block algorithm is faster than\n                # computation by recursion.\n                elif a.rows == 2 and num > 100000:\n                    try:", "answer": "C"}
{"uuid": "2109a4db-6677-4f45-883a-6fd42772979c", "issue_statement": "(-x/4 - S(1)/12)**x - 1 simplifies to an inequivalent expression\n    >>> from sympy import *\r\n    >>> x = Symbol('x')\r\n    >>> e = (-x/4 - S(1)/12)**x - 1\r\n    >>> e\r\n    (-x/4 - 1/12)**x - 1\r\n    >>> f = simplify(e)\r\n    >>> f\r\n    12**(-x)*(-12**x + (-3*x - 1)**x)\r\n    >>> a = S(9)/5\r\n    >>> simplify(e.subs(x,a))\r\n    -1 - 32*15**(1/5)*2**(2/5)/225\r\n    >>> simplify(f.subs(x,a))\r\n    -1 - 32*(-1)**(4/5)*60**(1/5)/225\r\n    >>> N(e.subs(x,a))\r\n    -1.32255049319339\r\n    >>> N(f.subs(x,a))\r\n    -0.739051169462523 - 0.189590423018741*I", "choices": "(A)                 g = igcd(div_m, expt.q)\n                if g != 1:\n                    out_rad *= Pow(prime, Rational(div_m//g, expt.q//g))\n                else:\n                    sqr_dict[prime] = div_m\n        # identify gcd of remaining powers\n        for p, ex in sqr_dict.items():\n            if sqr_gcd == 0:\n                sqr_gcd = ex\n            else:\n                sqr_gcd = igcd(sqr_gcd, ex)\n                if sqr_gcd == 1:\n                    break\n        for k, v in sqr_dict.items():\n            sqr_int *= k**(v//sqr_gcd)\n        if sqr_int == self and out_int == 1 and out_rad == 1:\n            result = None\n        else:\n            result = out_int*out_rad*Pow(sqr_int, Rational(sqr_gcd, expt.q))\n        return result\n\n    def _eval_is_prime(self):\n        from sympy.ntheory import isprime\n\n        return isprime(self)\n\n    def _eval_is_composite(self):\n        if self > 1:\n            return fuzzy_not(self.is_prime)\n        else:\n            return False\n\n    def as_numer_denom(self):\n        return self, S.One\n\n    def __floordiv__(self, other):\n        return Integer(self.p // Integer(other).p)\n\n    def __rfloordiv__(self, other):\n        return Integer(Integer(other).p // self.p)\n\n# Add sympify converters\nfor i_type in integer_types:\n    converter[i_type] = Integer\n(B)         sqr_gcd = 0\n        sqr_dict = {}\n        for prime, exponent in dict.items():\n            exponent *= expt.p\n            # remove multiples of expt.q: (2**12)**(1/10) -> 2*(2**2)**(1/10)\n            div_e, div_m = divmod(exponent, expt.q)\n            if div_e > 0:\n                out_int *= prime**div_e\n            if div_m > 0:\n                # see if the reduced exponent shares a gcd with e.q\n                # (2**2)**(1/10) -> 2**(1/5)\n                g = igcd(div_m, expt.q)\n                if g != 1:\n                    out_rad *= Pow(prime, Rational(div_m//g, expt.q//g))\n                else:\n                    sqr_dict[prime] = div_m\n        # identify gcd of remaining powers\n        for p, ex in sqr_dict.items():\n            if sqr_gcd == 0:\n                sqr_gcd = ex\n            else:\n                sqr_gcd = igcd(sqr_gcd, ex)\n                if sqr_gcd == 1:\n                    break\n        for k, v in sqr_dict.items():\n            sqr_int *= k**(v//sqr_gcd)\n        if sqr_int == self and out_int == 1 and out_rad == 1:\n            result = None\n        else:\n            result = out_int*out_rad*Pow(sqr_int, Rational(sqr_gcd, expt.q))\n        return result\n\n    def _eval_is_prime(self):\n        from sympy.ntheory import isprime\n\n        return isprime(self)\n\n    def _eval_is_composite(self):\n        if self > 1:\n            return fuzzy_not(self.is_prime)\n        else:\n            return False\n\n    def as_numer_denom(self):\n(C)             result = Integer(x**abs(expt.p))\n            if self.is_negative:\n                result *= S.NegativeOne**expt\n            return result\n\n        # The following is an algorithm where we collect perfect roots\n        # from the factors of base.\n\n        # if it's not an nth root, it still might be a perfect power\n        b_pos = int(abs(self.p))\n        p = perfect_power(b_pos)\n        if p is not False:\n            dict = {p[0]: p[1]}\n        else:\n            dict = Integer(self).factors(limit=2**15)\n\n        # now process the dict of factors\n        if self.is_negative:\n            dict[-1] = 1\n        out_int = 1  # integer part\n        out_rad = 1  # extracted radicals\n        sqr_int = 1\n        sqr_gcd = 0\n        sqr_dict = {}\n        for prime, exponent in dict.items():\n            exponent *= expt.p\n            # remove multiples of expt.q: (2**12)**(1/10) -> 2*(2**2)**(1/10)\n            div_e, div_m = divmod(exponent, expt.q)\n            if div_e > 0:\n                out_int *= prime**div_e\n            if div_m > 0:\n                # see if the reduced exponent shares a gcd with e.q\n                # (2**2)**(1/10) -> 2**(1/5)\n                g = igcd(div_m, expt.q)\n                if g != 1:\n                    out_rad *= Pow(prime, Rational(div_m//g, expt.q//g))\n                else:\n                    sqr_dict[prime] = div_m\n        # identify gcd of remaining powers\n        for p, ex in sqr_dict.items():\n            if sqr_gcd == 0:\n                sqr_gcd = ex\n            else:\n                sqr_gcd = igcd(sqr_gcd, ex)\n(D)         if p is not False:\n            dict = {p[0]: p[1]}\n        else:\n            dict = Integer(self).factors(limit=2**15)\n\n        # now process the dict of factors\n        if self.is_negative:\n            dict[-1] = 1\n        out_int = 1  # integer part\n        out_rad = 1  # extracted radicals\n        sqr_int = 1\n        sqr_gcd = 0\n        sqr_dict = {}\n        for prime, exponent in dict.items():\n            exponent *= expt.p\n            # remove multiples of expt.q: (2**12)**(1/10) -> 2*(2**2)**(1/10)\n            div_e, div_m = divmod(exponent, expt.q)\n            if div_e > 0:\n                out_int *= prime**div_e\n            if div_m > 0:\n                # see if the reduced exponent shares a gcd with e.q\n                # (2**2)**(1/10) -> 2**(1/5)\n                g = igcd(div_m, expt.q)\n                if g != 1:\n                    out_rad *= Pow(prime, Rational(div_m//g, expt.q//g))\n                else:\n                    sqr_dict[prime] = div_m\n        # identify gcd of remaining powers\n        for p, ex in sqr_dict.items():\n            if sqr_gcd == 0:\n                sqr_gcd = ex\n            else:\n                sqr_gcd = igcd(sqr_gcd, ex)\n                if sqr_gcd == 1:\n                    break\n        for k, v in sqr_dict.items():\n            sqr_int *= k**(v//sqr_gcd)\n        if sqr_int == self and out_int == 1 and out_rad == 1:\n            result = None\n        else:\n            result = out_int*out_rad*Pow(sqr_int, Rational(sqr_gcd, expt.q))\n        return result\n\n    def _eval_is_prime(self):", "answer": "D"}
{"uuid": "796dc46c-f2ca-4a16-80aa-5012eba16c02", "issue_statement": "Issue with a substitution that leads to an undefined expression\n```\r\nPython 3.6.4 |Anaconda custom (64-bit)| (default, Dec 21 2017, 15:39:08) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: from sympy import *\r\n\r\nIn [2]: a,b = symbols('a,b')\r\n\r\nIn [3]: r = (1/(a+b) + 1/(a-b))/(1/(a+b) - 1/(a-b))\r\n\r\nIn [4]: r.subs(b,a)\r\nOut[4]: 1\r\n\r\nIn [6]: import sympy\r\n\r\nIn [7]: sympy.__version__\r\nOut[7]: '1.1.1'\r\n```\r\n\r\nIf b is substituted by a, r is undefined. It is possible to calculate the limit\r\n`r.limit(b,a) # -1`\r\n\r\nBut whenever a subexpression of r is undefined, r itself is undefined.", "choices": "(A)         # infinite loop\n        for i in range(2):\n            new_c_powers = []\n            changed = False\n            for b, e in c_powers:\n                if e.is_zero:\n                    continue\n                if e is S.One:\n                    if b.is_Number:\n                        coeff *= b\n                        continue\n(B)                         coeff *= b\n                        continue\n                    p = b\n                if e is not S.One:\n                    p = Pow(b, e)\n                    # check to make sure that the base doesn't change\n                    # after exponentiation; to allow for unevaluated\n                    # Pow, we only do so if b is not already a Pow\n                    if p.is_Pow and not b.is_Pow:\n                        bi = b\n                        b, e = p.as_base_exp()\n(C)             changed = False\n            for b, e in c_powers:\n                if e.is_zero:\n                    continue\n                if e is S.One:\n                    if b.is_Number:\n                        coeff *= b\n                        continue\n                    p = b\n                if e is not S.One:\n                    p = Pow(b, e)\n(D)                     continue\n                if e is S.One:\n                    if b.is_Number:\n                        coeff *= b\n                        continue\n                    p = b\n                if e is not S.One:\n                    p = Pow(b, e)\n                    # check to make sure that the base doesn't change\n                    # after exponentiation; to allow for unevaluated\n                    # Pow, we only do so if b is not already a Pow", "answer": "C"}
{"uuid": "767cae29-4e0c-48b5-b77b-2a1b8f2a79dd", "issue_statement": "Display of SeqFormula()\n```\r\nimport sympy as sp\r\nk, m, n = sp.symbols('k m n', integer=True)\r\nsp.init_printing()\r\n\r\nsp.SeqFormula(n**2, (n,0,sp.oo))\r\n```\r\n\r\nThe Jupyter rendering of this command backslash-escapes the brackets producing:\r\n\r\n`\\left\\[0, 1, 4, 9, \\ldots\\right\\]`\r\n\r\nCopying this output to a markdown cell this does not render properly.  Whereas:\r\n\r\n`[0, 1, 4, 9, \\ldots ]`\r\n\r\ndoes render just fine.  \r\n\r\nSo - sequence output should not backslash-escape square brackets, or, `\\]` should instead render?", "choices": "(A)               + r\"\\right\\]\")\n\n    _print_SeqPer = _print_SeqFormula\n    _print_SeqAdd = _print_SeqFormula\n    _print_SeqMul = _print_SeqFormula\n\n    def _print_Interval(self, i):\n        if i.start == i.end:\n            return r\"\\left\\{%s\\right\\}\" % self._print(i.start)\n(B)             printset.append(r'\\ldots')\n        else:\n            printset = tuple(s)\n\n        return (r\"\\left\\[\"\n              + r\", \".join(self._print(el) for el in printset)\n              + r\"\\right\\]\")\n\n    _print_SeqPer = _print_SeqFormula\n(C)     _print_SeqPer = _print_SeqFormula\n    _print_SeqAdd = _print_SeqFormula\n    _print_SeqMul = _print_SeqFormula\n\n    def _print_Interval(self, i):\n        if i.start == i.end:\n            return r\"\\left\\{%s\\right\\}\" % self._print(i.start)\n\n        else:\n(D) \n        return (r\"\\left\\[\"\n              + r\", \".join(self._print(el) for el in printset)\n              + r\"\\right\\]\")\n\n    _print_SeqPer = _print_SeqFormula\n    _print_SeqAdd = _print_SeqFormula\n    _print_SeqMul = _print_SeqFormula\n", "answer": "D"}
{"uuid": "d70f4fde-238c-42a8-a54f-2eaaef844dac", "issue_statement": "Inconsistency when simplifying (-a)**x * a**(-x), a a positive integer\nCompare:\r\n\r\n```\r\n>>> a = Symbol('a', integer=True, positive=True)\r\n>>> e = (-a)**x * a**(-x)\r\n>>> f = simplify(e)\r\n>>> print(e)\r\na**(-x)*(-a)**x\r\n>>> print(f)\r\n(-1)**x\r\n>>> t = -S(10)/3\r\n>>> n1 = e.subs(x,t)\r\n>>> n2 = f.subs(x,t)\r\n>>> print(N(n1))\r\n-0.5 + 0.866025403784439*I\r\n>>> print(N(n2))\r\n-0.5 + 0.866025403784439*I\r\n```\r\n\r\nvs\r\n\r\n```\r\n>>> a = S(2)\r\n>>> e = (-a)**x * a**(-x)\r\n>>> f = simplify(e)\r\n>>> print(e)\r\n(-2)**x*2**(-x)\r\n>>> print(f)\r\n(-1)**x\r\n>>> t = -S(10)/3\r\n>>> n1 = e.subs(x,t)\r\n>>> n2 = f.subs(x,t)\r\n>>> print(N(n1))\r\n0.5 - 0.866025403784439*I\r\n>>> print(N(n2))\r\n-0.5 + 0.866025403784439*I\r\n```", "choices": "(A)                 if (ne is S.One):\n                    return Rational(self.q, self.p)\n                if self.is_negative:\n                    if expt.q != 1:\n                        return -(S.NegativeOne)**((expt.p % expt.q) /\n                               S(expt.q))*Rational(self.q, -self.p)**ne\n                    else:\n                        return S.NegativeOne**ne*Rational(self.q, -self.p)**ne\n                else:\n                    return Rational(self.q, self.p)**ne\n            if expt is S.Infinity:  # -oo already caught by test for negative\n                if self.p > self.q:\n                    # (3/2)**oo -> oo\n                    return S.Infinity\n                if self.p < -self.q:\n                    # (-3/2)**oo -> oo + I*oo\n                    return S.Infinity + S.Infinity*S.ImaginaryUnit\n                return S.Zero\n            if isinstance(expt, Integer):\n                # (4/3)**2 -> 4**2 / 3**2\n                return Rational(self.p**expt.p, self.q**expt.p, 1)\n            if isinstance(expt, Rational):\n                if self.p != 1:\n                    # (4/3)**(5/6) -> 4**(5/6)*3**(-5/6)\n                    return Integer(self.p)**expt*Integer(self.q)**(-expt)\n                # as the above caught negative self.p, now self is positive\n                return Integer(self.q)**Rational(\n                expt.p*(expt.q - 1), expt.q) / \\\n                    Integer(self.q)**Integer(expt.p)\n\n        if self.is_negative and expt.is_even:\n            return (-self)**expt\n\n        return\n\n    def _as_mpf_val(self, prec):\n        return mlib.from_rational(self.p, self.q, prec, rnd)\n\n    def _mpmath_(self, prec, rnd):\n        return mpmath.make_mpf(mlib.from_rational(self.p, self.q, prec, rnd))\n\n    def __abs__(self):\n        return Rational(abs(self.p), self.q)\n\n    def __int__(self):\n        p, q = self.p, self.q\n        if p < 0:\n            return -int(-p//q)\n        return int(p//q)\n\n    __long__ = __int__\n\n    def floor(self):\n        return Integer(self.p // self.q)\n\n    def ceiling(self):\n        return -Integer(-self.p // self.q)\n\n    def __eq__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            return NotImplemented\n        if other.is_NumberSymbol:\n            if other.is_irrational:\n                return False\n            return other.__eq__(self)\n        if other.is_Number:\n            if other.is_Rational:\n                # a Rational is always in reduced form so will never be 2/4\n                # so we can just check equivalence of args\n                return self.p == other.p and self.q == other.q\n            if other.is_Float:\n                return mlib.mpf_eq(self._as_mpf_val(other._prec), other._mpf_)\n        return False\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __gt__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s > %s\" % (self, other))\n        if other.is_NumberSymbol:\n            return other.__lt__(self)\n        expr = self\n        if other.is_Number:\n            if other.is_Rational:\n                return _sympify(bool(self.p*other.q > self.q*other.p))\n            if other.is_Float:\n                return _sympify(bool(mlib.mpf_gt(\n                    self._as_mpf_val(other._prec), other._mpf_)))\n        elif other.is_number and other.is_real:\n            expr, other = Integer(self.p), self.q*other\n        return Expr.__gt__(expr, other)\n\n    def __ge__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s >= %s\" % (self, other))\n        if other.is_NumberSymbol:\n            return other.__le__(self)\n        expr = self\n        if other.is_Number:\n            if other.is_Rational:\n                 return _sympify(bool(self.p*other.q >= self.q*other.p))\n            if other.is_Float:\n                return _sympify(bool(mlib.mpf_ge(\n                    self._as_mpf_val(other._prec), other._mpf_)))\n        elif other.is_number and other.is_real:\n            expr, other = Integer(self.p), self.q*other\n        return Expr.__ge__(expr, other)\n\n    def __lt__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s < %s\" % (self, other))\n        if other.is_NumberSymbol:\n            return other.__gt__(self)\n        expr = self\n        if other.is_Number:\n            if other.is_Rational:\n                return _sympify(bool(self.p*other.q < self.q*other.p))\n            if other.is_Float:\n                return _sympify(bool(mlib.mpf_lt(\n                    self._as_mpf_val(other._prec), other._mpf_)))\n        elif other.is_number and other.is_real:\n            expr, other = Integer(self.p), self.q*other\n        return Expr.__lt__(expr, other)\n\n    def __le__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s <= %s\" % (self, other))\n        expr = self\n        if other.is_NumberSymbol:\n            return other.__ge__(self)\n        elif other.is_Number:\n            if other.is_Rational:\n                return _sympify(bool(self.p*other.q <= self.q*other.p))\n            if other.is_Float:\n                return _sympify(bool(mlib.mpf_le(\n                    self._as_mpf_val(other._prec), other._mpf_)))\n        elif other.is_number and other.is_real:\n            expr, other = Integer(self.p), self.q*other\n        return Expr.__le__(expr, other)\n\n    def __hash__(self):\n        return super(Rational, self).__hash__()\n\n    def factors(self, limit=None, use_trial=True, use_rho=False,\n                use_pm1=False, verbose=False, visual=False):\n        \"\"\"A wrapper to factorint which return factors of self that are\n        smaller than limit (or cheap to compute). Special methods of\n        factoring are disabled by default so that only trial division is used.\n        \"\"\"\n        from sympy.ntheory import factorrat\n\n        return factorrat(self, limit=limit, use_trial=use_trial,\n                      use_rho=use_rho, use_pm1=use_pm1,\n                      verbose=verbose).copy()\n\n    @_sympifyit('other', NotImplemented)\n    def gcd(self, other):\n        if isinstance(other, Rational):\n            if other is S.Zero:\n                return other\n            return Rational(\n                Integer(igcd(self.p, other.p)),\n                Integer(ilcm(self.q, other.q)))\n        return Number.gcd(self, other)\n\n    @_sympifyit('other', NotImplemented)\n    def lcm(self, other):\n        if isinstance(other, Rational):\n            return Rational(\n                self.p*other.p//igcd(self.p, other.p),\n                igcd(self.q, other.q))\n        return Number.lcm(self, other)\n\n    def as_numer_denom(self):\n        return Integer(self.p), Integer(self.q)\n\n    def _sage_(self):\n        import sage.all as sage\n        return sage.Integer(self.p)/sage.Integer(self.q)\n\n    def as_content_primitive(self, radical=False, clear=True):\n        \"\"\"Return the tuple (R, self/R) where R is the positive Rational\n        extracted from self.\n\n        Examples\n        ========\n\n        >>> from sympy import S\n        >>> (S(-3)/2).as_content_primitive()\n        (3/2, -1)\n\n        See docstring of Expr.as_content_primitive for more examples.\n        \"\"\"\n\n        if self:\n            if self.is_positive:\n                return self, S.One\n            return -self, S.NegativeOne\n        return S.One, self\n\n    def as_coeff_Mul(self, rational=False):\n        \"\"\"Efficiently extract the coefficient of a product. \"\"\"\n        return self, S.One\n\n    def as_coeff_Add(self, rational=False):\n        \"\"\"Efficiently extract the coefficient of a summation. \"\"\"\n        return self, S.Zero\n\n\n# int -> Integer\n_intcache = {}\n\n\n# TODO move this tracing facility to sympy/core/trace.py  ?\ndef _intcache_printinfo():\n    ints = sorted(_intcache.keys())\n    nhit = _intcache_hits\n    nmiss = _intcache_misses\n\n    if nhit == 0 and nmiss == 0:\n        print()\n        print('Integer cache statistic was not collected')\n        return\n\n    miss_ratio = float(nmiss) / (nhit + nmiss)\n\n    print()\n    print('Integer cache statistic')\n    print('-----------------------')\n    print()\n    print('#items: %i' % len(ints))\n    print()\n    print(' #hit   #miss               #total')\n    print()\n    print('%5i   %5i (%7.5f %%)   %5i' % (\n        nhit, nmiss, miss_ratio*100, nhit + nmiss)\n    )\n    print()\n    print(ints)\n\n_intcache_hits = 0\n_intcache_misses = 0\n\n\ndef int_trace(f):\n    import os\n    if os.getenv('SYMPY_TRACE_INT', 'no').lower() != 'yes':\n        return f\n\n    def Integer_tracer(cls, i):\n        global _intcache_hits, _intcache_misses\n\n        try:\n            _intcache_hits += 1\n            return _intcache[i]\n        except KeyError:\n            _intcache_hits -= 1\n            _intcache_misses += 1\n\n            return f(cls, i)\n\n    # also we want to hook our _intcache_printinfo into sys.atexit\n    import atexit\n    atexit.register(_intcache_printinfo)\n\n    return Integer_tracer\n\n\nclass Integer(Rational):\n\n    q = 1\n    is_integer = True\n    is_number = True\n\n    is_Integer = True\n\n    __slots__ = ['p']\n\n    def _as_mpf_val(self, prec):\n        return mlib.from_int(self.p, prec, rnd)\n\n    def _mpmath_(self, prec, rnd):\n        return mpmath.make_mpf(self._as_mpf_val(prec))\n\n    # TODO caching with decorator, but not to degrade performance\n    @int_trace\n    def __new__(cls, i):\n        if isinstance(i, string_types):\n            i = i.replace(' ', '')\n        # whereas we cannot, in general, make a Rational from an\n        # arbitrary expression, we can make an Integer unambiguously\n        # (except when a non-integer expression happens to round to\n        # an integer). So we proceed by taking int() of the input and\n        # let the int routines determine whether the expression can\n        # be made into an int or whether an error should be raised.\n        try:\n            ival = int(i)\n        except TypeError:\n            raise TypeError(\n                'Integer can only work with integer expressions.')\n        try:\n            return _intcache[ival]\n        except KeyError:\n            # We only work with well-behaved integer types. This converts, for\n            # example, numpy.int32 instances.\n            obj = Expr.__new__(cls)\n            obj.p = ival\n\n            _intcache[ival] = obj\n            return obj\n\n    def __getnewargs__(self):\n        return (self.p,)\n\n    # Arithmetic operations are here for efficiency\n    def __int__(self):\n        return self.p\n\n    __long__ = __int__\n\n    def floor(self):\n        return Integer(self.p)\n\n    def ceiling(self):\n        return Integer(self.p)\n\n    def __neg__(self):\n        return Integer(-self.p)\n\n    def __abs__(self):\n        if self.p >= 0:\n            return self\n        else:\n            return Integer(-self.p)\n\n    def __divmod__(self, other):\n        from .containers import Tuple\n        if isinstance(other, Integer) and global_evaluate[0]:\n            return Tuple(*(divmod(self.p, other.p)))\n        else:\n            return Number.__divmod__(self, other)\n\n    def __rdivmod__(self, other):\n        from .containers import Tuple\n        if isinstance(other, integer_types) and global_evaluate[0]:\n            return Tuple(*(divmod(other, self.p)))\n        else:\n            try:\n                other = Number(other)\n            except TypeError:\n                msg = \"unsupported operand type(s) for divmod(): '%s' and '%s'\"\n                oname = type(other).__name__\n                sname = type(self).__name__\n                raise TypeError(msg % (oname, sname))\n            return Number.__divmod__(other, self)\n\n    # TODO make it decorator + bytecodehacks?\n    def __add__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p + other)\n            elif isinstance(other, Integer):\n                return Integer(self.p + other.p)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.q + other.p, other.q, 1)\n            return Rational.__add__(self, other)\n        else:\n            return Add(self, other)\n\n    def __radd__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other + self.p)\n            elif isinstance(other, Rational):\n                return Rational(other.p + self.p*other.q, other.q, 1)\n            return Rational.__radd__(self, other)\n        return Rational.__radd__(self, other)\n\n    def __sub__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p - other)\n            elif isinstance(other, Integer):\n                return Integer(self.p - other.p)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.q - other.p, other.q, 1)\n            return Rational.__sub__(self, other)\n        return Rational.__sub__(self, other)\n\n    def __rsub__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other - self.p)\n            elif isinstance(other, Rational):\n                return Rational(other.p - self.p*other.q, other.q, 1)\n            return Rational.__rsub__(self, other)\n        return Rational.__rsub__(self, other)\n\n    def __mul__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p*other)\n            elif isinstance(other, Integer):\n                return Integer(self.p*other.p)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.p, other.q, igcd(self.p, other.q))\n            return Rational.__mul__(self, other)\n        return Rational.__mul__(self, other)\n\n    def __rmul__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other*self.p)\n            elif isinstance(other, Rational):\n                return Rational(other.p*self.p, other.q, igcd(self.p, other.q))\n            return Rational.__rmul__(self, other)\n        return Rational.__rmul__(self, other)\n\n    def __mod__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p % other)\n            elif isinstance(other, Integer):\n                return Integer(self.p % other.p)\n            return Rational.__mod__(self, other)\n        return Rational.__mod__(self, other)\n\n    def __rmod__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other % self.p)\n            elif isinstance(other, Integer):\n                return Integer(other.p % self.p)\n            return Rational.__rmod__(self, other)\n        return Rational.__rmod__(self, other)\n\n    def __eq__(self, other):\n        if isinstance(other, integer_types):\n            return (self.p == other)\n        elif isinstance(other, Integer):\n            return (self.p == other.p)\n        return Rational.__eq__(self, other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __gt__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s > %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p > other.p)\n        return Rational.__gt__(self, other)\n\n    def __lt__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s < %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p < other.p)\n        return Rational.__lt__(self, other)\n\n    def __ge__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s >= %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p >= other.p)\n        return Rational.__ge__(self, other)\n\n    def __le__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s <= %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p <= other.p)\n        return Rational.__le__(self, other)\n\n    def __hash__(self):\n        return hash(self.p)\n\n    def __index__(self):\n        return self.p\n\n    ########################################\n\n    def _eval_is_odd(self):\n        return bool(self.p % 2)\n\n    def _eval_power(self, expt):\n        \"\"\"\n        Tries to do some simplifications on self**expt\n\n        Returns None if no further simplifications can be done\n\n        When exponent is a fraction (so we have for example a square root),\n        we try to find a simpler representation by factoring the argument\n        up to factors of 2**15, e.g.\n\n          - sqrt(4) becomes 2\n          - sqrt(-4) becomes 2*I\n          - (2**(3+7)*3**(6+7))**Rational(1,7) becomes 6*18**(3/7)\n\n        Further simplification would require a special call to factorint on\n        the argument which is not done here for sake of speed.\n\n        \"\"\"\n        from sympy import perfect_power\n\n        if expt is S.Infinity:\n            if self.p > S.One:\n                return S.Infinity\n            # cases -1, 0, 1 are done in their respective classes\n            return S.Infinity + S.ImaginaryUnit*S.Infinity\n        if expt is S.NegativeInfinity:\n            return Rational(1, self)**S.Infinity\n        if not isinstance(expt, Number):\n            # simplify when expt is even\n            # (-2)**k --> 2**k\n            if self.is_negative and expt.is_even:\n                return (-self)**expt\n        if isinstance(expt, Float):\n            # Rational knows how to exponentiate by a Float\n            return super(Integer, self)._eval_power(expt)\n        if not isinstance(expt, Rational):\n            return\n        if expt is S.Half and self.is_negative:\n            # we extract I for this special case since everyone is doing so\n            return S.ImaginaryUnit*Pow(-self, expt)\n        if expt.is_negative:\n            # invert base and change sign on exponent\n            ne = -expt\n            if self.is_negative:\n(B)             raise TypeError(\"Invalid comparison %s <= %s\" % (self, other))\n        expr = self\n        if other.is_NumberSymbol:\n            return other.__ge__(self)\n        elif other.is_Number:\n            if other.is_Rational:\n                return _sympify(bool(self.p*other.q <= self.q*other.p))\n            if other.is_Float:\n                return _sympify(bool(mlib.mpf_le(\n                    self._as_mpf_val(other._prec), other._mpf_)))\n        elif other.is_number and other.is_real:\n            expr, other = Integer(self.p), self.q*other\n        return Expr.__le__(expr, other)\n\n    def __hash__(self):\n        return super(Rational, self).__hash__()\n\n    def factors(self, limit=None, use_trial=True, use_rho=False,\n                use_pm1=False, verbose=False, visual=False):\n        \"\"\"A wrapper to factorint which return factors of self that are\n        smaller than limit (or cheap to compute). Special methods of\n        factoring are disabled by default so that only trial division is used.\n        \"\"\"\n        from sympy.ntheory import factorrat\n\n        return factorrat(self, limit=limit, use_trial=use_trial,\n                      use_rho=use_rho, use_pm1=use_pm1,\n                      verbose=verbose).copy()\n\n    @_sympifyit('other', NotImplemented)\n    def gcd(self, other):\n        if isinstance(other, Rational):\n            if other is S.Zero:\n                return other\n            return Rational(\n                Integer(igcd(self.p, other.p)),\n                Integer(ilcm(self.q, other.q)))\n        return Number.gcd(self, other)\n\n    @_sympifyit('other', NotImplemented)\n    def lcm(self, other):\n        if isinstance(other, Rational):\n            return Rational(\n                self.p*other.p//igcd(self.p, other.p),\n                igcd(self.q, other.q))\n        return Number.lcm(self, other)\n\n    def as_numer_denom(self):\n        return Integer(self.p), Integer(self.q)\n\n    def _sage_(self):\n        import sage.all as sage\n        return sage.Integer(self.p)/sage.Integer(self.q)\n\n    def as_content_primitive(self, radical=False, clear=True):\n        \"\"\"Return the tuple (R, self/R) where R is the positive Rational\n        extracted from self.\n\n        Examples\n        ========\n\n        >>> from sympy import S\n        >>> (S(-3)/2).as_content_primitive()\n        (3/2, -1)\n\n        See docstring of Expr.as_content_primitive for more examples.\n        \"\"\"\n\n        if self:\n            if self.is_positive:\n                return self, S.One\n            return -self, S.NegativeOne\n        return S.One, self\n\n    def as_coeff_Mul(self, rational=False):\n        \"\"\"Efficiently extract the coefficient of a product. \"\"\"\n        return self, S.One\n\n    def as_coeff_Add(self, rational=False):\n        \"\"\"Efficiently extract the coefficient of a summation. \"\"\"\n        return self, S.Zero\n\n\n# int -> Integer\n_intcache = {}\n\n\n# TODO move this tracing facility to sympy/core/trace.py  ?\ndef _intcache_printinfo():\n    ints = sorted(_intcache.keys())\n    nhit = _intcache_hits\n    nmiss = _intcache_misses\n\n    if nhit == 0 and nmiss == 0:\n        print()\n        print('Integer cache statistic was not collected')\n        return\n\n    miss_ratio = float(nmiss) / (nhit + nmiss)\n\n    print()\n    print('Integer cache statistic')\n    print('-----------------------')\n    print()\n    print('#items: %i' % len(ints))\n    print()\n    print(' #hit   #miss               #total')\n    print()\n    print('%5i   %5i (%7.5f %%)   %5i' % (\n        nhit, nmiss, miss_ratio*100, nhit + nmiss)\n    )\n    print()\n    print(ints)\n\n_intcache_hits = 0\n_intcache_misses = 0\n\n\ndef int_trace(f):\n    import os\n    if os.getenv('SYMPY_TRACE_INT', 'no').lower() != 'yes':\n        return f\n\n    def Integer_tracer(cls, i):\n        global _intcache_hits, _intcache_misses\n\n        try:\n            _intcache_hits += 1\n            return _intcache[i]\n        except KeyError:\n            _intcache_hits -= 1\n            _intcache_misses += 1\n\n            return f(cls, i)\n\n    # also we want to hook our _intcache_printinfo into sys.atexit\n    import atexit\n    atexit.register(_intcache_printinfo)\n\n    return Integer_tracer\n\n\nclass Integer(Rational):\n\n    q = 1\n    is_integer = True\n    is_number = True\n\n    is_Integer = True\n\n    __slots__ = ['p']\n\n    def _as_mpf_val(self, prec):\n        return mlib.from_int(self.p, prec, rnd)\n\n    def _mpmath_(self, prec, rnd):\n        return mpmath.make_mpf(self._as_mpf_val(prec))\n\n    # TODO caching with decorator, but not to degrade performance\n    @int_trace\n    def __new__(cls, i):\n        if isinstance(i, string_types):\n            i = i.replace(' ', '')\n        # whereas we cannot, in general, make a Rational from an\n        # arbitrary expression, we can make an Integer unambiguously\n        # (except when a non-integer expression happens to round to\n        # an integer). So we proceed by taking int() of the input and\n        # let the int routines determine whether the expression can\n        # be made into an int or whether an error should be raised.\n        try:\n            ival = int(i)\n        except TypeError:\n            raise TypeError(\n                'Integer can only work with integer expressions.')\n        try:\n            return _intcache[ival]\n        except KeyError:\n            # We only work with well-behaved integer types. This converts, for\n            # example, numpy.int32 instances.\n            obj = Expr.__new__(cls)\n            obj.p = ival\n\n            _intcache[ival] = obj\n            return obj\n\n    def __getnewargs__(self):\n        return (self.p,)\n\n    # Arithmetic operations are here for efficiency\n    def __int__(self):\n        return self.p\n\n    __long__ = __int__\n\n    def floor(self):\n        return Integer(self.p)\n\n    def ceiling(self):\n        return Integer(self.p)\n\n    def __neg__(self):\n        return Integer(-self.p)\n\n    def __abs__(self):\n        if self.p >= 0:\n            return self\n        else:\n            return Integer(-self.p)\n\n    def __divmod__(self, other):\n        from .containers import Tuple\n        if isinstance(other, Integer) and global_evaluate[0]:\n            return Tuple(*(divmod(self.p, other.p)))\n        else:\n            return Number.__divmod__(self, other)\n\n    def __rdivmod__(self, other):\n        from .containers import Tuple\n        if isinstance(other, integer_types) and global_evaluate[0]:\n            return Tuple(*(divmod(other, self.p)))\n        else:\n            try:\n                other = Number(other)\n            except TypeError:\n                msg = \"unsupported operand type(s) for divmod(): '%s' and '%s'\"\n                oname = type(other).__name__\n                sname = type(self).__name__\n                raise TypeError(msg % (oname, sname))\n            return Number.__divmod__(other, self)\n\n    # TODO make it decorator + bytecodehacks?\n    def __add__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p + other)\n            elif isinstance(other, Integer):\n                return Integer(self.p + other.p)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.q + other.p, other.q, 1)\n            return Rational.__add__(self, other)\n        else:\n            return Add(self, other)\n\n    def __radd__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other + self.p)\n            elif isinstance(other, Rational):\n                return Rational(other.p + self.p*other.q, other.q, 1)\n            return Rational.__radd__(self, other)\n        return Rational.__radd__(self, other)\n\n    def __sub__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p - other)\n            elif isinstance(other, Integer):\n                return Integer(self.p - other.p)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.q - other.p, other.q, 1)\n            return Rational.__sub__(self, other)\n        return Rational.__sub__(self, other)\n\n    def __rsub__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other - self.p)\n            elif isinstance(other, Rational):\n                return Rational(other.p - self.p*other.q, other.q, 1)\n            return Rational.__rsub__(self, other)\n        return Rational.__rsub__(self, other)\n\n    def __mul__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p*other)\n            elif isinstance(other, Integer):\n                return Integer(self.p*other.p)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.p, other.q, igcd(self.p, other.q))\n            return Rational.__mul__(self, other)\n        return Rational.__mul__(self, other)\n\n    def __rmul__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other*self.p)\n            elif isinstance(other, Rational):\n                return Rational(other.p*self.p, other.q, igcd(self.p, other.q))\n            return Rational.__rmul__(self, other)\n        return Rational.__rmul__(self, other)\n\n    def __mod__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p % other)\n            elif isinstance(other, Integer):\n                return Integer(self.p % other.p)\n            return Rational.__mod__(self, other)\n        return Rational.__mod__(self, other)\n\n    def __rmod__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other % self.p)\n            elif isinstance(other, Integer):\n                return Integer(other.p % self.p)\n            return Rational.__rmod__(self, other)\n        return Rational.__rmod__(self, other)\n\n    def __eq__(self, other):\n        if isinstance(other, integer_types):\n            return (self.p == other)\n        elif isinstance(other, Integer):\n            return (self.p == other.p)\n        return Rational.__eq__(self, other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __gt__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s > %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p > other.p)\n        return Rational.__gt__(self, other)\n\n    def __lt__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s < %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p < other.p)\n        return Rational.__lt__(self, other)\n\n    def __ge__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s >= %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p >= other.p)\n        return Rational.__ge__(self, other)\n\n    def __le__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s <= %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p <= other.p)\n        return Rational.__le__(self, other)\n\n    def __hash__(self):\n        return hash(self.p)\n\n    def __index__(self):\n        return self.p\n\n    ########################################\n\n    def _eval_is_odd(self):\n        return bool(self.p % 2)\n\n    def _eval_power(self, expt):\n        \"\"\"\n        Tries to do some simplifications on self**expt\n\n        Returns None if no further simplifications can be done\n\n        When exponent is a fraction (so we have for example a square root),\n        we try to find a simpler representation by factoring the argument\n        up to factors of 2**15, e.g.\n\n          - sqrt(4) becomes 2\n          - sqrt(-4) becomes 2*I\n          - (2**(3+7)*3**(6+7))**Rational(1,7) becomes 6*18**(3/7)\n\n        Further simplification would require a special call to factorint on\n        the argument which is not done here for sake of speed.\n\n        \"\"\"\n        from sympy import perfect_power\n\n        if expt is S.Infinity:\n            if self.p > S.One:\n                return S.Infinity\n            # cases -1, 0, 1 are done in their respective classes\n            return S.Infinity + S.ImaginaryUnit*S.Infinity\n        if expt is S.NegativeInfinity:\n            return Rational(1, self)**S.Infinity\n        if not isinstance(expt, Number):\n            # simplify when expt is even\n            # (-2)**k --> 2**k\n            if self.is_negative and expt.is_even:\n                return (-self)**expt\n        if isinstance(expt, Float):\n            # Rational knows how to exponentiate by a Float\n            return super(Integer, self)._eval_power(expt)\n        if not isinstance(expt, Rational):\n            return\n        if expt is S.Half and self.is_negative:\n            # we extract I for this special case since everyone is doing so\n            return S.ImaginaryUnit*Pow(-self, expt)\n        if expt.is_negative:\n            # invert base and change sign on exponent\n            ne = -expt\n            if self.is_negative:\n                if expt.q != 1:\n                    return -(S.NegativeOne)**((expt.p % expt.q) /\n                            S(expt.q))*Rational(1, -self)**ne\n                else:\n                    return (S.NegativeOne)**ne*Rational(1, -self)**ne\n            else:\n                return Rational(1, self.p)**ne\n        # see if base is a perfect root, sqrt(4) --> 2\n        x, xexact = integer_nthroot(abs(self.p), expt.q)\n        if xexact:\n            # if it's a perfect root we've finished\n            result = Integer(x**abs(expt.p))\n            if self.is_negative:\n                result *= S.NegativeOne**expt\n            return result\n\n        # The following is an algorithm where we collect perfect roots\n        # from the factors of base.\n\n        # if it's not an nth root, it still might be a perfect power\n        b_pos = int(abs(self.p))\n        p = perfect_power(b_pos)\n        if p is not False:\n            dict = {p[0]: p[1]}\n        else:\n            dict = Integer(b_pos).factors(limit=2**15)\n\n        # now process the dict of factors\n        out_int = 1  # integer part\n        out_rad = 1  # extracted radicals\n        sqr_int = 1\n        sqr_gcd = 0\n        sqr_dict = {}\n        for prime, exponent in dict.items():\n            exponent *= expt.p\n            # remove multiples of expt.q: (2**12)**(1/10) -> 2*(2**2)**(1/10)\n            div_e, div_m = divmod(exponent, expt.q)\n            if div_e > 0:\n                out_int *= prime**div_e\n            if div_m > 0:\n                # see if the reduced exponent shares a gcd with e.q\n                # (2**2)**(1/10) -> 2**(1/5)\n                g = igcd(div_m, expt.q)\n                if g != 1:\n                    out_rad *= Pow(prime, Rational(div_m//g, expt.q//g))\n                else:\n                    sqr_dict[prime] = div_m\n        # identify gcd of remaining powers\n        for p, ex in sqr_dict.items():\n            if sqr_gcd == 0:\n                sqr_gcd = ex\n            else:\n                sqr_gcd = igcd(sqr_gcd, ex)\n                if sqr_gcd == 1:\n                    break\n        for k, v in sqr_dict.items():\n            sqr_int *= k**(v//sqr_gcd)\n        if sqr_int == b_pos and out_int == 1 and out_rad == 1:\n            result = None\n        else:\n            result = out_int*out_rad*Pow(sqr_int, Rational(sqr_gcd, expt.q))\n            if self.is_negative:\n                result *= Pow(S.NegativeOne, expt)\n        return result\n\n    def _eval_is_prime(self):\n        from sympy.ntheory import isprime\n\n        return isprime(self)\n\n    def _eval_is_composite(self):\n        if self > 1:\n            return fuzzy_not(self.is_prime)\n        else:\n            return False\n\n    def as_numer_denom(self):\n        return self, S.One\n\n    def __floordiv__(self, other):\n        return Integer(self.p // Integer(other).p)\n\n    def __rfloordiv__(self, other):\n        return Integer(Integer(other).p // self.p)\n\n# Add sympify converters\nfor i_type in integer_types:\n    converter[i_type] = Integer\n\n\nclass AlgebraicNumber(Expr):\n    \"\"\"Class for representing algebraic numbers in SymPy. \"\"\"\n\n    __slots__ = ['rep', 'root', 'alias', 'minpoly']\n\n    is_AlgebraicNumber = True\n    is_algebraic = True\n    is_number = True\n\n    def __new__(cls, expr, coeffs=None, alias=None, **args):\n        \"\"\"Construct a new algebraic number. \"\"\"\n        from sympy import Poly\n        from sympy.polys.polyclasses import ANP, DMP\n        from sympy.polys.numberfields import minimal_polynomial\n        from sympy.core.symbol import Symbol\n\n        expr = sympify(expr)\n\n        if isinstance(expr, (tuple, Tuple)):\n            minpoly, root = expr\n\n            if not minpoly.is_Poly:\n                minpoly = Poly(minpoly)\n        elif expr.is_AlgebraicNumber:\n            minpoly, root = expr.minpoly, expr.root\n        else:\n            minpoly, root = minimal_polynomial(\n                expr, args.get('gen'), polys=True), expr\n\n        dom = minpoly.get_domain()\n\n        if coeffs is not None:\n            if not isinstance(coeffs, ANP):\n                rep = DMP.from_sympy_list(sympify(coeffs), 0, dom)\n                scoeffs = Tuple(*coeffs)\n            else:\n                rep = DMP.from_list(coeffs.to_list(), 0, dom)\n                scoeffs = Tuple(*coeffs.to_list())\n\n            if rep.degree() >= minpoly.degree():\n                rep = rep.rem(minpoly.rep)\n\n        else:\n            rep = DMP.from_list([1, 0], 0, dom)\n            scoeffs = Tuple(1, 0)\n\n            if root.is_negative:\n(C)     atexit.register(_intcache_printinfo)\n\n    return Integer_tracer\n\n\nclass Integer(Rational):\n\n    q = 1\n    is_integer = True\n    is_number = True\n\n    is_Integer = True\n\n    __slots__ = ['p']\n\n    def _as_mpf_val(self, prec):\n        return mlib.from_int(self.p, prec, rnd)\n\n    def _mpmath_(self, prec, rnd):\n        return mpmath.make_mpf(self._as_mpf_val(prec))\n\n    # TODO caching with decorator, but not to degrade performance\n    @int_trace\n    def __new__(cls, i):\n        if isinstance(i, string_types):\n            i = i.replace(' ', '')\n        # whereas we cannot, in general, make a Rational from an\n        # arbitrary expression, we can make an Integer unambiguously\n        # (except when a non-integer expression happens to round to\n        # an integer). So we proceed by taking int() of the input and\n        # let the int routines determine whether the expression can\n        # be made into an int or whether an error should be raised.\n        try:\n            ival = int(i)\n        except TypeError:\n            raise TypeError(\n                'Integer can only work with integer expressions.')\n        try:\n            return _intcache[ival]\n        except KeyError:\n            # We only work with well-behaved integer types. This converts, for\n            # example, numpy.int32 instances.\n            obj = Expr.__new__(cls)\n            obj.p = ival\n\n            _intcache[ival] = obj\n            return obj\n\n    def __getnewargs__(self):\n        return (self.p,)\n\n    # Arithmetic operations are here for efficiency\n    def __int__(self):\n        return self.p\n\n    __long__ = __int__\n\n    def floor(self):\n        return Integer(self.p)\n\n    def ceiling(self):\n        return Integer(self.p)\n\n    def __neg__(self):\n        return Integer(-self.p)\n\n    def __abs__(self):\n        if self.p >= 0:\n            return self\n        else:\n            return Integer(-self.p)\n\n    def __divmod__(self, other):\n        from .containers import Tuple\n        if isinstance(other, Integer) and global_evaluate[0]:\n            return Tuple(*(divmod(self.p, other.p)))\n        else:\n            return Number.__divmod__(self, other)\n\n    def __rdivmod__(self, other):\n        from .containers import Tuple\n        if isinstance(other, integer_types) and global_evaluate[0]:\n            return Tuple(*(divmod(other, self.p)))\n        else:\n            try:\n                other = Number(other)\n            except TypeError:\n                msg = \"unsupported operand type(s) for divmod(): '%s' and '%s'\"\n                oname = type(other).__name__\n                sname = type(self).__name__\n                raise TypeError(msg % (oname, sname))\n            return Number.__divmod__(other, self)\n\n    # TODO make it decorator + bytecodehacks?\n    def __add__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p + other)\n            elif isinstance(other, Integer):\n                return Integer(self.p + other.p)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.q + other.p, other.q, 1)\n            return Rational.__add__(self, other)\n        else:\n            return Add(self, other)\n\n    def __radd__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other + self.p)\n            elif isinstance(other, Rational):\n                return Rational(other.p + self.p*other.q, other.q, 1)\n            return Rational.__radd__(self, other)\n        return Rational.__radd__(self, other)\n\n    def __sub__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p - other)\n            elif isinstance(other, Integer):\n                return Integer(self.p - other.p)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.q - other.p, other.q, 1)\n            return Rational.__sub__(self, other)\n        return Rational.__sub__(self, other)\n\n    def __rsub__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other - self.p)\n            elif isinstance(other, Rational):\n                return Rational(other.p - self.p*other.q, other.q, 1)\n            return Rational.__rsub__(self, other)\n        return Rational.__rsub__(self, other)\n\n    def __mul__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p*other)\n            elif isinstance(other, Integer):\n                return Integer(self.p*other.p)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.p, other.q, igcd(self.p, other.q))\n            return Rational.__mul__(self, other)\n        return Rational.__mul__(self, other)\n\n    def __rmul__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other*self.p)\n            elif isinstance(other, Rational):\n                return Rational(other.p*self.p, other.q, igcd(self.p, other.q))\n            return Rational.__rmul__(self, other)\n        return Rational.__rmul__(self, other)\n\n    def __mod__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p % other)\n            elif isinstance(other, Integer):\n                return Integer(self.p % other.p)\n            return Rational.__mod__(self, other)\n        return Rational.__mod__(self, other)\n\n    def __rmod__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other % self.p)\n            elif isinstance(other, Integer):\n                return Integer(other.p % self.p)\n            return Rational.__rmod__(self, other)\n        return Rational.__rmod__(self, other)\n\n    def __eq__(self, other):\n        if isinstance(other, integer_types):\n            return (self.p == other)\n        elif isinstance(other, Integer):\n            return (self.p == other.p)\n        return Rational.__eq__(self, other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __gt__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s > %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p > other.p)\n        return Rational.__gt__(self, other)\n\n    def __lt__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s < %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p < other.p)\n        return Rational.__lt__(self, other)\n\n    def __ge__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s >= %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p >= other.p)\n        return Rational.__ge__(self, other)\n\n    def __le__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s <= %s\" % (self, other))\n        if other.is_Integer:\n            return _sympify(self.p <= other.p)\n        return Rational.__le__(self, other)\n\n    def __hash__(self):\n        return hash(self.p)\n\n    def __index__(self):\n        return self.p\n\n    ########################################\n\n    def _eval_is_odd(self):\n        return bool(self.p % 2)\n\n    def _eval_power(self, expt):\n        \"\"\"\n        Tries to do some simplifications on self**expt\n\n        Returns None if no further simplifications can be done\n\n        When exponent is a fraction (so we have for example a square root),\n        we try to find a simpler representation by factoring the argument\n        up to factors of 2**15, e.g.\n\n          - sqrt(4) becomes 2\n          - sqrt(-4) becomes 2*I\n          - (2**(3+7)*3**(6+7))**Rational(1,7) becomes 6*18**(3/7)\n\n        Further simplification would require a special call to factorint on\n        the argument which is not done here for sake of speed.\n\n        \"\"\"\n        from sympy import perfect_power\n\n        if expt is S.Infinity:\n            if self.p > S.One:\n                return S.Infinity\n            # cases -1, 0, 1 are done in their respective classes\n            return S.Infinity + S.ImaginaryUnit*S.Infinity\n        if expt is S.NegativeInfinity:\n            return Rational(1, self)**S.Infinity\n        if not isinstance(expt, Number):\n            # simplify when expt is even\n            # (-2)**k --> 2**k\n            if self.is_negative and expt.is_even:\n                return (-self)**expt\n        if isinstance(expt, Float):\n            # Rational knows how to exponentiate by a Float\n            return super(Integer, self)._eval_power(expt)\n        if not isinstance(expt, Rational):\n            return\n        if expt is S.Half and self.is_negative:\n            # we extract I for this special case since everyone is doing so\n            return S.ImaginaryUnit*Pow(-self, expt)\n        if expt.is_negative:\n            # invert base and change sign on exponent\n            ne = -expt\n            if self.is_negative:\n                if expt.q != 1:\n                    return -(S.NegativeOne)**((expt.p % expt.q) /\n                            S(expt.q))*Rational(1, -self)**ne\n                else:\n                    return (S.NegativeOne)**ne*Rational(1, -self)**ne\n            else:\n                return Rational(1, self.p)**ne\n        # see if base is a perfect root, sqrt(4) --> 2\n        x, xexact = integer_nthroot(abs(self.p), expt.q)\n        if xexact:\n            # if it's a perfect root we've finished\n            result = Integer(x**abs(expt.p))\n            if self.is_negative:\n                result *= S.NegativeOne**expt\n            return result\n\n        # The following is an algorithm where we collect perfect roots\n        # from the factors of base.\n\n        # if it's not an nth root, it still might be a perfect power\n        b_pos = int(abs(self.p))\n        p = perfect_power(b_pos)\n        if p is not False:\n            dict = {p[0]: p[1]}\n        else:\n            dict = Integer(b_pos).factors(limit=2**15)\n\n        # now process the dict of factors\n        out_int = 1  # integer part\n        out_rad = 1  # extracted radicals\n        sqr_int = 1\n        sqr_gcd = 0\n        sqr_dict = {}\n        for prime, exponent in dict.items():\n            exponent *= expt.p\n            # remove multiples of expt.q: (2**12)**(1/10) -> 2*(2**2)**(1/10)\n            div_e, div_m = divmod(exponent, expt.q)\n            if div_e > 0:\n                out_int *= prime**div_e\n            if div_m > 0:\n                # see if the reduced exponent shares a gcd with e.q\n                # (2**2)**(1/10) -> 2**(1/5)\n                g = igcd(div_m, expt.q)\n                if g != 1:\n                    out_rad *= Pow(prime, Rational(div_m//g, expt.q//g))\n                else:\n                    sqr_dict[prime] = div_m\n        # identify gcd of remaining powers\n        for p, ex in sqr_dict.items():\n            if sqr_gcd == 0:\n                sqr_gcd = ex\n            else:\n                sqr_gcd = igcd(sqr_gcd, ex)\n                if sqr_gcd == 1:\n                    break\n        for k, v in sqr_dict.items():\n            sqr_int *= k**(v//sqr_gcd)\n        if sqr_int == b_pos and out_int == 1 and out_rad == 1:\n            result = None\n        else:\n            result = out_int*out_rad*Pow(sqr_int, Rational(sqr_gcd, expt.q))\n            if self.is_negative:\n                result *= Pow(S.NegativeOne, expt)\n        return result\n\n    def _eval_is_prime(self):\n        from sympy.ntheory import isprime\n\n        return isprime(self)\n\n    def _eval_is_composite(self):\n        if self > 1:\n            return fuzzy_not(self.is_prime)\n        else:\n            return False\n\n    def as_numer_denom(self):\n        return self, S.One\n\n    def __floordiv__(self, other):\n        return Integer(self.p // Integer(other).p)\n\n    def __rfloordiv__(self, other):\n        return Integer(Integer(other).p // self.p)\n\n# Add sympify converters\nfor i_type in integer_types:\n    converter[i_type] = Integer\n\n\nclass AlgebraicNumber(Expr):\n    \"\"\"Class for representing algebraic numbers in SymPy. \"\"\"\n\n    __slots__ = ['rep', 'root', 'alias', 'minpoly']\n\n    is_AlgebraicNumber = True\n    is_algebraic = True\n    is_number = True\n\n    def __new__(cls, expr, coeffs=None, alias=None, **args):\n        \"\"\"Construct a new algebraic number. \"\"\"\n        from sympy import Poly\n        from sympy.polys.polyclasses import ANP, DMP\n        from sympy.polys.numberfields import minimal_polynomial\n        from sympy.core.symbol import Symbol\n\n        expr = sympify(expr)\n\n        if isinstance(expr, (tuple, Tuple)):\n            minpoly, root = expr\n\n            if not minpoly.is_Poly:\n                minpoly = Poly(minpoly)\n        elif expr.is_AlgebraicNumber:\n            minpoly, root = expr.minpoly, expr.root\n        else:\n            minpoly, root = minimal_polynomial(\n                expr, args.get('gen'), polys=True), expr\n\n        dom = minpoly.get_domain()\n\n        if coeffs is not None:\n            if not isinstance(coeffs, ANP):\n                rep = DMP.from_sympy_list(sympify(coeffs), 0, dom)\n                scoeffs = Tuple(*coeffs)\n            else:\n                rep = DMP.from_list(coeffs.to_list(), 0, dom)\n                scoeffs = Tuple(*coeffs.to_list())\n\n            if rep.degree() >= minpoly.degree():\n                rep = rep.rem(minpoly.rep)\n\n        else:\n            rep = DMP.from_list([1, 0], 0, dom)\n            scoeffs = Tuple(1, 0)\n\n            if root.is_negative:\n                rep = -rep\n                scoeffs = Tuple(-1, 0)\n\n        sargs = (root, scoeffs)\n\n        if alias is not None:\n            if not isinstance(alias, Symbol):\n                alias = Symbol(alias)\n            sargs = sargs + (alias,)\n\n        obj = Expr.__new__(cls, *sargs)\n\n        obj.rep = rep\n        obj.root = root\n        obj.alias = alias\n        obj.minpoly = minpoly\n\n        return obj\n\n    def __hash__(self):\n        return super(AlgebraicNumber, self).__hash__()\n\n    def _eval_evalf(self, prec):\n        return self.as_expr()._evalf(prec)\n\n    @property\n    def is_aliased(self):\n        \"\"\"Returns ``True`` if ``alias`` was set. \"\"\"\n        return self.alias is not None\n\n    def as_poly(self, x=None):\n        \"\"\"Create a Poly instance from ``self``. \"\"\"\n        from sympy import Dummy, Poly, PurePoly\n        if x is not None:\n            return Poly.new(self.rep, x)\n        else:\n            if self.alias is not None:\n                return Poly.new(self.rep, self.alias)\n            else:\n                return PurePoly.new(self.rep, Dummy('x'))\n\n    def as_expr(self, x=None):\n        \"\"\"Create a Basic expression from ``self``. \"\"\"\n        return self.as_poly(x or self.root).as_expr().expand()\n\n    def coeffs(self):\n        \"\"\"Returns all SymPy coefficients of an algebraic number. \"\"\"\n        return [ self.rep.dom.to_sympy(c) for c in self.rep.all_coeffs() ]\n\n    def native_coeffs(self):\n        \"\"\"Returns all native coefficients of an algebraic number. \"\"\"\n        return self.rep.all_coeffs()\n\n    def to_algebraic_integer(self):\n        \"\"\"Convert ``self`` to an algebraic integer. \"\"\"\n        from sympy import Poly\n        f = self.minpoly\n\n        if f.LC() == 1:\n            return self\n\n        coeff = f.LC()**(f.degree() - 1)\n        poly = f.compose(Poly(f.gen/f.LC()))\n\n        minpoly = poly*coeff\n        root = f.LC()*self.root\n\n        return AlgebraicNumber((minpoly, root), self.coeffs())\n\n    def _eval_simplify(self, ratio, measure):\n        from sympy.polys import CRootOf, minpoly\n\n        for r in [r for r in self.minpoly.all_roots() if r.func != CRootOf]:\n            if minpoly(self.root - r).is_Symbol:\n                # use the matching root if it's simpler\n                if measure(r) < ratio*measure(self.root):\n                    return AlgebraicNumber(r)\n        return self\n\n\nclass RationalConstant(Rational):\n    \"\"\"\n    Abstract base class for rationals with specific behaviors\n\n    Derived classes must define class attributes p and q and should probably all\n    be singletons.\n    \"\"\"\n    __slots__ = []\n\n    def __new__(cls):\n        return AtomicExpr.__new__(cls)\n\n\nclass IntegerConstant(Integer):\n    __slots__ = []\n\n    def __new__(cls):\n        return AtomicExpr.__new__(cls)\n\n\nclass Zero(with_metaclass(Singleton, IntegerConstant)):\n    \"\"\"The number zero.\n\n    Zero is a singleton, and can be accessed by ``S.Zero``\n\n    Examples\n    ========\n\n    >>> from sympy import S, Integer, zoo\n    >>> Integer(0) is S.Zero\n    True\n    >>> 1/S.Zero\n    zoo\n\n    References\n    ==========\n\n    .. [1] http://en.wikipedia.org/wiki/Zero\n    \"\"\"\n\n    p = 0\n    q = 1\n    is_positive = False\n    is_negative = False\n    is_zero = True\n    is_number = True\n\n    __slots__ = []\n\n    @staticmethod\n    def __abs__():\n        return S.Zero\n\n    @staticmethod\n    def __neg__():\n        return S.Zero\n\n(D) \n    def limit_denominator(self, max_denominator=1000000):\n        \"\"\"Closest Rational to self with denominator at most max_denominator.\n\n        >>> from sympy import Rational\n        >>> Rational('3.141592653589793').limit_denominator(10)\n        22/7\n        >>> Rational('3.141592653589793').limit_denominator(100)\n        311/99\n\n        \"\"\"\n        f = fractions.Fraction(self.p, self.q)\n        return Rational(f.limit_denominator(fractions.Fraction(int(max_denominator))))\n\n    def __getnewargs__(self):\n        return (self.p, self.q)\n\n    def _hashable_content(self):\n        return (self.p, self.q)\n\n    def _eval_is_positive(self):\n        return self.p > 0\n\n    def _eval_is_zero(self):\n        return self.p == 0\n\n    def __neg__(self):\n        return Rational(-self.p, self.q)\n\n    @_sympifyit('other', NotImplemented)\n    def __add__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, Integer):\n                return Rational(self.p + self.q*other.p, self.q, 1)\n            elif isinstance(other, Rational):\n                #TODO: this can probably be optimized more\n                return Rational(self.p*other.q + self.q*other.p, self.q*other.q)\n            elif isinstance(other, Float):\n                return other + self\n            else:\n                return Number.__add__(self, other)\n        return Number.__add__(self, other)\n    __radd__ = __add__\n\n    @_sympifyit('other', NotImplemented)\n    def __sub__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, Integer):\n                return Rational(self.p - self.q*other.p, self.q, 1)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.q - self.q*other.p, self.q*other.q)\n            elif isinstance(other, Float):\n                return -other + self\n            else:\n                return Number.__sub__(self, other)\n        return Number.__sub__(self, other)\n    @_sympifyit('other', NotImplemented)\n    def __rsub__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, Integer):\n                return Rational(self.q*other.p - self.p, self.q, 1)\n            elif isinstance(other, Rational):\n                return Rational(self.q*other.p - self.p*other.q, self.q*other.q)\n            elif isinstance(other, Float):\n                return -self + other\n            else:\n                return Number.__rsub__(self, other)\n        return Number.__rsub__(self, other)\n    @_sympifyit('other', NotImplemented)\n    def __mul__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, Integer):\n                return Rational(self.p*other.p, self.q, igcd(other.p, self.q))\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.p, self.q*other.q, igcd(self.p, other.q)*igcd(self.q, other.p))\n            elif isinstance(other, Float):\n                return other*self\n            else:\n                return Number.__mul__(self, other)\n        return Number.__mul__(self, other)\n    __rmul__ = __mul__\n\n    @_sympifyit('other', NotImplemented)\n    def __div__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, Integer):\n                if self.p and other.p == S.Zero:\n                    return S.ComplexInfinity\n                else:\n                    return Rational(self.p, self.q*other.p, igcd(self.p, other.p))\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.q, self.q*other.p, igcd(self.p, other.p)*igcd(self.q, other.q))\n            elif isinstance(other, Float):\n                return self*(1/other)\n            else:\n                return Number.__div__(self, other)\n        return Number.__div__(self, other)\n    @_sympifyit('other', NotImplemented)\n    def __rdiv__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, Integer):\n                return Rational(other.p*self.q, self.p, igcd(self.p, other.p))\n            elif isinstance(other, Rational):\n                return Rational(other.p*self.q, other.q*self.p, igcd(self.p, other.p)*igcd(self.q, other.q))\n            elif isinstance(other, Float):\n                return other*(1/self)\n            else:\n                return Number.__rdiv__(self, other)\n        return Number.__rdiv__(self, other)\n    __truediv__ = __div__\n\n    @_sympifyit('other', NotImplemented)\n    def __mod__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, Rational):\n                n = (self.p*other.q) // (other.p*self.q)\n                return Rational(self.p*other.q - n*other.p*self.q, self.q*other.q)\n            if isinstance(other, Float):\n                # calculate mod with Rationals, *then* round the answer\n                return Float(self.__mod__(Rational(other)),\n                             precision=other._prec)\n            return Number.__mod__(self, other)\n        return Number.__mod__(self, other)\n\n    @_sympifyit('other', NotImplemented)\n    def __rmod__(self, other):\n        if isinstance(other, Rational):\n            return Rational.__mod__(other, self)\n        return Number.__rmod__(self, other)\n\n    def _eval_power(self, expt):\n        if isinstance(expt, Number):\n            if isinstance(expt, Float):\n                return self._eval_evalf(expt._prec)**expt\n            if expt.is_negative:\n                # (3/4)**-2 -> (4/3)**2\n                ne = -expt\n                if (ne is S.One):\n                    return Rational(self.q, self.p)\n                if self.is_negative:\n                    if expt.q != 1:\n                        return -(S.NegativeOne)**((expt.p % expt.q) /\n                               S(expt.q))*Rational(self.q, -self.p)**ne\n                    else:\n                        return S.NegativeOne**ne*Rational(self.q, -self.p)**ne\n                else:\n                    return Rational(self.q, self.p)**ne\n            if expt is S.Infinity:  # -oo already caught by test for negative\n                if self.p > self.q:\n                    # (3/2)**oo -> oo\n                    return S.Infinity\n                if self.p < -self.q:\n                    # (-3/2)**oo -> oo + I*oo\n                    return S.Infinity + S.Infinity*S.ImaginaryUnit\n                return S.Zero\n            if isinstance(expt, Integer):\n                # (4/3)**2 -> 4**2 / 3**2\n                return Rational(self.p**expt.p, self.q**expt.p, 1)\n            if isinstance(expt, Rational):\n                if self.p != 1:\n                    # (4/3)**(5/6) -> 4**(5/6)*3**(-5/6)\n                    return Integer(self.p)**expt*Integer(self.q)**(-expt)\n                # as the above caught negative self.p, now self is positive\n                return Integer(self.q)**Rational(\n                expt.p*(expt.q - 1), expt.q) / \\\n                    Integer(self.q)**Integer(expt.p)\n\n        if self.is_negative and expt.is_even:\n            return (-self)**expt\n\n        return\n\n    def _as_mpf_val(self, prec):\n        return mlib.from_rational(self.p, self.q, prec, rnd)\n\n    def _mpmath_(self, prec, rnd):\n        return mpmath.make_mpf(mlib.from_rational(self.p, self.q, prec, rnd))\n\n    def __abs__(self):\n        return Rational(abs(self.p), self.q)\n\n    def __int__(self):\n        p, q = self.p, self.q\n        if p < 0:\n            return -int(-p//q)\n        return int(p//q)\n\n    __long__ = __int__\n\n    def floor(self):\n        return Integer(self.p // self.q)\n\n    def ceiling(self):\n        return -Integer(-self.p // self.q)\n\n    def __eq__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            return NotImplemented\n        if other.is_NumberSymbol:\n            if other.is_irrational:\n                return False\n            return other.__eq__(self)\n        if other.is_Number:\n            if other.is_Rational:\n                # a Rational is always in reduced form so will never be 2/4\n                # so we can just check equivalence of args\n                return self.p == other.p and self.q == other.q\n            if other.is_Float:\n                return mlib.mpf_eq(self._as_mpf_val(other._prec), other._mpf_)\n        return False\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __gt__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s > %s\" % (self, other))\n        if other.is_NumberSymbol:\n            return other.__lt__(self)\n        expr = self\n        if other.is_Number:\n            if other.is_Rational:\n                return _sympify(bool(self.p*other.q > self.q*other.p))\n            if other.is_Float:\n                return _sympify(bool(mlib.mpf_gt(\n                    self._as_mpf_val(other._prec), other._mpf_)))\n        elif other.is_number and other.is_real:\n            expr, other = Integer(self.p), self.q*other\n        return Expr.__gt__(expr, other)\n\n    def __ge__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s >= %s\" % (self, other))\n        if other.is_NumberSymbol:\n            return other.__le__(self)\n        expr = self\n        if other.is_Number:\n            if other.is_Rational:\n                 return _sympify(bool(self.p*other.q >= self.q*other.p))\n            if other.is_Float:\n                return _sympify(bool(mlib.mpf_ge(\n                    self._as_mpf_val(other._prec), other._mpf_)))\n        elif other.is_number and other.is_real:\n            expr, other = Integer(self.p), self.q*other\n        return Expr.__ge__(expr, other)\n\n    def __lt__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s < %s\" % (self, other))\n        if other.is_NumberSymbol:\n            return other.__gt__(self)\n        expr = self\n        if other.is_Number:\n            if other.is_Rational:\n                return _sympify(bool(self.p*other.q < self.q*other.p))\n            if other.is_Float:\n                return _sympify(bool(mlib.mpf_lt(\n                    self._as_mpf_val(other._prec), other._mpf_)))\n        elif other.is_number and other.is_real:\n            expr, other = Integer(self.p), self.q*other\n        return Expr.__lt__(expr, other)\n\n    def __le__(self, other):\n        try:\n            other = _sympify(other)\n        except SympifyError:\n            raise TypeError(\"Invalid comparison %s <= %s\" % (self, other))\n        expr = self\n        if other.is_NumberSymbol:\n            return other.__ge__(self)\n        elif other.is_Number:\n            if other.is_Rational:\n                return _sympify(bool(self.p*other.q <= self.q*other.p))\n            if other.is_Float:\n                return _sympify(bool(mlib.mpf_le(\n                    self._as_mpf_val(other._prec), other._mpf_)))\n        elif other.is_number and other.is_real:\n            expr, other = Integer(self.p), self.q*other\n        return Expr.__le__(expr, other)\n\n    def __hash__(self):\n        return super(Rational, self).__hash__()\n\n    def factors(self, limit=None, use_trial=True, use_rho=False,\n                use_pm1=False, verbose=False, visual=False):\n        \"\"\"A wrapper to factorint which return factors of self that are\n        smaller than limit (or cheap to compute). Special methods of\n        factoring are disabled by default so that only trial division is used.\n        \"\"\"\n        from sympy.ntheory import factorrat\n\n        return factorrat(self, limit=limit, use_trial=use_trial,\n                      use_rho=use_rho, use_pm1=use_pm1,\n                      verbose=verbose).copy()\n\n    @_sympifyit('other', NotImplemented)\n    def gcd(self, other):\n        if isinstance(other, Rational):\n            if other is S.Zero:\n                return other\n            return Rational(\n                Integer(igcd(self.p, other.p)),\n                Integer(ilcm(self.q, other.q)))\n        return Number.gcd(self, other)\n\n    @_sympifyit('other', NotImplemented)\n    def lcm(self, other):\n        if isinstance(other, Rational):\n            return Rational(\n                self.p*other.p//igcd(self.p, other.p),\n                igcd(self.q, other.q))\n        return Number.lcm(self, other)\n\n    def as_numer_denom(self):\n        return Integer(self.p), Integer(self.q)\n\n    def _sage_(self):\n        import sage.all as sage\n        return sage.Integer(self.p)/sage.Integer(self.q)\n\n    def as_content_primitive(self, radical=False, clear=True):\n        \"\"\"Return the tuple (R, self/R) where R is the positive Rational\n        extracted from self.\n\n        Examples\n        ========\n\n        >>> from sympy import S\n        >>> (S(-3)/2).as_content_primitive()\n        (3/2, -1)\n\n        See docstring of Expr.as_content_primitive for more examples.\n        \"\"\"\n\n        if self:\n            if self.is_positive:\n                return self, S.One\n            return -self, S.NegativeOne\n        return S.One, self\n\n    def as_coeff_Mul(self, rational=False):\n        \"\"\"Efficiently extract the coefficient of a product. \"\"\"\n        return self, S.One\n\n    def as_coeff_Add(self, rational=False):\n        \"\"\"Efficiently extract the coefficient of a summation. \"\"\"\n        return self, S.Zero\n\n\n# int -> Integer\n_intcache = {}\n\n\n# TODO move this tracing facility to sympy/core/trace.py  ?\ndef _intcache_printinfo():\n    ints = sorted(_intcache.keys())\n    nhit = _intcache_hits\n    nmiss = _intcache_misses\n\n    if nhit == 0 and nmiss == 0:\n        print()\n        print('Integer cache statistic was not collected')\n        return\n\n    miss_ratio = float(nmiss) / (nhit + nmiss)\n\n    print()\n    print('Integer cache statistic')\n    print('-----------------------')\n    print()\n    print('#items: %i' % len(ints))\n    print()\n    print(' #hit   #miss               #total')\n    print()\n    print('%5i   %5i (%7.5f %%)   %5i' % (\n        nhit, nmiss, miss_ratio*100, nhit + nmiss)\n    )\n    print()\n    print(ints)\n\n_intcache_hits = 0\n_intcache_misses = 0\n\n\ndef int_trace(f):\n    import os\n    if os.getenv('SYMPY_TRACE_INT', 'no').lower() != 'yes':\n        return f\n\n    def Integer_tracer(cls, i):\n        global _intcache_hits, _intcache_misses\n\n        try:\n            _intcache_hits += 1\n            return _intcache[i]\n        except KeyError:\n            _intcache_hits -= 1\n            _intcache_misses += 1\n\n            return f(cls, i)\n\n    # also we want to hook our _intcache_printinfo into sys.atexit\n    import atexit\n    atexit.register(_intcache_printinfo)\n\n    return Integer_tracer\n\n\nclass Integer(Rational):\n\n    q = 1\n    is_integer = True\n    is_number = True\n\n    is_Integer = True\n\n    __slots__ = ['p']\n\n    def _as_mpf_val(self, prec):\n        return mlib.from_int(self.p, prec, rnd)\n\n    def _mpmath_(self, prec, rnd):\n        return mpmath.make_mpf(self._as_mpf_val(prec))\n\n    # TODO caching with decorator, but not to degrade performance\n    @int_trace\n    def __new__(cls, i):\n        if isinstance(i, string_types):\n            i = i.replace(' ', '')\n        # whereas we cannot, in general, make a Rational from an\n        # arbitrary expression, we can make an Integer unambiguously\n        # (except when a non-integer expression happens to round to\n        # an integer). So we proceed by taking int() of the input and\n        # let the int routines determine whether the expression can\n        # be made into an int or whether an error should be raised.\n        try:\n            ival = int(i)\n        except TypeError:\n            raise TypeError(\n                'Integer can only work with integer expressions.')\n        try:\n            return _intcache[ival]\n        except KeyError:\n            # We only work with well-behaved integer types. This converts, for\n            # example, numpy.int32 instances.\n            obj = Expr.__new__(cls)\n            obj.p = ival\n\n            _intcache[ival] = obj\n            return obj\n\n    def __getnewargs__(self):\n        return (self.p,)\n\n    # Arithmetic operations are here for efficiency\n    def __int__(self):\n        return self.p\n\n    __long__ = __int__\n\n    def floor(self):\n        return Integer(self.p)\n\n    def ceiling(self):\n        return Integer(self.p)\n\n    def __neg__(self):\n        return Integer(-self.p)\n\n    def __abs__(self):\n        if self.p >= 0:\n            return self\n        else:\n            return Integer(-self.p)\n\n    def __divmod__(self, other):\n        from .containers import Tuple\n        if isinstance(other, Integer) and global_evaluate[0]:\n            return Tuple(*(divmod(self.p, other.p)))\n        else:\n            return Number.__divmod__(self, other)\n\n    def __rdivmod__(self, other):\n        from .containers import Tuple\n        if isinstance(other, integer_types) and global_evaluate[0]:\n            return Tuple(*(divmod(other, self.p)))\n        else:\n            try:\n                other = Number(other)\n            except TypeError:\n                msg = \"unsupported operand type(s) for divmod(): '%s' and '%s'\"\n                oname = type(other).__name__\n                sname = type(self).__name__\n                raise TypeError(msg % (oname, sname))\n            return Number.__divmod__(other, self)\n\n    # TODO make it decorator + bytecodehacks?\n    def __add__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p + other)\n            elif isinstance(other, Integer):\n                return Integer(self.p + other.p)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.q + other.p, other.q, 1)\n            return Rational.__add__(self, other)\n        else:\n            return Add(self, other)\n\n    def __radd__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other + self.p)\n            elif isinstance(other, Rational):\n                return Rational(other.p + self.p*other.q, other.q, 1)\n            return Rational.__radd__(self, other)\n        return Rational.__radd__(self, other)\n\n    def __sub__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(self.p - other)\n            elif isinstance(other, Integer):\n                return Integer(self.p - other.p)\n            elif isinstance(other, Rational):\n                return Rational(self.p*other.q - other.p, other.q, 1)\n            return Rational.__sub__(self, other)\n        return Rational.__sub__(self, other)\n\n    def __rsub__(self, other):\n        if global_evaluate[0]:\n            if isinstance(other, integer_types):\n                return Integer(other - self.p)\n            elif isinstance(other, Rational):\n                return Rational(other.p - self.p*other.q, other.q, 1)\n            return Rational.__rsub__(self, other)\n        return Rational.__rsub__(self, other)\n\n    def __mul__(self, other):\n        if global_evaluate[0]:", "answer": "A"}
{"uuid": "04f7d615-4dd8-4985-8025-08316f65b203", "issue_statement": "vectors break pretty printing\n```py\r\nIn [1]: from sympy.vector import *\r\n\r\nIn [2]: e = CoordSysCartesian('e')\r\n\r\nIn [3]: (x/y)**t*e.j\r\nOut[3]:\r\n\u239b   t\u239e e_j\r\n\u239c\u239bx\u239e e_j \u239f\r\n\u239c\u239c\u2500\u239f \u239f\r\n\u239d\u239dy\u23a0 \u23a0\r\n```\r\n\r\nAlso, when it does print correctly, the baseline is wrong (it should be centered).", "choices": "(A)                     #We always wrap the measure numbers in\n                    #parentheses\n                    arg_str = self._print(\n                        v).parens()[0]\n\n                    o1.append(arg_str + ' ' + k._pretty_form)\n                vectstrs.append(k._pretty_form)\n\n        #outstr = u(\"\").join(o1)\n        if o1[0].startswith(u\" + \"):\n            o1[0] = o1[0][3:]\n        elif o1[0].startswith(\" \"):\n            o1[0] = o1[0][1:]\n        #Fixing the newlines\n        lengths = []\n        strs = ['']\n        for i, partstr in enumerate(o1):\n            # XXX: What is this hack?\n            if '\\n' in partstr:\n                tempstr = partstr\n                tempstr = tempstr.replace(vectstrs[i], '')\n                tempstr = tempstr.replace(u'\\N{RIGHT PARENTHESIS UPPER HOOK}',\n                                          u'\\N{RIGHT PARENTHESIS UPPER HOOK}'\n                                          + ' ' + vectstrs[i])\n                o1[i] = tempstr\n        o1 = [x.split('\\n') for x in o1]\n        n_newlines = max([len(x) for x in o1])\n        for parts in o1:\n            lengths.append(len(parts[0]))\n            for j in range(n_newlines):\n                if j+1 <= len(parts):\n                    if j >= len(strs):\n                        strs.append(' ' * (sum(lengths[:-1]) +\n                                           3*(len(lengths)-1)))\n                    if j == 0:\n                        strs[0] += parts[0] + ' + '\n                    else:\n                        strs[j] += parts[j] + ' '*(lengths[-1] -\n                                                   len(parts[j])+\n                                                   3)\n                else:\n                    if j >= len(strs):\n                        strs.append(' ' * (sum(lengths[:-1]) +\n                                           3*(len(lengths)-1)))\n                    strs[j] += ' '*(lengths[-1]+3)\n\n        return prettyForm(u'\\n'.join([s[:-3] for s in strs]))\n\n    def _print_NDimArray(self, expr):\n(B)                                                    len(parts[j])+\n                                                   3)\n                else:\n                    if j >= len(strs):\n                        strs.append(' ' * (sum(lengths[:-1]) +\n                                           3*(len(lengths)-1)))\n                    strs[j] += ' '*(lengths[-1]+3)\n\n        return prettyForm(u'\\n'.join([s[:-3] for s in strs]))\n\n    def _print_NDimArray(self, expr):\n        from sympy import ImmutableMatrix\n\n        if expr.rank() == 0:\n            return self._print(expr[()])\n\n        level_str = [[]] + [[] for i in range(expr.rank())]\n        shape_ranges = [list(range(i)) for i in expr.shape]\n        for outer_i in itertools.product(*shape_ranges):\n            level_str[-1].append(expr[outer_i])\n            even = True\n            for back_outer_i in range(expr.rank()-1, -1, -1):\n                if len(level_str[back_outer_i+1]) < expr.shape[back_outer_i]:\n                    break\n                if even:\n                    level_str[back_outer_i].append(level_str[back_outer_i+1])\n                else:\n                    level_str[back_outer_i].append(ImmutableMatrix(level_str[back_outer_i+1]))\n                    if len(level_str[back_outer_i + 1]) == 1:\n                        level_str[back_outer_i][-1] = ImmutableMatrix([[level_str[back_outer_i][-1]]])\n                even = not even\n                level_str[back_outer_i+1] = []\n\n        out_expr = level_str[0][0]\n        if expr.rank() % 2 == 1:\n            out_expr = ImmutableMatrix([out_expr])\n\n        return self._print(out_expr)\n\n    _print_ImmutableDenseNDimArray = _print_NDimArray\n    _print_ImmutableSparseNDimArray = _print_NDimArray\n    _print_MutableDenseNDimArray = _print_NDimArray\n    _print_MutableSparseNDimArray = _print_NDimArray\n\n    def _print_Piecewise(self, pexpr):\n\n        P = {}\n        for n, ec in enumerate(pexpr.args):\n            P[n, 0] = self._print(ec.expr)\n(C)         n_newlines = max([len(x) for x in o1])\n        for parts in o1:\n            lengths.append(len(parts[0]))\n            for j in range(n_newlines):\n                if j+1 <= len(parts):\n                    if j >= len(strs):\n                        strs.append(' ' * (sum(lengths[:-1]) +\n                                           3*(len(lengths)-1)))\n                    if j == 0:\n                        strs[0] += parts[0] + ' + '\n                    else:\n                        strs[j] += parts[j] + ' '*(lengths[-1] -\n                                                   len(parts[j])+\n                                                   3)\n                else:\n                    if j >= len(strs):\n                        strs.append(' ' * (sum(lengths[:-1]) +\n                                           3*(len(lengths)-1)))\n                    strs[j] += ' '*(lengths[-1]+3)\n\n        return prettyForm(u'\\n'.join([s[:-3] for s in strs]))\n\n    def _print_NDimArray(self, expr):\n        from sympy import ImmutableMatrix\n\n        if expr.rank() == 0:\n            return self._print(expr[()])\n\n        level_str = [[]] + [[] for i in range(expr.rank())]\n        shape_ranges = [list(range(i)) for i in expr.shape]\n        for outer_i in itertools.product(*shape_ranges):\n            level_str[-1].append(expr[outer_i])\n            even = True\n            for back_outer_i in range(expr.rank()-1, -1, -1):\n                if len(level_str[back_outer_i+1]) < expr.shape[back_outer_i]:\n                    break\n                if even:\n                    level_str[back_outer_i].append(level_str[back_outer_i+1])\n                else:\n                    level_str[back_outer_i].append(ImmutableMatrix(level_str[back_outer_i+1]))\n                    if len(level_str[back_outer_i + 1]) == 1:\n                        level_str[back_outer_i][-1] = ImmutableMatrix([[level_str[back_outer_i][-1]]])\n                even = not even\n                level_str[back_outer_i+1] = []\n\n        out_expr = level_str[0][0]\n        if expr.rank() % 2 == 1:\n            out_expr = ImmutableMatrix([out_expr])\n\n(D)         #Fixing the newlines\n        lengths = []\n        strs = ['']\n        for i, partstr in enumerate(o1):\n            # XXX: What is this hack?\n            if '\\n' in partstr:\n                tempstr = partstr\n                tempstr = tempstr.replace(vectstrs[i], '')\n                tempstr = tempstr.replace(u'\\N{RIGHT PARENTHESIS UPPER HOOK}',\n                                          u'\\N{RIGHT PARENTHESIS UPPER HOOK}'\n                                          + ' ' + vectstrs[i])\n                o1[i] = tempstr\n        o1 = [x.split('\\n') for x in o1]\n        n_newlines = max([len(x) for x in o1])\n        for parts in o1:\n            lengths.append(len(parts[0]))\n            for j in range(n_newlines):\n                if j+1 <= len(parts):\n                    if j >= len(strs):\n                        strs.append(' ' * (sum(lengths[:-1]) +\n                                           3*(len(lengths)-1)))\n                    if j == 0:\n                        strs[0] += parts[0] + ' + '\n                    else:\n                        strs[j] += parts[j] + ' '*(lengths[-1] -\n                                                   len(parts[j])+\n                                                   3)\n                else:\n                    if j >= len(strs):\n                        strs.append(' ' * (sum(lengths[:-1]) +\n                                           3*(len(lengths)-1)))\n                    strs[j] += ' '*(lengths[-1]+3)\n\n        return prettyForm(u'\\n'.join([s[:-3] for s in strs]))\n\n    def _print_NDimArray(self, expr):\n        from sympy import ImmutableMatrix\n\n        if expr.rank() == 0:\n            return self._print(expr[()])\n\n        level_str = [[]] + [[] for i in range(expr.rank())]\n        shape_ranges = [list(range(i)) for i in expr.shape]\n        for outer_i in itertools.product(*shape_ranges):\n            level_str[-1].append(expr[outer_i])\n            even = True\n            for back_outer_i in range(expr.rank()-1, -1, -1):\n                if len(level_str[back_outer_i+1]) < expr.shape[back_outer_i]:\n                    break", "answer": "D"}
{"uuid": "ada49f3c-e5e2-4cdc-be8c-be432885bbdd", "issue_statement": "LaTeX printer does not use the same order of monomials as pretty and str \nWhen printing a Poly, the str and pretty printers use the logical order of monomials, from highest to lowest degrees. But latex printer does not. \r\n```\r\n>>> var('a b c x')\r\n>>> p = Poly([a, 1, b, 2, c, 3], x)\r\n>>> p\r\nPoly(a*x**5 + x**4 + b*x**3 + 2*x**2 + c*x + 3, x, domain='ZZ[a,b,c]')\r\n>>> pretty(p)\r\n\"Poly(a*x**5 + x**4 + b*x**3 + 2*x**2 + c*x + 3, x, domain='ZZ[a,b,c]')\"\r\n>>> latex(p)\r\n'\\\\operatorname{Poly}{\\\\left( a x^{5} + b x^{3} + c x + x^{4} + 2 x^{2} + 3, x, domain=\\\\mathbb{Z}\\\\left[a, b, c\\\\right] \\\\right)}'\r\n```", "choices": "(A)     def _print_Contains(self, e):\n        return r\"%s \\in %s\" % tuple(self._print(a) for a in e.args)\n\n    def _print_FourierSeries(self, s):\n        return self._print_Add(s.truncate()) + self._print(r' + \\ldots')\n\n    def _print_FormalPowerSeries(self, s):\n        return self._print_Add(s.infinite)\n\n    def _print_FiniteField(self, expr):\n        return r\"\\mathbb{F}_{%s}\" % expr.mod\n\n    def _print_IntegerRing(self, expr):\n        return r\"\\mathbb{Z}\"\n\n    def _print_RationalField(self, expr):\n        return r\"\\mathbb{Q}\"\n\n    def _print_RealField(self, expr):\n        return r\"\\mathbb{R}\"\n\n    def _print_ComplexField(self, expr):\n        return r\"\\mathbb{C}\"\n\n    def _print_PolynomialRing(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left[%s\\right]\" % (domain, symbols)\n\n    def _print_FractionField(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left(%s\\right)\" % (domain, symbols)\n\n    def _print_PolynomialRingBase(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        inv = \"\"\n        if not expr.is_Poly:\n            inv = r\"S_<^{-1}\"\n        return r\"%s%s\\left[%s\\right]\" % (inv, domain, symbols)\n\n    def _print_Poly(self, poly):\n        cls = poly.__class__.__name__\n        expr = self._print(poly.as_expr())\n        gens = list(map(self._print, poly.gens))\n        domain = \"domain=%s\" % self._print(poly.get_domain())\n\n        args = \", \".join([expr] + gens + [domain])\n        if cls in accepted_latex_functions:\n(B)         return r\"\\mathbb{Z}\"\n\n    def _print_RationalField(self, expr):\n        return r\"\\mathbb{Q}\"\n\n    def _print_RealField(self, expr):\n        return r\"\\mathbb{R}\"\n\n    def _print_ComplexField(self, expr):\n        return r\"\\mathbb{C}\"\n\n    def _print_PolynomialRing(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left[%s\\right]\" % (domain, symbols)\n\n    def _print_FractionField(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left(%s\\right)\" % (domain, symbols)\n\n    def _print_PolynomialRingBase(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        inv = \"\"\n        if not expr.is_Poly:\n            inv = r\"S_<^{-1}\"\n        return r\"%s%s\\left[%s\\right]\" % (inv, domain, symbols)\n\n    def _print_Poly(self, poly):\n        cls = poly.__class__.__name__\n        expr = self._print(poly.as_expr())\n        gens = list(map(self._print, poly.gens))\n        domain = \"domain=%s\" % self._print(poly.get_domain())\n\n        args = \", \".join([expr] + gens + [domain])\n        if cls in accepted_latex_functions:\n            tex = r\"\\%s {\\left (%s \\right )}\" % (cls, args)\n        else:\n            tex = r\"\\operatorname{%s}{\\left( %s \\right)}\" % (cls, args)\n\n        return tex\n\n    def _print_ComplexRootOf(self, root):\n        cls = root.__class__.__name__\n        if cls == \"ComplexRootOf\":\n            cls = \"CRootOf\"\n        expr = self._print(root.expr)\n        index = root.index\n        if cls in accepted_latex_functions:\n(C)         domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left[%s\\right]\" % (domain, symbols)\n\n    def _print_FractionField(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left(%s\\right)\" % (domain, symbols)\n\n    def _print_PolynomialRingBase(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        inv = \"\"\n        if not expr.is_Poly:\n            inv = r\"S_<^{-1}\"\n        return r\"%s%s\\left[%s\\right]\" % (inv, domain, symbols)\n\n    def _print_Poly(self, poly):\n        cls = poly.__class__.__name__\n        expr = self._print(poly.as_expr())\n        gens = list(map(self._print, poly.gens))\n        domain = \"domain=%s\" % self._print(poly.get_domain())\n\n        args = \", \".join([expr] + gens + [domain])\n        if cls in accepted_latex_functions:\n            tex = r\"\\%s {\\left (%s \\right )}\" % (cls, args)\n        else:\n            tex = r\"\\operatorname{%s}{\\left( %s \\right)}\" % (cls, args)\n\n        return tex\n\n    def _print_ComplexRootOf(self, root):\n        cls = root.__class__.__name__\n        if cls == \"ComplexRootOf\":\n            cls = \"CRootOf\"\n        expr = self._print(root.expr)\n        index = root.index\n        if cls in accepted_latex_functions:\n            return r\"\\%s {\\left(%s, %d\\right)}\" % (cls, expr, index)\n        else:\n            return r\"\\operatorname{%s} {\\left(%s, %d\\right)}\" % (cls, expr, index)\n\n    def _print_RootSum(self, expr):\n        cls = expr.__class__.__name__\n        args = [self._print(expr.expr)]\n\n        if expr.fun is not S.IdentityFunction:\n            args.append(self._print(expr.fun))\n\n        if cls in accepted_latex_functions:\n(D)         return r\"\\left\\{%s\\; |\\; %s \\in %s \\wedge %s \\right\\}\" % (\n            vars_print,\n            vars_print,\n            self._print(s.base_set),\n            self._print(s.condition.as_expr()))\n\n    def _print_ComplexRegion(self, s):\n        vars_print = ', '.join([self._print(var) for var in s.variables])\n        return r\"\\left\\{%s\\; |\\; %s \\in %s \\right\\}\" % (\n            self._print(s.expr),\n            vars_print,\n            self._print(s.sets))\n\n    def _print_Contains(self, e):\n        return r\"%s \\in %s\" % tuple(self._print(a) for a in e.args)\n\n    def _print_FourierSeries(self, s):\n        return self._print_Add(s.truncate()) + self._print(r' + \\ldots')\n\n    def _print_FormalPowerSeries(self, s):\n        return self._print_Add(s.infinite)\n\n    def _print_FiniteField(self, expr):\n        return r\"\\mathbb{F}_{%s}\" % expr.mod\n\n    def _print_IntegerRing(self, expr):\n        return r\"\\mathbb{Z}\"\n\n    def _print_RationalField(self, expr):\n        return r\"\\mathbb{Q}\"\n\n    def _print_RealField(self, expr):\n        return r\"\\mathbb{R}\"\n\n    def _print_ComplexField(self, expr):\n        return r\"\\mathbb{C}\"\n\n    def _print_PolynomialRing(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left[%s\\right]\" % (domain, symbols)\n\n    def _print_FractionField(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left(%s\\right)\" % (domain, symbols)\n\n    def _print_PolynomialRingBase(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))", "answer": "A"}
{"uuid": "91e6442d-d3e3-40fa-bb26-4f34d3ae1f29", "issue_statement": "Poly(domain='RR[y,z]') doesn't work\n``` py\nIn [14]: Poly(1.2*x*y*z, x)\nOut[14]: Poly(1.2*y*z*x, x, domain='RR[y,z]')\n\nIn [15]: Poly(1.2*x*y*z, x, domain='RR[y,z]')\n---------------------------------------------------------------------------\nOptionError                               Traceback (most recent call last)\n<ipython-input-15-d83389519ae1> in <module>()\n----> 1 Poly(1.2*x*y*z, x, domain='RR[y,z]')\n\n/Users/aaronmeurer/Documents/Python/sympy/sympy-scratch/sympy/polys/polytools.py in __new__(cls, rep, *gens, **args)\n     69     def __new__(cls, rep, *gens, **args):\n     70         \"\"\"Create a new polynomial instance out of something useful. \"\"\"\n---> 71         opt = options.build_options(gens, args)\n     72\n     73         if 'order' in opt:\n\n/Users/aaronmeurer/Documents/Python/sympy/sympy-scratch/sympy/polys/polyoptions.py in build_options(gens, args)\n    718\n    719     if len(args) != 1 or 'opt' not in args or gens:\n--> 720         return Options(gens, args)\n    721     else:\n    722         return args['opt']\n\n/Users/aaronmeurer/Documents/Python/sympy/sympy-scratch/sympy/polys/polyoptions.py in __init__(self, gens, args, flags, strict)\n    151                     self[option] = cls.preprocess(value)\n    152\n--> 153         preprocess_options(args)\n    154\n    155         for key, value in dict(defaults).items():\n\n/Users/aaronmeurer/Documents/Python/sympy/sympy-scratch/sympy/polys/polyoptions.py in preprocess_options(args)\n    149\n    150                 if value is not None:\n--> 151                     self[option] = cls.preprocess(value)\n    152\n    153         preprocess_options(args)\n\n/Users/aaronmeurer/Documents/Python/sympy/sympy-scratch/sympy/polys/polyoptions.py in preprocess(cls, domain)\n    480                 return sympy.polys.domains.QQ.algebraic_field(*gens)\n    481\n--> 482         raise OptionError('expected a valid domain specification, got %s' % domain)\n    483\n    484     @classmethod\n\nOptionError: expected a valid domain specification, got RR[y,z]\n```\n\nAlso, the wording of error message could be improved", "choices": "(A)             r = cls._re_complexfield.match(domain)\n\n            if r is not None:\n                _, _, prec = r.groups()\n\n                if prec is None:\n                    return sympy.polys.domains.CC\n                else:\n                    return sympy.polys.domains.ComplexField(int(prec))\n\n            r = cls._re_finitefield.match(domain)\n\n            if r is not None:\n                return sympy.polys.domains.FF(int(r.groups()[1]))\n\n            r = cls._re_polynomial.match(domain)\n\n            if r is not None:\n                ground, gens = r.groups()\n\n                gens = list(map(sympify, gens.split(',')))\n\n                if ground in ['Z', 'ZZ']:\n                    return sympy.polys.domains.ZZ.poly_ring(*gens)\n                else:\n                    return sympy.polys.domains.QQ.poly_ring(*gens)\n\n            r = cls._re_fraction.match(domain)\n\n            if r is not None:\n                ground, gens = r.groups()\n\n                gens = list(map(sympify, gens.split(',')))\n\n                if ground in ['Z', 'ZZ']:\n                    return sympy.polys.domains.ZZ.frac_field(*gens)\n                else:\n                    return sympy.polys.domains.QQ.frac_field(*gens)\n\n            r = cls._re_algebraic.match(domain)\n\n            if r is not None:\n                gens = list(map(sympify, r.groups()[1].split(',')))\n                return sympy.polys.domains.QQ.algebraic_field(*gens)\n\n        raise OptionError('expected a valid domain specification, got %s' % domain)\n\n    @classmethod\n    def postprocess(cls, options):\n        if 'gens' in options and 'domain' in options and options['domain'].is_Composite and \\\n                (set(options['domain'].symbols) & set(options['gens'])):\n            raise GeneratorsError(\n                \"ground domain and generators interfere together\")\n        elif ('gens' not in options or not options['gens']) and \\\n                'domain' in options and options['domain'] == sympy.polys.domains.EX:\n            raise GeneratorsError(\"you have to provide generators because EX domain was requested\")\n\n\nclass Split(with_metaclass(OptionType, BooleanOption)):\n    \"\"\"``split`` option to polynomial manipulation functions. \"\"\"\n\n    option = 'split'\n\n    requires = []\n    excludes = ['field', 'greedy', 'domain', 'gaussian', 'extension',\n        'modulus', 'symmetric']\n(B)     _re_realfield = re.compile(r\"^(R|RR)(_(\\d+))?$\")\n    _re_complexfield = re.compile(r\"^(C|CC)(_(\\d+))?$\")\n    _re_finitefield = re.compile(r\"^(FF|GF)\\((\\d+)\\)$\")\n    _re_polynomial = re.compile(r\"^(Z|ZZ|Q|QQ)\\[(.+)\\]$\")\n    _re_fraction = re.compile(r\"^(Z|ZZ|Q|QQ)\\((.+)\\)$\")\n    _re_algebraic = re.compile(r\"^(Q|QQ)\\<(.+)\\>$\")\n\n    @classmethod\n    def preprocess(cls, domain):\n        if isinstance(domain, sympy.polys.domains.Domain):\n            return domain\n        elif hasattr(domain, 'to_domain'):\n            return domain.to_domain()\n        elif isinstance(domain, string_types):\n            if domain in ['Z', 'ZZ']:\n                return sympy.polys.domains.ZZ\n\n            if domain in ['Q', 'QQ']:\n                return sympy.polys.domains.QQ\n\n            if domain == 'EX':\n                return sympy.polys.domains.EX\n\n            r = cls._re_realfield.match(domain)\n\n            if r is not None:\n                _, _, prec = r.groups()\n\n                if prec is None:\n                    return sympy.polys.domains.RR\n                else:\n                    return sympy.polys.domains.RealField(int(prec))\n\n            r = cls._re_complexfield.match(domain)\n\n            if r is not None:\n                _, _, prec = r.groups()\n\n                if prec is None:\n                    return sympy.polys.domains.CC\n                else:\n                    return sympy.polys.domains.ComplexField(int(prec))\n\n            r = cls._re_finitefield.match(domain)\n\n            if r is not None:\n                return sympy.polys.domains.FF(int(r.groups()[1]))\n\n            r = cls._re_polynomial.match(domain)\n\n            if r is not None:\n                ground, gens = r.groups()\n\n                gens = list(map(sympify, gens.split(',')))\n\n                if ground in ['Z', 'ZZ']:\n                    return sympy.polys.domains.ZZ.poly_ring(*gens)\n                else:\n                    return sympy.polys.domains.QQ.poly_ring(*gens)\n\n            r = cls._re_fraction.match(domain)\n\n            if r is not None:\n                ground, gens = r.groups()\n\n                gens = list(map(sympify, gens.split(',')))\n(C)             if domain in ['Q', 'QQ']:\n                return sympy.polys.domains.QQ\n\n            if domain == 'EX':\n                return sympy.polys.domains.EX\n\n            r = cls._re_realfield.match(domain)\n\n            if r is not None:\n                _, _, prec = r.groups()\n\n                if prec is None:\n                    return sympy.polys.domains.RR\n                else:\n                    return sympy.polys.domains.RealField(int(prec))\n\n            r = cls._re_complexfield.match(domain)\n\n            if r is not None:\n                _, _, prec = r.groups()\n\n                if prec is None:\n                    return sympy.polys.domains.CC\n                else:\n                    return sympy.polys.domains.ComplexField(int(prec))\n\n            r = cls._re_finitefield.match(domain)\n\n            if r is not None:\n                return sympy.polys.domains.FF(int(r.groups()[1]))\n\n            r = cls._re_polynomial.match(domain)\n\n            if r is not None:\n                ground, gens = r.groups()\n\n                gens = list(map(sympify, gens.split(',')))\n\n                if ground in ['Z', 'ZZ']:\n                    return sympy.polys.domains.ZZ.poly_ring(*gens)\n                else:\n                    return sympy.polys.domains.QQ.poly_ring(*gens)\n\n            r = cls._re_fraction.match(domain)\n\n            if r is not None:\n                ground, gens = r.groups()\n\n                gens = list(map(sympify, gens.split(',')))\n\n                if ground in ['Z', 'ZZ']:\n                    return sympy.polys.domains.ZZ.frac_field(*gens)\n                else:\n                    return sympy.polys.domains.QQ.frac_field(*gens)\n\n            r = cls._re_algebraic.match(domain)\n\n            if r is not None:\n                gens = list(map(sympify, r.groups()[1].split(',')))\n                return sympy.polys.domains.QQ.algebraic_field(*gens)\n\n        raise OptionError('expected a valid domain specification, got %s' % domain)\n\n    @classmethod\n    def postprocess(cls, options):\n        if 'gens' in options and 'domain' in options and options['domain'].is_Composite and \\\n(D)     def default(cls):\n        return None\n\n    requires = []\n    excludes = ['domain', 'split', 'gaussian', 'extension', 'modulus', 'symmetric']\n\n\nclass Domain(with_metaclass(OptionType, Option)):\n    \"\"\"``domain`` option to polynomial manipulation functions. \"\"\"\n\n    option = 'domain'\n\n    requires = []\n    excludes = ['field', 'greedy', 'split', 'gaussian', 'extension']\n\n    after = ['gens']\n\n    _re_realfield = re.compile(r\"^(R|RR)(_(\\d+))?$\")\n    _re_complexfield = re.compile(r\"^(C|CC)(_(\\d+))?$\")\n    _re_finitefield = re.compile(r\"^(FF|GF)\\((\\d+)\\)$\")\n    _re_polynomial = re.compile(r\"^(Z|ZZ|Q|QQ)\\[(.+)\\]$\")\n    _re_fraction = re.compile(r\"^(Z|ZZ|Q|QQ)\\((.+)\\)$\")\n    _re_algebraic = re.compile(r\"^(Q|QQ)\\<(.+)\\>$\")\n\n    @classmethod\n    def preprocess(cls, domain):\n        if isinstance(domain, sympy.polys.domains.Domain):\n            return domain\n        elif hasattr(domain, 'to_domain'):\n            return domain.to_domain()\n        elif isinstance(domain, string_types):\n            if domain in ['Z', 'ZZ']:\n                return sympy.polys.domains.ZZ\n\n            if domain in ['Q', 'QQ']:\n                return sympy.polys.domains.QQ\n\n            if domain == 'EX':\n                return sympy.polys.domains.EX\n\n            r = cls._re_realfield.match(domain)\n\n            if r is not None:\n                _, _, prec = r.groups()\n\n                if prec is None:\n                    return sympy.polys.domains.RR\n                else:\n                    return sympy.polys.domains.RealField(int(prec))\n\n            r = cls._re_complexfield.match(domain)\n\n            if r is not None:\n                _, _, prec = r.groups()\n\n                if prec is None:\n                    return sympy.polys.domains.CC\n                else:\n                    return sympy.polys.domains.ComplexField(int(prec))\n\n            r = cls._re_finitefield.match(domain)\n\n            if r is not None:\n                return sympy.polys.domains.FF(int(r.groups()[1]))\n\n            r = cls._re_polynomial.match(domain)", "answer": "B"}
{"uuid": "d1a79e9a-5176-4f55-8079-89fefa58acf3", "issue_statement": "Latex printer does not support full inverse trig function names for acsc and asec\nFor example\r\n`latex(asin(x), inv_trig_style=\"full\")` works as expected returning `'\\\\arcsin{\\\\left (x \\\\right )}'`\r\nBut `latex(acsc(x), inv_trig_style=\"full\")` gives `'\\\\operatorname{acsc}{\\\\left (x \\\\right )}'` instead of `'\\\\operatorname{arccsc}{\\\\left (x \\\\right )}'`\r\n\r\nA fix seems to be to change line 743 of sympy/printing/latex.py from\r\n`inv_trig_table = [\"asin\", \"acos\", \"atan\", \"acot\"]` to\r\n`inv_trig_table = [\"asin\", \"acos\", \"atan\", \"acsc\", \"asec\", \"acot\"]`", "choices": "(A)                 len(args) == 1 and \\\n                not self._needs_function_brackets(expr.args[0])\n\n            inv_trig_table = [\"asin\", \"acos\", \"atan\", \"acot\"]\n\n            # If the function is an inverse trig function, handle the style\n            if func in inv_trig_table:\n(B) \n            # If the function is an inverse trig function, handle the style\n            if func in inv_trig_table:\n                if inv_trig_style == \"abbreviated\":\n                    func = func\n                elif inv_trig_style == \"full\":\n                    func = \"arc\" + func[1:]\n(C) \n            inv_trig_table = [\"asin\", \"acos\", \"atan\", \"acot\"]\n\n            # If the function is an inverse trig function, handle the style\n            if func in inv_trig_table:\n                if inv_trig_style == \"abbreviated\":\n                    func = func\n(D)             # If it is applicable to fold the argument brackets\n            can_fold_brackets = self._settings['fold_func_brackets'] and \\\n                len(args) == 1 and \\\n                not self._needs_function_brackets(expr.args[0])\n\n            inv_trig_table = [\"asin\", \"acos\", \"atan\", \"acot\"]\n", "answer": "A"}
{"uuid": "1a36ce9f-a213-4a5c-8422-ed7405c7f9d8", "issue_statement": "Error pretty printing MatAdd\n```py\r\n>>> pprint(MatrixSymbol('x', n, n) + MatrixSymbol('y*', n, n))\r\nTraceback (most recent call last):\r\n  File \"./sympy/core/sympify.py\", line 368, in sympify\r\n    expr = parse_expr(a, local_dict=locals, transformations=transformations, evaluate=evaluate)\r\n  File \"./sympy/parsing/sympy_parser.py\", line 950, in parse_expr\r\n    return eval_expr(code, local_dict, global_dict)\r\n  File \"./sympy/parsing/sympy_parser.py\", line 863, in eval_expr\r\n    code, global_dict, local_dict)  # take local objects in preference\r\n  File \"<string>\", line 1\r\n    Symbol ('y' )*\r\n                 ^\r\nSyntaxError: unexpected EOF while parsing\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"./sympy/printing/pretty/pretty.py\", line 2371, in pretty_print\r\n    use_unicode_sqrt_char=use_unicode_sqrt_char))\r\n  File \"./sympy/printing/pretty/pretty.py\", line 2331, in pretty\r\n    return pp.doprint(expr)\r\n  File \"./sympy/printing/pretty/pretty.py\", line 62, in doprint\r\n    return self._print(expr).render(**self._settings)\r\n  File \"./sympy/printing/printer.py\", line 274, in _print\r\n    return getattr(self, printmethod)(expr, *args, **kwargs)\r\n  File \"./sympy/printing/pretty/pretty.py\", line 828, in _print_MatAdd\r\n    if S(item.args[0]).is_negative:\r\n  File \"./sympy/core/sympify.py\", line 370, in sympify\r\n    raise SympifyError('could not parse %r' % a, exc)\r\nsympy.core.sympify.SympifyError: Sympify of expression 'could not parse 'y*'' failed, because of exception being raised:\r\nSyntaxError: unexpected EOF while parsing (<string>, line 1)\r\n```\r\n\r\nThe code shouldn't be using sympify to handle string arguments from MatrixSymbol.\r\n\r\nI don't even understand what the code is doing. Why does it omit the `+` when the first argument is negative? This seems to assume that the arguments of MatAdd have a certain form, and that they will always print a certain way if they are negative.", "choices": "(A)         for item in expr.args:\n            pform = self._print(item)\n            if s is None:\n                s = pform     # First element\n            else:\n                if S(item.args[0]).is_negative:\n                    s = prettyForm(*stringPict.next(s, ' '))\n                    pform = self._print(item)\n(B)             if s is None:\n                s = pform     # First element\n            else:\n                if S(item.args[0]).is_negative:\n                    s = prettyForm(*stringPict.next(s, ' '))\n                    pform = self._print(item)\n                else:\n                    s = prettyForm(*stringPict.next(s, ' + '))\n(C)                     s = prettyForm(*stringPict.next(s, ' '))\n                    pform = self._print(item)\n                else:\n                    s = prettyForm(*stringPict.next(s, ' + '))\n                s = prettyForm(*stringPict.next(s, pform))\n\n        return s\n\n(D)             else:\n                if S(item.args[0]).is_negative:\n                    s = prettyForm(*stringPict.next(s, ' '))\n                    pform = self._print(item)\n                else:\n                    s = prettyForm(*stringPict.next(s, ' + '))\n                s = prettyForm(*stringPict.next(s, pform))\n", "answer": "B"}
{"uuid": "ca1c5e5e-b854-40f3-871d-decb7d422b8c", "issue_statement": "lambdify does not work with certain MatrixSymbol names even with dummify=True\n`lambdify` is happy with curly braces in a symbol name and with `MatrixSymbol`s, but not with both at the same time, even if `dummify` is `True`.\r\n\r\nHere is some basic code that gives the error.\r\n```\r\nimport sympy as sy\r\ncurlyx = sy.symbols(\"{x}\")\r\nv = sy.MatrixSymbol(\"v\", 2, 1)\r\ncurlyv = sy.MatrixSymbol(\"{v}\", 2, 1)\r\n```\r\n\r\nThe following two lines of code work:\r\n```\r\ncurlyScalarId = sy.lambdify(curlyx, curlyx)\r\nvectorId = sy.lambdify(v,v)\r\n```\r\n\r\nThe following two lines of code give a `SyntaxError`:\r\n```\r\ncurlyVectorId = sy.lambdify(curlyv, curlyv)\r\ncurlyVectorIdDummified = sy.lambdify(curlyv, curlyv, dummify=True)\r\n```", "choices": "(A)             return isinstance(ident, str) and cls._safe_ident_re.match(ident) \\\n                and not (keyword.iskeyword(ident) or ident == 'None')\n\n\n    def _preprocess(self, args, expr):\n        \"\"\"Preprocess args, expr to replace arguments that do not map\n        to valid Python identifiers.\n\n        Returns string form of args, and updated expr.\n        \"\"\"\n        from sympy import Dummy, Symbol, Function, flatten\n        from sympy.matrices import DeferredVector\n\n        dummify = self._dummify\n\n        # Args of type Dummy can cause name collisions with args\n        # of type Symbol.  Force dummify of everything in this\n        # situation.\n        if not dummify:\n            dummify = any(isinstance(arg, Dummy) for arg in flatten(args))\n\n        argstrs = []\n        for arg in args:\n            if iterable(arg):\n                nested_argstrs, expr = self._preprocess(arg, expr)\n                argstrs.append(nested_argstrs)\n            elif isinstance(arg, DeferredVector):\n                argstrs.append(str(arg))\n            elif isinstance(arg, Symbol):\n                argrep = self._argrepr(arg)\n\n                if dummify or not self._is_safe_ident(argrep):\n                    dummy = Dummy()\n                    argstrs.append(self._argrepr(dummy))\n                    expr = self._subexpr(expr, {arg: dummy})\n                else:\n                    argstrs.append(argrep)\n            elif isinstance(arg, Function):\n                dummy = Dummy()\n                argstrs.append(self._argrepr(dummy))\n                expr = self._subexpr(expr, {arg: dummy})\n            else:\n                argstrs.append(str(arg))\n\n        return argstrs, expr\n\n    def _subexpr(self, expr, dummies_dict):\n        from sympy.matrices import DeferredVector\n        from sympy import sympify\n\n        try:\n            expr = sympify(expr).xreplace(dummies_dict)\n(B) \n        return '\\n'.join(funclines) + '\\n'\n\n    if PY3:\n        @classmethod\n        def _is_safe_ident(cls, ident):\n            return isinstance(ident, str) and ident.isidentifier() \\\n                    and not keyword.iskeyword(ident)\n    else:\n        _safe_ident_re = re.compile('^[a-zA-Z_][a-zA-Z0-9_]*$')\n\n        @classmethod\n        def _is_safe_ident(cls, ident):\n            return isinstance(ident, str) and cls._safe_ident_re.match(ident) \\\n                and not (keyword.iskeyword(ident) or ident == 'None')\n\n\n    def _preprocess(self, args, expr):\n        \"\"\"Preprocess args, expr to replace arguments that do not map\n        to valid Python identifiers.\n\n        Returns string form of args, and updated expr.\n        \"\"\"\n        from sympy import Dummy, Symbol, Function, flatten\n        from sympy.matrices import DeferredVector\n\n        dummify = self._dummify\n\n        # Args of type Dummy can cause name collisions with args\n        # of type Symbol.  Force dummify of everything in this\n        # situation.\n        if not dummify:\n            dummify = any(isinstance(arg, Dummy) for arg in flatten(args))\n\n        argstrs = []\n        for arg in args:\n            if iterable(arg):\n                nested_argstrs, expr = self._preprocess(arg, expr)\n                argstrs.append(nested_argstrs)\n            elif isinstance(arg, DeferredVector):\n                argstrs.append(str(arg))\n            elif isinstance(arg, Symbol):\n                argrep = self._argrepr(arg)\n\n                if dummify or not self._is_safe_ident(argrep):\n                    dummy = Dummy()\n                    argstrs.append(self._argrepr(dummy))\n                    expr = self._subexpr(expr, {arg: dummy})\n                else:\n                    argstrs.append(argrep)\n            elif isinstance(arg, Function):\n                dummy = Dummy()\n(C)         dummify = self._dummify\n\n        # Args of type Dummy can cause name collisions with args\n        # of type Symbol.  Force dummify of everything in this\n        # situation.\n        if not dummify:\n            dummify = any(isinstance(arg, Dummy) for arg in flatten(args))\n\n        argstrs = []\n        for arg in args:\n            if iterable(arg):\n                nested_argstrs, expr = self._preprocess(arg, expr)\n                argstrs.append(nested_argstrs)\n            elif isinstance(arg, DeferredVector):\n                argstrs.append(str(arg))\n            elif isinstance(arg, Symbol):\n                argrep = self._argrepr(arg)\n\n                if dummify or not self._is_safe_ident(argrep):\n                    dummy = Dummy()\n                    argstrs.append(self._argrepr(dummy))\n                    expr = self._subexpr(expr, {arg: dummy})\n                else:\n                    argstrs.append(argrep)\n            elif isinstance(arg, Function):\n                dummy = Dummy()\n                argstrs.append(self._argrepr(dummy))\n                expr = self._subexpr(expr, {arg: dummy})\n            else:\n                argstrs.append(str(arg))\n\n        return argstrs, expr\n\n    def _subexpr(self, expr, dummies_dict):\n        from sympy.matrices import DeferredVector\n        from sympy import sympify\n\n        try:\n            expr = sympify(expr).xreplace(dummies_dict)\n        except Exception:\n            if isinstance(expr, DeferredVector):\n                pass\n            elif isinstance(expr, dict):\n                k = [self._subexpr(sympify(a), dummies_dict) for a in expr.keys()]\n                v = [self._subexpr(sympify(a), dummies_dict) for a in expr.values()]\n                expr = dict(zip(k, v))\n            elif isinstance(expr, tuple):\n                expr = tuple(self._subexpr(sympify(a), dummies_dict) for a in expr)\n            elif isinstance(expr, list):\n                expr = [self._subexpr(sympify(a), dummies_dict) for a in expr]\n        return expr\n\n(D)             elif isinstance(arg, DeferredVector):\n                argstrs.append(str(arg))\n            elif isinstance(arg, Symbol):\n                argrep = self._argrepr(arg)\n\n                if dummify or not self._is_safe_ident(argrep):\n                    dummy = Dummy()\n                    argstrs.append(self._argrepr(dummy))\n                    expr = self._subexpr(expr, {arg: dummy})\n                else:\n                    argstrs.append(argrep)\n            elif isinstance(arg, Function):\n                dummy = Dummy()\n                argstrs.append(self._argrepr(dummy))\n                expr = self._subexpr(expr, {arg: dummy})\n            else:\n                argstrs.append(str(arg))\n\n        return argstrs, expr\n\n    def _subexpr(self, expr, dummies_dict):\n        from sympy.matrices import DeferredVector\n        from sympy import sympify\n\n        try:\n            expr = sympify(expr).xreplace(dummies_dict)\n        except Exception:\n            if isinstance(expr, DeferredVector):\n                pass\n            elif isinstance(expr, dict):\n                k = [self._subexpr(sympify(a), dummies_dict) for a in expr.keys()]\n                v = [self._subexpr(sympify(a), dummies_dict) for a in expr.values()]\n                expr = dict(zip(k, v))\n            elif isinstance(expr, tuple):\n                expr = tuple(self._subexpr(sympify(a), dummies_dict) for a in expr)\n            elif isinstance(expr, list):\n                expr = [self._subexpr(sympify(a), dummies_dict) for a in expr]\n        return expr\n\n    def _print_funcargwrapping(self, args):\n        \"\"\"Generate argument wrapping code.\n\n        args is the argument list of the generated function (strings).\n\n        Return value is a list of lines of code that will be inserted  at\n        the beginning of the function definition.\n        \"\"\"\n        return []\n\n    def _print_unpacking(self, unpackto, arg):\n        \"\"\"Generate argument unpacking code.\n", "answer": "A"}
{"uuid": "e1fa44d3-cfa8-47c1-a309-3d1ee0301d4c", "issue_statement": "LaTeX printing for Matrix Expression\n```py\r\n>>> A = MatrixSymbol(\"A\", n, n)\r\n>>> latex(trace(A**2))\r\n'Trace(A**2)'\r\n```\r\n\r\nThe bad part is not only is Trace not recognized, but whatever printer is being used doesn't fallback to the LaTeX printer for the inner expression (it should be `A^2`).", "choices": "(A)         else:\n            return expr\n\n    def _print_bool(self, e):\n        return r\"\\mathrm{%s}\" % e\n\n    _print_BooleanTrue = _print_bool\n    _print_BooleanFalse = _print_bool\n\n    def _print_NoneType(self, e):\n        return r\"\\mathrm{%s}\" % e\n\n\n    def _print_Add(self, expr, order=None):\n        if self.order == 'none':\n            terms = list(expr.args)\n        else:\n            terms = self._as_ordered_terms(expr, order=order)\n\n        tex = \"\"\n        for i, term in enumerate(terms):\n            if i == 0:\n                pass\n            elif _coeff_isneg(term):\n                tex += \" - \"\n                term = -term\n            else:\n                tex += \" + \"\n            term_tex = self._print(term)\n            if self._needs_add_brackets(term):\n                term_tex = r\"\\left(%s\\right)\" % term_tex\n            tex += term_tex\n\n        return tex\n\n    def _print_Cycle(self, expr):\n        from sympy.combinatorics.permutations import Permutation\n        if expr.size == 0:\n            return r\"\\left( \\right)\"\n        expr = Permutation(expr)\n        expr_perm = expr.cyclic_form\n        siz = expr.size\n        if expr.array_form[-1] == siz - 1:\n            expr_perm = expr_perm + [[siz - 1]]\n        term_tex = ''\n        for i in expr_perm:\n            term_tex += str(i).replace(',', r\"\\;\")\n        term_tex = term_tex.replace('[', r\"\\left( \")\n        term_tex = term_tex.replace(']', r\"\\right)\")\n        return term_tex\n\n    _print_Permutation = _print_Cycle\n\n    def _print_Float(self, expr):\n        # Based off of that in StrPrinter\n        dps = prec_to_dps(expr._prec)\n        str_real = mlib.to_str(expr._mpf_, dps, strip_zeros=True)\n\n        # Must always have a mul symbol (as 2.5 10^{20} just looks odd)\n        # thus we use the number separator\n        separator = self._settings['mul_symbol_latex_numbers']\n\n        if 'e' in str_real:\n            (mant, exp) = str_real.split('e')\n\n            if exp[0] == '+':\n                exp = exp[1:]\n\n            return r\"%s%s10^{%s}\" % (mant, separator, exp)\n        elif str_real == \"+inf\":\n            return r\"\\infty\"\n        elif str_real == \"-inf\":\n            return r\"- \\infty\"\n        else:\n            return str_real\n\n    def _print_Cross(self, expr):\n        vec1 = expr._expr1\n        vec2 = expr._expr2\n        return r\"%s \\times %s\" % (self.parenthesize(vec1, PRECEDENCE['Mul']),\n                                  self.parenthesize(vec2, PRECEDENCE['Mul']))\n\n    def _print_Curl(self, expr):\n        vec = expr._expr\n        return r\"\\nabla\\times %s\" % self.parenthesize(vec, PRECEDENCE['Mul'])\n\n    def _print_Divergence(self, expr):\n        vec = expr._expr\n        return r\"\\nabla\\cdot %s\" % self.parenthesize(vec, PRECEDENCE['Mul'])\n\n    def _print_Dot(self, expr):\n        vec1 = expr._expr1\n        vec2 = expr._expr2\n        return r\"%s \\cdot %s\" % (self.parenthesize(vec1, PRECEDENCE['Mul']),\n                                  self.parenthesize(vec2, PRECEDENCE['Mul']))\n\n    def _print_Gradient(self, expr):\n        func = expr._expr\n        return r\"\\nabla\\cdot %s\" % self.parenthesize(func, PRECEDENCE['Mul'])\n\n    def _print_Mul(self, expr):\n        from sympy.core.power import Pow\n        from sympy.physics.units import Quantity\n        include_parens = False\n        if _coeff_isneg(expr):\n            expr = -expr\n            tex = \"- \"\n            if expr.is_Add:\n                tex += \"(\"\n                include_parens = True\n        else:\n            tex = \"\"\n\n        from sympy.simplify import fraction\n        numer, denom = fraction(expr, exact=True)\n        separator = self._settings['mul_symbol_latex']\n        numbersep = self._settings['mul_symbol_latex_numbers']\n\n        def convert(expr):\n            if not expr.is_Mul:\n                return str(self._print(expr))\n            else:\n                _tex = last_term_tex = \"\"\n\n                if self.order not in ('old', 'none'):\n                    args = expr.as_ordered_factors()\n                else:\n                    args = list(expr.args)\n\n                # If quantities are present append them at the back\n                args = sorted(args, key=lambda x: isinstance(x, Quantity) or\n                             (isinstance(x, Pow) and isinstance(x.base, Quantity)))\n\n                for i, term in enumerate(args):\n                    term_tex = self._print(term)\n\n                    if self._needs_mul_brackets(term, first=(i == 0),\n                                                last=(i == len(args) - 1)):\n                        term_tex = r\"\\left(%s\\right)\" % term_tex\n\n                    if _between_two_numbers_p[0].search(last_term_tex) and \\\n                            _between_two_numbers_p[1].match(term_tex):\n                        # between two numbers\n                        _tex += numbersep\n                    elif _tex:\n                        _tex += separator\n\n                    _tex += term_tex\n                    last_term_tex = term_tex\n                return _tex\n\n        if denom is S.One and Pow(1, -1, evaluate=False) not in expr.args:\n            # use the original expression here, since fraction() may have\n            # altered it when producing numer and denom\n            tex += convert(expr)\n\n        else:\n            snumer = convert(numer)\n            sdenom = convert(denom)\n            ldenom = len(sdenom.split())\n            ratio = self._settings['long_frac_ratio']\n            if self._settings['fold_short_frac'] \\\n                   and ldenom <= 2 and not \"^\" in sdenom:\n                # handle short fractions\n                if self._needs_mul_brackets(numer, last=False):\n                    tex += r\"\\left(%s\\right) / %s\" % (snumer, sdenom)\n                else:\n                    tex += r\"%s / %s\" % (snumer, sdenom)\n            elif ratio is not None and \\\n                    len(snumer.split()) > ratio*ldenom:\n                # handle long fractions\n                if self._needs_mul_brackets(numer, last=True):\n                    tex += r\"\\frac{1}{%s}%s\\left(%s\\right)\" \\\n                        % (sdenom, separator, snumer)\n                elif numer.is_Mul:\n                    # split a long numerator\n                    a = S.One\n                    b = S.One\n                    for x in numer.args:\n                        if self._needs_mul_brackets(x, last=False) or \\\n                                len(convert(a*x).split()) > ratio*ldenom or \\\n                                (b.is_commutative is x.is_commutative is False):\n                            b *= x\n                        else:\n                            a *= x\n                    if self._needs_mul_brackets(b, last=True):\n                        tex += r\"\\frac{%s}{%s}%s\\left(%s\\right)\" \\\n                            % (convert(a), sdenom, separator, convert(b))\n                    else:\n                        tex += r\"\\frac{%s}{%s}%s%s\" \\\n                            % (convert(a), sdenom, separator, convert(b))\n                else:\n                    tex += r\"\\frac{1}{%s}%s%s\" % (sdenom, separator, snumer)\n            else:\n                tex += r\"\\frac{%s}{%s}\" % (snumer, sdenom)\n\n        if include_parens:\n            tex += \")\"\n        return tex\n\n    def _print_Pow(self, expr):\n        # Treat x**Rational(1,n) as special case\n        if expr.exp.is_Rational and abs(expr.exp.p) == 1 and expr.exp.q != 1:\n            base = self._print(expr.base)\n            expq = expr.exp.q\n\n            if expq == 2:\n                tex = r\"\\sqrt{%s}\" % base\n            elif self._settings['itex']:\n                tex = r\"\\root{%d}{%s}\" % (expq, base)\n            else:\n                tex = r\"\\sqrt[%d]{%s}\" % (expq, base)\n\n            if expr.exp.is_negative:\n                return r\"\\frac{1}{%s}\" % tex\n            else:\n                return tex\n        elif self._settings['fold_frac_powers'] \\\n            and expr.exp.is_Rational \\\n                and expr.exp.q != 1:\n            base, p, q = self.parenthesize(expr.base, PRECEDENCE['Pow']), expr.exp.p, expr.exp.q\n            # issue #12886: add parentheses for superscripts raised to powers\n            if '^' in base and expr.base.is_Symbol:\n                base = r\"\\left(%s\\right)\" % base\n            if expr.base.is_Function:\n                return self._print(expr.base, exp=\"%s/%s\" % (p, q))\n            return r\"%s^{%s/%s}\" % (base, p, q)\n        elif expr.exp.is_Rational and expr.exp.is_negative and expr.base.is_commutative:\n            # special case for 1^(-x), issue 9216\n            if expr.base == 1:\n                return r\"%s^{%s}\" % (expr.base, expr.exp)\n            # things like 1/x\n            return self._print_Mul(expr)\n        else:\n            if expr.base.is_Function:\n                return self._print(expr.base, exp=self._print(expr.exp))\n            else:\n                tex = r\"%s^{%s}\"\n                exp = self._print(expr.exp)\n                # issue #12886: add parentheses around superscripts raised to powers\n                base = self.parenthesize(expr.base, PRECEDENCE['Pow'])\n                if '^' in base and expr.base.is_Symbol:\n                    base = r\"\\left(%s\\right)\" % base\n                elif isinstance(expr.base, Derivative\n                        ) and base.startswith(r'\\left('\n                        ) and re.match(r'\\\\left\\(\\\\d?d?dot', base\n                        ) and base.endswith(r'\\right)'):\n                    # don't use parentheses around dotted derivative\n                    base = base[6: -7]  # remove outermost added parens\n\n                return tex % (base, exp)\n\n    def _print_UnevaluatedExpr(self, expr):\n        return self._print(expr.args[0])\n\n    def _print_Sum(self, expr):\n        if len(expr.limits) == 1:\n            tex = r\"\\sum_{%s=%s}^{%s} \" % \\\n                tuple([ self._print(i) for i in expr.limits[0] ])\n        else:\n            def _format_ineq(l):\n                return r\"%s \\leq %s \\leq %s\" % \\\n                    tuple([self._print(s) for s in (l[1], l[0], l[2])])\n\n            tex = r\"\\sum_{\\substack{%s}} \" % \\\n                str.join('\\\\\\\\', [ _format_ineq(l) for l in expr.limits ])\n\n        if isinstance(expr.function, Add):\n            tex += r\"\\left(%s\\right)\" % self._print(expr.function)\n        else:\n            tex += self._print(expr.function)\n\n        return tex\n\n    def _print_Product(self, expr):\n        if len(expr.limits) == 1:\n            tex = r\"\\prod_{%s=%s}^{%s} \" % \\\n                tuple([ self._print(i) for i in expr.limits[0] ])\n        else:\n            def _format_ineq(l):\n                return r\"%s \\leq %s \\leq %s\" % \\\n                    tuple([self._print(s) for s in (l[1], l[0], l[2])])\n\n            tex = r\"\\prod_{\\substack{%s}} \" % \\\n                str.join('\\\\\\\\', [ _format_ineq(l) for l in expr.limits ])\n\n        if isinstance(expr.function, Add):\n            tex += r\"\\left(%s\\right)\" % self._print(expr.function)\n        else:\n            tex += self._print(expr.function)\n\n        return tex\n\n    def _print_BasisDependent(self, expr):\n        from sympy.vector import Vector\n\n        o1 = []\n        if expr == expr.zero:\n            return expr.zero._latex_form\n        if isinstance(expr, Vector):\n            items = expr.separate().items()\n        else:\n            items = [(0, expr)]\n\n        for system, vect in items:\n            inneritems = list(vect.components.items())\n            inneritems.sort(key = lambda x:x[0].__str__())\n            for k, v in inneritems:\n                if v == 1:\n                    o1.append(' + ' + k._latex_form)\n                elif v == -1:\n                    o1.append(' - ' + k._latex_form)\n                else:\n                    arg_str = '(' + LatexPrinter().doprint(v) + ')'\n                    o1.append(' + ' + arg_str + k._latex_form)\n\n        outstr = (''.join(o1))\n        if outstr[1] != '-':\n            outstr = outstr[3:]\n        else:\n            outstr = outstr[1:]\n        return outstr\n\n    def _print_Indexed(self, expr):\n        tex_base = self._print(expr.base)\n        tex = '{'+tex_base+'}'+'_{%s}' % ','.join(\n            map(self._print, expr.indices))\n        return tex\n\n    def _print_IndexedBase(self, expr):\n        return self._print(expr.label)\n\n    def _print_Derivative(self, expr):\n        if requires_partial(expr):\n            diff_symbol = r'\\partial'\n        else:\n            diff_symbol = r'd'\n\n        tex = \"\"\n        dim = 0\n        for x, num in reversed(expr.variable_count):\n            dim += num\n            if num == 1:\n                tex += r\"%s %s\" % (diff_symbol, self._print(x))\n            else:\n                tex += r\"%s %s^{%s}\" % (diff_symbol, self._print(x), num)\n\n        if dim == 1:\n            tex = r\"\\frac{%s}{%s}\" % (diff_symbol, tex)\n        else:\n            tex = r\"\\frac{%s^{%s}}{%s}\" % (diff_symbol, dim, tex)\n\n        return r\"%s %s\" % (tex, self.parenthesize(expr.expr, PRECEDENCE[\"Mul\"], strict=True))\n\n    def _print_Subs(self, subs):\n        expr, old, new = subs.args\n        latex_expr = self._print(expr)\n        latex_old = (self._print(e) for e in old)\n        latex_new = (self._print(e) for e in new)\n        latex_subs = r'\\\\ '.join(\n            e[0] + '=' + e[1] for e in zip(latex_old, latex_new))\n        return r'\\left. %s \\right|_{\\substack{ %s }}' % (latex_expr, latex_subs)\n\n    def _print_Integral(self, expr):\n        tex, symbols = \"\", []\n\n        # Only up to \\iiiint exists\n        if len(expr.limits) <= 4 and all(len(lim) == 1 for lim in expr.limits):\n            # Use len(expr.limits)-1 so that syntax highlighters don't think\n            # \\\" is an escaped quote\n            tex = r\"\\i\" + \"i\"*(len(expr.limits) - 1) + \"nt\"\n            symbols = [r\"\\, d%s\" % self._print(symbol[0])\n                       for symbol in expr.limits]\n\n        else:\n            for lim in reversed(expr.limits):\n                symbol = lim[0]\n                tex += r\"\\int\"\n\n                if len(lim) > 1:\n                    if self._settings['mode'] in ['equation', 'equation*'] \\\n                            and not self._settings['itex']:\n                        tex += r\"\\limits\"\n\n                    if len(lim) == 3:\n                        tex += \"_{%s}^{%s}\" % (self._print(lim[1]),\n                                               self._print(lim[2]))\n                    if len(lim) == 2:\n                        tex += \"^{%s}\" % (self._print(lim[1]))\n\n                symbols.insert(0, r\"\\, d%s\" % self._print(symbol))\n\n        return r\"%s %s%s\" % (tex,\n            self.parenthesize(expr.function, PRECEDENCE[\"Mul\"], strict=True), \"\".join(symbols))\n\n    def _print_Limit(self, expr):\n        e, z, z0, dir = expr.args\n\n        tex = r\"\\lim_{%s \\to \" % self._print(z)\n        if str(dir) == '+-' or z0 in (S.Infinity, S.NegativeInfinity):\n            tex += r\"%s}\" % self._print(z0)\n        else:\n            tex += r\"%s^%s}\" % (self._print(z0), self._print(dir))\n\n        if isinstance(e, AssocOp):\n            return r\"%s\\left(%s\\right)\" % (tex, self._print(e))\n        else:\n            return r\"%s %s\" % (tex, self._print(e))\n\n    def _hprint_Function(self, func):\n        r'''\n        Logic to decide how to render a function to latex\n          - if it is a recognized latex name, use the appropriate latex command\n          - if it is a single letter, just use that letter\n          - if it is a longer name, then put \\operatorname{} around it and be\n            mindful of undercores in the name\n        '''\n        func = self._deal_with_super_sub(func)\n        if func in accepted_latex_functions:\n            name = r\"\\%s\" % func\n        elif len(func) == 1 or func.startswith('\\\\'):\n            name = func\n        else:\n            name = r\"\\operatorname{%s}\" % func\n        return name\n\n    def _print_Function(self, expr, exp=None):\n        r'''\n        Render functions to LaTeX, handling functions that LaTeX knows about\n        e.g., sin, cos, ... by using the proper LaTeX command (\\sin, \\cos, ...).\n        For single-letter function names, render them as regular LaTeX math\n        symbols. For multi-letter function names that LaTeX does not know\n        about, (e.g., Li, sech) use \\operatorname{} so that the function name\n        is rendered in Roman font and LaTeX handles spacing properly.\n\n        expr is the expression involving the function\n        exp is an exponent\n        '''\n        func = expr.func.__name__\n        if hasattr(self, '_print_' + func) and \\\n            not isinstance(expr.func, UndefinedFunction):\n            return getattr(self, '_print_' + func)(expr, exp)\n        else:\n            args = [ str(self._print(arg)) for arg in expr.args ]\n            # How inverse trig functions should be displayed, formats are:\n            # abbreviated: asin, full: arcsin, power: sin^-1\n            inv_trig_style = self._settings['inv_trig_style']\n            # If we are dealing with a power-style inverse trig function\n            inv_trig_power_case = False\n            # If it is applicable to fold the argument brackets\n            can_fold_brackets = self._settings['fold_func_brackets'] and \\\n                len(args) == 1 and \\\n                not self._needs_function_brackets(expr.args[0])\n\n            inv_trig_table = [\"asin\", \"acos\", \"atan\", \"acsc\", \"asec\", \"acot\"]\n\n            # If the function is an inverse trig function, handle the style\n            if func in inv_trig_table:\n                if inv_trig_style == \"abbreviated\":\n                    func = func\n                elif inv_trig_style == \"full\":\n                    func = \"arc\" + func[1:]\n                elif inv_trig_style == \"power\":\n                    func = func[1:]\n                    inv_trig_power_case = True\n\n                    # Can never fold brackets if we're raised to a power\n                    if exp is not None:\n                        can_fold_brackets = False\n\n            if inv_trig_power_case:\n                if func in accepted_latex_functions:\n                    name = r\"\\%s^{-1}\" % func\n                else:\n                    name = r\"\\operatorname{%s}^{-1}\" % func\n            elif exp is not None:\n                name = r'%s^{%s}' % (self._hprint_Function(func), exp)\n            else:\n                name = self._hprint_Function(func)\n\n            if can_fold_brackets:\n                if func in accepted_latex_functions:\n                    # Wrap argument safely to avoid parse-time conflicts\n                    # with the function name itself\n                    name += r\" {%s}\"\n                else:\n                    name += r\"%s\"\n            else:\n                name += r\"{\\left (%s \\right )}\"\n\n            if inv_trig_power_case and exp is not None:\n                name += r\"^{%s}\" % exp\n\n            return name % \",\".join(args)\n\n    def _print_UndefinedFunction(self, expr):\n        return self._hprint_Function(str(expr))\n\n    @property\n    def _special_function_classes(self):\n        from sympy.functions.special.tensor_functions import KroneckerDelta\n        from sympy.functions.special.gamma_functions import gamma, lowergamma\n        from sympy.functions.special.beta_functions import beta\n        from sympy.functions.special.delta_functions import DiracDelta\n        from sympy.functions.special.error_functions import Chi\n        return {KroneckerDelta: r'\\delta',\n                gamma:  r'\\Gamma',\n                lowergamma: r'\\gamma',\n                beta: r'\\operatorname{B}',\n                DiracDelta: r'\\delta',\n                Chi: r'\\operatorname{Chi}'}\n\n    def _print_FunctionClass(self, expr):\n        for cls in self._special_function_classes:\n            if issubclass(expr, cls) and expr.__name__ == cls.__name__:\n                return self._special_function_classes[cls]\n        return self._hprint_Function(str(expr))\n\n    def _print_Lambda(self, expr):\n        symbols, expr = expr.args\n\n        if len(symbols) == 1:\n            symbols = self._print(symbols[0])\n        else:\n            symbols = self._print(tuple(symbols))\n\n        args = (symbols, self._print(expr))\n        tex = r\"\\left( %s \\mapsto %s \\right)\" % (symbols, self._print(expr))\n\n        return tex\n\n    def _hprint_variadic_function(self, expr, exp=None):\n        args = sorted(expr.args, key=default_sort_key)\n        texargs = [r\"%s\" % self._print(symbol) for symbol in args]\n        tex = r\"\\%s\\left(%s\\right)\" % (self._print((str(expr.func)).lower()), \", \".join(texargs))\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    _print_Min = _print_Max = _hprint_variadic_function\n\n    def _print_floor(self, expr, exp=None):\n        tex = r\"\\lfloor{%s}\\rfloor\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_ceiling(self, expr, exp=None):\n        tex = r\"\\lceil{%s}\\rceil\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_log(self, expr, exp=None):\n        if not self._settings[\"ln_notation\"]:\n            tex = r\"\\log{\\left (%s \\right )}\" % self._print(expr.args[0])\n        else:\n            tex = r\"\\ln{\\left (%s \\right )}\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_Abs(self, expr, exp=None):\n        tex = r\"\\left|{%s}\\right|\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n    _print_Determinant = _print_Abs\n\n    def _print_re(self, expr, exp=None):\n        tex = r\"\\Re{%s}\" % self.parenthesize(expr.args[0], PRECEDENCE['Atom'])\n\n        return self._do_exponent(tex, exp)\n\n    def _print_im(self, expr, exp=None):\n        tex = r\"\\Im{%s}\" % self.parenthesize(expr.args[0], PRECEDENCE['Func'])\n\n        return self._do_exponent(tex, exp)\n\n    def _print_Not(self, e):\n        from sympy import Equivalent, Implies\n        if isinstance(e.args[0], Equivalent):\n            return self._print_Equivalent(e.args[0], r\"\\not\\Leftrightarrow\")\n        if isinstance(e.args[0], Implies):\n            return self._print_Implies(e.args[0], r\"\\not\\Rightarrow\")\n        if (e.args[0].is_Boolean):\n            return r\"\\neg (%s)\" % self._print(e.args[0])\n        else:\n            return r\"\\neg %s\" % self._print(e.args[0])\n\n    def _print_LogOp(self, args, char):\n        arg = args[0]\n        if arg.is_Boolean and not arg.is_Not:\n            tex = r\"\\left(%s\\right)\" % self._print(arg)\n        else:\n            tex = r\"%s\" % self._print(arg)\n\n        for arg in args[1:]:\n            if arg.is_Boolean and not arg.is_Not:\n                tex += r\" %s \\left(%s\\right)\" % (char, self._print(arg))\n            else:\n                tex += r\" %s %s\" % (char, self._print(arg))\n\n        return tex\n\n    def _print_And(self, e):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, r\"\\wedge\")\n\n    def _print_Or(self, e):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, r\"\\vee\")\n\n    def _print_Xor(self, e):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, r\"\\veebar\")\n\n    def _print_Implies(self, e, altchar=None):\n        return self._print_LogOp(e.args, altchar or r\"\\Rightarrow\")\n\n    def _print_Equivalent(self, e, altchar=None):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, altchar or r\"\\Leftrightarrow\")\n\n    def _print_conjugate(self, expr, exp=None):\n        tex = r\"\\overline{%s}\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_polar_lift(self, expr, exp=None):\n        func = r\"\\operatorname{polar\\_lift}\"\n        arg = r\"{\\left (%s \\right )}\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}%s\" % (func, exp, arg)\n        else:\n            return r\"%s%s\" % (func, arg)\n\n    def _print_ExpBase(self, expr, exp=None):\n        # TODO should exp_polar be printed differently?\n        #      what about exp_polar(0), exp_polar(1)?\n        tex = r\"e^{%s}\" % self._print(expr.args[0])\n        return self._do_exponent(tex, exp)\n\n    def _print_elliptic_k(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"K^{%s}%s\" % (exp, tex)\n        else:\n            return r\"K%s\" % tex\n\n    def _print_elliptic_f(self, expr, exp=None):\n        tex = r\"\\left(%s\\middle| %s\\right)\" % \\\n            (self._print(expr.args[0]), self._print(expr.args[1]))\n        if exp is not None:\n            return r\"F^{%s}%s\" % (exp, tex)\n        else:\n            return r\"F%s\" % tex\n\n    def _print_elliptic_e(self, expr, exp=None):\n        if len(expr.args) == 2:\n            tex = r\"\\left(%s\\middle| %s\\right)\" % \\\n                (self._print(expr.args[0]), self._print(expr.args[1]))\n        else:\n            tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"E^{%s}%s\" % (exp, tex)\n        else:\n            return r\"E%s\" % tex\n\n    def _print_elliptic_pi(self, expr, exp=None):\n        if len(expr.args) == 3:\n            tex = r\"\\left(%s; %s\\middle| %s\\right)\" % \\\n                (self._print(expr.args[0]), self._print(expr.args[1]), \\\n                 self._print(expr.args[2]))\n        else:\n            tex = r\"\\left(%s\\middle| %s\\right)\" % \\\n                (self._print(expr.args[0]), self._print(expr.args[1]))\n        if exp is not None:\n            return r\"\\Pi^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\Pi%s\" % tex\n\n    def _print_beta(self, expr, exp=None):\n        tex = r\"\\left(%s, %s\\right)\" % (self._print(expr.args[0]),\n                                        self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"\\operatorname{B}^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\operatorname{B}%s\" % tex\n\n    def _print_uppergamma(self, expr, exp=None):\n        tex = r\"\\left(%s, %s\\right)\" % (self._print(expr.args[0]),\n                                        self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"\\Gamma^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\Gamma%s\" % tex\n\n    def _print_lowergamma(self, expr, exp=None):\n        tex = r\"\\left(%s, %s\\right)\" % (self._print(expr.args[0]),\n                                        self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"\\gamma^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\gamma%s\" % tex\n\n    def _hprint_one_arg_func(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}%s\" % (self._print(expr.func), exp, tex)\n        else:\n            return r\"%s%s\" % (self._print(expr.func), tex)\n\n    _print_gamma = _hprint_one_arg_func\n\n    def _print_Chi(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"\\operatorname{Chi}^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\operatorname{Chi}%s\" % tex\n\n    def _print_expint(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[1])\n        nu = self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"\\operatorname{E}_{%s}^{%s}%s\" % (nu, exp, tex)\n        else:\n            return r\"\\operatorname{E}_{%s}%s\" % (nu, tex)\n\n    def _print_fresnels(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"S^{%s}%s\" % (exp, tex)\n        else:\n            return r\"S%s\" % tex\n\n    def _print_fresnelc(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"C^{%s}%s\" % (exp, tex)\n        else:\n            return r\"C%s\" % tex\n\n    def _print_subfactorial(self, expr, exp=None):\n        tex = r\"!%s\" % self.parenthesize(expr.args[0], PRECEDENCE[\"Func\"])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_factorial(self, expr, exp=None):\n        tex = r\"%s!\" % self.parenthesize(expr.args[0], PRECEDENCE[\"Func\"])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_factorial2(self, expr, exp=None):\n        tex = r\"%s!!\" % self.parenthesize(expr.args[0], PRECEDENCE[\"Func\"])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_binomial(self, expr, exp=None):\n        tex = r\"{\\binom{%s}{%s}}\" % (self._print(expr.args[0]),\n                                     self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_RisingFactorial(self, expr, exp=None):\n        n, k = expr.args\n        base = r\"%s\" % self.parenthesize(n, PRECEDENCE['Func'])\n\n        tex = r\"{%s}^{\\left(%s\\right)}\" % (base, self._print(k))\n\n        return self._do_exponent(tex, exp)\n\n    def _print_FallingFactorial(self, expr, exp=None):\n        n, k = expr.args\n        sub = r\"%s\" % self.parenthesize(k, PRECEDENCE['Func'])\n\n        tex = r\"{\\left(%s\\right)}_{%s}\" % (self._print(n), sub)\n\n        return self._do_exponent(tex, exp)\n\n    def _hprint_BesselBase(self, expr, exp, sym):\n        tex = r\"%s\" % (sym)\n\n        need_exp = False\n        if exp is not None:\n            if tex.find('^') == -1:\n                tex = r\"%s^{%s}\" % (tex, self._print(exp))\n            else:\n                need_exp = True\n\n        tex = r\"%s_{%s}\\left(%s\\right)\" % (tex, self._print(expr.order),\n                                           self._print(expr.argument))\n\n        if need_exp:\n            tex = self._do_exponent(tex, exp)\n        return tex\n\n    def _hprint_vec(self, vec):\n        if len(vec) == 0:\n            return \"\"\n        s = \"\"\n        for i in vec[:-1]:\n            s += \"%s, \" % self._print(i)\n        s += self._print(vec[-1])\n        return s\n\n    def _print_besselj(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'J')\n\n    def _print_besseli(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'I')\n\n    def _print_besselk(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'K')\n\n    def _print_bessely(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'Y')\n\n    def _print_yn(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'y')\n\n    def _print_jn(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'j')\n\n    def _print_hankel1(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'H^{(1)}')\n\n    def _print_hankel2(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'H^{(2)}')\n\n    def _print_hn1(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'h^{(1)}')\n\n    def _print_hn2(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'h^{(2)}')\n\n    def _hprint_airy(self, expr, exp=None, notation=\"\"):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}%s\" % (notation, exp, tex)\n        else:\n            return r\"%s%s\" % (notation, tex)\n\n    def _hprint_airy_prime(self, expr, exp=None, notation=\"\"):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"{%s^\\prime}^{%s}%s\" % (notation, exp, tex)\n        else:\n            return r\"%s^\\prime%s\" % (notation, tex)\n\n    def _print_airyai(self, expr, exp=None):\n        return self._hprint_airy(expr, exp, 'Ai')\n\n    def _print_airybi(self, expr, exp=None):\n        return self._hprint_airy(expr, exp, 'Bi')\n\n    def _print_airyaiprime(self, expr, exp=None):\n        return self._hprint_airy_prime(expr, exp, 'Ai')\n\n    def _print_airybiprime(self, expr, exp=None):\n        return self._hprint_airy_prime(expr, exp, 'Bi')\n\n    def _print_hyper(self, expr, exp=None):\n        tex = r\"{{}_{%s}F_{%s}\\left(\\begin{matrix} %s \\\\ %s \\end{matrix}\" \\\n              r\"\\middle| {%s} \\right)}\" % \\\n            (self._print(len(expr.ap)), self._print(len(expr.bq)),\n              self._hprint_vec(expr.ap), self._hprint_vec(expr.bq),\n              self._print(expr.argument))\n\n        if exp is not None:\n            tex = r\"{%s}^{%s}\" % (tex, self._print(exp))\n        return tex\n\n    def _print_meijerg(self, expr, exp=None):\n        tex = r\"{G_{%s, %s}^{%s, %s}\\left(\\begin{matrix} %s & %s \\\\\" \\\n              r\"%s & %s \\end{matrix} \\middle| {%s} \\right)}\" % \\\n            (self._print(len(expr.ap)), self._print(len(expr.bq)),\n              self._print(len(expr.bm)), self._print(len(expr.an)),\n              self._hprint_vec(expr.an), self._hprint_vec(expr.aother),\n              self._hprint_vec(expr.bm), self._hprint_vec(expr.bother),\n              self._print(expr.argument))\n\n        if exp is not None:\n            tex = r\"{%s}^{%s}\" % (tex, self._print(exp))\n        return tex\n\n    def _print_dirichlet_eta(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"\\eta^{%s}%s\" % (self._print(exp), tex)\n        return r\"\\eta%s\" % tex\n\n    def _print_zeta(self, expr, exp=None):\n        if len(expr.args) == 2:\n            tex = r\"\\left(%s, %s\\right)\" % tuple(map(self._print, expr.args))\n        else:\n            tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"\\zeta^{%s}%s\" % (self._print(exp), tex)\n        return r\"\\zeta%s\" % tex\n\n    def _print_lerchphi(self, expr, exp=None):\n        tex = r\"\\left(%s, %s, %s\\right)\" % tuple(map(self._print, expr.args))\n        if exp is None:\n            return r\"\\Phi%s\" % tex\n        return r\"\\Phi^{%s}%s\" % (self._print(exp), tex)\n\n    def _print_polylog(self, expr, exp=None):\n        s, z = map(self._print, expr.args)\n        tex = r\"\\left(%s\\right)\" % z\n        if exp is None:\n            return r\"\\operatorname{Li}_{%s}%s\" % (s, tex)\n        return r\"\\operatorname{Li}_{%s}^{%s}%s\" % (s, self._print(exp), tex)\n\n    def _print_jacobi(self, expr, exp=None):\n        n, a, b, x = map(self._print, expr.args)\n        tex = r\"P_{%s}^{\\left(%s,%s\\right)}\\left(%s\\right)\" % (n, a, b, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_gegenbauer(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"C_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_chebyshevt(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"T_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_chebyshevu(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"U_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_legendre(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"P_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_assoc_legendre(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"P_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_hermite(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"H_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_laguerre(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"L_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_assoc_laguerre(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"L_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Ynm(self, expr, exp=None):\n        n, m, theta, phi = map(self._print, expr.args)\n        tex = r\"Y_{%s}^{%s}\\left(%s,%s\\right)\" % (n, m, theta, phi)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Znm(self, expr, exp=None):\n        n, m, theta, phi = map(self._print, expr.args)\n        tex = r\"Z_{%s}^{%s}\\left(%s,%s\\right)\" % (n, m, theta, phi)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Rational(self, expr):\n        if expr.q != 1:\n            sign = \"\"\n            p = expr.p\n            if expr.p < 0:\n                sign = \"- \"\n                p = -p\n            if self._settings['fold_short_frac']:\n                return r\"%s%d / %d\" % (sign, p, expr.q)\n            return r\"%s\\frac{%d}{%d}\" % (sign, p, expr.q)\n        else:\n            return self._print(expr.p)\n\n    def _print_Order(self, expr):\n        s = self._print(expr.expr)\n        if expr.point and any(p != S.Zero for p in expr.point) or \\\n           len(expr.variables) > 1:\n            s += '; '\n            if len(expr.variables) > 1:\n                s += self._print(expr.variables)\n            elif len(expr.variables):\n                s += self._print(expr.variables[0])\n            s += r'\\rightarrow '\n            if len(expr.point) > 1:\n                s += self._print(expr.point)\n            else:\n                s += self._print(expr.point[0])\n        return r\"O\\left(%s\\right)\" % s\n\n    def _print_Symbol(self, expr):\n        if expr in self._settings['symbol_names']:\n            return self._settings['symbol_names'][expr]\n\n        return self._deal_with_super_sub(expr.name) if \\\n            '\\\\' not in expr.name else expr.name\n\n    _print_RandomSymbol = _print_Symbol\n    _print_MatrixSymbol = _print_Symbol\n\n    def _deal_with_super_sub(self, string):\n        if '{' in string:\n            return string\n\n        name, supers, subs = split_super_sub(string)\n\n        name = translate(name)\n        supers = [translate(sup) for sup in supers]\n        subs = [translate(sub) for sub in subs]\n\n        # glue all items together:\n        if len(supers) > 0:\n            name += \"^{%s}\" % \" \".join(supers)\n        if len(subs) > 0:\n            name += \"_{%s}\" % \" \".join(subs)\n\n        return name\n\n    def _print_Relational(self, expr):\n        if self._settings['itex']:\n            gt = r\"\\gt\"\n            lt = r\"\\lt\"\n        else:\n            gt = \">\"\n            lt = \"<\"\n\n        charmap = {\n            \"==\": \"=\",\n            \">\": gt,\n            \"<\": lt,\n            \">=\": r\"\\geq\",\n            \"<=\": r\"\\leq\",\n            \"!=\": r\"\\neq\",\n        }\n\n        return \"%s %s %s\" % (self._print(expr.lhs),\n            charmap[expr.rel_op], self._print(expr.rhs))\n\n    def _print_Piecewise(self, expr):\n        ecpairs = [r\"%s & \\text{for}\\: %s\" % (self._print(e), self._print(c))\n                   for e, c in expr.args[:-1]]\n        if expr.args[-1].cond == true:\n            ecpairs.append(r\"%s & \\text{otherwise}\" %\n                           self._print(expr.args[-1].expr))\n        else:\n            ecpairs.append(r\"%s & \\text{for}\\: %s\" %\n                           (self._print(expr.args[-1].expr),\n                            self._print(expr.args[-1].cond)))\n        tex = r\"\\begin{cases} %s \\end{cases}\"\n        return tex % r\" \\\\\".join(ecpairs)\n\n    def _print_MatrixBase(self, expr):\n        lines = []\n\n        for line in range(expr.rows):  # horrible, should be 'rows'\n            lines.append(\" & \".join([ self._print(i) for i in expr[line, :] ]))\n\n        mat_str = self._settings['mat_str']\n        if mat_str is None:\n            if self._settings['mode'] == 'inline':\n                mat_str = 'smallmatrix'\n            else:\n                if (expr.cols <= 10) is True:\n                    mat_str = 'matrix'\n                else:\n                    mat_str = 'array'\n\n        out_str = r'\\begin{%MATSTR%}%s\\end{%MATSTR%}'\n        out_str = out_str.replace('%MATSTR%', mat_str)\n        if mat_str == 'array':\n            out_str = out_str.replace('%s', '{' + 'c'*expr.cols + '}%s')\n        if self._settings['mat_delim']:\n            left_delim = self._settings['mat_delim']\n            right_delim = self._delim_dict[left_delim]\n            out_str = r'\\left' + left_delim + out_str + \\\n                      r'\\right' + right_delim\n        return out_str % r\"\\\\\".join(lines)\n    _print_ImmutableMatrix = _print_ImmutableDenseMatrix \\\n                           = _print_Matrix \\\n                           = _print_MatrixBase\n\n    def _print_MatrixElement(self, expr):\n        return self.parenthesize(expr.parent, PRECEDENCE[\"Atom\"], strict=True) \\\n            + '_{%s, %s}' % (expr.i, expr.j)\n\n    def _print_MatrixSlice(self, expr):\n        def latexslice(x):\n            x = list(x)\n            if x[2] == 1:\n                del x[2]\n            if x[1] == x[0] + 1:\n                del x[1]\n            if x[0] == 0:\n                x[0] = ''\n            return ':'.join(map(self._print, x))\n        return (self._print(expr.parent) + r'\\left[' +\n                latexslice(expr.rowslice) + ', ' +\n                latexslice(expr.colslice) + r'\\right]')\n\n    def _print_BlockMatrix(self, expr):\n        return self._print(expr.blocks)\n\n    def _print_Transpose(self, expr):\n        mat = expr.arg\n        from sympy.matrices import MatrixSymbol\n        if not isinstance(mat, MatrixSymbol):\n            return r\"\\left(%s\\right)^T\" % self._print(mat)\n        else:\n            return \"%s^T\" % self._print(mat)\n\n    def _print_Adjoint(self, expr):\n        mat = expr.arg\n        from sympy.matrices import MatrixSymbol\n        if not isinstance(mat, MatrixSymbol):\n            return r\"\\left(%s\\right)^\\dagger\" % self._print(mat)\n        else:\n            return r\"%s^\\dagger\" % self._print(mat)\n\n    def _print_MatAdd(self, expr):\n        terms = [self._print(t) for t in expr.args]\n        l = []\n(B)         if (e.args[0].is_Boolean):\n            return r\"\\neg (%s)\" % self._print(e.args[0])\n        else:\n            return r\"\\neg %s\" % self._print(e.args[0])\n\n    def _print_LogOp(self, args, char):\n        arg = args[0]\n        if arg.is_Boolean and not arg.is_Not:\n            tex = r\"\\left(%s\\right)\" % self._print(arg)\n        else:\n            tex = r\"%s\" % self._print(arg)\n\n        for arg in args[1:]:\n            if arg.is_Boolean and not arg.is_Not:\n                tex += r\" %s \\left(%s\\right)\" % (char, self._print(arg))\n            else:\n                tex += r\" %s %s\" % (char, self._print(arg))\n\n        return tex\n\n    def _print_And(self, e):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, r\"\\wedge\")\n\n    def _print_Or(self, e):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, r\"\\vee\")\n\n    def _print_Xor(self, e):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, r\"\\veebar\")\n\n    def _print_Implies(self, e, altchar=None):\n        return self._print_LogOp(e.args, altchar or r\"\\Rightarrow\")\n\n    def _print_Equivalent(self, e, altchar=None):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, altchar or r\"\\Leftrightarrow\")\n\n    def _print_conjugate(self, expr, exp=None):\n        tex = r\"\\overline{%s}\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_polar_lift(self, expr, exp=None):\n        func = r\"\\operatorname{polar\\_lift}\"\n        arg = r\"{\\left (%s \\right )}\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}%s\" % (func, exp, arg)\n        else:\n            return r\"%s%s\" % (func, arg)\n\n    def _print_ExpBase(self, expr, exp=None):\n        # TODO should exp_polar be printed differently?\n        #      what about exp_polar(0), exp_polar(1)?\n        tex = r\"e^{%s}\" % self._print(expr.args[0])\n        return self._do_exponent(tex, exp)\n\n    def _print_elliptic_k(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"K^{%s}%s\" % (exp, tex)\n        else:\n            return r\"K%s\" % tex\n\n    def _print_elliptic_f(self, expr, exp=None):\n        tex = r\"\\left(%s\\middle| %s\\right)\" % \\\n            (self._print(expr.args[0]), self._print(expr.args[1]))\n        if exp is not None:\n            return r\"F^{%s}%s\" % (exp, tex)\n        else:\n            return r\"F%s\" % tex\n\n    def _print_elliptic_e(self, expr, exp=None):\n        if len(expr.args) == 2:\n            tex = r\"\\left(%s\\middle| %s\\right)\" % \\\n                (self._print(expr.args[0]), self._print(expr.args[1]))\n        else:\n            tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"E^{%s}%s\" % (exp, tex)\n        else:\n            return r\"E%s\" % tex\n\n    def _print_elliptic_pi(self, expr, exp=None):\n        if len(expr.args) == 3:\n            tex = r\"\\left(%s; %s\\middle| %s\\right)\" % \\\n                (self._print(expr.args[0]), self._print(expr.args[1]), \\\n                 self._print(expr.args[2]))\n        else:\n            tex = r\"\\left(%s\\middle| %s\\right)\" % \\\n                (self._print(expr.args[0]), self._print(expr.args[1]))\n        if exp is not None:\n            return r\"\\Pi^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\Pi%s\" % tex\n\n    def _print_beta(self, expr, exp=None):\n        tex = r\"\\left(%s, %s\\right)\" % (self._print(expr.args[0]),\n                                        self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"\\operatorname{B}^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\operatorname{B}%s\" % tex\n\n    def _print_uppergamma(self, expr, exp=None):\n        tex = r\"\\left(%s, %s\\right)\" % (self._print(expr.args[0]),\n                                        self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"\\Gamma^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\Gamma%s\" % tex\n\n    def _print_lowergamma(self, expr, exp=None):\n        tex = r\"\\left(%s, %s\\right)\" % (self._print(expr.args[0]),\n                                        self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"\\gamma^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\gamma%s\" % tex\n\n    def _hprint_one_arg_func(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}%s\" % (self._print(expr.func), exp, tex)\n        else:\n            return r\"%s%s\" % (self._print(expr.func), tex)\n\n    _print_gamma = _hprint_one_arg_func\n\n    def _print_Chi(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"\\operatorname{Chi}^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\operatorname{Chi}%s\" % tex\n\n    def _print_expint(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[1])\n        nu = self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"\\operatorname{E}_{%s}^{%s}%s\" % (nu, exp, tex)\n        else:\n            return r\"\\operatorname{E}_{%s}%s\" % (nu, tex)\n\n    def _print_fresnels(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"S^{%s}%s\" % (exp, tex)\n        else:\n            return r\"S%s\" % tex\n\n    def _print_fresnelc(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"C^{%s}%s\" % (exp, tex)\n        else:\n            return r\"C%s\" % tex\n\n    def _print_subfactorial(self, expr, exp=None):\n        tex = r\"!%s\" % self.parenthesize(expr.args[0], PRECEDENCE[\"Func\"])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_factorial(self, expr, exp=None):\n        tex = r\"%s!\" % self.parenthesize(expr.args[0], PRECEDENCE[\"Func\"])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_factorial2(self, expr, exp=None):\n        tex = r\"%s!!\" % self.parenthesize(expr.args[0], PRECEDENCE[\"Func\"])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_binomial(self, expr, exp=None):\n        tex = r\"{\\binom{%s}{%s}}\" % (self._print(expr.args[0]),\n                                     self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_RisingFactorial(self, expr, exp=None):\n        n, k = expr.args\n        base = r\"%s\" % self.parenthesize(n, PRECEDENCE['Func'])\n\n        tex = r\"{%s}^{\\left(%s\\right)}\" % (base, self._print(k))\n\n        return self._do_exponent(tex, exp)\n\n    def _print_FallingFactorial(self, expr, exp=None):\n        n, k = expr.args\n        sub = r\"%s\" % self.parenthesize(k, PRECEDENCE['Func'])\n\n        tex = r\"{\\left(%s\\right)}_{%s}\" % (self._print(n), sub)\n\n        return self._do_exponent(tex, exp)\n\n    def _hprint_BesselBase(self, expr, exp, sym):\n        tex = r\"%s\" % (sym)\n\n        need_exp = False\n        if exp is not None:\n            if tex.find('^') == -1:\n                tex = r\"%s^{%s}\" % (tex, self._print(exp))\n            else:\n                need_exp = True\n\n        tex = r\"%s_{%s}\\left(%s\\right)\" % (tex, self._print(expr.order),\n                                           self._print(expr.argument))\n\n        if need_exp:\n            tex = self._do_exponent(tex, exp)\n        return tex\n\n    def _hprint_vec(self, vec):\n        if len(vec) == 0:\n            return \"\"\n        s = \"\"\n        for i in vec[:-1]:\n            s += \"%s, \" % self._print(i)\n        s += self._print(vec[-1])\n        return s\n\n    def _print_besselj(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'J')\n\n    def _print_besseli(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'I')\n\n    def _print_besselk(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'K')\n\n    def _print_bessely(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'Y')\n\n    def _print_yn(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'y')\n\n    def _print_jn(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'j')\n\n    def _print_hankel1(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'H^{(1)}')\n\n    def _print_hankel2(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'H^{(2)}')\n\n    def _print_hn1(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'h^{(1)}')\n\n    def _print_hn2(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'h^{(2)}')\n\n    def _hprint_airy(self, expr, exp=None, notation=\"\"):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}%s\" % (notation, exp, tex)\n        else:\n            return r\"%s%s\" % (notation, tex)\n\n    def _hprint_airy_prime(self, expr, exp=None, notation=\"\"):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"{%s^\\prime}^{%s}%s\" % (notation, exp, tex)\n        else:\n            return r\"%s^\\prime%s\" % (notation, tex)\n\n    def _print_airyai(self, expr, exp=None):\n        return self._hprint_airy(expr, exp, 'Ai')\n\n    def _print_airybi(self, expr, exp=None):\n        return self._hprint_airy(expr, exp, 'Bi')\n\n    def _print_airyaiprime(self, expr, exp=None):\n        return self._hprint_airy_prime(expr, exp, 'Ai')\n\n    def _print_airybiprime(self, expr, exp=None):\n        return self._hprint_airy_prime(expr, exp, 'Bi')\n\n    def _print_hyper(self, expr, exp=None):\n        tex = r\"{{}_{%s}F_{%s}\\left(\\begin{matrix} %s \\\\ %s \\end{matrix}\" \\\n              r\"\\middle| {%s} \\right)}\" % \\\n            (self._print(len(expr.ap)), self._print(len(expr.bq)),\n              self._hprint_vec(expr.ap), self._hprint_vec(expr.bq),\n              self._print(expr.argument))\n\n        if exp is not None:\n            tex = r\"{%s}^{%s}\" % (tex, self._print(exp))\n        return tex\n\n    def _print_meijerg(self, expr, exp=None):\n        tex = r\"{G_{%s, %s}^{%s, %s}\\left(\\begin{matrix} %s & %s \\\\\" \\\n              r\"%s & %s \\end{matrix} \\middle| {%s} \\right)}\" % \\\n            (self._print(len(expr.ap)), self._print(len(expr.bq)),\n              self._print(len(expr.bm)), self._print(len(expr.an)),\n              self._hprint_vec(expr.an), self._hprint_vec(expr.aother),\n              self._hprint_vec(expr.bm), self._hprint_vec(expr.bother),\n              self._print(expr.argument))\n\n        if exp is not None:\n            tex = r\"{%s}^{%s}\" % (tex, self._print(exp))\n        return tex\n\n    def _print_dirichlet_eta(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"\\eta^{%s}%s\" % (self._print(exp), tex)\n        return r\"\\eta%s\" % tex\n\n    def _print_zeta(self, expr, exp=None):\n        if len(expr.args) == 2:\n            tex = r\"\\left(%s, %s\\right)\" % tuple(map(self._print, expr.args))\n        else:\n            tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"\\zeta^{%s}%s\" % (self._print(exp), tex)\n        return r\"\\zeta%s\" % tex\n\n    def _print_lerchphi(self, expr, exp=None):\n        tex = r\"\\left(%s, %s, %s\\right)\" % tuple(map(self._print, expr.args))\n        if exp is None:\n            return r\"\\Phi%s\" % tex\n        return r\"\\Phi^{%s}%s\" % (self._print(exp), tex)\n\n    def _print_polylog(self, expr, exp=None):\n        s, z = map(self._print, expr.args)\n        tex = r\"\\left(%s\\right)\" % z\n        if exp is None:\n            return r\"\\operatorname{Li}_{%s}%s\" % (s, tex)\n        return r\"\\operatorname{Li}_{%s}^{%s}%s\" % (s, self._print(exp), tex)\n\n    def _print_jacobi(self, expr, exp=None):\n        n, a, b, x = map(self._print, expr.args)\n        tex = r\"P_{%s}^{\\left(%s,%s\\right)}\\left(%s\\right)\" % (n, a, b, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_gegenbauer(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"C_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_chebyshevt(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"T_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_chebyshevu(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"U_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_legendre(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"P_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_assoc_legendre(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"P_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_hermite(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"H_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_laguerre(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"L_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_assoc_laguerre(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"L_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Ynm(self, expr, exp=None):\n        n, m, theta, phi = map(self._print, expr.args)\n        tex = r\"Y_{%s}^{%s}\\left(%s,%s\\right)\" % (n, m, theta, phi)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Znm(self, expr, exp=None):\n        n, m, theta, phi = map(self._print, expr.args)\n        tex = r\"Z_{%s}^{%s}\\left(%s,%s\\right)\" % (n, m, theta, phi)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Rational(self, expr):\n        if expr.q != 1:\n            sign = \"\"\n            p = expr.p\n            if expr.p < 0:\n                sign = \"- \"\n                p = -p\n            if self._settings['fold_short_frac']:\n                return r\"%s%d / %d\" % (sign, p, expr.q)\n            return r\"%s\\frac{%d}{%d}\" % (sign, p, expr.q)\n        else:\n            return self._print(expr.p)\n\n    def _print_Order(self, expr):\n        s = self._print(expr.expr)\n        if expr.point and any(p != S.Zero for p in expr.point) or \\\n           len(expr.variables) > 1:\n            s += '; '\n            if len(expr.variables) > 1:\n                s += self._print(expr.variables)\n            elif len(expr.variables):\n                s += self._print(expr.variables[0])\n            s += r'\\rightarrow '\n            if len(expr.point) > 1:\n                s += self._print(expr.point)\n            else:\n                s += self._print(expr.point[0])\n        return r\"O\\left(%s\\right)\" % s\n\n    def _print_Symbol(self, expr):\n        if expr in self._settings['symbol_names']:\n            return self._settings['symbol_names'][expr]\n\n        return self._deal_with_super_sub(expr.name) if \\\n            '\\\\' not in expr.name else expr.name\n\n    _print_RandomSymbol = _print_Symbol\n    _print_MatrixSymbol = _print_Symbol\n\n    def _deal_with_super_sub(self, string):\n        if '{' in string:\n            return string\n\n        name, supers, subs = split_super_sub(string)\n\n        name = translate(name)\n        supers = [translate(sup) for sup in supers]\n        subs = [translate(sub) for sub in subs]\n\n        # glue all items together:\n        if len(supers) > 0:\n            name += \"^{%s}\" % \" \".join(supers)\n        if len(subs) > 0:\n            name += \"_{%s}\" % \" \".join(subs)\n\n        return name\n\n    def _print_Relational(self, expr):\n        if self._settings['itex']:\n            gt = r\"\\gt\"\n            lt = r\"\\lt\"\n        else:\n            gt = \">\"\n            lt = \"<\"\n\n        charmap = {\n            \"==\": \"=\",\n            \">\": gt,\n            \"<\": lt,\n            \">=\": r\"\\geq\",\n            \"<=\": r\"\\leq\",\n            \"!=\": r\"\\neq\",\n        }\n\n        return \"%s %s %s\" % (self._print(expr.lhs),\n            charmap[expr.rel_op], self._print(expr.rhs))\n\n    def _print_Piecewise(self, expr):\n        ecpairs = [r\"%s & \\text{for}\\: %s\" % (self._print(e), self._print(c))\n                   for e, c in expr.args[:-1]]\n        if expr.args[-1].cond == true:\n            ecpairs.append(r\"%s & \\text{otherwise}\" %\n                           self._print(expr.args[-1].expr))\n        else:\n            ecpairs.append(r\"%s & \\text{for}\\: %s\" %\n                           (self._print(expr.args[-1].expr),\n                            self._print(expr.args[-1].cond)))\n        tex = r\"\\begin{cases} %s \\end{cases}\"\n        return tex % r\" \\\\\".join(ecpairs)\n\n    def _print_MatrixBase(self, expr):\n        lines = []\n\n        for line in range(expr.rows):  # horrible, should be 'rows'\n            lines.append(\" & \".join([ self._print(i) for i in expr[line, :] ]))\n\n        mat_str = self._settings['mat_str']\n        if mat_str is None:\n            if self._settings['mode'] == 'inline':\n                mat_str = 'smallmatrix'\n            else:\n                if (expr.cols <= 10) is True:\n                    mat_str = 'matrix'\n                else:\n                    mat_str = 'array'\n\n        out_str = r'\\begin{%MATSTR%}%s\\end{%MATSTR%}'\n        out_str = out_str.replace('%MATSTR%', mat_str)\n        if mat_str == 'array':\n            out_str = out_str.replace('%s', '{' + 'c'*expr.cols + '}%s')\n        if self._settings['mat_delim']:\n            left_delim = self._settings['mat_delim']\n            right_delim = self._delim_dict[left_delim]\n            out_str = r'\\left' + left_delim + out_str + \\\n                      r'\\right' + right_delim\n        return out_str % r\"\\\\\".join(lines)\n    _print_ImmutableMatrix = _print_ImmutableDenseMatrix \\\n                           = _print_Matrix \\\n                           = _print_MatrixBase\n\n    def _print_MatrixElement(self, expr):\n        return self.parenthesize(expr.parent, PRECEDENCE[\"Atom\"], strict=True) \\\n            + '_{%s, %s}' % (expr.i, expr.j)\n\n    def _print_MatrixSlice(self, expr):\n        def latexslice(x):\n            x = list(x)\n            if x[2] == 1:\n                del x[2]\n            if x[1] == x[0] + 1:\n                del x[1]\n            if x[0] == 0:\n                x[0] = ''\n            return ':'.join(map(self._print, x))\n        return (self._print(expr.parent) + r'\\left[' +\n                latexslice(expr.rowslice) + ', ' +\n                latexslice(expr.colslice) + r'\\right]')\n\n    def _print_BlockMatrix(self, expr):\n        return self._print(expr.blocks)\n\n    def _print_Transpose(self, expr):\n        mat = expr.arg\n        from sympy.matrices import MatrixSymbol\n        if not isinstance(mat, MatrixSymbol):\n            return r\"\\left(%s\\right)^T\" % self._print(mat)\n        else:\n            return \"%s^T\" % self._print(mat)\n\n    def _print_Adjoint(self, expr):\n        mat = expr.arg\n        from sympy.matrices import MatrixSymbol\n        if not isinstance(mat, MatrixSymbol):\n            return r\"\\left(%s\\right)^\\dagger\" % self._print(mat)\n        else:\n            return r\"%s^\\dagger\" % self._print(mat)\n\n    def _print_MatAdd(self, expr):\n        terms = [self._print(t) for t in expr.args]\n        l = []\n        for t in terms:\n            if t.startswith('-'):\n                sign = \"-\"\n                t = t[1:]\n            else:\n                sign = \"+\"\n            l.extend([sign, t])\n        sign = l.pop(0)\n        if sign == '+':\n            sign = \"\"\n        return sign + ' '.join(l)\n\n    def _print_MatMul(self, expr):\n        from sympy import Add, MatAdd, HadamardProduct, MatMul, Mul\n\n        def parens(x):\n            if isinstance(x, (Add, MatAdd, HadamardProduct)):\n                return r\"\\left(%s\\right)\" % self._print(x)\n            return self._print(x)\n\n        if isinstance(expr, MatMul) and expr.args[0].is_Number and expr.args[0]<0:\n            expr = Mul(-1*expr.args[0], MatMul(*expr.args[1:]))\n            return '-' + ' '.join(map(parens, expr.args))\n        else:\n            return ' '.join(map(parens, expr.args))\n\n    def _print_Mod(self, expr, exp=None):\n        if exp is not None:\n            return r'\\left(%s\\bmod{%s}\\right)^{%s}' % (self.parenthesize(expr.args[0],\n                    PRECEDENCE['Mul'], strict=True), self._print(expr.args[1]), self._print(exp))\n        return r'%s\\bmod{%s}' % (self.parenthesize(expr.args[0],\n                PRECEDENCE['Mul'], strict=True), self._print(expr.args[1]))\n\n    def _print_HadamardProduct(self, expr):\n        from sympy import Add, MatAdd, MatMul\n\n        def parens(x):\n            if isinstance(x, (Add, MatAdd, MatMul)):\n                return r\"\\left(%s\\right)\" % self._print(x)\n            return self._print(x)\n        return r' \\circ '.join(map(parens, expr.args))\n\n    def _print_KroneckerProduct(self, expr):\n        from sympy import Add, MatAdd, MatMul\n\n        def parens(x):\n            if isinstance(x, (Add, MatAdd, MatMul)):\n                return r\"\\left(%s\\right)\" % self._print(x)\n            return self._print(x)\n        return r' \\otimes '.join(map(parens, expr.args))\n\n    def _print_MatPow(self, expr):\n        base, exp = expr.base, expr.exp\n        from sympy.matrices import MatrixSymbol\n        if not isinstance(base, MatrixSymbol):\n            return r\"\\left(%s\\right)^{%s}\" % (self._print(base), self._print(exp))\n        else:\n            return \"%s^{%s}\" % (self._print(base), self._print(exp))\n\n    def _print_ZeroMatrix(self, Z):\n        return r\"\\mathbb{0}\"\n\n    def _print_Identity(self, I):\n        return r\"\\mathbb{I}\"\n\n    def _print_NDimArray(self, expr):\n\n        if expr.rank() == 0:\n            return self._print(expr[()])\n\n        mat_str = self._settings['mat_str']\n        if mat_str is None:\n            if self._settings['mode'] == 'inline':\n                mat_str = 'smallmatrix'\n            else:\n                if (expr.rank() == 0) or (expr.shape[-1] <= 10):\n                    mat_str = 'matrix'\n                else:\n                    mat_str = 'array'\n        block_str = r'\\begin{%MATSTR%}%s\\end{%MATSTR%}'\n        block_str = block_str.replace('%MATSTR%', mat_str)\n        if self._settings['mat_delim']:\n            left_delim = self._settings['mat_delim']\n            right_delim = self._delim_dict[left_delim]\n            block_str = r'\\left' + left_delim + block_str + \\\n                      r'\\right' + right_delim\n\n        if expr.rank() == 0:\n            return block_str % \"\"\n\n        level_str = [[]] + [[] for i in range(expr.rank())]\n        shape_ranges = [list(range(i)) for i in expr.shape]\n        for outer_i in itertools.product(*shape_ranges):\n            level_str[-1].append(self._print(expr[outer_i]))\n            even = True\n            for back_outer_i in range(expr.rank()-1, -1, -1):\n                if len(level_str[back_outer_i+1]) < expr.shape[back_outer_i]:\n                    break\n                if even:\n                    level_str[back_outer_i].append(r\" & \".join(level_str[back_outer_i+1]))\n                else:\n                    level_str[back_outer_i].append(block_str % (r\"\\\\\".join(level_str[back_outer_i+1])))\n                    if len(level_str[back_outer_i+1]) == 1:\n                        level_str[back_outer_i][-1] = r\"\\left[\" + level_str[back_outer_i][-1] + r\"\\right]\"\n                even = not even\n                level_str[back_outer_i+1] = []\n\n        out_str = level_str[0][0]\n\n        if expr.rank() % 2 == 1:\n            out_str = block_str % out_str\n\n        return out_str\n\n    _print_ImmutableDenseNDimArray = _print_NDimArray\n    _print_ImmutableSparseNDimArray = _print_NDimArray\n    _print_MutableDenseNDimArray = _print_NDimArray\n    _print_MutableSparseNDimArray = _print_NDimArray\n\n    def _printer_tensor_indices(self, name, indices, index_map={}):\n        out_str = self._print(name)\n        last_valence = None\n        prev_map = None\n        for index in indices:\n            new_valence = index.is_up\n            if ((index in index_map) or prev_map) and last_valence == new_valence:\n                out_str += \",\"\n            if last_valence != new_valence:\n                if last_valence is not None:\n                    out_str += \"}\"\n                if index.is_up:\n                    out_str += \"{}^{\"\n                else:\n                    out_str += \"{}_{\"\n            out_str += self._print(index.args[0])\n            if index in index_map:\n                out_str += \"=\"\n                out_str += self._print(index_map[index])\n                prev_map = True\n            else:\n                prev_map = False\n            last_valence = new_valence\n        if last_valence is not None:\n            out_str += \"}\"\n        return out_str\n\n    def _print_Tensor(self, expr):\n        name = expr.args[0].args[0]\n        indices = expr.get_indices()\n        return self._printer_tensor_indices(name, indices)\n\n    def _print_TensorElement(self, expr):\n        name = expr.expr.args[0].args[0]\n        indices = expr.expr.get_indices()\n        index_map = expr.index_map\n        return self._printer_tensor_indices(name, indices, index_map)\n\n    def _print_TensMul(self, expr):\n        # prints expressions like \"A(a)\", \"3*A(a)\", \"(1+x)*A(a)\"\n        sign, args = expr._get_args_for_traditional_printer()\n        return sign + \"\".join(\n            [self.parenthesize(arg, precedence(expr)) for arg in args]\n        )\n\n    def _print_TensAdd(self, expr):\n        a = []\n        args = expr.args\n        for x in args:\n            a.append(self.parenthesize(x, precedence(expr)))\n        a.sort()\n        s = ' + '.join(a)\n        s = s.replace('+ -', '- ')\n        return s\n\n    def _print_TensorIndex(self, expr):\n        return \"{}%s{%s}\" % (\n            \"^\" if expr.is_up else \"_\",\n            self._print(expr.args[0])\n        )\n        return self._print(expr.args[0])\n\n    def _print_tuple(self, expr):\n        return r\"\\left ( %s\\right )\" % \\\n            r\", \\quad \".join([ self._print(i) for i in expr ])\n\n    def _print_TensorProduct(self, expr):\n        elements = [self._print(a) for a in expr.args]\n        return r' \\otimes '.join(elements)\n\n    def _print_WedgeProduct(self, expr):\n        elements = [self._print(a) for a in expr.args]\n        return r' \\wedge '.join(elements)\n\n    def _print_Tuple(self, expr):\n        return self._print_tuple(expr)\n\n    def _print_list(self, expr):\n        return r\"\\left [ %s\\right ]\" % \\\n            r\", \\quad \".join([ self._print(i) for i in expr ])\n\n    def _print_dict(self, d):\n        keys = sorted(d.keys(), key=default_sort_key)\n        items = []\n\n        for key in keys:\n            val = d[key]\n            items.append(\"%s : %s\" % (self._print(key), self._print(val)))\n\n        return r\"\\left \\{ %s\\right \\}\" % r\", \\quad \".join(items)\n\n    def _print_Dict(self, expr):\n        return self._print_dict(expr)\n\n    def _print_DiracDelta(self, expr, exp=None):\n        if len(expr.args) == 1 or expr.args[1] == 0:\n            tex = r\"\\delta\\left(%s\\right)\" % self._print(expr.args[0])\n        else:\n            tex = r\"\\delta^{\\left( %s \\right)}\\left( %s \\right)\" % (\n                self._print(expr.args[1]), self._print(expr.args[0]))\n        if exp:\n            tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n        return tex\n\n    def _print_SingularityFunction(self, expr):\n        shift = self._print(expr.args[0] - expr.args[1])\n        power = self._print(expr.args[2])\n        tex = r\"{\\langle %s \\rangle}^{%s}\" % (shift, power)\n        return tex\n\n    def _print_Heaviside(self, expr, exp=None):\n        tex = r\"\\theta\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp:\n            tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n        return tex\n\n    def _print_KroneckerDelta(self, expr, exp=None):\n        i = self._print(expr.args[0])\n        j = self._print(expr.args[1])\n        if expr.args[0].is_Atom and expr.args[1].is_Atom:\n            tex = r'\\delta_{%s %s}' % (i, j)\n        else:\n            tex = r'\\delta_{%s, %s}' % (i, j)\n        if exp:\n            tex = r'\\left(%s\\right)^{%s}' % (tex, exp)\n        return tex\n\n    def _print_LeviCivita(self, expr, exp=None):\n        indices = map(self._print, expr.args)\n        if all(x.is_Atom for x in expr.args):\n            tex = r'\\varepsilon_{%s}' % \" \".join(indices)\n        else:\n            tex = r'\\varepsilon_{%s}' % \", \".join(indices)\n        if exp:\n            tex = r'\\left(%s\\right)^{%s}' % (tex, exp)\n        return tex\n\n    def _print_ProductSet(self, p):\n        if len(p.sets) > 1 and not has_variety(p.sets):\n            return self._print(p.sets[0]) + \"^%d\" % len(p.sets)\n        else:\n            return r\" \\times \".join(self._print(set) for set in p.sets)\n\n    def _print_RandomDomain(self, d):\n        if hasattr(d, 'as_boolean'):\n            return 'Domain: ' + self._print(d.as_boolean())\n        elif hasattr(d, 'set'):\n            return ('Domain: ' + self._print(d.symbols) + ' in ' +\n                    self._print(d.set))\n        elif hasattr(d, 'symbols'):\n            return 'Domain on ' + self._print(d.symbols)\n        else:\n            return self._print(None)\n\n    def _print_FiniteSet(self, s):\n        items = sorted(s.args, key=default_sort_key)\n        return self._print_set(items)\n\n    def _print_set(self, s):\n        items = sorted(s, key=default_sort_key)\n        items = \", \".join(map(self._print, items))\n        return r\"\\left\\{%s\\right\\}\" % items\n\n    _print_frozenset = _print_set\n\n    def _print_Range(self, s):\n        dots = r'\\ldots'\n\n        if s.start.is_infinite:\n            printset = s.start, dots, s[-1] - s.step, s[-1]\n        elif s.stop.is_infinite or len(s) > 4:\n            it = iter(s)\n            printset = next(it), next(it), dots, s[-1]\n        else:\n            printset = tuple(s)\n\n        return (r\"\\left\\{\"\n              + r\", \".join(self._print(el) for el in printset)\n              + r\"\\right\\}\")\n\n    def _print_SeqFormula(self, s):\n        if s.start is S.NegativeInfinity:\n            stop = s.stop\n            printset = (r'\\ldots', s.coeff(stop - 3), s.coeff(stop - 2),\n                s.coeff(stop - 1), s.coeff(stop))\n        elif s.stop is S.Infinity or s.length > 4:\n            printset = s[:4]\n            printset.append(r'\\ldots')\n        else:\n            printset = tuple(s)\n\n        return (r\"\\left[\"\n              + r\", \".join(self._print(el) for el in printset)\n              + r\"\\right]\")\n\n    _print_SeqPer = _print_SeqFormula\n    _print_SeqAdd = _print_SeqFormula\n    _print_SeqMul = _print_SeqFormula\n\n    def _print_Interval(self, i):\n        if i.start == i.end:\n            return r\"\\left\\{%s\\right\\}\" % self._print(i.start)\n\n        else:\n            if i.left_open:\n                left = '('\n            else:\n                left = '['\n\n            if i.right_open:\n                right = ')'\n            else:\n                right = ']'\n\n            return r\"\\left%s%s, %s\\right%s\" % \\\n                   (left, self._print(i.start), self._print(i.end), right)\n\n    def _print_AccumulationBounds(self, i):\n        return r\"\\langle %s, %s\\rangle\" % \\\n                (self._print(i.min), self._print(i.max))\n\n    def _print_Union(self, u):\n        return r\" \\cup \".join([self._print(i) for i in u.args])\n\n    def _print_Complement(self, u):\n        return r\" \\setminus \".join([self._print(i) for i in u.args])\n\n    def _print_Intersection(self, u):\n        return r\" \\cap \".join([self._print(i) for i in u.args])\n\n    def _print_SymmetricDifference(self, u):\n        return r\" \\triangle \".join([self._print(i) for i in u.args])\n\n    def _print_EmptySet(self, e):\n        return r\"\\emptyset\"\n\n    def _print_Naturals(self, n):\n        return r\"\\mathbb{N}\"\n\n    def _print_Naturals0(self, n):\n        return r\"\\mathbb{N}_0\"\n\n    def _print_Integers(self, i):\n        return r\"\\mathbb{Z}\"\n\n    def _print_Reals(self, i):\n        return r\"\\mathbb{R}\"\n\n    def _print_Complexes(self, i):\n        return r\"\\mathbb{C}\"\n\n    def _print_ImageSet(self, s):\n        sets = s.args[1:]\n        varsets = [r\"%s \\in %s\" % (self._print(var), self._print(setv))\n            for var, setv in zip(s.lamda.variables, sets)]\n        return r\"\\left\\{%s\\; |\\; %s\\right\\}\" % (\n            self._print(s.lamda.expr),\n            ', '.join(varsets))\n\n    def _print_ConditionSet(self, s):\n        vars_print = ', '.join([self._print(var) for var in Tuple(s.sym)])\n        if s.base_set is S.UniversalSet:\n            return r\"\\left\\{%s \\mid %s \\right\\}\" % (\n            vars_print,\n            self._print(s.condition.as_expr()))\n\n        return r\"\\left\\{%s \\mid %s \\in %s \\wedge %s \\right\\}\" % (\n            vars_print,\n            vars_print,\n            self._print(s.base_set),\n            self._print(s.condition.as_expr()))\n\n    def _print_ComplexRegion(self, s):\n        vars_print = ', '.join([self._print(var) for var in s.variables])\n        return r\"\\left\\{%s\\; |\\; %s \\in %s \\right\\}\" % (\n            self._print(s.expr),\n            vars_print,\n            self._print(s.sets))\n\n    def _print_Contains(self, e):\n        return r\"%s \\in %s\" % tuple(self._print(a) for a in e.args)\n\n    def _print_FourierSeries(self, s):\n        return self._print_Add(s.truncate()) + self._print(r' + \\ldots')\n\n    def _print_FormalPowerSeries(self, s):\n        return self._print_Add(s.infinite)\n\n    def _print_FiniteField(self, expr):\n        return r\"\\mathbb{F}_{%s}\" % expr.mod\n\n    def _print_IntegerRing(self, expr):\n        return r\"\\mathbb{Z}\"\n\n    def _print_RationalField(self, expr):\n        return r\"\\mathbb{Q}\"\n\n    def _print_RealField(self, expr):\n        return r\"\\mathbb{R}\"\n\n    def _print_ComplexField(self, expr):\n        return r\"\\mathbb{C}\"\n\n    def _print_PolynomialRing(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left[%s\\right]\" % (domain, symbols)\n\n    def _print_FractionField(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left(%s\\right)\" % (domain, symbols)\n\n    def _print_PolynomialRingBase(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        inv = \"\"\n        if not expr.is_Poly:\n            inv = r\"S_<^{-1}\"\n        return r\"%s%s\\left[%s\\right]\" % (inv, domain, symbols)\n\n    def _print_Poly(self, poly):\n        cls = poly.__class__.__name__\n        terms = []\n        for monom, coeff in poly.terms():\n            s_monom = ''\n            for i, exp in enumerate(monom):\n                if exp > 0:\n                    if exp == 1:\n                        s_monom += self._print(poly.gens[i])\n                    else:\n                        s_monom += self._print(pow(poly.gens[i], exp))\n\n            if coeff.is_Add:\n                if s_monom:\n                    s_coeff = r\"\\left(%s\\right)\" % self._print(coeff)\n                else:\n                    s_coeff = self._print(coeff)\n            else:\n                if s_monom:\n                    if coeff is S.One:\n                        terms.extend(['+', s_monom])\n                        continue\n\n                    if coeff is S.NegativeOne:\n                        terms.extend(['-', s_monom])\n                        continue\n\n                s_coeff = self._print(coeff)\n\n            if not s_monom:\n                s_term = s_coeff\n            else:\n                s_term = s_coeff + \" \" + s_monom\n\n            if s_term.startswith('-'):\n                terms.extend(['-', s_term[1:]])\n            else:\n                terms.extend(['+', s_term])\n\n        if terms[0] in ['-', '+']:\n            modifier = terms.pop(0)\n\n            if modifier == '-':\n                terms[0] = '-' + terms[0]\n\n        expr = ' '.join(terms)\n        gens = list(map(self._print, poly.gens))\n        domain = \"domain=%s\" % self._print(poly.get_domain())\n\n        args = \", \".join([expr] + gens + [domain])\n        if cls in accepted_latex_functions:\n            tex = r\"\\%s {\\left (%s \\right )}\" % (cls, args)\n        else:\n            tex = r\"\\operatorname{%s}{\\left( %s \\right)}\" % (cls, args)\n\n        return tex\n\n    def _print_ComplexRootOf(self, root):\n        cls = root.__class__.__name__\n        if cls == \"ComplexRootOf\":\n            cls = \"CRootOf\"\n        expr = self._print(root.expr)\n        index = root.index\n        if cls in accepted_latex_functions:\n            return r\"\\%s {\\left(%s, %d\\right)}\" % (cls, expr, index)\n        else:\n            return r\"\\operatorname{%s} {\\left(%s, %d\\right)}\" % (cls, expr, index)\n\n    def _print_RootSum(self, expr):\n        cls = expr.__class__.__name__\n        args = [self._print(expr.expr)]\n\n        if expr.fun is not S.IdentityFunction:\n            args.append(self._print(expr.fun))\n\n        if cls in accepted_latex_functions:\n            return r\"\\%s {\\left(%s\\right)}\" % (cls, \", \".join(args))\n        else:\n            return r\"\\operatorname{%s} {\\left(%s\\right)}\" % (cls, \", \".join(args))\n\n    def _print_PolyElement(self, poly):\n        mul_symbol = self._settings['mul_symbol_latex']\n        return poly.str(self, PRECEDENCE, \"{%s}^{%d}\", mul_symbol)\n\n    def _print_FracElement(self, frac):\n        if frac.denom == 1:\n            return self._print(frac.numer)\n        else:\n            numer = self._print(frac.numer)\n            denom = self._print(frac.denom)\n            return r\"\\frac{%s}{%s}\" % (numer, denom)\n\n    def _print_euler(self, expr, exp=None):\n        m, x = (expr.args[0], None) if len(expr.args) == 1 else expr.args\n        tex = r\"E_{%s}\" % self._print(m)\n        if exp is not None:\n            tex = r\"%s^{%s}\" % (tex, self._print(exp))\n        if x is not None:\n            tex = r\"%s\\left(%s\\right)\" % (tex, self._print(x))\n        return tex\n\n    def _print_catalan(self, expr, exp=None):\n        tex = r\"C_{%s}\" % self._print(expr.args[0])\n        if exp is not None:\n            tex = r\"%s^{%s}\" % (tex, self._print(exp))\n        return tex\n\n    def _print_MellinTransform(self, expr):\n        return r\"\\mathcal{M}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_InverseMellinTransform(self, expr):\n        return r\"\\mathcal{M}^{-1}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_LaplaceTransform(self, expr):\n        return r\"\\mathcal{L}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_InverseLaplaceTransform(self, expr):\n        return r\"\\mathcal{L}^{-1}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_FourierTransform(self, expr):\n        return r\"\\mathcal{F}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_InverseFourierTransform(self, expr):\n        return r\"\\mathcal{F}^{-1}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_SineTransform(self, expr):\n        return r\"\\mathcal{SIN}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_InverseSineTransform(self, expr):\n        return r\"\\mathcal{SIN}^{-1}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_CosineTransform(self, expr):\n        return r\"\\mathcal{COS}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_InverseCosineTransform(self, expr):\n        return r\"\\mathcal{COS}^{-1}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_DMP(self, p):\n        try:\n            if p.ring is not None:\n                # TODO incorporate order\n                return self._print(p.ring.to_sympy(p))\n        except SympifyError:\n            pass\n        return self._print(repr(p))\n\n    def _print_DMF(self, p):\n        return self._print_DMP(p)\n\n    def _print_Object(self, object):\n        return self._print(Symbol(object.name))\n\n    def _print_Morphism(self, morphism):\n        domain = self._print(morphism.domain)\n(C) \n    def _print_airyaiprime(self, expr, exp=None):\n        return self._hprint_airy_prime(expr, exp, 'Ai')\n\n    def _print_airybiprime(self, expr, exp=None):\n        return self._hprint_airy_prime(expr, exp, 'Bi')\n\n    def _print_hyper(self, expr, exp=None):\n        tex = r\"{{}_{%s}F_{%s}\\left(\\begin{matrix} %s \\\\ %s \\end{matrix}\" \\\n              r\"\\middle| {%s} \\right)}\" % \\\n            (self._print(len(expr.ap)), self._print(len(expr.bq)),\n              self._hprint_vec(expr.ap), self._hprint_vec(expr.bq),\n              self._print(expr.argument))\n\n        if exp is not None:\n            tex = r\"{%s}^{%s}\" % (tex, self._print(exp))\n        return tex\n\n    def _print_meijerg(self, expr, exp=None):\n        tex = r\"{G_{%s, %s}^{%s, %s}\\left(\\begin{matrix} %s & %s \\\\\" \\\n              r\"%s & %s \\end{matrix} \\middle| {%s} \\right)}\" % \\\n            (self._print(len(expr.ap)), self._print(len(expr.bq)),\n              self._print(len(expr.bm)), self._print(len(expr.an)),\n              self._hprint_vec(expr.an), self._hprint_vec(expr.aother),\n              self._hprint_vec(expr.bm), self._hprint_vec(expr.bother),\n              self._print(expr.argument))\n\n        if exp is not None:\n            tex = r\"{%s}^{%s}\" % (tex, self._print(exp))\n        return tex\n\n    def _print_dirichlet_eta(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"\\eta^{%s}%s\" % (self._print(exp), tex)\n        return r\"\\eta%s\" % tex\n\n    def _print_zeta(self, expr, exp=None):\n        if len(expr.args) == 2:\n            tex = r\"\\left(%s, %s\\right)\" % tuple(map(self._print, expr.args))\n        else:\n            tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"\\zeta^{%s}%s\" % (self._print(exp), tex)\n        return r\"\\zeta%s\" % tex\n\n    def _print_lerchphi(self, expr, exp=None):\n        tex = r\"\\left(%s, %s, %s\\right)\" % tuple(map(self._print, expr.args))\n        if exp is None:\n            return r\"\\Phi%s\" % tex\n        return r\"\\Phi^{%s}%s\" % (self._print(exp), tex)\n\n    def _print_polylog(self, expr, exp=None):\n        s, z = map(self._print, expr.args)\n        tex = r\"\\left(%s\\right)\" % z\n        if exp is None:\n            return r\"\\operatorname{Li}_{%s}%s\" % (s, tex)\n        return r\"\\operatorname{Li}_{%s}^{%s}%s\" % (s, self._print(exp), tex)\n\n    def _print_jacobi(self, expr, exp=None):\n        n, a, b, x = map(self._print, expr.args)\n        tex = r\"P_{%s}^{\\left(%s,%s\\right)}\\left(%s\\right)\" % (n, a, b, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_gegenbauer(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"C_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_chebyshevt(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"T_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_chebyshevu(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"U_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_legendre(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"P_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_assoc_legendre(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"P_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_hermite(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"H_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_laguerre(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"L_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_assoc_laguerre(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"L_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Ynm(self, expr, exp=None):\n        n, m, theta, phi = map(self._print, expr.args)\n        tex = r\"Y_{%s}^{%s}\\left(%s,%s\\right)\" % (n, m, theta, phi)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Znm(self, expr, exp=None):\n        n, m, theta, phi = map(self._print, expr.args)\n        tex = r\"Z_{%s}^{%s}\\left(%s,%s\\right)\" % (n, m, theta, phi)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Rational(self, expr):\n        if expr.q != 1:\n            sign = \"\"\n            p = expr.p\n            if expr.p < 0:\n                sign = \"- \"\n                p = -p\n            if self._settings['fold_short_frac']:\n                return r\"%s%d / %d\" % (sign, p, expr.q)\n            return r\"%s\\frac{%d}{%d}\" % (sign, p, expr.q)\n        else:\n            return self._print(expr.p)\n\n    def _print_Order(self, expr):\n        s = self._print(expr.expr)\n        if expr.point and any(p != S.Zero for p in expr.point) or \\\n           len(expr.variables) > 1:\n            s += '; '\n            if len(expr.variables) > 1:\n                s += self._print(expr.variables)\n            elif len(expr.variables):\n                s += self._print(expr.variables[0])\n            s += r'\\rightarrow '\n            if len(expr.point) > 1:\n                s += self._print(expr.point)\n            else:\n                s += self._print(expr.point[0])\n        return r\"O\\left(%s\\right)\" % s\n\n    def _print_Symbol(self, expr):\n        if expr in self._settings['symbol_names']:\n            return self._settings['symbol_names'][expr]\n\n        return self._deal_with_super_sub(expr.name) if \\\n            '\\\\' not in expr.name else expr.name\n\n    _print_RandomSymbol = _print_Symbol\n    _print_MatrixSymbol = _print_Symbol\n\n    def _deal_with_super_sub(self, string):\n        if '{' in string:\n            return string\n\n        name, supers, subs = split_super_sub(string)\n\n        name = translate(name)\n        supers = [translate(sup) for sup in supers]\n        subs = [translate(sub) for sub in subs]\n\n        # glue all items together:\n        if len(supers) > 0:\n            name += \"^{%s}\" % \" \".join(supers)\n        if len(subs) > 0:\n            name += \"_{%s}\" % \" \".join(subs)\n\n        return name\n\n    def _print_Relational(self, expr):\n        if self._settings['itex']:\n            gt = r\"\\gt\"\n            lt = r\"\\lt\"\n        else:\n            gt = \">\"\n            lt = \"<\"\n\n        charmap = {\n            \"==\": \"=\",\n            \">\": gt,\n            \"<\": lt,\n            \">=\": r\"\\geq\",\n            \"<=\": r\"\\leq\",\n            \"!=\": r\"\\neq\",\n        }\n\n        return \"%s %s %s\" % (self._print(expr.lhs),\n            charmap[expr.rel_op], self._print(expr.rhs))\n\n    def _print_Piecewise(self, expr):\n        ecpairs = [r\"%s & \\text{for}\\: %s\" % (self._print(e), self._print(c))\n                   for e, c in expr.args[:-1]]\n        if expr.args[-1].cond == true:\n            ecpairs.append(r\"%s & \\text{otherwise}\" %\n                           self._print(expr.args[-1].expr))\n        else:\n            ecpairs.append(r\"%s & \\text{for}\\: %s\" %\n                           (self._print(expr.args[-1].expr),\n                            self._print(expr.args[-1].cond)))\n        tex = r\"\\begin{cases} %s \\end{cases}\"\n        return tex % r\" \\\\\".join(ecpairs)\n\n    def _print_MatrixBase(self, expr):\n        lines = []\n\n        for line in range(expr.rows):  # horrible, should be 'rows'\n            lines.append(\" & \".join([ self._print(i) for i in expr[line, :] ]))\n\n        mat_str = self._settings['mat_str']\n        if mat_str is None:\n            if self._settings['mode'] == 'inline':\n                mat_str = 'smallmatrix'\n            else:\n                if (expr.cols <= 10) is True:\n                    mat_str = 'matrix'\n                else:\n                    mat_str = 'array'\n\n        out_str = r'\\begin{%MATSTR%}%s\\end{%MATSTR%}'\n        out_str = out_str.replace('%MATSTR%', mat_str)\n        if mat_str == 'array':\n            out_str = out_str.replace('%s', '{' + 'c'*expr.cols + '}%s')\n        if self._settings['mat_delim']:\n            left_delim = self._settings['mat_delim']\n            right_delim = self._delim_dict[left_delim]\n            out_str = r'\\left' + left_delim + out_str + \\\n                      r'\\right' + right_delim\n        return out_str % r\"\\\\\".join(lines)\n    _print_ImmutableMatrix = _print_ImmutableDenseMatrix \\\n                           = _print_Matrix \\\n                           = _print_MatrixBase\n\n    def _print_MatrixElement(self, expr):\n        return self.parenthesize(expr.parent, PRECEDENCE[\"Atom\"], strict=True) \\\n            + '_{%s, %s}' % (expr.i, expr.j)\n\n    def _print_MatrixSlice(self, expr):\n        def latexslice(x):\n            x = list(x)\n            if x[2] == 1:\n                del x[2]\n            if x[1] == x[0] + 1:\n                del x[1]\n            if x[0] == 0:\n                x[0] = ''\n            return ':'.join(map(self._print, x))\n        return (self._print(expr.parent) + r'\\left[' +\n                latexslice(expr.rowslice) + ', ' +\n                latexslice(expr.colslice) + r'\\right]')\n\n    def _print_BlockMatrix(self, expr):\n        return self._print(expr.blocks)\n\n    def _print_Transpose(self, expr):\n        mat = expr.arg\n        from sympy.matrices import MatrixSymbol\n        if not isinstance(mat, MatrixSymbol):\n            return r\"\\left(%s\\right)^T\" % self._print(mat)\n        else:\n            return \"%s^T\" % self._print(mat)\n\n    def _print_Adjoint(self, expr):\n        mat = expr.arg\n        from sympy.matrices import MatrixSymbol\n        if not isinstance(mat, MatrixSymbol):\n            return r\"\\left(%s\\right)^\\dagger\" % self._print(mat)\n        else:\n            return r\"%s^\\dagger\" % self._print(mat)\n\n    def _print_MatAdd(self, expr):\n        terms = [self._print(t) for t in expr.args]\n        l = []\n        for t in terms:\n            if t.startswith('-'):\n                sign = \"-\"\n                t = t[1:]\n            else:\n                sign = \"+\"\n            l.extend([sign, t])\n        sign = l.pop(0)\n        if sign == '+':\n            sign = \"\"\n        return sign + ' '.join(l)\n\n    def _print_MatMul(self, expr):\n        from sympy import Add, MatAdd, HadamardProduct, MatMul, Mul\n\n        def parens(x):\n            if isinstance(x, (Add, MatAdd, HadamardProduct)):\n                return r\"\\left(%s\\right)\" % self._print(x)\n            return self._print(x)\n\n        if isinstance(expr, MatMul) and expr.args[0].is_Number and expr.args[0]<0:\n            expr = Mul(-1*expr.args[0], MatMul(*expr.args[1:]))\n            return '-' + ' '.join(map(parens, expr.args))\n        else:\n            return ' '.join(map(parens, expr.args))\n\n    def _print_Mod(self, expr, exp=None):\n        if exp is not None:\n            return r'\\left(%s\\bmod{%s}\\right)^{%s}' % (self.parenthesize(expr.args[0],\n                    PRECEDENCE['Mul'], strict=True), self._print(expr.args[1]), self._print(exp))\n        return r'%s\\bmod{%s}' % (self.parenthesize(expr.args[0],\n                PRECEDENCE['Mul'], strict=True), self._print(expr.args[1]))\n\n    def _print_HadamardProduct(self, expr):\n        from sympy import Add, MatAdd, MatMul\n\n        def parens(x):\n            if isinstance(x, (Add, MatAdd, MatMul)):\n                return r\"\\left(%s\\right)\" % self._print(x)\n            return self._print(x)\n        return r' \\circ '.join(map(parens, expr.args))\n\n    def _print_KroneckerProduct(self, expr):\n        from sympy import Add, MatAdd, MatMul\n\n        def parens(x):\n            if isinstance(x, (Add, MatAdd, MatMul)):\n                return r\"\\left(%s\\right)\" % self._print(x)\n            return self._print(x)\n        return r' \\otimes '.join(map(parens, expr.args))\n\n    def _print_MatPow(self, expr):\n        base, exp = expr.base, expr.exp\n        from sympy.matrices import MatrixSymbol\n        if not isinstance(base, MatrixSymbol):\n            return r\"\\left(%s\\right)^{%s}\" % (self._print(base), self._print(exp))\n        else:\n            return \"%s^{%s}\" % (self._print(base), self._print(exp))\n\n    def _print_ZeroMatrix(self, Z):\n        return r\"\\mathbb{0}\"\n\n    def _print_Identity(self, I):\n        return r\"\\mathbb{I}\"\n\n    def _print_NDimArray(self, expr):\n\n        if expr.rank() == 0:\n            return self._print(expr[()])\n\n        mat_str = self._settings['mat_str']\n        if mat_str is None:\n            if self._settings['mode'] == 'inline':\n                mat_str = 'smallmatrix'\n            else:\n                if (expr.rank() == 0) or (expr.shape[-1] <= 10):\n                    mat_str = 'matrix'\n                else:\n                    mat_str = 'array'\n        block_str = r'\\begin{%MATSTR%}%s\\end{%MATSTR%}'\n        block_str = block_str.replace('%MATSTR%', mat_str)\n        if self._settings['mat_delim']:\n            left_delim = self._settings['mat_delim']\n            right_delim = self._delim_dict[left_delim]\n            block_str = r'\\left' + left_delim + block_str + \\\n                      r'\\right' + right_delim\n\n        if expr.rank() == 0:\n            return block_str % \"\"\n\n        level_str = [[]] + [[] for i in range(expr.rank())]\n        shape_ranges = [list(range(i)) for i in expr.shape]\n        for outer_i in itertools.product(*shape_ranges):\n            level_str[-1].append(self._print(expr[outer_i]))\n            even = True\n            for back_outer_i in range(expr.rank()-1, -1, -1):\n                if len(level_str[back_outer_i+1]) < expr.shape[back_outer_i]:\n                    break\n                if even:\n                    level_str[back_outer_i].append(r\" & \".join(level_str[back_outer_i+1]))\n                else:\n                    level_str[back_outer_i].append(block_str % (r\"\\\\\".join(level_str[back_outer_i+1])))\n                    if len(level_str[back_outer_i+1]) == 1:\n                        level_str[back_outer_i][-1] = r\"\\left[\" + level_str[back_outer_i][-1] + r\"\\right]\"\n                even = not even\n                level_str[back_outer_i+1] = []\n\n        out_str = level_str[0][0]\n\n        if expr.rank() % 2 == 1:\n            out_str = block_str % out_str\n\n        return out_str\n\n    _print_ImmutableDenseNDimArray = _print_NDimArray\n    _print_ImmutableSparseNDimArray = _print_NDimArray\n    _print_MutableDenseNDimArray = _print_NDimArray\n    _print_MutableSparseNDimArray = _print_NDimArray\n\n    def _printer_tensor_indices(self, name, indices, index_map={}):\n        out_str = self._print(name)\n        last_valence = None\n        prev_map = None\n        for index in indices:\n            new_valence = index.is_up\n            if ((index in index_map) or prev_map) and last_valence == new_valence:\n                out_str += \",\"\n            if last_valence != new_valence:\n                if last_valence is not None:\n                    out_str += \"}\"\n                if index.is_up:\n                    out_str += \"{}^{\"\n                else:\n                    out_str += \"{}_{\"\n            out_str += self._print(index.args[0])\n            if index in index_map:\n                out_str += \"=\"\n                out_str += self._print(index_map[index])\n                prev_map = True\n            else:\n                prev_map = False\n            last_valence = new_valence\n        if last_valence is not None:\n            out_str += \"}\"\n        return out_str\n\n    def _print_Tensor(self, expr):\n        name = expr.args[0].args[0]\n        indices = expr.get_indices()\n        return self._printer_tensor_indices(name, indices)\n\n    def _print_TensorElement(self, expr):\n        name = expr.expr.args[0].args[0]\n        indices = expr.expr.get_indices()\n        index_map = expr.index_map\n        return self._printer_tensor_indices(name, indices, index_map)\n\n    def _print_TensMul(self, expr):\n        # prints expressions like \"A(a)\", \"3*A(a)\", \"(1+x)*A(a)\"\n        sign, args = expr._get_args_for_traditional_printer()\n        return sign + \"\".join(\n            [self.parenthesize(arg, precedence(expr)) for arg in args]\n        )\n\n    def _print_TensAdd(self, expr):\n        a = []\n        args = expr.args\n        for x in args:\n            a.append(self.parenthesize(x, precedence(expr)))\n        a.sort()\n        s = ' + '.join(a)\n        s = s.replace('+ -', '- ')\n        return s\n\n    def _print_TensorIndex(self, expr):\n        return \"{}%s{%s}\" % (\n            \"^\" if expr.is_up else \"_\",\n            self._print(expr.args[0])\n        )\n        return self._print(expr.args[0])\n\n    def _print_tuple(self, expr):\n        return r\"\\left ( %s\\right )\" % \\\n            r\", \\quad \".join([ self._print(i) for i in expr ])\n\n    def _print_TensorProduct(self, expr):\n        elements = [self._print(a) for a in expr.args]\n        return r' \\otimes '.join(elements)\n\n    def _print_WedgeProduct(self, expr):\n        elements = [self._print(a) for a in expr.args]\n        return r' \\wedge '.join(elements)\n\n    def _print_Tuple(self, expr):\n        return self._print_tuple(expr)\n\n    def _print_list(self, expr):\n        return r\"\\left [ %s\\right ]\" % \\\n            r\", \\quad \".join([ self._print(i) for i in expr ])\n\n    def _print_dict(self, d):\n        keys = sorted(d.keys(), key=default_sort_key)\n        items = []\n\n        for key in keys:\n            val = d[key]\n            items.append(\"%s : %s\" % (self._print(key), self._print(val)))\n\n        return r\"\\left \\{ %s\\right \\}\" % r\", \\quad \".join(items)\n\n    def _print_Dict(self, expr):\n        return self._print_dict(expr)\n\n    def _print_DiracDelta(self, expr, exp=None):\n        if len(expr.args) == 1 or expr.args[1] == 0:\n            tex = r\"\\delta\\left(%s\\right)\" % self._print(expr.args[0])\n        else:\n            tex = r\"\\delta^{\\left( %s \\right)}\\left( %s \\right)\" % (\n                self._print(expr.args[1]), self._print(expr.args[0]))\n        if exp:\n            tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n        return tex\n\n    def _print_SingularityFunction(self, expr):\n        shift = self._print(expr.args[0] - expr.args[1])\n        power = self._print(expr.args[2])\n        tex = r\"{\\langle %s \\rangle}^{%s}\" % (shift, power)\n        return tex\n\n    def _print_Heaviside(self, expr, exp=None):\n        tex = r\"\\theta\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp:\n            tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n        return tex\n\n    def _print_KroneckerDelta(self, expr, exp=None):\n        i = self._print(expr.args[0])\n        j = self._print(expr.args[1])\n        if expr.args[0].is_Atom and expr.args[1].is_Atom:\n            tex = r'\\delta_{%s %s}' % (i, j)\n        else:\n            tex = r'\\delta_{%s, %s}' % (i, j)\n        if exp:\n            tex = r'\\left(%s\\right)^{%s}' % (tex, exp)\n        return tex\n\n    def _print_LeviCivita(self, expr, exp=None):\n        indices = map(self._print, expr.args)\n        if all(x.is_Atom for x in expr.args):\n            tex = r'\\varepsilon_{%s}' % \" \".join(indices)\n        else:\n            tex = r'\\varepsilon_{%s}' % \", \".join(indices)\n        if exp:\n            tex = r'\\left(%s\\right)^{%s}' % (tex, exp)\n        return tex\n\n    def _print_ProductSet(self, p):\n        if len(p.sets) > 1 and not has_variety(p.sets):\n            return self._print(p.sets[0]) + \"^%d\" % len(p.sets)\n        else:\n            return r\" \\times \".join(self._print(set) for set in p.sets)\n\n    def _print_RandomDomain(self, d):\n        if hasattr(d, 'as_boolean'):\n            return 'Domain: ' + self._print(d.as_boolean())\n        elif hasattr(d, 'set'):\n            return ('Domain: ' + self._print(d.symbols) + ' in ' +\n                    self._print(d.set))\n        elif hasattr(d, 'symbols'):\n            return 'Domain on ' + self._print(d.symbols)\n        else:\n            return self._print(None)\n\n    def _print_FiniteSet(self, s):\n        items = sorted(s.args, key=default_sort_key)\n        return self._print_set(items)\n\n    def _print_set(self, s):\n        items = sorted(s, key=default_sort_key)\n        items = \", \".join(map(self._print, items))\n        return r\"\\left\\{%s\\right\\}\" % items\n\n    _print_frozenset = _print_set\n\n    def _print_Range(self, s):\n        dots = r'\\ldots'\n\n        if s.start.is_infinite:\n            printset = s.start, dots, s[-1] - s.step, s[-1]\n        elif s.stop.is_infinite or len(s) > 4:\n            it = iter(s)\n            printset = next(it), next(it), dots, s[-1]\n        else:\n            printset = tuple(s)\n\n        return (r\"\\left\\{\"\n              + r\", \".join(self._print(el) for el in printset)\n              + r\"\\right\\}\")\n\n    def _print_SeqFormula(self, s):\n        if s.start is S.NegativeInfinity:\n            stop = s.stop\n            printset = (r'\\ldots', s.coeff(stop - 3), s.coeff(stop - 2),\n                s.coeff(stop - 1), s.coeff(stop))\n        elif s.stop is S.Infinity or s.length > 4:\n            printset = s[:4]\n            printset.append(r'\\ldots')\n        else:\n            printset = tuple(s)\n\n        return (r\"\\left[\"\n              + r\", \".join(self._print(el) for el in printset)\n              + r\"\\right]\")\n\n    _print_SeqPer = _print_SeqFormula\n    _print_SeqAdd = _print_SeqFormula\n    _print_SeqMul = _print_SeqFormula\n\n    def _print_Interval(self, i):\n        if i.start == i.end:\n            return r\"\\left\\{%s\\right\\}\" % self._print(i.start)\n\n        else:\n            if i.left_open:\n                left = '('\n            else:\n                left = '['\n\n            if i.right_open:\n                right = ')'\n            else:\n                right = ']'\n\n            return r\"\\left%s%s, %s\\right%s\" % \\\n                   (left, self._print(i.start), self._print(i.end), right)\n\n    def _print_AccumulationBounds(self, i):\n        return r\"\\langle %s, %s\\rangle\" % \\\n                (self._print(i.min), self._print(i.max))\n\n    def _print_Union(self, u):\n        return r\" \\cup \".join([self._print(i) for i in u.args])\n\n    def _print_Complement(self, u):\n        return r\" \\setminus \".join([self._print(i) for i in u.args])\n\n    def _print_Intersection(self, u):\n        return r\" \\cap \".join([self._print(i) for i in u.args])\n\n    def _print_SymmetricDifference(self, u):\n        return r\" \\triangle \".join([self._print(i) for i in u.args])\n\n    def _print_EmptySet(self, e):\n        return r\"\\emptyset\"\n\n    def _print_Naturals(self, n):\n        return r\"\\mathbb{N}\"\n\n    def _print_Naturals0(self, n):\n        return r\"\\mathbb{N}_0\"\n\n    def _print_Integers(self, i):\n        return r\"\\mathbb{Z}\"\n\n    def _print_Reals(self, i):\n        return r\"\\mathbb{R}\"\n\n    def _print_Complexes(self, i):\n        return r\"\\mathbb{C}\"\n\n    def _print_ImageSet(self, s):\n        sets = s.args[1:]\n        varsets = [r\"%s \\in %s\" % (self._print(var), self._print(setv))\n            for var, setv in zip(s.lamda.variables, sets)]\n        return r\"\\left\\{%s\\; |\\; %s\\right\\}\" % (\n            self._print(s.lamda.expr),\n            ', '.join(varsets))\n\n    def _print_ConditionSet(self, s):\n        vars_print = ', '.join([self._print(var) for var in Tuple(s.sym)])\n        if s.base_set is S.UniversalSet:\n            return r\"\\left\\{%s \\mid %s \\right\\}\" % (\n            vars_print,\n            self._print(s.condition.as_expr()))\n\n        return r\"\\left\\{%s \\mid %s \\in %s \\wedge %s \\right\\}\" % (\n            vars_print,\n            vars_print,\n            self._print(s.base_set),\n            self._print(s.condition.as_expr()))\n\n    def _print_ComplexRegion(self, s):\n        vars_print = ', '.join([self._print(var) for var in s.variables])\n        return r\"\\left\\{%s\\; |\\; %s \\in %s \\right\\}\" % (\n            self._print(s.expr),\n            vars_print,\n            self._print(s.sets))\n\n    def _print_Contains(self, e):\n        return r\"%s \\in %s\" % tuple(self._print(a) for a in e.args)\n\n    def _print_FourierSeries(self, s):\n        return self._print_Add(s.truncate()) + self._print(r' + \\ldots')\n\n    def _print_FormalPowerSeries(self, s):\n        return self._print_Add(s.infinite)\n\n    def _print_FiniteField(self, expr):\n        return r\"\\mathbb{F}_{%s}\" % expr.mod\n\n    def _print_IntegerRing(self, expr):\n        return r\"\\mathbb{Z}\"\n\n    def _print_RationalField(self, expr):\n        return r\"\\mathbb{Q}\"\n\n    def _print_RealField(self, expr):\n        return r\"\\mathbb{R}\"\n\n    def _print_ComplexField(self, expr):\n        return r\"\\mathbb{C}\"\n\n    def _print_PolynomialRing(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left[%s\\right]\" % (domain, symbols)\n\n    def _print_FractionField(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        return r\"%s\\left(%s\\right)\" % (domain, symbols)\n\n    def _print_PolynomialRingBase(self, expr):\n        domain = self._print(expr.domain)\n        symbols = \", \".join(map(self._print, expr.symbols))\n        inv = \"\"\n        if not expr.is_Poly:\n            inv = r\"S_<^{-1}\"\n        return r\"%s%s\\left[%s\\right]\" % (inv, domain, symbols)\n\n    def _print_Poly(self, poly):\n        cls = poly.__class__.__name__\n        terms = []\n        for monom, coeff in poly.terms():\n            s_monom = ''\n            for i, exp in enumerate(monom):\n                if exp > 0:\n                    if exp == 1:\n                        s_monom += self._print(poly.gens[i])\n                    else:\n                        s_monom += self._print(pow(poly.gens[i], exp))\n\n            if coeff.is_Add:\n                if s_monom:\n                    s_coeff = r\"\\left(%s\\right)\" % self._print(coeff)\n                else:\n                    s_coeff = self._print(coeff)\n            else:\n                if s_monom:\n                    if coeff is S.One:\n                        terms.extend(['+', s_monom])\n                        continue\n\n                    if coeff is S.NegativeOne:\n                        terms.extend(['-', s_monom])\n                        continue\n\n                s_coeff = self._print(coeff)\n\n            if not s_monom:\n                s_term = s_coeff\n            else:\n                s_term = s_coeff + \" \" + s_monom\n\n            if s_term.startswith('-'):\n                terms.extend(['-', s_term[1:]])\n            else:\n                terms.extend(['+', s_term])\n\n        if terms[0] in ['-', '+']:\n            modifier = terms.pop(0)\n\n            if modifier == '-':\n                terms[0] = '-' + terms[0]\n\n        expr = ' '.join(terms)\n        gens = list(map(self._print, poly.gens))\n        domain = \"domain=%s\" % self._print(poly.get_domain())\n\n        args = \", \".join([expr] + gens + [domain])\n        if cls in accepted_latex_functions:\n            tex = r\"\\%s {\\left (%s \\right )}\" % (cls, args)\n        else:\n            tex = r\"\\operatorname{%s}{\\left( %s \\right)}\" % (cls, args)\n\n        return tex\n\n    def _print_ComplexRootOf(self, root):\n        cls = root.__class__.__name__\n        if cls == \"ComplexRootOf\":\n            cls = \"CRootOf\"\n        expr = self._print(root.expr)\n        index = root.index\n        if cls in accepted_latex_functions:\n            return r\"\\%s {\\left(%s, %d\\right)}\" % (cls, expr, index)\n        else:\n            return r\"\\operatorname{%s} {\\left(%s, %d\\right)}\" % (cls, expr, index)\n\n    def _print_RootSum(self, expr):\n        cls = expr.__class__.__name__\n        args = [self._print(expr.expr)]\n\n        if expr.fun is not S.IdentityFunction:\n            args.append(self._print(expr.fun))\n\n        if cls in accepted_latex_functions:\n            return r\"\\%s {\\left(%s\\right)}\" % (cls, \", \".join(args))\n        else:\n            return r\"\\operatorname{%s} {\\left(%s\\right)}\" % (cls, \", \".join(args))\n\n    def _print_PolyElement(self, poly):\n        mul_symbol = self._settings['mul_symbol_latex']\n        return poly.str(self, PRECEDENCE, \"{%s}^{%d}\", mul_symbol)\n\n    def _print_FracElement(self, frac):\n        if frac.denom == 1:\n            return self._print(frac.numer)\n        else:\n            numer = self._print(frac.numer)\n            denom = self._print(frac.denom)\n            return r\"\\frac{%s}{%s}\" % (numer, denom)\n\n    def _print_euler(self, expr, exp=None):\n        m, x = (expr.args[0], None) if len(expr.args) == 1 else expr.args\n        tex = r\"E_{%s}\" % self._print(m)\n        if exp is not None:\n            tex = r\"%s^{%s}\" % (tex, self._print(exp))\n        if x is not None:\n            tex = r\"%s\\left(%s\\right)\" % (tex, self._print(x))\n        return tex\n\n    def _print_catalan(self, expr, exp=None):\n        tex = r\"C_{%s}\" % self._print(expr.args[0])\n        if exp is not None:\n            tex = r\"%s^{%s}\" % (tex, self._print(exp))\n        return tex\n\n    def _print_MellinTransform(self, expr):\n        return r\"\\mathcal{M}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_InverseMellinTransform(self, expr):\n        return r\"\\mathcal{M}^{-1}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_LaplaceTransform(self, expr):\n        return r\"\\mathcal{L}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_InverseLaplaceTransform(self, expr):\n        return r\"\\mathcal{L}^{-1}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_FourierTransform(self, expr):\n        return r\"\\mathcal{F}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_InverseFourierTransform(self, expr):\n        return r\"\\mathcal{F}^{-1}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_SineTransform(self, expr):\n        return r\"\\mathcal{SIN}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_InverseSineTransform(self, expr):\n        return r\"\\mathcal{SIN}^{-1}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_CosineTransform(self, expr):\n        return r\"\\mathcal{COS}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_InverseCosineTransform(self, expr):\n        return r\"\\mathcal{COS}^{-1}_{%s}\\left[%s\\right]\\left(%s\\right)\" % (self._print(expr.args[1]), self._print(expr.args[0]), self._print(expr.args[2]))\n\n    def _print_DMP(self, p):\n        try:\n            if p.ring is not None:\n                # TODO incorporate order\n                return self._print(p.ring.to_sympy(p))\n        except SympifyError:\n            pass\n        return self._print(repr(p))\n\n    def _print_DMF(self, p):\n        return self._print_DMP(p)\n\n    def _print_Object(self, object):\n        return self._print(Symbol(object.name))\n\n    def _print_Morphism(self, morphism):\n        domain = self._print(morphism.domain)\n        codomain = self._print(morphism.codomain)\n        return \"%s\\\\rightarrow %s\" % (domain, codomain)\n\n    def _print_NamedMorphism(self, morphism):\n        pretty_name = self._print(Symbol(morphism.name))\n        pretty_morphism = self._print_Morphism(morphism)\n        return \"%s:%s\" % (pretty_name, pretty_morphism)\n\n    def _print_IdentityMorphism(self, morphism):\n        from sympy.categories import NamedMorphism\n        return self._print_NamedMorphism(NamedMorphism(\n            morphism.domain, morphism.codomain, \"id\"))\n\n    def _print_CompositeMorphism(self, morphism):\n        # All components of the morphism have names and it is thus\n        # possible to build the name of the composite.\n        component_names_list = [self._print(Symbol(component.name)) for\n                                component in morphism.components]\n        component_names_list.reverse()\n        component_names = \"\\\\circ \".join(component_names_list) + \":\"\n\n        pretty_morphism = self._print_Morphism(morphism)\n        return component_names + pretty_morphism\n\n    def _print_Category(self, morphism):\n        return \"\\\\mathbf{%s}\" % self._print(Symbol(morphism.name))\n\n    def _print_Diagram(self, diagram):\n        if not diagram.premises:\n            # This is an empty diagram.\n            return self._print(S.EmptySet)\n\n        latex_result = self._print(diagram.premises)\n        if diagram.conclusions:\n            latex_result += \"\\\\Longrightarrow %s\" % \\\n                            self._print(diagram.conclusions)\n\n        return latex_result\n\n    def _print_DiagramGrid(self, grid):\n        latex_result = \"\\\\begin{array}{%s}\\n\" % (\"c\" * grid.width)\n\n        for i in range(grid.height):\n            for j in range(grid.width):\n                if grid[i, j]:\n                    latex_result += latex(grid[i, j])\n                latex_result += \" \"\n                if j != grid.width - 1:\n                    latex_result += \"& \"\n\n            if i != grid.height - 1:\n                latex_result += \"\\\\\\\\\"\n            latex_result += \"\\n\"\n\n        latex_result += \"\\\\end{array}\\n\"\n        return latex_result\n\n    def _print_FreeModule(self, M):\n        return '{%s}^{%s}' % (self._print(M.ring), self._print(M.rank))\n\n    def _print_FreeModuleElement(self, m):\n        # Print as row vector for convenience, for now.\n        return r\"\\left[ %s \\right]\" % \",\".join(\n            '{' + self._print(x) + '}' for x in m)\n\n    def _print_SubModule(self, m):\n        return r\"\\left< %s \\right>\" % \",\".join(\n            '{' + self._print(x) + '}' for x in m.gens)\n\n    def _print_ModuleImplementedIdeal(self, m):\n        return r\"\\left< %s \\right>\" % \",\".join(\n            '{' + self._print(x) + '}' for [x] in m._module.gens)\n\n    def _print_Quaternion(self, expr):\n        # TODO: This expression is potentially confusing,\n        # shall we print it as `Quaternion( ... )`?\n        s = [self.parenthesize(i, PRECEDENCE[\"Mul\"], strict=True) for i in expr.args]\n        a = [s[0]] + [i+\" \"+j for i, j in zip(s[1:], \"ijk\")]\n        return \" + \".join(a)\n\n    def _print_QuotientRing(self, R):\n        # TODO nicer fractions for few generators...\n        return r\"\\frac{%s}{%s}\" % (self._print(R.ring), self._print(R.base_ideal))\n\n    def _print_QuotientRingElement(self, x):\n        return r\"{%s} + {%s}\" % (self._print(x.data), self._print(x.ring.base_ideal))\n\n    def _print_QuotientModuleElement(self, m):\n        return r\"{%s} + {%s}\" % (self._print(m.data),\n                                 self._print(m.module.killed_module))\n\n    def _print_QuotientModule(self, M):\n        # TODO nicer fractions for few generators...\n        return r\"\\frac{%s}{%s}\" % (self._print(M.base),\n                                   self._print(M.killed_module))\n\n    def _print_MatrixHomomorphism(self, h):\n        return r\"{%s} : {%s} \\to {%s}\" % (self._print(h._sympy_matrix()),\n            self._print(h.domain), self._print(h.codomain))\n\n    def _print_BaseScalarField(self, field):\n        string = field._coord_sys._names[field._index]\n        return r'\\boldsymbol{\\mathrm{%s}}' % self._print(Symbol(string))\n\n    def _print_BaseVectorField(self, field):\n        string = field._coord_sys._names[field._index]\n        return r'\\partial_{%s}' % self._print(Symbol(string))\n\n    def _print_Differential(self, diff):\n        field = diff._form_field\n        if hasattr(field, '_coord_sys'):\n            string = field._coord_sys._names[field._index]\n            return r'\\mathrm{d}%s' % self._print(Symbol(string))\n        else:\n            return 'd(%s)' % self._print(field)\n            string = self._print(field)\n            return r'\\mathrm{d}\\left(%s\\right)' % string\n\n    def _print_Tr(self, p):\n        #Todo: Handle indices\n        contents = self._print(p.args[0])\n        return r'\\mbox{Tr}\\left(%s\\right)' % (contents)\n\n    def _print_totient(self, expr, exp=None):\n        if exp is not None:\n            return r'\\left(\\phi\\left(%s\\right)\\right)^{%s}' % (self._print(expr.args[0]),\n                    self._print(exp))\n        return r'\\phi\\left(%s\\right)' % self._print(expr.args[0])\n\n    def _print_reduced_totient(self, expr, exp=None):\n        if exp is not None:\n            return r'\\left(\\lambda\\left(%s\\right)\\right)^{%s}' % (self._print(expr.args[0]),\n                    self._print(exp))\n        return r'\\lambda\\left(%s\\right)' % self._print(expr.args[0])\n\n    def _print_divisor_sigma(self, expr, exp=None):\n        if len(expr.args) == 2:\n            tex = r\"_%s\\left(%s\\right)\" % tuple(map(self._print,\n                                                (expr.args[1], expr.args[0])))\n        else:\n            tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"\\sigma^{%s}%s\" % (self._print(exp), tex)\n        return r\"\\sigma%s\" % tex\n\n    def _print_udivisor_sigma(self, expr, exp=None):\n        if len(expr.args) == 2:\n            tex = r\"_%s\\left(%s\\right)\" % tuple(map(self._print,\n                                                (expr.args[1], expr.args[0])))\n        else:\n            tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"\\sigma^*^{%s}%s\" % (self._print(exp), tex)\n        return r\"\\sigma^*%s\" % tex\n\n    def _print_primenu(self, expr, exp=None):\n        if exp is not None:\n            return r'\\left(\\nu\\left(%s\\right)\\right)^{%s}' % (self._print(expr.args[0]),\n                    self._print(exp))\n        return r'\\nu\\left(%s\\right)' % self._print(expr.args[0])\n\n    def _print_primeomega(self, expr, exp=None):\n        if exp is not None:\n            return r'\\left(\\Omega\\left(%s\\right)\\right)^{%s}' % (self._print(expr.args[0]),\n                    self._print(exp))\n        return r'\\Omega\\left(%s\\right)' % self._print(expr.args[0])\n\n\ndef translate(s):\n    r'''\n    Check for a modifier ending the string.  If present, convert the\n    modifier to latex and translate the rest recursively.\n\n    Given a description of a Greek letter or other special character,\n    return the appropriate latex.\n\n    Let everything else pass as given.\n\n    >>> from sympy.printing.latex import translate\n    >>> translate('alphahatdotprime')\n    \"{\\\\dot{\\\\hat{\\\\alpha}}}'\"\n    '''\n    # Process the rest\n    tex = tex_greek_dictionary.get(s)\n    if tex:\n        return tex\n    elif s.lower() in greek_letters_set:\n        return \"\\\\\" + s.lower()\n    elif s in other_symbols:\n        return \"\\\\\" + s\n    else:\n        # Process modifiers, if any, and recurse\n        for key in sorted(modifier_dict.keys(), key=lambda k:len(k), reverse=True):\n            if s.lower().endswith(key) and len(s)>len(key):\n                return modifier_dict[key](translate(s[:-len(key)]))\n        return s\n\n\ndef latex(expr, fold_frac_powers=False, fold_func_brackets=False,\n    fold_short_frac=None, inv_trig_style=\"abbreviated\",\n    itex=False, ln_notation=False, long_frac_ratio=None,\n    mat_delim=\"[\", mat_str=None, mode=\"plain\", mul_symbol=None,\n    order=None, symbol_names=None):\n    r\"\"\"Convert the given expression to LaTeX string representation.\n\n    Parameters\n    ==========\n    fold_frac_powers : boolean, optional\n        Emit ``^{p/q}`` instead of ``^{\\frac{p}{q}}`` for fractional powers.\n    fold_func_brackets : boolean, optional\n        Fold function brackets where applicable.\n    fold_short_frac : boolean, optional\n        Emit ``p / q`` instead of ``\\frac{p}{q}`` when the denominator is\n        simple enough (at most two terms and no powers). The default value is\n        ``True`` for inline mode, ``False`` otherwise.\n    inv_trig_style : string, optional\n        How inverse trig functions should be displayed. Can be one of\n        ``abbreviated``, ``full``, or ``power``. Defaults to ``abbreviated``.\n    itex : boolean, optional\n        Specifies if itex-specific syntax is used, including emitting\n        ``$$...$$``.\n    ln_notation : boolean, optional\n        If set to ``True``, ``\\ln`` is used instead of default ``\\log``.\n    long_frac_ratio : float or None, optional\n        The allowed ratio of the width of the numerator to the width of the\n        denominator before the printer breaks off long fractions. If ``None``\n        (the default value), long fractions are not broken up.\n    mat_delim : string, optional\n        The delimiter to wrap around matrices. Can be one of ``[``, ``(``, or\n        the empty string. Defaults to ``[``.\n    mat_str : string, optional\n        Which matrix environment string to emit. ``smallmatrix``, ``matrix``,\n        ``array``, etc. Defaults to ``smallmatrix`` for inline mode, ``matrix``\n        for matrices of no more than 10 columns, and ``array`` otherwise.\n    mode: string, optional\n        Specifies how the generated code will be delimited. ``mode`` can be one\n        of ``plain``, ``inline``, ``equation`` or ``equation*``.  If ``mode``\n        is set to ``plain``, then the resulting code will not be delimited at\n        all (this is the default). If ``mode`` is set to ``inline`` then inline\n        LaTeX ``$...$`` will be used. If ``mode`` is set to ``equation`` or\n        ``equation*``, the resulting code will be enclosed in the ``equation``\n        or ``equation*`` environment (remember to import ``amsmath`` for\n        ``equation*``), unless the ``itex`` option is set. In the latter case,\n        the ``$$...$$`` syntax is used.\n    mul_symbol : string or None, optional\n        The symbol to use for multiplication. Can be one of ``None``, ``ldot``,\n        ``dot``, or ``times``.\n    order: string, optional\n        Any of the supported monomial orderings (currently ``lex``, ``grlex``,\n        or ``grevlex``), ``old``, and ``none``. This parameter does nothing for\n        Mul objects. Setting order to ``old`` uses the compatibility ordering\n        for Add defined in Printer. For very large expressions, set the\n        ``order`` keyword to ``none`` if speed is a concern.\n    symbol_names : dictionary of strings mapped to symbols, optional\n        Dictionary of symbols and the custom strings they should be emitted as.\n\n    Notes\n    =====\n\n    Not using a print statement for printing, results in double backslashes for\n    latex commands since that's the way Python escapes backslashes in strings.\n\n    >>> from sympy import latex, Rational\n    >>> from sympy.abc import tau\n    >>> latex((2*tau)**Rational(7,2))\n    '8 \\\\sqrt{2} \\\\tau^{\\\\frac{7}{2}}'\n    >>> print(latex((2*tau)**Rational(7,2)))\n    8 \\sqrt{2} \\tau^{\\frac{7}{2}}\n\n    Examples\n    ========\n\n    >>> from sympy import latex, pi, sin, asin, Integral, Matrix, Rational, log\n    >>> from sympy.abc import x, y, mu, r, tau\n\n    Basic usage:\n\n    >>> print(latex((2*tau)**Rational(7,2)))\n    8 \\sqrt{2} \\tau^{\\frac{7}{2}}\n\n    ``mode`` and ``itex`` options:\n\n    >>> print(latex((2*mu)**Rational(7,2), mode='plain'))\n    8 \\sqrt{2} \\mu^{\\frac{7}{2}}\n    >>> print(latex((2*tau)**Rational(7,2), mode='inline'))\n    $8 \\sqrt{2} \\tau^{7 / 2}$\n    >>> print(latex((2*mu)**Rational(7,2), mode='equation*'))\n    \\begin{equation*}8 \\sqrt{2} \\mu^{\\frac{7}{2}}\\end{equation*}\n    >>> print(latex((2*mu)**Rational(7,2), mode='equation'))\n    \\begin{equation}8 \\sqrt{2} \\mu^{\\frac{7}{2}}\\end{equation}\n    >>> print(latex((2*mu)**Rational(7,2), mode='equation', itex=True))\n    $$8 \\sqrt{2} \\mu^{\\frac{7}{2}}$$\n    >>> print(latex((2*mu)**Rational(7,2), mode='plain'))\n    8 \\sqrt{2} \\mu^{\\frac{7}{2}}\n    >>> print(latex((2*tau)**Rational(7,2), mode='inline'))\n    $8 \\sqrt{2} \\tau^{7 / 2}$\n    >>> print(latex((2*mu)**Rational(7,2), mode='equation*'))\n(D)         if expr == expr.zero:\n            return expr.zero._latex_form\n        if isinstance(expr, Vector):\n            items = expr.separate().items()\n        else:\n            items = [(0, expr)]\n\n        for system, vect in items:\n            inneritems = list(vect.components.items())\n            inneritems.sort(key = lambda x:x[0].__str__())\n            for k, v in inneritems:\n                if v == 1:\n                    o1.append(' + ' + k._latex_form)\n                elif v == -1:\n                    o1.append(' - ' + k._latex_form)\n                else:\n                    arg_str = '(' + LatexPrinter().doprint(v) + ')'\n                    o1.append(' + ' + arg_str + k._latex_form)\n\n        outstr = (''.join(o1))\n        if outstr[1] != '-':\n            outstr = outstr[3:]\n        else:\n            outstr = outstr[1:]\n        return outstr\n\n    def _print_Indexed(self, expr):\n        tex_base = self._print(expr.base)\n        tex = '{'+tex_base+'}'+'_{%s}' % ','.join(\n            map(self._print, expr.indices))\n        return tex\n\n    def _print_IndexedBase(self, expr):\n        return self._print(expr.label)\n\n    def _print_Derivative(self, expr):\n        if requires_partial(expr):\n            diff_symbol = r'\\partial'\n        else:\n            diff_symbol = r'd'\n\n        tex = \"\"\n        dim = 0\n        for x, num in reversed(expr.variable_count):\n            dim += num\n            if num == 1:\n                tex += r\"%s %s\" % (diff_symbol, self._print(x))\n            else:\n                tex += r\"%s %s^{%s}\" % (diff_symbol, self._print(x), num)\n\n        if dim == 1:\n            tex = r\"\\frac{%s}{%s}\" % (diff_symbol, tex)\n        else:\n            tex = r\"\\frac{%s^{%s}}{%s}\" % (diff_symbol, dim, tex)\n\n        return r\"%s %s\" % (tex, self.parenthesize(expr.expr, PRECEDENCE[\"Mul\"], strict=True))\n\n    def _print_Subs(self, subs):\n        expr, old, new = subs.args\n        latex_expr = self._print(expr)\n        latex_old = (self._print(e) for e in old)\n        latex_new = (self._print(e) for e in new)\n        latex_subs = r'\\\\ '.join(\n            e[0] + '=' + e[1] for e in zip(latex_old, latex_new))\n        return r'\\left. %s \\right|_{\\substack{ %s }}' % (latex_expr, latex_subs)\n\n    def _print_Integral(self, expr):\n        tex, symbols = \"\", []\n\n        # Only up to \\iiiint exists\n        if len(expr.limits) <= 4 and all(len(lim) == 1 for lim in expr.limits):\n            # Use len(expr.limits)-1 so that syntax highlighters don't think\n            # \\\" is an escaped quote\n            tex = r\"\\i\" + \"i\"*(len(expr.limits) - 1) + \"nt\"\n            symbols = [r\"\\, d%s\" % self._print(symbol[0])\n                       for symbol in expr.limits]\n\n        else:\n            for lim in reversed(expr.limits):\n                symbol = lim[0]\n                tex += r\"\\int\"\n\n                if len(lim) > 1:\n                    if self._settings['mode'] in ['equation', 'equation*'] \\\n                            and not self._settings['itex']:\n                        tex += r\"\\limits\"\n\n                    if len(lim) == 3:\n                        tex += \"_{%s}^{%s}\" % (self._print(lim[1]),\n                                               self._print(lim[2]))\n                    if len(lim) == 2:\n                        tex += \"^{%s}\" % (self._print(lim[1]))\n\n                symbols.insert(0, r\"\\, d%s\" % self._print(symbol))\n\n        return r\"%s %s%s\" % (tex,\n            self.parenthesize(expr.function, PRECEDENCE[\"Mul\"], strict=True), \"\".join(symbols))\n\n    def _print_Limit(self, expr):\n        e, z, z0, dir = expr.args\n\n        tex = r\"\\lim_{%s \\to \" % self._print(z)\n        if str(dir) == '+-' or z0 in (S.Infinity, S.NegativeInfinity):\n            tex += r\"%s}\" % self._print(z0)\n        else:\n            tex += r\"%s^%s}\" % (self._print(z0), self._print(dir))\n\n        if isinstance(e, AssocOp):\n            return r\"%s\\left(%s\\right)\" % (tex, self._print(e))\n        else:\n            return r\"%s %s\" % (tex, self._print(e))\n\n    def _hprint_Function(self, func):\n        r'''\n        Logic to decide how to render a function to latex\n          - if it is a recognized latex name, use the appropriate latex command\n          - if it is a single letter, just use that letter\n          - if it is a longer name, then put \\operatorname{} around it and be\n            mindful of undercores in the name\n        '''\n        func = self._deal_with_super_sub(func)\n        if func in accepted_latex_functions:\n            name = r\"\\%s\" % func\n        elif len(func) == 1 or func.startswith('\\\\'):\n            name = func\n        else:\n            name = r\"\\operatorname{%s}\" % func\n        return name\n\n    def _print_Function(self, expr, exp=None):\n        r'''\n        Render functions to LaTeX, handling functions that LaTeX knows about\n        e.g., sin, cos, ... by using the proper LaTeX command (\\sin, \\cos, ...).\n        For single-letter function names, render them as regular LaTeX math\n        symbols. For multi-letter function names that LaTeX does not know\n        about, (e.g., Li, sech) use \\operatorname{} so that the function name\n        is rendered in Roman font and LaTeX handles spacing properly.\n\n        expr is the expression involving the function\n        exp is an exponent\n        '''\n        func = expr.func.__name__\n        if hasattr(self, '_print_' + func) and \\\n            not isinstance(expr.func, UndefinedFunction):\n            return getattr(self, '_print_' + func)(expr, exp)\n        else:\n            args = [ str(self._print(arg)) for arg in expr.args ]\n            # How inverse trig functions should be displayed, formats are:\n            # abbreviated: asin, full: arcsin, power: sin^-1\n            inv_trig_style = self._settings['inv_trig_style']\n            # If we are dealing with a power-style inverse trig function\n            inv_trig_power_case = False\n            # If it is applicable to fold the argument brackets\n            can_fold_brackets = self._settings['fold_func_brackets'] and \\\n                len(args) == 1 and \\\n                not self._needs_function_brackets(expr.args[0])\n\n            inv_trig_table = [\"asin\", \"acos\", \"atan\", \"acsc\", \"asec\", \"acot\"]\n\n            # If the function is an inverse trig function, handle the style\n            if func in inv_trig_table:\n                if inv_trig_style == \"abbreviated\":\n                    func = func\n                elif inv_trig_style == \"full\":\n                    func = \"arc\" + func[1:]\n                elif inv_trig_style == \"power\":\n                    func = func[1:]\n                    inv_trig_power_case = True\n\n                    # Can never fold brackets if we're raised to a power\n                    if exp is not None:\n                        can_fold_brackets = False\n\n            if inv_trig_power_case:\n                if func in accepted_latex_functions:\n                    name = r\"\\%s^{-1}\" % func\n                else:\n                    name = r\"\\operatorname{%s}^{-1}\" % func\n            elif exp is not None:\n                name = r'%s^{%s}' % (self._hprint_Function(func), exp)\n            else:\n                name = self._hprint_Function(func)\n\n            if can_fold_brackets:\n                if func in accepted_latex_functions:\n                    # Wrap argument safely to avoid parse-time conflicts\n                    # with the function name itself\n                    name += r\" {%s}\"\n                else:\n                    name += r\"%s\"\n            else:\n                name += r\"{\\left (%s \\right )}\"\n\n            if inv_trig_power_case and exp is not None:\n                name += r\"^{%s}\" % exp\n\n            return name % \",\".join(args)\n\n    def _print_UndefinedFunction(self, expr):\n        return self._hprint_Function(str(expr))\n\n    @property\n    def _special_function_classes(self):\n        from sympy.functions.special.tensor_functions import KroneckerDelta\n        from sympy.functions.special.gamma_functions import gamma, lowergamma\n        from sympy.functions.special.beta_functions import beta\n        from sympy.functions.special.delta_functions import DiracDelta\n        from sympy.functions.special.error_functions import Chi\n        return {KroneckerDelta: r'\\delta',\n                gamma:  r'\\Gamma',\n                lowergamma: r'\\gamma',\n                beta: r'\\operatorname{B}',\n                DiracDelta: r'\\delta',\n                Chi: r'\\operatorname{Chi}'}\n\n    def _print_FunctionClass(self, expr):\n        for cls in self._special_function_classes:\n            if issubclass(expr, cls) and expr.__name__ == cls.__name__:\n                return self._special_function_classes[cls]\n        return self._hprint_Function(str(expr))\n\n    def _print_Lambda(self, expr):\n        symbols, expr = expr.args\n\n        if len(symbols) == 1:\n            symbols = self._print(symbols[0])\n        else:\n            symbols = self._print(tuple(symbols))\n\n        args = (symbols, self._print(expr))\n        tex = r\"\\left( %s \\mapsto %s \\right)\" % (symbols, self._print(expr))\n\n        return tex\n\n    def _hprint_variadic_function(self, expr, exp=None):\n        args = sorted(expr.args, key=default_sort_key)\n        texargs = [r\"%s\" % self._print(symbol) for symbol in args]\n        tex = r\"\\%s\\left(%s\\right)\" % (self._print((str(expr.func)).lower()), \", \".join(texargs))\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    _print_Min = _print_Max = _hprint_variadic_function\n\n    def _print_floor(self, expr, exp=None):\n        tex = r\"\\lfloor{%s}\\rfloor\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_ceiling(self, expr, exp=None):\n        tex = r\"\\lceil{%s}\\rceil\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_log(self, expr, exp=None):\n        if not self._settings[\"ln_notation\"]:\n            tex = r\"\\log{\\left (%s \\right )}\" % self._print(expr.args[0])\n        else:\n            tex = r\"\\ln{\\left (%s \\right )}\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_Abs(self, expr, exp=None):\n        tex = r\"\\left|{%s}\\right|\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n    _print_Determinant = _print_Abs\n\n    def _print_re(self, expr, exp=None):\n        tex = r\"\\Re{%s}\" % self.parenthesize(expr.args[0], PRECEDENCE['Atom'])\n\n        return self._do_exponent(tex, exp)\n\n    def _print_im(self, expr, exp=None):\n        tex = r\"\\Im{%s}\" % self.parenthesize(expr.args[0], PRECEDENCE['Func'])\n\n        return self._do_exponent(tex, exp)\n\n    def _print_Not(self, e):\n        from sympy import Equivalent, Implies\n        if isinstance(e.args[0], Equivalent):\n            return self._print_Equivalent(e.args[0], r\"\\not\\Leftrightarrow\")\n        if isinstance(e.args[0], Implies):\n            return self._print_Implies(e.args[0], r\"\\not\\Rightarrow\")\n        if (e.args[0].is_Boolean):\n            return r\"\\neg (%s)\" % self._print(e.args[0])\n        else:\n            return r\"\\neg %s\" % self._print(e.args[0])\n\n    def _print_LogOp(self, args, char):\n        arg = args[0]\n        if arg.is_Boolean and not arg.is_Not:\n            tex = r\"\\left(%s\\right)\" % self._print(arg)\n        else:\n            tex = r\"%s\" % self._print(arg)\n\n        for arg in args[1:]:\n            if arg.is_Boolean and not arg.is_Not:\n                tex += r\" %s \\left(%s\\right)\" % (char, self._print(arg))\n            else:\n                tex += r\" %s %s\" % (char, self._print(arg))\n\n        return tex\n\n    def _print_And(self, e):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, r\"\\wedge\")\n\n    def _print_Or(self, e):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, r\"\\vee\")\n\n    def _print_Xor(self, e):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, r\"\\veebar\")\n\n    def _print_Implies(self, e, altchar=None):\n        return self._print_LogOp(e.args, altchar or r\"\\Rightarrow\")\n\n    def _print_Equivalent(self, e, altchar=None):\n        args = sorted(e.args, key=default_sort_key)\n        return self._print_LogOp(args, altchar or r\"\\Leftrightarrow\")\n\n    def _print_conjugate(self, expr, exp=None):\n        tex = r\"\\overline{%s}\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_polar_lift(self, expr, exp=None):\n        func = r\"\\operatorname{polar\\_lift}\"\n        arg = r\"{\\left (%s \\right )}\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}%s\" % (func, exp, arg)\n        else:\n            return r\"%s%s\" % (func, arg)\n\n    def _print_ExpBase(self, expr, exp=None):\n        # TODO should exp_polar be printed differently?\n        #      what about exp_polar(0), exp_polar(1)?\n        tex = r\"e^{%s}\" % self._print(expr.args[0])\n        return self._do_exponent(tex, exp)\n\n    def _print_elliptic_k(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"K^{%s}%s\" % (exp, tex)\n        else:\n            return r\"K%s\" % tex\n\n    def _print_elliptic_f(self, expr, exp=None):\n        tex = r\"\\left(%s\\middle| %s\\right)\" % \\\n            (self._print(expr.args[0]), self._print(expr.args[1]))\n        if exp is not None:\n            return r\"F^{%s}%s\" % (exp, tex)\n        else:\n            return r\"F%s\" % tex\n\n    def _print_elliptic_e(self, expr, exp=None):\n        if len(expr.args) == 2:\n            tex = r\"\\left(%s\\middle| %s\\right)\" % \\\n                (self._print(expr.args[0]), self._print(expr.args[1]))\n        else:\n            tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"E^{%s}%s\" % (exp, tex)\n        else:\n            return r\"E%s\" % tex\n\n    def _print_elliptic_pi(self, expr, exp=None):\n        if len(expr.args) == 3:\n            tex = r\"\\left(%s; %s\\middle| %s\\right)\" % \\\n                (self._print(expr.args[0]), self._print(expr.args[1]), \\\n                 self._print(expr.args[2]))\n        else:\n            tex = r\"\\left(%s\\middle| %s\\right)\" % \\\n                (self._print(expr.args[0]), self._print(expr.args[1]))\n        if exp is not None:\n            return r\"\\Pi^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\Pi%s\" % tex\n\n    def _print_beta(self, expr, exp=None):\n        tex = r\"\\left(%s, %s\\right)\" % (self._print(expr.args[0]),\n                                        self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"\\operatorname{B}^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\operatorname{B}%s\" % tex\n\n    def _print_uppergamma(self, expr, exp=None):\n        tex = r\"\\left(%s, %s\\right)\" % (self._print(expr.args[0]),\n                                        self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"\\Gamma^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\Gamma%s\" % tex\n\n    def _print_lowergamma(self, expr, exp=None):\n        tex = r\"\\left(%s, %s\\right)\" % (self._print(expr.args[0]),\n                                        self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"\\gamma^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\gamma%s\" % tex\n\n    def _hprint_one_arg_func(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}%s\" % (self._print(expr.func), exp, tex)\n        else:\n            return r\"%s%s\" % (self._print(expr.func), tex)\n\n    _print_gamma = _hprint_one_arg_func\n\n    def _print_Chi(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"\\operatorname{Chi}^{%s}%s\" % (exp, tex)\n        else:\n            return r\"\\operatorname{Chi}%s\" % tex\n\n    def _print_expint(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[1])\n        nu = self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"\\operatorname{E}_{%s}^{%s}%s\" % (nu, exp, tex)\n        else:\n            return r\"\\operatorname{E}_{%s}%s\" % (nu, tex)\n\n    def _print_fresnels(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"S^{%s}%s\" % (exp, tex)\n        else:\n            return r\"S%s\" % tex\n\n    def _print_fresnelc(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"C^{%s}%s\" % (exp, tex)\n        else:\n            return r\"C%s\" % tex\n\n    def _print_subfactorial(self, expr, exp=None):\n        tex = r\"!%s\" % self.parenthesize(expr.args[0], PRECEDENCE[\"Func\"])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_factorial(self, expr, exp=None):\n        tex = r\"%s!\" % self.parenthesize(expr.args[0], PRECEDENCE[\"Func\"])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_factorial2(self, expr, exp=None):\n        tex = r\"%s!!\" % self.parenthesize(expr.args[0], PRECEDENCE[\"Func\"])\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_binomial(self, expr, exp=None):\n        tex = r\"{\\binom{%s}{%s}}\" % (self._print(expr.args[0]),\n                                     self._print(expr.args[1]))\n\n        if exp is not None:\n            return r\"%s^{%s}\" % (tex, exp)\n        else:\n            return tex\n\n    def _print_RisingFactorial(self, expr, exp=None):\n        n, k = expr.args\n        base = r\"%s\" % self.parenthesize(n, PRECEDENCE['Func'])\n\n        tex = r\"{%s}^{\\left(%s\\right)}\" % (base, self._print(k))\n\n        return self._do_exponent(tex, exp)\n\n    def _print_FallingFactorial(self, expr, exp=None):\n        n, k = expr.args\n        sub = r\"%s\" % self.parenthesize(k, PRECEDENCE['Func'])\n\n        tex = r\"{\\left(%s\\right)}_{%s}\" % (self._print(n), sub)\n\n        return self._do_exponent(tex, exp)\n\n    def _hprint_BesselBase(self, expr, exp, sym):\n        tex = r\"%s\" % (sym)\n\n        need_exp = False\n        if exp is not None:\n            if tex.find('^') == -1:\n                tex = r\"%s^{%s}\" % (tex, self._print(exp))\n            else:\n                need_exp = True\n\n        tex = r\"%s_{%s}\\left(%s\\right)\" % (tex, self._print(expr.order),\n                                           self._print(expr.argument))\n\n        if need_exp:\n            tex = self._do_exponent(tex, exp)\n        return tex\n\n    def _hprint_vec(self, vec):\n        if len(vec) == 0:\n            return \"\"\n        s = \"\"\n        for i in vec[:-1]:\n            s += \"%s, \" % self._print(i)\n        s += self._print(vec[-1])\n        return s\n\n    def _print_besselj(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'J')\n\n    def _print_besseli(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'I')\n\n    def _print_besselk(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'K')\n\n    def _print_bessely(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'Y')\n\n    def _print_yn(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'y')\n\n    def _print_jn(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'j')\n\n    def _print_hankel1(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'H^{(1)}')\n\n    def _print_hankel2(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'H^{(2)}')\n\n    def _print_hn1(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'h^{(1)}')\n\n    def _print_hn2(self, expr, exp=None):\n        return self._hprint_BesselBase(expr, exp, 'h^{(2)}')\n\n    def _hprint_airy(self, expr, exp=None, notation=\"\"):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"%s^{%s}%s\" % (notation, exp, tex)\n        else:\n            return r\"%s%s\" % (notation, tex)\n\n    def _hprint_airy_prime(self, expr, exp=None, notation=\"\"):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n\n        if exp is not None:\n            return r\"{%s^\\prime}^{%s}%s\" % (notation, exp, tex)\n        else:\n            return r\"%s^\\prime%s\" % (notation, tex)\n\n    def _print_airyai(self, expr, exp=None):\n        return self._hprint_airy(expr, exp, 'Ai')\n\n    def _print_airybi(self, expr, exp=None):\n        return self._hprint_airy(expr, exp, 'Bi')\n\n    def _print_airyaiprime(self, expr, exp=None):\n        return self._hprint_airy_prime(expr, exp, 'Ai')\n\n    def _print_airybiprime(self, expr, exp=None):\n        return self._hprint_airy_prime(expr, exp, 'Bi')\n\n    def _print_hyper(self, expr, exp=None):\n        tex = r\"{{}_{%s}F_{%s}\\left(\\begin{matrix} %s \\\\ %s \\end{matrix}\" \\\n              r\"\\middle| {%s} \\right)}\" % \\\n            (self._print(len(expr.ap)), self._print(len(expr.bq)),\n              self._hprint_vec(expr.ap), self._hprint_vec(expr.bq),\n              self._print(expr.argument))\n\n        if exp is not None:\n            tex = r\"{%s}^{%s}\" % (tex, self._print(exp))\n        return tex\n\n    def _print_meijerg(self, expr, exp=None):\n        tex = r\"{G_{%s, %s}^{%s, %s}\\left(\\begin{matrix} %s & %s \\\\\" \\\n              r\"%s & %s \\end{matrix} \\middle| {%s} \\right)}\" % \\\n            (self._print(len(expr.ap)), self._print(len(expr.bq)),\n              self._print(len(expr.bm)), self._print(len(expr.an)),\n              self._hprint_vec(expr.an), self._hprint_vec(expr.aother),\n              self._hprint_vec(expr.bm), self._hprint_vec(expr.bother),\n              self._print(expr.argument))\n\n        if exp is not None:\n            tex = r\"{%s}^{%s}\" % (tex, self._print(exp))\n        return tex\n\n    def _print_dirichlet_eta(self, expr, exp=None):\n        tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"\\eta^{%s}%s\" % (self._print(exp), tex)\n        return r\"\\eta%s\" % tex\n\n    def _print_zeta(self, expr, exp=None):\n        if len(expr.args) == 2:\n            tex = r\"\\left(%s, %s\\right)\" % tuple(map(self._print, expr.args))\n        else:\n            tex = r\"\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp is not None:\n            return r\"\\zeta^{%s}%s\" % (self._print(exp), tex)\n        return r\"\\zeta%s\" % tex\n\n    def _print_lerchphi(self, expr, exp=None):\n        tex = r\"\\left(%s, %s, %s\\right)\" % tuple(map(self._print, expr.args))\n        if exp is None:\n            return r\"\\Phi%s\" % tex\n        return r\"\\Phi^{%s}%s\" % (self._print(exp), tex)\n\n    def _print_polylog(self, expr, exp=None):\n        s, z = map(self._print, expr.args)\n        tex = r\"\\left(%s\\right)\" % z\n        if exp is None:\n            return r\"\\operatorname{Li}_{%s}%s\" % (s, tex)\n        return r\"\\operatorname{Li}_{%s}^{%s}%s\" % (s, self._print(exp), tex)\n\n    def _print_jacobi(self, expr, exp=None):\n        n, a, b, x = map(self._print, expr.args)\n        tex = r\"P_{%s}^{\\left(%s,%s\\right)}\\left(%s\\right)\" % (n, a, b, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_gegenbauer(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"C_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_chebyshevt(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"T_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_chebyshevu(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"U_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_legendre(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"P_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_assoc_legendre(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"P_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_hermite(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"H_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_laguerre(self, expr, exp=None):\n        n, x = map(self._print, expr.args)\n        tex = r\"L_{%s}\\left(%s\\right)\" % (n, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_assoc_laguerre(self, expr, exp=None):\n        n, a, x = map(self._print, expr.args)\n        tex = r\"L_{%s}^{\\left(%s\\right)}\\left(%s\\right)\" % (n, a, x)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Ynm(self, expr, exp=None):\n        n, m, theta, phi = map(self._print, expr.args)\n        tex = r\"Y_{%s}^{%s}\\left(%s,%s\\right)\" % (n, m, theta, phi)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Znm(self, expr, exp=None):\n        n, m, theta, phi = map(self._print, expr.args)\n        tex = r\"Z_{%s}^{%s}\\left(%s,%s\\right)\" % (n, m, theta, phi)\n        if exp is not None:\n            tex = r\"\\left(\" + tex + r\"\\right)^{%s}\" % (self._print(exp))\n        return tex\n\n    def _print_Rational(self, expr):\n        if expr.q != 1:\n            sign = \"\"\n            p = expr.p\n            if expr.p < 0:\n                sign = \"- \"\n                p = -p\n            if self._settings['fold_short_frac']:\n                return r\"%s%d / %d\" % (sign, p, expr.q)\n            return r\"%s\\frac{%d}{%d}\" % (sign, p, expr.q)\n        else:\n            return self._print(expr.p)\n\n    def _print_Order(self, expr):\n        s = self._print(expr.expr)\n        if expr.point and any(p != S.Zero for p in expr.point) or \\\n           len(expr.variables) > 1:\n            s += '; '\n            if len(expr.variables) > 1:\n                s += self._print(expr.variables)\n            elif len(expr.variables):\n                s += self._print(expr.variables[0])\n            s += r'\\rightarrow '\n            if len(expr.point) > 1:\n                s += self._print(expr.point)\n            else:\n                s += self._print(expr.point[0])\n        return r\"O\\left(%s\\right)\" % s\n\n    def _print_Symbol(self, expr):\n        if expr in self._settings['symbol_names']:\n            return self._settings['symbol_names'][expr]\n\n        return self._deal_with_super_sub(expr.name) if \\\n            '\\\\' not in expr.name else expr.name\n\n    _print_RandomSymbol = _print_Symbol\n    _print_MatrixSymbol = _print_Symbol\n\n    def _deal_with_super_sub(self, string):\n        if '{' in string:\n            return string\n\n        name, supers, subs = split_super_sub(string)\n\n        name = translate(name)\n        supers = [translate(sup) for sup in supers]\n        subs = [translate(sub) for sub in subs]\n\n        # glue all items together:\n        if len(supers) > 0:\n            name += \"^{%s}\" % \" \".join(supers)\n        if len(subs) > 0:\n            name += \"_{%s}\" % \" \".join(subs)\n\n        return name\n\n    def _print_Relational(self, expr):\n        if self._settings['itex']:\n            gt = r\"\\gt\"\n            lt = r\"\\lt\"\n        else:\n            gt = \">\"\n            lt = \"<\"\n\n        charmap = {\n            \"==\": \"=\",\n            \">\": gt,\n            \"<\": lt,\n            \">=\": r\"\\geq\",\n            \"<=\": r\"\\leq\",\n            \"!=\": r\"\\neq\",\n        }\n\n        return \"%s %s %s\" % (self._print(expr.lhs),\n            charmap[expr.rel_op], self._print(expr.rhs))\n\n    def _print_Piecewise(self, expr):\n        ecpairs = [r\"%s & \\text{for}\\: %s\" % (self._print(e), self._print(c))\n                   for e, c in expr.args[:-1]]\n        if expr.args[-1].cond == true:\n            ecpairs.append(r\"%s & \\text{otherwise}\" %\n                           self._print(expr.args[-1].expr))\n        else:\n            ecpairs.append(r\"%s & \\text{for}\\: %s\" %\n                           (self._print(expr.args[-1].expr),\n                            self._print(expr.args[-1].cond)))\n        tex = r\"\\begin{cases} %s \\end{cases}\"\n        return tex % r\" \\\\\".join(ecpairs)\n\n    def _print_MatrixBase(self, expr):\n        lines = []\n\n        for line in range(expr.rows):  # horrible, should be 'rows'\n            lines.append(\" & \".join([ self._print(i) for i in expr[line, :] ]))\n\n        mat_str = self._settings['mat_str']\n        if mat_str is None:\n            if self._settings['mode'] == 'inline':\n                mat_str = 'smallmatrix'\n            else:\n                if (expr.cols <= 10) is True:\n                    mat_str = 'matrix'\n                else:\n                    mat_str = 'array'\n\n        out_str = r'\\begin{%MATSTR%}%s\\end{%MATSTR%}'\n        out_str = out_str.replace('%MATSTR%', mat_str)\n        if mat_str == 'array':\n            out_str = out_str.replace('%s', '{' + 'c'*expr.cols + '}%s')\n        if self._settings['mat_delim']:\n            left_delim = self._settings['mat_delim']\n            right_delim = self._delim_dict[left_delim]\n            out_str = r'\\left' + left_delim + out_str + \\\n                      r'\\right' + right_delim\n        return out_str % r\"\\\\\".join(lines)\n    _print_ImmutableMatrix = _print_ImmutableDenseMatrix \\\n                           = _print_Matrix \\\n                           = _print_MatrixBase\n\n    def _print_MatrixElement(self, expr):\n        return self.parenthesize(expr.parent, PRECEDENCE[\"Atom\"], strict=True) \\\n            + '_{%s, %s}' % (expr.i, expr.j)\n\n    def _print_MatrixSlice(self, expr):\n        def latexslice(x):\n            x = list(x)\n            if x[2] == 1:\n                del x[2]\n            if x[1] == x[0] + 1:\n                del x[1]\n            if x[0] == 0:\n                x[0] = ''\n            return ':'.join(map(self._print, x))\n        return (self._print(expr.parent) + r'\\left[' +\n                latexslice(expr.rowslice) + ', ' +\n                latexslice(expr.colslice) + r'\\right]')\n\n    def _print_BlockMatrix(self, expr):\n        return self._print(expr.blocks)\n\n    def _print_Transpose(self, expr):\n        mat = expr.arg\n        from sympy.matrices import MatrixSymbol\n        if not isinstance(mat, MatrixSymbol):\n            return r\"\\left(%s\\right)^T\" % self._print(mat)\n        else:\n            return \"%s^T\" % self._print(mat)\n\n    def _print_Adjoint(self, expr):\n        mat = expr.arg\n        from sympy.matrices import MatrixSymbol\n        if not isinstance(mat, MatrixSymbol):\n            return r\"\\left(%s\\right)^\\dagger\" % self._print(mat)\n        else:\n            return r\"%s^\\dagger\" % self._print(mat)\n\n    def _print_MatAdd(self, expr):\n        terms = [self._print(t) for t in expr.args]\n        l = []\n        for t in terms:\n            if t.startswith('-'):\n                sign = \"-\"\n                t = t[1:]\n            else:\n                sign = \"+\"\n            l.extend([sign, t])\n        sign = l.pop(0)\n        if sign == '+':\n            sign = \"\"\n        return sign + ' '.join(l)\n\n    def _print_MatMul(self, expr):\n        from sympy import Add, MatAdd, HadamardProduct, MatMul, Mul\n\n        def parens(x):\n            if isinstance(x, (Add, MatAdd, HadamardProduct)):\n                return r\"\\left(%s\\right)\" % self._print(x)\n            return self._print(x)\n\n        if isinstance(expr, MatMul) and expr.args[0].is_Number and expr.args[0]<0:\n            expr = Mul(-1*expr.args[0], MatMul(*expr.args[1:]))\n            return '-' + ' '.join(map(parens, expr.args))\n        else:\n            return ' '.join(map(parens, expr.args))\n\n    def _print_Mod(self, expr, exp=None):\n        if exp is not None:\n            return r'\\left(%s\\bmod{%s}\\right)^{%s}' % (self.parenthesize(expr.args[0],\n                    PRECEDENCE['Mul'], strict=True), self._print(expr.args[1]), self._print(exp))\n        return r'%s\\bmod{%s}' % (self.parenthesize(expr.args[0],\n                PRECEDENCE['Mul'], strict=True), self._print(expr.args[1]))\n\n    def _print_HadamardProduct(self, expr):\n        from sympy import Add, MatAdd, MatMul\n\n        def parens(x):\n            if isinstance(x, (Add, MatAdd, MatMul)):\n                return r\"\\left(%s\\right)\" % self._print(x)\n            return self._print(x)\n        return r' \\circ '.join(map(parens, expr.args))\n\n    def _print_KroneckerProduct(self, expr):\n        from sympy import Add, MatAdd, MatMul\n\n        def parens(x):\n            if isinstance(x, (Add, MatAdd, MatMul)):\n                return r\"\\left(%s\\right)\" % self._print(x)\n            return self._print(x)\n        return r' \\otimes '.join(map(parens, expr.args))\n\n    def _print_MatPow(self, expr):\n        base, exp = expr.base, expr.exp\n        from sympy.matrices import MatrixSymbol\n        if not isinstance(base, MatrixSymbol):\n            return r\"\\left(%s\\right)^{%s}\" % (self._print(base), self._print(exp))\n        else:\n            return \"%s^{%s}\" % (self._print(base), self._print(exp))\n\n    def _print_ZeroMatrix(self, Z):\n        return r\"\\mathbb{0}\"\n\n    def _print_Identity(self, I):\n        return r\"\\mathbb{I}\"\n\n    def _print_NDimArray(self, expr):\n\n        if expr.rank() == 0:\n            return self._print(expr[()])\n\n        mat_str = self._settings['mat_str']\n        if mat_str is None:\n            if self._settings['mode'] == 'inline':\n                mat_str = 'smallmatrix'\n            else:\n                if (expr.rank() == 0) or (expr.shape[-1] <= 10):\n                    mat_str = 'matrix'\n                else:\n                    mat_str = 'array'\n        block_str = r'\\begin{%MATSTR%}%s\\end{%MATSTR%}'\n        block_str = block_str.replace('%MATSTR%', mat_str)\n        if self._settings['mat_delim']:\n            left_delim = self._settings['mat_delim']\n            right_delim = self._delim_dict[left_delim]\n            block_str = r'\\left' + left_delim + block_str + \\\n                      r'\\right' + right_delim\n\n        if expr.rank() == 0:\n            return block_str % \"\"\n\n        level_str = [[]] + [[] for i in range(expr.rank())]\n        shape_ranges = [list(range(i)) for i in expr.shape]\n        for outer_i in itertools.product(*shape_ranges):\n            level_str[-1].append(self._print(expr[outer_i]))\n            even = True\n            for back_outer_i in range(expr.rank()-1, -1, -1):\n                if len(level_str[back_outer_i+1]) < expr.shape[back_outer_i]:\n                    break\n                if even:\n                    level_str[back_outer_i].append(r\" & \".join(level_str[back_outer_i+1]))\n                else:\n                    level_str[back_outer_i].append(block_str % (r\"\\\\\".join(level_str[back_outer_i+1])))\n                    if len(level_str[back_outer_i+1]) == 1:\n                        level_str[back_outer_i][-1] = r\"\\left[\" + level_str[back_outer_i][-1] + r\"\\right]\"\n                even = not even\n                level_str[back_outer_i+1] = []\n\n        out_str = level_str[0][0]\n\n        if expr.rank() % 2 == 1:\n            out_str = block_str % out_str\n\n        return out_str\n\n    _print_ImmutableDenseNDimArray = _print_NDimArray\n    _print_ImmutableSparseNDimArray = _print_NDimArray\n    _print_MutableDenseNDimArray = _print_NDimArray\n    _print_MutableSparseNDimArray = _print_NDimArray\n\n    def _printer_tensor_indices(self, name, indices, index_map={}):\n        out_str = self._print(name)\n        last_valence = None\n        prev_map = None\n        for index in indices:\n            new_valence = index.is_up\n            if ((index in index_map) or prev_map) and last_valence == new_valence:\n                out_str += \",\"\n            if last_valence != new_valence:\n                if last_valence is not None:\n                    out_str += \"}\"\n                if index.is_up:\n                    out_str += \"{}^{\"\n                else:\n                    out_str += \"{}_{\"\n            out_str += self._print(index.args[0])\n            if index in index_map:\n                out_str += \"=\"\n                out_str += self._print(index_map[index])\n                prev_map = True\n            else:\n                prev_map = False\n            last_valence = new_valence\n        if last_valence is not None:\n            out_str += \"}\"\n        return out_str\n\n    def _print_Tensor(self, expr):\n        name = expr.args[0].args[0]\n        indices = expr.get_indices()\n        return self._printer_tensor_indices(name, indices)\n\n    def _print_TensorElement(self, expr):\n        name = expr.expr.args[0].args[0]\n        indices = expr.expr.get_indices()\n        index_map = expr.index_map\n        return self._printer_tensor_indices(name, indices, index_map)\n\n    def _print_TensMul(self, expr):\n        # prints expressions like \"A(a)\", \"3*A(a)\", \"(1+x)*A(a)\"\n        sign, args = expr._get_args_for_traditional_printer()\n        return sign + \"\".join(\n            [self.parenthesize(arg, precedence(expr)) for arg in args]\n        )\n\n    def _print_TensAdd(self, expr):\n        a = []\n        args = expr.args\n        for x in args:\n            a.append(self.parenthesize(x, precedence(expr)))\n        a.sort()\n        s = ' + '.join(a)\n        s = s.replace('+ -', '- ')\n        return s\n\n    def _print_TensorIndex(self, expr):\n        return \"{}%s{%s}\" % (\n            \"^\" if expr.is_up else \"_\",\n            self._print(expr.args[0])\n        )\n        return self._print(expr.args[0])\n\n    def _print_tuple(self, expr):\n        return r\"\\left ( %s\\right )\" % \\\n            r\", \\quad \".join([ self._print(i) for i in expr ])\n\n    def _print_TensorProduct(self, expr):\n        elements = [self._print(a) for a in expr.args]\n        return r' \\otimes '.join(elements)\n\n    def _print_WedgeProduct(self, expr):\n        elements = [self._print(a) for a in expr.args]\n        return r' \\wedge '.join(elements)\n\n    def _print_Tuple(self, expr):\n        return self._print_tuple(expr)\n\n    def _print_list(self, expr):\n        return r\"\\left [ %s\\right ]\" % \\\n            r\", \\quad \".join([ self._print(i) for i in expr ])\n\n    def _print_dict(self, d):\n        keys = sorted(d.keys(), key=default_sort_key)\n        items = []\n\n        for key in keys:\n            val = d[key]\n            items.append(\"%s : %s\" % (self._print(key), self._print(val)))\n\n        return r\"\\left \\{ %s\\right \\}\" % r\", \\quad \".join(items)\n\n    def _print_Dict(self, expr):\n        return self._print_dict(expr)\n\n    def _print_DiracDelta(self, expr, exp=None):\n        if len(expr.args) == 1 or expr.args[1] == 0:\n            tex = r\"\\delta\\left(%s\\right)\" % self._print(expr.args[0])\n        else:\n            tex = r\"\\delta^{\\left( %s \\right)}\\left( %s \\right)\" % (\n                self._print(expr.args[1]), self._print(expr.args[0]))\n        if exp:\n            tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n        return tex\n\n    def _print_SingularityFunction(self, expr):\n        shift = self._print(expr.args[0] - expr.args[1])\n        power = self._print(expr.args[2])\n        tex = r\"{\\langle %s \\rangle}^{%s}\" % (shift, power)\n        return tex\n\n    def _print_Heaviside(self, expr, exp=None):\n        tex = r\"\\theta\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp:\n            tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n        return tex\n\n    def _print_KroneckerDelta(self, expr, exp=None):\n        i = self._print(expr.args[0])\n        j = self._print(expr.args[1])\n        if expr.args[0].is_Atom and expr.args[1].is_Atom:\n            tex = r'\\delta_{%s %s}' % (i, j)\n        else:\n            tex = r'\\delta_{%s, %s}' % (i, j)\n        if exp:\n            tex = r'\\left(%s\\right)^{%s}' % (tex, exp)\n        return tex\n\n    def _print_LeviCivita(self, expr, exp=None):\n        indices = map(self._print, expr.args)\n        if all(x.is_Atom for x in expr.args):\n            tex = r'\\varepsilon_{%s}' % \" \".join(indices)\n        else:\n            tex = r'\\varepsilon_{%s}' % \", \".join(indices)\n        if exp:\n            tex = r'\\left(%s\\right)^{%s}' % (tex, exp)\n        return tex\n\n    def _print_ProductSet(self, p):\n        if len(p.sets) > 1 and not has_variety(p.sets):\n            return self._print(p.sets[0]) + \"^%d\" % len(p.sets)\n        else:\n            return r\" \\times \".join(self._print(set) for set in p.sets)\n\n    def _print_RandomDomain(self, d):\n        if hasattr(d, 'as_boolean'):\n            return 'Domain: ' + self._print(d.as_boolean())\n        elif hasattr(d, 'set'):\n            return ('Domain: ' + self._print(d.symbols) + ' in ' +\n                    self._print(d.set))\n        elif hasattr(d, 'symbols'):\n            return 'Domain on ' + self._print(d.symbols)\n        else:\n            return self._print(None)\n\n    def _print_FiniteSet(self, s):\n        items = sorted(s.args, key=default_sort_key)\n        return self._print_set(items)\n\n    def _print_set(self, s):\n        items = sorted(s, key=default_sort_key)\n        items = \", \".join(map(self._print, items))\n        return r\"\\left\\{%s\\right\\}\" % items\n\n    _print_frozenset = _print_set\n\n    def _print_Range(self, s):\n        dots = r'\\ldots'\n\n        if s.start.is_infinite:\n            printset = s.start, dots, s[-1] - s.step, s[-1]\n        elif s.stop.is_infinite or len(s) > 4:\n            it = iter(s)\n            printset = next(it), next(it), dots, s[-1]\n        else:\n            printset = tuple(s)\n\n        return (r\"\\left\\{\"\n              + r\", \".join(self._print(el) for el in printset)", "answer": "A"}
{"uuid": "b2e66c81-4011-4e31-8204-b5984dd0d943", "issue_statement": "mathematica_code gives wrong output with Max\nIf I run the code\r\n\r\n```\r\nx = symbols('x')\r\nmathematica_code(Max(x,2))\r\n```\r\n\r\nthen I would expect the output `'Max[x,2]'` which is valid Mathematica code but instead I get `'Max(2, x)'` which is not valid Mathematica code.", "choices": "(A) known_functions = {\n    \"exp\": [(lambda x: True, \"Exp\")],\n    \"log\": [(lambda x: True, \"Log\")],\n    \"sin\": [(lambda x: True, \"Sin\")],\n    \"cos\": [(lambda x: True, \"Cos\")],\n    \"tan\": [(lambda x: True, \"Tan\")],\n    \"cot\": [(lambda x: True, \"Cot\")],\n    \"asin\": [(lambda x: True, \"ArcSin\")],\n    \"acos\": [(lambda x: True, \"ArcCos\")],\n    \"atan\": [(lambda x: True, \"ArcTan\")],\n    \"sinh\": [(lambda x: True, \"Sinh\")],\n    \"cosh\": [(lambda x: True, \"Cosh\")],\n    \"tanh\": [(lambda x: True, \"Tanh\")],\n    \"coth\": [(lambda x: True, \"Coth\")],\n    \"sech\": [(lambda x: True, \"Sech\")],\n    \"csch\": [(lambda x: True, \"Csch\")],\n    \"asinh\": [(lambda x: True, \"ArcSinh\")],\n    \"acosh\": [(lambda x: True, \"ArcCosh\")],\n    \"atanh\": [(lambda x: True, \"ArcTanh\")],\n    \"acoth\": [(lambda x: True, \"ArcCoth\")],\n    \"asech\": [(lambda x: True, \"ArcSech\")],\n    \"acsch\": [(lambda x: True, \"ArcCsch\")],\n    \"conjugate\": [(lambda x: True, \"Conjugate\")],\n\n}\n\n\nclass MCodePrinter(CodePrinter):\n    \"\"\"A printer to convert python expressions to\n    strings of the Wolfram's Mathematica code\n    \"\"\"\n    printmethod = \"_mcode\"\n\n    _default_settings = {\n        'order': None,\n        'full_prec': 'auto',\n        'precision': 15,\n        'user_functions': {},\n        'human': True,\n        'allow_unknown_functions': False,\n    }\n\n    _number_symbols = set()\n    _not_supported = set()\n\n    def __init__(self, settings={}):\n        \"\"\"Register function mappings supplied by user\"\"\"\n        CodePrinter.__init__(self, settings)\n        self.known_functions = dict(known_functions)\n        userfuncs = settings.get('user_functions', {})\n        for k, v in userfuncs.items():\n            if not isinstance(v, list):\n                userfuncs[k] = [(lambda *x: True, v)]\n                self.known_functions.update(userfuncs)\n\n    doprint = StrPrinter.doprint\n\n    def _print_Pow(self, expr):\n        PREC = precedence(expr)\n        return '%s^%s' % (self.parenthesize(expr.base, PREC),\n                          self.parenthesize(expr.exp, PREC))\n\n    def _print_Mul(self, expr):\n        PREC = precedence(expr)\n        c, nc = expr.args_cnc()\n        res = super(MCodePrinter, self)._print_Mul(expr.func(*c))\n        if nc:\n            res += '*'\n            res += '**'.join(self.parenthesize(a, PREC) for a in nc)\n        return res\n\n    def _print_Pi(self, expr):\n        return 'Pi'\n\n    def _print_Infinity(self, expr):\n        return 'Infinity'\n\n    def _print_NegativeInfinity(self, expr):\n        return '-Infinity'\n(B)     }\n\n    _number_symbols = set()\n    _not_supported = set()\n\n    def __init__(self, settings={}):\n        \"\"\"Register function mappings supplied by user\"\"\"\n        CodePrinter.__init__(self, settings)\n        self.known_functions = dict(known_functions)\n        userfuncs = settings.get('user_functions', {})\n        for k, v in userfuncs.items():\n            if not isinstance(v, list):\n                userfuncs[k] = [(lambda *x: True, v)]\n                self.known_functions.update(userfuncs)\n\n    doprint = StrPrinter.doprint\n\n    def _print_Pow(self, expr):\n        PREC = precedence(expr)\n        return '%s^%s' % (self.parenthesize(expr.base, PREC),\n                          self.parenthesize(expr.exp, PREC))\n\n    def _print_Mul(self, expr):\n        PREC = precedence(expr)\n        c, nc = expr.args_cnc()\n        res = super(MCodePrinter, self)._print_Mul(expr.func(*c))\n        if nc:\n            res += '*'\n            res += '**'.join(self.parenthesize(a, PREC) for a in nc)\n        return res\n\n    def _print_Pi(self, expr):\n        return 'Pi'\n\n    def _print_Infinity(self, expr):\n        return 'Infinity'\n\n    def _print_NegativeInfinity(self, expr):\n        return '-Infinity'\n\n    def _print_list(self, expr):\n        return '{' + ', '.join(self.doprint(a) for a in expr) + '}'\n    _print_tuple = _print_list\n    _print_Tuple = _print_list\n\n    def _print_Function(self, expr):\n        if expr.func.__name__ in self.known_functions:\n            cond_mfunc = self.known_functions[expr.func.__name__]\n            for cond, mfunc in cond_mfunc:\n                if cond(*expr.args):\n                    return \"%s[%s]\" % (mfunc, self.stringify(expr.args, \", \"))\n        return expr.func.__name__ + \"[%s]\" % self.stringify(expr.args, \", \")\n\n    def _print_Integral(self, expr):\n        if len(expr.variables) == 1 and not expr.limits[0][1:]:\n            args = [expr.args[0], expr.variables[0]]\n        else:\n            args = expr.args\n        return \"Hold[Integrate[\" + ', '.join(self.doprint(a) for a in args) + \"]]\"\n\n    def _print_Sum(self, expr):\n        return \"Hold[Sum[\" + ', '.join(self.doprint(a) for a in expr.args) + \"]]\"\n\n    def _print_Derivative(self, expr):\n        dexpr = expr.expr\n        dvars = [i[0] if i[1] == 1 else i for i in expr.variable_count]\n        return \"Hold[D[\" + ', '.join(self.doprint(a) for a in [dexpr] + dvars) + \"]]\"\n\n\ndef mathematica_code(expr, **settings):\n    r\"\"\"Converts an expr to a string of the Wolfram Mathematica code\n\n    Examples\n    ========\n\n    >>> from sympy import mathematica_code as mcode, symbols, sin\n    >>> x = symbols('x')\n    >>> mcode(sin(x).series(x).removeO())\n    '(1/120)*x^5 - 1/6*x^3 + x'\n(C)     \"asech\": [(lambda x: True, \"ArcSech\")],\n    \"acsch\": [(lambda x: True, \"ArcCsch\")],\n    \"conjugate\": [(lambda x: True, \"Conjugate\")],\n\n}\n\n\nclass MCodePrinter(CodePrinter):\n    \"\"\"A printer to convert python expressions to\n    strings of the Wolfram's Mathematica code\n    \"\"\"\n    printmethod = \"_mcode\"\n\n    _default_settings = {\n        'order': None,\n        'full_prec': 'auto',\n        'precision': 15,\n        'user_functions': {},\n        'human': True,\n        'allow_unknown_functions': False,\n    }\n\n    _number_symbols = set()\n    _not_supported = set()\n\n    def __init__(self, settings={}):\n        \"\"\"Register function mappings supplied by user\"\"\"\n        CodePrinter.__init__(self, settings)\n        self.known_functions = dict(known_functions)\n        userfuncs = settings.get('user_functions', {})\n        for k, v in userfuncs.items():\n            if not isinstance(v, list):\n                userfuncs[k] = [(lambda *x: True, v)]\n                self.known_functions.update(userfuncs)\n\n    doprint = StrPrinter.doprint\n\n    def _print_Pow(self, expr):\n        PREC = precedence(expr)\n        return '%s^%s' % (self.parenthesize(expr.base, PREC),\n                          self.parenthesize(expr.exp, PREC))\n\n    def _print_Mul(self, expr):\n        PREC = precedence(expr)\n        c, nc = expr.args_cnc()\n        res = super(MCodePrinter, self)._print_Mul(expr.func(*c))\n        if nc:\n            res += '*'\n            res += '**'.join(self.parenthesize(a, PREC) for a in nc)\n        return res\n\n    def _print_Pi(self, expr):\n        return 'Pi'\n\n    def _print_Infinity(self, expr):\n        return 'Infinity'\n\n    def _print_NegativeInfinity(self, expr):\n        return '-Infinity'\n\n    def _print_list(self, expr):\n        return '{' + ', '.join(self.doprint(a) for a in expr) + '}'\n    _print_tuple = _print_list\n    _print_Tuple = _print_list\n\n    def _print_Function(self, expr):\n        if expr.func.__name__ in self.known_functions:\n            cond_mfunc = self.known_functions[expr.func.__name__]\n            for cond, mfunc in cond_mfunc:\n                if cond(*expr.args):\n                    return \"%s[%s]\" % (mfunc, self.stringify(expr.args, \", \"))\n        return expr.func.__name__ + \"[%s]\" % self.stringify(expr.args, \", \")\n\n    def _print_Integral(self, expr):\n        if len(expr.variables) == 1 and not expr.limits[0][1:]:\n            args = [expr.args[0], expr.variables[0]]\n        else:\n            args = expr.args\n        return \"Hold[Integrate[\" + ', '.join(self.doprint(a) for a in args) + \"]]\"", "answer": "C"}
{"uuid": "061ff1a0-7b0b-4e62-ad40-e34b3b7ab4a3", "issue_statement": "can't simplify sin/cos with Rational?\nlatest cloned sympy, python 3 on windows\r\nfirstly, cos, sin with symbols can be simplified; rational number can be simplified\r\n```python\r\nfrom sympy import *\r\n\r\nx, y = symbols('x, y', real=True)\r\nr = sin(x)*sin(y) + cos(x)*cos(y)\r\nprint(r)\r\nprint(r.simplify())\r\nprint()\r\n\r\nr = Rational(1, 50) - Rational(1, 25)\r\nprint(r)\r\nprint(r.simplify())\r\nprint()\r\n```\r\nsays\r\n```cmd\r\nsin(x)*sin(y) + cos(x)*cos(y)\r\ncos(x - y)\r\n\r\n-1/50\r\n-1/50\r\n```\r\n\r\nbut\r\n```python\r\nt1 = Matrix([sin(Rational(1, 50)), cos(Rational(1, 50)), 0])\r\nt2 = Matrix([sin(Rational(1, 25)), cos(Rational(1, 25)), 0])\r\nr = t1.dot(t2)\r\nprint(r)\r\nprint(r.simplify())\r\nprint()\r\n\r\nr = sin(Rational(1, 50))*sin(Rational(1, 25)) + cos(Rational(1, 50))*cos(Rational(1, 25))\r\nprint(r)\r\nprint(r.simplify())\r\nprint()\r\n\r\nprint(acos(r))\r\nprint(acos(r).simplify())\r\nprint()\r\n```\r\nsays\r\n```cmd\r\nsin(1/50)*sin(1/25) + cos(1/50)*cos(1/25)\r\nsin(1/50)*sin(1/25) + cos(1/50)*cos(1/25)\r\n\r\nsin(1/50)*sin(1/25) + cos(1/50)*cos(1/25)\r\nsin(1/50)*sin(1/25) + cos(1/50)*cos(1/25)\r\n\r\nacos(sin(1/50)*sin(1/25) + cos(1/50)*cos(1/25))\r\nacos(sin(1/50)*sin(1/25) + cos(1/50)*cos(1/25))\r\n```", "choices": "(A)         TR10,  # sin-cos of sums -> sin-cos prod\n        TR11, TR6, # reduce double angles and rewrite cos pows\n        lambda x: _eapply(factor, x, trigs),\n        TR14,  # factored powers of identities\n        [identity, lambda x: _eapply(_mexpand, x, trigs)],\n        TRmorrie,\n        TR10i,  # sin-cos products > sin-cos of sums\n        [identity, TR8],  # sin-cos products -> sin-cos of sums\n(B)         lambda x: _eapply(factor, x, trigs),\n        TR14,  # factored powers of identities\n        [identity, lambda x: _eapply(_mexpand, x, trigs)],\n        TRmorrie,\n        TR10i,  # sin-cos products > sin-cos of sums\n        [identity, TR8],  # sin-cos products -> sin-cos of sums\n        [identity, lambda x: TR2i(TR2(x))],  # tan -> sin-cos -> tan\n        [\n(C)         [identity, lambda x: _eapply(_mexpand, x, trigs)],\n        TRmorrie,\n        TR10i,  # sin-cos products > sin-cos of sums\n        [identity, TR8],  # sin-cos products -> sin-cos of sums\n        [identity, lambda x: TR2i(TR2(x))],  # tan -> sin-cos -> tan\n        [\n            lambda x: _eapply(expand_mul, TR5(x), trigs),\n            lambda x: _eapply(\n(D)         TR10i,  # sin-cos products > sin-cos of sums\n        [identity, TR8],  # sin-cos products -> sin-cos of sums\n        [identity, lambda x: TR2i(TR2(x))],  # tan -> sin-cos -> tan\n        [\n            lambda x: _eapply(expand_mul, TR5(x), trigs),\n            lambda x: _eapply(\n                expand_mul, TR15(x), trigs)], # pos/neg powers of sin\n        [", "answer": "B"}
{"uuid": "db21cf2f-1777-475b-8163-4b81e0cf457e", "issue_statement": "Indexed matrix-expression LaTeX printer is not compilable\n```python\r\ni, j, k = symbols(\"i j k\")\r\nM = MatrixSymbol(\"M\", k, k)\r\nN = MatrixSymbol(\"N\", k, k)\r\nlatex((M*N)[i, j])\r\n```\r\n\r\nThe LaTeX string produced by the last command is:\r\n```\r\n\\sum_{i_{1}=0}^{k - 1} M_{i, _i_1} N_{_i_1, j}\r\n```\r\nLaTeX complains about a double subscript `_`. This expression won't render in MathJax either.", "choices": "(A)     _print_ImmutableMatrix = _print_ImmutableDenseMatrix \\\n                           = _print_Matrix \\\n                           = _print_MatrixBase\n\n    def _print_MatrixElement(self, expr):\n        return self.parenthesize(expr.parent, PRECEDENCE[\"Atom\"], strict=True) \\\n            + '_{%s, %s}' % (expr.i, expr.j)\n\n    def _print_MatrixSlice(self, expr):\n        def latexslice(x):\n(B) \n    def _print_MatrixElement(self, expr):\n        return self.parenthesize(expr.parent, PRECEDENCE[\"Atom\"], strict=True) \\\n            + '_{%s, %s}' % (expr.i, expr.j)\n\n    def _print_MatrixSlice(self, expr):\n        def latexslice(x):\n            x = list(x)\n            if x[2] == 1:\n                del x[2]\n(C)     def _print_MatrixSlice(self, expr):\n        def latexslice(x):\n            x = list(x)\n            if x[2] == 1:\n                del x[2]\n            if x[1] == x[0] + 1:\n                del x[1]\n            if x[0] == 0:\n                x[0] = ''\n            return ':'.join(map(self._print, x))\n(D)             + '_{%s, %s}' % (expr.i, expr.j)\n\n    def _print_MatrixSlice(self, expr):\n        def latexslice(x):\n            x = list(x)\n            if x[2] == 1:\n                del x[2]\n            if x[1] == x[0] + 1:\n                del x[1]\n            if x[0] == 0:", "answer": "B"}
{"uuid": "68459280-90f3-46fd-9803-98b700822fad", "issue_statement": "Some issues with idiff\nidiff doesn't support Eq, and it also doesn't support f(x) instead of y. Both should be easy to correct.\r\n\r\n```\r\n>>> idiff(Eq(y*exp(y), x*exp(x)), y, x)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"./sympy/geometry/util.py\", line 582, in idiff\r\n    yp = solve(eq.diff(x), dydx)[0].subs(derivs)\r\nIndexError: list index out of range\r\n>>> idiff(f(x)*exp(f(x)) - x*exp(x), f(x), x)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"./sympy/geometry/util.py\", line 574, in idiff\r\n    raise ValueError(\"expecting x-dependent symbol(s) but got: %s\" % y)\r\nValueError: expecting x-dependent symbol(s) but got: f(x)\r\n>>> idiff(y*exp(y)- x*exp(x), y, x)\r\n(x + 1)*exp(x - y)/(y + 1)\r\n```", "choices": "(A)     sympy.core.function.diff: explicitly differentiates wrt symbols\n\n    \"\"\"\n    if is_sequence(y):\n        dep = set(y)\n        y = y[0]\n    elif isinstance(y, Symbol):\n        dep = {y}\n    else:\n        raise ValueError(\"expecting x-dependent symbol(s) but got: %s\" % y)\n\n    f = dict([(s, Function(\n        s.name)(x)) for s in eq.free_symbols if s != x and s in dep])\n    dydx = Function(y.name)(x).diff(x)\n    eq = eq.subs(f)\n    derivs = {}\n    for i in range(n):\n        yp = solve(eq.diff(x), dydx)[0].subs(derivs)\n        if i == n - 1:\n(B)         y = y[0]\n    elif isinstance(y, Symbol):\n        dep = {y}\n    else:\n        raise ValueError(\"expecting x-dependent symbol(s) but got: %s\" % y)\n\n    f = dict([(s, Function(\n        s.name)(x)) for s in eq.free_symbols if s != x and s in dep])\n    dydx = Function(y.name)(x).diff(x)\n    eq = eq.subs(f)\n    derivs = {}\n    for i in range(n):\n        yp = solve(eq.diff(x), dydx)[0].subs(derivs)\n        if i == n - 1:\n            return yp.subs([(v, k) for k, v in f.items()])\n        derivs[dydx] = yp\n        eq = dydx - yp\n        dydx = dydx.diff(x)\n\n(C)     derivs = {}\n    for i in range(n):\n        yp = solve(eq.diff(x), dydx)[0].subs(derivs)\n        if i == n - 1:\n            return yp.subs([(v, k) for k, v in f.items()])\n        derivs[dydx] = yp\n        eq = dydx - yp\n        dydx = dydx.diff(x)\n\n\ndef intersection(*entities, **kwargs):\n    \"\"\"The intersection of a collection of GeometryEntity instances.\n\n    Parameters\n    ==========\n    entities : sequence of GeometryEntity\n    pairwise (keyword argument) : Can be either True or False\n\n    Returns\n(D) \n    f = dict([(s, Function(\n        s.name)(x)) for s in eq.free_symbols if s != x and s in dep])\n    dydx = Function(y.name)(x).diff(x)\n    eq = eq.subs(f)\n    derivs = {}\n    for i in range(n):\n        yp = solve(eq.diff(x), dydx)[0].subs(derivs)\n        if i == n - 1:\n            return yp.subs([(v, k) for k, v in f.items()])\n        derivs[dydx] = yp\n        eq = dydx - yp\n        dydx = dydx.diff(x)\n\n\ndef intersection(*entities, **kwargs):\n    \"\"\"The intersection of a collection of GeometryEntity instances.\n\n    Parameters", "answer": "B"}
{"uuid": "4ee18487-3abb-4400-9760-e57b69459ea7", "issue_statement": "mathml printer for IndexedBase required\nWriting an `Indexed` object to MathML fails with a `TypeError` exception: `TypeError: 'Indexed' object is not iterable`:\r\n\r\n```\r\nIn [340]: sympy.__version__\r\nOut[340]: '1.0.1.dev'\r\n\r\nIn [341]: from sympy.abc import (a, b)\r\n\r\nIn [342]: sympy.printing.mathml(sympy.IndexedBase(a)[b])\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-342-b32e493b70d3> in <module>()\r\n----> 1 sympy.printing.mathml(sympy.IndexedBase(a)[b])\r\n\r\n/dev/shm/gerrit/venv/stable-3.5/lib/python3.5/site-packages/sympy/printing/mathml.py in mathml(expr, **settings)\r\n    442 def mathml(expr, **settings):\r\n    443     \"\"\"Returns the MathML representation of expr\"\"\"\r\n--> 444     return MathMLPrinter(settings).doprint(expr)\r\n    445 \r\n    446 \r\n\r\n/dev/shm/gerrit/venv/stable-3.5/lib/python3.5/site-packages/sympy/printing/mathml.py in doprint(self, expr)\r\n     36         Prints the expression as MathML.\r\n     37         \"\"\"\r\n---> 38         mathML = Printer._print(self, expr)\r\n     39         unistr = mathML.toxml()\r\n     40         xmlbstr = unistr.encode('ascii', 'xmlcharrefreplace')\r\n\r\n/dev/shm/gerrit/venv/stable-3.5/lib/python3.5/site-packages/sympy/printing/printer.py in _print(self, expr, *args, **kwargs)\r\n    255                 printmethod = '_print_' + cls.__name__\r\n    256                 if hasattr(self, printmethod):\r\n--> 257                     return getattr(self, printmethod)(expr, *args, **kwargs)\r\n    258             # Unknown object, fall back to the emptyPrinter.\r\n    259             return self.emptyPrinter(expr)\r\n\r\n/dev/shm/gerrit/venv/stable-3.5/lib/python3.5/site-packages/sympy/printing/mathml.py in _print_Basic(self, e)\r\n    356     def _print_Basic(self, e):\r\n    357         x = self.dom.createElement(self.mathml_tag(e))\r\n--> 358         for arg in e:\r\n    359             x.appendChild(self._print(arg))\r\n    360         return x\r\n\r\nTypeError: 'Indexed' object is not iterable\r\n```\r\n\r\nIt also fails for more complex expressions where at least one element is Indexed.", "choices": "(A) def print_mathml(expr, printer='content', **settings):\n    \"\"\"\n    Prints a pretty representation of the MathML code for expr. If printer is\n    presentation then prints Presentation MathML else prints content MathML.\n\n    Examples\n    ========\n\n    >>> ##\n    >>> from sympy.printing.mathml import print_mathml\n    >>> from sympy.abc import x\n    >>> print_mathml(x+1) #doctest: +NORMALIZE_WHITESPACE\n    <apply>\n        <plus/>\n        <ci>x</ci>\n        <cn>1</cn>\n    </apply>\n    >>> print_mathml(x+1, printer='presentation')\n    <mrow>\n        <mi>x</mi>\n        <mo>+</mo>\n        <mn>1</mn>\n    </mrow>\n\n    \"\"\"\n    if printer == 'presentation':\n(B)             symbols = self._print(symbols)\n        mrow.appendChild(symbols)\n        mo = self.dom.createElement('mo')\n        mo.appendChild(self.dom.createTextNode('&#x21A6;'))\n        mrow.appendChild(mo)\n        mrow.appendChild(self._print(e.args[1]))\n        x.appendChild(mrow)\n        return x\n\n\ndef mathml(expr, printer='content', **settings):\n    \"\"\"Returns the MathML representation of expr. If printer is presentation then\n     prints Presentation MathML else prints content MathML.\n    \"\"\"\n    if printer == 'presentation':\n        return MathMLPresentationPrinter(settings).doprint(expr)\n    else:\n        return MathMLContentPrinter(settings).doprint(expr)\n\n\ndef print_mathml(expr, printer='content', **settings):\n    \"\"\"\n    Prints a pretty representation of the MathML code for expr. If printer is\n    presentation then prints Presentation MathML else prints content MathML.\n\n    Examples\n(C)     if printer == 'presentation':\n        return MathMLPresentationPrinter(settings).doprint(expr)\n    else:\n        return MathMLContentPrinter(settings).doprint(expr)\n\n\ndef print_mathml(expr, printer='content', **settings):\n    \"\"\"\n    Prints a pretty representation of the MathML code for expr. If printer is\n    presentation then prints Presentation MathML else prints content MathML.\n\n    Examples\n    ========\n\n    >>> ##\n    >>> from sympy.printing.mathml import print_mathml\n    >>> from sympy.abc import x\n    >>> print_mathml(x+1) #doctest: +NORMALIZE_WHITESPACE\n    <apply>\n        <plus/>\n        <ci>x</ci>\n        <cn>1</cn>\n    </apply>\n    >>> print_mathml(x+1, printer='presentation')\n    <mrow>\n        <mi>x</mi>\n(D)         return x\n\n\ndef mathml(expr, printer='content', **settings):\n    \"\"\"Returns the MathML representation of expr. If printer is presentation then\n     prints Presentation MathML else prints content MathML.\n    \"\"\"\n    if printer == 'presentation':\n        return MathMLPresentationPrinter(settings).doprint(expr)\n    else:\n        return MathMLContentPrinter(settings).doprint(expr)\n\n\ndef print_mathml(expr, printer='content', **settings):\n    \"\"\"\n    Prints a pretty representation of the MathML code for expr. If printer is\n    presentation then prints Presentation MathML else prints content MathML.\n\n    Examples\n    ========\n\n    >>> ##\n    >>> from sympy.printing.mathml import print_mathml\n    >>> from sympy.abc import x\n    >>> print_mathml(x+1) #doctest: +NORMALIZE_WHITESPACE\n    <apply>", "answer": "D"}
{"uuid": "7a0a2ed9-b068-4296-a494-9b18c0e2404f", "issue_statement": "Product pretty print could be improved\nThis is what the pretty printing for `Product` looks like:\r\n\r\n```\r\n>>> pprint(Product(1, (n, 1, oo)))\r\n  \u221e\r\n\u252c\u2500\u2500\u2500\u252c\r\n\u2502   \u2502 1\r\n\u2502   \u2502\r\nn = 1\r\n>>> pprint(Product(1/n, (n, 1, oo)))\r\n   \u221e\r\n\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\r\n\u2502      \u2502 1\r\n\u2502      \u2502 \u2500\r\n\u2502      \u2502 n\r\n\u2502      \u2502\r\n n = 1\r\n>>> pprint(Product(1/n**2, (n, 1, oo)))\r\n    \u221e\r\n\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\r\n\u2502        \u2502 1\r\n\u2502        \u2502 \u2500\u2500\r\n\u2502        \u2502  2\r\n\u2502        \u2502 n\r\n\u2502        \u2502\r\n  n = 1\r\n>>> pprint(Product(1, (n, 1, oo)), use_unicode=False)\r\n  oo\r\n_____\r\n|   | 1\r\n|   |\r\nn = 1\r\n>>> pprint(Product(1/n, (n, 1, oo)), use_unicode=False)\r\n   oo\r\n________\r\n|      | 1\r\n|      | -\r\n|      | n\r\n|      |\r\n n = 1\r\n>>> pprint(Product(1/n**2, (n, 1, oo)), use_unicode=False)\r\n    oo\r\n__________\r\n|        | 1\r\n|        | --\r\n|        |  2\r\n|        | n\r\n|        |\r\n  n = 1\r\n```\r\n\r\n(if those don't look good in your browser copy paste them into the terminal)\r\n\r\nThis could be improved:\r\n\r\n- Why is there always an empty line at the bottom of the \u220f? Keeping everything below the horizontal line is good, but the bottom looks asymmetric, and it makes the \u220f bigger than it needs to be.\r\n\r\n- The \u220f is too fat IMO. \r\n\r\n- It might look better if we extended the top bar. I'm unsure about this. \r\n\r\nCompare this\r\n\r\n```\r\n    \u221e\r\n\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\r\n \u2502     \u2502  1\r\n \u2502     \u2502  \u2500\u2500\r\n \u2502     \u2502   2\r\n \u2502     \u2502  n\r\n  n = 1\r\n```\r\n\r\nThat's still almost twice as wide as the equivalent Sum, but if you make it much skinnier it starts to look bad.\r\n\r\n```\r\n  \u221e\r\n ____\r\n \u2572\r\n  \u2572   1\r\n   \u2572  \u2500\u2500\r\n   \u2571   2\r\n  \u2571   n\r\n \u2571\r\n \u203e\u203e\u203e\u203e\r\nn = 1\r\n```", "choices": "(A)             sign_lines = []\n            sign_lines.append(corner_chr + (horizontal_chr*width) + corner_chr)\n            for i in range(func_height + 1):\n                sign_lines.append(vertical_chr + (' '*width) + vertical_chr)\n\n            pretty_sign = stringPict('')\n            pretty_sign = prettyForm(*pretty_sign.stack(*sign_lines))\n\n            pretty_upper = self._print(lim[2])\n(B)             for i in range(func_height + 1):\n                sign_lines.append(vertical_chr + (' '*width) + vertical_chr)\n\n            pretty_sign = stringPict('')\n            pretty_sign = prettyForm(*pretty_sign.stack(*sign_lines))\n\n            pretty_upper = self._print(lim[2])\n            pretty_lower = self._print(Equality(lim[0], lim[1]))\n\n(C) \n        for lim in expr.limits:\n            width = (func_height + 2) * 5 // 3 - 2\n            sign_lines = []\n            sign_lines.append(corner_chr + (horizontal_chr*width) + corner_chr)\n            for i in range(func_height + 1):\n                sign_lines.append(vertical_chr + (' '*width) + vertical_chr)\n\n            pretty_sign = stringPict('')\n(D)         first = True\n        max_upper = 0\n        sign_height = 0\n\n        for lim in expr.limits:\n            width = (func_height + 2) * 5 // 3 - 2\n            sign_lines = []\n            sign_lines.append(corner_chr + (horizontal_chr*width) + corner_chr)\n            for i in range(func_height + 1):", "answer": "C"}
{"uuid": "bb54ecce-f7fd-48cb-855b-baf651c428cd", "issue_statement": "Bad centering for Sum pretty print\n```\r\n>>> pprint(Sum(x, (x, 1, oo)) + 3)\r\n  \u221e\r\n ___\r\n \u2572\r\n  \u2572   x\r\n  \u2571     + 3\r\n \u2571\r\n \u203e\u203e\u203e\r\nx = 1\r\n```\r\n\r\nThe `x` and the `+ 3` should be aligned. I'm not sure if the `x` should be lower of if the `+ 3` should be higher.", "choices": "(A)                 for i in reversed(range(1, d)):\n                    lines.append('%s/%s' % (' '*i, ' '*(w - i)))\n                lines.append(\"/\" + \"_\"*(w - 1) + ',')\n                return d, h + more, lines, 0\n            else:\n                w = w + more\n                d = d + more\n                vsum = vobj('sum', 4)\n                lines.append(\"_\"*(w))\n                for i in range(0, d):\n                    lines.append('%s%s%s' % (' '*i, vsum[2], ' '*(w - i - 1)))\n                for i in reversed(range(0, d)):\n                    lines.append('%s%s%s' % (' '*i, vsum[4], ' '*(w - i - 1)))\n                lines.append(vsum[8]*(w))\n                return d, h + 2*more, lines, more\n\n        f = expr.function\n\n        prettyF = self._print(f)\n\n        if f.is_Add:  # add parens\n            prettyF = prettyForm(*prettyF.parens())\n\n        H = prettyF.height() + 2\n\n        # \\sum \\sum \\sum ...\n        first = True\n        max_upper = 0\n        sign_height = 0\n\n        for lim in expr.limits:\n            if len(lim) == 3:\n                prettyUpper = self._print(lim[2])\n                prettyLower = self._print(Equality(lim[0], lim[1]))\n            elif len(lim) == 2:\n                prettyUpper = self._print(\"\")\n                prettyLower = self._print(Equality(lim[0], lim[1]))\n            elif len(lim) == 1:\n                prettyUpper = self._print(\"\")\n                prettyLower = self._print(lim[0])\n\n            max_upper = max(max_upper, prettyUpper.height())\n\n            # Create sum sign based on the height of the argument\n            d, h, slines, adjustment = asum(\n                H, prettyLower.width(), prettyUpper.width(), ascii_mode)\n            prettySign = stringPict('')\n            prettySign = prettyForm(*prettySign.stack(*slines))\n\n            if first:\n                sign_height = prettySign.height()\n\n            prettySign = prettyForm(*prettySign.above(prettyUpper))\n            prettySign = prettyForm(*prettySign.below(prettyLower))\n\n            if first:\n                # change F baseline so it centers on the sign\n                prettyF.baseline -= d - (prettyF.height()//2 -\n                                         prettyF.baseline) - adjustment\n                first = False\n\n            # put padding to the right\n            pad = stringPict('')\n            pad = prettyForm(*pad.stack(*[' ']*h))\n            prettySign = prettyForm(*prettySign.right(pad))\n            # put the present prettyF to the right\n            prettyF = prettyForm(*prettySign.right(prettyF))\n\n        prettyF.baseline = max_upper + sign_height//2\n        prettyF.binding = prettyForm.MUL\n        return prettyF\n\n    def _print_Limit(self, l):\n        e, z, z0, dir = l.args\n\n        E = self._print(e)\n(B) \n        if f.is_Add:  # add parens\n            prettyF = prettyForm(*prettyF.parens())\n\n        H = prettyF.height() + 2\n\n        # \\sum \\sum \\sum ...\n        first = True\n        max_upper = 0\n        sign_height = 0\n\n        for lim in expr.limits:\n            if len(lim) == 3:\n                prettyUpper = self._print(lim[2])\n                prettyLower = self._print(Equality(lim[0], lim[1]))\n            elif len(lim) == 2:\n                prettyUpper = self._print(\"\")\n                prettyLower = self._print(Equality(lim[0], lim[1]))\n            elif len(lim) == 1:\n                prettyUpper = self._print(\"\")\n                prettyLower = self._print(lim[0])\n\n            max_upper = max(max_upper, prettyUpper.height())\n\n            # Create sum sign based on the height of the argument\n            d, h, slines, adjustment = asum(\n                H, prettyLower.width(), prettyUpper.width(), ascii_mode)\n            prettySign = stringPict('')\n            prettySign = prettyForm(*prettySign.stack(*slines))\n\n            if first:\n                sign_height = prettySign.height()\n\n            prettySign = prettyForm(*prettySign.above(prettyUpper))\n            prettySign = prettyForm(*prettySign.below(prettyLower))\n\n            if first:\n                # change F baseline so it centers on the sign\n                prettyF.baseline -= d - (prettyF.height()//2 -\n                                         prettyF.baseline) - adjustment\n                first = False\n\n            # put padding to the right\n            pad = stringPict('')\n            pad = prettyForm(*pad.stack(*[' ']*h))\n            prettySign = prettyForm(*prettySign.right(pad))\n            # put the present prettyF to the right\n            prettyF = prettyForm(*prettySign.right(prettyF))\n\n        prettyF.baseline = max_upper + sign_height//2\n        prettyF.binding = prettyForm.MUL\n        return prettyF\n\n    def _print_Limit(self, l):\n        e, z, z0, dir = l.args\n\n        E = self._print(e)\n        if precedence(e) <= PRECEDENCE[\"Mul\"]:\n            E = prettyForm(*E.parens('(', ')'))\n        Lim = prettyForm('lim')\n\n        LimArg = self._print(z)\n        if self._use_unicode:\n            LimArg = prettyForm(*LimArg.right(u'\\N{BOX DRAWINGS LIGHT HORIZONTAL}\\N{RIGHTWARDS ARROW}'))\n        else:\n            LimArg = prettyForm(*LimArg.right('->'))\n        LimArg = prettyForm(*LimArg.right(self._print(z0)))\n\n        if str(dir) == '+-' or z0 in (S.Infinity, S.NegativeInfinity):\n            dir = \"\"\n        else:\n            if self._use_unicode:\n                dir = u'\\N{SUPERSCRIPT PLUS SIGN}' if str(dir) == \"+\" else u'\\N{SUPERSCRIPT MINUS}'\n\n        LimArg = prettyForm(*LimArg.right(self._print(dir)))\n\n(C)                 half = need//2\n                lead = ' '*half\n                if how == \">\":\n                    return \" \"*need + s\n                return lead + s + ' '*(need - len(lead))\n\n            h = max(hrequired, 2)\n            d = h//2\n            w = d + 1\n            more = hrequired % 2\n\n            lines = []\n            if use_ascii:\n                lines.append(\"_\"*(w) + ' ')\n                lines.append(r\"\\%s`\" % (' '*(w - 1)))\n                for i in range(1, d):\n                    lines.append('%s\\\\%s' % (' '*i, ' '*(w - i)))\n                if more:\n                    lines.append('%s)%s' % (' '*(d), ' '*(w - d)))\n                for i in reversed(range(1, d)):\n                    lines.append('%s/%s' % (' '*i, ' '*(w - i)))\n                lines.append(\"/\" + \"_\"*(w - 1) + ',')\n                return d, h + more, lines, 0\n            else:\n                w = w + more\n                d = d + more\n                vsum = vobj('sum', 4)\n                lines.append(\"_\"*(w))\n                for i in range(0, d):\n                    lines.append('%s%s%s' % (' '*i, vsum[2], ' '*(w - i - 1)))\n                for i in reversed(range(0, d)):\n                    lines.append('%s%s%s' % (' '*i, vsum[4], ' '*(w - i - 1)))\n                lines.append(vsum[8]*(w))\n                return d, h + 2*more, lines, more\n\n        f = expr.function\n\n        prettyF = self._print(f)\n\n        if f.is_Add:  # add parens\n            prettyF = prettyForm(*prettyF.parens())\n\n        H = prettyF.height() + 2\n\n        # \\sum \\sum \\sum ...\n        first = True\n        max_upper = 0\n        sign_height = 0\n\n        for lim in expr.limits:\n            if len(lim) == 3:\n                prettyUpper = self._print(lim[2])\n                prettyLower = self._print(Equality(lim[0], lim[1]))\n            elif len(lim) == 2:\n                prettyUpper = self._print(\"\")\n                prettyLower = self._print(Equality(lim[0], lim[1]))\n            elif len(lim) == 1:\n                prettyUpper = self._print(\"\")\n                prettyLower = self._print(lim[0])\n\n            max_upper = max(max_upper, prettyUpper.height())\n\n            # Create sum sign based on the height of the argument\n            d, h, slines, adjustment = asum(\n                H, prettyLower.width(), prettyUpper.width(), ascii_mode)\n            prettySign = stringPict('')\n            prettySign = prettyForm(*prettySign.stack(*slines))\n\n            if first:\n                sign_height = prettySign.height()\n\n            prettySign = prettyForm(*prettySign.above(prettyUpper))\n            prettySign = prettyForm(*prettySign.below(prettyLower))\n\n            if first:\n                # change F baseline so it centers on the sign\n(D)                 prettyUpper = self._print(\"\")\n                prettyLower = self._print(lim[0])\n\n            max_upper = max(max_upper, prettyUpper.height())\n\n            # Create sum sign based on the height of the argument\n            d, h, slines, adjustment = asum(\n                H, prettyLower.width(), prettyUpper.width(), ascii_mode)\n            prettySign = stringPict('')\n            prettySign = prettyForm(*prettySign.stack(*slines))\n\n            if first:\n                sign_height = prettySign.height()\n\n            prettySign = prettyForm(*prettySign.above(prettyUpper))\n            prettySign = prettyForm(*prettySign.below(prettyLower))\n\n            if first:\n                # change F baseline so it centers on the sign\n                prettyF.baseline -= d - (prettyF.height()//2 -\n                                         prettyF.baseline) - adjustment\n                first = False\n\n            # put padding to the right\n            pad = stringPict('')\n            pad = prettyForm(*pad.stack(*[' ']*h))\n            prettySign = prettyForm(*prettySign.right(pad))\n            # put the present prettyF to the right\n            prettyF = prettyForm(*prettySign.right(prettyF))\n\n        prettyF.baseline = max_upper + sign_height//2\n        prettyF.binding = prettyForm.MUL\n        return prettyF\n\n    def _print_Limit(self, l):\n        e, z, z0, dir = l.args\n\n        E = self._print(e)\n        if precedence(e) <= PRECEDENCE[\"Mul\"]:\n            E = prettyForm(*E.parens('(', ')'))\n        Lim = prettyForm('lim')\n\n        LimArg = self._print(z)\n        if self._use_unicode:\n            LimArg = prettyForm(*LimArg.right(u'\\N{BOX DRAWINGS LIGHT HORIZONTAL}\\N{RIGHTWARDS ARROW}'))\n        else:\n            LimArg = prettyForm(*LimArg.right('->'))\n        LimArg = prettyForm(*LimArg.right(self._print(z0)))\n\n        if str(dir) == '+-' or z0 in (S.Infinity, S.NegativeInfinity):\n            dir = \"\"\n        else:\n            if self._use_unicode:\n                dir = u'\\N{SUPERSCRIPT PLUS SIGN}' if str(dir) == \"+\" else u'\\N{SUPERSCRIPT MINUS}'\n\n        LimArg = prettyForm(*LimArg.right(self._print(dir)))\n\n        Lim = prettyForm(*Lim.below(LimArg))\n        Lim = prettyForm(*Lim.right(E), binding=prettyForm.MUL)\n\n        return Lim\n\n    def _print_matrix_contents(self, e):\n        \"\"\"\n        This method factors out what is essentially grid printing.\n        \"\"\"\n        M = e   # matrix\n        Ms = {}  # i,j -> pretty(M[i,j])\n        for i in range(M.rows):\n            for j in range(M.cols):\n                Ms[i, j] = self._print(M[i, j])\n\n        # h- and v- spacers\n        hsep = 2\n        vsep = 1\n", "answer": "A"}
{"uuid": "7a26dd28-7c1a-445e-b686-33e8890e618e", "issue_statement": "autowrap with cython backend fails when array arguments do not appear in wrapped expr\nWhen using the cython backend for autowrap, it appears that the code is not correctly generated when the function in question has array arguments that do not appear in the final expression. A minimal counterexample is:\r\n\r\n```python\r\nfrom sympy.utilities.autowrap import autowrap\r\nfrom sympy import MatrixSymbol\r\nimport numpy as np\r\n\r\nx = MatrixSymbol('x', 2, 1)\r\nexpr = 1.0\r\nf = autowrap(expr, args=(x,), backend='cython')\r\n\r\nf(np.array([[1.0, 2.0]]))\r\n```\r\n\r\nThis should of course return `1.0` but instead fails with:\r\n```python\r\nTypeError: only size-1 arrays can be converted to Python scalars\r\n```\r\n\r\nA little inspection reveals that this is because the corresponding C function is generated with an incorrect signature:\r\n\r\n```C\r\ndouble autofunc(double x) {\r\n\r\n   double autofunc_result;\r\n   autofunc_result = 1.0;\r\n   return autofunc_result;\r\n\r\n}\r\n```\r\n\r\n(`x` should be `double *`, not `double` in this case)\r\n\r\nI've found that this error won't occur so long as `expr` depends at least in part on each argument. For example this slight modification of the above counterexample works perfectly:\r\n\r\n```python\r\nfrom sympy.utilities.autowrap import autowrap\r\nfrom sympy import MatrixSymbol\r\nimport numpy as np\r\n\r\nx = MatrixSymbol('x', 2, 1)\r\n# now output depends on x\r\nexpr = x[0,0]\r\nf = autowrap(expr, args=(x,), backend='cython')\r\n\r\n# returns 1.0 as expected, without failure\r\nf(np.array([[1.0, 2.0]]))\r\n```\r\n\r\nThis may seem like a silly issue (\"why even have `x` as an argument if it doesn't appear in the expression you're trying to evaluate?\"). But of course in interfacing with external libraries (e.g. for numerical integration), one often needs functions to have a pre-defined signature regardless of whether a given argument contributes to the output.\r\n\r\nI think I've identified the problem in `codegen` and will suggest a PR shortly.", "choices": "(A)                         OutputArgument(symbol, out_arg, expr, dimensions=dims))\n\n                # remove duplicate arguments when they are not local variables\n                if symbol not in local_vars:\n                    # avoid duplicate arguments\n                    symbols.remove(symbol)\n            elif isinstance(expr, (ImmutableMatrix, MatrixSlice)):\n                # Create a \"dummy\" MatrixSymbol to use as the Output arg\n                out_arg = MatrixSymbol('out_%s' % abs(hash(expr)), *expr.shape)\n                dims = tuple([(S.Zero, dim - 1) for dim in out_arg.shape])\n                output_args.append(\n                    OutputArgument(out_arg, out_arg, expr, dimensions=dims))\n            else:\n                return_val.append(Result(expr))\n\n        arg_list = []\n\n        # setup input argument list\n        array_symbols = {}\n        for array in expressions.atoms(Indexed) | local_expressions.atoms(Indexed):\n            array_symbols[array.base.label] = array\n        for array in expressions.atoms(MatrixSymbol) | local_expressions.atoms(MatrixSymbol):\n            array_symbols[array] = array\n\n        for symbol in sorted(symbols, key=str):\n            if symbol in array_symbols:\n                dims = []\n                array = array_symbols[symbol]\n                for dim in array.shape:\n                    dims.append((S.Zero, dim - 1))\n                metadata = {'dimensions': dims}\n            else:\n                metadata = {}\n\n            arg_list.append(InputArgument(symbol, **metadata))\n\n        output_args.sort(key=lambda x: str(x.name))\n        arg_list.extend(output_args)\n\n        if argument_sequence is not None:\n            # if the user has supplied IndexedBase instances, we'll accept that\n            new_sequence = []\n            for arg in argument_sequence:\n                if isinstance(arg, IndexedBase):\n                    new_sequence.append(arg.label)\n                else:\n                    new_sequence.append(arg)\n            argument_sequence = new_sequence\n\n            missing = [x for x in arg_list if x.name not in argument_sequence]\n            if missing:\n                msg = \"Argument list didn't specify: {0} \"\n                msg = msg.format(\", \".join([str(m.name) for m in missing]))\n                raise CodeGenArgumentListError(msg, missing)\n\n            # create redundant arguments to produce the requested sequence\n            name_arg_dict = {x.name: x for x in arg_list}\n(B)         arg_list = []\n\n        # setup input argument list\n        array_symbols = {}\n        for array in expressions.atoms(Indexed) | local_expressions.atoms(Indexed):\n            array_symbols[array.base.label] = array\n        for array in expressions.atoms(MatrixSymbol) | local_expressions.atoms(MatrixSymbol):\n            array_symbols[array] = array\n\n        for symbol in sorted(symbols, key=str):\n            if symbol in array_symbols:\n                dims = []\n                array = array_symbols[symbol]\n                for dim in array.shape:\n                    dims.append((S.Zero, dim - 1))\n                metadata = {'dimensions': dims}\n            else:\n                metadata = {}\n\n            arg_list.append(InputArgument(symbol, **metadata))\n\n        output_args.sort(key=lambda x: str(x.name))\n        arg_list.extend(output_args)\n\n        if argument_sequence is not None:\n            # if the user has supplied IndexedBase instances, we'll accept that\n            new_sequence = []\n            for arg in argument_sequence:\n                if isinstance(arg, IndexedBase):\n                    new_sequence.append(arg.label)\n                else:\n                    new_sequence.append(arg)\n            argument_sequence = new_sequence\n\n            missing = [x for x in arg_list if x.name not in argument_sequence]\n            if missing:\n                msg = \"Argument list didn't specify: {0} \"\n                msg = msg.format(\", \".join([str(m.name) for m in missing]))\n                raise CodeGenArgumentListError(msg, missing)\n\n            # create redundant arguments to produce the requested sequence\n            name_arg_dict = {x.name: x for x in arg_list}\n            new_args = []\n            for symbol in argument_sequence:\n                try:\n                    new_args.append(name_arg_dict[symbol])\n                except KeyError:\n                    new_args.append(InputArgument(symbol))\n            arg_list = new_args\n\n        return Routine(name, arg_list, return_val, local_vars, global_vars)\n\n    def write(self, routines, prefix, to_files=False, header=True, empty=True):\n        \"\"\"Writes all the source code files for the given routines.\n\n        The generated source is returned as a list of (filename, contents)\n        tuples, or is written to files (see below).  Each filename consists\n(C)                     new_sequence.append(arg.label)\n                else:\n                    new_sequence.append(arg)\n            argument_sequence = new_sequence\n\n            missing = [x for x in arg_list if x.name not in argument_sequence]\n            if missing:\n                msg = \"Argument list didn't specify: {0} \"\n                msg = msg.format(\", \".join([str(m.name) for m in missing]))\n                raise CodeGenArgumentListError(msg, missing)\n\n            # create redundant arguments to produce the requested sequence\n            name_arg_dict = {x.name: x for x in arg_list}\n            new_args = []\n            for symbol in argument_sequence:\n                try:\n                    new_args.append(name_arg_dict[symbol])\n                except KeyError:\n                    new_args.append(InputArgument(symbol))\n            arg_list = new_args\n\n        return Routine(name, arg_list, return_val, local_vars, global_vars)\n\n    def write(self, routines, prefix, to_files=False, header=True, empty=True):\n        \"\"\"Writes all the source code files for the given routines.\n\n        The generated source is returned as a list of (filename, contents)\n        tuples, or is written to files (see below).  Each filename consists\n        of the given prefix, appended with an appropriate extension.\n\n        Parameters\n        ==========\n\n        routines : list\n            A list of Routine instances to be written\n\n        prefix : string\n            The prefix for the output files\n\n        to_files : bool, optional\n            When True, the output is written to files.  Otherwise, a list\n            of (filename, contents) tuples is returned.  [default: False]\n\n        header : bool, optional\n            When True, a header comment is included on top of each source\n            file. [default: True]\n\n        empty : bool, optional\n            When True, empty lines are included to structure the source\n            files. [default: True]\n\n        \"\"\"\n        if to_files:\n            for dump_fn in self.dump_fns:\n                filename = \"%s.%s\" % (prefix, dump_fn.extension)\n                with open(filename, \"w\") as f:\n                    dump_fn(self, routines, f, prefix, header, empty)\n(D)                 metadata = {'dimensions': dims}\n            else:\n                metadata = {}\n\n            arg_list.append(InputArgument(symbol, **metadata))\n\n        output_args.sort(key=lambda x: str(x.name))\n        arg_list.extend(output_args)\n\n        if argument_sequence is not None:\n            # if the user has supplied IndexedBase instances, we'll accept that\n            new_sequence = []\n            for arg in argument_sequence:\n                if isinstance(arg, IndexedBase):\n                    new_sequence.append(arg.label)\n                else:\n                    new_sequence.append(arg)\n            argument_sequence = new_sequence\n\n            missing = [x for x in arg_list if x.name not in argument_sequence]\n            if missing:\n                msg = \"Argument list didn't specify: {0} \"\n                msg = msg.format(\", \".join([str(m.name) for m in missing]))\n                raise CodeGenArgumentListError(msg, missing)\n\n            # create redundant arguments to produce the requested sequence\n            name_arg_dict = {x.name: x for x in arg_list}\n            new_args = []\n            for symbol in argument_sequence:\n                try:\n                    new_args.append(name_arg_dict[symbol])\n                except KeyError:\n                    new_args.append(InputArgument(symbol))\n            arg_list = new_args\n\n        return Routine(name, arg_list, return_val, local_vars, global_vars)\n\n    def write(self, routines, prefix, to_files=False, header=True, empty=True):\n        \"\"\"Writes all the source code files for the given routines.\n\n        The generated source is returned as a list of (filename, contents)\n        tuples, or is written to files (see below).  Each filename consists\n        of the given prefix, appended with an appropriate extension.\n\n        Parameters\n        ==========\n\n        routines : list\n            A list of Routine instances to be written\n\n        prefix : string\n            The prefix for the output files\n\n        to_files : bool, optional\n            When True, the output is written to files.  Otherwise, a list\n            of (filename, contents) tuples is returned.  [default: False]\n", "answer": "B"}
{"uuid": "b172ba3c-af91-4d27-a016-e1105cb49622", "issue_statement": "Intersection should remove duplicates\n```python\r\n>>> Intersection({1},{1},{x})\r\nEmptySet()\r\n>>> Intersection({1},{x})\r\n{1}\r\n```\r\nThe answer should be `Piecewise(({1}, Eq(x, 1)), (S.EmptySet, True))` or remain unevaluated.\r\n\r\nThe routine should give the same answer if duplicates are present; my initial guess is that duplicates should just be removed at the outset of instantiation. Ordering them will produce canonical processing.", "choices": "(A)         evaluate = kwargs.get('evaluate', global_evaluate[0])\n\n        # flatten inputs to merge intersections and iterables\n        args = _sympify(args)\n\n        # Reduce sets using known rules\n        if evaluate:\n(B) \n    def __new__(cls, *args, **kwargs):\n        evaluate = kwargs.get('evaluate', global_evaluate[0])\n\n        # flatten inputs to merge intersections and iterables\n        args = _sympify(args)\n\n(C)         # flatten inputs to merge intersections and iterables\n        args = _sympify(args)\n\n        # Reduce sets using known rules\n        if evaluate:\n            args = list(cls._new_args_filter(args))\n            return simplify_intersection(args)\n(D) \n        # Reduce sets using known rules\n        if evaluate:\n            args = list(cls._new_args_filter(args))\n            return simplify_intersection(args)\n\n        args = list(ordered(args, Set._infimum_key))", "answer": "A"}
{"uuid": "6de59968-ec06-4f33-a346-92fb6307d1a7", "issue_statement": "Lambdify misinterprets some matrix expressions\nUsing lambdify on an expression containing an identity matrix gives us an unexpected result:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> n = symbols('n', integer=True)\r\n>>> A = MatrixSymbol(\"A\", n, n)\r\n>>> a = np.array([[1, 2], [3, 4]])\r\n>>> f = lambdify(A, A + Identity(n))\r\n>>> f(a)\r\narray([[1.+1.j, 2.+1.j],\r\n       [3.+1.j, 4.+1.j]])\r\n```\r\n\r\nInstead, the output should be  `array([[2, 2], [3, 5]])`, since we're adding an identity matrix to the array. Inspecting the globals and source code of `f` shows us why we get the result:\r\n\r\n```python\r\n>>> import inspect\r\n>>> print(inspect.getsource(f))\r\ndef _lambdifygenerated(A):\r\n    return (I + A)\r\n>>> f.__globals__['I']\r\n1j\r\n```\r\n\r\nThe code printer prints `I`, which is currently being interpreted as a Python built-in complex number. The printer should support printing identity matrices, and signal an error for unsupported expressions that might be misinterpreted.", "choices": "(A)         return '{0}({1})'.format(self._module_format('numpy.block'),\n                                 self._print(expr.args[0].tolist()))\n\n    def _print_CodegenArrayTensorProduct(self, expr):\n        array_list = [j for i, arg in enumerate(expr.args) for j in\n                (self._print(arg), \"[%i, %i]\" % (2*i, 2*i+1))]\n        return \"%s(%s)\" % (self._module_format('numpy.einsum'), \", \".join(array_list))\n\n    def _print_CodegenArrayContraction(self, expr):\n        from sympy.codegen.array_utils import CodegenArrayTensorProduct\n        base = expr.expr\n        contraction_indices = expr.contraction_indices\n        if not contraction_indices:\n(B)             func = self._module_format('numpy.array')\n        return \"%s(%s)\" % (func, self._print(expr.tolist()))\n\n    def _print_BlockMatrix(self, expr):\n        return '{0}({1})'.format(self._module_format('numpy.block'),\n                                 self._print(expr.args[0].tolist()))\n\n    def _print_CodegenArrayTensorProduct(self, expr):\n        array_list = [j for i, arg in enumerate(expr.args) for j in\n                (self._print(arg), \"[%i, %i]\" % (2*i, 2*i+1))]\n        return \"%s(%s)\" % (self._module_format('numpy.einsum'), \", \".join(array_list))\n\n    def _print_CodegenArrayContraction(self, expr):\n(C) \n    def _print_MatrixBase(self, expr):\n        func = self.known_functions.get(expr.__class__.__name__, None)\n        if func is None:\n            func = self._module_format('numpy.array')\n        return \"%s(%s)\" % (func, self._print(expr.tolist()))\n\n    def _print_BlockMatrix(self, expr):\n        return '{0}({1})'.format(self._module_format('numpy.block'),\n                                 self._print(expr.args[0].tolist()))\n\n    def _print_CodegenArrayTensorProduct(self, expr):\n        array_list = [j for i, arg in enumerate(expr.args) for j in\n(D)     def _print_CodegenArrayTensorProduct(self, expr):\n        array_list = [j for i, arg in enumerate(expr.args) for j in\n                (self._print(arg), \"[%i, %i]\" % (2*i, 2*i+1))]\n        return \"%s(%s)\" % (self._module_format('numpy.einsum'), \", \".join(array_list))\n\n    def _print_CodegenArrayContraction(self, expr):\n        from sympy.codegen.array_utils import CodegenArrayTensorProduct\n        base = expr.expr\n        contraction_indices = expr.contraction_indices\n        if not contraction_indices:\n            return self._print(base)\n        if isinstance(base, CodegenArrayTensorProduct):\n            counter = 0", "answer": "B"}
{"uuid": "5eb5806e-0d0b-4963-86cb-a47e2944c0fc", "issue_statement": "simplify(cos(x)**I): Invalid comparison of complex I (fu.py)\n```\r\n>>> from sympy import *\r\n>>> x = Symbol('x')\r\n>>> print(simplify(cos(x)**I))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/e/se/sympy/simplify/simplify.py\", line 587, in simplify\r\n    expr = trigsimp(expr, deep=True)\r\n  File \"/home/e/se/sympy/simplify/trigsimp.py\", line 508, in trigsimp\r\n    return trigsimpfunc(expr)\r\n  File \"/home/e/se/sympy/simplify/trigsimp.py\", line 501, in <lambda>\r\n    'matching': (lambda x: futrig(x)),\r\n  File \"/home/e/se/sympy/simplify/trigsimp.py\", line 1101, in futrig\r\n    e = bottom_up(e, lambda x: _futrig(x, **kwargs))\r\n  File \"/home/e/se/sympy/simplify/simplify.py\", line 1081, in bottom_up\r\n    rv = F(rv)\r\n  File \"/home/e/se/sympy/simplify/trigsimp.py\", line 1101, in <lambda>\r\n    e = bottom_up(e, lambda x: _futrig(x, **kwargs))\r\n  File \"/home/e/se/sympy/simplify/trigsimp.py\", line 1169, in _futrig\r\n    e = greedy(tree, objective=Lops)(e)\r\n  File \"/home/e/se/sympy/strategies/core.py\", line 115, in minrule\r\n    return min([rule(expr) for rule in rules], key=objective)\r\n  File \"/home/e/se/sympy/strategies/core.py\", line 115, in <listcomp>\r\n    return min([rule(expr) for rule in rules], key=objective)\r\n  File \"/home/e/se/sympy/strategies/core.py\", line 44, in chain_rl\r\n    expr = rule(expr)\r\n  File \"/home/e/se/sympy/simplify/fu.py\", line 566, in TR6\r\n    return _TR56(rv, cos, sin, lambda x: 1 - x, max=max, pow=pow)\r\n  File \"/home/e/se/sympy/simplify/fu.py\", line 524, in _TR56\r\n    return bottom_up(rv, _f)\r\n  File \"/home/e/se/sympy/simplify/simplify.py\", line 1081, in bottom_up\r\n    rv = F(rv)\r\n  File \"/home/e/se/sympy/simplify/fu.py\", line 504, in _f\r\n    if (rv.exp < 0) == True:\r\n  File \"/home/e/se/sympy/core/expr.py\", line 406, in __lt__\r\n    raise TypeError(\"Invalid comparison of complex %s\" % me)\r\nTypeError: Invalid comparison of complex I\r\n```", "choices": "(A)         # change is not going to allow a simplification as far as I can tell.\n        if not (rv.is_Pow and rv.base.func == f):\n            return rv\n\n        if (rv.exp < 0) == True:\n            return rv\n        if (rv.exp > max) == True:\n            return rv\n(B)         # or only those expressible as powers of 2. Also, should it only\n        # make the changes in powers that appear in sums -- making an isolated\n        # change is not going to allow a simplification as far as I can tell.\n        if not (rv.is_Pow and rv.base.func == f):\n            return rv\n\n        if (rv.exp < 0) == True:\n            return rv\n(C)             return rv\n\n        if (rv.exp < 0) == True:\n            return rv\n        if (rv.exp > max) == True:\n            return rv\n        if rv.exp == 2:\n            return h(g(rv.base.args[0])**2)\n(D)         if (rv.exp < 0) == True:\n            return rv\n        if (rv.exp > max) == True:\n            return rv\n        if rv.exp == 2:\n            return h(g(rv.base.args[0])**2)\n        else:\n            if rv.exp == 4:", "answer": "A"}
{"uuid": "972bb8e6-9643-455e-894d-67ff4814adce", "issue_statement": "Exception when multiplying BlockMatrix containing ZeroMatrix blocks\nWhen a block matrix with zero blocks is defined\r\n\r\n```\r\n>>> from sympy import *\r\n>>> a = MatrixSymbol(\"a\", 2, 2)\r\n>>> z = ZeroMatrix(2, 2)\r\n>>> b = BlockMatrix([[a, z], [z, z]])\r\n```\r\n\r\nthen block-multiplying it once seems to work fine:\r\n\r\n```\r\n>>> block_collapse(b * b)\r\nMatrix([\r\n[a**2, 0],\r\n[0, 0]])\r\n>>> b._blockmul(b)\r\nMatrix([\r\n[a**2, 0],\r\n[0, 0]])\r\n```\r\n\r\nbut block-multiplying twice throws an exception:\r\n\r\n```\r\n>>> block_collapse(b * b * b)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/matrices/expressions/blockmatrix.py\", line 297, in block_collapse\r\n    result = rule(expr)\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/strategies/core.py\", line 11, in exhaustive_rl\r\n    new, old = rule(expr), expr\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/strategies/core.py\", line 44, in chain_rl\r\n    expr = rule(expr)\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/strategies/core.py\", line 11, in exhaustive_rl\r\n    new, old = rule(expr), expr\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/strategies/core.py\", line 33, in conditioned_rl\r\n    return rule(expr)\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/strategies/core.py\", line 95, in switch_rl\r\n    return rl(expr)\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/matrices/expressions/blockmatrix.py\", line 361, in bc_matmul\r\n    matrices[i] = A._blockmul(B)\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/matrices/expressions/blockmatrix.py\", line 91, in _blockmul\r\n    self.colblocksizes == other.rowblocksizes):\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/matrices/expressions/blockmatrix.py\", line 80, in colblocksizes\r\n    return [self.blocks[0, i].cols for i in range(self.blockshape[1])]\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/matrices/expressions/blockmatrix.py\", line 80, in <listcomp>\r\n    return [self.blocks[0, i].cols for i in range(self.blockshape[1])]\r\nAttributeError: 'Zero' object has no attribute 'cols'\r\n>>> b._blockmul(b)._blockmul(b)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/matrices/expressions/blockmatrix.py\", line 91, in _blockmul\r\n    self.colblocksizes == other.rowblocksizes):\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/matrices/expressions/blockmatrix.py\", line 80, in colblocksizes\r\n    return [self.blocks[0, i].cols for i in range(self.blockshape[1])]\r\n  File \"/home/jan/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sympy/matrices/expressions/blockmatrix.py\", line 80, in <listcomp>\r\n    return [self.blocks[0, i].cols for i in range(self.blockshape[1])]\r\nAttributeError: 'Zero' object has no attribute 'cols'\r\n```\r\n\r\nThis seems to be caused by the fact that the zeros in `b._blockmul(b)` are not `ZeroMatrix` but `Zero`:\r\n\r\n```\r\n>>> type(b._blockmul(b).blocks[0, 1])\r\n<class 'sympy.core.numbers.Zero'>\r\n```\r\n\r\nHowever, I don't understand SymPy internals well enough to find out why this happens. I use Python 3.7.4 and sympy 1.4 (installed with pip).", "choices": "(A) \n        return mat_class(cls._from_args(nonmatrices), *matrices).doit(deep=False)\n    return _postprocessor\n\n\nBasic._constructor_postprocessor_mapping[MatrixExpr] = {\n    \"Mul\": [get_postprocessor(Mul)],\n    \"Add\": [get_postprocessor(Add)],\n(B)     return _postprocessor\n\n\nBasic._constructor_postprocessor_mapping[MatrixExpr] = {\n    \"Mul\": [get_postprocessor(Mul)],\n    \"Add\": [get_postprocessor(Add)],\n}\n\n(C)                 # manipulate them like non-commutative scalars.\n                return cls._from_args(nonmatrices + [mat_class(*matrices).doit(deep=False)])\n\n        return mat_class(cls._from_args(nonmatrices), *matrices).doit(deep=False)\n    return _postprocessor\n\n\nBasic._constructor_postprocessor_mapping[MatrixExpr] = {\n(D)                 # raising an exception. That way different algorithms can\n                # replace matrix expressions with non-commutative symbols to\n                # manipulate them like non-commutative scalars.\n                return cls._from_args(nonmatrices + [mat_class(*matrices).doit(deep=False)])\n\n        return mat_class(cls._from_args(nonmatrices), *matrices).doit(deep=False)\n    return _postprocessor\n", "answer": "C"}
{"uuid": "748a50f7-ced7-4e63-936a-69720261b326", "issue_statement": "Unexpected exception when multiplying geometry.Point and number\n```python\r\nfrom sympy import geometry as ge\r\nimport sympy\r\n\r\npoint1 = ge.Point(0,0)\r\npoint2 = ge.Point(1,1)\r\n```\r\n\r\nThis line works fine\r\n```python\r\npoint1 + point2 * sympy.sympify(2.0)\r\n```\r\n\r\nBut when I write the same this way it raises an exception\r\n```python\r\npoint1 + sympy.sympify(2.0) * point2\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n~/.virtualenvs/test/lib/python3.6/site-packages/sympy/geometry/point.py in __add__(self, other)\r\n    219         try:\r\n--> 220             s, o = Point._normalize_dimension(self, Point(other, evaluate=False))\r\n    221         except TypeError:\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/sympy/geometry/point.py in __new__(cls, *args, **kwargs)\r\n    128                 Expecting sequence of coordinates, not `{}`'''\r\n--> 129                                        .format(func_name(coords))))\r\n    130         # A point where only `dim` is specified is initialized\r\n\r\nTypeError: \r\nExpecting sequence of coordinates, not `Mul`\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nGeometryError                             Traceback (most recent call last)\r\n<ipython-input-20-6dcbddac1ee2> in <module>\r\n----> 1 point1 + sympy.sympify(2.0)* point2\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/sympy/geometry/point.py in __add__(self, other)\r\n    220             s, o = Point._normalize_dimension(self, Point(other, evaluate=False))\r\n    221         except TypeError:\r\n--> 222             raise GeometryError(\"Don't know how to add {} and a Point object\".format(other))\r\n    223 \r\n    224         coords = [simplify(a + b) for a, b in zip(s, o)]\r\n\r\nGeometryError: Don't know how to add 2.0*Point2D(1, 1) and a Point object\r\n```\r\n\r\nThe expected behaviour is, that both lines give the same result", "choices": "(A)         sympy.geometry.point.Point.scale\n        \"\"\"\n        factor = sympify(factor)\n        coords = [simplify(x*factor) for x in self.args]\n        return Point(coords, evaluate=False)\n\n    def __neg__(self):\n        \"\"\"Negate the point.\"\"\"\n        coords = [-x for x in self.args]\n        return Point(coords, evaluate=False)\n(B)     def __neg__(self):\n        \"\"\"Negate the point.\"\"\"\n        coords = [-x for x in self.args]\n        return Point(coords, evaluate=False)\n\n    def __sub__(self, other):\n        \"\"\"Subtract two points, or subtract a factor from this point's\n        coordinates.\"\"\"\n        return self + [-x for x in other]\n\n(C)         coords = [simplify(x*factor) for x in self.args]\n        return Point(coords, evaluate=False)\n\n    def __neg__(self):\n        \"\"\"Negate the point.\"\"\"\n        coords = [-x for x in self.args]\n        return Point(coords, evaluate=False)\n\n    def __sub__(self, other):\n        \"\"\"Subtract two points, or subtract a factor from this point's\n(D)         coords = [-x for x in self.args]\n        return Point(coords, evaluate=False)\n\n    def __sub__(self, other):\n        \"\"\"Subtract two points, or subtract a factor from this point's\n        coordinates.\"\"\"\n        return self + [-x for x in other]\n\n    @classmethod\n    def _normalize_dimension(cls, *points, **kwargs):", "answer": "C"}
{"uuid": "85536f51-75d3-4fd5-9b64-f23ecdd2f8f2", "issue_statement": "Sympy incorrectly attempts to eval reprs in its __eq__ method\nPassing strings produced by unknown objects into eval is **very bad**. It is especially surprising for an equality check to trigger that kind of behavior. This should be fixed ASAP.\r\n\r\nRepro code:\r\n\r\n```\r\nimport sympy\r\nclass C:\r\n    def __repr__(self):\r\n        return 'x.y'\r\n_ = sympy.Symbol('x') == C()\r\n```\r\n\r\nResults in:\r\n\r\n```\r\nE   AttributeError: 'Symbol' object has no attribute 'y'\r\n```\r\n\r\nOn the line:\r\n\r\n```\r\n    expr = eval(\r\n        code, global_dict, local_dict)  # take local objects in preference\r\n```\r\n\r\nWhere code is:\r\n\r\n```\r\nSymbol ('x' ).y\r\n```\r\n\r\nFull trace:\r\n\r\n```\r\nFAILED                   [100%]\r\n        class C:\r\n            def __repr__(self):\r\n                return 'x.y'\r\n    \r\n>       _ = sympy.Symbol('x') == C()\r\n\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nsympy/core/expr.py:124: in __eq__\r\n    other = sympify(other)\r\nsympy/core/sympify.py:385: in sympify\r\n    expr = parse_expr(a, local_dict=locals, transformations=transformations, evaluate=evaluate)\r\nsympy/parsing/sympy_parser.py:1011: in parse_expr\r\n    return eval_expr(code, local_dict, global_dict)\r\nsympy/parsing/sympy_parser.py:906: in eval_expr\r\n    code, global_dict, local_dict)  # take local objects in preference\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n>   ???\r\nE   AttributeError: 'Symbol' object has no attribute 'y'\r\n\r\n<string>:1: AttributeError\r\n```\r\n\r\nRelated issue: an unknown object whose repr is `x` will incorrectly compare as equal to a sympy symbol x:\r\n\r\n```\r\n    class C:\r\n        def __repr__(self):\r\n            return 'x'\r\n\r\n    assert sympy.Symbol('x') != C()  # fails\r\n```", "choices": "(A)         try:\n            other = sympify(other)\n            if not isinstance(other, Expr):\n                return False\n        except (SympifyError, SyntaxError):\n            return False\n        # check for pure number expr\n(B) \n    def __eq__(self, other):\n        try:\n            other = sympify(other)\n            if not isinstance(other, Expr):\n                return False\n        except (SympifyError, SyntaxError):\n(C)             if not isinstance(other, Expr):\n                return False\n        except (SympifyError, SyntaxError):\n            return False\n        # check for pure number expr\n        if  not (self.is_Number and other.is_Number) and (\n                type(self) != type(other)):\n(D)         been defined by a class. See note about this in Basic.__eq__.\"\"\"\n        return self._args\n\n    def __eq__(self, other):\n        try:\n            other = sympify(other)\n            if not isinstance(other, Expr):", "answer": "B"}
{"uuid": "6929414e-e5e3-4a36-87f3-5ea9bfe76c54", "issue_statement": "Simplify of simple trig expression fails\ntrigsimp in various versions, including 1.5, incorrectly simplifies cos(x)+sqrt(sin(x)**2) as though it were cos(x)+sin(x) for general complex x. (Oddly it gets this right if x is real.)\r\n\r\nEmbarrassingly I found this by accident while writing sympy-based teaching material...", "choices": "(A)         values = [self.factors[k] for k in keys]\n        return hash((keys, values))\n\n    def __repr__(self):  # Factors\n        return \"Factors({%s})\" % ', '.join(\n            ['%s: %s' % (k, v) for k, v in ordered(self.factors.items())])\n\n    @property\n    def is_zero(self):  # Factors\n        \"\"\"\n        >>> from sympy.core.exprtools import Factors\n        >>> Factors(0).is_zero\n        True\n        \"\"\"\n        f = self.factors\n        return len(f) == 1 and S.Zero in f\n\n    @property\n    def is_one(self):  # Factors\n        \"\"\"\n        >>> from sympy.core.exprtools import Factors\n        >>> Factors(1).is_one\n        True\n        \"\"\"\n        return not self.factors\n\n    def as_expr(self):  # Factors\n        \"\"\"Return the underlying expression.\n\n        Examples\n        ========\n\n        >>> from sympy.core.exprtools import Factors\n        >>> from sympy.abc import x, y\n        >>> Factors((x*y**2).as_powers_dict()).as_expr()\n        x*y**2\n\n        \"\"\"\n\n        args = []\n        for factor, exp in self.factors.items():\n            if exp != 1:\n                b, e = factor.as_base_exp()\n                if isinstance(exp, int):\n                    e = _keep_coeff(Integer(exp), e)\n                elif isinstance(exp, Rational):\n                    e = _keep_coeff(exp, e)\n                else:\n                    e *= exp\n                args.append(b**e)\n            else:\n                args.append(factor)\n        return Mul(*args)\n\n    def mul(self, other):  # Factors\n        \"\"\"Return Factors of ``self * other``.\n\n        Examples\n        ========\n\n        >>> from sympy.core.exprtools import Factors\n        >>> from sympy.abc import x, y, z\n        >>> a = Factors((x*y**2).as_powers_dict())\n        >>> b = Factors((x*y/z).as_powers_dict())\n        >>> a.mul(b)\n        Factors({x: 2, y: 3, z: -1})\n        >>> a*b\n        Factors({x: 2, y: 3, z: -1})\n        \"\"\"\n        if not isinstance(other, Factors):\n            other = Factors(other)\n        if any(f.is_zero for f in (self, other)):\n            return Factors(S.Zero)\n        factors = dict(self.factors)\n\n        for factor, exp in other.factors.items():\n            if factor in factors:\n                exp = factors[factor] + exp\n\n                if not exp:\n                    del factors[factor]\n                    continue\n\n            factors[factor] = exp\n\n        return Factors(factors)\n\n    def normal(self, other):\n        \"\"\"Return ``self`` and ``other`` with ``gcd`` removed from each.\n        The only differences between this and method ``div`` is that this\n        is 1) optimized for the case when there are few factors in common and\n        2) this does not raise an error if ``other`` is zero.\n\n        See Also\n        ========\n        div\n\n        \"\"\"\n        if not isinstance(other, Factors):\n            other = Factors(other)\n            if other.is_zero:\n                return (Factors(), Factors(S.Zero))\n(B)             for f in list(factors.keys()):\n                if isinstance(f, Rational) and not isinstance(f, Integer):\n                    p, q = Integer(f.p), Integer(f.q)\n                    factors[p] = (factors[p] if p in factors else 0) + factors[f]\n                    factors[q] = (factors[q] if q in factors else 0) - factors[f]\n                    factors.pop(f)\n            if i:\n                factors[I] = S.One*i\n            if nc:\n                factors[Mul(*nc, evaluate=False)] = S.One\n        else:\n            factors = factors.copy()  # /!\\ should be dict-like\n\n            # tidy up -/+1 and I exponents if Rational\n\n            handle = []\n            for k in factors:\n                if k is I or k in (-1, 1):\n                    handle.append(k)\n            if handle:\n                i1 = S.One\n                for k in handle:\n                    if not _isnumber(factors[k]):\n                        continue\n                    i1 *= k**factors.pop(k)\n                if i1 is not S.One:\n                    for a in i1.args if i1.is_Mul else [i1]:  # at worst, -1.0*I*(-1)**e\n                        if a is S.NegativeOne:\n                            factors[a] = S.One\n                        elif a is I:\n                            factors[I] = S.One\n                        elif a.is_Pow:\n                            if S.NegativeOne not in factors:\n                                factors[S.NegativeOne] = S.Zero\n                            factors[S.NegativeOne] += a.exp\n                        elif a == 1:\n                            factors[a] = S.One\n                        elif a == -1:\n                            factors[-a] = S.One\n                            factors[S.NegativeOne] = S.One\n                        else:\n                            raise ValueError('unexpected factor in i1: %s' % a)\n\n        self.factors = factors\n        keys = getattr(factors, 'keys', None)\n        if keys is None:\n            raise TypeError('expecting Expr or dictionary')\n        self.gens = frozenset(keys())\n\n    def __hash__(self):  # Factors\n        keys = tuple(ordered(self.factors.keys()))\n        values = [self.factors[k] for k in keys]\n        return hash((keys, values))\n\n    def __repr__(self):  # Factors\n        return \"Factors({%s})\" % ', '.join(\n            ['%s: %s' % (k, v) for k, v in ordered(self.factors.items())])\n\n    @property\n    def is_zero(self):  # Factors\n        \"\"\"\n        >>> from sympy.core.exprtools import Factors\n        >>> Factors(0).is_zero\n        True\n        \"\"\"\n        f = self.factors\n        return len(f) == 1 and S.Zero in f\n\n    @property\n    def is_one(self):  # Factors\n        \"\"\"\n        >>> from sympy.core.exprtools import Factors\n        >>> Factors(1).is_one\n        True\n        \"\"\"\n        return not self.factors\n\n    def as_expr(self):  # Factors\n        \"\"\"Return the underlying expression.\n\n        Examples\n        ========\n\n        >>> from sympy.core.exprtools import Factors\n        >>> from sympy.abc import x, y\n        >>> Factors((x*y**2).as_powers_dict()).as_expr()\n        x*y**2\n\n        \"\"\"\n\n        args = []\n        for factor, exp in self.factors.items():\n            if exp != 1:\n                b, e = factor.as_base_exp()\n                if isinstance(exp, int):\n                    e = _keep_coeff(Integer(exp), e)\n                elif isinstance(exp, Rational):\n                    e = _keep_coeff(exp, e)\n                else:\n                    e *= exp\n                args.append(b**e)\n            else:\n(C)             n = factors\n            factors = {}\n            if n < 0:\n                factors[S.NegativeOne] = S.One\n                n = -n\n            if n is not S.One:\n                if n.is_Float or n.is_Integer or n is S.Infinity:\n                    factors[n] = S.One\n                elif n.is_Rational:\n                    # since we're processing Numbers, the denominator is\n                    # stored with a negative exponent; all other factors\n                    # are left .\n                    if n.p != 1:\n                        factors[Integer(n.p)] = S.One\n                    factors[Integer(n.q)] = S.NegativeOne\n                else:\n                    raise ValueError('Expected Float|Rational|Integer, not %s' % n)\n        elif isinstance(factors, Basic) and not factors.args:\n            factors = {factors: S.One}\n        elif isinstance(factors, Expr):\n            c, nc = factors.args_cnc()\n            i = c.count(I)\n            for _ in range(i):\n                c.remove(I)\n            factors = dict(Mul._from_args(c).as_powers_dict())\n            # Handle all rational Coefficients\n            for f in list(factors.keys()):\n                if isinstance(f, Rational) and not isinstance(f, Integer):\n                    p, q = Integer(f.p), Integer(f.q)\n                    factors[p] = (factors[p] if p in factors else 0) + factors[f]\n                    factors[q] = (factors[q] if q in factors else 0) - factors[f]\n                    factors.pop(f)\n            if i:\n                factors[I] = S.One*i\n            if nc:\n                factors[Mul(*nc, evaluate=False)] = S.One\n        else:\n            factors = factors.copy()  # /!\\ should be dict-like\n\n            # tidy up -/+1 and I exponents if Rational\n\n            handle = []\n            for k in factors:\n                if k is I or k in (-1, 1):\n                    handle.append(k)\n            if handle:\n                i1 = S.One\n                for k in handle:\n                    if not _isnumber(factors[k]):\n                        continue\n                    i1 *= k**factors.pop(k)\n                if i1 is not S.One:\n                    for a in i1.args if i1.is_Mul else [i1]:  # at worst, -1.0*I*(-1)**e\n                        if a is S.NegativeOne:\n                            factors[a] = S.One\n                        elif a is I:\n                            factors[I] = S.One\n                        elif a.is_Pow:\n                            if S.NegativeOne not in factors:\n                                factors[S.NegativeOne] = S.Zero\n                            factors[S.NegativeOne] += a.exp\n                        elif a == 1:\n                            factors[a] = S.One\n                        elif a == -1:\n                            factors[-a] = S.One\n                            factors[S.NegativeOne] = S.One\n                        else:\n                            raise ValueError('unexpected factor in i1: %s' % a)\n\n        self.factors = factors\n        keys = getattr(factors, 'keys', None)\n        if keys is None:\n            raise TypeError('expecting Expr or dictionary')\n        self.gens = frozenset(keys())\n\n    def __hash__(self):  # Factors\n        keys = tuple(ordered(self.factors.keys()))\n        values = [self.factors[k] for k in keys]\n        return hash((keys, values))\n\n    def __repr__(self):  # Factors\n        return \"Factors({%s})\" % ', '.join(\n            ['%s: %s' % (k, v) for k, v in ordered(self.factors.items())])\n\n    @property\n    def is_zero(self):  # Factors\n        \"\"\"\n        >>> from sympy.core.exprtools import Factors\n        >>> Factors(0).is_zero\n        True\n        \"\"\"\n        f = self.factors\n        return len(f) == 1 and S.Zero in f\n\n    @property\n    def is_one(self):  # Factors\n        \"\"\"\n        >>> from sympy.core.exprtools import Factors\n        >>> Factors(1).is_one\n        True\n        \"\"\"\n        return not self.factors\n(D)                     for a in i1.args if i1.is_Mul else [i1]:  # at worst, -1.0*I*(-1)**e\n                        if a is S.NegativeOne:\n                            factors[a] = S.One\n                        elif a is I:\n                            factors[I] = S.One\n                        elif a.is_Pow:\n                            if S.NegativeOne not in factors:\n                                factors[S.NegativeOne] = S.Zero\n                            factors[S.NegativeOne] += a.exp\n                        elif a == 1:\n                            factors[a] = S.One\n                        elif a == -1:\n                            factors[-a] = S.One\n                            factors[S.NegativeOne] = S.One\n                        else:\n                            raise ValueError('unexpected factor in i1: %s' % a)\n\n        self.factors = factors\n        keys = getattr(factors, 'keys', None)\n        if keys is None:\n            raise TypeError('expecting Expr or dictionary')\n        self.gens = frozenset(keys())\n\n    def __hash__(self):  # Factors\n        keys = tuple(ordered(self.factors.keys()))\n        values = [self.factors[k] for k in keys]\n        return hash((keys, values))\n\n    def __repr__(self):  # Factors\n        return \"Factors({%s})\" % ', '.join(\n            ['%s: %s' % (k, v) for k, v in ordered(self.factors.items())])\n\n    @property\n    def is_zero(self):  # Factors\n        \"\"\"\n        >>> from sympy.core.exprtools import Factors\n        >>> Factors(0).is_zero\n        True\n        \"\"\"\n        f = self.factors\n        return len(f) == 1 and S.Zero in f\n\n    @property\n    def is_one(self):  # Factors\n        \"\"\"\n        >>> from sympy.core.exprtools import Factors\n        >>> Factors(1).is_one\n        True\n        \"\"\"\n        return not self.factors\n\n    def as_expr(self):  # Factors\n        \"\"\"Return the underlying expression.\n\n        Examples\n        ========\n\n        >>> from sympy.core.exprtools import Factors\n        >>> from sympy.abc import x, y\n        >>> Factors((x*y**2).as_powers_dict()).as_expr()\n        x*y**2\n\n        \"\"\"\n\n        args = []\n        for factor, exp in self.factors.items():\n            if exp != 1:\n                b, e = factor.as_base_exp()\n                if isinstance(exp, int):\n                    e = _keep_coeff(Integer(exp), e)\n                elif isinstance(exp, Rational):\n                    e = _keep_coeff(exp, e)\n                else:\n                    e *= exp\n                args.append(b**e)\n            else:\n                args.append(factor)\n        return Mul(*args)\n\n    def mul(self, other):  # Factors\n        \"\"\"Return Factors of ``self * other``.\n\n        Examples\n        ========\n\n        >>> from sympy.core.exprtools import Factors\n        >>> from sympy.abc import x, y, z\n        >>> a = Factors((x*y**2).as_powers_dict())\n        >>> b = Factors((x*y/z).as_powers_dict())\n        >>> a.mul(b)\n        Factors({x: 2, y: 3, z: -1})\n        >>> a*b\n        Factors({x: 2, y: 3, z: -1})\n        \"\"\"\n        if not isinstance(other, Factors):\n            other = Factors(other)\n        if any(f.is_zero for f in (self, other)):\n            return Factors(S.Zero)\n        factors = dict(self.factors)\n\n        for factor, exp in other.factors.items():\n            if factor in factors:", "answer": "B"}
{"uuid": "7d734ad6-0f64-4b07-afe0-0b898df85887", "issue_statement": "diophantine: incomplete results depending on syms order with permute=True\n```\r\nIn [10]: diophantine(n**4 + m**4 - 2**4 - 3**4, syms=(m,n), permute=True)\r\nOut[10]: {(-3, -2), (-3, 2), (-2, -3), (-2, 3), (2, -3), (2, 3), (3, -2), (3, 2)}\r\n\r\nIn [11]: diophantine(n**4 + m**4 - 2**4 - 3**4, syms=(n,m), permute=True)\r\nOut[11]: {(3, 2)}\r\n```\r\n\ndiophantine: incomplete results depending on syms order with permute=True\n```\r\nIn [10]: diophantine(n**4 + m**4 - 2**4 - 3**4, syms=(m,n), permute=True)\r\nOut[10]: {(-3, -2), (-3, 2), (-2, -3), (-2, 3), (2, -3), (2, 3), (3, -2), (3, 2)}\r\n\r\nIn [11]: diophantine(n**4 + m**4 - 2**4 - 3**4, syms=(n,m), permute=True)\r\nOut[11]: {(3, 2)}\r\n```", "choices": "(A)                 return {tuple([t[dict_sym_index[i]] for i in var])\n                            for t in diophantine(eq, param)}\n        n, d = eq.as_numer_denom()\n        if n.is_number:\n            return set()\n        if not d.is_number:\n            dsol = diophantine(d)\n(B)         n, d = eq.as_numer_denom()\n        if n.is_number:\n            return set()\n        if not d.is_number:\n            dsol = diophantine(d)\n            good = diophantine(n) - dsol\n            return {s for s in good if _mexpand(d.subs(zip(var, s)))}\n(C)             if syms != var:\n                dict_sym_index = dict(zip(syms, range(len(syms))))\n                return {tuple([t[dict_sym_index[i]] for i in var])\n                            for t in diophantine(eq, param)}\n        n, d = eq.as_numer_denom()\n        if n.is_number:\n            return set()\n(D)                     'syms should be given as a sequence, e.g. a list')\n            syms = [i for i in syms if i in var]\n            if syms != var:\n                dict_sym_index = dict(zip(syms, range(len(syms))))\n                return {tuple([t[dict_sym_index[i]] for i in var])\n                            for t in diophantine(eq, param)}\n        n, d = eq.as_numer_denom()", "answer": "C"}
{"uuid": "36e1fa8c-0ba2-494a-b415-2a658cecd9ac", "issue_statement": "nthroot_mod function misses one root of x = 0 mod p.\nWhen in the equation x**n = a mod p , when a % p == 0. Then x = 0 mod p is also a root of this equation. But right now `nthroot_mod` does not check for this condition. `nthroot_mod(17*17, 5 , 17)` has a root `0 mod 17`. But it does not return it.", "choices": "(A)         adm = pow(adm, 2**(s - 1 - i), p)\n        if adm % p == p - 1:\n            m += 2**i\n    #assert A*pow(D, m, p) % p == 1\n    x = pow(a, (t + 1)//2, p)*pow(D, m//2, p) % p\n    return x\n\n\ndef sqrt_mod(a, p, all_roots=False):\n    \"\"\"\n    Find a root of ``x**2 = a mod p``\n\n    Parameters\n    ==========\n\n    a : integer\n    p : positive integer\n    all_roots : if True the list of roots is returned or None\n\n    Notes\n    =====\n\n    If there is no root it is returned None; else the returned root\n    is less or equal to ``p // 2``; in general is not the smallest one.\n    It is returned ``p // 2`` only if it is the only root.\n\n    Use ``all_roots`` only when it is expected that all the roots fit\n    in memory; otherwise use ``sqrt_mod_iter``.\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory import sqrt_mod\n    >>> sqrt_mod(11, 43)\n    21\n    >>> sqrt_mod(17, 32, True)\n    [7, 9, 23, 25]\n    \"\"\"\n    if all_roots:\n        return sorted(list(sqrt_mod_iter(a, p)))\n    try:\n        p = abs(as_int(p))\n        it = sqrt_mod_iter(a, p)\n        r = next(it)\n        if r > p // 2:\n            return p - r\n        elif r < p // 2:\n            return r\n        else:\n            try:\n                r = next(it)\n                if r > p // 2:\n                    return p - r\n            except StopIteration:\n                pass\n            return r\n    except StopIteration:\n        return None\n\n\ndef _product(*iters):\n    \"\"\"\n    Cartesian product generator\n\n    Notes\n    =====\n\n    Unlike itertools.product, it works also with iterables which do not fit\n    in memory. See http://bugs.python.org/issue10109\n\n    Author: Fernando Sumudu\n    with small changes\n    \"\"\"\n    import itertools\n    inf_iters = tuple(itertools.cycle(enumerate(it)) for it in iters)\n    num_iters = len(inf_iters)\n    cur_val = [None]*num_iters\n\n    first_v = True\n    while True:\n        i, p = 0, num_iters\n        while p and not i:\n            p -= 1\n            i, cur_val[p] = next(inf_iters[p])\n\n        if not p and not i:\n            if first_v:\n                first_v = False\n            else:\n                break\n\n        yield cur_val\n\n\ndef sqrt_mod_iter(a, p, domain=int):\n    \"\"\"\n    Iterate over solutions to ``x**2 = a mod p``\n\n    Parameters\n    ==========\n\n    a : integer\n    p : positive integer\n    domain : integer domain, ``int``, ``ZZ`` or ``Integer``\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import sqrt_mod_iter\n    >>> list(sqrt_mod_iter(11, 43))\n    [21, 22]\n    \"\"\"\n    from sympy.polys.galoistools import gf_crt1, gf_crt2\n    from sympy.polys.domains import ZZ\n    a, p = as_int(a), abs(as_int(p))\n    if isprime(p):\n        a = a % p\n        if a == 0:\n            res = _sqrt_mod1(a, p, 1)\n        else:\n            res = _sqrt_mod_prime_power(a, p, 1)\n        if res:\n            if domain is ZZ:\n                for x in res:\n                    yield x\n            else:\n                for x in res:\n                    yield domain(x)\n    else:\n        f = factorint(p)\n        v = []\n        pv = []\n        for px, ex in f.items():\n            if a % px == 0:\n                rx = _sqrt_mod1(a, px, ex)\n                if not rx:\n                    return\n            else:\n                rx = _sqrt_mod_prime_power(a, px, ex)\n                if not rx:\n                    return\n            v.append(rx)\n            pv.append(px**ex)\n        mm, e, s = gf_crt1(pv, ZZ)\n        if domain is ZZ:\n            for vx in _product(*v):\n                r = gf_crt2(vx, pv, mm, e, s, ZZ)\n                yield r\n        else:\n            for vx in _product(*v):\n                r = gf_crt2(vx, pv, mm, e, s, ZZ)\n                yield domain(r)\n\n\ndef _sqrt_mod_prime_power(a, p, k):\n    \"\"\"\n    Find the solutions to ``x**2 = a mod p**k`` when ``a % p != 0``\n\n    Parameters\n    ==========\n\n    a : integer\n    p : prime number\n    k : positive integer\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import _sqrt_mod_prime_power\n    >>> _sqrt_mod_prime_power(11, 43, 1)\n    [21, 22]\n\n    References\n    ==========\n\n    .. [1] P. Hackman \"Elementary Number Theory\" (2009), page 160\n    .. [2] http://www.numbertheory.org/php/squareroot.html\n    .. [3] [Gathen99]_\n    \"\"\"\n    from sympy.core.numbers import igcdex\n    from sympy.polys.domains import ZZ\n\n    pk = p**k\n    a = a % pk\n\n    if k == 1:\n        if p == 2:\n            return [ZZ(a)]\n        if not (a % p < 2 or pow(a, (p - 1) // 2, p) == 1):\n            return None\n\n        if p % 4 == 3:\n            res = pow(a, (p + 1) // 4, p)\n        elif p % 8 == 5:\n            sign = pow(a, (p - 1) // 4, p)\n            if sign == 1:\n                res = pow(a, (p + 3) // 8, p)\n            else:\n                b = pow(4*a, (p - 5) // 8, p)\n                x =  (2*a*b) % p\n                if pow(x, 2, p) == a:\n                    res = x\n        else:\n            res = _sqrt_mod_tonelli_shanks(a, p)\n\n        # ``_sqrt_mod_tonelli_shanks(a, p)`` is not deterministic;\n        # sort to get always the same result\n        return sorted([ZZ(res), ZZ(p - res)])\n\n    if k > 1:\n        # see Ref.[2]\n        if p == 2:\n            if a % 8 != 1:\n                return None\n            if k <= 3:\n               s = set()\n               for i in range(0, pk, 4):\n                    s.add(1 + i)\n                    s.add(-1 + i)\n               return list(s)\n            # according to Ref.[2] for k > 2 there are two solutions\n            # (mod 2**k-1), that is four solutions (mod 2**k), which can be\n            # obtained from the roots of x**2 = 0 (mod 8)\n            rv = [ZZ(1), ZZ(3), ZZ(5), ZZ(7)]\n            # hensel lift them to solutions of x**2 = 0 (mod 2**k)\n            # if r**2 - a = 0 mod 2**nx but not mod 2**(nx+1)\n            # then r + 2**(nx - 1) is a root mod 2**(nx+1)\n            n = 3\n            res = []\n            for r in rv:\n                nx = n\n                while nx < k:\n                    r1 = (r**2 - a) >> nx\n                    if r1 % 2:\n                        r = r + (1 << (nx - 1))\n                    #assert (r**2 - a)% (1 << (nx + 1)) == 0\n                    nx += 1\n                if r not in res:\n                    res.append(r)\n                x = r + (1 << (k - 1))\n                #assert (x**2 - a) % pk == 0\n                if x < (1 << nx) and x not in res:\n                    if (x**2 - a) % pk == 0:\n                        res.append(x)\n            return res\n        rv = _sqrt_mod_prime_power(a, p, 1)\n        if not rv:\n            return None\n        r = rv[0]\n        fr = r**2 - a\n        # hensel lifting with Newton iteration, see Ref.[3] chapter 9\n        # with f(x) = x**2 - a; one has f'(a) != 0 (mod p) for p != 2\n        n = 1\n        px = p\n        while 1:\n            n1 = n\n            n1 *= 2\n            if n1 > k:\n                break\n            n = n1\n            px = px**2\n            frinv = igcdex(2*r, px)[0]\n            r = (r - fr*frinv) % px\n            fr = r**2 - a\n        if n < k:\n            px = p**k\n            frinv = igcdex(2*r, px)[0]\n            r = (r - fr*frinv) % px\n        return [r, px - r]\n\n\ndef _sqrt_mod1(a, p, n):\n    \"\"\"\n    Find solution to ``x**2 == a mod p**n`` when ``a % p == 0``\n\n    see http://www.numbertheory.org/php/squareroot.html\n    \"\"\"\n    pn = p**n\n    a = a % pn\n    if a == 0:\n        # case gcd(a, p**k) = p**n\n        m = n // 2\n        if n % 2 == 1:\n            pm1 = p**(m + 1)\n            def _iter0a():\n                i = 0\n                while i < pn:\n                    yield i\n                    i += pm1\n            return _iter0a()\n        else:\n            pm = p**m\n            def _iter0b():\n                i = 0\n                while i < pn:\n                    yield i\n                    i += pm\n            return _iter0b()\n\n    # case gcd(a, p**k) = p**r, r < n\n    f = factorint(a)\n    r = f[p]\n    if r % 2 == 1:\n        return None\n    m = r // 2\n    a1 = a >> r\n    if p == 2:\n        if n - r == 1:\n            pnm1 = 1 << (n - m + 1)\n            pm1 = 1 << (m + 1)\n            def _iter1():\n                k = 1 << (m + 2)\n                i = 1 << m\n                while i < pnm1:\n                    j = i\n                    while j < pn:\n                        yield j\n                        j += k\n                    i += pm1\n            return _iter1()\n        if n - r == 2:\n            res = _sqrt_mod_prime_power(a1, p, n - r)\n            if res is None:\n                return None\n            pnm = 1 << (n - m)\n            def _iter2():\n                s = set()\n                for r in res:\n                    i = 0\n                    while i < pn:\n                        x = (r << m) + i\n                        if x not in s:\n                            s.add(x)\n                            yield x\n                        i += pnm\n            return _iter2()\n        if n - r > 2:\n            res = _sqrt_mod_prime_power(a1, p, n - r)\n            if res is None:\n                return None\n            pnm1 = 1 << (n - m - 1)\n            def _iter3():\n                s = set()\n                for r in res:\n                    i = 0\n                    while i < pn:\n                        x = ((r << m) + i) % pn\n                        if x not in s:\n                            s.add(x)\n                            yield x\n                        i += pnm1\n            return _iter3()\n    else:\n        m = r // 2\n        a1 = a // p**r\n        res1 = _sqrt_mod_prime_power(a1, p, n - r)\n        if res1 is None:\n            return None\n        pm = p**m\n        pnr = p**(n-r)\n        pnm = p**(n-m)\n\n        def _iter4():\n            s = set()\n            pm = p**m\n            for rx in res1:\n                i = 0\n                while i < pnm:\n                    x = ((rx + i) % pn)\n                    if x not in s:\n                        s.add(x)\n                        yield x*pm\n                    i += pnr\n        return _iter4()\n\n\ndef is_quad_residue(a, p):\n    \"\"\"\n    Returns True if ``a`` (mod ``p``) is in the set of squares mod ``p``,\n    i.e a % p in set([i**2 % p for i in range(p)]). If ``p`` is an odd\n    prime, an iterative method is used to make the determination:\n\n    >>> from sympy.ntheory import is_quad_residue\n    >>> sorted(set([i**2 % 7 for i in range(7)]))\n    [0, 1, 2, 4]\n    >>> [j for j in range(7) if is_quad_residue(j, 7)]\n    [0, 1, 2, 4]\n\n    See Also\n    ========\n\n    legendre_symbol, jacobi_symbol\n    \"\"\"\n    a, p = as_int(a), as_int(p)\n    if p < 1:\n        raise ValueError('p must be > 0')\n    if a >= p or a < 0:\n        a = a % p\n    if a < 2 or p < 3:\n        return True\n    if not isprime(p):\n        if p % 2 and jacobi_symbol(a, p) == -1:\n            return False\n        r = sqrt_mod(a, p)\n        if r is None:\n            return False\n        else:\n            return True\n\n    return pow(a, (p - 1) // 2, p) == 1\n\n\ndef is_nthpow_residue(a, n, m):\n    \"\"\"\n    Returns True if ``x**n == a (mod m)`` has solutions.\n\n    References\n    ==========\n\n    .. [1] P. Hackman \"Elementary Number Theory\" (2009), page 76\n\n    \"\"\"\n    a, n, m = as_int(a), as_int(n), as_int(m)\n    if m <= 0:\n        raise ValueError('m must be > 0')\n    if n < 0:\n        raise ValueError('n must be >= 0')\n    if a < 0:\n        raise ValueError('a must be >= 0')\n    if n == 0:\n        if m == 1:\n            return False\n        return a == 1\n    if a % m == 0:\n        return True\n    if n == 1:\n        return True\n    if n == 2:\n        return is_quad_residue(a, m)\n    return _is_nthpow_residue_bign(a, n, m)\n\n\ndef _is_nthpow_residue_bign(a, n, m):\n    \"\"\"Returns True if ``x**n == a (mod m)`` has solutions for n > 2.\"\"\"\n    # assert n > 2\n    # assert a > 0 and m > 0\n    if primitive_root(m) is None:\n        # assert m >= 8\n        for prime, power in factorint(m).items():\n            if not _is_nthpow_residue_bign_prime_power(a, n, prime, power):\n                return False\n        return True\n    f = totient(m)\n    k = f // igcd(f, n)\n    return pow(a, k, m) == 1\n\n\ndef _is_nthpow_residue_bign_prime_power(a, n, p, k):\n    \"\"\"Returns True/False if a solution for ``x**n == a (mod(p**k))``\n    does/doesn't exist.\"\"\"\n    # assert a > 0\n    # assert n > 2\n    # assert p is prime\n    # assert k > 0\n    if a % p:\n        if p != 2:\n            return _is_nthpow_residue_bign(a, n, pow(p, k))\n        if n & 1:\n            return True\n        c = trailing(n)\n        return a % pow(2, min(c + 2, k)) == 1\n    else:\n        a %= pow(p, k)\n        if not a:\n            return True\n        mu = multiplicity(p, a)\n        if mu % n:\n            return False\n        pm = pow(p, mu)\n        return _is_nthpow_residue_bign_prime_power(a//pm, n, p, k - mu)\n\n\ndef _nthroot_mod2(s, q, p):\n    f = factorint(q)\n    v = []\n    for b, e in f.items():\n        v.extend([b]*e)\n    for qx in v:\n        s = _nthroot_mod1(s, qx, p, False)\n    return s\n\n\ndef _nthroot_mod1(s, q, p, all_roots):\n    \"\"\"\n    Root of ``x**q = s mod p``, ``p`` prime and ``q`` divides ``p - 1``\n\n    References\n    ==========\n\n    .. [1] A. M. Johnston \"A Generalized qth Root Algorithm\"\n\n    \"\"\"\n    g = primitive_root(p)\n    if not isprime(q):\n        r = _nthroot_mod2(s, q, p)\n    else:\n        f = p - 1\n        assert (p - 1) % q == 0\n        # determine k\n        k = 0\n        while f % q == 0:\n            k += 1\n            f = f // q\n        # find z, x, r1\n        f1 = igcdex(-f, q)[0] % q\n        z = f*f1\n        x = (1 + z) // q\n        r1 = pow(s, x, p)\n        s1 = pow(s, f, p)\n        h = pow(g, f*q, p)\n        t = discrete_log(p, s1, h)\n        g2 = pow(g, z*t, p)\n        g3 = igcdex(g2, p)[0]\n        r = r1*g3 % p\n        #assert pow(r, q, p) == s\n    res = [r]\n    h = pow(g, (p - 1) // q, p)\n    #assert pow(h, q, p) == 1\n    hx = r\n    for i in range(q - 1):\n        hx = (hx*h) % p\n        res.append(hx)\n    if all_roots:\n        res.sort()\n        return res\n    return min(res)\n\n\ndef nthroot_mod(a, n, p, all_roots=False):\n    \"\"\"\n    Find the solutions to ``x**n = a mod p``\n\n    Parameters\n    ==========\n\n    a : integer\n    n : positive integer\n    p : positive integer\n    all_roots : if False returns the smallest root, else the list of roots\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import nthroot_mod\n    >>> nthroot_mod(11, 4, 19)\n    8\n    >>> nthroot_mod(11, 4, 19, True)\n    [8, 11]\n    >>> nthroot_mod(68, 3, 109)\n    23\n    \"\"\"\n    from sympy.core.numbers import igcdex\n    a, n, p = as_int(a), as_int(n), as_int(p)\n    if n == 2:\n        return sqrt_mod(a, p, all_roots)\n    # see Hackman \"Elementary Number Theory\" (2009), page 76\n    if not is_nthpow_residue(a, n, p):\n        return None\n    if not isprime(p):\n        raise NotImplementedError(\"Not implemented for composite p\")\n\n    if (p - 1) % n == 0:\n        return _nthroot_mod1(a, n, p, all_roots)\n    # The roots of ``x**n - a = 0 (mod p)`` are roots of\n    # ``gcd(x**n - a, x**(p - 1) - 1) = 0 (mod p)``\n    pa = n\n    pb = p - 1\n    b = 1\n    if pa < pb:\n        a, pa, b, pb = b, pb, a, pa\n    while pb:\n        # x**pa - a = 0; x**pb - b = 0\n        # x**pa - a = x**(q*pb + r) - a = (x**pb)**q * x**r - a =\n        #             b**q * x**r - a; x**r - c = 0; c = b**-q * a mod p\n        q, r = divmod(pa, pb)\n        c = pow(b, q, p)\n        c = igcdex(c, p)[0]\n        c = (c * a) % p\n        pa, pb = pb, r\n        a, b = b, c\n    if pa == 1:\n        if all_roots:\n            res = [a]\n        else:\n            res = a\n    elif pa == 2:\n        return sqrt_mod(a, p , all_roots)\n    else:\n        res = _nthroot_mod1(a, pa, p, all_roots)\n    return res\n\n\ndef quadratic_residues(p):\n    \"\"\"\n    Returns the list of quadratic residues.\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import quadratic_residues\n    >>> quadratic_residues(7)\n    [0, 1, 2, 4]\n    \"\"\"\n    p = as_int(p)\n    r = set()\n    for i in range(p // 2 + 1):\n        r.add(pow(i, 2, p))\n    return sorted(list(r))\n\n\ndef legendre_symbol(a, p):\n    r\"\"\"\n    Returns the Legendre symbol `(a / p)`.\n\n    For an integer ``a`` and an odd prime ``p``, the Legendre symbol is\n    defined as\n\n    .. math ::\n        \\genfrac(){}{}{a}{p} = \\begin{cases}\n             0 & \\text{if } p \\text{ divides } a\\\\\n             1 & \\text{if } a \\text{ is a quadratic residue modulo } p\\\\\n            -1 & \\text{if } a \\text{ is a quadratic nonresidue modulo } p\n        \\end{cases}\n\n    Parameters\n    ==========\n\n    a : integer\n    p : odd prime\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory import legendre_symbol\n    >>> [legendre_symbol(i, 7) for i in range(7)]\n    [0, 1, 1, -1, 1, -1, -1]\n    >>> sorted(set([i**2 % 7 for i in range(7)]))\n    [0, 1, 2, 4]\n\n    See Also\n    ========\n\n    is_quad_residue, jacobi_symbol\n\n    \"\"\"\n    a, p = as_int(a), as_int(p)\n    if not isprime(p) or p == 2:\n        raise ValueError(\"p should be an odd prime\")\n    a = a % p\n    if not a:\n        return 0\n    if pow(a, (p - 1) // 2, p) == 1:\n        return 1\n    return -1\n\n\ndef jacobi_symbol(m, n):\n    r\"\"\"\n    Returns the Jacobi symbol `(m / n)`.\n\n    For any integer ``m`` and any positive odd integer ``n`` the Jacobi symbol\n    is defined as the product of the Legendre symbols corresponding to the\n    prime factors of ``n``:\n\n    .. math ::\n        \\genfrac(){}{}{m}{n} =\n            \\genfrac(){}{}{m}{p^{1}}^{\\alpha_1}\n            \\genfrac(){}{}{m}{p^{2}}^{\\alpha_2}\n            ...\n            \\genfrac(){}{}{m}{p^{k}}^{\\alpha_k}\n            \\text{ where } n =\n                p_1^{\\alpha_1}\n                p_2^{\\alpha_2}\n                ...\n                p_k^{\\alpha_k}\n\n    Like the Legendre symbol, if the Jacobi symbol `\\genfrac(){}{}{m}{n} = -1`\n    then ``m`` is a quadratic nonresidue modulo ``n``.\n\n    But, unlike the Legendre symbol, if the Jacobi symbol\n    `\\genfrac(){}{}{m}{n} = 1` then ``m`` may or may not be a quadratic residue\n    modulo ``n``.\n\n    Parameters\n    ==========\n\n    m : integer\n    n : odd positive integer\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory import jacobi_symbol, legendre_symbol\n    >>> from sympy import Mul, S\n    >>> jacobi_symbol(45, 77)\n    -1\n    >>> jacobi_symbol(60, 121)\n    1\n\n    The relationship between the ``jacobi_symbol`` and ``legendre_symbol`` can\n    be demonstrated as follows:\n\n    >>> L = legendre_symbol\n    >>> S(45).factors()\n    {3: 2, 5: 1}\n    >>> jacobi_symbol(7, 45) == L(7, 3)**2 * L(7, 5)**1\n    True\n\n    See Also\n    ========\n\n    is_quad_residue, legendre_symbol\n    \"\"\"\n    m, n = as_int(m), as_int(n)\n    if n < 0 or not n % 2:\n        raise ValueError(\"n should be an odd positive integer\")\n    if m < 0 or m > n:\n        m = m % n\n    if not m:\n        return int(n == 1)\n    if n == 1 or m == 1:\n        return 1\n    if igcd(m, n) != 1:\n        return 0\n\n    j = 1\n    if m < 0:\n        m = -m\n        if n % 4 == 3:\n            j = -j\n    while m != 0:\n        while m % 2 == 0 and m > 0:\n            m >>= 1\n            if n % 8 in [3, 5]:\n                j = -j\n        m, n = n, m\n        if m % 4 == 3 and n % 4 == 3:\n            j = -j\n        m %= n\n    if n != 1:\n        j = 0\n    return j\n\n\nclass mobius(Function):\n    \"\"\"\n    Mobius function maps natural number to {-1, 0, 1}\n\n    It is defined as follows:\n        1) `1` if `n = 1`.\n        2) `0` if `n` has a squared prime factor.\n        3) `(-1)^k` if `n` is a square-free positive integer with `k`\n           number of prime factors.\n\n    It is an important multiplicative function in number theory\n    and combinatorics.  It has applications in mathematical series,\n    algebraic number theory and also physics (Fermion operator has very\n    concrete realization with Mobius Function model).\n\n    Parameters\n    ==========\n\n    n : positive integer\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory import mobius\n    >>> mobius(13*7)\n    1\n    >>> mobius(1)\n    1\n    >>> mobius(13*7*5)\n    -1\n    >>> mobius(13**2)\n    0\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/M%C3%B6bius_function\n    .. [2] Thomas Koshy \"Elementary Number Theory with Applications\"\n\n    \"\"\"\n    @classmethod\n    def eval(cls, n):\n        if n.is_integer:\n            if n.is_positive is not True:\n                raise ValueError(\"n should be a positive integer\")\n        else:\n            raise TypeError(\"n should be an integer\")\n        if n.is_prime:\n            return S.NegativeOne\n        elif n is S.One:\n            return S.One\n        elif n.is_Integer:\n            a = factorint(n)\n            if any(i > 1 for i in a.values()):\n                return S.Zero\n            return S.NegativeOne**len(a)\n\n\ndef _discrete_log_trial_mul(n, a, b, order=None):\n    \"\"\"\n    Trial multiplication algorithm for computing the discrete logarithm of\n    ``a`` to the base ``b`` modulo ``n``.\n\n    The algorithm finds the discrete logarithm using exhaustive search. This\n    naive method is used as fallback algorithm of ``discrete_log`` when the\n    group order is very small.\n\n    Examples\n    ========\n\n(B)         # sort to get always the same result\n        return sorted([ZZ(res), ZZ(p - res)])\n\n    if k > 1:\n        # see Ref.[2]\n        if p == 2:\n            if a % 8 != 1:\n                return None\n            if k <= 3:\n               s = set()\n               for i in range(0, pk, 4):\n                    s.add(1 + i)\n                    s.add(-1 + i)\n               return list(s)\n            # according to Ref.[2] for k > 2 there are two solutions\n            # (mod 2**k-1), that is four solutions (mod 2**k), which can be\n            # obtained from the roots of x**2 = 0 (mod 8)\n            rv = [ZZ(1), ZZ(3), ZZ(5), ZZ(7)]\n            # hensel lift them to solutions of x**2 = 0 (mod 2**k)\n            # if r**2 - a = 0 mod 2**nx but not mod 2**(nx+1)\n            # then r + 2**(nx - 1) is a root mod 2**(nx+1)\n            n = 3\n            res = []\n            for r in rv:\n                nx = n\n                while nx < k:\n                    r1 = (r**2 - a) >> nx\n                    if r1 % 2:\n                        r = r + (1 << (nx - 1))\n                    #assert (r**2 - a)% (1 << (nx + 1)) == 0\n                    nx += 1\n                if r not in res:\n                    res.append(r)\n                x = r + (1 << (k - 1))\n                #assert (x**2 - a) % pk == 0\n                if x < (1 << nx) and x not in res:\n                    if (x**2 - a) % pk == 0:\n                        res.append(x)\n            return res\n        rv = _sqrt_mod_prime_power(a, p, 1)\n        if not rv:\n            return None\n        r = rv[0]\n        fr = r**2 - a\n        # hensel lifting with Newton iteration, see Ref.[3] chapter 9\n        # with f(x) = x**2 - a; one has f'(a) != 0 (mod p) for p != 2\n        n = 1\n        px = p\n        while 1:\n            n1 = n\n            n1 *= 2\n            if n1 > k:\n                break\n            n = n1\n            px = px**2\n            frinv = igcdex(2*r, px)[0]\n            r = (r - fr*frinv) % px\n            fr = r**2 - a\n        if n < k:\n            px = p**k\n            frinv = igcdex(2*r, px)[0]\n            r = (r - fr*frinv) % px\n        return [r, px - r]\n\n\ndef _sqrt_mod1(a, p, n):\n    \"\"\"\n    Find solution to ``x**2 == a mod p**n`` when ``a % p == 0``\n\n    see http://www.numbertheory.org/php/squareroot.html\n    \"\"\"\n    pn = p**n\n    a = a % pn\n    if a == 0:\n        # case gcd(a, p**k) = p**n\n        m = n // 2\n        if n % 2 == 1:\n            pm1 = p**(m + 1)\n            def _iter0a():\n                i = 0\n                while i < pn:\n                    yield i\n                    i += pm1\n            return _iter0a()\n        else:\n            pm = p**m\n            def _iter0b():\n                i = 0\n                while i < pn:\n                    yield i\n                    i += pm\n            return _iter0b()\n\n    # case gcd(a, p**k) = p**r, r < n\n    f = factorint(a)\n    r = f[p]\n    if r % 2 == 1:\n        return None\n    m = r // 2\n    a1 = a >> r\n    if p == 2:\n        if n - r == 1:\n            pnm1 = 1 << (n - m + 1)\n            pm1 = 1 << (m + 1)\n            def _iter1():\n                k = 1 << (m + 2)\n                i = 1 << m\n                while i < pnm1:\n                    j = i\n                    while j < pn:\n                        yield j\n                        j += k\n                    i += pm1\n            return _iter1()\n        if n - r == 2:\n            res = _sqrt_mod_prime_power(a1, p, n - r)\n            if res is None:\n                return None\n            pnm = 1 << (n - m)\n            def _iter2():\n                s = set()\n                for r in res:\n                    i = 0\n                    while i < pn:\n                        x = (r << m) + i\n                        if x not in s:\n                            s.add(x)\n                            yield x\n                        i += pnm\n            return _iter2()\n        if n - r > 2:\n            res = _sqrt_mod_prime_power(a1, p, n - r)\n            if res is None:\n                return None\n            pnm1 = 1 << (n - m - 1)\n            def _iter3():\n                s = set()\n                for r in res:\n                    i = 0\n                    while i < pn:\n                        x = ((r << m) + i) % pn\n                        if x not in s:\n                            s.add(x)\n                            yield x\n                        i += pnm1\n            return _iter3()\n    else:\n        m = r // 2\n        a1 = a // p**r\n        res1 = _sqrt_mod_prime_power(a1, p, n - r)\n        if res1 is None:\n            return None\n        pm = p**m\n        pnr = p**(n-r)\n        pnm = p**(n-m)\n\n        def _iter4():\n            s = set()\n            pm = p**m\n            for rx in res1:\n                i = 0\n                while i < pnm:\n                    x = ((rx + i) % pn)\n                    if x not in s:\n                        s.add(x)\n                        yield x*pm\n                    i += pnr\n        return _iter4()\n\n\ndef is_quad_residue(a, p):\n    \"\"\"\n    Returns True if ``a`` (mod ``p``) is in the set of squares mod ``p``,\n    i.e a % p in set([i**2 % p for i in range(p)]). If ``p`` is an odd\n    prime, an iterative method is used to make the determination:\n\n    >>> from sympy.ntheory import is_quad_residue\n    >>> sorted(set([i**2 % 7 for i in range(7)]))\n    [0, 1, 2, 4]\n    >>> [j for j in range(7) if is_quad_residue(j, 7)]\n    [0, 1, 2, 4]\n\n    See Also\n    ========\n\n    legendre_symbol, jacobi_symbol\n    \"\"\"\n    a, p = as_int(a), as_int(p)\n    if p < 1:\n        raise ValueError('p must be > 0')\n    if a >= p or a < 0:\n        a = a % p\n    if a < 2 or p < 3:\n        return True\n    if not isprime(p):\n        if p % 2 and jacobi_symbol(a, p) == -1:\n            return False\n        r = sqrt_mod(a, p)\n        if r is None:\n            return False\n        else:\n            return True\n\n    return pow(a, (p - 1) // 2, p) == 1\n\n\ndef is_nthpow_residue(a, n, m):\n    \"\"\"\n    Returns True if ``x**n == a (mod m)`` has solutions.\n\n    References\n    ==========\n\n    .. [1] P. Hackman \"Elementary Number Theory\" (2009), page 76\n\n    \"\"\"\n    a, n, m = as_int(a), as_int(n), as_int(m)\n    if m <= 0:\n        raise ValueError('m must be > 0')\n    if n < 0:\n        raise ValueError('n must be >= 0')\n    if a < 0:\n        raise ValueError('a must be >= 0')\n    if n == 0:\n        if m == 1:\n            return False\n        return a == 1\n    if a % m == 0:\n        return True\n    if n == 1:\n        return True\n    if n == 2:\n        return is_quad_residue(a, m)\n    return _is_nthpow_residue_bign(a, n, m)\n\n\ndef _is_nthpow_residue_bign(a, n, m):\n    \"\"\"Returns True if ``x**n == a (mod m)`` has solutions for n > 2.\"\"\"\n    # assert n > 2\n    # assert a > 0 and m > 0\n    if primitive_root(m) is None:\n        # assert m >= 8\n        for prime, power in factorint(m).items():\n            if not _is_nthpow_residue_bign_prime_power(a, n, prime, power):\n                return False\n        return True\n    f = totient(m)\n    k = f // igcd(f, n)\n    return pow(a, k, m) == 1\n\n\ndef _is_nthpow_residue_bign_prime_power(a, n, p, k):\n    \"\"\"Returns True/False if a solution for ``x**n == a (mod(p**k))``\n    does/doesn't exist.\"\"\"\n    # assert a > 0\n    # assert n > 2\n    # assert p is prime\n    # assert k > 0\n    if a % p:\n        if p != 2:\n            return _is_nthpow_residue_bign(a, n, pow(p, k))\n        if n & 1:\n            return True\n        c = trailing(n)\n        return a % pow(2, min(c + 2, k)) == 1\n    else:\n        a %= pow(p, k)\n        if not a:\n            return True\n        mu = multiplicity(p, a)\n        if mu % n:\n            return False\n        pm = pow(p, mu)\n        return _is_nthpow_residue_bign_prime_power(a//pm, n, p, k - mu)\n\n\ndef _nthroot_mod2(s, q, p):\n    f = factorint(q)\n    v = []\n    for b, e in f.items():\n        v.extend([b]*e)\n    for qx in v:\n        s = _nthroot_mod1(s, qx, p, False)\n    return s\n\n\ndef _nthroot_mod1(s, q, p, all_roots):\n    \"\"\"\n    Root of ``x**q = s mod p``, ``p`` prime and ``q`` divides ``p - 1``\n\n    References\n    ==========\n\n    .. [1] A. M. Johnston \"A Generalized qth Root Algorithm\"\n\n    \"\"\"\n    g = primitive_root(p)\n    if not isprime(q):\n        r = _nthroot_mod2(s, q, p)\n    else:\n        f = p - 1\n        assert (p - 1) % q == 0\n        # determine k\n        k = 0\n        while f % q == 0:\n            k += 1\n            f = f // q\n        # find z, x, r1\n        f1 = igcdex(-f, q)[0] % q\n        z = f*f1\n        x = (1 + z) // q\n        r1 = pow(s, x, p)\n        s1 = pow(s, f, p)\n        h = pow(g, f*q, p)\n        t = discrete_log(p, s1, h)\n        g2 = pow(g, z*t, p)\n        g3 = igcdex(g2, p)[0]\n        r = r1*g3 % p\n        #assert pow(r, q, p) == s\n    res = [r]\n    h = pow(g, (p - 1) // q, p)\n    #assert pow(h, q, p) == 1\n    hx = r\n    for i in range(q - 1):\n        hx = (hx*h) % p\n        res.append(hx)\n    if all_roots:\n        res.sort()\n        return res\n    return min(res)\n\n\ndef nthroot_mod(a, n, p, all_roots=False):\n    \"\"\"\n    Find the solutions to ``x**n = a mod p``\n\n    Parameters\n    ==========\n\n    a : integer\n    n : positive integer\n    p : positive integer\n    all_roots : if False returns the smallest root, else the list of roots\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import nthroot_mod\n    >>> nthroot_mod(11, 4, 19)\n    8\n    >>> nthroot_mod(11, 4, 19, True)\n    [8, 11]\n    >>> nthroot_mod(68, 3, 109)\n    23\n    \"\"\"\n    from sympy.core.numbers import igcdex\n    a, n, p = as_int(a), as_int(n), as_int(p)\n    if n == 2:\n        return sqrt_mod(a, p, all_roots)\n    # see Hackman \"Elementary Number Theory\" (2009), page 76\n    if not is_nthpow_residue(a, n, p):\n        return None\n    if not isprime(p):\n        raise NotImplementedError(\"Not implemented for composite p\")\n\n    if (p - 1) % n == 0:\n        return _nthroot_mod1(a, n, p, all_roots)\n    # The roots of ``x**n - a = 0 (mod p)`` are roots of\n    # ``gcd(x**n - a, x**(p - 1) - 1) = 0 (mod p)``\n    pa = n\n    pb = p - 1\n    b = 1\n    if pa < pb:\n        a, pa, b, pb = b, pb, a, pa\n    while pb:\n        # x**pa - a = 0; x**pb - b = 0\n        # x**pa - a = x**(q*pb + r) - a = (x**pb)**q * x**r - a =\n        #             b**q * x**r - a; x**r - c = 0; c = b**-q * a mod p\n        q, r = divmod(pa, pb)\n        c = pow(b, q, p)\n        c = igcdex(c, p)[0]\n        c = (c * a) % p\n        pa, pb = pb, r\n        a, b = b, c\n    if pa == 1:\n        if all_roots:\n            res = [a]\n        else:\n            res = a\n    elif pa == 2:\n        return sqrt_mod(a, p , all_roots)\n    else:\n        res = _nthroot_mod1(a, pa, p, all_roots)\n    return res\n\n\ndef quadratic_residues(p):\n    \"\"\"\n    Returns the list of quadratic residues.\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import quadratic_residues\n    >>> quadratic_residues(7)\n    [0, 1, 2, 4]\n    \"\"\"\n    p = as_int(p)\n    r = set()\n    for i in range(p // 2 + 1):\n        r.add(pow(i, 2, p))\n    return sorted(list(r))\n\n\ndef legendre_symbol(a, p):\n    r\"\"\"\n    Returns the Legendre symbol `(a / p)`.\n\n    For an integer ``a`` and an odd prime ``p``, the Legendre symbol is\n    defined as\n\n    .. math ::\n        \\genfrac(){}{}{a}{p} = \\begin{cases}\n             0 & \\text{if } p \\text{ divides } a\\\\\n             1 & \\text{if } a \\text{ is a quadratic residue modulo } p\\\\\n            -1 & \\text{if } a \\text{ is a quadratic nonresidue modulo } p\n        \\end{cases}\n\n    Parameters\n    ==========\n\n    a : integer\n    p : odd prime\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory import legendre_symbol\n    >>> [legendre_symbol(i, 7) for i in range(7)]\n    [0, 1, 1, -1, 1, -1, -1]\n    >>> sorted(set([i**2 % 7 for i in range(7)]))\n    [0, 1, 2, 4]\n\n    See Also\n    ========\n\n    is_quad_residue, jacobi_symbol\n\n    \"\"\"\n    a, p = as_int(a), as_int(p)\n    if not isprime(p) or p == 2:\n        raise ValueError(\"p should be an odd prime\")\n    a = a % p\n    if not a:\n        return 0\n    if pow(a, (p - 1) // 2, p) == 1:\n        return 1\n    return -1\n\n\ndef jacobi_symbol(m, n):\n    r\"\"\"\n    Returns the Jacobi symbol `(m / n)`.\n\n    For any integer ``m`` and any positive odd integer ``n`` the Jacobi symbol\n    is defined as the product of the Legendre symbols corresponding to the\n    prime factors of ``n``:\n\n    .. math ::\n        \\genfrac(){}{}{m}{n} =\n            \\genfrac(){}{}{m}{p^{1}}^{\\alpha_1}\n            \\genfrac(){}{}{m}{p^{2}}^{\\alpha_2}\n            ...\n            \\genfrac(){}{}{m}{p^{k}}^{\\alpha_k}\n            \\text{ where } n =\n                p_1^{\\alpha_1}\n                p_2^{\\alpha_2}\n                ...\n                p_k^{\\alpha_k}\n\n    Like the Legendre symbol, if the Jacobi symbol `\\genfrac(){}{}{m}{n} = -1`\n    then ``m`` is a quadratic nonresidue modulo ``n``.\n\n    But, unlike the Legendre symbol, if the Jacobi symbol\n    `\\genfrac(){}{}{m}{n} = 1` then ``m`` may or may not be a quadratic residue\n    modulo ``n``.\n\n    Parameters\n    ==========\n\n    m : integer\n    n : odd positive integer\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory import jacobi_symbol, legendre_symbol\n    >>> from sympy import Mul, S\n    >>> jacobi_symbol(45, 77)\n    -1\n    >>> jacobi_symbol(60, 121)\n    1\n\n    The relationship between the ``jacobi_symbol`` and ``legendre_symbol`` can\n    be demonstrated as follows:\n\n    >>> L = legendre_symbol\n    >>> S(45).factors()\n    {3: 2, 5: 1}\n    >>> jacobi_symbol(7, 45) == L(7, 3)**2 * L(7, 5)**1\n    True\n\n    See Also\n    ========\n\n    is_quad_residue, legendre_symbol\n    \"\"\"\n    m, n = as_int(m), as_int(n)\n    if n < 0 or not n % 2:\n        raise ValueError(\"n should be an odd positive integer\")\n    if m < 0 or m > n:\n        m = m % n\n    if not m:\n        return int(n == 1)\n    if n == 1 or m == 1:\n        return 1\n    if igcd(m, n) != 1:\n        return 0\n\n    j = 1\n    if m < 0:\n        m = -m\n        if n % 4 == 3:\n            j = -j\n    while m != 0:\n        while m % 2 == 0 and m > 0:\n            m >>= 1\n            if n % 8 in [3, 5]:\n                j = -j\n        m, n = n, m\n        if m % 4 == 3 and n % 4 == 3:\n            j = -j\n        m %= n\n    if n != 1:\n        j = 0\n    return j\n\n\nclass mobius(Function):\n    \"\"\"\n    Mobius function maps natural number to {-1, 0, 1}\n\n    It is defined as follows:\n        1) `1` if `n = 1`.\n        2) `0` if `n` has a squared prime factor.\n        3) `(-1)^k` if `n` is a square-free positive integer with `k`\n           number of prime factors.\n\n    It is an important multiplicative function in number theory\n    and combinatorics.  It has applications in mathematical series,\n    algebraic number theory and also physics (Fermion operator has very\n    concrete realization with Mobius Function model).\n\n    Parameters\n    ==========\n\n    n : positive integer\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory import mobius\n    >>> mobius(13*7)\n    1\n    >>> mobius(1)\n    1\n    >>> mobius(13*7*5)\n    -1\n    >>> mobius(13**2)\n    0\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/M%C3%B6bius_function\n    .. [2] Thomas Koshy \"Elementary Number Theory with Applications\"\n\n    \"\"\"\n    @classmethod\n    def eval(cls, n):\n        if n.is_integer:\n            if n.is_positive is not True:\n                raise ValueError(\"n should be a positive integer\")\n        else:\n            raise TypeError(\"n should be an integer\")\n        if n.is_prime:\n            return S.NegativeOne\n        elif n is S.One:\n            return S.One\n        elif n.is_Integer:\n            a = factorint(n)\n            if any(i > 1 for i in a.values()):\n                return S.Zero\n            return S.NegativeOne**len(a)\n\n\ndef _discrete_log_trial_mul(n, a, b, order=None):\n    \"\"\"\n    Trial multiplication algorithm for computing the discrete logarithm of\n    ``a`` to the base ``b`` modulo ``n``.\n\n    The algorithm finds the discrete logarithm using exhaustive search. This\n    naive method is used as fallback algorithm of ``discrete_log`` when the\n    group order is very small.\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import _discrete_log_trial_mul\n    >>> _discrete_log_trial_mul(41, 15, 7)\n    3\n\n    See Also\n    ========\n\n    discrete_log\n\n    References\n    ==========\n\n    .. [1] \"Handbook of applied cryptography\", Menezes, A. J., Van, O. P. C., &\n        Vanstone, S. A. (1997).\n    \"\"\"\n    a %= n\n    b %= n\n    if order is None:\n        order = n\n    x = 1\n    for i in range(order):\n        if x == a:\n            return i\n        x = x * b % n\n    raise ValueError(\"Log does not exist\")\n\n\ndef _discrete_log_shanks_steps(n, a, b, order=None):\n    \"\"\"\n    Baby-step giant-step algorithm for computing the discrete logarithm of\n    ``a`` to the base ``b`` modulo ``n``.\n\n    The algorithm is a time-memory trade-off of the method of exhaustive\n    search. It uses `O(sqrt(m))` memory, where `m` is the group order.\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import _discrete_log_shanks_steps\n    >>> _discrete_log_shanks_steps(41, 15, 7)\n    3\n\n    See Also\n    ========\n\n    discrete_log\n\n    References\n    ==========\n\n    .. [1] \"Handbook of applied cryptography\", Menezes, A. J., Van, O. P. C., &\n        Vanstone, S. A. (1997).\n    \"\"\"\n    a %= n\n    b %= n\n    if order is None:\n        order = n_order(b, n)\n    m = isqrt(order) + 1\n    T = dict()\n    x = 1\n    for i in range(m):\n        T[x] = i\n        x = x * b % n\n    z = mod_inverse(b, n)\n    z = pow(z, m, n)\n    x = a\n    for i in range(m):\n        if x in T:\n            return i * m + T[x]\n        x = x * z % n\n    raise ValueError(\"Log does not exist\")\n\n\ndef _discrete_log_pollard_rho(n, a, b, order=None, retries=10, rseed=None):\n    \"\"\"\n    Pollard's Rho algorithm for computing the discrete logarithm of ``a`` to\n    the base ``b`` modulo ``n``.\n\n    It is a randomized algorithm with the same expected running time as\n    ``_discrete_log_shanks_steps``, but requires a negligible amount of memory.\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import _discrete_log_pollard_rho\n    >>> _discrete_log_pollard_rho(227, 3**7, 3)\n    7\n\n    See Also\n    ========\n\n    discrete_log\n\n    References\n    ==========\n\n    .. [1] \"Handbook of applied cryptography\", Menezes, A. J., Van, O. P. C., &\n        Vanstone, S. A. (1997).\n    \"\"\"\n    a %= n\n    b %= n\n\n    if order is None:\n        order = n_order(b, n)\n    prng = Random()\n    if rseed is not None:\n        prng.seed(rseed)\n\n    for i in range(retries):\n        aa = prng.randint(1, order - 1)\n        ba = prng.randint(1, order - 1)\n        xa = pow(b, aa, n) * pow(a, ba, n) % n\n\n        c = xa % 3\n        if c == 0:\n            xb = a * xa % n\n            ab = aa\n            bb = (ba + 1) % order\n        elif c == 1:\n            xb = xa * xa % n\n            ab = (aa + aa) % order\n            bb = (ba + ba) % order\n        else:\n            xb = b * xa % n\n            ab = (aa + 1) % order\n            bb = ba\n\n        for j in range(order):\n            c = xa % 3\n            if c == 0:\n                xa = a * xa % n\n                ba = (ba + 1) % order\n            elif c == 1:\n                xa = xa * xa % n\n                aa = (aa + aa) % order\n                ba = (ba + ba) % order\n            else:\n                xa = b * xa % n\n                aa = (aa + 1) % order\n\n            c = xb % 3\n            if c == 0:\n                xb = a * xb % n\n                bb = (bb + 1) % order\n            elif c == 1:\n                xb = xb * xb % n\n                ab = (ab + ab) % order\n                bb = (bb + bb) % order\n            else:\n                xb = b * xb % n\n                ab = (ab + 1) % order\n\n            c = xb % 3\n            if c == 0:\n                xb = a * xb % n\n                bb = (bb + 1) % order\n            elif c == 1:\n                xb = xb * xb % n\n                ab = (ab + ab) % order\n                bb = (bb + bb) % order\n            else:\n                xb = b * xb % n\n                ab = (ab + 1) % order\n\n            if xa == xb:\n                r = (ba - bb) % order\n                try:\n                    e = mod_inverse(r, order) * (ab - aa) % order\n                    if (pow(b, e, n) - a) % n == 0:\n                        return e\n                except ValueError:\n                    pass\n                break\n    raise ValueError(\"Pollard's Rho failed to find logarithm\")\n\n\ndef _discrete_log_pohlig_hellman(n, a, b, order=None):\n    \"\"\"\n    Pohlig-Hellman algorithm for computing the discrete logarithm of ``a`` to\n    the base ``b`` modulo ``n``.\n\n    In order to compute the discrete logarithm, the algorithm takes advantage\n    of the factorization of the group order. It is more efficient when the\n    group order factors into many small primes.\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import _discrete_log_pohlig_hellman\n    >>> _discrete_log_pohlig_hellman(251, 210, 71)\n    197\n\n    See Also\n    ========\n\n    discrete_log\n\n    References\n    ==========\n\n    .. [1] \"Handbook of applied cryptography\", Menezes, A. J., Van, O. P. C., &\n        Vanstone, S. A. (1997).\n    \"\"\"\n    from .modular import crt\n    a %= n\n    b %= n\n(C) \nfrom sympy.core.compatibility import as_int, range\nfrom sympy.core.function import Function\nfrom sympy.core.numbers import igcd, igcdex, mod_inverse\nfrom sympy.core.power import isqrt\nfrom sympy.core.singleton import S\nfrom .primetest import isprime\nfrom .factor_ import factorint, trailing, totient, multiplicity\nfrom random import randint, Random\n\n\n\ndef n_order(a, n):\n    \"\"\"Returns the order of ``a`` modulo ``n``.\n\n    The order of ``a`` modulo ``n`` is the smallest integer\n    ``k`` such that ``a**k`` leaves a remainder of 1 with ``n``.\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory import n_order\n    >>> n_order(3, 7)\n    6\n    >>> n_order(4, 7)\n    3\n    \"\"\"\n    from collections import defaultdict\n    a, n = as_int(a), as_int(n)\n    if igcd(a, n) != 1:\n        raise ValueError(\"The two numbers should be relatively prime\")\n    factors = defaultdict(int)\n    f = factorint(n)\n    for px, kx in f.items():\n        if kx > 1:\n            factors[px] += kx - 1\n        fpx = factorint(px - 1)\n        for py, ky in fpx.items():\n            factors[py] += ky\n    group_order = 1\n    for px, kx in factors.items():\n        group_order *= px**kx\n    order = 1\n    if a > n:\n        a = a % n\n    for p, e in factors.items():\n        exponent = group_order\n        for f in range(e + 1):\n            if pow(a, exponent, n) != 1:\n                order *= p ** (e - f + 1)\n                break\n            exponent = exponent // p\n    return order\n\n\ndef _primitive_root_prime_iter(p):\n    \"\"\"\n    Generates the primitive roots for a prime ``p``\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import _primitive_root_prime_iter\n    >>> list(_primitive_root_prime_iter(19))\n    [2, 3, 10, 13, 14, 15]\n\n    References\n    ==========\n\n    .. [1] W. Stein \"Elementary Number Theory\" (2011), page 44\n\n    \"\"\"\n    # it is assumed that p is an int\n    v = [(p - 1) // i for i in factorint(p - 1).keys()]\n    a = 2\n    while a < p:\n        for pw in v:\n            # a TypeError below may indicate that p was not an int\n            if pow(a, pw, p) == 1:\n                break\n        else:\n            yield a\n        a += 1\n\n\ndef primitive_root(p):\n    \"\"\"\n    Returns the smallest primitive root or None\n\n    Parameters\n    ==========\n\n    p : positive integer\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import primitive_root\n    >>> primitive_root(19)\n    2\n\n    References\n    ==========\n\n    .. [1] W. Stein \"Elementary Number Theory\" (2011), page 44\n    .. [2] P. Hackman \"Elementary Number Theory\" (2009), Chapter C\n\n    \"\"\"\n    p = as_int(p)\n    if p < 1:\n        raise ValueError('p is required to be positive')\n    if p <= 2:\n        return 1\n    f = factorint(p)\n    if len(f) > 2:\n        return None\n    if len(f) == 2:\n        if 2 not in f or f[2] > 1:\n            return None\n\n        # case p = 2*p1**k, p1 prime\n        for p1, e1 in f.items():\n            if p1 != 2:\n                break\n        i = 1\n        while i < p:\n            i += 2\n            if i % p1 == 0:\n                continue\n            if is_primitive_root(i, p):\n                return i\n\n    else:\n        if 2 in f:\n            if p == 4:\n                return 3\n            return None\n        p1, n = list(f.items())[0]\n        if n > 1:\n            # see Ref [2], page 81\n            g = primitive_root(p1)\n            if is_primitive_root(g, p1**2):\n                return g\n            else:\n                for i in range(2, g + p1 + 1):\n                    if igcd(i, p) == 1 and is_primitive_root(i, p):\n                        return i\n\n    return next(_primitive_root_prime_iter(p))\n\n\ndef is_primitive_root(a, p):\n    \"\"\"\n    Returns True if ``a`` is a primitive root of ``p``\n\n    ``a`` is said to be the primitive root of ``p`` if gcd(a, p) == 1 and\n    totient(p) is the smallest positive number s.t.\n\n        a**totient(p) cong 1 mod(p)\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory import is_primitive_root, n_order, totient\n    >>> is_primitive_root(3, 10)\n    True\n    >>> is_primitive_root(9, 10)\n    False\n    >>> n_order(3, 10) == totient(10)\n    True\n    >>> n_order(9, 10) == totient(10)\n    False\n\n    \"\"\"\n    a, p = as_int(a), as_int(p)\n    if igcd(a, p) != 1:\n        raise ValueError(\"The two numbers should be relatively prime\")\n    if a > p:\n        a = a % p\n    return n_order(a, p) == totient(p)\n\n\ndef _sqrt_mod_tonelli_shanks(a, p):\n    \"\"\"\n    Returns the square root in the case of ``p`` prime with ``p == 1 (mod 8)``\n\n    References\n    ==========\n\n    .. [1] R. Crandall and C. Pomerance \"Prime Numbers\", 2nt Ed., page 101\n\n    \"\"\"\n    s = trailing(p - 1)\n    t = p >> s\n    # find a non-quadratic residue\n    while 1:\n        d = randint(2, p - 1)\n        r = legendre_symbol(d, p)\n        if r == -1:\n            break\n    #assert legendre_symbol(d, p) == -1\n    A = pow(a, t, p)\n    D = pow(d, t, p)\n    m = 0\n    for i in range(s):\n        adm = A*pow(D, m, p) % p\n        adm = pow(adm, 2**(s - 1 - i), p)\n        if adm % p == p - 1:\n            m += 2**i\n    #assert A*pow(D, m, p) % p == 1\n    x = pow(a, (t + 1)//2, p)*pow(D, m//2, p) % p\n    return x\n\n\ndef sqrt_mod(a, p, all_roots=False):\n    \"\"\"\n    Find a root of ``x**2 = a mod p``\n\n    Parameters\n    ==========\n\n    a : integer\n    p : positive integer\n    all_roots : if True the list of roots is returned or None\n\n    Notes\n    =====\n\n    If there is no root it is returned None; else the returned root\n    is less or equal to ``p // 2``; in general is not the smallest one.\n    It is returned ``p // 2`` only if it is the only root.\n\n    Use ``all_roots`` only when it is expected that all the roots fit\n    in memory; otherwise use ``sqrt_mod_iter``.\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory import sqrt_mod\n    >>> sqrt_mod(11, 43)\n    21\n    >>> sqrt_mod(17, 32, True)\n    [7, 9, 23, 25]\n    \"\"\"\n    if all_roots:\n        return sorted(list(sqrt_mod_iter(a, p)))\n    try:\n        p = abs(as_int(p))\n        it = sqrt_mod_iter(a, p)\n        r = next(it)\n        if r > p // 2:\n            return p - r\n        elif r < p // 2:\n            return r\n        else:\n            try:\n                r = next(it)\n                if r > p // 2:\n                    return p - r\n            except StopIteration:\n                pass\n            return r\n    except StopIteration:\n        return None\n\n\ndef _product(*iters):\n    \"\"\"\n    Cartesian product generator\n\n    Notes\n    =====\n\n    Unlike itertools.product, it works also with iterables which do not fit\n    in memory. See http://bugs.python.org/issue10109\n\n    Author: Fernando Sumudu\n    with small changes\n    \"\"\"\n    import itertools\n    inf_iters = tuple(itertools.cycle(enumerate(it)) for it in iters)\n    num_iters = len(inf_iters)\n    cur_val = [None]*num_iters\n\n    first_v = True\n    while True:\n        i, p = 0, num_iters\n        while p and not i:\n            p -= 1\n            i, cur_val[p] = next(inf_iters[p])\n\n        if not p and not i:\n            if first_v:\n                first_v = False\n            else:\n                break\n\n        yield cur_val\n\n\ndef sqrt_mod_iter(a, p, domain=int):\n    \"\"\"\n    Iterate over solutions to ``x**2 = a mod p``\n\n    Parameters\n    ==========\n\n    a : integer\n    p : positive integer\n    domain : integer domain, ``int``, ``ZZ`` or ``Integer``\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import sqrt_mod_iter\n    >>> list(sqrt_mod_iter(11, 43))\n    [21, 22]\n    \"\"\"\n    from sympy.polys.galoistools import gf_crt1, gf_crt2\n    from sympy.polys.domains import ZZ\n    a, p = as_int(a), abs(as_int(p))\n    if isprime(p):\n        a = a % p\n        if a == 0:\n            res = _sqrt_mod1(a, p, 1)\n        else:\n            res = _sqrt_mod_prime_power(a, p, 1)\n        if res:\n            if domain is ZZ:\n                for x in res:\n                    yield x\n            else:\n                for x in res:\n                    yield domain(x)\n    else:\n        f = factorint(p)\n        v = []\n        pv = []\n        for px, ex in f.items():\n            if a % px == 0:\n                rx = _sqrt_mod1(a, px, ex)\n                if not rx:\n                    return\n            else:\n                rx = _sqrt_mod_prime_power(a, px, ex)\n                if not rx:\n                    return\n            v.append(rx)\n            pv.append(px**ex)\n        mm, e, s = gf_crt1(pv, ZZ)\n        if domain is ZZ:\n            for vx in _product(*v):\n                r = gf_crt2(vx, pv, mm, e, s, ZZ)\n                yield r\n        else:\n            for vx in _product(*v):\n                r = gf_crt2(vx, pv, mm, e, s, ZZ)\n                yield domain(r)\n\n\ndef _sqrt_mod_prime_power(a, p, k):\n    \"\"\"\n    Find the solutions to ``x**2 = a mod p**k`` when ``a % p != 0``\n\n    Parameters\n    ==========\n\n    a : integer\n    p : prime number\n    k : positive integer\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import _sqrt_mod_prime_power\n    >>> _sqrt_mod_prime_power(11, 43, 1)\n    [21, 22]\n\n    References\n    ==========\n\n    .. [1] P. Hackman \"Elementary Number Theory\" (2009), page 160\n    .. [2] http://www.numbertheory.org/php/squareroot.html\n    .. [3] [Gathen99]_\n    \"\"\"\n    from sympy.core.numbers import igcdex\n    from sympy.polys.domains import ZZ\n\n    pk = p**k\n    a = a % pk\n\n    if k == 1:\n        if p == 2:\n            return [ZZ(a)]\n        if not (a % p < 2 or pow(a, (p - 1) // 2, p) == 1):\n            return None\n\n        if p % 4 == 3:\n            res = pow(a, (p + 1) // 4, p)\n        elif p % 8 == 5:\n            sign = pow(a, (p - 1) // 4, p)\n            if sign == 1:\n                res = pow(a, (p + 3) // 8, p)\n            else:\n                b = pow(4*a, (p - 5) // 8, p)\n                x =  (2*a*b) % p\n                if pow(x, 2, p) == a:\n                    res = x\n        else:\n            res = _sqrt_mod_tonelli_shanks(a, p)\n\n        # ``_sqrt_mod_tonelli_shanks(a, p)`` is not deterministic;\n        # sort to get always the same result\n        return sorted([ZZ(res), ZZ(p - res)])\n\n    if k > 1:\n        # see Ref.[2]\n        if p == 2:\n            if a % 8 != 1:\n                return None\n            if k <= 3:\n               s = set()\n               for i in range(0, pk, 4):\n                    s.add(1 + i)\n                    s.add(-1 + i)\n               return list(s)\n            # according to Ref.[2] for k > 2 there are two solutions\n            # (mod 2**k-1), that is four solutions (mod 2**k), which can be\n            # obtained from the roots of x**2 = 0 (mod 8)\n            rv = [ZZ(1), ZZ(3), ZZ(5), ZZ(7)]\n            # hensel lift them to solutions of x**2 = 0 (mod 2**k)\n            # if r**2 - a = 0 mod 2**nx but not mod 2**(nx+1)\n            # then r + 2**(nx - 1) is a root mod 2**(nx+1)\n            n = 3\n            res = []\n            for r in rv:\n                nx = n\n                while nx < k:\n                    r1 = (r**2 - a) >> nx\n                    if r1 % 2:\n                        r = r + (1 << (nx - 1))\n                    #assert (r**2 - a)% (1 << (nx + 1)) == 0\n                    nx += 1\n                if r not in res:\n                    res.append(r)\n                x = r + (1 << (k - 1))\n                #assert (x**2 - a) % pk == 0\n                if x < (1 << nx) and x not in res:\n                    if (x**2 - a) % pk == 0:\n                        res.append(x)\n            return res\n        rv = _sqrt_mod_prime_power(a, p, 1)\n        if not rv:\n            return None\n        r = rv[0]\n        fr = r**2 - a\n        # hensel lifting with Newton iteration, see Ref.[3] chapter 9\n        # with f(x) = x**2 - a; one has f'(a) != 0 (mod p) for p != 2\n        n = 1\n        px = p\n        while 1:\n            n1 = n\n            n1 *= 2\n            if n1 > k:\n                break\n            n = n1\n            px = px**2\n            frinv = igcdex(2*r, px)[0]\n            r = (r - fr*frinv) % px\n            fr = r**2 - a\n        if n < k:\n            px = p**k\n            frinv = igcdex(2*r, px)[0]\n            r = (r - fr*frinv) % px\n        return [r, px - r]\n\n\ndef _sqrt_mod1(a, p, n):\n    \"\"\"\n    Find solution to ``x**2 == a mod p**n`` when ``a % p == 0``\n\n    see http://www.numbertheory.org/php/squareroot.html\n    \"\"\"\n    pn = p**n\n    a = a % pn\n    if a == 0:\n        # case gcd(a, p**k) = p**n\n        m = n // 2\n        if n % 2 == 1:\n            pm1 = p**(m + 1)\n            def _iter0a():\n                i = 0\n                while i < pn:\n                    yield i\n                    i += pm1\n            return _iter0a()\n        else:\n            pm = p**m\n            def _iter0b():\n                i = 0\n                while i < pn:\n                    yield i\n                    i += pm\n            return _iter0b()\n\n    # case gcd(a, p**k) = p**r, r < n\n    f = factorint(a)\n    r = f[p]\n    if r % 2 == 1:\n        return None\n    m = r // 2\n    a1 = a >> r\n    if p == 2:\n        if n - r == 1:\n            pnm1 = 1 << (n - m + 1)\n            pm1 = 1 << (m + 1)\n            def _iter1():\n                k = 1 << (m + 2)\n                i = 1 << m\n                while i < pnm1:\n                    j = i\n                    while j < pn:\n                        yield j\n                        j += k\n                    i += pm1\n            return _iter1()\n        if n - r == 2:\n            res = _sqrt_mod_prime_power(a1, p, n - r)\n            if res is None:\n                return None\n            pnm = 1 << (n - m)\n            def _iter2():\n                s = set()\n                for r in res:\n                    i = 0\n                    while i < pn:\n                        x = (r << m) + i\n                        if x not in s:\n                            s.add(x)\n                            yield x\n                        i += pnm\n            return _iter2()\n        if n - r > 2:\n            res = _sqrt_mod_prime_power(a1, p, n - r)\n            if res is None:\n                return None\n            pnm1 = 1 << (n - m - 1)\n            def _iter3():\n                s = set()\n                for r in res:\n                    i = 0\n                    while i < pn:\n                        x = ((r << m) + i) % pn\n                        if x not in s:\n                            s.add(x)\n                            yield x\n                        i += pnm1\n            return _iter3()\n    else:\n        m = r // 2\n        a1 = a // p**r\n        res1 = _sqrt_mod_prime_power(a1, p, n - r)\n        if res1 is None:\n            return None\n        pm = p**m\n        pnr = p**(n-r)\n        pnm = p**(n-m)\n\n        def _iter4():\n            s = set()\n            pm = p**m\n            for rx in res1:\n                i = 0\n                while i < pnm:\n                    x = ((rx + i) % pn)\n                    if x not in s:\n                        s.add(x)\n                        yield x*pm\n                    i += pnr\n        return _iter4()\n\n\ndef is_quad_residue(a, p):\n    \"\"\"\n    Returns True if ``a`` (mod ``p``) is in the set of squares mod ``p``,\n    i.e a % p in set([i**2 % p for i in range(p)]). If ``p`` is an odd\n    prime, an iterative method is used to make the determination:\n\n    >>> from sympy.ntheory import is_quad_residue\n    >>> sorted(set([i**2 % 7 for i in range(7)]))\n    [0, 1, 2, 4]\n    >>> [j for j in range(7) if is_quad_residue(j, 7)]\n    [0, 1, 2, 4]\n\n    See Also\n    ========\n\n    legendre_symbol, jacobi_symbol\n    \"\"\"\n    a, p = as_int(a), as_int(p)\n    if p < 1:\n        raise ValueError('p must be > 0')\n    if a >= p or a < 0:\n        a = a % p\n    if a < 2 or p < 3:\n        return True\n    if not isprime(p):\n        if p % 2 and jacobi_symbol(a, p) == -1:\n            return False\n        r = sqrt_mod(a, p)\n        if r is None:\n            return False\n        else:\n            return True\n\n    return pow(a, (p - 1) // 2, p) == 1\n\n\ndef is_nthpow_residue(a, n, m):\n    \"\"\"\n    Returns True if ``x**n == a (mod m)`` has solutions.\n\n    References\n    ==========\n\n    .. [1] P. Hackman \"Elementary Number Theory\" (2009), page 76\n\n    \"\"\"\n    a, n, m = as_int(a), as_int(n), as_int(m)\n    if m <= 0:\n        raise ValueError('m must be > 0')\n    if n < 0:\n        raise ValueError('n must be >= 0')\n    if a < 0:\n        raise ValueError('a must be >= 0')\n    if n == 0:\n        if m == 1:\n            return False\n        return a == 1\n    if a % m == 0:\n        return True\n    if n == 1:\n        return True\n    if n == 2:\n        return is_quad_residue(a, m)\n    return _is_nthpow_residue_bign(a, n, m)\n\n\ndef _is_nthpow_residue_bign(a, n, m):\n    \"\"\"Returns True if ``x**n == a (mod m)`` has solutions for n > 2.\"\"\"\n    # assert n > 2\n    # assert a > 0 and m > 0\n    if primitive_root(m) is None:\n        # assert m >= 8\n        for prime, power in factorint(m).items():\n            if not _is_nthpow_residue_bign_prime_power(a, n, prime, power):\n                return False\n        return True\n    f = totient(m)\n    k = f // igcd(f, n)\n    return pow(a, k, m) == 1\n\n\ndef _is_nthpow_residue_bign_prime_power(a, n, p, k):\n    \"\"\"Returns True/False if a solution for ``x**n == a (mod(p**k))``\n    does/doesn't exist.\"\"\"\n    # assert a > 0\n    # assert n > 2\n    # assert p is prime\n    # assert k > 0\n    if a % p:\n        if p != 2:\n            return _is_nthpow_residue_bign(a, n, pow(p, k))\n        if n & 1:\n            return True\n        c = trailing(n)\n        return a % pow(2, min(c + 2, k)) == 1\n    else:\n        a %= pow(p, k)\n        if not a:\n            return True\n        mu = multiplicity(p, a)\n        if mu % n:\n            return False\n        pm = pow(p, mu)\n        return _is_nthpow_residue_bign_prime_power(a//pm, n, p, k - mu)\n\n\ndef _nthroot_mod2(s, q, p):\n    f = factorint(q)\n    v = []\n    for b, e in f.items():\n        v.extend([b]*e)\n    for qx in v:\n        s = _nthroot_mod1(s, qx, p, False)\n    return s\n\n\ndef _nthroot_mod1(s, q, p, all_roots):\n    \"\"\"\n    Root of ``x**q = s mod p``, ``p`` prime and ``q`` divides ``p - 1``\n\n    References\n    ==========\n\n    .. [1] A. M. Johnston \"A Generalized qth Root Algorithm\"\n\n    \"\"\"\n    g = primitive_root(p)\n    if not isprime(q):\n        r = _nthroot_mod2(s, q, p)\n    else:\n        f = p - 1\n        assert (p - 1) % q == 0\n        # determine k\n        k = 0\n        while f % q == 0:\n            k += 1\n            f = f // q\n        # find z, x, r1\n        f1 = igcdex(-f, q)[0] % q\n        z = f*f1\n        x = (1 + z) // q\n        r1 = pow(s, x, p)\n        s1 = pow(s, f, p)\n        h = pow(g, f*q, p)\n        t = discrete_log(p, s1, h)\n        g2 = pow(g, z*t, p)\n        g3 = igcdex(g2, p)[0]\n        r = r1*g3 % p\n        #assert pow(r, q, p) == s\n    res = [r]\n    h = pow(g, (p - 1) // q, p)\n    #assert pow(h, q, p) == 1\n    hx = r\n    for i in range(q - 1):\n        hx = (hx*h) % p\n        res.append(hx)\n    if all_roots:\n        res.sort()\n        return res\n    return min(res)\n\n\ndef nthroot_mod(a, n, p, all_roots=False):\n    \"\"\"\n    Find the solutions to ``x**n = a mod p``\n\n    Parameters\n    ==========\n\n    a : integer\n    n : positive integer\n    p : positive integer\n    all_roots : if False returns the smallest root, else the list of roots\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import nthroot_mod\n    >>> nthroot_mod(11, 4, 19)\n    8\n    >>> nthroot_mod(11, 4, 19, True)\n    [8, 11]\n    >>> nthroot_mod(68, 3, 109)\n    23\n    \"\"\"\n    from sympy.core.numbers import igcdex\n    a, n, p = as_int(a), as_int(n), as_int(p)\n    if n == 2:\n        return sqrt_mod(a, p, all_roots)\n    # see Hackman \"Elementary Number Theory\" (2009), page 76\n    if not is_nthpow_residue(a, n, p):\n        return None\n    if not isprime(p):\n        raise NotImplementedError(\"Not implemented for composite p\")\n\n    if (p - 1) % n == 0:\n        return _nthroot_mod1(a, n, p, all_roots)\n    # The roots of ``x**n - a = 0 (mod p)`` are roots of\n    # ``gcd(x**n - a, x**(p - 1) - 1) = 0 (mod p)``\n    pa = n\n    pb = p - 1\n    b = 1\n    if pa < pb:\n        a, pa, b, pb = b, pb, a, pa\n    while pb:\n        # x**pa - a = 0; x**pb - b = 0\n        # x**pa - a = x**(q*pb + r) - a = (x**pb)**q * x**r - a =\n        #             b**q * x**r - a; x**r - c = 0; c = b**-q * a mod p\n        q, r = divmod(pa, pb)\n        c = pow(b, q, p)\n        c = igcdex(c, p)[0]\n        c = (c * a) % p\n        pa, pb = pb, r\n        a, b = b, c\n    if pa == 1:\n        if all_roots:\n            res = [a]\n        else:\n            res = a\n    elif pa == 2:\n        return sqrt_mod(a, p , all_roots)\n    else:\n        res = _nthroot_mod1(a, pa, p, all_roots)\n    return res\n\n\ndef quadratic_residues(p):\n    \"\"\"\n    Returns the list of quadratic residues.\n\n    Examples\n    ========\n\n    >>> from sympy.ntheory.residue_ntheory import quadratic_residues\n    >>> quadratic_residues(7)\n    [0, 1, 2, 4]\n    \"\"\"\n    p = as_int(p)\n    r = set()\n    for i in range(p // 2 + 1):\n        r.add(pow(i, 2, p))\n    return sorted(list(r))", "answer": "C"}
{"uuid": "a7b9aab7-2077-49bb-a0e5-cb28479112bc", "issue_statement": "expr.atoms() should return objects with no args instead of subclasses of Atom\n`expr.atoms()` with no arguments returns subclasses of `Atom` in `expr`. But the correct definition of a leaf node should be that it has no `.args`. \n\nThis should be easy to fix, but one needs to check that this doesn't affect the performance.", "choices": "(A)         {I*pi, 2*sin(y + I*pi)}\n\n        \"\"\"\n        if types:\n            types = tuple(\n                [t if isinstance(t, type) else type(t) for t in types])\n        else:\n            types = (Atom,)\n        result = set()\n        for expr in preorder_traversal(self):\n            if isinstance(expr, types):\n(B)         else:\n            types = (Atom,)\n        result = set()\n        for expr in preorder_traversal(self):\n            if isinstance(expr, types):\n                result.add(expr)\n        return result\n\n    @property\n    def free_symbols(self):\n        \"\"\"Return from the atoms of self those which are free symbols.\n(C)         for expr in preorder_traversal(self):\n            if isinstance(expr, types):\n                result.add(expr)\n        return result\n\n    @property\n    def free_symbols(self):\n        \"\"\"Return from the atoms of self those which are free symbols.\n\n        For most expressions, all symbols are free symbols. For some classes\n        this is not true. e.g. Integrals use Symbols for the dummy variables\n(D)         if types:\n            types = tuple(\n                [t if isinstance(t, type) else type(t) for t in types])\n        else:\n            types = (Atom,)\n        result = set()\n        for expr in preorder_traversal(self):\n            if isinstance(expr, types):\n                result.add(expr)\n        return result\n", "answer": "D"}
{"uuid": "d3ea9c60-5991-4a6c-b19d-35a464b8578b", "issue_statement": "BlockDiagMatrix with one element cannot be converted to regular Matrix\nCreating a BlockDiagMatrix with one Matrix element will raise if trying to convert it back to a regular Matrix:\r\n\r\n```python\r\nM = sympy.Matrix([[1, 2], [3, 4]])\r\nD = sympy.BlockDiagMatrix(M)\r\nB = sympy.Matrix(D)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-37-5b65c1f8f23e>\", line 3, in <module>\r\n    B = sympy.Matrix(D)\r\n\r\n  File \"/home/rikard/.local/lib/python3.7/site-packages/sympy/matrices/dense.py\", line 430, in __new__\r\n    return cls._new(*args, **kwargs)\r\n\r\n  File \"/home/rikard/.local/lib/python3.7/site-packages/sympy/matrices/dense.py\", line 442, in _new\r\n    rows, cols, flat_list = cls._handle_creation_inputs(*args, **kwargs)\r\n\r\n  File \"/home/rikard/.local/lib/python3.7/site-packages/sympy/matrices/matrices.py\", line 2528, in _handle_creation_inputs\r\n    return args[0].rows, args[0].cols, args[0].as_explicit()._mat\r\n\r\n  File \"/home/rikard/.local/lib/python3.7/site-packages/sympy/matrices/expressions/matexpr.py\", line 340, in as_explicit\r\n    for i in range(self.rows)])\r\n\r\n  File \"/home/rikard/.local/lib/python3.7/site-packages/sympy/matrices/expressions/matexpr.py\", line 340, in <listcomp>\r\n    for i in range(self.rows)])\r\n\r\n  File \"/home/rikard/.local/lib/python3.7/site-packages/sympy/matrices/expressions/matexpr.py\", line 339, in <listcomp>\r\n    for j in range(self.cols)]\r\n\r\n  File \"/home/rikard/.local/lib/python3.7/site-packages/sympy/matrices/expressions/matexpr.py\", line 289, in __getitem__\r\n    return self._entry(i, j)\r\n\r\n  File \"/home/rikard/.local/lib/python3.7/site-packages/sympy/matrices/expressions/blockmatrix.py\", line 248, in _entry\r\n    return self.blocks[row_block, col_block][i, j]\r\n\r\nTypeError: 'One' object is not subscriptable\r\n```\r\n\r\nInstead having two elements will work as expected:\r\n\r\n```python\r\nM = sympy.Matrix([[1, 2], [3, 4]])\r\nD = sympy.BlockDiagMatrix(M, M)\r\nB = sympy.Matrix(D)\r\n```\r\n\r\n```\r\nMatrix([\r\n[1, 2, 0, 0],\r\n[3, 4, 0, 0],\r\n[0, 0, 1, 2],\r\n[0, 0, 3, 4]])\r\n```\r\nThis issue exists for sympy 1.5.1 but not for sympy 1.4", "choices": "(A)         from sympy.matrices.immutable import ImmutableDenseMatrix\n        mats = self.args\n        data = [[mats[i] if i == j else ZeroMatrix(mats[i].rows, mats[j].cols)\n                        for j in range(len(mats))]\n                        for i in range(len(mats))]\n        return ImmutableDenseMatrix(data)\n\n(B)                         for i in range(len(mats))]\n        return ImmutableDenseMatrix(data)\n\n    @property\n    def shape(self):\n        return (sum(block.rows for block in self.args),\n                sum(block.cols for block in self.args))\n(C) \n    @property\n    def shape(self):\n        return (sum(block.rows for block in self.args),\n                sum(block.cols for block in self.args))\n\n    @property\n(D)         data = [[mats[i] if i == j else ZeroMatrix(mats[i].rows, mats[j].cols)\n                        for j in range(len(mats))]\n                        for i in range(len(mats))]\n        return ImmutableDenseMatrix(data)\n\n    @property\n    def shape(self):", "answer": "D"}
{"uuid": "f04e7902-f39f-42bf-ac33-0713c9a360d1", "issue_statement": "uniq modifies list argument\nWhen you iterate over a dictionary or set and try to modify it while doing so you get an error from Python:\r\n```python\r\n>>> multiset('THISTLE')\r\n{'T': 2, 'H': 1, 'I': 1, 'S': 1, 'L': 1, 'E': 1}\r\n>>> for i in _:\r\n...   _.pop(i)\r\n...\r\n2\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: dictionary changed size during iteration\r\n```\r\nIt would be good to do the same thing from within `uniq` because the output will silently be wrong if you modify a passed list:\r\n```python\r\n>>> f=list('THISTLE')\r\n>>> for i in uniq(f):\r\n...   f.remove(i)\r\n...   i\r\n...\r\n'T'\r\n'I'\r\n'L'\r\n```\r\nI think this would entail recording the size at the start and then checking the size and raising a similar RuntimeError if the size changes.", "choices": "(A) \n    >>> list(uniq(dat))\n    [1, 4, 5, 2]\n    >>> list(uniq(x for x in dat))\n    [1, 4, 5, 2]\n    >>> list(uniq([[1], [2, 1], [1]]))\n    [[1], [2, 1]]\n    \"\"\"\n    try:\n        seen = set()\n        result = result or []\n        for i, s in enumerate(seq):\n            if not (s in seen or seen.add(s)):\n                yield s\n    except TypeError:\n        if s not in result:\n            yield s\n            result.append(s)\n        if hasattr(seq, '__getitem__'):\n            for s in uniq(seq[i + 1:], result):\n                yield s\n        else:\n            for s in uniq(seq, result):\n                yield s\n\n\ndef generate_bell(n):\n    \"\"\"Return permutations of [0, 1, ..., n - 1] such that each permutation\n    differs from the last by the exchange of a single pair of neighbors.\n    The ``n!`` permutations are returned as an iterator. In order to obtain\n    the next permutation from a random starting permutation, use the\n    ``next_trotterjohnson`` method of the Permutation class (which generates\n    the same sequence in a different manner).\n\n    Examples\n    ========\n\n    >>> from itertools import permutations\n    >>> from sympy.utilities.iterables import generate_bell\n    >>> from sympy import zeros, Matrix\n\n    This is the sort of permutation used in the ringing of physical bells,\n    and does not produce permutations in lexicographical order. Rather, the\n    permutations differ from each other by exactly one inversion, and the\n    position at which the swapping occurs varies periodically in a simple\n    fashion. Consider the first few permutations of 4 elements generated\n    by ``permutations`` and ``generate_bell``:\n\n    >>> list(permutations(range(4)))[:5]\n    [(0, 1, 2, 3), (0, 1, 3, 2), (0, 2, 1, 3), (0, 2, 3, 1), (0, 3, 1, 2)]\n(B)     True\n    >>> has_variety((1, 1, 1))\n    False\n    \"\"\"\n    for i, s in enumerate(seq):\n        if i == 0:\n            sentinel = s\n        else:\n            if s != sentinel:\n                return True\n    return False\n\n\ndef uniq(seq, result=None):\n    \"\"\"\n    Yield unique elements from ``seq`` as an iterator. The second\n    parameter ``result``  is used internally; it is not necessary to pass\n    anything for this.\n\n    Examples\n    ========\n\n    >>> from sympy.utilities.iterables import uniq\n    >>> dat = [1, 4, 1, 5, 4, 2, 1, 2]\n    >>> type(uniq(dat)) in (list, tuple)\n    False\n\n    >>> list(uniq(dat))\n    [1, 4, 5, 2]\n    >>> list(uniq(x for x in dat))\n    [1, 4, 5, 2]\n    >>> list(uniq([[1], [2, 1], [1]]))\n    [[1], [2, 1]]\n    \"\"\"\n    try:\n        seen = set()\n        result = result or []\n        for i, s in enumerate(seq):\n            if not (s in seen or seen.add(s)):\n                yield s\n    except TypeError:\n        if s not in result:\n            yield s\n            result.append(s)\n        if hasattr(seq, '__getitem__'):\n            for s in uniq(seq[i + 1:], result):\n                yield s\n        else:\n            for s in uniq(seq, result):\n                yield s\n(C)             if not (s in seen or seen.add(s)):\n                yield s\n    except TypeError:\n        if s not in result:\n            yield s\n            result.append(s)\n        if hasattr(seq, '__getitem__'):\n            for s in uniq(seq[i + 1:], result):\n                yield s\n        else:\n            for s in uniq(seq, result):\n                yield s\n\n\ndef generate_bell(n):\n    \"\"\"Return permutations of [0, 1, ..., n - 1] such that each permutation\n    differs from the last by the exchange of a single pair of neighbors.\n    The ``n!`` permutations are returned as an iterator. In order to obtain\n    the next permutation from a random starting permutation, use the\n    ``next_trotterjohnson`` method of the Permutation class (which generates\n    the same sequence in a different manner).\n\n    Examples\n    ========\n\n    >>> from itertools import permutations\n    >>> from sympy.utilities.iterables import generate_bell\n    >>> from sympy import zeros, Matrix\n\n    This is the sort of permutation used in the ringing of physical bells,\n    and does not produce permutations in lexicographical order. Rather, the\n    permutations differ from each other by exactly one inversion, and the\n    position at which the swapping occurs varies periodically in a simple\n    fashion. Consider the first few permutations of 4 elements generated\n    by ``permutations`` and ``generate_bell``:\n\n    >>> list(permutations(range(4)))[:5]\n    [(0, 1, 2, 3), (0, 1, 3, 2), (0, 2, 1, 3), (0, 2, 3, 1), (0, 3, 1, 2)]\n    >>> list(generate_bell(4))[:5]\n    [(0, 1, 2, 3), (0, 1, 3, 2), (0, 3, 1, 2), (3, 0, 1, 2), (3, 0, 2, 1)]\n\n    Notice how the 2nd and 3rd lexicographical permutations have 3 elements\n    out of place whereas each \"bell\" permutation always has only two\n    elements out of place relative to the previous permutation (and so the\n    signature (+/-1) of a permutation is opposite of the signature of the\n    previous permutation).\n\n    How the position of inversion varies across the elements can be seen\n    by tracing out where the largest number appears in the permutations:\n\n(D) def uniq(seq, result=None):\n    \"\"\"\n    Yield unique elements from ``seq`` as an iterator. The second\n    parameter ``result``  is used internally; it is not necessary to pass\n    anything for this.\n\n    Examples\n    ========\n\n    >>> from sympy.utilities.iterables import uniq\n    >>> dat = [1, 4, 1, 5, 4, 2, 1, 2]\n    >>> type(uniq(dat)) in (list, tuple)\n    False\n\n    >>> list(uniq(dat))\n    [1, 4, 5, 2]\n    >>> list(uniq(x for x in dat))\n    [1, 4, 5, 2]\n    >>> list(uniq([[1], [2, 1], [1]]))\n    [[1], [2, 1]]\n    \"\"\"\n    try:\n        seen = set()\n        result = result or []\n        for i, s in enumerate(seq):\n            if not (s in seen or seen.add(s)):\n                yield s\n    except TypeError:\n        if s not in result:\n            yield s\n            result.append(s)\n        if hasattr(seq, '__getitem__'):\n            for s in uniq(seq[i + 1:], result):\n                yield s\n        else:\n            for s in uniq(seq, result):\n                yield s\n\n\ndef generate_bell(n):\n    \"\"\"Return permutations of [0, 1, ..., n - 1] such that each permutation\n    differs from the last by the exchange of a single pair of neighbors.\n    The ``n!`` permutations are returned as an iterator. In order to obtain\n    the next permutation from a random starting permutation, use the\n    ``next_trotterjohnson`` method of the Permutation class (which generates\n    the same sequence in a different manner).\n\n    Examples\n    ========\n", "answer": "D"}
{"uuid": "66f15ff4-4745-44e8-a583-388b7d844964", "issue_statement": "Wrong matrix element fetched from BlockMatrix\nGiven this code:\r\n```\r\nfrom sympy import *\r\nn, i = symbols('n, i', integer=True)\r\nA = MatrixSymbol('A', 1, 1)\r\nB = MatrixSymbol('B', n, 1)\r\nC = BlockMatrix([[A], [B]])\r\nprint('C is')\r\npprint(C)\r\nprint('C[i, 0] is')\r\npprint(C[i, 0])\r\n```\r\nI get this output:\r\n```\r\nC is\r\n\u23a1A\u23a4\r\n\u23a2 \u23a5\r\n\u23a3B\u23a6\r\nC[i, 0] is\r\n(A)[i, 0]\r\n```\r\n`(A)[i, 0]` is the wrong here. `C[i, 0]` should not be simplified as that element may come from either `A` or `B`.", "choices": "(A)         M = self.blocks\n        for i in range(M.shape[0]):\n            numrows += M[i, 0].shape[0]\n        for i in range(M.shape[1]):\n            numcols += M[0, i].shape[1]\n        return (numrows, numcols)\n\n    @property\n    def blockshape(self):\n        return self.blocks.shape\n\n    @property\n    def blocks(self):\n        return self.args[0]\n\n    @property\n    def rowblocksizes(self):\n        return [self.blocks[i, 0].rows for i in range(self.blockshape[0])]\n\n    @property\n    def colblocksizes(self):\n        return [self.blocks[0, i].cols for i in range(self.blockshape[1])]\n\n    def structurally_equal(self, other):\n        return (isinstance(other, BlockMatrix)\n            and self.shape == other.shape\n            and self.blockshape == other.blockshape\n            and self.rowblocksizes == other.rowblocksizes\n            and self.colblocksizes == other.colblocksizes)\n\n    def _blockmul(self, other):\n        if (isinstance(other, BlockMatrix) and\n                self.colblocksizes == other.rowblocksizes):\n            return BlockMatrix(self.blocks*other.blocks)\n\n        return self * other\n\n    def _blockadd(self, other):\n        if (isinstance(other, BlockMatrix)\n                and self.structurally_equal(other)):\n            return BlockMatrix(self.blocks + other.blocks)\n\n        return self + other\n\n    def _eval_transpose(self):\n        # Flip all the individual matrices\n        matrices = [transpose(matrix) for matrix in self.blocks]\n        # Make a copy\n        M = Matrix(self.blockshape[0], self.blockshape[1], matrices)\n        # Transpose the block structure\n        M = M.transpose()\n        return BlockMatrix(M)\n\n    def _eval_trace(self):\n        if self.rowblocksizes == self.colblocksizes:\n            return Add(*[Trace(self.blocks[i, i])\n                        for i in range(self.blockshape[0])])\n        raise NotImplementedError(\n            \"Can't perform trace of irregular blockshape\")\n\n    def _eval_determinant(self):\n        if self.blockshape == (2, 2):\n            [[A, B],\n             [C, D]] = self.blocks.tolist()\n            if ask(Q.invertible(A)):\n                return det(A)*det(D - C*A.I*B)\n            elif ask(Q.invertible(D)):\n                return det(D)*det(A - B*D.I*C)\n        return Determinant(self)\n\n    def as_real_imag(self):\n        real_matrices = [re(matrix) for matrix in self.blocks]\n        real_matrices = Matrix(self.blockshape[0], self.blockshape[1], real_matrices)\n\n        im_matrices = [im(matrix) for matrix in self.blocks]\n        im_matrices = Matrix(self.blockshape[0], self.blockshape[1], im_matrices)\n\n        return (real_matrices, im_matrices)\n\n    def transpose(self):\n        \"\"\"Return transpose of matrix.\n\n        Examples\n        ========\n\n        >>> from sympy import MatrixSymbol, BlockMatrix, ZeroMatrix\n        >>> from sympy.abc import l, m, n\n        >>> X = MatrixSymbol('X', n, n)\n        >>> Y = MatrixSymbol('Y', m ,m)\n        >>> Z = MatrixSymbol('Z', n, m)\n        >>> B = BlockMatrix([[X, Z], [ZeroMatrix(m,n), Y]])\n        >>> B.transpose()\n        Matrix([\n        [X.T,  0],\n        [Z.T, Y.T]])\n        >>> _.transpose()\n        Matrix([\n        [X, Z],\n        [0, Y]])\n        \"\"\"\n        return self._eval_transpose()\n\n    def _entry(self, i, j, **kwargs):\n        # Find row entry\n        for row_block, numrows in enumerate(self.rowblocksizes):\n            if (i < numrows) != False:\n                break\n            else:\n                i -= numrows\n        for col_block, numcols in enumerate(self.colblocksizes):\n            if (j < numcols) != False:\n                break\n            else:\n                j -= numcols\n        return self.blocks[row_block, col_block][i, j]\n\n    @property\n    def is_Identity(self):\n        if self.blockshape[0] != self.blockshape[1]:\n            return False\n        for i in range(self.blockshape[0]):\n            for j in range(self.blockshape[1]):\n                if i==j and not self.blocks[i, j].is_Identity:\n                    return False\n                if i!=j and not self.blocks[i, j].is_ZeroMatrix:\n                    return False\n        return True\n\n    @property\n    def is_structurally_symmetric(self):\n        return self.rowblocksizes == self.colblocksizes\n\n    def equals(self, other):\n        if self == other:\n            return True\n        if (isinstance(other, BlockMatrix) and self.blocks == other.blocks):\n            return True\n        return super(BlockMatrix, self).equals(other)\n\n\nclass BlockDiagMatrix(BlockMatrix):\n    \"\"\"\n    A BlockDiagMatrix is a BlockMatrix with matrices only along the diagonal\n\n    >>> from sympy import MatrixSymbol, BlockDiagMatrix, symbols, Identity\n    >>> n, m, l = symbols('n m l')\n    >>> X = MatrixSymbol('X', n, n)\n    >>> Y = MatrixSymbol('Y', m ,m)\n    >>> BlockDiagMatrix(X, Y)\n    Matrix([\n    [X, 0],\n    [0, Y]])\n\n    See Also\n    ========\n    sympy.matrices.dense.diag\n    \"\"\"\n    def __new__(cls, *mats):\n        return Basic.__new__(BlockDiagMatrix, *mats)\n\n    @property\n    def diag(self):\n        return self.args\n\n    @property\n    def blocks(self):\n        from sympy.matrices.immutable import ImmutableDenseMatrix\n        mats = self.args\n        data = [[mats[i] if i == j else ZeroMatrix(mats[i].rows, mats[j].cols)\n                        for j in range(len(mats))]\n                        for i in range(len(mats))]\n        return ImmutableDenseMatrix(data, evaluate=False)\n\n    @property\n    def shape(self):\n        return (sum(block.rows for block in self.args),\n                sum(block.cols for block in self.args))\n\n    @property\n    def blockshape(self):\n        n = len(self.args)\n        return (n, n)\n\n    @property\n    def rowblocksizes(self):\n        return [block.rows for block in self.args]\n\n    @property\n    def colblocksizes(self):\n        return [block.cols for block in self.args]\n\n    def _eval_inverse(self, expand='ignored'):\n        return BlockDiagMatrix(*[mat.inverse() for mat in self.args])\n\n    def _eval_transpose(self):\n        return BlockDiagMatrix(*[mat.transpose() for mat in self.args])\n\n    def _blockmul(self, other):\n        if (isinstance(other, BlockDiagMatrix) and\n                self.colblocksizes == other.rowblocksizes):\n            return BlockDiagMatrix(*[a*b for a, b in zip(self.args, other.args)])\n        else:\n            return BlockMatrix._blockmul(self, other)\n\n    def _blockadd(self, other):\n        if (isinstance(other, BlockDiagMatrix) and\n                self.blockshape == other.blockshape and\n                self.rowblocksizes == other.rowblocksizes and\n                self.colblocksizes == other.colblocksizes):\n            return BlockDiagMatrix(*[a + b for a, b in zip(self.args, other.args)])\n        else:\n            return BlockMatrix._blockadd(self, other)\n\n\ndef block_collapse(expr):\n    \"\"\"Evaluates a block matrix expression\n\n    >>> from sympy import MatrixSymbol, BlockMatrix, symbols, \\\n                          Identity, Matrix, ZeroMatrix, block_collapse\n    >>> n,m,l = symbols('n m l')\n    >>> X = MatrixSymbol('X', n, n)\n    >>> Y = MatrixSymbol('Y', m ,m)\n    >>> Z = MatrixSymbol('Z', n, m)\n    >>> B = BlockMatrix([[X, Z], [ZeroMatrix(m, n), Y]])\n    >>> print(B)\n    Matrix([\n    [X, Z],\n    [0, Y]])\n\n    >>> C = BlockMatrix([[Identity(n), Z]])\n    >>> print(C)\n    Matrix([[I, Z]])\n\n    >>> print(block_collapse(C*B))\n    Matrix([[X, Z + Z*Y]])\n    \"\"\"\n    from sympy.strategies.util import expr_fns\n\n    hasbm = lambda expr: isinstance(expr, MatrixExpr) and expr.has(BlockMatrix)\n\n    conditioned_rl = condition(\n        hasbm,\n        typed(\n            {MatAdd: do_one(bc_matadd, bc_block_plus_ident),\n             MatMul: do_one(bc_matmul, bc_dist),\n             MatPow: bc_matmul,\n             Transpose: bc_transpose,\n             Inverse: bc_inverse,\n             BlockMatrix: do_one(bc_unpack, deblock)}\n        )\n    )\n(B)     [1, 1, 2, 2, 2],\n    [3, 3, 3, 4, 4],\n    [3, 3, 3, 4, 4]])\n\n    See Also\n    ========\n    sympy.matrices.matrices.MatrixBase.irregular\n    \"\"\"\n    def __new__(cls, *args, **kwargs):\n        from sympy.matrices.immutable import ImmutableDenseMatrix\n        from sympy.utilities.iterables import is_sequence\n        isMat = lambda i: getattr(i, 'is_Matrix', False)\n        if len(args) != 1 or \\\n                not is_sequence(args[0]) or \\\n                len(set([isMat(r) for r in args[0]])) != 1:\n            raise ValueError(filldedent('''\n                expecting a sequence of 1 or more rows\n                containing Matrices.'''))\n        rows = args[0] if args else []\n        if not isMat(rows):\n            if rows and isMat(rows[0]):\n                rows = [rows]  # rows is not list of lists or []\n            # regularity check\n            # same number of matrices in each row\n            blocky = ok = len(set([len(r) for r in rows])) == 1\n            if ok:\n                # same number of rows for each matrix in a row\n                for r in rows:\n                    ok = len(set([i.rows for i in r])) == 1\n                    if not ok:\n                        break\n                blocky = ok\n                # same number of cols for each matrix in each col\n                for c in range(len(rows[0])):\n                    ok = len(set([rows[i][c].cols\n                        for i in range(len(rows))])) == 1\n                    if not ok:\n                        break\n            if not ok:\n                # same total cols in each row\n                ok = len(set([\n                    sum([i.cols for i in r]) for r in rows])) == 1\n                if blocky and ok:\n                    raise ValueError(filldedent('''\n                        Although this matrix is comprised of blocks,\n                        the blocks do not fill the matrix in a\n                        size-symmetric fashion. To create a full matrix\n                        from these arguments, pass them directly to\n                        Matrix.'''))\n                raise ValueError(filldedent('''\n                    When there are not the same number of rows in each\n                    row's matrices or there are not the same number of\n                    total columns in each row, the matrix is not a\n                    block matrix. If this matrix is known to consist of\n                    blocks fully filling a 2-D space then see\n                    Matrix.irregular.'''))\n        mat = ImmutableDenseMatrix(rows, evaluate=False)\n        obj = Basic.__new__(cls, mat)\n        return obj\n\n    @property\n    def shape(self):\n        numrows = numcols = 0\n        M = self.blocks\n        for i in range(M.shape[0]):\n            numrows += M[i, 0].shape[0]\n        for i in range(M.shape[1]):\n            numcols += M[0, i].shape[1]\n        return (numrows, numcols)\n\n    @property\n    def blockshape(self):\n        return self.blocks.shape\n\n    @property\n    def blocks(self):\n        return self.args[0]\n\n    @property\n    def rowblocksizes(self):\n        return [self.blocks[i, 0].rows for i in range(self.blockshape[0])]\n\n    @property\n    def colblocksizes(self):\n        return [self.blocks[0, i].cols for i in range(self.blockshape[1])]\n\n    def structurally_equal(self, other):\n        return (isinstance(other, BlockMatrix)\n            and self.shape == other.shape\n            and self.blockshape == other.blockshape\n            and self.rowblocksizes == other.rowblocksizes\n            and self.colblocksizes == other.colblocksizes)\n\n    def _blockmul(self, other):\n        if (isinstance(other, BlockMatrix) and\n                self.colblocksizes == other.rowblocksizes):\n            return BlockMatrix(self.blocks*other.blocks)\n\n        return self * other\n\n    def _blockadd(self, other):\n        if (isinstance(other, BlockMatrix)\n                and self.structurally_equal(other)):\n            return BlockMatrix(self.blocks + other.blocks)\n\n        return self + other\n\n    def _eval_transpose(self):\n        # Flip all the individual matrices\n        matrices = [transpose(matrix) for matrix in self.blocks]\n        # Make a copy\n        M = Matrix(self.blockshape[0], self.blockshape[1], matrices)\n        # Transpose the block structure\n        M = M.transpose()\n        return BlockMatrix(M)\n\n    def _eval_trace(self):\n        if self.rowblocksizes == self.colblocksizes:\n            return Add(*[Trace(self.blocks[i, i])\n                        for i in range(self.blockshape[0])])\n        raise NotImplementedError(\n            \"Can't perform trace of irregular blockshape\")\n\n    def _eval_determinant(self):\n        if self.blockshape == (2, 2):\n            [[A, B],\n             [C, D]] = self.blocks.tolist()\n            if ask(Q.invertible(A)):\n                return det(A)*det(D - C*A.I*B)\n            elif ask(Q.invertible(D)):\n                return det(D)*det(A - B*D.I*C)\n        return Determinant(self)\n\n    def as_real_imag(self):\n        real_matrices = [re(matrix) for matrix in self.blocks]\n        real_matrices = Matrix(self.blockshape[0], self.blockshape[1], real_matrices)\n\n        im_matrices = [im(matrix) for matrix in self.blocks]\n        im_matrices = Matrix(self.blockshape[0], self.blockshape[1], im_matrices)\n\n        return (real_matrices, im_matrices)\n\n    def transpose(self):\n        \"\"\"Return transpose of matrix.\n\n        Examples\n        ========\n\n        >>> from sympy import MatrixSymbol, BlockMatrix, ZeroMatrix\n        >>> from sympy.abc import l, m, n\n        >>> X = MatrixSymbol('X', n, n)\n        >>> Y = MatrixSymbol('Y', m ,m)\n        >>> Z = MatrixSymbol('Z', n, m)\n        >>> B = BlockMatrix([[X, Z], [ZeroMatrix(m,n), Y]])\n        >>> B.transpose()\n        Matrix([\n        [X.T,  0],\n        [Z.T, Y.T]])\n        >>> _.transpose()\n        Matrix([\n        [X, Z],\n        [0, Y]])\n        \"\"\"\n        return self._eval_transpose()\n\n    def _entry(self, i, j, **kwargs):\n        # Find row entry\n        for row_block, numrows in enumerate(self.rowblocksizes):\n            if (i < numrows) != False:\n                break\n            else:\n                i -= numrows\n        for col_block, numcols in enumerate(self.colblocksizes):\n            if (j < numcols) != False:\n                break\n            else:\n                j -= numcols\n        return self.blocks[row_block, col_block][i, j]\n\n    @property\n    def is_Identity(self):\n        if self.blockshape[0] != self.blockshape[1]:\n            return False\n        for i in range(self.blockshape[0]):\n            for j in range(self.blockshape[1]):\n                if i==j and not self.blocks[i, j].is_Identity:\n                    return False\n                if i!=j and not self.blocks[i, j].is_ZeroMatrix:\n                    return False\n        return True\n\n    @property\n    def is_structurally_symmetric(self):\n        return self.rowblocksizes == self.colblocksizes\n\n    def equals(self, other):\n        if self == other:\n            return True\n        if (isinstance(other, BlockMatrix) and self.blocks == other.blocks):\n            return True\n        return super(BlockMatrix, self).equals(other)\n\n\nclass BlockDiagMatrix(BlockMatrix):\n    \"\"\"\n    A BlockDiagMatrix is a BlockMatrix with matrices only along the diagonal\n\n    >>> from sympy import MatrixSymbol, BlockDiagMatrix, symbols, Identity\n    >>> n, m, l = symbols('n m l')\n    >>> X = MatrixSymbol('X', n, n)\n    >>> Y = MatrixSymbol('Y', m ,m)\n    >>> BlockDiagMatrix(X, Y)\n    Matrix([\n    [X, 0],\n    [0, Y]])\n\n    See Also\n    ========\n    sympy.matrices.dense.diag\n    \"\"\"\n    def __new__(cls, *mats):\n        return Basic.__new__(BlockDiagMatrix, *mats)\n\n    @property\n    def diag(self):\n        return self.args\n\n    @property\n    def blocks(self):\n        from sympy.matrices.immutable import ImmutableDenseMatrix\n        mats = self.args\n        data = [[mats[i] if i == j else ZeroMatrix(mats[i].rows, mats[j].cols)\n                        for j in range(len(mats))]\n                        for i in range(len(mats))]\n        return ImmutableDenseMatrix(data, evaluate=False)\n\n    @property\n    def shape(self):\n        return (sum(block.rows for block in self.args),\n                sum(block.cols for block in self.args))\n\n    @property\n    def blockshape(self):\n        n = len(self.args)\n        return (n, n)\n\n    @property\n    def rowblocksizes(self):\n        return [block.rows for block in self.args]\n\n    @property\n(C) from sympy.utilities import sift\nfrom sympy.utilities.misc import filldedent\n\nfrom sympy.matrices.expressions.matexpr import MatrixExpr, ZeroMatrix, Identity\nfrom sympy.matrices.expressions.matmul import MatMul\nfrom sympy.matrices.expressions.matadd import MatAdd\nfrom sympy.matrices.expressions.matpow import MatPow\nfrom sympy.matrices.expressions.transpose import Transpose, transpose\nfrom sympy.matrices.expressions.trace import Trace\nfrom sympy.matrices.expressions.determinant import det, Determinant\nfrom sympy.matrices.expressions.slice import MatrixSlice\nfrom sympy.matrices.expressions.inverse import Inverse\nfrom sympy.matrices import Matrix, ShapeError\nfrom sympy.functions.elementary.complexes import re, im\n\nclass BlockMatrix(MatrixExpr):\n    \"\"\"A BlockMatrix is a Matrix comprised of other matrices.\n\n    The submatrices are stored in a SymPy Matrix object but accessed as part of\n    a Matrix Expression\n\n    >>> from sympy import (MatrixSymbol, BlockMatrix, symbols,\n    ...     Identity, ZeroMatrix, block_collapse)\n    >>> n,m,l = symbols('n m l')\n    >>> X = MatrixSymbol('X', n, n)\n    >>> Y = MatrixSymbol('Y', m ,m)\n    >>> Z = MatrixSymbol('Z', n, m)\n    >>> B = BlockMatrix([[X, Z], [ZeroMatrix(m,n), Y]])\n    >>> print(B)\n    Matrix([\n    [X, Z],\n    [0, Y]])\n\n    >>> C = BlockMatrix([[Identity(n), Z]])\n    >>> print(C)\n    Matrix([[I, Z]])\n\n    >>> print(block_collapse(C*B))\n    Matrix([[X, Z + Z*Y]])\n\n    Some matrices might be comprised of rows of blocks with\n    the matrices in each row having the same height and the\n    rows all having the same total number of columns but\n    not having the same number of columns for each matrix\n    in each row. In this case, the matrix is not a block\n    matrix and should be instantiated by Matrix.\n\n    >>> from sympy import ones, Matrix\n    >>> dat = [\n    ... [ones(3,2), ones(3,3)*2],\n    ... [ones(2,3)*3, ones(2,2)*4]]\n    ...\n    >>> BlockMatrix(dat)\n    Traceback (most recent call last):\n    ...\n    ValueError:\n    Although this matrix is comprised of blocks, the blocks do not fill\n    the matrix in a size-symmetric fashion. To create a full matrix from\n    these arguments, pass them directly to Matrix.\n    >>> Matrix(dat)\n    Matrix([\n    [1, 1, 2, 2, 2],\n    [1, 1, 2, 2, 2],\n    [1, 1, 2, 2, 2],\n    [3, 3, 3, 4, 4],\n    [3, 3, 3, 4, 4]])\n\n    See Also\n    ========\n    sympy.matrices.matrices.MatrixBase.irregular\n    \"\"\"\n    def __new__(cls, *args, **kwargs):\n        from sympy.matrices.immutable import ImmutableDenseMatrix\n        from sympy.utilities.iterables import is_sequence\n        isMat = lambda i: getattr(i, 'is_Matrix', False)\n        if len(args) != 1 or \\\n                not is_sequence(args[0]) or \\\n                len(set([isMat(r) for r in args[0]])) != 1:\n            raise ValueError(filldedent('''\n                expecting a sequence of 1 or more rows\n                containing Matrices.'''))\n        rows = args[0] if args else []\n        if not isMat(rows):\n            if rows and isMat(rows[0]):\n                rows = [rows]  # rows is not list of lists or []\n            # regularity check\n            # same number of matrices in each row\n            blocky = ok = len(set([len(r) for r in rows])) == 1\n            if ok:\n                # same number of rows for each matrix in a row\n                for r in rows:\n                    ok = len(set([i.rows for i in r])) == 1\n                    if not ok:\n                        break\n                blocky = ok\n                # same number of cols for each matrix in each col\n                for c in range(len(rows[0])):\n                    ok = len(set([rows[i][c].cols\n                        for i in range(len(rows))])) == 1\n                    if not ok:\n                        break\n            if not ok:\n                # same total cols in each row\n                ok = len(set([\n                    sum([i.cols for i in r]) for r in rows])) == 1\n                if blocky and ok:\n                    raise ValueError(filldedent('''\n                        Although this matrix is comprised of blocks,\n                        the blocks do not fill the matrix in a\n                        size-symmetric fashion. To create a full matrix\n                        from these arguments, pass them directly to\n                        Matrix.'''))\n                raise ValueError(filldedent('''\n                    When there are not the same number of rows in each\n                    row's matrices or there are not the same number of\n                    total columns in each row, the matrix is not a\n                    block matrix. If this matrix is known to consist of\n                    blocks fully filling a 2-D space then see\n                    Matrix.irregular.'''))\n        mat = ImmutableDenseMatrix(rows, evaluate=False)\n        obj = Basic.__new__(cls, mat)\n        return obj\n\n    @property\n    def shape(self):\n        numrows = numcols = 0\n        M = self.blocks\n        for i in range(M.shape[0]):\n            numrows += M[i, 0].shape[0]\n        for i in range(M.shape[1]):\n            numcols += M[0, i].shape[1]\n        return (numrows, numcols)\n\n    @property\n    def blockshape(self):\n        return self.blocks.shape\n\n    @property\n    def blocks(self):\n        return self.args[0]\n\n    @property\n    def rowblocksizes(self):\n        return [self.blocks[i, 0].rows for i in range(self.blockshape[0])]\n\n    @property\n    def colblocksizes(self):\n        return [self.blocks[0, i].cols for i in range(self.blockshape[1])]\n\n    def structurally_equal(self, other):\n        return (isinstance(other, BlockMatrix)\n            and self.shape == other.shape\n            and self.blockshape == other.blockshape\n            and self.rowblocksizes == other.rowblocksizes\n            and self.colblocksizes == other.colblocksizes)\n\n    def _blockmul(self, other):\n        if (isinstance(other, BlockMatrix) and\n                self.colblocksizes == other.rowblocksizes):\n            return BlockMatrix(self.blocks*other.blocks)\n\n        return self * other\n\n    def _blockadd(self, other):\n        if (isinstance(other, BlockMatrix)\n                and self.structurally_equal(other)):\n            return BlockMatrix(self.blocks + other.blocks)\n\n        return self + other\n\n    def _eval_transpose(self):\n        # Flip all the individual matrices\n        matrices = [transpose(matrix) for matrix in self.blocks]\n        # Make a copy\n        M = Matrix(self.blockshape[0], self.blockshape[1], matrices)\n        # Transpose the block structure\n        M = M.transpose()\n        return BlockMatrix(M)\n\n    def _eval_trace(self):\n        if self.rowblocksizes == self.colblocksizes:\n            return Add(*[Trace(self.blocks[i, i])\n                        for i in range(self.blockshape[0])])\n        raise NotImplementedError(\n            \"Can't perform trace of irregular blockshape\")\n\n    def _eval_determinant(self):\n        if self.blockshape == (2, 2):\n            [[A, B],\n             [C, D]] = self.blocks.tolist()\n            if ask(Q.invertible(A)):\n                return det(A)*det(D - C*A.I*B)\n            elif ask(Q.invertible(D)):\n                return det(D)*det(A - B*D.I*C)\n        return Determinant(self)\n\n    def as_real_imag(self):\n        real_matrices = [re(matrix) for matrix in self.blocks]\n        real_matrices = Matrix(self.blockshape[0], self.blockshape[1], real_matrices)\n\n        im_matrices = [im(matrix) for matrix in self.blocks]\n        im_matrices = Matrix(self.blockshape[0], self.blockshape[1], im_matrices)\n\n        return (real_matrices, im_matrices)\n\n    def transpose(self):\n        \"\"\"Return transpose of matrix.\n\n        Examples\n        ========\n\n        >>> from sympy import MatrixSymbol, BlockMatrix, ZeroMatrix\n        >>> from sympy.abc import l, m, n\n        >>> X = MatrixSymbol('X', n, n)\n        >>> Y = MatrixSymbol('Y', m ,m)\n        >>> Z = MatrixSymbol('Z', n, m)\n        >>> B = BlockMatrix([[X, Z], [ZeroMatrix(m,n), Y]])\n        >>> B.transpose()\n        Matrix([\n        [X.T,  0],\n        [Z.T, Y.T]])\n        >>> _.transpose()\n        Matrix([\n        [X, Z],\n        [0, Y]])\n        \"\"\"\n        return self._eval_transpose()\n\n    def _entry(self, i, j, **kwargs):\n        # Find row entry\n        for row_block, numrows in enumerate(self.rowblocksizes):\n            if (i < numrows) != False:\n                break\n            else:\n                i -= numrows\n        for col_block, numcols in enumerate(self.colblocksizes):\n            if (j < numcols) != False:\n                break\n            else:\n                j -= numcols\n        return self.blocks[row_block, col_block][i, j]\n\n    @property\n    def is_Identity(self):\n        if self.blockshape[0] != self.blockshape[1]:\n            return False\n        for i in range(self.blockshape[0]):\n            for j in range(self.blockshape[1]):\n                if i==j and not self.blocks[i, j].is_Identity:\n                    return False\n                if i!=j and not self.blocks[i, j].is_ZeroMatrix:\n(D)              [C, D]] = self.blocks.tolist()\n            if ask(Q.invertible(A)):\n                return det(A)*det(D - C*A.I*B)\n            elif ask(Q.invertible(D)):\n                return det(D)*det(A - B*D.I*C)\n        return Determinant(self)\n\n    def as_real_imag(self):\n        real_matrices = [re(matrix) for matrix in self.blocks]\n        real_matrices = Matrix(self.blockshape[0], self.blockshape[1], real_matrices)\n\n        im_matrices = [im(matrix) for matrix in self.blocks]\n        im_matrices = Matrix(self.blockshape[0], self.blockshape[1], im_matrices)\n\n        return (real_matrices, im_matrices)\n\n    def transpose(self):\n        \"\"\"Return transpose of matrix.\n\n        Examples\n        ========\n\n        >>> from sympy import MatrixSymbol, BlockMatrix, ZeroMatrix\n        >>> from sympy.abc import l, m, n\n        >>> X = MatrixSymbol('X', n, n)\n        >>> Y = MatrixSymbol('Y', m ,m)\n        >>> Z = MatrixSymbol('Z', n, m)\n        >>> B = BlockMatrix([[X, Z], [ZeroMatrix(m,n), Y]])\n        >>> B.transpose()\n        Matrix([\n        [X.T,  0],\n        [Z.T, Y.T]])\n        >>> _.transpose()\n        Matrix([\n        [X, Z],\n        [0, Y]])\n        \"\"\"\n        return self._eval_transpose()\n\n    def _entry(self, i, j, **kwargs):\n        # Find row entry\n        for row_block, numrows in enumerate(self.rowblocksizes):\n            if (i < numrows) != False:\n                break\n            else:\n                i -= numrows\n        for col_block, numcols in enumerate(self.colblocksizes):\n            if (j < numcols) != False:\n                break\n            else:\n                j -= numcols\n        return self.blocks[row_block, col_block][i, j]\n\n    @property\n    def is_Identity(self):\n        if self.blockshape[0] != self.blockshape[1]:\n            return False\n        for i in range(self.blockshape[0]):\n            for j in range(self.blockshape[1]):\n                if i==j and not self.blocks[i, j].is_Identity:\n                    return False\n                if i!=j and not self.blocks[i, j].is_ZeroMatrix:\n                    return False\n        return True\n\n    @property\n    def is_structurally_symmetric(self):\n        return self.rowblocksizes == self.colblocksizes\n\n    def equals(self, other):\n        if self == other:\n            return True\n        if (isinstance(other, BlockMatrix) and self.blocks == other.blocks):\n            return True\n        return super(BlockMatrix, self).equals(other)\n\n\nclass BlockDiagMatrix(BlockMatrix):\n    \"\"\"\n    A BlockDiagMatrix is a BlockMatrix with matrices only along the diagonal\n\n    >>> from sympy import MatrixSymbol, BlockDiagMatrix, symbols, Identity\n    >>> n, m, l = symbols('n m l')\n    >>> X = MatrixSymbol('X', n, n)\n    >>> Y = MatrixSymbol('Y', m ,m)\n    >>> BlockDiagMatrix(X, Y)\n    Matrix([\n    [X, 0],\n    [0, Y]])\n\n    See Also\n    ========\n    sympy.matrices.dense.diag\n    \"\"\"\n    def __new__(cls, *mats):\n        return Basic.__new__(BlockDiagMatrix, *mats)\n\n    @property\n    def diag(self):\n        return self.args\n\n    @property\n    def blocks(self):\n        from sympy.matrices.immutable import ImmutableDenseMatrix\n        mats = self.args\n        data = [[mats[i] if i == j else ZeroMatrix(mats[i].rows, mats[j].cols)\n                        for j in range(len(mats))]\n                        for i in range(len(mats))]\n        return ImmutableDenseMatrix(data, evaluate=False)\n\n    @property\n    def shape(self):\n        return (sum(block.rows for block in self.args),\n                sum(block.cols for block in self.args))\n\n    @property\n    def blockshape(self):\n        n = len(self.args)\n        return (n, n)\n\n    @property\n    def rowblocksizes(self):\n        return [block.rows for block in self.args]\n\n    @property\n    def colblocksizes(self):\n        return [block.cols for block in self.args]\n\n    def _eval_inverse(self, expand='ignored'):\n        return BlockDiagMatrix(*[mat.inverse() for mat in self.args])\n\n    def _eval_transpose(self):\n        return BlockDiagMatrix(*[mat.transpose() for mat in self.args])\n\n    def _blockmul(self, other):\n        if (isinstance(other, BlockDiagMatrix) and\n                self.colblocksizes == other.rowblocksizes):\n            return BlockDiagMatrix(*[a*b for a, b in zip(self.args, other.args)])\n        else:\n            return BlockMatrix._blockmul(self, other)\n\n    def _blockadd(self, other):\n        if (isinstance(other, BlockDiagMatrix) and\n                self.blockshape == other.blockshape and\n                self.rowblocksizes == other.rowblocksizes and\n                self.colblocksizes == other.colblocksizes):\n            return BlockDiagMatrix(*[a + b for a, b in zip(self.args, other.args)])\n        else:\n            return BlockMatrix._blockadd(self, other)\n\n\ndef block_collapse(expr):\n    \"\"\"Evaluates a block matrix expression\n\n    >>> from sympy import MatrixSymbol, BlockMatrix, symbols, \\\n                          Identity, Matrix, ZeroMatrix, block_collapse\n    >>> n,m,l = symbols('n m l')\n    >>> X = MatrixSymbol('X', n, n)\n    >>> Y = MatrixSymbol('Y', m ,m)\n    >>> Z = MatrixSymbol('Z', n, m)\n    >>> B = BlockMatrix([[X, Z], [ZeroMatrix(m, n), Y]])\n    >>> print(B)\n    Matrix([\n    [X, Z],\n    [0, Y]])\n\n    >>> C = BlockMatrix([[Identity(n), Z]])\n    >>> print(C)\n    Matrix([[I, Z]])\n\n    >>> print(block_collapse(C*B))\n    Matrix([[X, Z + Z*Y]])\n    \"\"\"\n    from sympy.strategies.util import expr_fns\n\n    hasbm = lambda expr: isinstance(expr, MatrixExpr) and expr.has(BlockMatrix)\n\n    conditioned_rl = condition(\n        hasbm,\n        typed(\n            {MatAdd: do_one(bc_matadd, bc_block_plus_ident),\n             MatMul: do_one(bc_matmul, bc_dist),\n             MatPow: bc_matmul,\n             Transpose: bc_transpose,\n             Inverse: bc_inverse,\n             BlockMatrix: do_one(bc_unpack, deblock)}\n        )\n    )\n\n    rule = exhaust(\n        bottom_up(\n            exhaust(conditioned_rl),\n            fns=expr_fns\n        )\n    )\n\n    result = rule(expr)\n    doit = getattr(result, 'doit', None)\n    if doit is not None:\n        return doit()\n    else:\n        return result\n\ndef bc_unpack(expr):\n    if expr.blockshape == (1, 1):\n        return expr.blocks[0, 0]\n    return expr\n\ndef bc_matadd(expr):\n    args = sift(expr.args, lambda M: isinstance(M, BlockMatrix))\n    blocks = args[True]\n    if not blocks:\n        return expr\n\n    nonblocks = args[False]\n    block = blocks[0]\n    for b in blocks[1:]:\n        block = block._blockadd(b)\n    if nonblocks:\n        return MatAdd(*nonblocks) + block\n    else:\n        return block\n\ndef bc_block_plus_ident(expr):\n    idents = [arg for arg in expr.args if arg.is_Identity]\n    if not idents:\n        return expr\n\n    blocks = [arg for arg in expr.args if isinstance(arg, BlockMatrix)]\n    if (blocks and all(b.structurally_equal(blocks[0]) for b in blocks)\n               and blocks[0].is_structurally_symmetric):\n        block_id = BlockDiagMatrix(*[Identity(k)\n                                        for k in blocks[0].rowblocksizes])\n        return MatAdd(block_id * len(idents), *blocks).doit()\n\n    return expr\n\ndef bc_dist(expr):\n    \"\"\" Turn  a*[X, Y] into [a*X, a*Y] \"\"\"\n    factor, mat = expr.as_coeff_mmul()\n    if factor == 1:\n        return expr\n\n    unpacked = unpack(mat)\n\n    if isinstance(unpacked, BlockDiagMatrix):\n        B = unpacked.diag\n        new_B = [factor * mat for mat in B]\n        return BlockDiagMatrix(*new_B)\n    elif isinstance(unpacked, BlockMatrix):\n        B = unpacked.blocks", "answer": "C"}
{"uuid": "35d73f35-6ebc-440c-b1a2-449c7e9281dc", "issue_statement": "sympy.polys.factortools.dmp_zz_mignotte_bound improvement\nThe method `dup_zz_mignotte_bound(f, K)` can be significantly improved by using the **Knuth-Cohen bound** instead. After our research with Prof. Ag.Akritas we have implemented the Knuth-Cohen bound among others, and compare them among dozens of polynomials with different degree, density and coefficients range. Considering the results and the feedback from Mr.Kalevi Suominen, our proposal is that the mignotte_bound should be replaced by the knuth-cohen bound.\r\nAlso, `dmp_zz_mignotte_bound(f, u, K)` for mutli-variants polynomials should be replaced appropriately.", "choices": "(A) \n        deg(f) = deg(g) + deg(h)\n        deg(s) < deg(h)\n        deg(t) < deg(g)\n\n    returns polynomials `G`, `H`, `S` and `T`, such that::\n\n        f = G*H (mod m**2)\n        S*G + T*H = 1 (mod m**2)\n\n    References\n    ==========\n\n    .. [1] [Gathen99]_\n\n    \"\"\"\n    M = m**2\n\n    e = dup_sub_mul(f, g, h, K)\n    e = dup_trunc(e, M, K)\n\n    q, r = dup_div(dup_mul(s, e, K), h, K)\n\n    q = dup_trunc(q, M, K)\n    r = dup_trunc(r, M, K)\n\n    u = dup_add(dup_mul(t, e, K), dup_mul(q, g, K), K)\n    G = dup_trunc(dup_add(g, u, K), M, K)\n    H = dup_trunc(dup_add(h, r, K), M, K)\n\n    u = dup_add(dup_mul(s, G, K), dup_mul(t, H, K), K)\n    b = dup_trunc(dup_sub(u, [K.one], K), M, K)\n\n    c, d = dup_div(dup_mul(s, b, K), H, K)\n\n    c = dup_trunc(c, M, K)\n    d = dup_trunc(d, M, K)\n\n    u = dup_add(dup_mul(t, b, K), dup_mul(c, G, K), K)\n    S = dup_trunc(dup_sub(s, d, K), M, K)\n    T = dup_trunc(dup_sub(t, u, K), M, K)\n\n    return G, H, S, T\n\n\ndef dup_zz_hensel_lift(p, f, f_list, l, K):\n    \"\"\"\n    Multifactor Hensel lifting in `Z[x]`.\n\n    Given a prime `p`, polynomial `f` over `Z[x]` such that `lc(f)`\n    is a unit modulo `p`, monic pair-wise coprime polynomials `f_i`\n    over `Z[x]` satisfying::\n\n        f = lc(f) f_1 ... f_r (mod p)\n\n    and a positive integer `l`, returns a list of monic polynomials\n    `F_1`, `F_2`, ..., `F_r` satisfying::\n\n       f = lc(f) F_1 ... F_r (mod p**l)\n\n       F_i = f_i (mod p), i = 1..r\n\n    References\n    ==========\n(B) \n    return K.sqrt(K(n + 1))*2**n*a*b\n\n\ndef dup_zz_hensel_step(m, f, g, h, s, t, K):\n    \"\"\"\n    One step in Hensel lifting in `Z[x]`.\n\n    Given positive integer `m` and `Z[x]` polynomials `f`, `g`, `h`, `s`\n    and `t` such that::\n\n        f = g*h (mod m)\n        s*g + t*h = 1 (mod m)\n\n        lc(f) is not a zero divisor (mod m)\n        lc(h) = 1\n\n        deg(f) = deg(g) + deg(h)\n        deg(s) < deg(h)\n        deg(t) < deg(g)\n\n    returns polynomials `G`, `H`, `S` and `T`, such that::\n\n        f = G*H (mod m**2)\n        S*G + T*H = 1 (mod m**2)\n\n    References\n    ==========\n\n    .. [1] [Gathen99]_\n\n    \"\"\"\n    M = m**2\n\n    e = dup_sub_mul(f, g, h, K)\n    e = dup_trunc(e, M, K)\n\n    q, r = dup_div(dup_mul(s, e, K), h, K)\n\n    q = dup_trunc(q, M, K)\n    r = dup_trunc(r, M, K)\n\n    u = dup_add(dup_mul(t, e, K), dup_mul(q, g, K), K)\n    G = dup_trunc(dup_add(g, u, K), M, K)\n    H = dup_trunc(dup_add(h, r, K), M, K)\n\n    u = dup_add(dup_mul(s, G, K), dup_mul(t, H, K), K)\n    b = dup_trunc(dup_sub(u, [K.one], K), M, K)\n\n    c, d = dup_div(dup_mul(s, b, K), H, K)\n\n    c = dup_trunc(c, M, K)\n    d = dup_trunc(d, M, K)\n\n    u = dup_add(dup_mul(t, b, K), dup_mul(c, G, K), K)\n    S = dup_trunc(dup_sub(s, d, K), M, K)\n    T = dup_trunc(dup_sub(t, u, K), M, K)\n\n    return G, H, S, T\n\n\ndef dup_zz_hensel_lift(p, f, f_list, l, K):\n    \"\"\"\n    Multifactor Hensel lifting in `Z[x]`.\n(C)     result = []\n\n    for factor in factors:\n        k = 0\n\n        while True:\n            q, r = dmp_div(f, factor, u, K)\n\n            if dmp_zero_p(r, u):\n                f, k = q, k + 1\n            else:\n                break\n\n        result.append((factor, k))\n\n    return _sort_factors(result)\n\n\ndef dup_zz_mignotte_bound(f, K):\n    \"\"\"Mignotte bound for univariate polynomials in `K[x]`. \"\"\"\n    a = dup_max_norm(f, K)\n    b = abs(dup_LC(f, K))\n    n = dup_degree(f)\n\n    return K.sqrt(K(n + 1))*2**n*a*b\n\n\ndef dmp_zz_mignotte_bound(f, u, K):\n    \"\"\"Mignotte bound for multivariate polynomials in `K[X]`. \"\"\"\n    a = dmp_max_norm(f, u, K)\n    b = abs(dmp_ground_LC(f, u, K))\n    n = sum(dmp_degree_list(f, u))\n\n    return K.sqrt(K(n + 1))*2**n*a*b\n\n\ndef dup_zz_hensel_step(m, f, g, h, s, t, K):\n    \"\"\"\n    One step in Hensel lifting in `Z[x]`.\n\n    Given positive integer `m` and `Z[x]` polynomials `f`, `g`, `h`, `s`\n    and `t` such that::\n\n        f = g*h (mod m)\n        s*g + t*h = 1 (mod m)\n\n        lc(f) is not a zero divisor (mod m)\n        lc(h) = 1\n\n        deg(f) = deg(g) + deg(h)\n        deg(s) < deg(h)\n        deg(t) < deg(g)\n\n    returns polynomials `G`, `H`, `S` and `T`, such that::\n\n        f = G*H (mod m**2)\n        S*G + T*H = 1 (mod m**2)\n\n    References\n    ==========\n\n    .. [1] [Gathen99]_\n\n    \"\"\"\n(D) \n\ndef dup_zz_mignotte_bound(f, K):\n    \"\"\"Mignotte bound for univariate polynomials in `K[x]`. \"\"\"\n    a = dup_max_norm(f, K)\n    b = abs(dup_LC(f, K))\n    n = dup_degree(f)\n\n    return K.sqrt(K(n + 1))*2**n*a*b\n\n\ndef dmp_zz_mignotte_bound(f, u, K):\n    \"\"\"Mignotte bound for multivariate polynomials in `K[X]`. \"\"\"\n    a = dmp_max_norm(f, u, K)\n    b = abs(dmp_ground_LC(f, u, K))\n    n = sum(dmp_degree_list(f, u))\n\n    return K.sqrt(K(n + 1))*2**n*a*b\n\n\ndef dup_zz_hensel_step(m, f, g, h, s, t, K):\n    \"\"\"\n    One step in Hensel lifting in `Z[x]`.\n\n    Given positive integer `m` and `Z[x]` polynomials `f`, `g`, `h`, `s`\n    and `t` such that::\n\n        f = g*h (mod m)\n        s*g + t*h = 1 (mod m)\n\n        lc(f) is not a zero divisor (mod m)\n        lc(h) = 1\n\n        deg(f) = deg(g) + deg(h)\n        deg(s) < deg(h)\n        deg(t) < deg(g)\n\n    returns polynomials `G`, `H`, `S` and `T`, such that::\n\n        f = G*H (mod m**2)\n        S*G + T*H = 1 (mod m**2)\n\n    References\n    ==========\n\n    .. [1] [Gathen99]_\n\n    \"\"\"\n    M = m**2\n\n    e = dup_sub_mul(f, g, h, K)\n    e = dup_trunc(e, M, K)\n\n    q, r = dup_div(dup_mul(s, e, K), h, K)\n\n    q = dup_trunc(q, M, K)\n    r = dup_trunc(r, M, K)\n\n    u = dup_add(dup_mul(t, e, K), dup_mul(q, g, K), K)\n    G = dup_trunc(dup_add(g, u, K), M, K)\n    H = dup_trunc(dup_add(h, r, K), M, K)\n\n    u = dup_add(dup_mul(s, G, K), dup_mul(t, H, K), K)\n    b = dup_trunc(dup_sub(u, [K.one], K), M, K)", "answer": "D"}
{"uuid": "7ffd16b6-d22d-4d40-aa29-8dcbcb8a66ca", "issue_statement": "Rewrite sign as abs\nIn sympy the `sign` function is defined as\r\n```\r\n    sign(z)  :=  z / Abs(z)\r\n```\r\nfor all complex non-zero `z`. There should be a way to rewrite the sign in terms of `Abs` e.g.:\r\n```\r\n>>> sign(x).rewrite(Abs)                                                                                                                   \r\n x \r\n\u2500\u2500\u2500\r\n\u2502x\u2502\r\n```\r\nI'm not sure how the possibility of `x` being zero should be handled currently we have\r\n```\r\n>>> sign(0)                                                                                                                               \r\n0\r\n>>> 0 / Abs(0)                                                                                                                            \r\nnan\r\n```\r\nMaybe `sign(0)` should be `nan` as well. Otherwise maybe rewrite as Abs would have to be careful about the possibility of the arg being zero (that would make the rewrite fail in most cases).", "choices": "(A)     def _eval_simplify(self, **kwargs):\n        return self.func(self.args[0].factor())  # XXX include doit?\n\n\nclass Abs(Function):\n    \"\"\"\n    Return the absolute value of the argument.\n\n    This is an extension of the built-in function abs() to accept symbolic\n(B) \n\nclass Abs(Function):\n    \"\"\"\n    Return the absolute value of the argument.\n\n    This is an extension of the built-in function abs() to accept symbolic\n    values.  If you pass a SymPy expression to the built-in abs(), it will\n    pass it automatically to Abs().\n(C)         if arg.is_extended_real:\n            return Heaviside(arg, H0=S(1)/2) * 2 - 1\n\n    def _eval_simplify(self, **kwargs):\n        return self.func(self.args[0].factor())  # XXX include doit?\n\n\nclass Abs(Function):\n    \"\"\"\n(D) \n    def _eval_rewrite_as_Heaviside(self, arg, **kwargs):\n        from sympy.functions.special.delta_functions import Heaviside\n        if arg.is_extended_real:\n            return Heaviside(arg, H0=S(1)/2) * 2 - 1\n\n    def _eval_simplify(self, **kwargs):\n        return self.func(self.args[0].factor())  # XXX include doit?\n", "answer": "C"}
{"uuid": "85ba47f6-6513-41f5-851f-6cdcf2890c31", "issue_statement": "Point.vel() should calculate the velocity if possible\nIf you specify the orientation of two reference frames and then ask for the angular velocity between the two reference frames the angular velocity will be calculated. But if you try to do the same thing with velocities, this doesn't work. See below:\r\n\r\n```\r\nIn [1]: import sympy as sm                                                                               \r\n\r\nIn [2]: import sympy.physics.mechanics as me                                                             \r\n\r\nIn [3]: A = me.ReferenceFrame('A')                                                                       \r\n\r\nIn [5]: q = me.dynamicsymbols('q')                                                                       \r\n\r\nIn [6]: B = A.orientnew('B', 'Axis', (q, A.x))                                                           \r\n\r\nIn [7]: B.ang_vel_in(A)                                                                                  \r\nOut[7]: q'*A.x\r\n\r\nIn [9]: P = me.Point('P')                                                                                \r\n\r\nIn [10]: Q = me.Point('Q')                                                                               \r\n\r\nIn [11]: r = q*A.x + 2*q*A.y                                                                             \r\n\r\nIn [12]: Q.set_pos(P, r)                                                                                 \r\n\r\nIn [13]: Q.vel(A)                                                                                        \r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-13-0fc8041904cc> in <module>\r\n----> 1 Q.vel(A)\r\n\r\n~/miniconda3/lib/python3.6/site-packages/sympy/physics/vector/point.py in vel(self, frame)\r\n    453         if not (frame in self._vel_dict):\r\n    454             raise ValueError('Velocity of point ' + self.name + ' has not been'\r\n--> 455                              ' defined in ReferenceFrame ' + frame.name)\r\n    456         return self._vel_dict[frame]\r\n    457 \r\n\r\nValueError: Velocity of point Q has not been defined in ReferenceFrame A\r\n```\r\n\r\nThe expected result of the `Q.vel(A)` should be:\r\n\r\n```\r\nIn [14]: r.dt(A)                                                                                         \r\nOut[14]: q'*A.x + 2*q'*A.y\r\n```\r\n\r\nI think that this is possible. Maybe there is a reason it isn't implemented. But we should try to implement it because it is confusing why this works for orientations and not positions.", "choices": "(A)         >>> P = O.locatenew('P', 10 * B.x)\n        >>> O.set_vel(N, 5 * N.x)\n        >>> P.v2pt_theory(O, N, B)\n        5*N.x + 10*q'*B.y\n\n        \"\"\"\n\n        _check_frame(outframe)\n        _check_frame(fixedframe)\n        self._check_point(otherpoint)\n        dist = self.pos_from(otherpoint)\n        v = otherpoint.vel(outframe)\n        omega = fixedframe.ang_vel_in(outframe)\n        self.set_vel(outframe, v + (omega ^ dist))\n        return self.vel(outframe)\n\n    def vel(self, frame):\n        \"\"\"The velocity Vector of this Point in the ReferenceFrame.\n\n        Parameters\n        ==========\n\n        frame : ReferenceFrame\n            The frame in which the returned velocity vector will be defined in\n\n        Examples\n        ========\n\n        >>> from sympy.physics.vector import Point, ReferenceFrame\n        >>> N = ReferenceFrame('N')\n        >>> p1 = Point('p1')\n        >>> p1.set_vel(N, 10 * N.x)\n        >>> p1.vel(N)\n        10*N.x\n\n        \"\"\"\n\n        _check_frame(frame)\n        if not (frame in self._vel_dict):\n            raise ValueError('Velocity of point ' + self.name + ' has not been'\n                             ' defined in ReferenceFrame ' + frame.name)\n        return self._vel_dict[frame]\n\n    def partial_velocity(self, frame, *gen_speeds):\n        \"\"\"Returns the partial velocities of the linear velocity vector of this\n        point in the given frame with respect to one or more provided\n        generalized speeds.\n\n        Parameters\n(B)         Examples\n        ========\n\n        >>> from sympy.physics.vector import Point, ReferenceFrame\n        >>> N = ReferenceFrame('N')\n        >>> p1 = Point('p1')\n        >>> p1.set_vel(N, 10 * N.x)\n        >>> p1.vel(N)\n        10*N.x\n\n        \"\"\"\n\n        _check_frame(frame)\n        if not (frame in self._vel_dict):\n            raise ValueError('Velocity of point ' + self.name + ' has not been'\n                             ' defined in ReferenceFrame ' + frame.name)\n        return self._vel_dict[frame]\n\n    def partial_velocity(self, frame, *gen_speeds):\n        \"\"\"Returns the partial velocities of the linear velocity vector of this\n        point in the given frame with respect to one or more provided\n        generalized speeds.\n\n        Parameters\n        ==========\n        frame : ReferenceFrame\n            The frame with which the velocity is defined in.\n        gen_speeds : functions of time\n            The generalized speeds.\n\n        Returns\n        =======\n        partial_velocities : tuple of Vector\n            The partial velocity vectors corresponding to the provided\n            generalized speeds.\n\n        Examples\n        ========\n\n        >>> from sympy.physics.vector import ReferenceFrame, Point\n        >>> from sympy.physics.vector import dynamicsymbols\n        >>> N = ReferenceFrame('N')\n        >>> A = ReferenceFrame('A')\n        >>> p = Point('p')\n        >>> u1, u2 = dynamicsymbols('u1, u2')\n        >>> p.set_vel(N, u1 * N.x + u2 * A.y)\n        >>> p.partial_velocity(N, u1)\n        N.x\n        >>> p.partial_velocity(N, u1, u2)\n(C)         self.set_vel(outframe, v + (omega ^ dist))\n        return self.vel(outframe)\n\n    def vel(self, frame):\n        \"\"\"The velocity Vector of this Point in the ReferenceFrame.\n\n        Parameters\n        ==========\n\n        frame : ReferenceFrame\n            The frame in which the returned velocity vector will be defined in\n\n        Examples\n        ========\n\n        >>> from sympy.physics.vector import Point, ReferenceFrame\n        >>> N = ReferenceFrame('N')\n        >>> p1 = Point('p1')\n        >>> p1.set_vel(N, 10 * N.x)\n        >>> p1.vel(N)\n        10*N.x\n\n        \"\"\"\n\n        _check_frame(frame)\n        if not (frame in self._vel_dict):\n            raise ValueError('Velocity of point ' + self.name + ' has not been'\n                             ' defined in ReferenceFrame ' + frame.name)\n        return self._vel_dict[frame]\n\n    def partial_velocity(self, frame, *gen_speeds):\n        \"\"\"Returns the partial velocities of the linear velocity vector of this\n        point in the given frame with respect to one or more provided\n        generalized speeds.\n\n        Parameters\n        ==========\n        frame : ReferenceFrame\n            The frame with which the velocity is defined in.\n        gen_speeds : functions of time\n            The generalized speeds.\n\n        Returns\n        =======\n        partial_velocities : tuple of Vector\n            The partial velocity vectors corresponding to the provided\n            generalized speeds.\n\n        Examples\n(D)             The frame in which both points are fixed (B)\n\n        Examples\n        ========\n\n        >>> from sympy.physics.vector import Point, ReferenceFrame, dynamicsymbols\n        >>> from sympy.physics.vector import init_vprinting\n        >>> init_vprinting(pretty_print=False)\n        >>> q = dynamicsymbols('q')\n        >>> qd = dynamicsymbols('q', 1)\n        >>> N = ReferenceFrame('N')\n        >>> B = N.orientnew('B', 'Axis', [q, N.z])\n        >>> O = Point('O')\n        >>> P = O.locatenew('P', 10 * B.x)\n        >>> O.set_vel(N, 5 * N.x)\n        >>> P.v2pt_theory(O, N, B)\n        5*N.x + 10*q'*B.y\n\n        \"\"\"\n\n        _check_frame(outframe)\n        _check_frame(fixedframe)\n        self._check_point(otherpoint)\n        dist = self.pos_from(otherpoint)\n        v = otherpoint.vel(outframe)\n        omega = fixedframe.ang_vel_in(outframe)\n        self.set_vel(outframe, v + (omega ^ dist))\n        return self.vel(outframe)\n\n    def vel(self, frame):\n        \"\"\"The velocity Vector of this Point in the ReferenceFrame.\n\n        Parameters\n        ==========\n\n        frame : ReferenceFrame\n            The frame in which the returned velocity vector will be defined in\n\n        Examples\n        ========\n\n        >>> from sympy.physics.vector import Point, ReferenceFrame\n        >>> N = ReferenceFrame('N')\n        >>> p1 = Point('p1')\n        >>> p1.set_vel(N, 10 * N.x)\n        >>> p1.vel(N)\n        10*N.x\n\n        \"\"\"", "answer": "A"}
{"uuid": "a2197ee8-153b-4022-8eb0-50f63293266d", "issue_statement": "partitions() reusing the output dictionaries\nThe partitions() iterator in sympy.utilities.iterables reuses the output dictionaries. There is a caveat about it in the docstring. \r\n\r\nI'm wondering if it's really that important for it to do this. It shouldn't be that much of a performance loss to copy the dictionary before yielding it. This behavior is very confusing. It means that something as simple as list(partitions()) will give an apparently wrong result. And it can lead to much more subtle bugs if the partitions are used in a nontrivial way.", "choices": "(A)     if n == 0:\n        if size:\n            yield 1, {0: 1}\n        else:\n            yield {0: 1}\n        return\n\n    k = min(k or n, n)\n\n    n, m, k = as_int(n), as_int(m), as_int(k)\n    q, r = divmod(n, k)\n    ms = {k: q}\n    keys = [k]  # ms.keys(), from largest to smallest\n    if r:\n        ms[r] = 1\n        keys.append(r)\n    room = m - q - bool(r)\n    if size:\n        yield sum(ms.values()), ms\n    else:\n        yield ms\n\n    while keys != [1]:\n        # Reuse any 1's.\n        if keys[-1] == 1:\n            del keys[-1]\n            reuse = ms.pop(1)\n            room += reuse\n        else:\n            reuse = 0\n\n        while 1:\n            # Let i be the smallest key larger than 1.  Reuse one\n            # instance of i.\n            i = keys[-1]\n            newcount = ms[i] = ms[i] - 1\n            reuse += i\n            if newcount == 0:\n                del keys[-1], ms[i]\n            room += 1\n\n            # Break the remainder into pieces of size i-1.\n            i -= 1\n            q, r = divmod(reuse, i)\n            need = q + bool(r)\n            if need > room:\n                if not keys:\n                    return\n                continue\n\n            ms[i] = q\n            keys.append(i)\n            if r:\n                ms[r] = 1\n                keys.append(r)\n            break\n        room -= need\n        if size:\n            yield sum(ms.values()), ms\n        else:\n            yield ms\n\n\ndef ordered_partitions(n, m=None, sort=True):\n    \"\"\"Generates ordered partitions of integer ``n``.\n\n    Parameters\n    ==========\n\n    m : integer (default None)\n        The default value gives partitions of all sizes else only\n        those with size m. In addition, if ``m`` is not None then\n        partitions are generated *in place* (see examples).\n    sort : bool (default True)\n        Controls whether partitions are\n        returned in sorted order when ``m`` is not None; when False,\n        the partitions are returned as fast as possible with elements\n        sorted, but when m|n the partitions will not be in\n        ascending lexicographical order.\n\n    Examples\n    ========\n\n    >>> from sympy.utilities.iterables import ordered_partitions\n\n    All partitions of 5 in ascending lexicographical:\n\n    >>> for p in ordered_partitions(5):\n    ...     print(p)\n    [1, 1, 1, 1, 1]\n    [1, 1, 1, 2]\n    [1, 1, 3]\n    [1, 2, 2]\n    [1, 4]\n    [2, 3]\n    [5]\n\n    Only partitions of 5 with two parts:\n(B) \n    Examples\n    ========\n\n    >>> from sympy.utilities.iterables import partitions\n\n    The numbers appearing in the partition (the key of the returned dict)\n    are limited with k:\n\n    >>> for p in partitions(6, k=2):  # doctest: +SKIP\n    ...     print(p)\n    {2: 3}\n    {1: 2, 2: 2}\n    {1: 4, 2: 1}\n    {1: 6}\n\n    The maximum number of parts in the partition (the sum of the values in\n    the returned dict) are limited with m (default value, None, gives\n    partitions from 1 through n):\n\n    >>> for p in partitions(6, m=2):  # doctest: +SKIP\n    ...     print(p)\n    ...\n    {6: 1}\n    {1: 1, 5: 1}\n    {2: 1, 4: 1}\n    {3: 2}\n\n    Note that the _same_ dictionary object is returned each time.\n    This is for speed:  generating each partition goes quickly,\n    taking constant time, independent of n.\n\n    >>> [p for p in partitions(6, k=2)]\n    [{1: 6}, {1: 6}, {1: 6}, {1: 6}]\n\n    If you want to build a list of the returned dictionaries then\n    make a copy of them:\n\n    >>> [p.copy() for p in partitions(6, k=2)]  # doctest: +SKIP\n    [{2: 3}, {1: 2, 2: 2}, {1: 4, 2: 1}, {1: 6}]\n    >>> [(M, p.copy()) for M, p in partitions(6, k=2, size=True)]  # doctest: +SKIP\n    [(3, {2: 3}), (4, {1: 2, 2: 2}), (5, {1: 4, 2: 1}), (6, {1: 6})]\n\n    References\n    ==========\n\n    .. [1] modified from Tim Peter's version to allow for k and m values:\n           http://code.activestate.com/recipes/218332-generator-for-integer-partitions/\n\n    See Also\n    ========\n\n    sympy.combinatorics.partitions.Partition\n    sympy.combinatorics.partitions.IntegerPartition\n\n    \"\"\"\n    if (n <= 0 or\n        m is not None and m < 1 or\n        k is not None and k < 1 or\n        m and k and m*k < n):\n        # the empty set is the only way to handle these inputs\n        # and returning {} to represent it is consistent with\n        # the counting convention, e.g. nT(0) == 1.\n        if size:\n            yield 0, {}\n        else:\n            yield {}\n        return\n\n    if m is None:\n        m = n\n    else:\n        m = min(m, n)\n\n    if n == 0:\n        if size:\n            yield 1, {0: 1}\n        else:\n            yield {0: 1}\n        return\n\n    k = min(k or n, n)\n\n    n, m, k = as_int(n), as_int(m), as_int(k)\n    q, r = divmod(n, k)\n    ms = {k: q}\n    keys = [k]  # ms.keys(), from largest to smallest\n    if r:\n        ms[r] = 1\n        keys.append(r)\n    room = m - q - bool(r)\n    if size:\n        yield sum(ms.values()), ms\n    else:\n        yield ms\n\n    while keys != [1]:\n        # Reuse any 1's.\n(C)     ========\n\n    sympy.combinatorics.partitions.Partition\n    sympy.combinatorics.partitions.IntegerPartition\n\n    \"\"\"\n    if (n <= 0 or\n        m is not None and m < 1 or\n        k is not None and k < 1 or\n        m and k and m*k < n):\n        # the empty set is the only way to handle these inputs\n        # and returning {} to represent it is consistent with\n        # the counting convention, e.g. nT(0) == 1.\n        if size:\n            yield 0, {}\n        else:\n            yield {}\n        return\n\n    if m is None:\n        m = n\n    else:\n        m = min(m, n)\n\n    if n == 0:\n        if size:\n            yield 1, {0: 1}\n        else:\n            yield {0: 1}\n        return\n\n    k = min(k or n, n)\n\n    n, m, k = as_int(n), as_int(m), as_int(k)\n    q, r = divmod(n, k)\n    ms = {k: q}\n    keys = [k]  # ms.keys(), from largest to smallest\n    if r:\n        ms[r] = 1\n        keys.append(r)\n    room = m - q - bool(r)\n    if size:\n        yield sum(ms.values()), ms\n    else:\n        yield ms\n\n    while keys != [1]:\n        # Reuse any 1's.\n        if keys[-1] == 1:\n            del keys[-1]\n            reuse = ms.pop(1)\n            room += reuse\n        else:\n            reuse = 0\n\n        while 1:\n            # Let i be the smallest key larger than 1.  Reuse one\n            # instance of i.\n            i = keys[-1]\n            newcount = ms[i] = ms[i] - 1\n            reuse += i\n            if newcount == 0:\n                del keys[-1], ms[i]\n            room += 1\n\n            # Break the remainder into pieces of size i-1.\n            i -= 1\n            q, r = divmod(reuse, i)\n            need = q + bool(r)\n            if need > room:\n                if not keys:\n                    return\n                continue\n\n            ms[i] = q\n            keys.append(i)\n            if r:\n                ms[r] = 1\n                keys.append(r)\n            break\n        room -= need\n        if size:\n            yield sum(ms.values()), ms\n        else:\n            yield ms\n\n\ndef ordered_partitions(n, m=None, sort=True):\n    \"\"\"Generates ordered partitions of integer ``n``.\n\n    Parameters\n    ==========\n\n    m : integer (default None)\n        The default value gives partitions of all sizes else only\n        those with size m. In addition, if ``m`` is not None then\n        partitions are generated *in place* (see examples).\n    sort : bool (default True)\n(D)     {2: 1, 4: 1}\n    {3: 2}\n\n    Note that the _same_ dictionary object is returned each time.\n    This is for speed:  generating each partition goes quickly,\n    taking constant time, independent of n.\n\n    >>> [p for p in partitions(6, k=2)]\n    [{1: 6}, {1: 6}, {1: 6}, {1: 6}]\n\n    If you want to build a list of the returned dictionaries then\n    make a copy of them:\n\n    >>> [p.copy() for p in partitions(6, k=2)]  # doctest: +SKIP\n    [{2: 3}, {1: 2, 2: 2}, {1: 4, 2: 1}, {1: 6}]\n    >>> [(M, p.copy()) for M, p in partitions(6, k=2, size=True)]  # doctest: +SKIP\n    [(3, {2: 3}), (4, {1: 2, 2: 2}), (5, {1: 4, 2: 1}), (6, {1: 6})]\n\n    References\n    ==========\n\n    .. [1] modified from Tim Peter's version to allow for k and m values:\n           http://code.activestate.com/recipes/218332-generator-for-integer-partitions/\n\n    See Also\n    ========\n\n    sympy.combinatorics.partitions.Partition\n    sympy.combinatorics.partitions.IntegerPartition\n\n    \"\"\"\n    if (n <= 0 or\n        m is not None and m < 1 or\n        k is not None and k < 1 or\n        m and k and m*k < n):\n        # the empty set is the only way to handle these inputs\n        # and returning {} to represent it is consistent with\n        # the counting convention, e.g. nT(0) == 1.\n        if size:\n            yield 0, {}\n        else:\n            yield {}\n        return\n\n    if m is None:\n        m = n\n    else:\n        m = min(m, n)\n\n    if n == 0:\n        if size:\n            yield 1, {0: 1}\n        else:\n            yield {0: 1}\n        return\n\n    k = min(k or n, n)\n\n    n, m, k = as_int(n), as_int(m), as_int(k)\n    q, r = divmod(n, k)\n    ms = {k: q}\n    keys = [k]  # ms.keys(), from largest to smallest\n    if r:\n        ms[r] = 1\n        keys.append(r)\n    room = m - q - bool(r)\n    if size:\n        yield sum(ms.values()), ms\n    else:\n        yield ms\n\n    while keys != [1]:\n        # Reuse any 1's.\n        if keys[-1] == 1:\n            del keys[-1]\n            reuse = ms.pop(1)\n            room += reuse\n        else:\n            reuse = 0\n\n        while 1:\n            # Let i be the smallest key larger than 1.  Reuse one\n            # instance of i.\n            i = keys[-1]\n            newcount = ms[i] = ms[i] - 1\n            reuse += i\n            if newcount == 0:\n                del keys[-1], ms[i]\n            room += 1\n\n            # Break the remainder into pieces of size i-1.\n            i -= 1\n            q, r = divmod(reuse, i)\n            need = q + bool(r)\n            if need > room:\n                if not keys:\n                    return\n                continue", "answer": "D"}
{"uuid": "dd513310-5c7e-4a94-9857-75465e68a181", "issue_statement": "0**-oo produces 0, the documentation says it should produce zoo\nUsing SymPy 1.5.1, evaluate `0**-oo` produces `0`.\r\n\r\nThe documentation for the Pow class states that it should return `ComplexInfinity`, aka `zoo`\r\n\r\n| expr | value | reason |\r\n| :-- | :-- | :--|\r\n| `0**-oo` | `zoo` | This is not strictly true, as 0**oo may be oscillating between positive and negative values or rotating in the complex plane. It is convenient, however, when the base is positive.|", "choices": "(A)         if evaluate:\n            if e is S.ComplexInfinity:\n                return S.NaN\n            if e is S.Zero:\n                return S.One\n            elif e is S.One:\n                return b\n            elif e == -1 and not b:\n(B)                 issue=19445,\n                deprecated_since_version=\"1.7\"\n            ).warn()\n\n        if evaluate:\n            if e is S.ComplexInfinity:\n                return S.NaN\n            if e is S.Zero:\n(C)                 return S.NaN\n            if e is S.Zero:\n                return S.One\n            elif e is S.One:\n                return b\n            elif e == -1 and not b:\n                return S.ComplexInfinity\n            # Only perform autosimplification if exponent or base is a Symbol or number\n(D)             ).warn()\n\n        if evaluate:\n            if e is S.ComplexInfinity:\n                return S.NaN\n            if e is S.Zero:\n                return S.One\n            elif e is S.One:", "answer": "D"}
{"uuid": "9b195749-eace-4a4f-b5b8-2c6792a4bbfb", "issue_statement": "Inconsistent behavior for sympify/simplify with ceiling\nIn sympy v1.5.1:\r\n```python\r\nIn [16]: sympy.sympify('4*ceiling(x/4 - 3/4)', evaluate=False).simplify()\r\nOut[16]: 4*ceiling(x/4 - 3/4)\r\n\r\nIn [17]: sympy.sympify('4*ceiling(x/4 - 3/4)', evaluate=True).simplify()\r\nOut[17]: 4*ceiling(x/4 - 3/4)\r\n```\r\n\r\nIn sympy v.1.6.2:\r\n```python\r\nIn [16]: sympy.sympify('4*ceiling(x/4 - 3/4)', evaluate=False).simplify()\r\nOut[16]: 4*ceiling(x/4) - 3\r\n\r\nIn [17]: sympy.sympify('4*ceiling(x/4 - 3/4)', evaluate=True).simplify()\r\nOut [17]: 4*ceiling(x/4 - 3/4)\r\n```\r\n\r\nIs there a way to ensure that the behavior is consistent, even though evaluate is equal to `False` when parsing?", "choices": "(A) from .singleton import S\nfrom .operations import AssocOp, AssocOpDispatcher\nfrom .cache import cacheit\nfrom .logic import fuzzy_not, _fuzzy_group, fuzzy_and\nfrom .compatibility import reduce\nfrom .expr import Expr\nfrom .parameters import global_parameters\n\n\n\n# internal marker to indicate:\n#   \"there are still non-commutative objects -- don't forget to process them\"\nclass NC_Marker:\n    is_Order = False\n    is_Mul = False\n    is_Number = False\n    is_Poly = False\n\n    is_commutative = False\n\n\n# Key for sorting commutative args in canonical order\n_args_sortkey = cmp_to_key(Basic.compare)\ndef _mulsort(args):\n    # in-place sorting of args\n    args.sort(key=_args_sortkey)\n\n\ndef _unevaluated_Mul(*args):\n    \"\"\"Return a well-formed unevaluated Mul: Numbers are collected and\n    put in slot 0, any arguments that are Muls will be flattened, and args\n    are sorted. Use this when args have changed but you still want to return\n    an unevaluated Mul.\n\n    Examples\n    ========\n\n    >>> from sympy.core.mul import _unevaluated_Mul as uMul\n    >>> from sympy import S, sqrt, Mul\n    >>> from sympy.abc import x\n    >>> a = uMul(*[S(3.0), x, S(2)])\n    >>> a.args[0]\n    6.00000000000000\n    >>> a.args[1]\n    x\n\n    Two unevaluated Muls with the same arguments will\n    always compare as equal during testing:\n\n    >>> m = uMul(sqrt(2), sqrt(3))\n    >>> m == uMul(sqrt(3), sqrt(2))\n    True\n    >>> u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    >>> m == uMul(u)\n    True\n    >>> m == Mul(*m.args)\n    False\n\n    \"\"\"\n    args = list(args)\n    newargs = []\n    ncargs = []\n    co = S.One\n    while args:\n        a = args.pop()\n        if a.is_Mul:\n            c, nc = a.args_cnc()\n            args.extend(c)\n            if nc:\n                ncargs.append(Mul._from_args(nc))\n        elif a.is_Number:\n            co *= a\n        else:\n            newargs.append(a)\n    _mulsort(newargs)\n    if co is not S.One:\n        newargs.insert(0, co)\n    if ncargs:\n        newargs.append(Mul._from_args(ncargs))\n    return Mul._from_args(newargs)\n\n\nclass Mul(Expr, AssocOp):\n\n    __slots__ = ()\n\n    is_Mul = True\n\n    _args_type = Expr\n\n    def __neg__(self):\n        c, args = self.as_coeff_mul()\n        c = -c\n        if c is not S.One:\n            if args[0].is_Number:\n                args = list(args)\n                if c is S.NegativeOne:\n                    args[0] = -args[0]\n                else:\n                    args[0] *= c\n            else:\n                args = (c,) + args\n        return self._from_args(args, self.is_commutative)\n\n    @classmethod\n    def flatten(cls, seq):\n        \"\"\"Return commutative, noncommutative and order arguments by\n        combining related terms.\n\n        Notes\n        =====\n            * In an expression like ``a*b*c``, python process this through sympy\n              as ``Mul(Mul(a, b), c)``. This can have undesirable consequences.\n\n              -  Sometimes terms are not combined as one would like:\n                 {c.f. https://github.com/sympy/sympy/issues/4596}\n\n                >>> from sympy import Mul, sqrt\n                >>> from sympy.abc import x, y, z\n                >>> 2*(x + 1) # this is the 2-arg Mul behavior\n                2*x + 2\n                >>> y*(x + 1)*2\n                2*y*(x + 1)\n                >>> 2*(x + 1)*y # 2-arg result will be obtained first\n                y*(2*x + 2)\n                >>> Mul(2, x + 1, y) # all 3 args simultaneously processed\n                2*y*(x + 1)\n                >>> 2*((x + 1)*y) # parentheses can control this behavior\n                2*y*(x + 1)\n\n                Powers with compound bases may not find a single base to\n                combine with unless all arguments are processed at once.\n                Post-processing may be necessary in such cases.\n                {c.f. https://github.com/sympy/sympy/issues/5728}\n\n                >>> a = sqrt(x*sqrt(y))\n                >>> a**3\n                (x*sqrt(y))**(3/2)\n                >>> Mul(a,a,a)\n                (x*sqrt(y))**(3/2)\n                >>> a*a*a\n                x*sqrt(y)*sqrt(x*sqrt(y))\n                >>> _.subs(a.base, z).subs(z, a.base)\n                (x*sqrt(y))**(3/2)\n\n              -  If more than two terms are being multiplied then all the\n                 previous terms will be re-processed for each new argument.\n                 So if each of ``a``, ``b`` and ``c`` were :class:`Mul`\n                 expression, then ``a*b*c`` (or building up the product\n                 with ``*=``) will process all the arguments of ``a`` and\n                 ``b`` twice: once when ``a*b`` is computed and again when\n                 ``c`` is multiplied.\n\n                 Using ``Mul(a, b, c)`` will process all arguments once.\n\n            * The results of Mul are cached according to arguments, so flatten\n              will only be called once for ``Mul(a, b, c)``. If you can\n              structure a calculation so the arguments are most likely to be\n              repeats then this can save time in computing the answer. For\n              example, say you had a Mul, M, that you wished to divide by ``d[i]``\n              and multiply by ``n[i]`` and you suspect there are many repeats\n              in ``n``. It would be better to compute ``M*n[i]/d[i]`` rather\n              than ``M/d[i]*n[i]`` since every time n[i] is a repeat, the\n              product, ``M*n[i]`` will be returned without flattening -- the\n              cached value will be returned. If you divide by the ``d[i]``\n              first (and those are more unique than the ``n[i]``) then that will\n              create a new Mul, ``M/d[i]`` the args of which will be traversed\n              again when it is multiplied by ``n[i]``.\n\n              {c.f. https://github.com/sympy/sympy/issues/5706}\n\n              This consideration is moot if the cache is turned off.\n\n            NB\n            --\n              The validity of the above notes depends on the implementation\n              details of Mul and flatten which may change at any time. Therefore,\n              you should only consider them when your code is highly performance\n              sensitive.\n\n              Removal of 1 from the sequence is already handled by AssocOp.__new__.\n        \"\"\"\n\n        from sympy.calculus.util import AccumBounds\n        from sympy.matrices.expressions import MatrixExpr\n        rv = None\n        if len(seq) == 2:\n            a, b = seq\n            if b.is_Rational:\n                a, b = b, a\n                seq = [a, b]\n            assert not a is S.One\n            if not a.is_zero and a.is_Rational:\n                r, b = b.as_coeff_Mul()\n                if b.is_Add:\n                    if r is not S.One:  # 2-arg hack\n                        # leave the Mul as a Mul?\n                        ar = a*r\n                        if ar is S.One:\n                            arb = b\n                        else:\n                            arb = cls(a*r, b, evaluate=False)\n                        rv = [arb], [], None\n                    elif global_parameters.distribute and b.is_commutative:\n                        r, b = b.as_coeff_Add()\n                        bargs = [_keep_coeff(a, bi) for bi in Add.make_args(b)]\n                        _addsort(bargs)\n                        ar = a*r\n                        if ar:\n                            bargs.insert(0, ar)\n                        bargs = [Add._from_args(bargs)]\n                        rv = bargs, [], None\n            if rv:\n                return rv\n\n        # apply associativity, separate commutative part of seq\n        c_part = []         # out: commutative factors\n        nc_part = []        # out: non-commutative factors\n\n        nc_seq = []\n\n        coeff = S.One       # standalone term\n                            # e.g. 3 * ...\n\n        c_powers = []       # (base,exp)      n\n                            # e.g. (x,n) for x\n\n        num_exp = []        # (num-base, exp)           y\n                            # e.g.  (3, y)  for  ... * 3  * ...\n\n        neg1e = S.Zero      # exponent on -1 extracted from Number-based Pow and I\n\n        pnum_rat = {}       # (num-base, Rat-exp)          1/2\n                            # e.g.  (3, 1/2)  for  ... * 3     * ...\n\n        order_symbols = None\n\n        # --- PART 1 ---\n        #\n        # \"collect powers and coeff\":\n        #\n        # o coeff\n        # o c_powers\n        # o num_exp\n        # o neg1e\n        # o pnum_rat\n        #\n        # NOTE: this is optimized for all-objects-are-commutative case\n        for o in seq:\n            # O(x)\n            if o.is_Order:\n                o, order_symbols = o.as_expr_variables(order_symbols)\n\n            # Mul([...])\n            if o.is_Mul:\n                if o.is_commutative:\n                    seq.extend(o.args)    # XXX zerocopy?\n\n                else:\n                    # NCMul can have commutative parts as well\n                    for q in o.args:\n                        if q.is_commutative:\n                            seq.append(q)\n                        else:\n                            nc_seq.append(q)\n\n                    # append non-commutative marker, so we don't forget to\n                    # process scheduled non-commutative objects\n                    seq.append(NC_Marker)\n\n                continue\n\n            # 3\n            elif o.is_Number:\n                if o is S.NaN or coeff is S.ComplexInfinity and o.is_zero:\n                    # we know for sure the result will be nan\n                    return [S.NaN], [], None\n                elif coeff.is_Number or isinstance(coeff, AccumBounds):  # it could be zoo\n                    coeff *= o\n                    if coeff is S.NaN:\n                        # we know for sure the result will be nan\n                        return [S.NaN], [], None\n                continue\n\n            elif isinstance(o, AccumBounds):\n                coeff = o.__mul__(coeff)\n                continue\n\n            elif o is S.ComplexInfinity:\n                if not coeff:\n                    # 0 * zoo = NaN\n                    return [S.NaN], [], None\n                coeff = S.ComplexInfinity\n                continue\n\n            elif o is S.ImaginaryUnit:\n                neg1e += S.Half\n                continue\n\n            elif o.is_commutative:\n                #      e\n                # o = b\n                b, e = o.as_base_exp()\n\n                #  y\n                # 3\n                if o.is_Pow:\n                    if b.is_Number:\n\n                        # get all the factors with numeric base so they can be\n                        # combined below, but don't combine negatives unless\n                        # the exponent is an integer\n                        if e.is_Rational:\n                            if e.is_Integer:\n                                coeff *= Pow(b, e)  # it is an unevaluated power\n                                continue\n                            elif e.is_negative:    # also a sign of an unevaluated power\n                                seq.append(Pow(b, e))\n                                continue\n                            elif b.is_negative:\n                                neg1e += e\n                                b = -b\n                            if b is not S.One:\n                                pnum_rat.setdefault(b, []).append(e)\n                            continue\n                        elif b.is_positive or e.is_integer:\n                            num_exp.append((b, e))\n                            continue\n\n                c_powers.append((b, e))\n\n            # NON-COMMUTATIVE\n            # TODO: Make non-commutative exponents not combine automatically\n            else:\n                if o is not NC_Marker:\n                    nc_seq.append(o)\n\n                # process nc_seq (if any)\n                while nc_seq:\n                    o = nc_seq.pop(0)\n                    if not nc_part:\n                        nc_part.append(o)\n                        continue\n\n                    #                             b    c       b+c\n                    # try to combine last terms: a  * a   ->  a\n                    o1 = nc_part.pop()\n                    b1, e1 = o1.as_base_exp()\n                    b2, e2 = o.as_base_exp()\n                    new_exp = e1 + e2\n                    # Only allow powers to combine if the new exponent is\n                    # not an Add. This allow things like a**2*b**3 == a**5\n                    # if a.is_commutative == False, but prohibits\n                    # a**x*a**y and x**a*x**b from combining (x,y commute).\n                    if b1 == b2 and (not new_exp.is_Add):\n                        o12 = b1 ** new_exp\n\n                        # now o12 could be a commutative object\n                        if o12.is_commutative:\n                            seq.append(o12)\n                            continue\n                        else:\n                            nc_seq.insert(0, o12)\n\n                    else:\n                        nc_part.append(o1)\n                        nc_part.append(o)\n\n        # We do want a combined exponent if it would not be an Add, such as\n        #  y    2y     3y\n        # x  * x   -> x\n        # We determine if two exponents have the same term by using\n        # as_coeff_Mul.\n        #\n        # Unfortunately, this isn't smart enough to consider combining into\n        # exponents that might already be adds, so things like:\n        #  z - y    y\n        # x      * x  will be left alone.  This is because checking every possible\n        # combination can slow things down.\n\n        # gather exponents of common bases...\n        def _gather(c_powers):\n            common_b = {}  # b:e\n            for b, e in c_powers:\n                co = e.as_coeff_Mul()\n                common_b.setdefault(b, {}).setdefault(\n                    co[1], []).append(co[0])\n            for b, d in common_b.items():\n                for di, li in d.items():\n                    d[di] = Add(*li)\n            new_c_powers = []\n            for b, e in common_b.items():\n                new_c_powers.extend([(b, c*t) for t, c in e.items()])\n            return new_c_powers\n\n        # in c_powers\n        c_powers = _gather(c_powers)\n\n        # and in num_exp\n        num_exp = _gather(num_exp)\n\n        # --- PART 2 ---\n        #\n        # o process collected powers  (x**0 -> 1; x**1 -> x; otherwise Pow)\n        # o combine collected powers  (2**x * 3**x -> 6**x)\n        #   with numeric base\n\n        # ................................\n        # now we have:\n        # - coeff:\n        # - c_powers:    (b, e)\n        # - num_exp:     (2, e)\n        # - pnum_rat:    {(1/3, [1/3, 2/3, 1/4])}\n\n        #  0             1\n        # x  -> 1       x  -> x\n\n        # this should only need to run twice; if it fails because\n        # it needs to be run more times, perhaps this should be\n        # changed to a \"while True\" loop -- the only reason it\n        # isn't such now is to allow a less-than-perfect result to\n        # be obtained rather than raising an error or entering an\n        # infinite loop\n        for i in range(2):\n            new_c_powers = []\n            changed = False\n            for b, e in c_powers:\n                if e.is_zero:\n                    # canceling out infinities yields NaN\n                    if (b.is_Add or b.is_Mul) and any(infty in b.args\n                        for infty in (S.ComplexInfinity, S.Infinity,\n                                      S.NegativeInfinity)):\n                        return [S.NaN], [], None\n                    continue\n                if e is S.One:\n                    if b.is_Number:\n                        coeff *= b\n                        continue\n                    p = b\n                if e is not S.One:\n                    p = Pow(b, e)\n                    # check to make sure that the base doesn't change\n                    # after exponentiation; to allow for unevaluated\n                    # Pow, we only do so if b is not already a Pow\n                    if p.is_Pow and not b.is_Pow:\n                        bi = b\n                        b, e = p.as_base_exp()\n                        if b != bi:\n                            changed = True\n                c_part.append(p)\n                new_c_powers.append((b, e))\n            # there might have been a change, but unless the base\n            # matches some other base, there is nothing to do\n            if changed and len({\n                    b for b, e in new_c_powers}) != len(new_c_powers):\n                # start over again\n                c_part = []\n                c_powers = _gather(new_c_powers)\n            else:\n                break\n\n        #  x    x     x\n        # 2  * 3  -> 6\n        inv_exp_dict = {}   # exp:Mul(num-bases)     x    x\n                            # e.g.  x:6  for  ... * 2  * 3  * ...\n        for b, e in num_exp:\n            inv_exp_dict.setdefault(e, []).append(b)\n        for e, b in inv_exp_dict.items():\n            inv_exp_dict[e] = cls(*b)\n        c_part.extend([Pow(b, e) for e, b in inv_exp_dict.items() if e])\n\n        # b, e -> e' = sum(e), b\n        # {(1/5, [1/3]), (1/2, [1/12, 1/4]} -> {(1/3, [1/5, 1/2])}\n        comb_e = {}\n        for b, e in pnum_rat.items():\n            comb_e.setdefault(Add(*e), []).append(b)\n        del pnum_rat\n        # process them, reducing exponents to values less than 1\n        # and updating coeff if necessary else adding them to\n        # num_rat for further processing\n        num_rat = []\n        for e, b in comb_e.items():\n            b = cls(*b)\n            if e.q == 1:\n                coeff *= Pow(b, e)\n                continue\n            if e.p > e.q:\n                e_i, ep = divmod(e.p, e.q)\n                coeff *= Pow(b, e_i)\n                e = Rational(ep, e.q)\n            num_rat.append((b, e))\n        del comb_e\n\n        # extract gcd of bases in num_rat\n        # 2**(1/3)*6**(1/4) -> 2**(1/3+1/4)*3**(1/4)\n        pnew = defaultdict(list)\n        i = 0  # steps through num_rat which may grow\n        while i < len(num_rat):\n            bi, ei = num_rat[i]\n            grow = []\n            for j in range(i + 1, len(num_rat)):\n                bj, ej = num_rat[j]\n                g = bi.gcd(bj)\n                if g is not S.One:\n                    # 4**r1*6**r2 -> 2**(r1+r2)  *  2**r1 *  3**r2\n                    # this might have a gcd with something else\n                    e = ei + ej\n                    if e.q == 1:\n                        coeff *= Pow(g, e)\n                    else:\n                        if e.p > e.q:\n                            e_i, ep = divmod(e.p, e.q)  # change e in place\n                            coeff *= Pow(g, e_i)\n                            e = Rational(ep, e.q)\n                        grow.append((g, e))\n                    # update the jth item\n                    num_rat[j] = (bj/g, ej)\n                    # update bi that we are checking with\n                    bi = bi/g\n                    if bi is S.One:\n                        break\n            if bi is not S.One:\n                obj = Pow(bi, ei)\n                if obj.is_Number:\n                    coeff *= obj\n                else:\n                    # changes like sqrt(12) -> 2*sqrt(3)\n                    for obj in Mul.make_args(obj):\n                        if obj.is_Number:\n                            coeff *= obj\n                        else:\n                            assert obj.is_Pow\n                            bi, ei = obj.args\n                            pnew[ei].append(bi)\n\n            num_rat.extend(grow)\n            i += 1\n\n        # combine bases of the new powers\n        for e, b in pnew.items():\n            pnew[e] = cls(*b)\n\n        # handle -1 and I\n        if neg1e:\n            # treat I as (-1)**(1/2) and compute -1's total exponent\n            p, q =  neg1e.as_numer_denom()\n            # if the integer part is odd, extract -1\n            n, p = divmod(p, q)\n            if n % 2:\n                coeff = -coeff\n            # if it's a multiple of 1/2 extract I\n            if q == 2:\n                c_part.append(S.ImaginaryUnit)\n            elif p:\n                # see if there is any positive base this power of\n                # -1 can join\n                neg1e = Rational(p, q)\n                for e, b in pnew.items():\n                    if e == neg1e and b.is_positive:\n                        pnew[e] = -b\n                        break\n                else:\n                    # keep it separate; we've already evaluated it as\n                    # much as possible so evaluate=False\n                    c_part.append(Pow(S.NegativeOne, neg1e, evaluate=False))\n\n        # add all the pnew powers\n        c_part.extend([Pow(b, e) for e, b in pnew.items()])\n\n        # oo, -oo\n        if (coeff is S.Infinity) or (coeff is S.NegativeInfinity):\n            def _handle_for_oo(c_part, coeff_sign):\n                new_c_part = []\n                for t in c_part:\n                    if t.is_extended_positive:\n                        continue\n                    if t.is_extended_negative:\n                        coeff_sign *= -1\n                        continue\n                    new_c_part.append(t)\n                return new_c_part, coeff_sign\n            c_part, coeff_sign = _handle_for_oo(c_part, 1)\n            nc_part, coeff_sign = _handle_for_oo(nc_part, coeff_sign)\n            coeff *= coeff_sign\n\n        # zoo\n        if coeff is S.ComplexInfinity:\n            # zoo might be\n            #   infinite_real + bounded_im\n            #   bounded_real + infinite_im\n            #   infinite_real + infinite_im\n            # and non-zero real or imaginary will not change that status.\n            c_part = [c for c in c_part if not (fuzzy_not(c.is_zero) and\n                                                c.is_extended_real is not None)]\n            nc_part = [c for c in nc_part if not (fuzzy_not(c.is_zero) and\n                                                  c.is_extended_real is not None)]\n\n        # 0\n        elif coeff.is_zero:\n            # we know for sure the result will be 0 except the multiplicand\n            # is infinity or a matrix\n            if any(isinstance(c, MatrixExpr) for c in nc_part):\n                return [coeff], nc_part, order_symbols\n            if any(c.is_finite == False for c in c_part):\n                return [S.NaN], [], order_symbols\n            return [coeff], [], order_symbols\n\n        # check for straggling Numbers that were produced\n        _new = []\n        for i in c_part:\n            if i.is_Number:\n                coeff *= i\n            else:\n                _new.append(i)\n        c_part = _new\n\n        # order commutative part canonically\n        _mulsort(c_part)\n\n        # current code expects coeff to be always in slot-0\n        if coeff is not S.One:\n            c_part.insert(0, coeff)\n\n        # we are done\n        if (global_parameters.distribute and not nc_part and len(c_part) == 2 and\n                c_part[0].is_Number and c_part[0].is_finite and c_part[1].is_Add):\n            # 2*(1+a) -> 2 + 2 * a\n            coeff = c_part[0]\n            c_part = [Add(*[coeff*f for f in c_part[1].args])]\n\n        return c_part, nc_part, order_symbols\n\n    def _eval_power(self, e):\n\n        # don't break up NC terms: (A*B)**3 != A**3*B**3, it is A*B*A*B*A*B\n        cargs, nc = self.args_cnc(split_1=False)\n\n        if e.is_Integer:\n            return Mul(*[Pow(b, e, evaluate=False) for b in cargs]) * \\\n                Pow(Mul._from_args(nc), e, evaluate=False)\n        if e.is_Rational and e.q == 2:\n            from sympy.core.power import integer_nthroot\n            from sympy.functions.elementary.complexes import sign\n            if self.is_imaginary:\n                a = self.as_real_imag()[1]\n                if a.is_Rational:\n                    n, d = abs(a/2).as_numer_denom()\n                    n, t = integer_nthroot(n, 2)\n                    if t:\n                        d, t = integer_nthroot(d, 2)\n                        if t:\n                            r = sympify(n)/d\n                            return _unevaluated_Mul(r**e.p, (1 + sign(a)*S.ImaginaryUnit)**e.p)\n\n        p = Pow(self, e, evaluate=False)\n\n        if e.is_Rational or e.is_Float:\n            return p._eval_expand_power_base()\n\n        return p\n\n    @classmethod\n    def class_key(cls):\n        return 3, 0, cls.__name__\n\n    def _eval_evalf(self, prec):\n        c, m = self.as_coeff_Mul()\n        if c is S.NegativeOne:\n            if m.is_Mul:\n                rv = -AssocOp._eval_evalf(m, prec)\n            else:\n                mnew = m._eval_evalf(prec)\n                if mnew is not None:\n                    m = mnew\n                rv = -m\n        else:\n            rv = AssocOp._eval_evalf(self, prec)\n        if rv.is_number:\n            return rv.expand()\n        return rv\n\n    @property\n    def _mpc_(self):\n        \"\"\"\n        Convert self to an mpmath mpc if possible\n        \"\"\"\n        from sympy.core.numbers import I, Float\n        im_part, imag_unit = self.as_coeff_Mul()\n        if not imag_unit == I:\n            # ValueError may seem more reasonable but since it's a @property,\n            # we need to use AttributeError to keep from confusing things like\n            # hasattr.\n            raise AttributeError(\"Cannot convert Mul to mpc. Must be of the form Number*I\")\n\n        return (Float(0)._mpf_, Float(im_part)._mpf_)\n\n    @cacheit\n    def as_two_terms(self):\n        \"\"\"Return head and tail of self.\n\n        This is the most efficient way to get the head and tail of an\n        expression.\n\n        - if you want only the head, use self.args[0];\n        - if you want to process the arguments of the tail then use\n          self.as_coef_mul() which gives the head and a tuple containing\n          the arguments of the tail when treated as a Mul.\n        - if you want the coefficient when self is treated as an Add\n          then use self.as_coeff_add()[0]\n\n        Examples\n        ========\n\n        >>> from sympy.abc import x, y\n        >>> (3*x*y).as_two_terms()\n        (3, x*y)\n        \"\"\"\n        args = self.args\n\n        if len(args) == 1:\n            return S.One, self\n        elif len(args) == 2:\n            return args\n\n        else:\n            return args[0], self._new_rawargs(*args[1:])\n\n    @cacheit\n    def as_coefficients_dict(self):\n        \"\"\"Return a dictionary mapping terms to their coefficient.\n        Since the dictionary is a defaultdict, inquiries about terms which\n        were not present will return a coefficient of 0. The dictionary\n        is considered to have a single term.\n\n        Examples\n        ========\n\n        >>> from sympy.abc import a, x\n        >>> (3*a*x).as_coefficients_dict()\n        {a*x: 3}\n        >>> _[a]\n        0\n        \"\"\"\n\n        d = defaultdict(int)\n        args = self.args\n\n        if len(args) == 1 or not args[0].is_Number:\n            d[self] = S.One\n        else:\n            d[self._new_rawargs(*args[1:])] = args[0]\n\n        return d\n\n    @cacheit\n    def as_coeff_mul(self, *deps, rational=True, **kwargs):\n        if deps:\n            from sympy.utilities.iterables import sift\n            l1, l2 = sift(self.args, lambda x: x.has(*deps), binary=True)\n            return self._new_rawargs(*l2), tuple(l1)\n        args = self.args\n        if args[0].is_Number:\n            if not rational or args[0].is_Rational:\n                return args[0], args[1:]\n            elif args[0].is_extended_negative:\n                return S.NegativeOne, (-args[0],) + args[1:]\n        return S.One, args\n\n    def as_coeff_Mul(self, rational=False):\n        \"\"\"\n        Efficiently extract the coefficient of a product.\n        \"\"\"\n        coeff, args = self.args[0], self.args[1:]\n\n        if coeff.is_Number:\n            if not rational or coeff.is_Rational:\n                if len(args) == 1:\n                    return coeff, args[0]\n                else:\n                    return coeff, self._new_rawargs(*args)\n            elif coeff.is_extended_negative:\n                return S.NegativeOne, self._new_rawargs(*((-coeff,) + args))\n        return S.One, self\n\n    def as_real_imag(self, deep=True, **hints):\n        from sympy import Abs, expand_mul, im, re\n        other = []\n        coeffr = []\n        coeffi = []\n        addterms = S.One\n        for a in self.args:\n            r, i = a.as_real_imag()\n            if i.is_zero:\n                coeffr.append(r)\n            elif r.is_zero:\n                coeffi.append(i*S.ImaginaryUnit)\n            elif a.is_commutative:\n                # search for complex conjugate pairs:\n                for i, x in enumerate(other):\n                    if x == a.conjugate():\n                        coeffr.append(Abs(x)**2)\n                        del other[i]\n                        break\n                else:\n                    if a.is_Add:\n                        addterms *= a\n                    else:\n                        other.append(a)\n            else:\n                other.append(a)\n        m = self.func(*other)\n        if hints.get('ignore') == m:\n            return\n        if len(coeffi) % 2:\n            imco = im(coeffi.pop(0))\n            # all other pairs make a real factor; they will be\n            # put into reco below\n        else:\n            imco = S.Zero\n        reco = self.func(*(coeffr + coeffi))\n        r, i = (reco*re(m), reco*im(m))\n        if addterms == 1:\n            if m == 1:\n                if imco.is_zero:\n                    return (reco, S.Zero)\n                else:\n                    return (S.Zero, reco*imco)\n            if imco is S.Zero:\n                return (r, i)\n            return (-imco*i, imco*r)\n        addre, addim = expand_mul(addterms, deep=False).as_real_imag()\n        if imco is S.Zero:\n            return (r*addre - i*addim, i*addre + r*addim)\n        else:\n            r, i = -imco*i, imco*r\n            return (r*addre - i*addim, r*addim + i*addre)\n\n    @staticmethod\n    def _expandsums(sums):\n        \"\"\"\n        Helper function for _eval_expand_mul.\n\n        sums must be a list of instances of Basic.\n        \"\"\"\n\n        L = len(sums)\n        if L == 1:\n            return sums[0].args\n        terms = []\n        left = Mul._expandsums(sums[:L//2])\n        right = Mul._expandsums(sums[L//2:])\n\n        terms = [Mul(a, b) for a in left for b in right]\n        added = Add(*terms)\n        return Add.make_args(added)  # it may have collapsed down to one term\n\n    def _eval_expand_mul(self, **hints):\n        from sympy import fraction\n\n        # Handle things like 1/(x*(x + 1)), which are automatically converted\n        # to 1/x*1/(x + 1)\n        expr = self\n        n, d = fraction(expr)\n        if d.is_Mul:\n            n, d = [i._eval_expand_mul(**hints) if i.is_Mul else i\n                for i in (n, d)]\n            expr = n/d\n            if not expr.is_Mul:\n                return expr\n\n        plain, sums, rewrite = [], [], False\n        for factor in expr.args:\n            if factor.is_Add:\n                sums.append(factor)\n                rewrite = True\n            else:\n                if factor.is_commutative:\n                    plain.append(factor)\n                else:\n                    sums.append(Basic(factor))  # Wrapper\n\n        if not rewrite:\n            return expr\n        else:\n            plain = self.func(*plain)\n            if sums:\n                deep = hints.get(\"deep\", False)\n                terms = self.func._expandsums(sums)\n                args = []\n                for term in terms:\n                    t = self.func(plain, term)\n                    if t.is_Mul and any(a.is_Add for a in t.args) and deep:\n                        t = t._eval_expand_mul()\n                    args.append(t)\n                return Add(*args)\n            else:\n                return plain\n\n    @cacheit\n    def _eval_derivative(self, s):\n        args = list(self.args)\n        terms = []\n        for i in range(len(args)):\n            d = args[i].diff(s)\n            if d:\n                # Note: reduce is used in step of Mul as Mul is unable to\n                # handle subtypes and operation priority:\n                terms.append(reduce(lambda x, y: x*y, (args[:i] + [d] + args[i + 1:]), S.One))\n        return Add.fromiter(terms)\n\n    @cacheit\n    def _eval_derivative_n_times(self, s, n):\n        from sympy import Integer, factorial, prod, Sum, Max\n        from sympy.ntheory.multinomial import multinomial_coefficients_iterator\n        from .function import AppliedUndef\n        from .symbol import Symbol, symbols, Dummy\n        if not isinstance(s, AppliedUndef) and not isinstance(s, Symbol):\n            # other types of s may not be well behaved, e.g.\n            # (cos(x)*sin(y)).diff([[x, y, z]])\n            return super()._eval_derivative_n_times(s, n)\n        args = self.args\n        m = len(args)\n        if isinstance(n, (int, Integer)):\n            # https://en.wikipedia.org/wiki/General_Leibniz_rule#More_than_two_factors\n            terms = []\n            for kvals, c in multinomial_coefficients_iterator(m, n):\n                p = prod([arg.diff((s, k)) for k, arg in zip(kvals, args)])\n                terms.append(c * p)\n            return Add(*terms)\n        kvals = symbols(\"k1:%i\" % m, cls=Dummy)\n        klast = n - sum(kvals)\n        nfact = factorial(n)\n        e, l = (# better to use the multinomial?\n            nfact/prod(map(factorial, kvals))/factorial(klast)*\\\n            prod([args[t].diff((s, kvals[t])) for t in range(m-1)])*\\\n            args[-1].diff((s, Max(0, klast))),\n            [(k, 0, n) for k in kvals])\n        return Sum(e, *l)\n\n    def _eval_difference_delta(self, n, step):\n        from sympy.series.limitseq import difference_delta as dd\n        arg0 = self.args[0]\n        rest = Mul(*self.args[1:])\n        return (arg0.subs(n, n + step) * dd(rest, n, step) + dd(arg0, n, step) *\n                rest)\n\n    def _matches_simple(self, expr, repl_dict):\n        # handle (w*3).matches('x*5') -> {w: x*5/3}\n        coeff, terms = self.as_coeff_Mul()\n        terms = Mul.make_args(terms)\n        if len(terms) == 1:\n            newexpr = self.__class__._combine_inverse(expr, coeff)\n            return terms[0].matches(newexpr, repl_dict)\n        return\n\n    def matches(self, expr, repl_dict={}, old=False):\n        expr = sympify(expr)\n        repl_dict = repl_dict.copy()\n        if self.is_commutative and expr.is_commutative:\n            return self._matches_commutative(expr, repl_dict, old)\n        elif self.is_commutative is not expr.is_commutative:\n            return None\n\n        # Proceed only if both both expressions are non-commutative\n        c1, nc1 = self.args_cnc()\n        c2, nc2 = expr.args_cnc()\n        c1, c2 = [c or [1] for c in [c1, c2]]\n\n        # TODO: Should these be self.func?\n        comm_mul_self = Mul(*c1)\n        comm_mul_expr = Mul(*c2)\n\n        repl_dict = comm_mul_self.matches(comm_mul_expr, repl_dict, old)\n\n        # If the commutative arguments didn't match and aren't equal, then\n        # then the expression as a whole doesn't match\n        if repl_dict is None and c1 != c2:\n            return None\n\n        # Now match the non-commutative arguments, expanding powers to\n        # multiplications\n        nc1 = Mul._matches_expand_pows(nc1)\n        nc2 = Mul._matches_expand_pows(nc2)\n\n        repl_dict = Mul._matches_noncomm(nc1, nc2, repl_dict)\n\n        return repl_dict or None\n\n    @staticmethod\n    def _matches_expand_pows(arg_list):\n        new_args = []\n        for arg in arg_list:\n            if arg.is_Pow and arg.exp > 0:\n                new_args.extend([arg.base] * arg.exp)\n            else:\n                new_args.append(arg)\n        return new_args\n\n    @staticmethod\n    def _matches_noncomm(nodes, targets, repl_dict={}):\n        \"\"\"Non-commutative multiplication matcher.\n\n        `nodes` is a list of symbols within the matcher multiplication\n        expression, while `targets` is a list of arguments in the\n        multiplication expression being matched against.\n        \"\"\"\n        repl_dict = repl_dict.copy()\n        # List of possible future states to be considered\n        agenda = []\n        # The current matching state, storing index in nodes and targets\n        state = (0, 0)\n        node_ind, target_ind = state\n        # Mapping between wildcard indices and the index ranges they match\n        wildcard_dict = {}\n        repl_dict = repl_dict.copy()\n\n        while target_ind < len(targets) and node_ind < len(nodes):\n            node = nodes[node_ind]\n\n            if node.is_Wild:\n                Mul._matches_add_wildcard(wildcard_dict, state)\n\n            states_matches = Mul._matches_new_states(wildcard_dict, state,\n                                                     nodes, targets)\n            if states_matches:\n                new_states, new_matches = states_matches\n                agenda.extend(new_states)\n                if new_matches:\n                    for match in new_matches:\n                        repl_dict[match] = new_matches[match]\n            if not agenda:\n                return None\n            else:\n                state = agenda.pop()\n                node_ind, target_ind = state\n\n        return repl_dict\n\n    @staticmethod\n    def _matches_add_wildcard(dictionary, state):\n        node_ind, target_ind = state\n        if node_ind in dictionary:\n            begin, end = dictionary[node_ind]\n            dictionary[node_ind] = (begin, target_ind)\n        else:\n            dictionary[node_ind] = (target_ind, target_ind)\n\n    @staticmethod\n    def _matches_new_states(dictionary, state, nodes, targets):\n        node_ind, target_ind = state\n        node = nodes[node_ind]\n        target = targets[target_ind]\n\n        # Don't advance at all if we've exhausted the targets but not the nodes\n        if target_ind >= len(targets) - 1 and node_ind < len(nodes) - 1:\n            return None\n\n        if node.is_Wild:\n            match_attempt = Mul._matches_match_wilds(dictionary, node_ind,\n                                                     nodes, targets)\n            if match_attempt:\n                # If the same node has been matched before, don't return\n                # anything if the current match is diverging from the previous\n                # match\n                other_node_inds = Mul._matches_get_other_nodes(dictionary,\n                                                               nodes, node_ind)\n                for ind in other_node_inds:\n                    other_begin, other_end = dictionary[ind]\n                    curr_begin, curr_end = dictionary[node_ind]\n\n                    other_targets = targets[other_begin:other_end + 1]\n                    current_targets = targets[curr_begin:curr_end + 1]\n\n                    for curr, other in zip(current_targets, other_targets):\n                        if curr != other:\n                            return None\n\n                # A wildcard node can match more than one target, so only the\n                # target index is advanced\n                new_state = [(node_ind, target_ind + 1)]\n                # Only move on to the next node if there is one\n                if node_ind < len(nodes) - 1:\n                    new_state.append((node_ind + 1, target_ind + 1))\n                return new_state, match_attempt\n        else:\n            # If we're not at a wildcard, then make sure we haven't exhausted\n            # nodes but not targets, since in this case one node can only match\n            # one target\n            if node_ind >= len(nodes) - 1 and target_ind < len(targets) - 1:\n                return None\n\n            match_attempt = node.matches(target)\n\n            if match_attempt:\n                return [(node_ind + 1, target_ind + 1)], match_attempt\n            elif node == target:\n                return [(node_ind + 1, target_ind + 1)], None\n            else:\n                return None\n\n    @staticmethod\n    def _matches_match_wilds(dictionary, wildcard_ind, nodes, targets):\n        \"\"\"Determine matches of a wildcard with sub-expression in `target`.\"\"\"\n        wildcard = nodes[wildcard_ind]\n        begin, end = dictionary[wildcard_ind]\n        terms = targets[begin:end + 1]\n        # TODO: Should this be self.func?\n        mul = Mul(*terms) if len(terms) > 1 else terms[0]\n        return wildcard.matches(mul)\n\n    @staticmethod\n    def _matches_get_other_nodes(dictionary, nodes, node_ind):\n        \"\"\"Find other wildcards that may have already been matched.\"\"\"\n        other_node_inds = []\n        for ind in dictionary:\n            if nodes[ind] == nodes[node_ind]:\n                other_node_inds.append(ind)\n        return other_node_inds\n\n    @staticmethod\n    def _combine_inverse(lhs, rhs):\n        \"\"\"\n        Returns lhs/rhs, but treats arguments like symbols, so things\n        like oo/oo return 1 (instead of a nan) and ``I`` behaves like\n        a symbol instead of sqrt(-1).\n        \"\"\"\n        from .symbol import Dummy\n        if lhs == rhs:\n            return S.One\n\n        def check(l, r):\n            if l.is_Float and r.is_comparable:\n                # if both objects are added to 0 they will share the same \"normalization\"\n                # and are more likely to compare the same. Since Add(foo, 0) will not allow\n                # the 0 to pass, we use __add__ directly.\n                return l.__add__(0) == r.evalf().__add__(0)\n            return False\n        if check(lhs, rhs) or check(rhs, lhs):\n            return S.One\n        if any(i.is_Pow or i.is_Mul for i in (lhs, rhs)):\n            # gruntz and limit wants a literal I to not combine\n            # with a power of -1\n            d = Dummy('I')\n            _i = {S.ImaginaryUnit: d}\n            i_ = {d: S.ImaginaryUnit}\n            a = lhs.xreplace(_i).as_powers_dict()\n            b = rhs.xreplace(_i).as_powers_dict()\n            blen = len(b)\n            for bi in tuple(b.keys()):\n                if bi in a:\n                    a[bi] -= b.pop(bi)\n                    if not a[bi]:\n                        a.pop(bi)\n            if len(b) != blen:\n                lhs = Mul(*[k**v for k, v in a.items()]).xreplace(i_)\n                rhs = Mul(*[k**v for k, v in b.items()]).xreplace(i_)\n        return lhs/rhs\n\n    def as_powers_dict(self):\n        d = defaultdict(int)\n        for term in self.args:\n            for b, e in term.as_powers_dict().items():\n                d[b] += e\n        return d\n\n    def as_numer_denom(self):\n        # don't use _from_args to rebuild the numerators and denominators\n        # as the order is not guaranteed to be the same once they have\n        # been separated from each other\n        numers, denoms = list(zip(*[f.as_numer_denom() for f in self.args]))\n        return self.func(*numers), self.func(*denoms)\n\n    def as_base_exp(self):\n        e1 = None\n        bases = []\n        nc = 0\n        for m in self.args:\n            b, e = m.as_base_exp()\n            if not b.is_commutative:\n                nc += 1\n            if e1 is None:\n                e1 = e\n            elif e != e1 or nc > 1:\n                return self, S.One\n            bases.append(b)\n        return self.func(*bases), e1\n\n    def _eval_is_polynomial(self, syms):\n        return all(term._eval_is_polynomial(syms) for term in self.args)\n\n    def _eval_is_rational_function(self, syms):\n        return all(term._eval_is_rational_function(syms) for term in self.args)\n\n    def _eval_is_meromorphic(self, x, a):\n        return _fuzzy_group((arg.is_meromorphic(x, a) for arg in self.args),\n                            quick_exit=True)\n\n    def _eval_is_algebraic_expr(self, syms):\n        return all(term._eval_is_algebraic_expr(syms) for term in self.args)\n\n    _eval_is_commutative = lambda self: _fuzzy_group(\n        a.is_commutative for a in self.args)\n\n    def _eval_is_complex(self):\n        comp = _fuzzy_group(a.is_complex for a in self.args)\n        if comp is False:\n            if any(a.is_infinite for a in self.args):\n                if any(a.is_zero is not False for a in self.args):\n                    return None\n                return False\n        return comp\n\n    def _eval_is_finite(self):\n        if all(a.is_finite for a in self.args):\n            return True\n        if any(a.is_infinite for a in self.args):\n            if all(a.is_zero is False for a in self.args):\n                return False\n\n    def _eval_is_infinite(self):\n        if any(a.is_infinite for a in self.args):\n            if any(a.is_zero for a in self.args):\n                return S.NaN.is_infinite\n            if any(a.is_zero is None for a in self.args):\n                return None\n            return True\n\n    def _eval_is_rational(self):\n        r = _fuzzy_group((a.is_rational for a in self.args), quick_exit=True)\n        if r:\n            return r\n        elif r is False:\n            return self.is_zero\n\n    def _eval_is_algebraic(self):\n        r = _fuzzy_group((a.is_algebraic for a in self.args), quick_exit=True)\n        if r:\n            return r\n        elif r is False:\n            return self.is_zero\n\n    def _eval_is_zero(self):\n        zero = infinite = False\n        for a in self.args:\n            z = a.is_zero\n            if z:\n                if infinite:\n                    return  # 0*oo is nan and nan.is_zero is None\n                zero = True\n            else:\n                if not a.is_finite:\n                    if zero:\n                        return  # 0*oo is nan and nan.is_zero is None\n                    infinite = True\n                if zero is False and z is None:  # trap None\n                    zero = None\n        return zero\n\n    def _eval_is_integer(self):\n        from sympy import fraction\n        from sympy.core.numbers import Float\n\n        is_rational = self._eval_is_rational()\n        if is_rational is False:\n            return False\n\n        # use exact=True to avoid recomputing num or den\n        n, d = fraction(self, exact=True)\n        if is_rational:\n            if d is S.One:\n                return True\n        if d.is_even:\n            if d.is_prime:  # literal or symbolic 2\n                return n.is_even\n            if n.is_odd:\n                return False  # true even if d = 0\n        if n == d:\n            return fuzzy_and([not bool(self.atoms(Float)),\n            fuzzy_not(d.is_zero)])\n\n    def _eval_is_polar(self):\n        has_polar = any(arg.is_polar for arg in self.args)\n        return has_polar and \\\n            all(arg.is_polar or arg.is_positive for arg in self.args)\n\n    def _eval_is_extended_real(self):\n        return self._eval_real_imag(True)\n\n    def _eval_real_imag(self, real):\n        zero = False\n        t_not_re_im = None\n\n        for t in self.args:\n            if (t.is_complex or t.is_infinite) is False and t.is_extended_real is False:\n                return False\n            elif t.is_imaginary:  # I\n                real = not real\n            elif t.is_extended_real:  # 2\n                if not zero:\n                    z = t.is_zero\n                    if not z and zero is False:\n                        zero = z\n(B)                             r = sympify(n)/d\n                            return _unevaluated_Mul(r**e.p, (1 + sign(a)*S.ImaginaryUnit)**e.p)\n\n        p = Pow(self, e, evaluate=False)\n\n        if e.is_Rational or e.is_Float:\n            return p._eval_expand_power_base()\n\n        return p\n\n    @classmethod\n    def class_key(cls):\n        return 3, 0, cls.__name__\n\n    def _eval_evalf(self, prec):\n        c, m = self.as_coeff_Mul()\n        if c is S.NegativeOne:\n            if m.is_Mul:\n                rv = -AssocOp._eval_evalf(m, prec)\n            else:\n                mnew = m._eval_evalf(prec)\n                if mnew is not None:\n                    m = mnew\n                rv = -m\n        else:\n            rv = AssocOp._eval_evalf(self, prec)\n        if rv.is_number:\n            return rv.expand()\n        return rv\n\n    @property\n    def _mpc_(self):\n        \"\"\"\n        Convert self to an mpmath mpc if possible\n        \"\"\"\n        from sympy.core.numbers import I, Float\n        im_part, imag_unit = self.as_coeff_Mul()\n        if not imag_unit == I:\n            # ValueError may seem more reasonable but since it's a @property,\n            # we need to use AttributeError to keep from confusing things like\n            # hasattr.\n            raise AttributeError(\"Cannot convert Mul to mpc. Must be of the form Number*I\")\n\n        return (Float(0)._mpf_, Float(im_part)._mpf_)\n\n    @cacheit\n    def as_two_terms(self):\n        \"\"\"Return head and tail of self.\n\n        This is the most efficient way to get the head and tail of an\n        expression.\n\n        - if you want only the head, use self.args[0];\n        - if you want to process the arguments of the tail then use\n          self.as_coef_mul() which gives the head and a tuple containing\n          the arguments of the tail when treated as a Mul.\n        - if you want the coefficient when self is treated as an Add\n          then use self.as_coeff_add()[0]\n\n        Examples\n        ========\n\n        >>> from sympy.abc import x, y\n        >>> (3*x*y).as_two_terms()\n        (3, x*y)\n        \"\"\"\n        args = self.args\n\n        if len(args) == 1:\n            return S.One, self\n        elif len(args) == 2:\n            return args\n\n        else:\n            return args[0], self._new_rawargs(*args[1:])\n\n    @cacheit\n    def as_coefficients_dict(self):\n        \"\"\"Return a dictionary mapping terms to their coefficient.\n        Since the dictionary is a defaultdict, inquiries about terms which\n        were not present will return a coefficient of 0. The dictionary\n        is considered to have a single term.\n\n        Examples\n        ========\n\n        >>> from sympy.abc import a, x\n        >>> (3*a*x).as_coefficients_dict()\n        {a*x: 3}\n        >>> _[a]\n        0\n        \"\"\"\n\n        d = defaultdict(int)\n        args = self.args\n\n        if len(args) == 1 or not args[0].is_Number:\n            d[self] = S.One\n        else:\n            d[self._new_rawargs(*args[1:])] = args[0]\n\n        return d\n\n    @cacheit\n    def as_coeff_mul(self, *deps, rational=True, **kwargs):\n        if deps:\n            from sympy.utilities.iterables import sift\n            l1, l2 = sift(self.args, lambda x: x.has(*deps), binary=True)\n            return self._new_rawargs(*l2), tuple(l1)\n        args = self.args\n        if args[0].is_Number:\n            if not rational or args[0].is_Rational:\n                return args[0], args[1:]\n            elif args[0].is_extended_negative:\n                return S.NegativeOne, (-args[0],) + args[1:]\n        return S.One, args\n\n    def as_coeff_Mul(self, rational=False):\n        \"\"\"\n        Efficiently extract the coefficient of a product.\n        \"\"\"\n        coeff, args = self.args[0], self.args[1:]\n\n        if coeff.is_Number:\n            if not rational or coeff.is_Rational:\n                if len(args) == 1:\n                    return coeff, args[0]\n                else:\n                    return coeff, self._new_rawargs(*args)\n            elif coeff.is_extended_negative:\n                return S.NegativeOne, self._new_rawargs(*((-coeff,) + args))\n        return S.One, self\n\n    def as_real_imag(self, deep=True, **hints):\n        from sympy import Abs, expand_mul, im, re\n        other = []\n        coeffr = []\n        coeffi = []\n        addterms = S.One\n        for a in self.args:\n            r, i = a.as_real_imag()\n            if i.is_zero:\n                coeffr.append(r)\n            elif r.is_zero:\n                coeffi.append(i*S.ImaginaryUnit)\n            elif a.is_commutative:\n                # search for complex conjugate pairs:\n                for i, x in enumerate(other):\n                    if x == a.conjugate():\n                        coeffr.append(Abs(x)**2)\n                        del other[i]\n                        break\n                else:\n                    if a.is_Add:\n                        addterms *= a\n                    else:\n                        other.append(a)\n            else:\n                other.append(a)\n        m = self.func(*other)\n        if hints.get('ignore') == m:\n            return\n        if len(coeffi) % 2:\n            imco = im(coeffi.pop(0))\n            # all other pairs make a real factor; they will be\n            # put into reco below\n        else:\n            imco = S.Zero\n        reco = self.func(*(coeffr + coeffi))\n        r, i = (reco*re(m), reco*im(m))\n        if addterms == 1:\n            if m == 1:\n                if imco.is_zero:\n                    return (reco, S.Zero)\n                else:\n                    return (S.Zero, reco*imco)\n            if imco is S.Zero:\n                return (r, i)\n            return (-imco*i, imco*r)\n        addre, addim = expand_mul(addterms, deep=False).as_real_imag()\n        if imco is S.Zero:\n            return (r*addre - i*addim, i*addre + r*addim)\n        else:\n            r, i = -imco*i, imco*r\n            return (r*addre - i*addim, r*addim + i*addre)\n\n    @staticmethod\n    def _expandsums(sums):\n        \"\"\"\n        Helper function for _eval_expand_mul.\n\n        sums must be a list of instances of Basic.\n        \"\"\"\n\n        L = len(sums)\n        if L == 1:\n            return sums[0].args\n        terms = []\n        left = Mul._expandsums(sums[:L//2])\n        right = Mul._expandsums(sums[L//2:])\n\n        terms = [Mul(a, b) for a in left for b in right]\n        added = Add(*terms)\n        return Add.make_args(added)  # it may have collapsed down to one term\n\n    def _eval_expand_mul(self, **hints):\n        from sympy import fraction\n\n        # Handle things like 1/(x*(x + 1)), which are automatically converted\n        # to 1/x*1/(x + 1)\n        expr = self\n        n, d = fraction(expr)\n        if d.is_Mul:\n            n, d = [i._eval_expand_mul(**hints) if i.is_Mul else i\n                for i in (n, d)]\n            expr = n/d\n            if not expr.is_Mul:\n                return expr\n\n        plain, sums, rewrite = [], [], False\n        for factor in expr.args:\n            if factor.is_Add:\n                sums.append(factor)\n                rewrite = True\n            else:\n                if factor.is_commutative:\n                    plain.append(factor)\n                else:\n                    sums.append(Basic(factor))  # Wrapper\n\n        if not rewrite:\n            return expr\n        else:\n            plain = self.func(*plain)\n            if sums:\n                deep = hints.get(\"deep\", False)\n                terms = self.func._expandsums(sums)\n                args = []\n                for term in terms:\n                    t = self.func(plain, term)\n                    if t.is_Mul and any(a.is_Add for a in t.args) and deep:\n                        t = t._eval_expand_mul()\n                    args.append(t)\n                return Add(*args)\n            else:\n                return plain\n\n    @cacheit\n    def _eval_derivative(self, s):\n        args = list(self.args)\n        terms = []\n        for i in range(len(args)):\n            d = args[i].diff(s)\n            if d:\n                # Note: reduce is used in step of Mul as Mul is unable to\n                # handle subtypes and operation priority:\n                terms.append(reduce(lambda x, y: x*y, (args[:i] + [d] + args[i + 1:]), S.One))\n        return Add.fromiter(terms)\n\n    @cacheit\n    def _eval_derivative_n_times(self, s, n):\n        from sympy import Integer, factorial, prod, Sum, Max\n        from sympy.ntheory.multinomial import multinomial_coefficients_iterator\n        from .function import AppliedUndef\n        from .symbol import Symbol, symbols, Dummy\n        if not isinstance(s, AppliedUndef) and not isinstance(s, Symbol):\n            # other types of s may not be well behaved, e.g.\n            # (cos(x)*sin(y)).diff([[x, y, z]])\n            return super()._eval_derivative_n_times(s, n)\n        args = self.args\n        m = len(args)\n        if isinstance(n, (int, Integer)):\n            # https://en.wikipedia.org/wiki/General_Leibniz_rule#More_than_two_factors\n            terms = []\n            for kvals, c in multinomial_coefficients_iterator(m, n):\n                p = prod([arg.diff((s, k)) for k, arg in zip(kvals, args)])\n                terms.append(c * p)\n            return Add(*terms)\n        kvals = symbols(\"k1:%i\" % m, cls=Dummy)\n        klast = n - sum(kvals)\n        nfact = factorial(n)\n        e, l = (# better to use the multinomial?\n            nfact/prod(map(factorial, kvals))/factorial(klast)*\\\n            prod([args[t].diff((s, kvals[t])) for t in range(m-1)])*\\\n            args[-1].diff((s, Max(0, klast))),\n            [(k, 0, n) for k in kvals])\n        return Sum(e, *l)\n\n    def _eval_difference_delta(self, n, step):\n        from sympy.series.limitseq import difference_delta as dd\n        arg0 = self.args[0]\n        rest = Mul(*self.args[1:])\n        return (arg0.subs(n, n + step) * dd(rest, n, step) + dd(arg0, n, step) *\n                rest)\n\n    def _matches_simple(self, expr, repl_dict):\n        # handle (w*3).matches('x*5') -> {w: x*5/3}\n        coeff, terms = self.as_coeff_Mul()\n        terms = Mul.make_args(terms)\n        if len(terms) == 1:\n            newexpr = self.__class__._combine_inverse(expr, coeff)\n            return terms[0].matches(newexpr, repl_dict)\n        return\n\n    def matches(self, expr, repl_dict={}, old=False):\n        expr = sympify(expr)\n        repl_dict = repl_dict.copy()\n        if self.is_commutative and expr.is_commutative:\n            return self._matches_commutative(expr, repl_dict, old)\n        elif self.is_commutative is not expr.is_commutative:\n            return None\n\n        # Proceed only if both both expressions are non-commutative\n        c1, nc1 = self.args_cnc()\n        c2, nc2 = expr.args_cnc()\n        c1, c2 = [c or [1] for c in [c1, c2]]\n\n        # TODO: Should these be self.func?\n        comm_mul_self = Mul(*c1)\n        comm_mul_expr = Mul(*c2)\n\n        repl_dict = comm_mul_self.matches(comm_mul_expr, repl_dict, old)\n\n        # If the commutative arguments didn't match and aren't equal, then\n        # then the expression as a whole doesn't match\n        if repl_dict is None and c1 != c2:\n            return None\n\n        # Now match the non-commutative arguments, expanding powers to\n        # multiplications\n        nc1 = Mul._matches_expand_pows(nc1)\n        nc2 = Mul._matches_expand_pows(nc2)\n\n        repl_dict = Mul._matches_noncomm(nc1, nc2, repl_dict)\n\n        return repl_dict or None\n\n    @staticmethod\n    def _matches_expand_pows(arg_list):\n        new_args = []\n        for arg in arg_list:\n            if arg.is_Pow and arg.exp > 0:\n                new_args.extend([arg.base] * arg.exp)\n            else:\n                new_args.append(arg)\n        return new_args\n\n    @staticmethod\n    def _matches_noncomm(nodes, targets, repl_dict={}):\n        \"\"\"Non-commutative multiplication matcher.\n\n        `nodes` is a list of symbols within the matcher multiplication\n        expression, while `targets` is a list of arguments in the\n        multiplication expression being matched against.\n        \"\"\"\n        repl_dict = repl_dict.copy()\n        # List of possible future states to be considered\n        agenda = []\n        # The current matching state, storing index in nodes and targets\n        state = (0, 0)\n        node_ind, target_ind = state\n        # Mapping between wildcard indices and the index ranges they match\n        wildcard_dict = {}\n        repl_dict = repl_dict.copy()\n\n        while target_ind < len(targets) and node_ind < len(nodes):\n            node = nodes[node_ind]\n\n            if node.is_Wild:\n                Mul._matches_add_wildcard(wildcard_dict, state)\n\n            states_matches = Mul._matches_new_states(wildcard_dict, state,\n                                                     nodes, targets)\n            if states_matches:\n                new_states, new_matches = states_matches\n                agenda.extend(new_states)\n                if new_matches:\n                    for match in new_matches:\n                        repl_dict[match] = new_matches[match]\n            if not agenda:\n                return None\n            else:\n                state = agenda.pop()\n                node_ind, target_ind = state\n\n        return repl_dict\n\n    @staticmethod\n    def _matches_add_wildcard(dictionary, state):\n        node_ind, target_ind = state\n        if node_ind in dictionary:\n            begin, end = dictionary[node_ind]\n            dictionary[node_ind] = (begin, target_ind)\n        else:\n            dictionary[node_ind] = (target_ind, target_ind)\n\n    @staticmethod\n    def _matches_new_states(dictionary, state, nodes, targets):\n        node_ind, target_ind = state\n        node = nodes[node_ind]\n        target = targets[target_ind]\n\n        # Don't advance at all if we've exhausted the targets but not the nodes\n        if target_ind >= len(targets) - 1 and node_ind < len(nodes) - 1:\n            return None\n\n        if node.is_Wild:\n            match_attempt = Mul._matches_match_wilds(dictionary, node_ind,\n                                                     nodes, targets)\n            if match_attempt:\n                # If the same node has been matched before, don't return\n                # anything if the current match is diverging from the previous\n                # match\n                other_node_inds = Mul._matches_get_other_nodes(dictionary,\n                                                               nodes, node_ind)\n                for ind in other_node_inds:\n                    other_begin, other_end = dictionary[ind]\n                    curr_begin, curr_end = dictionary[node_ind]\n\n                    other_targets = targets[other_begin:other_end + 1]\n                    current_targets = targets[curr_begin:curr_end + 1]\n\n                    for curr, other in zip(current_targets, other_targets):\n                        if curr != other:\n                            return None\n\n                # A wildcard node can match more than one target, so only the\n                # target index is advanced\n                new_state = [(node_ind, target_ind + 1)]\n                # Only move on to the next node if there is one\n                if node_ind < len(nodes) - 1:\n                    new_state.append((node_ind + 1, target_ind + 1))\n                return new_state, match_attempt\n        else:\n            # If we're not at a wildcard, then make sure we haven't exhausted\n            # nodes but not targets, since in this case one node can only match\n            # one target\n            if node_ind >= len(nodes) - 1 and target_ind < len(targets) - 1:\n                return None\n\n            match_attempt = node.matches(target)\n\n            if match_attempt:\n                return [(node_ind + 1, target_ind + 1)], match_attempt\n            elif node == target:\n                return [(node_ind + 1, target_ind + 1)], None\n            else:\n                return None\n\n    @staticmethod\n    def _matches_match_wilds(dictionary, wildcard_ind, nodes, targets):\n        \"\"\"Determine matches of a wildcard with sub-expression in `target`.\"\"\"\n        wildcard = nodes[wildcard_ind]\n        begin, end = dictionary[wildcard_ind]\n        terms = targets[begin:end + 1]\n        # TODO: Should this be self.func?\n        mul = Mul(*terms) if len(terms) > 1 else terms[0]\n        return wildcard.matches(mul)\n\n    @staticmethod\n    def _matches_get_other_nodes(dictionary, nodes, node_ind):\n        \"\"\"Find other wildcards that may have already been matched.\"\"\"\n        other_node_inds = []\n        for ind in dictionary:\n            if nodes[ind] == nodes[node_ind]:\n                other_node_inds.append(ind)\n        return other_node_inds\n\n    @staticmethod\n    def _combine_inverse(lhs, rhs):\n        \"\"\"\n        Returns lhs/rhs, but treats arguments like symbols, so things\n        like oo/oo return 1 (instead of a nan) and ``I`` behaves like\n        a symbol instead of sqrt(-1).\n        \"\"\"\n        from .symbol import Dummy\n        if lhs == rhs:\n            return S.One\n\n        def check(l, r):\n            if l.is_Float and r.is_comparable:\n                # if both objects are added to 0 they will share the same \"normalization\"\n                # and are more likely to compare the same. Since Add(foo, 0) will not allow\n                # the 0 to pass, we use __add__ directly.\n                return l.__add__(0) == r.evalf().__add__(0)\n            return False\n        if check(lhs, rhs) or check(rhs, lhs):\n            return S.One\n        if any(i.is_Pow or i.is_Mul for i in (lhs, rhs)):\n            # gruntz and limit wants a literal I to not combine\n            # with a power of -1\n            d = Dummy('I')\n            _i = {S.ImaginaryUnit: d}\n            i_ = {d: S.ImaginaryUnit}\n            a = lhs.xreplace(_i).as_powers_dict()\n            b = rhs.xreplace(_i).as_powers_dict()\n            blen = len(b)\n            for bi in tuple(b.keys()):\n                if bi in a:\n                    a[bi] -= b.pop(bi)\n                    if not a[bi]:\n                        a.pop(bi)\n            if len(b) != blen:\n                lhs = Mul(*[k**v for k, v in a.items()]).xreplace(i_)\n                rhs = Mul(*[k**v for k, v in b.items()]).xreplace(i_)\n        return lhs/rhs\n\n    def as_powers_dict(self):\n        d = defaultdict(int)\n        for term in self.args:\n            for b, e in term.as_powers_dict().items():\n                d[b] += e\n        return d\n\n    def as_numer_denom(self):\n        # don't use _from_args to rebuild the numerators and denominators\n        # as the order is not guaranteed to be the same once they have\n        # been separated from each other\n        numers, denoms = list(zip(*[f.as_numer_denom() for f in self.args]))\n        return self.func(*numers), self.func(*denoms)\n\n    def as_base_exp(self):\n        e1 = None\n        bases = []\n        nc = 0\n        for m in self.args:\n            b, e = m.as_base_exp()\n            if not b.is_commutative:\n                nc += 1\n            if e1 is None:\n                e1 = e\n            elif e != e1 or nc > 1:\n                return self, S.One\n            bases.append(b)\n        return self.func(*bases), e1\n\n    def _eval_is_polynomial(self, syms):\n        return all(term._eval_is_polynomial(syms) for term in self.args)\n\n    def _eval_is_rational_function(self, syms):\n        return all(term._eval_is_rational_function(syms) for term in self.args)\n\n    def _eval_is_meromorphic(self, x, a):\n        return _fuzzy_group((arg.is_meromorphic(x, a) for arg in self.args),\n                            quick_exit=True)\n\n    def _eval_is_algebraic_expr(self, syms):\n        return all(term._eval_is_algebraic_expr(syms) for term in self.args)\n\n    _eval_is_commutative = lambda self: _fuzzy_group(\n        a.is_commutative for a in self.args)\n\n    def _eval_is_complex(self):\n        comp = _fuzzy_group(a.is_complex for a in self.args)\n        if comp is False:\n            if any(a.is_infinite for a in self.args):\n                if any(a.is_zero is not False for a in self.args):\n                    return None\n                return False\n        return comp\n\n    def _eval_is_finite(self):\n        if all(a.is_finite for a in self.args):\n            return True\n        if any(a.is_infinite for a in self.args):\n            if all(a.is_zero is False for a in self.args):\n                return False\n\n    def _eval_is_infinite(self):\n        if any(a.is_infinite for a in self.args):\n            if any(a.is_zero for a in self.args):\n                return S.NaN.is_infinite\n            if any(a.is_zero is None for a in self.args):\n                return None\n            return True\n\n    def _eval_is_rational(self):\n        r = _fuzzy_group((a.is_rational for a in self.args), quick_exit=True)\n        if r:\n            return r\n        elif r is False:\n            return self.is_zero\n\n    def _eval_is_algebraic(self):\n        r = _fuzzy_group((a.is_algebraic for a in self.args), quick_exit=True)\n        if r:\n            return r\n        elif r is False:\n            return self.is_zero\n\n    def _eval_is_zero(self):\n        zero = infinite = False\n        for a in self.args:\n            z = a.is_zero\n            if z:\n                if infinite:\n                    return  # 0*oo is nan and nan.is_zero is None\n                zero = True\n            else:\n                if not a.is_finite:\n                    if zero:\n                        return  # 0*oo is nan and nan.is_zero is None\n                    infinite = True\n                if zero is False and z is None:  # trap None\n                    zero = None\n        return zero\n\n    def _eval_is_integer(self):\n        from sympy import fraction\n        from sympy.core.numbers import Float\n\n        is_rational = self._eval_is_rational()\n        if is_rational is False:\n            return False\n\n        # use exact=True to avoid recomputing num or den\n        n, d = fraction(self, exact=True)\n        if is_rational:\n            if d is S.One:\n                return True\n        if d.is_even:\n            if d.is_prime:  # literal or symbolic 2\n                return n.is_even\n            if n.is_odd:\n                return False  # true even if d = 0\n        if n == d:\n            return fuzzy_and([not bool(self.atoms(Float)),\n            fuzzy_not(d.is_zero)])\n\n    def _eval_is_polar(self):\n        has_polar = any(arg.is_polar for arg in self.args)\n        return has_polar and \\\n            all(arg.is_polar or arg.is_positive for arg in self.args)\n\n    def _eval_is_extended_real(self):\n        return self._eval_real_imag(True)\n\n    def _eval_real_imag(self, real):\n        zero = False\n        t_not_re_im = None\n\n        for t in self.args:\n            if (t.is_complex or t.is_infinite) is False and t.is_extended_real is False:\n                return False\n            elif t.is_imaginary:  # I\n                real = not real\n            elif t.is_extended_real:  # 2\n                if not zero:\n                    z = t.is_zero\n                    if not z and zero is False:\n                        zero = z\n                    elif z:\n                        if all(a.is_finite for a in self.args):\n                            return True\n                        return\n            elif t.is_extended_real is False:\n                # symbolic or literal like `2 + I` or symbolic imaginary\n                if t_not_re_im:\n                    return  # complex terms might cancel\n                t_not_re_im = t\n            elif t.is_imaginary is False:  # symbolic like `2` or `2 + I`\n                if t_not_re_im:\n                    return  # complex terms might cancel\n                t_not_re_im = t\n            else:\n                return\n\n        if t_not_re_im:\n            if t_not_re_im.is_extended_real is False:\n                if real:  # like 3\n                    return zero  # 3*(smthng like 2 + I or i) is not real\n            if t_not_re_im.is_imaginary is False:  # symbolic 2 or 2 + I\n                if not real:  # like I\n                    return zero  # I*(smthng like 2 or 2 + I) is not real\n        elif zero is False:\n            return real  # can't be trumped by 0\n        elif real:\n            return real  # doesn't matter what zero is\n\n    def _eval_is_imaginary(self):\n        z = self.is_zero\n        if z:\n            return False\n        if self.is_finite is False:\n            return False\n        elif z is False and self.is_finite is True:\n            return self._eval_real_imag(False)\n\n    def _eval_is_hermitian(self):\n        return self._eval_herm_antiherm(True)\n\n    def _eval_herm_antiherm(self, real):\n        one_nc = zero = one_neither = False\n\n        for t in self.args:\n            if not t.is_commutative:\n                if one_nc:\n                    return\n                one_nc = True\n\n            if t.is_antihermitian:\n                real = not real\n            elif t.is_hermitian:\n                if not zero:\n                    z = t.is_zero\n                    if not z and zero is False:\n                        zero = z\n                    elif z:\n                        if all(a.is_finite for a in self.args):\n                            return True\n                        return\n            elif t.is_hermitian is False:\n                if one_neither:\n                    return\n                one_neither = True\n            else:\n                return\n\n        if one_neither:\n            if real:\n                return zero\n        elif zero is False or real:\n            return real\n\n    def _eval_is_antihermitian(self):\n        z = self.is_zero\n        if z:\n            return False\n        elif z is False:\n            return self._eval_herm_antiherm(False)\n\n    def _eval_is_irrational(self):\n        for t in self.args:\n            a = t.is_irrational\n            if a:\n                others = list(self.args)\n                others.remove(t)\n                if all((x.is_rational and fuzzy_not(x.is_zero)) is True for x in others):\n                    return True\n                return\n            if a is None:\n                return\n        if all(x.is_real for x in self.args):\n            return False\n\n    def _eval_is_extended_positive(self):\n        \"\"\"Return True if self is positive, False if not, and None if it\n        cannot be determined.\n\n        Explanation\n        ===========\n\n        This algorithm is non-recursive and works by keeping track of the\n        sign which changes when a negative or nonpositive is encountered.\n        Whether a nonpositive or nonnegative is seen is also tracked since\n        the presence of these makes it impossible to return True, but\n        possible to return False if the end result is nonpositive. e.g.\n\n            pos * neg * nonpositive -> pos or zero -> None is returned\n            pos * neg * nonnegative -> neg or zero -> False is returned\n        \"\"\"\n        return self._eval_pos_neg(1)\n\n    def _eval_pos_neg(self, sign):\n        saw_NON = saw_NOT = False\n        for t in self.args:\n            if t.is_extended_positive:\n                continue\n            elif t.is_extended_negative:\n                sign = -sign\n            elif t.is_zero:\n                if all(a.is_finite for a in self.args):\n                    return False\n                return\n            elif t.is_extended_nonpositive:\n                sign = -sign\n                saw_NON = True\n            elif t.is_extended_nonnegative:\n                saw_NON = True\n            # FIXME: is_positive/is_negative is False doesn't take account of\n            # Symbol('x', infinite=True, extended_real=True) which has\n            # e.g. is_positive is False but has uncertain sign.\n            elif t.is_positive is False:\n                sign = -sign\n                if saw_NOT:\n                    return\n                saw_NOT = True\n            elif t.is_negative is False:\n                if saw_NOT:\n                    return\n                saw_NOT = True\n            else:\n                return\n        if sign == 1 and saw_NON is False and saw_NOT is False:\n            return True\n        if sign < 0:\n            return False\n\n    def _eval_is_extended_negative(self):\n        return self._eval_pos_neg(-1)\n\n    def _eval_is_odd(self):\n        is_integer = self.is_integer\n\n        if is_integer:\n            r, acc = True, 1\n            for t in self.args:\n                if not t.is_integer:\n                    return None\n                elif t.is_even:\n                    r = False\n                elif t.is_integer:\n                    if r is False:\n                        pass\n                    elif acc != 1 and (acc + t).is_odd:\n                        r = False\n                    elif t.is_odd is None:\n                        r = None\n                acc = t\n            return r\n\n        # !integer -> !odd\n        elif is_integer is False:\n            return False\n\n    def _eval_is_even(self):\n        is_integer = self.is_integer\n\n        if is_integer:\n            return fuzzy_not(self.is_odd)\n\n        elif is_integer is False:\n            return False\n\n    def _eval_is_composite(self):\n        \"\"\"\n        Here we count the number of arguments that have a minimum value\n        greater than two.\n        If there are more than one of such a symbol then the result is composite.\n        Else, the result cannot be determined.\n        \"\"\"\n        number_of_args = 0 # count of symbols with minimum value greater than one\n        for arg in self.args:\n            if not (arg.is_integer and arg.is_positive):\n                return None\n            if (arg-1).is_positive:\n                number_of_args += 1\n\n        if number_of_args > 1:\n            return True\n\n    def _eval_subs(self, old, new):\n        from sympy.functions.elementary.complexes import sign\n        from sympy.ntheory.factor_ import multiplicity\n        from sympy.simplify.powsimp import powdenest\n        from sympy.simplify.radsimp import fraction\n\n        if not old.is_Mul:\n            return None\n\n        # try keep replacement literal so -2*x doesn't replace 4*x\n        if old.args[0].is_Number and old.args[0] < 0:\n            if self.args[0].is_Number:\n                if self.args[0] < 0:\n                    return self._subs(-old, -new)\n                return None\n\n        def base_exp(a):\n            # if I and -1 are in a Mul, they get both end up with\n            # a -1 base (see issue 6421); all we want here are the\n            # true Pow or exp separated into base and exponent\n            from sympy import exp\n            if a.is_Pow or isinstance(a, exp):\n                return a.as_base_exp()\n            return a, S.One\n\n        def breakup(eq):\n            \"\"\"break up powers of eq when treated as a Mul:\n                   b**(Rational*e) -> b**e, Rational\n                commutatives come back as a dictionary {b**e: Rational}\n                noncommutatives come back as a list [(b**e, Rational)]\n            \"\"\"\n\n            (c, nc) = (defaultdict(int), list())\n            for a in Mul.make_args(eq):\n                a = powdenest(a)\n                (b, e) = base_exp(a)\n                if e is not S.One:\n                    (co, _) = e.as_coeff_mul()\n                    b = Pow(b, e/co)\n                    e = co\n                if a.is_commutative:\n                    c[b] += e\n                else:\n                    nc.append([b, e])\n            return (c, nc)\n\n        def rejoin(b, co):\n            \"\"\"\n            Put rational back with exponent; in general this is not ok, but\n            since we took it from the exponent for analysis, it's ok to put\n            it back.\n            \"\"\"\n\n            (b, e) = base_exp(b)\n            return Pow(b, e*co)\n\n        def ndiv(a, b):\n            \"\"\"if b divides a in an extractive way (like 1/4 divides 1/2\n            but not vice versa, and 2/5 does not divide 1/3) then return\n            the integer number of times it divides, else return 0.\n            \"\"\"\n            if not b.q % a.q or not a.q % b.q:\n                return int(a/b)\n            return 0\n\n        # give Muls in the denominator a chance to be changed (see issue 5651)\n        # rv will be the default return value\n        rv = None\n        n, d = fraction(self)\n        self2 = self\n        if d is not S.One:\n            self2 = n._subs(old, new)/d._subs(old, new)\n            if not self2.is_Mul:\n                return self2._subs(old, new)\n            if self2 != self:\n                rv = self2\n\n        # Now continue with regular substitution.\n\n        # handle the leading coefficient and use it to decide if anything\n        # should even be started; we always know where to find the Rational\n        # so it's a quick test\n\n        co_self = self2.args[0]\n        co_old = old.args[0]\n        co_xmul = None\n        if co_old.is_Rational and co_self.is_Rational:\n            # if coeffs are the same there will be no updating to do\n            # below after breakup() step; so skip (and keep co_xmul=None)\n            if co_old != co_self:\n                co_xmul = co_self.extract_multiplicatively(co_old)\n        elif co_old.is_Rational:\n            return rv\n\n        # break self and old into factors\n\n        (c, nc) = breakup(self2)\n        (old_c, old_nc) = breakup(old)\n\n        # update the coefficients if we had an extraction\n        # e.g. if co_self were 2*(3/35*x)**2 and co_old = 3/5\n        # then co_self in c is replaced by (3/5)**2 and co_residual\n        # is 2*(1/7)**2\n\n        if co_xmul and co_xmul.is_Rational and abs(co_old) != 1:\n            mult = S(multiplicity(abs(co_old), co_self))\n            c.pop(co_self)\n            if co_old in c:\n                c[co_old] += mult\n            else:\n                c[co_old] = mult\n            co_residual = co_self/co_old**mult\n        else:\n            co_residual = 1\n\n        # do quick tests to see if we can't succeed\n\n        ok = True\n        if len(old_nc) > len(nc):\n            # more non-commutative terms\n            ok = False\n        elif len(old_c) > len(c):\n            # more commutative terms\n            ok = False\n        elif {i[0] for i in old_nc}.difference({i[0] for i in nc}):\n            # unmatched non-commutative bases\n            ok = False\n        elif set(old_c).difference(set(c)):\n            # unmatched commutative terms\n            ok = False\n        elif any(sign(c[b]) != sign(old_c[b]) for b in old_c):\n            # differences in sign\n            ok = False\n        if not ok:\n            return rv\n\n        if not old_c:\n            cdid = None\n        else:\n            rat = []\n            for (b, old_e) in old_c.items():\n                c_e = c[b]\n                rat.append(ndiv(c_e, old_e))\n                if not rat[-1]:\n                    return rv\n            cdid = min(rat)\n\n        if not old_nc:\n            ncdid = None\n            for i in range(len(nc)):\n                nc[i] = rejoin(*nc[i])\n        else:\n            ncdid = 0  # number of nc replacements we did\n            take = len(old_nc)  # how much to look at each time\n            limit = cdid or S.Infinity  # max number that we can take\n            failed = []  # failed terms will need subs if other terms pass\n            i = 0\n            while limit and i + take <= len(nc):\n                hit = False\n\n                # the bases must be equivalent in succession, and\n                # the powers must be extractively compatible on the\n                # first and last factor but equal in between.\n\n                rat = []\n                for j in range(take):\n                    if nc[i + j][0] != old_nc[j][0]:\n                        break\n                    elif j == 0:\n                        rat.append(ndiv(nc[i + j][1], old_nc[j][1]))\n                    elif j == take - 1:\n                        rat.append(ndiv(nc[i + j][1], old_nc[j][1]))\n                    elif nc[i + j][1] != old_nc[j][1]:\n                        break\n                    else:\n                        rat.append(1)\n                    j += 1\n                else:\n                    ndo = min(rat)\n                    if ndo:\n                        if take == 1:\n                            if cdid:\n                                ndo = min(cdid, ndo)\n                            nc[i] = Pow(new, ndo)*rejoin(nc[i][0],\n                                    nc[i][1] - ndo*old_nc[0][1])\n                        else:\n                            ndo = 1\n\n                            # the left residual\n\n                            l = rejoin(nc[i][0], nc[i][1] - ndo*\n                                    old_nc[0][1])\n\n                            # eliminate all middle terms\n\n                            mid = new\n\n                            # the right residual (which may be the same as the middle if take == 2)\n\n                            ir = i + take - 1\n                            r = (nc[ir][0], nc[ir][1] - ndo*\n                                 old_nc[-1][1])\n                            if r[1]:\n                                if i + take < len(nc):\n                                    nc[i:i + take] = [l*mid, r]\n                                else:\n                                    r = rejoin(*r)\n                                    nc[i:i + take] = [l*mid*r]\n                            else:\n\n                                # there was nothing left on the right\n\n                                nc[i:i + take] = [l*mid]\n\n                        limit -= ndo\n                        ncdid += ndo\n                        hit = True\n                if not hit:\n\n                    # do the subs on this failing factor\n\n                    failed.append(i)\n                i += 1\n            else:\n\n                if not ncdid:\n                    return rv\n\n                # although we didn't fail, certain nc terms may have\n                # failed so we rebuild them after attempting a partial\n                # subs on them\n\n                failed.extend(range(i, len(nc)))\n                for i in failed:\n                    nc[i] = rejoin(*nc[i]).subs(old, new)\n\n        # rebuild the expression\n\n        if cdid is None:\n            do = ncdid\n        elif ncdid is None:\n            do = cdid\n        else:\n            do = min(ncdid, cdid)\n\n        margs = []\n        for b in c:\n            if b in old_c:\n\n                # calculate the new exponent\n\n                e = c[b] - old_c[b]*do\n                margs.append(rejoin(b, e))\n            else:\n                margs.append(rejoin(b.subs(old, new), c[b]))\n        if cdid and not ncdid:\n\n            # in case we are replacing commutative with non-commutative,\n            # we want the new term to come at the front just like the\n            # rest of this routine\n\n            margs = [Pow(new, cdid)] + margs\n        return co_residual*self2.func(*margs)*self2.func(*nc)\n\n    def _eval_nseries(self, x, n, logx, cdir=0):\n        from sympy import degree, Mul, Order, ceiling, powsimp, PolynomialError\n        from itertools import product\n\n        def coeff_exp(term, x):\n            coeff, exp = S.One, S.Zero\n            for factor in Mul.make_args(term):\n                if factor.has(x):\n                    base, exp = factor.as_base_exp()\n                    if base != x:\n                        try:\n                            return term.leadterm(x)\n                        except ValueError:\n                            return term, S.Zero\n                else:\n                    coeff *= factor\n            return coeff, exp\n\n        ords = []\n\n        try:\n            for t in self.args:\n                coeff, exp = t.leadterm(x)\n                if not coeff.has(x):\n                    ords.append((t, exp))\n                else:\n                    raise ValueError\n\n            n0 = sum(t[1] for t in ords)\n            facs = []\n            for t, m in ords:\n                n1 = ceiling(n - n0 + m)\n                s = t.nseries(x, n=n1, logx=logx, cdir=cdir)\n                ns = s.getn()\n                if ns is not None:\n                    if ns < n1:  # less than expected\n                        n -= n1 - ns    # reduce n\n                facs.append(s.removeO())\n\n        except (ValueError, NotImplementedError, TypeError, AttributeError):\n            facs = [t.nseries(x, n=n, logx=logx, cdir=cdir) for t in self.args]\n            res = powsimp(self.func(*facs).expand(), combine='exp', deep=True)\n            if res.has(Order):\n                res += Order(x**n, x)\n            return res\n\n        res = 0\n        ords2 = [Add.make_args(factor) for factor in facs]\n\n        for fac in product(*ords2):\n            ords3 = [coeff_exp(term, x) for term in fac]\n            coeffs, powers = zip(*ords3)\n            power = sum(powers)\n            if power < n:\n                res += Mul(*coeffs)*(x**power)\n\n        if self.is_polynomial(x):\n            try:\n                if degree(self, x) != degree(res, x):\n                    res += Order(x**n, x)\n            except PolynomialError:\n                pass\n            else:\n                return res\n\n        for i in (1, 2, 3):\n            if (res - self).subs(x, i) is not S.Zero:\n                res += Order(x**n, x)\n                break\n        return res\n\n    def _eval_as_leading_term(self, x, cdir=0):\n        return self.func(*[t.as_leading_term(x, cdir=cdir) for t in self.args])\n\n    def _eval_conjugate(self):\n        return self.func(*[t.conjugate() for t in self.args])\n\n    def _eval_transpose(self):\n        return self.func(*[t.transpose() for t in self.args[::-1]])\n\n    def _eval_adjoint(self):\n        return self.func(*[t.adjoint() for t in self.args[::-1]])\n\n    def _sage_(self):\n        s = 1\n        for x in self.args:\n            s *= x._sage_()\n        return s\n\n    def as_content_primitive(self, radical=False, clear=True):\n        \"\"\"Return the tuple (R, self/R) where R is the positive Rational\n        extracted from self.\n\n        Examples\n        ========\n\n        >>> from sympy import sqrt\n        >>> (-3*sqrt(2)*(2 - 2*sqrt(2))).as_content_primitive()\n        (6, -sqrt(2)*(1 - sqrt(2)))\n\n        See docstring of Expr.as_content_primitive for more examples.\n        \"\"\"\n\n        coef = S.One\n        args = []\n        for i, a in enumerate(self.args):\n            c, p = a.as_content_primitive(radical=radical, clear=clear)\n            coef *= c\n            if p is not S.One:\n                args.append(p)\n        # don't use self._from_args here to reconstruct args\n        # since there may be identical args now that should be combined\n        # e.g. (2+2*x)*(3+3*x) should be (6, (1 + x)**2) not (6, (1+x)*(1+x))\n        return coef, self.func(*args)\n\n    def as_ordered_factors(self, order=None):\n        \"\"\"Transform an expression into an ordered list of factors.\n\n        Examples\n        ========\n\n        >>> from sympy import sin, cos\n        >>> from sympy.abc import x, y\n\n        >>> (2*x*y*sin(x)*cos(x)).as_ordered_factors()\n        [2, x, y, sin(x), cos(x)]\n\n        \"\"\"\n        cpart, ncpart = self.args_cnc()\n        cpart.sort(key=lambda expr: expr.sort_key(order=order))\n        return cpart + ncpart\n\n    @property\n    def _sorted_args(self):\n        return tuple(self.as_ordered_factors())\n\nmul = AssocOpDispatcher('mul')\n\ndef prod(a, start=1):\n    \"\"\"Return product of elements of a. Start with int 1 so if only\n       ints are included then an int result is returned.\n\n    Examples\n    ========\n\n    >>> from sympy import prod, S\n    >>> prod(range(3))\n    0\n    >>> type(_) is int\n    True\n    >>> prod([S(2), 3])\n    6\n    >>> _.is_Integer\n    True\n\n    You can start the product at something other than 1:\n\n    >>> prod([1, 2], 3)\n    6\n\n    \"\"\"\n    return reduce(operator.mul, a, start)\n\n\ndef _keep_coeff(coeff, factors, clear=True, sign=False):\n    \"\"\"Return ``coeff*factors`` unevaluated if necessary.\n\n    If ``clear`` is False, do not keep the coefficient as a factor\n    if it can be distributed on a single factor such that one or\n    more terms will still have integer coefficients.\n\n    If ``sign`` is True, allow a coefficient of -1 to remain factored out.\n\n    Examples\n    ========\n\n    >>> from sympy.core.mul import _keep_coeff\n    >>> from sympy.abc import x, y\n    >>> from sympy import S\n\n    >>> _keep_coeff(S.Half, x + 2)\n    (x + 2)/2\n    >>> _keep_coeff(S.Half, x + 2, clear=False)\n    x/2 + 1\n    >>> _keep_coeff(S.Half, (x + 2)*y, clear=False)\n    y*(x + 2)/2\n    >>> _keep_coeff(S(-1), x + y)\n(C)                             num_exp.append((b, e))\n                            continue\n\n                c_powers.append((b, e))\n\n            # NON-COMMUTATIVE\n            # TODO: Make non-commutative exponents not combine automatically\n            else:\n                if o is not NC_Marker:\n                    nc_seq.append(o)\n\n                # process nc_seq (if any)\n                while nc_seq:\n                    o = nc_seq.pop(0)\n                    if not nc_part:\n                        nc_part.append(o)\n                        continue\n\n                    #                             b    c       b+c\n                    # try to combine last terms: a  * a   ->  a\n                    o1 = nc_part.pop()\n                    b1, e1 = o1.as_base_exp()\n                    b2, e2 = o.as_base_exp()\n                    new_exp = e1 + e2\n                    # Only allow powers to combine if the new exponent is\n                    # not an Add. This allow things like a**2*b**3 == a**5\n                    # if a.is_commutative == False, but prohibits\n                    # a**x*a**y and x**a*x**b from combining (x,y commute).\n                    if b1 == b2 and (not new_exp.is_Add):\n                        o12 = b1 ** new_exp\n\n                        # now o12 could be a commutative object\n                        if o12.is_commutative:\n                            seq.append(o12)\n                            continue\n                        else:\n                            nc_seq.insert(0, o12)\n\n                    else:\n                        nc_part.append(o1)\n                        nc_part.append(o)\n\n        # We do want a combined exponent if it would not be an Add, such as\n        #  y    2y     3y\n        # x  * x   -> x\n        # We determine if two exponents have the same term by using\n        # as_coeff_Mul.\n        #\n        # Unfortunately, this isn't smart enough to consider combining into\n        # exponents that might already be adds, so things like:\n        #  z - y    y\n        # x      * x  will be left alone.  This is because checking every possible\n        # combination can slow things down.\n\n        # gather exponents of common bases...\n        def _gather(c_powers):\n            common_b = {}  # b:e\n            for b, e in c_powers:\n                co = e.as_coeff_Mul()\n                common_b.setdefault(b, {}).setdefault(\n                    co[1], []).append(co[0])\n            for b, d in common_b.items():\n                for di, li in d.items():\n                    d[di] = Add(*li)\n            new_c_powers = []\n            for b, e in common_b.items():\n                new_c_powers.extend([(b, c*t) for t, c in e.items()])\n            return new_c_powers\n\n        # in c_powers\n        c_powers = _gather(c_powers)\n\n        # and in num_exp\n        num_exp = _gather(num_exp)\n\n        # --- PART 2 ---\n        #\n        # o process collected powers  (x**0 -> 1; x**1 -> x; otherwise Pow)\n        # o combine collected powers  (2**x * 3**x -> 6**x)\n        #   with numeric base\n\n        # ................................\n        # now we have:\n        # - coeff:\n        # - c_powers:    (b, e)\n        # - num_exp:     (2, e)\n        # - pnum_rat:    {(1/3, [1/3, 2/3, 1/4])}\n\n        #  0             1\n        # x  -> 1       x  -> x\n\n        # this should only need to run twice; if it fails because\n        # it needs to be run more times, perhaps this should be\n        # changed to a \"while True\" loop -- the only reason it\n        # isn't such now is to allow a less-than-perfect result to\n        # be obtained rather than raising an error or entering an\n        # infinite loop\n        for i in range(2):\n            new_c_powers = []\n            changed = False\n            for b, e in c_powers:\n                if e.is_zero:\n                    # canceling out infinities yields NaN\n                    if (b.is_Add or b.is_Mul) and any(infty in b.args\n                        for infty in (S.ComplexInfinity, S.Infinity,\n                                      S.NegativeInfinity)):\n                        return [S.NaN], [], None\n                    continue\n                if e is S.One:\n                    if b.is_Number:\n                        coeff *= b\n                        continue\n                    p = b\n                if e is not S.One:\n                    p = Pow(b, e)\n                    # check to make sure that the base doesn't change\n                    # after exponentiation; to allow for unevaluated\n                    # Pow, we only do so if b is not already a Pow\n                    if p.is_Pow and not b.is_Pow:\n                        bi = b\n                        b, e = p.as_base_exp()\n                        if b != bi:\n                            changed = True\n                c_part.append(p)\n                new_c_powers.append((b, e))\n            # there might have been a change, but unless the base\n            # matches some other base, there is nothing to do\n            if changed and len({\n                    b for b, e in new_c_powers}) != len(new_c_powers):\n                # start over again\n                c_part = []\n                c_powers = _gather(new_c_powers)\n            else:\n                break\n\n        #  x    x     x\n        # 2  * 3  -> 6\n        inv_exp_dict = {}   # exp:Mul(num-bases)     x    x\n                            # e.g.  x:6  for  ... * 2  * 3  * ...\n        for b, e in num_exp:\n            inv_exp_dict.setdefault(e, []).append(b)\n        for e, b in inv_exp_dict.items():\n            inv_exp_dict[e] = cls(*b)\n        c_part.extend([Pow(b, e) for e, b in inv_exp_dict.items() if e])\n\n        # b, e -> e' = sum(e), b\n        # {(1/5, [1/3]), (1/2, [1/12, 1/4]} -> {(1/3, [1/5, 1/2])}\n        comb_e = {}\n        for b, e in pnum_rat.items():\n            comb_e.setdefault(Add(*e), []).append(b)\n        del pnum_rat\n        # process them, reducing exponents to values less than 1\n        # and updating coeff if necessary else adding them to\n        # num_rat for further processing\n        num_rat = []\n        for e, b in comb_e.items():\n            b = cls(*b)\n            if e.q == 1:\n                coeff *= Pow(b, e)\n                continue\n            if e.p > e.q:\n                e_i, ep = divmod(e.p, e.q)\n                coeff *= Pow(b, e_i)\n                e = Rational(ep, e.q)\n            num_rat.append((b, e))\n        del comb_e\n\n        # extract gcd of bases in num_rat\n        # 2**(1/3)*6**(1/4) -> 2**(1/3+1/4)*3**(1/4)\n        pnew = defaultdict(list)\n        i = 0  # steps through num_rat which may grow\n        while i < len(num_rat):\n            bi, ei = num_rat[i]\n            grow = []\n            for j in range(i + 1, len(num_rat)):\n                bj, ej = num_rat[j]\n                g = bi.gcd(bj)\n                if g is not S.One:\n                    # 4**r1*6**r2 -> 2**(r1+r2)  *  2**r1 *  3**r2\n                    # this might have a gcd with something else\n                    e = ei + ej\n                    if e.q == 1:\n                        coeff *= Pow(g, e)\n                    else:\n                        if e.p > e.q:\n                            e_i, ep = divmod(e.p, e.q)  # change e in place\n                            coeff *= Pow(g, e_i)\n                            e = Rational(ep, e.q)\n                        grow.append((g, e))\n                    # update the jth item\n                    num_rat[j] = (bj/g, ej)\n                    # update bi that we are checking with\n                    bi = bi/g\n                    if bi is S.One:\n                        break\n            if bi is not S.One:\n                obj = Pow(bi, ei)\n                if obj.is_Number:\n                    coeff *= obj\n                else:\n                    # changes like sqrt(12) -> 2*sqrt(3)\n                    for obj in Mul.make_args(obj):\n                        if obj.is_Number:\n                            coeff *= obj\n                        else:\n                            assert obj.is_Pow\n                            bi, ei = obj.args\n                            pnew[ei].append(bi)\n\n            num_rat.extend(grow)\n            i += 1\n\n        # combine bases of the new powers\n        for e, b in pnew.items():\n            pnew[e] = cls(*b)\n\n        # handle -1 and I\n        if neg1e:\n            # treat I as (-1)**(1/2) and compute -1's total exponent\n            p, q =  neg1e.as_numer_denom()\n            # if the integer part is odd, extract -1\n            n, p = divmod(p, q)\n            if n % 2:\n                coeff = -coeff\n            # if it's a multiple of 1/2 extract I\n            if q == 2:\n                c_part.append(S.ImaginaryUnit)\n            elif p:\n                # see if there is any positive base this power of\n                # -1 can join\n                neg1e = Rational(p, q)\n                for e, b in pnew.items():\n                    if e == neg1e and b.is_positive:\n                        pnew[e] = -b\n                        break\n                else:\n                    # keep it separate; we've already evaluated it as\n                    # much as possible so evaluate=False\n                    c_part.append(Pow(S.NegativeOne, neg1e, evaluate=False))\n\n        # add all the pnew powers\n        c_part.extend([Pow(b, e) for e, b in pnew.items()])\n\n        # oo, -oo\n        if (coeff is S.Infinity) or (coeff is S.NegativeInfinity):\n            def _handle_for_oo(c_part, coeff_sign):\n                new_c_part = []\n                for t in c_part:\n                    if t.is_extended_positive:\n                        continue\n                    if t.is_extended_negative:\n                        coeff_sign *= -1\n                        continue\n                    new_c_part.append(t)\n                return new_c_part, coeff_sign\n            c_part, coeff_sign = _handle_for_oo(c_part, 1)\n            nc_part, coeff_sign = _handle_for_oo(nc_part, coeff_sign)\n            coeff *= coeff_sign\n\n        # zoo\n        if coeff is S.ComplexInfinity:\n            # zoo might be\n            #   infinite_real + bounded_im\n            #   bounded_real + infinite_im\n            #   infinite_real + infinite_im\n            # and non-zero real or imaginary will not change that status.\n            c_part = [c for c in c_part if not (fuzzy_not(c.is_zero) and\n                                                c.is_extended_real is not None)]\n            nc_part = [c for c in nc_part if not (fuzzy_not(c.is_zero) and\n                                                  c.is_extended_real is not None)]\n\n        # 0\n        elif coeff.is_zero:\n            # we know for sure the result will be 0 except the multiplicand\n            # is infinity or a matrix\n            if any(isinstance(c, MatrixExpr) for c in nc_part):\n                return [coeff], nc_part, order_symbols\n            if any(c.is_finite == False for c in c_part):\n                return [S.NaN], [], order_symbols\n            return [coeff], [], order_symbols\n\n        # check for straggling Numbers that were produced\n        _new = []\n        for i in c_part:\n            if i.is_Number:\n                coeff *= i\n            else:\n                _new.append(i)\n        c_part = _new\n\n        # order commutative part canonically\n        _mulsort(c_part)\n\n        # current code expects coeff to be always in slot-0\n        if coeff is not S.One:\n            c_part.insert(0, coeff)\n\n        # we are done\n        if (global_parameters.distribute and not nc_part and len(c_part) == 2 and\n                c_part[0].is_Number and c_part[0].is_finite and c_part[1].is_Add):\n            # 2*(1+a) -> 2 + 2 * a\n            coeff = c_part[0]\n            c_part = [Add(*[coeff*f for f in c_part[1].args])]\n\n        return c_part, nc_part, order_symbols\n\n    def _eval_power(self, e):\n\n        # don't break up NC terms: (A*B)**3 != A**3*B**3, it is A*B*A*B*A*B\n        cargs, nc = self.args_cnc(split_1=False)\n\n        if e.is_Integer:\n            return Mul(*[Pow(b, e, evaluate=False) for b in cargs]) * \\\n                Pow(Mul._from_args(nc), e, evaluate=False)\n        if e.is_Rational and e.q == 2:\n            from sympy.core.power import integer_nthroot\n            from sympy.functions.elementary.complexes import sign\n            if self.is_imaginary:\n                a = self.as_real_imag()[1]\n                if a.is_Rational:\n                    n, d = abs(a/2).as_numer_denom()\n                    n, t = integer_nthroot(n, 2)\n                    if t:\n                        d, t = integer_nthroot(d, 2)\n                        if t:\n                            r = sympify(n)/d\n                            return _unevaluated_Mul(r**e.p, (1 + sign(a)*S.ImaginaryUnit)**e.p)\n\n        p = Pow(self, e, evaluate=False)\n\n        if e.is_Rational or e.is_Float:\n            return p._eval_expand_power_base()\n\n        return p\n\n    @classmethod\n    def class_key(cls):\n        return 3, 0, cls.__name__\n\n    def _eval_evalf(self, prec):\n        c, m = self.as_coeff_Mul()\n        if c is S.NegativeOne:\n            if m.is_Mul:\n                rv = -AssocOp._eval_evalf(m, prec)\n            else:\n                mnew = m._eval_evalf(prec)\n                if mnew is not None:\n                    m = mnew\n                rv = -m\n        else:\n            rv = AssocOp._eval_evalf(self, prec)\n        if rv.is_number:\n            return rv.expand()\n        return rv\n\n    @property\n    def _mpc_(self):\n        \"\"\"\n        Convert self to an mpmath mpc if possible\n        \"\"\"\n        from sympy.core.numbers import I, Float\n        im_part, imag_unit = self.as_coeff_Mul()\n        if not imag_unit == I:\n            # ValueError may seem more reasonable but since it's a @property,\n            # we need to use AttributeError to keep from confusing things like\n            # hasattr.\n            raise AttributeError(\"Cannot convert Mul to mpc. Must be of the form Number*I\")\n\n        return (Float(0)._mpf_, Float(im_part)._mpf_)\n\n    @cacheit\n    def as_two_terms(self):\n        \"\"\"Return head and tail of self.\n\n        This is the most efficient way to get the head and tail of an\n        expression.\n\n        - if you want only the head, use self.args[0];\n        - if you want to process the arguments of the tail then use\n          self.as_coef_mul() which gives the head and a tuple containing\n          the arguments of the tail when treated as a Mul.\n        - if you want the coefficient when self is treated as an Add\n          then use self.as_coeff_add()[0]\n\n        Examples\n        ========\n\n        >>> from sympy.abc import x, y\n        >>> (3*x*y).as_two_terms()\n        (3, x*y)\n        \"\"\"\n        args = self.args\n\n        if len(args) == 1:\n            return S.One, self\n        elif len(args) == 2:\n            return args\n\n        else:\n            return args[0], self._new_rawargs(*args[1:])\n\n    @cacheit\n    def as_coefficients_dict(self):\n        \"\"\"Return a dictionary mapping terms to their coefficient.\n        Since the dictionary is a defaultdict, inquiries about terms which\n        were not present will return a coefficient of 0. The dictionary\n        is considered to have a single term.\n\n        Examples\n        ========\n\n        >>> from sympy.abc import a, x\n        >>> (3*a*x).as_coefficients_dict()\n        {a*x: 3}\n        >>> _[a]\n        0\n        \"\"\"\n\n        d = defaultdict(int)\n        args = self.args\n\n        if len(args) == 1 or not args[0].is_Number:\n            d[self] = S.One\n        else:\n            d[self._new_rawargs(*args[1:])] = args[0]\n\n        return d\n\n    @cacheit\n    def as_coeff_mul(self, *deps, rational=True, **kwargs):\n        if deps:\n            from sympy.utilities.iterables import sift\n            l1, l2 = sift(self.args, lambda x: x.has(*deps), binary=True)\n            return self._new_rawargs(*l2), tuple(l1)\n        args = self.args\n        if args[0].is_Number:\n            if not rational or args[0].is_Rational:\n                return args[0], args[1:]\n            elif args[0].is_extended_negative:\n                return S.NegativeOne, (-args[0],) + args[1:]\n        return S.One, args\n\n    def as_coeff_Mul(self, rational=False):\n        \"\"\"\n        Efficiently extract the coefficient of a product.\n        \"\"\"\n        coeff, args = self.args[0], self.args[1:]\n\n        if coeff.is_Number:\n            if not rational or coeff.is_Rational:\n                if len(args) == 1:\n                    return coeff, args[0]\n                else:\n                    return coeff, self._new_rawargs(*args)\n            elif coeff.is_extended_negative:\n                return S.NegativeOne, self._new_rawargs(*((-coeff,) + args))\n        return S.One, self\n\n    def as_real_imag(self, deep=True, **hints):\n        from sympy import Abs, expand_mul, im, re\n        other = []\n        coeffr = []\n        coeffi = []\n        addterms = S.One\n        for a in self.args:\n            r, i = a.as_real_imag()\n            if i.is_zero:\n                coeffr.append(r)\n            elif r.is_zero:\n                coeffi.append(i*S.ImaginaryUnit)\n            elif a.is_commutative:\n                # search for complex conjugate pairs:\n                for i, x in enumerate(other):\n                    if x == a.conjugate():\n                        coeffr.append(Abs(x)**2)\n                        del other[i]\n                        break\n                else:\n                    if a.is_Add:\n                        addterms *= a\n                    else:\n                        other.append(a)\n            else:\n                other.append(a)\n        m = self.func(*other)\n        if hints.get('ignore') == m:\n            return\n        if len(coeffi) % 2:\n            imco = im(coeffi.pop(0))\n            # all other pairs make a real factor; they will be\n            # put into reco below\n        else:\n            imco = S.Zero\n        reco = self.func(*(coeffr + coeffi))\n        r, i = (reco*re(m), reco*im(m))\n        if addterms == 1:\n            if m == 1:\n                if imco.is_zero:\n                    return (reco, S.Zero)\n                else:\n                    return (S.Zero, reco*imco)\n            if imco is S.Zero:\n                return (r, i)\n            return (-imco*i, imco*r)\n        addre, addim = expand_mul(addterms, deep=False).as_real_imag()\n        if imco is S.Zero:\n            return (r*addre - i*addim, i*addre + r*addim)\n        else:\n            r, i = -imco*i, imco*r\n            return (r*addre - i*addim, r*addim + i*addre)\n\n    @staticmethod\n    def _expandsums(sums):\n        \"\"\"\n        Helper function for _eval_expand_mul.\n\n        sums must be a list of instances of Basic.\n        \"\"\"\n\n        L = len(sums)\n        if L == 1:\n            return sums[0].args\n        terms = []\n        left = Mul._expandsums(sums[:L//2])\n        right = Mul._expandsums(sums[L//2:])\n\n        terms = [Mul(a, b) for a in left for b in right]\n        added = Add(*terms)\n        return Add.make_args(added)  # it may have collapsed down to one term\n\n    def _eval_expand_mul(self, **hints):\n        from sympy import fraction\n\n        # Handle things like 1/(x*(x + 1)), which are automatically converted\n        # to 1/x*1/(x + 1)\n        expr = self\n        n, d = fraction(expr)\n        if d.is_Mul:\n            n, d = [i._eval_expand_mul(**hints) if i.is_Mul else i\n                for i in (n, d)]\n            expr = n/d\n            if not expr.is_Mul:\n                return expr\n\n        plain, sums, rewrite = [], [], False\n        for factor in expr.args:\n            if factor.is_Add:\n                sums.append(factor)\n                rewrite = True\n            else:\n                if factor.is_commutative:\n                    plain.append(factor)\n                else:\n                    sums.append(Basic(factor))  # Wrapper\n\n        if not rewrite:\n            return expr\n        else:\n            plain = self.func(*plain)\n            if sums:\n                deep = hints.get(\"deep\", False)\n                terms = self.func._expandsums(sums)\n                args = []\n                for term in terms:\n                    t = self.func(plain, term)\n                    if t.is_Mul and any(a.is_Add for a in t.args) and deep:\n                        t = t._eval_expand_mul()\n                    args.append(t)\n                return Add(*args)\n            else:\n                return plain\n\n    @cacheit\n    def _eval_derivative(self, s):\n        args = list(self.args)\n        terms = []\n        for i in range(len(args)):\n            d = args[i].diff(s)\n            if d:\n                # Note: reduce is used in step of Mul as Mul is unable to\n                # handle subtypes and operation priority:\n                terms.append(reduce(lambda x, y: x*y, (args[:i] + [d] + args[i + 1:]), S.One))\n        return Add.fromiter(terms)\n\n    @cacheit\n    def _eval_derivative_n_times(self, s, n):\n        from sympy import Integer, factorial, prod, Sum, Max\n        from sympy.ntheory.multinomial import multinomial_coefficients_iterator\n        from .function import AppliedUndef\n        from .symbol import Symbol, symbols, Dummy\n        if not isinstance(s, AppliedUndef) and not isinstance(s, Symbol):\n            # other types of s may not be well behaved, e.g.\n            # (cos(x)*sin(y)).diff([[x, y, z]])\n            return super()._eval_derivative_n_times(s, n)\n        args = self.args\n        m = len(args)\n        if isinstance(n, (int, Integer)):\n            # https://en.wikipedia.org/wiki/General_Leibniz_rule#More_than_two_factors\n            terms = []\n            for kvals, c in multinomial_coefficients_iterator(m, n):\n                p = prod([arg.diff((s, k)) for k, arg in zip(kvals, args)])\n                terms.append(c * p)\n            return Add(*terms)\n        kvals = symbols(\"k1:%i\" % m, cls=Dummy)\n        klast = n - sum(kvals)\n        nfact = factorial(n)\n        e, l = (# better to use the multinomial?\n            nfact/prod(map(factorial, kvals))/factorial(klast)*\\\n            prod([args[t].diff((s, kvals[t])) for t in range(m-1)])*\\\n            args[-1].diff((s, Max(0, klast))),\n            [(k, 0, n) for k in kvals])\n        return Sum(e, *l)\n\n    def _eval_difference_delta(self, n, step):\n        from sympy.series.limitseq import difference_delta as dd\n        arg0 = self.args[0]\n        rest = Mul(*self.args[1:])\n        return (arg0.subs(n, n + step) * dd(rest, n, step) + dd(arg0, n, step) *\n                rest)\n\n    def _matches_simple(self, expr, repl_dict):\n        # handle (w*3).matches('x*5') -> {w: x*5/3}\n        coeff, terms = self.as_coeff_Mul()\n        terms = Mul.make_args(terms)\n        if len(terms) == 1:\n            newexpr = self.__class__._combine_inverse(expr, coeff)\n            return terms[0].matches(newexpr, repl_dict)\n        return\n\n    def matches(self, expr, repl_dict={}, old=False):\n        expr = sympify(expr)\n        repl_dict = repl_dict.copy()\n        if self.is_commutative and expr.is_commutative:\n            return self._matches_commutative(expr, repl_dict, old)\n        elif self.is_commutative is not expr.is_commutative:\n            return None\n\n        # Proceed only if both both expressions are non-commutative\n        c1, nc1 = self.args_cnc()\n        c2, nc2 = expr.args_cnc()\n        c1, c2 = [c or [1] for c in [c1, c2]]\n\n        # TODO: Should these be self.func?\n        comm_mul_self = Mul(*c1)\n        comm_mul_expr = Mul(*c2)\n\n        repl_dict = comm_mul_self.matches(comm_mul_expr, repl_dict, old)\n\n        # If the commutative arguments didn't match and aren't equal, then\n        # then the expression as a whole doesn't match\n        if repl_dict is None and c1 != c2:\n            return None\n\n        # Now match the non-commutative arguments, expanding powers to\n        # multiplications\n        nc1 = Mul._matches_expand_pows(nc1)\n        nc2 = Mul._matches_expand_pows(nc2)\n\n        repl_dict = Mul._matches_noncomm(nc1, nc2, repl_dict)\n\n        return repl_dict or None\n\n    @staticmethod\n    def _matches_expand_pows(arg_list):\n        new_args = []\n        for arg in arg_list:\n            if arg.is_Pow and arg.exp > 0:\n                new_args.extend([arg.base] * arg.exp)\n            else:\n                new_args.append(arg)\n        return new_args\n\n    @staticmethod\n    def _matches_noncomm(nodes, targets, repl_dict={}):\n        \"\"\"Non-commutative multiplication matcher.\n\n        `nodes` is a list of symbols within the matcher multiplication\n        expression, while `targets` is a list of arguments in the\n        multiplication expression being matched against.\n        \"\"\"\n        repl_dict = repl_dict.copy()\n        # List of possible future states to be considered\n        agenda = []\n        # The current matching state, storing index in nodes and targets\n        state = (0, 0)\n        node_ind, target_ind = state\n        # Mapping between wildcard indices and the index ranges they match\n        wildcard_dict = {}\n        repl_dict = repl_dict.copy()\n\n        while target_ind < len(targets) and node_ind < len(nodes):\n            node = nodes[node_ind]\n\n            if node.is_Wild:\n                Mul._matches_add_wildcard(wildcard_dict, state)\n\n            states_matches = Mul._matches_new_states(wildcard_dict, state,\n                                                     nodes, targets)\n            if states_matches:\n                new_states, new_matches = states_matches\n                agenda.extend(new_states)\n                if new_matches:\n                    for match in new_matches:\n                        repl_dict[match] = new_matches[match]\n            if not agenda:\n                return None\n            else:\n                state = agenda.pop()\n                node_ind, target_ind = state\n\n        return repl_dict\n\n    @staticmethod\n    def _matches_add_wildcard(dictionary, state):\n        node_ind, target_ind = state\n        if node_ind in dictionary:\n            begin, end = dictionary[node_ind]\n            dictionary[node_ind] = (begin, target_ind)\n        else:\n            dictionary[node_ind] = (target_ind, target_ind)\n\n    @staticmethod\n    def _matches_new_states(dictionary, state, nodes, targets):\n        node_ind, target_ind = state\n        node = nodes[node_ind]\n        target = targets[target_ind]\n\n        # Don't advance at all if we've exhausted the targets but not the nodes\n        if target_ind >= len(targets) - 1 and node_ind < len(nodes) - 1:\n            return None\n\n        if node.is_Wild:\n            match_attempt = Mul._matches_match_wilds(dictionary, node_ind,\n                                                     nodes, targets)\n            if match_attempt:\n                # If the same node has been matched before, don't return\n                # anything if the current match is diverging from the previous\n                # match\n                other_node_inds = Mul._matches_get_other_nodes(dictionary,\n                                                               nodes, node_ind)\n                for ind in other_node_inds:\n                    other_begin, other_end = dictionary[ind]\n                    curr_begin, curr_end = dictionary[node_ind]\n\n                    other_targets = targets[other_begin:other_end + 1]\n                    current_targets = targets[curr_begin:curr_end + 1]\n\n                    for curr, other in zip(current_targets, other_targets):\n                        if curr != other:\n                            return None\n\n                # A wildcard node can match more than one target, so only the\n                # target index is advanced\n                new_state = [(node_ind, target_ind + 1)]\n                # Only move on to the next node if there is one\n                if node_ind < len(nodes) - 1:\n                    new_state.append((node_ind + 1, target_ind + 1))\n                return new_state, match_attempt\n        else:\n            # If we're not at a wildcard, then make sure we haven't exhausted\n            # nodes but not targets, since in this case one node can only match\n            # one target\n            if node_ind >= len(nodes) - 1 and target_ind < len(targets) - 1:\n                return None\n\n            match_attempt = node.matches(target)\n\n            if match_attempt:\n                return [(node_ind + 1, target_ind + 1)], match_attempt\n            elif node == target:\n                return [(node_ind + 1, target_ind + 1)], None\n            else:\n                return None\n\n    @staticmethod\n    def _matches_match_wilds(dictionary, wildcard_ind, nodes, targets):\n        \"\"\"Determine matches of a wildcard with sub-expression in `target`.\"\"\"\n        wildcard = nodes[wildcard_ind]\n        begin, end = dictionary[wildcard_ind]\n        terms = targets[begin:end + 1]\n        # TODO: Should this be self.func?\n        mul = Mul(*terms) if len(terms) > 1 else terms[0]\n        return wildcard.matches(mul)\n\n    @staticmethod\n    def _matches_get_other_nodes(dictionary, nodes, node_ind):\n        \"\"\"Find other wildcards that may have already been matched.\"\"\"\n        other_node_inds = []\n        for ind in dictionary:\n            if nodes[ind] == nodes[node_ind]:\n                other_node_inds.append(ind)\n        return other_node_inds\n\n    @staticmethod\n    def _combine_inverse(lhs, rhs):\n        \"\"\"\n        Returns lhs/rhs, but treats arguments like symbols, so things\n        like oo/oo return 1 (instead of a nan) and ``I`` behaves like\n        a symbol instead of sqrt(-1).\n        \"\"\"\n        from .symbol import Dummy\n        if lhs == rhs:\n            return S.One\n\n        def check(l, r):\n            if l.is_Float and r.is_comparable:\n                # if both objects are added to 0 they will share the same \"normalization\"\n                # and are more likely to compare the same. Since Add(foo, 0) will not allow\n                # the 0 to pass, we use __add__ directly.\n                return l.__add__(0) == r.evalf().__add__(0)\n            return False\n        if check(lhs, rhs) or check(rhs, lhs):\n            return S.One\n        if any(i.is_Pow or i.is_Mul for i in (lhs, rhs)):\n            # gruntz and limit wants a literal I to not combine\n            # with a power of -1\n            d = Dummy('I')\n            _i = {S.ImaginaryUnit: d}\n            i_ = {d: S.ImaginaryUnit}\n            a = lhs.xreplace(_i).as_powers_dict()\n            b = rhs.xreplace(_i).as_powers_dict()\n            blen = len(b)\n            for bi in tuple(b.keys()):\n                if bi in a:\n                    a[bi] -= b.pop(bi)\n                    if not a[bi]:\n                        a.pop(bi)\n            if len(b) != blen:\n                lhs = Mul(*[k**v for k, v in a.items()]).xreplace(i_)\n                rhs = Mul(*[k**v for k, v in b.items()]).xreplace(i_)\n        return lhs/rhs\n\n    def as_powers_dict(self):\n        d = defaultdict(int)\n        for term in self.args:\n            for b, e in term.as_powers_dict().items():\n                d[b] += e\n        return d\n\n    def as_numer_denom(self):\n        # don't use _from_args to rebuild the numerators and denominators\n        # as the order is not guaranteed to be the same once they have\n        # been separated from each other\n        numers, denoms = list(zip(*[f.as_numer_denom() for f in self.args]))\n        return self.func(*numers), self.func(*denoms)\n\n    def as_base_exp(self):\n        e1 = None\n        bases = []\n        nc = 0\n        for m in self.args:\n            b, e = m.as_base_exp()\n            if not b.is_commutative:\n                nc += 1\n            if e1 is None:\n                e1 = e\n            elif e != e1 or nc > 1:\n                return self, S.One\n            bases.append(b)\n        return self.func(*bases), e1\n\n    def _eval_is_polynomial(self, syms):\n        return all(term._eval_is_polynomial(syms) for term in self.args)\n\n    def _eval_is_rational_function(self, syms):\n        return all(term._eval_is_rational_function(syms) for term in self.args)\n\n    def _eval_is_meromorphic(self, x, a):\n        return _fuzzy_group((arg.is_meromorphic(x, a) for arg in self.args),\n                            quick_exit=True)\n\n    def _eval_is_algebraic_expr(self, syms):\n        return all(term._eval_is_algebraic_expr(syms) for term in self.args)\n\n    _eval_is_commutative = lambda self: _fuzzy_group(\n        a.is_commutative for a in self.args)\n\n    def _eval_is_complex(self):\n        comp = _fuzzy_group(a.is_complex for a in self.args)\n        if comp is False:\n            if any(a.is_infinite for a in self.args):\n                if any(a.is_zero is not False for a in self.args):\n                    return None\n                return False\n        return comp\n\n    def _eval_is_finite(self):\n        if all(a.is_finite for a in self.args):\n            return True\n        if any(a.is_infinite for a in self.args):\n            if all(a.is_zero is False for a in self.args):\n                return False\n\n    def _eval_is_infinite(self):\n        if any(a.is_infinite for a in self.args):\n            if any(a.is_zero for a in self.args):\n                return S.NaN.is_infinite\n            if any(a.is_zero is None for a in self.args):\n                return None\n            return True\n\n    def _eval_is_rational(self):\n        r = _fuzzy_group((a.is_rational for a in self.args), quick_exit=True)\n        if r:\n            return r\n        elif r is False:\n            return self.is_zero\n\n    def _eval_is_algebraic(self):\n        r = _fuzzy_group((a.is_algebraic for a in self.args), quick_exit=True)\n        if r:\n            return r\n        elif r is False:\n            return self.is_zero\n\n    def _eval_is_zero(self):\n        zero = infinite = False\n        for a in self.args:\n            z = a.is_zero\n            if z:\n                if infinite:\n                    return  # 0*oo is nan and nan.is_zero is None\n                zero = True\n            else:\n                if not a.is_finite:\n                    if zero:\n                        return  # 0*oo is nan and nan.is_zero is None\n                    infinite = True\n                if zero is False and z is None:  # trap None\n                    zero = None\n        return zero\n\n    def _eval_is_integer(self):\n        from sympy import fraction\n        from sympy.core.numbers import Float\n\n        is_rational = self._eval_is_rational()\n        if is_rational is False:\n            return False\n\n        # use exact=True to avoid recomputing num or den\n        n, d = fraction(self, exact=True)\n        if is_rational:\n            if d is S.One:\n                return True\n        if d.is_even:\n            if d.is_prime:  # literal or symbolic 2\n                return n.is_even\n            if n.is_odd:\n                return False  # true even if d = 0\n        if n == d:\n            return fuzzy_and([not bool(self.atoms(Float)),\n            fuzzy_not(d.is_zero)])\n\n    def _eval_is_polar(self):\n        has_polar = any(arg.is_polar for arg in self.args)\n        return has_polar and \\\n            all(arg.is_polar or arg.is_positive for arg in self.args)\n\n    def _eval_is_extended_real(self):\n        return self._eval_real_imag(True)\n\n    def _eval_real_imag(self, real):\n        zero = False\n        t_not_re_im = None\n\n        for t in self.args:\n            if (t.is_complex or t.is_infinite) is False and t.is_extended_real is False:\n                return False\n            elif t.is_imaginary:  # I\n                real = not real\n            elif t.is_extended_real:  # 2\n                if not zero:\n                    z = t.is_zero\n                    if not z and zero is False:\n                        zero = z\n                    elif z:\n                        if all(a.is_finite for a in self.args):\n                            return True\n                        return\n            elif t.is_extended_real is False:\n                # symbolic or literal like `2 + I` or symbolic imaginary\n                if t_not_re_im:\n                    return  # complex terms might cancel\n                t_not_re_im = t\n            elif t.is_imaginary is False:  # symbolic like `2` or `2 + I`\n                if t_not_re_im:\n                    return  # complex terms might cancel\n                t_not_re_im = t\n            else:\n                return\n\n        if t_not_re_im:\n            if t_not_re_im.is_extended_real is False:\n                if real:  # like 3\n                    return zero  # 3*(smthng like 2 + I or i) is not real\n            if t_not_re_im.is_imaginary is False:  # symbolic 2 or 2 + I\n                if not real:  # like I\n                    return zero  # I*(smthng like 2 or 2 + I) is not real\n        elif zero is False:\n            return real  # can't be trumped by 0\n        elif real:\n            return real  # doesn't matter what zero is\n\n    def _eval_is_imaginary(self):\n        z = self.is_zero\n        if z:\n            return False\n        if self.is_finite is False:\n            return False\n        elif z is False and self.is_finite is True:\n            return self._eval_real_imag(False)\n\n    def _eval_is_hermitian(self):\n        return self._eval_herm_antiherm(True)\n\n    def _eval_herm_antiherm(self, real):\n        one_nc = zero = one_neither = False\n\n        for t in self.args:\n            if not t.is_commutative:\n                if one_nc:\n                    return\n                one_nc = True\n\n            if t.is_antihermitian:\n                real = not real\n            elif t.is_hermitian:\n                if not zero:\n                    z = t.is_zero\n                    if not z and zero is False:\n                        zero = z\n                    elif z:\n                        if all(a.is_finite for a in self.args):\n                            return True\n                        return\n            elif t.is_hermitian is False:\n                if one_neither:\n                    return\n                one_neither = True\n            else:\n                return\n\n        if one_neither:\n            if real:\n                return zero\n        elif zero is False or real:\n            return real\n\n    def _eval_is_antihermitian(self):\n        z = self.is_zero\n        if z:\n            return False\n        elif z is False:\n            return self._eval_herm_antiherm(False)\n\n    def _eval_is_irrational(self):\n        for t in self.args:\n            a = t.is_irrational\n            if a:\n                others = list(self.args)\n                others.remove(t)\n                if all((x.is_rational and fuzzy_not(x.is_zero)) is True for x in others):\n                    return True\n                return\n            if a is None:\n                return\n        if all(x.is_real for x in self.args):\n            return False\n\n    def _eval_is_extended_positive(self):\n        \"\"\"Return True if self is positive, False if not, and None if it\n        cannot be determined.\n\n        Explanation\n        ===========\n\n        This algorithm is non-recursive and works by keeping track of the\n        sign which changes when a negative or nonpositive is encountered.\n        Whether a nonpositive or nonnegative is seen is also tracked since\n        the presence of these makes it impossible to return True, but\n        possible to return False if the end result is nonpositive. e.g.\n\n            pos * neg * nonpositive -> pos or zero -> None is returned\n            pos * neg * nonnegative -> neg or zero -> False is returned\n        \"\"\"\n        return self._eval_pos_neg(1)\n\n    def _eval_pos_neg(self, sign):\n        saw_NON = saw_NOT = False\n        for t in self.args:\n            if t.is_extended_positive:\n                continue\n            elif t.is_extended_negative:\n                sign = -sign\n            elif t.is_zero:\n                if all(a.is_finite for a in self.args):\n                    return False\n                return\n            elif t.is_extended_nonpositive:\n                sign = -sign\n                saw_NON = True\n            elif t.is_extended_nonnegative:\n                saw_NON = True\n            # FIXME: is_positive/is_negative is False doesn't take account of\n            # Symbol('x', infinite=True, extended_real=True) which has\n            # e.g. is_positive is False but has uncertain sign.\n            elif t.is_positive is False:\n                sign = -sign\n                if saw_NOT:\n                    return\n                saw_NOT = True\n            elif t.is_negative is False:\n                if saw_NOT:\n                    return\n                saw_NOT = True\n            else:\n                return\n        if sign == 1 and saw_NON is False and saw_NOT is False:\n            return True\n        if sign < 0:\n            return False\n\n    def _eval_is_extended_negative(self):\n        return self._eval_pos_neg(-1)\n\n    def _eval_is_odd(self):\n        is_integer = self.is_integer\n\n        if is_integer:\n            r, acc = True, 1\n            for t in self.args:\n                if not t.is_integer:\n                    return None\n                elif t.is_even:\n                    r = False\n                elif t.is_integer:\n                    if r is False:\n                        pass\n                    elif acc != 1 and (acc + t).is_odd:\n                        r = False\n                    elif t.is_odd is None:\n                        r = None\n                acc = t\n            return r\n\n        # !integer -> !odd\n        elif is_integer is False:\n            return False\n\n    def _eval_is_even(self):\n        is_integer = self.is_integer\n\n        if is_integer:\n            return fuzzy_not(self.is_odd)\n\n        elif is_integer is False:\n            return False\n\n    def _eval_is_composite(self):\n        \"\"\"\n        Here we count the number of arguments that have a minimum value\n        greater than two.\n        If there are more than one of such a symbol then the result is composite.\n        Else, the result cannot be determined.\n        \"\"\"\n        number_of_args = 0 # count of symbols with minimum value greater than one\n        for arg in self.args:\n            if not (arg.is_integer and arg.is_positive):\n                return None\n            if (arg-1).is_positive:\n                number_of_args += 1\n\n        if number_of_args > 1:\n            return True\n\n    def _eval_subs(self, old, new):\n        from sympy.functions.elementary.complexes import sign\n        from sympy.ntheory.factor_ import multiplicity\n        from sympy.simplify.powsimp import powdenest\n        from sympy.simplify.radsimp import fraction\n\n        if not old.is_Mul:\n            return None\n\n        # try keep replacement literal so -2*x doesn't replace 4*x\n        if old.args[0].is_Number and old.args[0] < 0:\n            if self.args[0].is_Number:\n                if self.args[0] < 0:\n                    return self._subs(-old, -new)\n                return None\n\n        def base_exp(a):\n            # if I and -1 are in a Mul, they get both end up with\n            # a -1 base (see issue 6421); all we want here are the\n            # true Pow or exp separated into base and exponent\n            from sympy import exp\n            if a.is_Pow or isinstance(a, exp):\n                return a.as_base_exp()\n            return a, S.One\n\n        def breakup(eq):\n            \"\"\"break up powers of eq when treated as a Mul:\n                   b**(Rational*e) -> b**e, Rational\n                commutatives come back as a dictionary {b**e: Rational}\n                noncommutatives come back as a list [(b**e, Rational)]\n            \"\"\"\n\n            (c, nc) = (defaultdict(int), list())\n            for a in Mul.make_args(eq):\n                a = powdenest(a)\n                (b, e) = base_exp(a)\n                if e is not S.One:\n                    (co, _) = e.as_coeff_mul()\n                    b = Pow(b, e/co)\n                    e = co\n                if a.is_commutative:\n                    c[b] += e\n                else:\n                    nc.append([b, e])\n            return (c, nc)\n\n        def rejoin(b, co):\n            \"\"\"\n            Put rational back with exponent; in general this is not ok, but\n            since we took it from the exponent for analysis, it's ok to put\n            it back.\n            \"\"\"\n\n            (b, e) = base_exp(b)\n            return Pow(b, e*co)\n\n        def ndiv(a, b):\n            \"\"\"if b divides a in an extractive way (like 1/4 divides 1/2\n            but not vice versa, and 2/5 does not divide 1/3) then return\n            the integer number of times it divides, else return 0.\n            \"\"\"\n            if not b.q % a.q or not a.q % b.q:\n                return int(a/b)\n            return 0\n\n        # give Muls in the denominator a chance to be changed (see issue 5651)\n        # rv will be the default return value\n        rv = None\n        n, d = fraction(self)\n        self2 = self\n        if d is not S.One:\n            self2 = n._subs(old, new)/d._subs(old, new)\n            if not self2.is_Mul:\n                return self2._subs(old, new)\n            if self2 != self:\n                rv = self2\n\n        # Now continue with regular substitution.\n\n        # handle the leading coefficient and use it to decide if anything\n        # should even be started; we always know where to find the Rational\n        # so it's a quick test\n\n        co_self = self2.args[0]\n        co_old = old.args[0]\n        co_xmul = None\n        if co_old.is_Rational and co_self.is_Rational:\n            # if coeffs are the same there will be no updating to do\n            # below after breakup() step; so skip (and keep co_xmul=None)\n            if co_old != co_self:\n                co_xmul = co_self.extract_multiplicatively(co_old)\n        elif co_old.is_Rational:\n            return rv\n\n        # break self and old into factors\n\n        (c, nc) = breakup(self2)\n        (old_c, old_nc) = breakup(old)\n\n        # update the coefficients if we had an extraction\n        # e.g. if co_self were 2*(3/35*x)**2 and co_old = 3/5\n        # then co_self in c is replaced by (3/5)**2 and co_residual\n        # is 2*(1/7)**2\n\n        if co_xmul and co_xmul.is_Rational and abs(co_old) != 1:\n            mult = S(multiplicity(abs(co_old), co_self))\n            c.pop(co_self)\n            if co_old in c:\n                c[co_old] += mult\n            else:\n                c[co_old] = mult\n            co_residual = co_self/co_old**mult\n        else:\n            co_residual = 1\n\n        # do quick tests to see if we can't succeed\n\n        ok = True\n        if len(old_nc) > len(nc):\n            # more non-commutative terms\n            ok = False\n        elif len(old_c) > len(c):\n            # more commutative terms\n            ok = False\n        elif {i[0] for i in old_nc}.difference({i[0] for i in nc}):\n            # unmatched non-commutative bases", "answer": "A"}
{"uuid": "ca2be9ef-4835-4422-9631-dd1805ac65a1", "issue_statement": "convert_to seems to combine orthogonal units\nTested in sympy 1.4, not presently in a position to install 1.5+.\r\nSimple example. Consider `J = kg*m**2/s**2 => J*s = kg*m**2/s`. The convert_to behavior is odd:\r\n```\r\n>>>convert_to(joule*second,joule)\r\n    joule**(7/9)\r\n```\r\nI would expect the unchanged original expression back, an expression in terms of base units, or an error. It appears that convert_to can only readily handle conversions where the full unit expression is valid.\r\n\r\nNote that the following three related examples give sensible results:\r\n```\r\n>>>convert_to(joule*second,joule*second)\r\n    joule*second\r\n```\r\n```\r\n>>>convert_to(J*s, kg*m**2/s)\r\n    kg*m**2/s\r\n```\r\n```\r\n>>>convert_to(J*s,mins)\r\n    J*mins/60\r\n```", "choices": "(A) \nfrom sympy import Add, Mul, Pow, Tuple, sympify\nfrom sympy.core.compatibility import reduce, Iterable, ordered\nfrom sympy.physics.units.dimensions import Dimension\nfrom sympy.physics.units.prefixes import Prefix\nfrom sympy.physics.units.quantities import Quantity\nfrom sympy.utilities.iterables import sift\n\n\ndef _get_conversion_matrix_for_expr(expr, target_units, unit_system):\n    from sympy import Matrix\n\n    dimension_system = unit_system.get_dimension_system()\n\n    expr_dim = Dimension(unit_system.get_dimensional_expr(expr))\n    dim_dependencies = dimension_system.get_dimensional_dependencies(expr_dim, mark_dimensionless=True)\n    target_dims = [Dimension(unit_system.get_dimensional_expr(x)) for x in target_units]\n    canon_dim_units = [i for x in target_dims for i in dimension_system.get_dimensional_dependencies(x, mark_dimensionless=True)]\n    canon_expr_units = {i for i in dim_dependencies}\n\n    if not canon_expr_units.issubset(set(canon_dim_units)):\n        return None\n\n    seen = set()\n    canon_dim_units = [i for i in canon_dim_units if not (i in seen or seen.add(i))]\n\n    camat = Matrix([[dimension_system.get_dimensional_dependencies(i, mark_dimensionless=True).get(j, 0) for i in target_dims] for j in canon_dim_units])\n    exprmat = Matrix([dim_dependencies.get(k, 0) for k in canon_dim_units])\n\n    res_exponents = camat.solve_least_squares(exprmat, method=None)\n    return res_exponents\n\n\ndef convert_to(expr, target_units, unit_system=\"SI\"):\n    \"\"\"\n    Convert ``expr`` to the same expression with all of its units and quantities\n    represented as factors of ``target_units``, whenever the dimension is compatible.\n\n(B)     res_exponents = camat.solve_least_squares(exprmat, method=None)\n    return res_exponents\n\n\ndef convert_to(expr, target_units, unit_system=\"SI\"):\n    \"\"\"\n    Convert ``expr`` to the same expression with all of its units and quantities\n    represented as factors of ``target_units``, whenever the dimension is compatible.\n\n    ``target_units`` may be a single unit/quantity, or a collection of\n    units/quantities.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.units import speed_of_light, meter, gram, second, day\n    >>> from sympy.physics.units import mile, newton, kilogram, atomic_mass_constant\n    >>> from sympy.physics.units import kilometer, centimeter\n    >>> from sympy.physics.units import gravitational_constant, hbar\n    >>> from sympy.physics.units import convert_to\n    >>> convert_to(mile, kilometer)\n    25146*kilometer/15625\n    >>> convert_to(mile, kilometer).n()\n    1.609344*kilometer\n    >>> convert_to(speed_of_light, meter/second)\n    299792458*meter/second\n    >>> convert_to(day, second)\n    86400*second\n    >>> 3*newton\n    3*newton\n    >>> convert_to(3*newton, kilogram*meter/second**2)\n    3*kilogram*meter/second**2\n    >>> convert_to(atomic_mass_constant, gram)\n    1.660539060e-24*gram\n\n    Conversion to multiple units:\n\n    >>> convert_to(speed_of_light, [meter, second])\n(C) \n    if not canon_expr_units.issubset(set(canon_dim_units)):\n        return None\n\n    seen = set()\n    canon_dim_units = [i for i in canon_dim_units if not (i in seen or seen.add(i))]\n\n    camat = Matrix([[dimension_system.get_dimensional_dependencies(i, mark_dimensionless=True).get(j, 0) for i in target_dims] for j in canon_dim_units])\n    exprmat = Matrix([dim_dependencies.get(k, 0) for k in canon_dim_units])\n\n    res_exponents = camat.solve_least_squares(exprmat, method=None)\n    return res_exponents\n\n\ndef convert_to(expr, target_units, unit_system=\"SI\"):\n    \"\"\"\n    Convert ``expr`` to the same expression with all of its units and quantities\n    represented as factors of ``target_units``, whenever the dimension is compatible.\n\n    ``target_units`` may be a single unit/quantity, or a collection of\n    units/quantities.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.units import speed_of_light, meter, gram, second, day\n    >>> from sympy.physics.units import mile, newton, kilogram, atomic_mass_constant\n    >>> from sympy.physics.units import kilometer, centimeter\n    >>> from sympy.physics.units import gravitational_constant, hbar\n    >>> from sympy.physics.units import convert_to\n    >>> convert_to(mile, kilometer)\n    25146*kilometer/15625\n    >>> convert_to(mile, kilometer).n()\n    1.609344*kilometer\n    >>> convert_to(speed_of_light, meter/second)\n    299792458*meter/second\n    >>> convert_to(day, second)\n    86400*second\n(D)     from sympy import Matrix\n\n    dimension_system = unit_system.get_dimension_system()\n\n    expr_dim = Dimension(unit_system.get_dimensional_expr(expr))\n    dim_dependencies = dimension_system.get_dimensional_dependencies(expr_dim, mark_dimensionless=True)\n    target_dims = [Dimension(unit_system.get_dimensional_expr(x)) for x in target_units]\n    canon_dim_units = [i for x in target_dims for i in dimension_system.get_dimensional_dependencies(x, mark_dimensionless=True)]\n    canon_expr_units = {i for i in dim_dependencies}\n\n    if not canon_expr_units.issubset(set(canon_dim_units)):\n        return None\n\n    seen = set()\n    canon_dim_units = [i for i in canon_dim_units if not (i in seen or seen.add(i))]\n\n    camat = Matrix([[dimension_system.get_dimensional_dependencies(i, mark_dimensionless=True).get(j, 0) for i in target_dims] for j in canon_dim_units])\n    exprmat = Matrix([dim_dependencies.get(k, 0) for k in canon_dim_units])\n\n    res_exponents = camat.solve_least_squares(exprmat, method=None)\n    return res_exponents\n\n\ndef convert_to(expr, target_units, unit_system=\"SI\"):\n    \"\"\"\n    Convert ``expr`` to the same expression with all of its units and quantities\n    represented as factors of ``target_units``, whenever the dimension is compatible.\n\n    ``target_units`` may be a single unit/quantity, or a collection of\n    units/quantities.\n\n    Examples\n    ========\n\n    >>> from sympy.physics.units import speed_of_light, meter, gram, second, day\n    >>> from sympy.physics.units import mile, newton, kilogram, atomic_mass_constant\n    >>> from sympy.physics.units import kilometer, centimeter\n    >>> from sympy.physics.units import gravitational_constant, hbar", "answer": "A"}
{"uuid": "7a408d12-8270-4fdc-91c9-a090d148fcfc", "issue_statement": "Symbol instances have __dict__ since 1.7?\nIn version 1.6.2 Symbol instances had no `__dict__` attribute\r\n```python\r\n>>> sympy.Symbol('s').__dict__\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-e2060d5eec73> in <module>\r\n----> 1 sympy.Symbol('s').__dict__\r\n\r\nAttributeError: 'Symbol' object has no attribute '__dict__'\r\n>>> sympy.Symbol('s').__slots__\r\n('name',)\r\n```\r\n\r\nThis changes in 1.7 where `sympy.Symbol('s').__dict__` now exists (and returns an empty dict)\r\nI may misinterpret this, but given the purpose of `__slots__`, I assume this is a bug, introduced because some parent class accidentally stopped defining `__slots__`.", "choices": "(A)     # Note, we always use the default ordering (lex) in __str__ and __repr__,\n    # regardless of the global setting. See issue 5487.\n    def __str__(self):\n        from sympy.printing.str import sstr\n        return sstr(self, order=None)\n\n    __repr__ = __str__\n\n    def _repr_disabled(self):\n        \"\"\"\n        No-op repr function used to disable jupyter display hooks.\n(B)         from sympy.printing.str import sstr\n        return sstr(self, order=None)\n\n    __repr__ = __str__\n\n    def _repr_disabled(self):\n        \"\"\"\n        No-op repr function used to disable jupyter display hooks.\n\n        When :func:`sympy.init_printing` is used to disable certain display\n        formats, this function is copied into the appropriate ``_repr_*_``\n(C)     This also adds support for LaTeX printing in jupyter notebooks.\n    \"\"\"\n\n    # Note, we always use the default ordering (lex) in __str__ and __repr__,\n    # regardless of the global setting. See issue 5487.\n    def __str__(self):\n        from sympy.printing.str import sstr\n        return sstr(self, order=None)\n\n    __repr__ = __str__\n\n(D)     even if ``str()`` was explicitly requested. Mix in this trait into\n    a class to get proper default printing.\n\n    This also adds support for LaTeX printing in jupyter notebooks.\n    \"\"\"\n\n    # Note, we always use the default ordering (lex) in __str__ and __repr__,\n    # regardless of the global setting. See issue 5487.\n    def __str__(self):\n        from sympy.printing.str import sstr\n        return sstr(self, order=None)", "answer": "C"}
{"uuid": "2e994bc9-cfab-49b0-8516-ad958ea78446", "issue_statement": "inaccurate rendering of pi**(1/E)\nThis claims to be version 1.5.dev; I just merged from the project master, so I hope this is current.  I didn't notice this bug among others in printing.pretty.\r\n\r\n```\r\nIn [52]: pi**(1/E)                                                               \r\nOut[52]: \r\n-1___\r\n\u2572\u2571 \u03c0 \r\n\r\n```\r\nLaTeX and str not fooled:\r\n```\r\nIn [53]: print(latex(pi**(1/E)))                                                 \r\n\\pi^{e^{-1}}\r\n\r\nIn [54]: str(pi**(1/E))                                                          \r\nOut[54]: 'pi**exp(-1)'\r\n```", "choices": "(A)                 a[i] = self._print(a[i])\n\n        for i in range(0, len(b)):\n            if (b[i].is_Add and len(b) > 1) or (i != len(b) - 1 and\n                    isinstance(b[i], (Integral, Piecewise, Product, Sum))):\n                b[i] = prettyForm(*self._print(b[i]).parens())\n            else:\n                b[i] = self._print(b[i])\n\n        # Construct a pretty form\n        if len(b) == 0:\n            return prettyForm.__mul__(*a)\n        else:\n            if len(a) == 0:\n                a.append( self._print(S.One) )\n            return prettyForm.__mul__(*a)/prettyForm.__mul__(*b)\n\n    # A helper function for _print_Pow to print x**(1/n)\n    def _print_nth_root(self, base, expt):\n        bpretty = self._print(base)\n\n        # In very simple cases, use a single-char root sign\n        if (self._settings['use_unicode_sqrt_char'] and self._use_unicode\n            and expt is S.Half and bpretty.height() == 1\n            and (bpretty.width() == 1\n                 or (base.is_Integer and base.is_nonnegative))):\n            return prettyForm(*bpretty.left('\\N{SQUARE ROOT}'))\n\n        # Construct root sign, start with the \\/ shape\n        _zZ = xobj('/', 1)\n        rootsign = xobj('\\\\', 1) + _zZ\n        # Make exponent number to put above it\n        if isinstance(expt, Rational):\n            exp = str(expt.q)\n            if exp == '2':\n                exp = ''\n        else:\n            exp = str(expt.args[0])\n        exp = exp.ljust(2)\n        if len(exp) > 2:\n            rootsign = ' '*(len(exp) - 2) + rootsign\n        # Stack the exponent\n        rootsign = stringPict(exp + '\\n' + rootsign)\n        rootsign.baseline = 0\n        # Diagonal: length is one less than height of base\n        linelength = bpretty.height() - 1\n        diagonal = stringPict('\\n'.join(\n            ' '*(linelength - i - 1) + _zZ + ' '*i\n            for i in range(linelength)\n        ))\n        # Put baseline just below lowest line: next to exp\n        diagonal.baseline = linelength - 1\n        # Make the root symbol\n        rootsign = prettyForm(*rootsign.right(diagonal))\n        # Det the baseline to match contents to fix the height\n        # but if the height of bpretty is one, the rootsign must be one higher\n        rootsign.baseline = max(1, bpretty.baseline)\n        #build result\n        s = prettyForm(hobj('_', 2 + bpretty.width()))\n        s = prettyForm(*bpretty.above(s))\n(B)         linelength = bpretty.height() - 1\n        diagonal = stringPict('\\n'.join(\n            ' '*(linelength - i - 1) + _zZ + ' '*i\n            for i in range(linelength)\n        ))\n        # Put baseline just below lowest line: next to exp\n        diagonal.baseline = linelength - 1\n        # Make the root symbol\n        rootsign = prettyForm(*rootsign.right(diagonal))\n        # Det the baseline to match contents to fix the height\n        # but if the height of bpretty is one, the rootsign must be one higher\n        rootsign.baseline = max(1, bpretty.baseline)\n        #build result\n        s = prettyForm(hobj('_', 2 + bpretty.width()))\n        s = prettyForm(*bpretty.above(s))\n        s = prettyForm(*s.left(rootsign))\n        return s\n\n    def _print_Pow(self, power):\n        from sympy.simplify.simplify import fraction\n        b, e = power.as_base_exp()\n        if power.is_commutative:\n            if e is S.NegativeOne:\n                return prettyForm(\"1\")/self._print(b)\n            n, d = fraction(e)\n            if n is S.One and d.is_Atom and not e.is_Integer and self._settings['root_notation']:\n                return self._print_nth_root(b, e)\n            if e.is_Rational and e < 0:\n                return prettyForm(\"1\")/self._print(Pow(b, -e, evaluate=False))\n\n        if b.is_Relational:\n            return prettyForm(*self._print(b).parens()).__pow__(self._print(e))\n\n        return self._print(b)**self._print(e)\n\n    def _print_UnevaluatedExpr(self, expr):\n        return self._print(expr.args[0])\n\n    def __print_numer_denom(self, p, q):\n        if q == 1:\n            if p < 0:\n                return prettyForm(str(p), binding=prettyForm.NEG)\n            else:\n                return prettyForm(str(p))\n        elif abs(p) >= 10 and abs(q) >= 10:\n            # If more than one digit in numer and denom, print larger fraction\n            if p < 0:\n                return prettyForm(str(p), binding=prettyForm.NEG)/prettyForm(str(q))\n                # Old printing method:\n                #pform = prettyForm(str(-p))/prettyForm(str(q))\n                #return prettyForm(binding=prettyForm.NEG, *pform.left('- '))\n            else:\n                return prettyForm(str(p))/prettyForm(str(q))\n        else:\n            return None\n\n    def _print_Rational(self, expr):\n        result = self.__print_numer_denom(expr.p, expr.q)\n\n        if result is not None:\n(C)             return prettyForm.__mul__(*a)/prettyForm.__mul__(*b)\n\n    # A helper function for _print_Pow to print x**(1/n)\n    def _print_nth_root(self, base, expt):\n        bpretty = self._print(base)\n\n        # In very simple cases, use a single-char root sign\n        if (self._settings['use_unicode_sqrt_char'] and self._use_unicode\n            and expt is S.Half and bpretty.height() == 1\n            and (bpretty.width() == 1\n                 or (base.is_Integer and base.is_nonnegative))):\n            return prettyForm(*bpretty.left('\\N{SQUARE ROOT}'))\n\n        # Construct root sign, start with the \\/ shape\n        _zZ = xobj('/', 1)\n        rootsign = xobj('\\\\', 1) + _zZ\n        # Make exponent number to put above it\n        if isinstance(expt, Rational):\n            exp = str(expt.q)\n            if exp == '2':\n                exp = ''\n        else:\n            exp = str(expt.args[0])\n        exp = exp.ljust(2)\n        if len(exp) > 2:\n            rootsign = ' '*(len(exp) - 2) + rootsign\n        # Stack the exponent\n        rootsign = stringPict(exp + '\\n' + rootsign)\n        rootsign.baseline = 0\n        # Diagonal: length is one less than height of base\n        linelength = bpretty.height() - 1\n        diagonal = stringPict('\\n'.join(\n            ' '*(linelength - i - 1) + _zZ + ' '*i\n            for i in range(linelength)\n        ))\n        # Put baseline just below lowest line: next to exp\n        diagonal.baseline = linelength - 1\n        # Make the root symbol\n        rootsign = prettyForm(*rootsign.right(diagonal))\n        # Det the baseline to match contents to fix the height\n        # but if the height of bpretty is one, the rootsign must be one higher\n        rootsign.baseline = max(1, bpretty.baseline)\n        #build result\n        s = prettyForm(hobj('_', 2 + bpretty.width()))\n        s = prettyForm(*bpretty.above(s))\n        s = prettyForm(*s.left(rootsign))\n        return s\n\n    def _print_Pow(self, power):\n        from sympy.simplify.simplify import fraction\n        b, e = power.as_base_exp()\n        if power.is_commutative:\n            if e is S.NegativeOne:\n                return prettyForm(\"1\")/self._print(b)\n            n, d = fraction(e)\n            if n is S.One and d.is_Atom and not e.is_Integer and self._settings['root_notation']:\n                return self._print_nth_root(b, e)\n            if e.is_Rational and e < 0:\n                return prettyForm(\"1\")/self._print(Pow(b, -e, evaluate=False))\n\n(D)         rootsign = xobj('\\\\', 1) + _zZ\n        # Make exponent number to put above it\n        if isinstance(expt, Rational):\n            exp = str(expt.q)\n            if exp == '2':\n                exp = ''\n        else:\n            exp = str(expt.args[0])\n        exp = exp.ljust(2)\n        if len(exp) > 2:\n            rootsign = ' '*(len(exp) - 2) + rootsign\n        # Stack the exponent\n        rootsign = stringPict(exp + '\\n' + rootsign)\n        rootsign.baseline = 0\n        # Diagonal: length is one less than height of base\n        linelength = bpretty.height() - 1\n        diagonal = stringPict('\\n'.join(\n            ' '*(linelength - i - 1) + _zZ + ' '*i\n            for i in range(linelength)\n        ))\n        # Put baseline just below lowest line: next to exp\n        diagonal.baseline = linelength - 1\n        # Make the root symbol\n        rootsign = prettyForm(*rootsign.right(diagonal))\n        # Det the baseline to match contents to fix the height\n        # but if the height of bpretty is one, the rootsign must be one higher\n        rootsign.baseline = max(1, bpretty.baseline)\n        #build result\n        s = prettyForm(hobj('_', 2 + bpretty.width()))\n        s = prettyForm(*bpretty.above(s))\n        s = prettyForm(*s.left(rootsign))\n        return s\n\n    def _print_Pow(self, power):\n        from sympy.simplify.simplify import fraction\n        b, e = power.as_base_exp()\n        if power.is_commutative:\n            if e is S.NegativeOne:\n                return prettyForm(\"1\")/self._print(b)\n            n, d = fraction(e)\n            if n is S.One and d.is_Atom and not e.is_Integer and self._settings['root_notation']:\n                return self._print_nth_root(b, e)\n            if e.is_Rational and e < 0:\n                return prettyForm(\"1\")/self._print(Pow(b, -e, evaluate=False))\n\n        if b.is_Relational:\n            return prettyForm(*self._print(b).parens()).__pow__(self._print(e))\n\n        return self._print(b)**self._print(e)\n\n    def _print_UnevaluatedExpr(self, expr):\n        return self._print(expr.args[0])\n\n    def __print_numer_denom(self, p, q):\n        if q == 1:\n            if p < 0:\n                return prettyForm(str(p), binding=prettyForm.NEG)\n            else:\n                return prettyForm(str(p))\n        elif abs(p) >= 10 and abs(q) >= 10:", "answer": "C"}
{"uuid": "661030c4-ba46-4b3a-a6d7-c7c6d57f0311", "issue_statement": "`refine()` does not understand how to simplify complex arguments\nJust learned about the refine-function, which would come in handy frequently for me.  But\r\n`refine()` does not recognize that argument functions simplify for real numbers.\r\n\r\n```\r\n>>> from sympy import *                                                     \r\n>>> var('a,x')                                                              \r\n>>> J = Integral(sin(x)*exp(-a*x),(x,0,oo))                                     \r\n>>> J.doit()\r\n\tPiecewise((1/(a**2 + 1), 2*Abs(arg(a)) < pi), (Integral(exp(-a*x)*sin(x), (x, 0, oo)), True))\r\n>>> refine(J.doit(),Q.positive(a))                                                 \r\n        Piecewise((1/(a**2 + 1), 2*Abs(arg(a)) < pi), (Integral(exp(-a*x)*sin(x), (x, 0, oo)), True))\r\n>>> refine(abs(a),Q.positive(a))                                            \r\n\ta\r\n>>> refine(arg(a),Q.positive(a))                                            \r\n\targ(a)\r\n```\r\nI cann't find any open issues identifying this.  Easy to fix, though.", "choices": "(A)     \"\"\"\n    arg = expr.args[0]\n    if ask(Q.real(arg), assumptions):\n        return arg\n    if ask(Q.imaginary(arg), assumptions):\n        return S.Zero\n    return _refine_reim(expr, assumptions)\n\n\ndef refine_im(expr, assumptions):\n    \"\"\"\n    Handler for imaginary part.\n\n    Explanation\n    ===========\n\n    >>> from sympy.assumptions.refine import refine_im\n    >>> from sympy import Q, im\n    >>> from sympy.abc import x\n    >>> refine_im(im(x), Q.real(x))\n    0\n    >>> refine_im(im(x), Q.imaginary(x))\n    -I*x\n    \"\"\"\n    arg = expr.args[0]\n    if ask(Q.real(arg), assumptions):\n        return S.Zero\n    if ask(Q.imaginary(arg), assumptions):\n        return - S.ImaginaryUnit * arg\n    return _refine_reim(expr, assumptions)\n\n\ndef _refine_reim(expr, assumptions):\n    # Helper function for refine_re & refine_im\n    expanded = expr.expand(complex = True)\n    if expanded != expr:\n        refined = refine(expanded, assumptions)\n        if refined != expanded:\n            return refined\n    # Best to leave the expression as is\n    return None\n\n\ndef refine_sign(expr, assumptions):\n    \"\"\"\n    Handler for sign.\n\n    Examples\n    ========\n\n    >>> from sympy.assumptions.refine import refine_sign\n    >>> from sympy import Symbol, Q, sign, im\n    >>> x = Symbol('x', real = True)\n    >>> expr = sign(x)\n    >>> refine_sign(expr, Q.positive(x) & Q.nonzero(x))\n    1\n    >>> refine_sign(expr, Q.negative(x) & Q.nonzero(x))\n    -1\n    >>> refine_sign(expr, Q.zero(x))\n    0\n    >>> y = Symbol('y', imaginary = True)\n    >>> expr = sign(y)\n    >>> refine_sign(expr, Q.positive(im(y)))\n    I\n    >>> refine_sign(expr, Q.negative(im(y)))\n    -I\n    \"\"\"\n    arg = expr.args[0]\n    if ask(Q.zero(arg), assumptions):\n        return S.Zero\n    if ask(Q.real(arg)):\n        if ask(Q.positive(arg), assumptions):\n            return S.One\n        if ask(Q.negative(arg), assumptions):\n            return S.NegativeOne\n    if ask(Q.imaginary(arg)):\n        arg_re, arg_im = arg.as_real_imag()\n        if ask(Q.positive(arg_im), assumptions):\n            return S.ImaginaryUnit\n        if ask(Q.negative(arg_im), assumptions):\n            return -S.ImaginaryUnit\n    return expr\n\n\ndef refine_matrixelement(expr, assumptions):\n    \"\"\"\n    Handler for symmetric part.\n\n    Examples\n    ========\n\n    >>> from sympy.assumptions.refine import refine_matrixelement\n    >>> from sympy import Q\n    >>> from sympy.matrices.expressions.matexpr import MatrixSymbol\n    >>> X = MatrixSymbol('X', 3, 3)\n    >>> refine_matrixelement(X[0, 1], Q.symmetric(X))\n    X[0, 1]\n    >>> refine_matrixelement(X[1, 0], Q.symmetric(X))\n    X[0, 1]\n    \"\"\"\n    from sympy.matrices.expressions.matexpr import MatrixElement\n    matrix, i, j = expr.args\n    if ask(Q.symmetric(matrix), assumptions):\n        if (i - j).could_extract_minus_sign():\n            return expr\n        return MatrixElement(matrix, j, i)\n\nhandlers_dict = {\n    'Abs': refine_abs,\n    'Pow': refine_Pow,\n    'atan2': refine_atan2,\n(B)     elif ask(Q.positive(y) & Q.negative(x), assumptions):\n        return atan(y / x) + S.Pi\n    elif ask(Q.zero(y) & Q.negative(x), assumptions):\n        return S.Pi\n    elif ask(Q.positive(y) & Q.zero(x), assumptions):\n        return S.Pi/2\n    elif ask(Q.negative(y) & Q.zero(x), assumptions):\n        return -S.Pi/2\n    elif ask(Q.zero(y) & Q.zero(x), assumptions):\n        return S.NaN\n    else:\n        return expr\n\n\ndef refine_re(expr, assumptions):\n    \"\"\"\n    Handler for real part.\n\n    Examples\n    ========\n\n    >>> from sympy.assumptions.refine import refine_re\n    >>> from sympy import Q, re\n    >>> from sympy.abc import x\n    >>> refine_re(re(x), Q.real(x))\n    x\n    >>> refine_re(re(x), Q.imaginary(x))\n    0\n    \"\"\"\n    arg = expr.args[0]\n    if ask(Q.real(arg), assumptions):\n        return arg\n    if ask(Q.imaginary(arg), assumptions):\n        return S.Zero\n    return _refine_reim(expr, assumptions)\n\n\ndef refine_im(expr, assumptions):\n    \"\"\"\n    Handler for imaginary part.\n\n    Explanation\n    ===========\n\n    >>> from sympy.assumptions.refine import refine_im\n    >>> from sympy import Q, im\n    >>> from sympy.abc import x\n    >>> refine_im(im(x), Q.real(x))\n    0\n    >>> refine_im(im(x), Q.imaginary(x))\n    -I*x\n    \"\"\"\n    arg = expr.args[0]\n    if ask(Q.real(arg), assumptions):\n        return S.Zero\n    if ask(Q.imaginary(arg), assumptions):\n        return - S.ImaginaryUnit * arg\n    return _refine_reim(expr, assumptions)\n\n\ndef _refine_reim(expr, assumptions):\n    # Helper function for refine_re & refine_im\n    expanded = expr.expand(complex = True)\n    if expanded != expr:\n        refined = refine(expanded, assumptions)\n        if refined != expanded:\n            return refined\n    # Best to leave the expression as is\n    return None\n\n\ndef refine_sign(expr, assumptions):\n    \"\"\"\n    Handler for sign.\n\n    Examples\n    ========\n\n    >>> from sympy.assumptions.refine import refine_sign\n    >>> from sympy import Symbol, Q, sign, im\n    >>> x = Symbol('x', real = True)\n    >>> expr = sign(x)\n    >>> refine_sign(expr, Q.positive(x) & Q.nonzero(x))\n    1\n    >>> refine_sign(expr, Q.negative(x) & Q.nonzero(x))\n    -1\n    >>> refine_sign(expr, Q.zero(x))\n    0\n    >>> y = Symbol('y', imaginary = True)\n    >>> expr = sign(y)\n    >>> refine_sign(expr, Q.positive(im(y)))\n    I\n    >>> refine_sign(expr, Q.negative(im(y)))\n    -I\n    \"\"\"\n    arg = expr.args[0]\n    if ask(Q.zero(arg), assumptions):\n        return S.Zero\n    if ask(Q.real(arg)):\n        if ask(Q.positive(arg), assumptions):\n            return S.One\n        if ask(Q.negative(arg), assumptions):\n            return S.NegativeOne\n    if ask(Q.imaginary(arg)):\n        arg_re, arg_im = arg.as_real_imag()\n        if ask(Q.positive(arg_im), assumptions):\n            return S.ImaginaryUnit\n        if ask(Q.negative(arg_im), assumptions):\n            return -S.ImaginaryUnit\n    return expr\n\n(C)     Examples\n    ========\n\n    >>> from sympy import Q, atan2\n    >>> from sympy.assumptions.refine import refine_atan2\n    >>> from sympy.abc import x, y\n    >>> refine_atan2(atan2(y,x), Q.real(y) & Q.positive(x))\n    atan(y/x)\n    >>> refine_atan2(atan2(y,x), Q.negative(y) & Q.negative(x))\n    atan(y/x) - pi\n    >>> refine_atan2(atan2(y,x), Q.positive(y) & Q.negative(x))\n    atan(y/x) + pi\n    >>> refine_atan2(atan2(y,x), Q.zero(y) & Q.negative(x))\n    pi\n    >>> refine_atan2(atan2(y,x), Q.positive(y) & Q.zero(x))\n    pi/2\n    >>> refine_atan2(atan2(y,x), Q.negative(y) & Q.zero(x))\n    -pi/2\n    >>> refine_atan2(atan2(y,x), Q.zero(y) & Q.zero(x))\n    nan\n    \"\"\"\n    from sympy.functions.elementary.trigonometric import atan\n    from sympy.core import S\n    y, x = expr.args\n    if ask(Q.real(y) & Q.positive(x), assumptions):\n        return atan(y / x)\n    elif ask(Q.negative(y) & Q.negative(x), assumptions):\n        return atan(y / x) - S.Pi\n    elif ask(Q.positive(y) & Q.negative(x), assumptions):\n        return atan(y / x) + S.Pi\n    elif ask(Q.zero(y) & Q.negative(x), assumptions):\n        return S.Pi\n    elif ask(Q.positive(y) & Q.zero(x), assumptions):\n        return S.Pi/2\n    elif ask(Q.negative(y) & Q.zero(x), assumptions):\n        return -S.Pi/2\n    elif ask(Q.zero(y) & Q.zero(x), assumptions):\n        return S.NaN\n    else:\n        return expr\n\n\ndef refine_re(expr, assumptions):\n    \"\"\"\n    Handler for real part.\n\n    Examples\n    ========\n\n    >>> from sympy.assumptions.refine import refine_re\n    >>> from sympy import Q, re\n    >>> from sympy.abc import x\n    >>> refine_re(re(x), Q.real(x))\n    x\n    >>> refine_re(re(x), Q.imaginary(x))\n    0\n    \"\"\"\n    arg = expr.args[0]\n    if ask(Q.real(arg), assumptions):\n        return arg\n    if ask(Q.imaginary(arg), assumptions):\n        return S.Zero\n    return _refine_reim(expr, assumptions)\n\n\ndef refine_im(expr, assumptions):\n    \"\"\"\n    Handler for imaginary part.\n\n    Explanation\n    ===========\n\n    >>> from sympy.assumptions.refine import refine_im\n    >>> from sympy import Q, im\n    >>> from sympy.abc import x\n    >>> refine_im(im(x), Q.real(x))\n    0\n    >>> refine_im(im(x), Q.imaginary(x))\n    -I*x\n    \"\"\"\n    arg = expr.args[0]\n    if ask(Q.real(arg), assumptions):\n        return S.Zero\n    if ask(Q.imaginary(arg), assumptions):\n        return - S.ImaginaryUnit * arg\n    return _refine_reim(expr, assumptions)\n\n\ndef _refine_reim(expr, assumptions):\n    # Helper function for refine_re & refine_im\n    expanded = expr.expand(complex = True)\n    if expanded != expr:\n        refined = refine(expanded, assumptions)\n        if refined != expanded:\n            return refined\n    # Best to leave the expression as is\n    return None\n\n\ndef refine_sign(expr, assumptions):\n    \"\"\"\n    Handler for sign.\n\n    Examples\n    ========\n\n    >>> from sympy.assumptions.refine import refine_sign\n    >>> from sympy import Symbol, Q, sign, im\n    >>> x = Symbol('x', real = True)\n    >>> expr = sign(x)\n    >>> refine_sign(expr, Q.positive(x) & Q.nonzero(x))\n(D)         return - S.ImaginaryUnit * arg\n    return _refine_reim(expr, assumptions)\n\n\ndef _refine_reim(expr, assumptions):\n    # Helper function for refine_re & refine_im\n    expanded = expr.expand(complex = True)\n    if expanded != expr:\n        refined = refine(expanded, assumptions)\n        if refined != expanded:\n            return refined\n    # Best to leave the expression as is\n    return None\n\n\ndef refine_sign(expr, assumptions):\n    \"\"\"\n    Handler for sign.\n\n    Examples\n    ========\n\n    >>> from sympy.assumptions.refine import refine_sign\n    >>> from sympy import Symbol, Q, sign, im\n    >>> x = Symbol('x', real = True)\n    >>> expr = sign(x)\n    >>> refine_sign(expr, Q.positive(x) & Q.nonzero(x))\n    1\n    >>> refine_sign(expr, Q.negative(x) & Q.nonzero(x))\n    -1\n    >>> refine_sign(expr, Q.zero(x))\n    0\n    >>> y = Symbol('y', imaginary = True)\n    >>> expr = sign(y)\n    >>> refine_sign(expr, Q.positive(im(y)))\n    I\n    >>> refine_sign(expr, Q.negative(im(y)))\n    -I\n    \"\"\"\n    arg = expr.args[0]\n    if ask(Q.zero(arg), assumptions):\n        return S.Zero\n    if ask(Q.real(arg)):\n        if ask(Q.positive(arg), assumptions):\n            return S.One\n        if ask(Q.negative(arg), assumptions):\n            return S.NegativeOne\n    if ask(Q.imaginary(arg)):\n        arg_re, arg_im = arg.as_real_imag()\n        if ask(Q.positive(arg_im), assumptions):\n            return S.ImaginaryUnit\n        if ask(Q.negative(arg_im), assumptions):\n            return -S.ImaginaryUnit\n    return expr\n\n\ndef refine_matrixelement(expr, assumptions):\n    \"\"\"\n    Handler for symmetric part.\n\n    Examples\n    ========\n\n    >>> from sympy.assumptions.refine import refine_matrixelement\n    >>> from sympy import Q\n    >>> from sympy.matrices.expressions.matexpr import MatrixSymbol\n    >>> X = MatrixSymbol('X', 3, 3)\n    >>> refine_matrixelement(X[0, 1], Q.symmetric(X))\n    X[0, 1]\n    >>> refine_matrixelement(X[1, 0], Q.symmetric(X))\n    X[0, 1]\n    \"\"\"\n    from sympy.matrices.expressions.matexpr import MatrixElement\n    matrix, i, j = expr.args\n    if ask(Q.symmetric(matrix), assumptions):\n        if (i - j).could_extract_minus_sign():\n            return expr\n        return MatrixElement(matrix, j, i)\n\nhandlers_dict = {\n    'Abs': refine_abs,\n    'Pow': refine_Pow,\n    'atan2': refine_atan2,\n    're': refine_re,\n    'im': refine_im,\n    'sign': refine_sign,\n    'MatrixElement': refine_matrixelement\n}  # type: Dict[str, Callable[[Expr, Boolean], Expr]]", "answer": "D"}
{"uuid": "c72c0871-e663-4857-9e6a-49230076c494", "issue_statement": "_print_SingularityFunction() got an unexpected keyword argument 'exp'\nOn a Jupyter Notebook cell, type the following:\r\n\r\n```python\r\nfrom sympy import *\r\nfrom sympy.physics.continuum_mechanics import Beam\r\n# Young's modulus\r\nE = symbols(\"E\")\r\n# length of the beam\r\nL = symbols(\"L\")\r\n# concentrated load at the end tip of the beam\r\nF = symbols(\"F\")\r\n# square cross section\r\nB, H = symbols(\"B, H\")\r\nI = B * H**3 / 12\r\n# numerical values (material: steel)\r\nd = {B: 1e-02, H: 1e-02, E: 210e09, L: 0.2, F: 100}\r\n\r\nb2 = Beam(L, E, I)\r\nb2.apply_load(-F, L / 2, -1)\r\nb2.apply_support(0, \"fixed\")\r\nR0, M0 = symbols(\"R_0, M_0\")\r\nb2.solve_for_reaction_loads(R0, M0)\r\n```\r\n\r\nThen:\r\n\r\n```\r\nb2.shear_force()\r\n```\r\n\r\nThe following error appears:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.8/dist-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    343             method = get_real_method(obj, self.print_method)\r\n    344             if method is not None:\r\n--> 345                 return method()\r\n    346             return None\r\n    347         else:\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/interactive/printing.py in _print_latex_png(o)\r\n    184         \"\"\"\r\n    185         if _can_print(o):\r\n--> 186             s = latex(o, mode=latex_mode, **settings)\r\n    187             if latex_mode == 'plain':\r\n    188                 s = '$\\\\displaystyle %s$' % s\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/printer.py in __call__(self, *args, **kwargs)\r\n    371 \r\n    372     def __call__(self, *args, **kwargs):\r\n--> 373         return self.__wrapped__(*args, **kwargs)\r\n    374 \r\n    375     @property\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/latex.py in latex(expr, **settings)\r\n   2913 \r\n   2914     \"\"\"\r\n-> 2915     return LatexPrinter(settings).doprint(expr)\r\n   2916 \r\n   2917 \r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/latex.py in doprint(self, expr)\r\n    252 \r\n    253     def doprint(self, expr):\r\n--> 254         tex = Printer.doprint(self, expr)\r\n    255 \r\n    256         if self._settings['mode'] == 'plain':\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/printer.py in doprint(self, expr)\r\n    289     def doprint(self, expr):\r\n    290         \"\"\"Returns printer's representation for expr (as a string)\"\"\"\r\n--> 291         return self._str(self._print(expr))\r\n    292 \r\n    293     def _print(self, expr, **kwargs):\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/printer.py in _print(self, expr, **kwargs)\r\n    327                 printmethod = '_print_' + cls.__name__\r\n    328                 if hasattr(self, printmethod):\r\n--> 329                     return getattr(self, printmethod)(expr, **kwargs)\r\n    330             # Unknown object, fall back to the emptyPrinter.\r\n    331             return self.emptyPrinter(expr)\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/latex.py in _print_Add(self, expr, order)\r\n    381             else:\r\n    382                 tex += \" + \"\r\n--> 383             term_tex = self._print(term)\r\n    384             if self._needs_add_brackets(term):\r\n    385                 term_tex = r\"\\left(%s\\right)\" % term_tex\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/printer.py in _print(self, expr, **kwargs)\r\n    327                 printmethod = '_print_' + cls.__name__\r\n    328                 if hasattr(self, printmethod):\r\n--> 329                     return getattr(self, printmethod)(expr, **kwargs)\r\n    330             # Unknown object, fall back to the emptyPrinter.\r\n    331             return self.emptyPrinter(expr)\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/latex.py in _print_Mul(self, expr)\r\n    565             # use the original expression here, since fraction() may have\r\n    566             # altered it when producing numer and denom\r\n--> 567             tex += convert(expr)\r\n    568 \r\n    569         else:\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/latex.py in convert(expr)\r\n    517                                isinstance(x.base, Quantity)))\r\n    518 \r\n--> 519                 return convert_args(args)\r\n    520 \r\n    521         def convert_args(args):\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/latex.py in convert_args(args)\r\n    523 \r\n    524                 for i, term in enumerate(args):\r\n--> 525                     term_tex = self._print(term)\r\n    526 \r\n    527                     if self._needs_mul_brackets(term, first=(i == 0),\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/printer.py in _print(self, expr, **kwargs)\r\n    327                 printmethod = '_print_' + cls.__name__\r\n    328                 if hasattr(self, printmethod):\r\n--> 329                     return getattr(self, printmethod)(expr, **kwargs)\r\n    330             # Unknown object, fall back to the emptyPrinter.\r\n    331             return self.emptyPrinter(expr)\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/latex.py in _print_Add(self, expr, order)\r\n    381             else:\r\n    382                 tex += \" + \"\r\n--> 383             term_tex = self._print(term)\r\n    384             if self._needs_add_brackets(term):\r\n    385                 term_tex = r\"\\left(%s\\right)\" % term_tex\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/printer.py in _print(self, expr, **kwargs)\r\n    327                 printmethod = '_print_' + cls.__name__\r\n    328                 if hasattr(self, printmethod):\r\n--> 329                     return getattr(self, printmethod)(expr, **kwargs)\r\n    330             # Unknown object, fall back to the emptyPrinter.\r\n    331             return self.emptyPrinter(expr)\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/latex.py in _print_Mul(self, expr)\r\n    569         else:\r\n    570             snumer = convert(numer)\r\n--> 571             sdenom = convert(denom)\r\n    572             ldenom = len(sdenom.split())\r\n    573             ratio = self._settings['long_frac_ratio']\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/latex.py in convert(expr)\r\n    505         def convert(expr):\r\n    506             if not expr.is_Mul:\r\n--> 507                 return str(self._print(expr))\r\n    508             else:\r\n    509                 if self.order not in ('old', 'none'):\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/printer.py in _print(self, expr, **kwargs)\r\n    327                 printmethod = '_print_' + cls.__name__\r\n    328                 if hasattr(self, printmethod):\r\n--> 329                     return getattr(self, printmethod)(expr, **kwargs)\r\n    330             # Unknown object, fall back to the emptyPrinter.\r\n    331             return self.emptyPrinter(expr)\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/latex.py in _print_Add(self, expr, order)\r\n    381             else:\r\n    382                 tex += \" + \"\r\n--> 383             term_tex = self._print(term)\r\n    384             if self._needs_add_brackets(term):\r\n    385                 term_tex = r\"\\left(%s\\right)\" % term_tex\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/printer.py in _print(self, expr, **kwargs)\r\n    327                 printmethod = '_print_' + cls.__name__\r\n    328                 if hasattr(self, printmethod):\r\n--> 329                     return getattr(self, printmethod)(expr, **kwargs)\r\n    330             # Unknown object, fall back to the emptyPrinter.\r\n    331             return self.emptyPrinter(expr)\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/latex.py in _print_Pow(self, expr)\r\n    649         else:\r\n    650             if expr.base.is_Function:\r\n--> 651                 return self._print(expr.base, exp=self._print(expr.exp))\r\n    652             else:\r\n    653                 tex = r\"%s^{%s}\"\r\n\r\n/usr/local/lib/python3.8/dist-packages/sympy/printing/printer.py in _print(self, expr, **kwargs)\r\n    327                 printmethod = '_print_' + cls.__name__\r\n    328                 if hasattr(self, printmethod):\r\n--> 329                     return getattr(self, printmethod)(expr, **kwargs)\r\n    330             # Unknown object, fall back to the emptyPrinter.\r\n    331             return self.emptyPrinter(expr)\r\n\r\nTypeError: _print_SingularityFunction() got an unexpected keyword argument 'exp'\r\n```", "choices": "(A)         tex = r\"{\\left\\langle %s \\right\\rangle}^{%s}\" % (shift, power)\n        return tex\n\n    def _print_Heaviside(self, expr, exp=None):\n        tex = r\"\\theta\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp:\n            tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n        return tex\n\n    def _print_KroneckerDelta(self, expr, exp=None):\n        i = self._print(expr.args[0])\n        j = self._print(expr.args[1])\n(B)     def _print_SingularityFunction(self, expr):\n        shift = self._print(expr.args[0] - expr.args[1])\n        power = self._print(expr.args[2])\n        tex = r\"{\\left\\langle %s \\right\\rangle}^{%s}\" % (shift, power)\n        return tex\n\n    def _print_Heaviside(self, expr, exp=None):\n        tex = r\"\\theta\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp:\n            tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n        return tex\n\n(C)             tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n        return tex\n\n    def _print_SingularityFunction(self, expr):\n        shift = self._print(expr.args[0] - expr.args[1])\n        power = self._print(expr.args[2])\n        tex = r\"{\\left\\langle %s \\right\\rangle}^{%s}\" % (shift, power)\n        return tex\n\n    def _print_Heaviside(self, expr, exp=None):\n        tex = r\"\\theta\\left(%s\\right)\" % self._print(expr.args[0])\n        if exp:\n(D)             tex = r\"\\delta^{\\left( %s \\right)}\\left( %s \\right)\" % (\n                self._print(expr.args[1]), self._print(expr.args[0]))\n        if exp:\n            tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n        return tex\n\n    def _print_SingularityFunction(self, expr):\n        shift = self._print(expr.args[0] - expr.args[1])\n        power = self._print(expr.args[2])\n        tex = r\"{\\left\\langle %s \\right\\rangle}^{%s}\" % (shift, power)\n        return tex\n", "answer": "C"}
{"uuid": "c5f0bdf0-fc3e-40a8-9766-464ec7ca0219", "issue_statement": "Unexpected `PolynomialError` when using simple `subs()` for particular expressions\nI am seeing weird behavior with `subs` for particular expressions with hyperbolic sinusoids with piecewise arguments. When applying `subs`, I obtain an unexpected `PolynomialError`. For context, I was umbrella-applying a casting from int to float of all int atoms for a bunch of random expressions before using a tensorflow lambdify to avoid potential tensorflow type errors. You can pretend the expression below has a `+ 1` at the end, but below is the MWE that I could produce.\r\n\r\nSee the expression below, and the conditions in which the exception arises.\r\n\r\nSympy version: 1.8.dev\r\n\r\n```python\r\nfrom sympy import *\r\nfrom sympy.core.cache import clear_cache\r\n\r\nx, y, z = symbols('x y z')\r\n\r\nclear_cache()\r\nexpr = exp(sinh(Piecewise((x, y > x), (y, True)) / z))\r\n# This works fine\r\nexpr.subs({1: 1.0})\r\n\r\nclear_cache()\r\nx, y, z = symbols('x y z', real=True)\r\nexpr = exp(sinh(Piecewise((x, y > x), (y, True)) / z))\r\n# This fails with \"PolynomialError: Piecewise generators do not make sense\"\r\nexpr.subs({1: 1.0})  # error\r\n# Now run it again (isympy...) w/o clearing cache and everything works as expected without error\r\nexpr.subs({1: 1.0})\r\n```\r\n\r\nI am not really sure where the issue is, but I think it has something to do with the order of assumptions in this specific type of expression. Here is what I found-\r\n\r\n- The error only (AFAIK) happens with `cosh` or `tanh` in place of `sinh`, otherwise it succeeds\r\n- The error goes away if removing the division by `z`\r\n- The error goes away if removing `exp` (but stays for most unary functions, `sin`, `log`, etc.)\r\n- The error only happens with real symbols for `x` and `y` (`z` does not have to be real)\r\n\r\nNot too sure how to debug this one.", "choices": "(A)         from sympy.core.mul import Mul\n        from sympy.core.singleton import S\n        from sympy.core.exprtools import gcd_terms\n        from sympy.polys.polytools import gcd\n\n        def doit(p, q):\n            \"\"\"Try to return p % q if both are numbers or +/-p is known\n            to be less than or equal q.\n            \"\"\"\n\n            if q.is_zero:\n                raise ZeroDivisionError(\"Modulo by zero\")\n            if p.is_finite is False or q.is_finite is False or p is nan or q is nan:\n                return nan\n            if p is S.Zero or p == q or p == -q or (p.is_integer and q == 1):\n                return S.Zero\n\n            if q.is_Number:\n                if p.is_Number:\n                    return p%q\n                if q == 2:\n                    if p.is_even:\n                        return S.Zero\n                    elif p.is_odd:\n                        return S.One\n\n            if hasattr(p, '_eval_Mod'):\n                rv = getattr(p, '_eval_Mod')(q)\n                if rv is not None:\n                    return rv\n\n            # by ratio\n            r = p/q\n            if r.is_integer:\n                return S.Zero\n            try:\n                d = int(r)\n            except TypeError:\n                pass\n            else:\n                if isinstance(d, int):\n                    rv = p - d*q\n                    if (rv*q < 0) == True:\n                        rv += q\n                    return rv\n\n            # by difference\n            # -2|q| < p < 2|q|\n            d = abs(p)\n            for _ in range(2):\n                d -= abs(q)\n                if d.is_negative:\n                    if q.is_positive:\n                        if p.is_positive:\n                            return d + q\n                        elif p.is_negative:\n                            return -d\n                    elif q.is_negative:\n                        if p.is_positive:\n                            return d\n                        elif p.is_negative:\n                            return -d + q\n                    break\n\n        rv = doit(p, q)\n        if rv is not None:\n            return rv\n\n        # denest\n        if isinstance(p, cls):\n            qinner = p.args[1]\n            if qinner % q == 0:\n                return cls(p.args[0], q)\n            elif (qinner*(q - qinner)).is_nonnegative:\n                # |qinner| < |q| and have same sign\n                return p\n        elif isinstance(-p, cls):\n            qinner = (-p).args[1]\n            if qinner % q == 0:\n                return cls(-(-p).args[0], q)\n            elif (qinner*(q + qinner)).is_nonpositive:\n                # |qinner| < |q| and have different sign\n                return p\n        elif isinstance(p, Add):\n            # separating into modulus and non modulus\n            both_l = non_mod_l, mod_l = [], []\n            for arg in p.args:\n                both_l[isinstance(arg, cls)].append(arg)\n            # if q same for all\n            if mod_l and all(inner.args[1] == q for inner in mod_l):\n                net = Add(*non_mod_l) + Add(*[i.args[0] for i in mod_l])\n                return cls(net, q)\n\n        elif isinstance(p, Mul):\n            # separating into modulus and non modulus\n            both_l = non_mod_l, mod_l = [], []\n            for arg in p.args:\n                both_l[isinstance(arg, cls)].append(arg)\n\n            if mod_l and all(inner.args[1] == q for inner in mod_l):\n                # finding distributive term\n                non_mod_l = [cls(x, q) for x in non_mod_l]\n                mod = []\n                non_mod = []\n                for j in non_mod_l:\n                    if isinstance(j, cls):\n                        mod.append(j.args[0])\n                    else:\n                        non_mod.append(j)\n                prod_mod = Mul(*mod)\n                prod_non_mod = Mul(*non_mod)\n                prod_mod1 = Mul(*[i.args[0] for i in mod_l])\n                net = prod_mod1*prod_mod\n                return prod_non_mod*cls(net, q)\n\n            if q.is_Integer and q is not S.One:\n                _ = []\n                for i in non_mod_l:\n                    if i.is_Integer and (i % q is not S.Zero):\n                        _.append(i%q)\n                    else:\n                        _.append(i)\n                non_mod_l = _\n\n            p = Mul(*(non_mod_l + mod_l))\n\n        # XXX other possibilities?\n\n        # extract gcd; any further simplification should be done by the user\n        G = gcd(p, q)\n        if G != 1:\n            p, q = [\n                gcd_terms(i/G, clear=False, fraction=False) for i in (p, q)]\n        pwas, qwas = p, q\n\n        # simplify terms\n        # (x + y + 2) % x -> Mod(y + 2, x)\n        if p.is_Add:\n            args = []\n            for i in p.args:\n(B) \nclass Mod(Function):\n    \"\"\"Represents a modulo operation on symbolic expressions.\n\n    Parameters\n    ==========\n\n    p : Expr\n        Dividend.\n\n    q : Expr\n        Divisor.\n\n    Notes\n    =====\n\n    The convention used is the same as Python's: the remainder always has the\n    same sign as the divisor.\n\n    Examples\n    ========\n\n    >>> from sympy.abc import x, y\n    >>> x**2 % y\n    Mod(x**2, y)\n    >>> _.subs({x: 5, y: 6})\n    1\n\n    \"\"\"\n\n    kind = NumberKind\n\n    @classmethod\n    def eval(cls, p, q):\n        from sympy.core.add import Add\n        from sympy.core.mul import Mul\n        from sympy.core.singleton import S\n        from sympy.core.exprtools import gcd_terms\n        from sympy.polys.polytools import gcd\n\n        def doit(p, q):\n            \"\"\"Try to return p % q if both are numbers or +/-p is known\n            to be less than or equal q.\n            \"\"\"\n\n            if q.is_zero:\n                raise ZeroDivisionError(\"Modulo by zero\")\n            if p.is_finite is False or q.is_finite is False or p is nan or q is nan:\n                return nan\n            if p is S.Zero or p == q or p == -q or (p.is_integer and q == 1):\n                return S.Zero\n\n            if q.is_Number:\n                if p.is_Number:\n                    return p%q\n                if q == 2:\n                    if p.is_even:\n                        return S.Zero\n                    elif p.is_odd:\n                        return S.One\n\n            if hasattr(p, '_eval_Mod'):\n                rv = getattr(p, '_eval_Mod')(q)\n                if rv is not None:\n                    return rv\n\n            # by ratio\n            r = p/q\n            if r.is_integer:\n                return S.Zero\n            try:\n                d = int(r)\n            except TypeError:\n                pass\n            else:\n                if isinstance(d, int):\n                    rv = p - d*q\n                    if (rv*q < 0) == True:\n                        rv += q\n                    return rv\n\n            # by difference\n            # -2|q| < p < 2|q|\n            d = abs(p)\n            for _ in range(2):\n                d -= abs(q)\n                if d.is_negative:\n                    if q.is_positive:\n                        if p.is_positive:\n                            return d + q\n                        elif p.is_negative:\n                            return -d\n                    elif q.is_negative:\n                        if p.is_positive:\n                            return d\n                        elif p.is_negative:\n                            return -d + q\n                    break\n\n        rv = doit(p, q)\n        if rv is not None:\n            return rv\n\n        # denest\n        if isinstance(p, cls):\n            qinner = p.args[1]\n            if qinner % q == 0:\n                return cls(p.args[0], q)\n            elif (qinner*(q - qinner)).is_nonnegative:\n                # |qinner| < |q| and have same sign\n                return p\n        elif isinstance(-p, cls):\n            qinner = (-p).args[1]\n            if qinner % q == 0:\n                return cls(-(-p).args[0], q)\n            elif (qinner*(q + qinner)).is_nonpositive:\n                # |qinner| < |q| and have different sign\n                return p\n        elif isinstance(p, Add):\n            # separating into modulus and non modulus\n            both_l = non_mod_l, mod_l = [], []\n            for arg in p.args:\n                both_l[isinstance(arg, cls)].append(arg)\n            # if q same for all\n            if mod_l and all(inner.args[1] == q for inner in mod_l):\n                net = Add(*non_mod_l) + Add(*[i.args[0] for i in mod_l])\n                return cls(net, q)\n\n        elif isinstance(p, Mul):\n            # separating into modulus and non modulus\n            both_l = non_mod_l, mod_l = [], []\n            for arg in p.args:\n                both_l[isinstance(arg, cls)].append(arg)\n\n            if mod_l and all(inner.args[1] == q for inner in mod_l):\n                # finding distributive term\n                non_mod_l = [cls(x, q) for x in non_mod_l]\n                mod = []\n                non_mod = []\n                for j in non_mod_l:\n(C)             try:\n                d = int(r)\n            except TypeError:\n                pass\n            else:\n                if isinstance(d, int):\n                    rv = p - d*q\n                    if (rv*q < 0) == True:\n                        rv += q\n                    return rv\n\n            # by difference\n            # -2|q| < p < 2|q|\n            d = abs(p)\n            for _ in range(2):\n                d -= abs(q)\n                if d.is_negative:\n                    if q.is_positive:\n                        if p.is_positive:\n                            return d + q\n                        elif p.is_negative:\n                            return -d\n                    elif q.is_negative:\n                        if p.is_positive:\n                            return d\n                        elif p.is_negative:\n                            return -d + q\n                    break\n\n        rv = doit(p, q)\n        if rv is not None:\n            return rv\n\n        # denest\n        if isinstance(p, cls):\n            qinner = p.args[1]\n            if qinner % q == 0:\n                return cls(p.args[0], q)\n            elif (qinner*(q - qinner)).is_nonnegative:\n                # |qinner| < |q| and have same sign\n                return p\n        elif isinstance(-p, cls):\n            qinner = (-p).args[1]\n            if qinner % q == 0:\n                return cls(-(-p).args[0], q)\n            elif (qinner*(q + qinner)).is_nonpositive:\n                # |qinner| < |q| and have different sign\n                return p\n        elif isinstance(p, Add):\n            # separating into modulus and non modulus\n            both_l = non_mod_l, mod_l = [], []\n            for arg in p.args:\n                both_l[isinstance(arg, cls)].append(arg)\n            # if q same for all\n            if mod_l and all(inner.args[1] == q for inner in mod_l):\n                net = Add(*non_mod_l) + Add(*[i.args[0] for i in mod_l])\n                return cls(net, q)\n\n        elif isinstance(p, Mul):\n            # separating into modulus and non modulus\n            both_l = non_mod_l, mod_l = [], []\n            for arg in p.args:\n                both_l[isinstance(arg, cls)].append(arg)\n\n            if mod_l and all(inner.args[1] == q for inner in mod_l):\n                # finding distributive term\n                non_mod_l = [cls(x, q) for x in non_mod_l]\n                mod = []\n                non_mod = []\n                for j in non_mod_l:\n                    if isinstance(j, cls):\n                        mod.append(j.args[0])\n                    else:\n                        non_mod.append(j)\n                prod_mod = Mul(*mod)\n                prod_non_mod = Mul(*non_mod)\n                prod_mod1 = Mul(*[i.args[0] for i in mod_l])\n                net = prod_mod1*prod_mod\n                return prod_non_mod*cls(net, q)\n\n            if q.is_Integer and q is not S.One:\n                _ = []\n                for i in non_mod_l:\n                    if i.is_Integer and (i % q is not S.Zero):\n                        _.append(i%q)\n                    else:\n                        _.append(i)\n                non_mod_l = _\n\n            p = Mul(*(non_mod_l + mod_l))\n\n        # XXX other possibilities?\n\n        # extract gcd; any further simplification should be done by the user\n        G = gcd(p, q)\n        if G != 1:\n            p, q = [\n                gcd_terms(i/G, clear=False, fraction=False) for i in (p, q)]\n        pwas, qwas = p, q\n\n        # simplify terms\n        # (x + y + 2) % x -> Mod(y + 2, x)\n        if p.is_Add:\n            args = []\n            for i in p.args:\n                a = cls(i, q)\n                if a.count(cls) > i.count(cls):\n                    args.append(i)\n                else:\n                    args.append(a)\n            if args != list(p.args):\n                p = Add(*args)\n\n        else:\n            # handle coefficients if they are not Rational\n            # since those are not handled by factor_terms\n            # e.g. Mod(.6*x, .3*y) -> 0.3*Mod(2*x, y)\n            cp, p = p.as_coeff_Mul()\n            cq, q = q.as_coeff_Mul()\n            ok = False\n            if not cp.is_Rational or not cq.is_Rational:\n                r = cp % cq\n                if r == 0:\n                    G *= cq\n                    p *= int(cp/cq)\n                    ok = True\n            if not ok:\n                p = cp*p\n                q = cq*q\n\n        # simple -1 extraction\n        if p.could_extract_minus_sign() and q.could_extract_minus_sign():\n            G, p, q = [-i for i in (G, p, q)]\n\n        # check again to see if p and q can now be handled as numbers\n        rv = doit(p, q)\n        if rv is not None:\n            return rv*G\n\n        # put 1.0 from G on inside", "answer": "A"}
{"uuid": "e973eb0f-068e-4fcb-9a3b-428df10a3b9e", "issue_statement": "Latex parsing of fractions yields wrong expression due to missing brackets\nProblematic latex expression: `\"\\\\frac{\\\\frac{a^3+b}{c}}{\\\\frac{1}{c^2}}\"`\r\n\r\nis parsed to: `((a**3 + b)/c)/1/(c**2)`.\r\n\r\nExpected is: `((a**3 + b)/c)/(1/(c**2))`. \r\n\r\nThe missing brackets in the denominator result in a wrong expression.\r\n\r\n## Tested on\r\n\r\n- 1.8\r\n- 1.6.2\r\n\r\n## Reproduce:\r\n\r\n```\r\nroot@d31ef1c26093:/# python3\r\nPython 3.6.9 (default, Jan 26 2021, 15:33:00)\r\n[GCC 8.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from sympy.parsing.latex import parse_latex\r\n>>> parse_latex(\"\\\\frac{\\\\frac{a^3+b}{c}}{\\\\frac{1}{c^2}}\")\r\n((a**3 + b)/c)/1/(c**2)", "choices": "(A)                     bool(item.exp.as_coeff_Mul()[0] < 0)):\n                if item.exp is not S.NegativeOne:\n                    b.append(apow(item))\n                else:\n                    if (len(item.args[0].args) != 1 and\n                            isinstance(item.base, Mul)):\n                        # To avoid situations like #14160\n(B)                     b.append(apow(item))\n                else:\n                    if (len(item.args[0].args) != 1 and\n                            isinstance(item.base, Mul)):\n                        # To avoid situations like #14160\n                        pow_paren.append(item)\n                    b.append(item.base)\n(C)                     if (len(item.args[0].args) != 1 and\n                            isinstance(item.base, Mul)):\n                        # To avoid situations like #14160\n                        pow_paren.append(item)\n                    b.append(item.base)\n            elif item.is_Rational and item is not S.Infinity:\n                if item.p != 1:\n(D)                         # To avoid situations like #14160\n                        pow_paren.append(item)\n                    b.append(item.base)\n            elif item.is_Rational and item is not S.Infinity:\n                if item.p != 1:\n                    a.append(Rational(item.p))\n                if item.q != 1:", "answer": "B"}
{"uuid": "0a23c9f0-f0a3-4042-9d28-894afb3d7dd9", "issue_statement": "Wrong Derivative kind attribute\nI'm playing around with the `kind` attribute.\r\n\r\nThe following is correct:\r\n\r\n```\r\nfrom sympy import Integral, Derivative\r\nfrom sympy import MatrixSymbol\r\nfrom sympy.abc import x\r\nA = MatrixSymbol('A', 2, 2)\r\ni = Integral(A, x)\r\ni.kind\r\n# MatrixKind(NumberKind)\r\n```\r\n\r\nThis one is wrong:\r\n```\r\nd = Derivative(A, x)\r\nd.kind\r\n# UndefinedKind\r\n```", "choices": "(A)             ret.update(count.free_symbols)\n        return ret\n\n    def _eval_subs(self, old, new):\n        # The substitution (old, new) cannot be done inside\n        # Derivative(expr, vars) for a variety of reasons\n        # as handled below.\n        if old in self._wrt_variables:\n            # first handle the counts\n            expr = self.func(self.expr, *[(v, c.subs(old, new))\n(B)         ret = self.expr.free_symbols\n        # Add symbolic counts to free_symbols\n        for var, count in self.variable_count:\n            ret.update(count.free_symbols)\n        return ret\n\n    def _eval_subs(self, old, new):\n        # The substitution (old, new) cannot be done inside\n        # Derivative(expr, vars) for a variety of reasons\n        # as handled below.\n(C)         # Derivative(expr, vars) for a variety of reasons\n        # as handled below.\n        if old in self._wrt_variables:\n            # first handle the counts\n            expr = self.func(self.expr, *[(v, c.subs(old, new))\n                for v, c in self.variable_count])\n            if expr != self:\n                return expr._eval_subs(old, new)\n            # quick exit case\n            if not getattr(new, '_diff_wrt', False):\n(D)     def _eval_subs(self, old, new):\n        # The substitution (old, new) cannot be done inside\n        # Derivative(expr, vars) for a variety of reasons\n        # as handled below.\n        if old in self._wrt_variables:\n            # first handle the counts\n            expr = self.func(self.expr, *[(v, c.subs(old, new))\n                for v, c in self.variable_count])\n            if expr != self:\n                return expr._eval_subs(old, new)", "answer": "A"}
{"uuid": "c128775f-bf2c-4083-9951-e79d6ca8ac6e", "issue_statement": "Bug: maximum recusion depth error when checking is_zero of cosh expression\nThe following code causes a `RecursionError: maximum recursion depth exceeded while calling a Python object` error when checked if it is zero:\r\n```\r\nexpr =sympify(\"cosh(acos(-i + acosh(-g + i)))\")\r\nexpr.is_zero\r\n```", "choices": "(A)             return -arg\n        if arg.is_imaginary:\n            arg2 = -S.ImaginaryUnit * arg\n            if arg2.is_extended_nonnegative:\n                return arg2\n        # reject result if all new conjugates are just wrappers around\n        # an expression that was already in the arg\n        conj = signsimp(arg.conjugate(), evaluate=False)\n(B)         # an expression that was already in the arg\n        conj = signsimp(arg.conjugate(), evaluate=False)\n        new_conj = conj.atoms(conjugate) - arg.atoms(conjugate)\n        if new_conj and all(arg.has(i.args[0]) for i in new_conj):\n            return\n        if arg != conj and arg != -conj:\n            ignore = arg.atoms(Abs)\n            abs_free_arg = arg.xreplace({i: Dummy(real=True) for i in ignore})\n(C)             arg2 = -S.ImaginaryUnit * arg\n            if arg2.is_extended_nonnegative:\n                return arg2\n        # reject result if all new conjugates are just wrappers around\n        # an expression that was already in the arg\n        conj = signsimp(arg.conjugate(), evaluate=False)\n        new_conj = conj.atoms(conjugate) - arg.atoms(conjugate)\n        if new_conj and all(arg.has(i.args[0]) for i in new_conj):\n(D)                 return arg2\n        # reject result if all new conjugates are just wrappers around\n        # an expression that was already in the arg\n        conj = signsimp(arg.conjugate(), evaluate=False)\n        new_conj = conj.atoms(conjugate) - arg.atoms(conjugate)\n        if new_conj and all(arg.has(i.args[0]) for i in new_conj):\n            return\n        if arg != conj and arg != -conj:", "answer": "C"}
{"uuid": "f14bb65e-21e0-4e3a-a922-c6988b4fc54f", "issue_statement": "itermonomials returns incorrect monomials when using min_degrees argument\n`itermonomials` returns incorrect monomials when using optional `min_degrees` argument\r\n\r\nFor example, the following code introduces three symbolic variables and generates monomials with max and min degree of 3:\r\n\r\n\r\n```\r\nimport sympy as sp\r\nfrom sympy.polys.orderings import monomial_key\r\n\r\nx1, x2, x3 = sp.symbols('x1, x2, x3')\r\nstates = [x1, x2, x3]\r\nmax_degrees = 3\r\nmin_degrees = 3\r\nmonomials = sorted(sp.itermonomials(states, max_degrees, min_degrees=min_degrees), \r\n                   key=monomial_key('grlex', states))\r\nprint(monomials)\r\n```\r\nThe code returns `[x3**3, x2**3, x1**3]`, when it _should_ also return monomials such as `x1*x2**2, x2*x3**2, etc...` that also have total degree of 3. This behaviour is inconsistent with the documentation that states that \r\n\r\n> A generator of all monomials `monom` is returned, such that either `min_degree <= total_degree(monom) <= max_degree`...\r\n\r\nThe monomials are also missing when `max_degrees` is increased above `min_degrees`.", "choices": "(A)             monomials_list_comm = []\n            for item in combinations_with_replacement(variables, max_degree):\n                powers = dict()\n                for variable in variables:\n                    powers[variable] = 0\n                for variable in item:\n                    if variable != 1:\n                        powers[variable] += 1\n                if max(powers.values()) >= min_degree:\n                    monomials_list_comm.append(Mul(*item))\n            yield from set(monomials_list_comm)\n        else:\n            monomials_list_non_comm = []\n            for item in product(variables, repeat=max_degree):\n                powers = dict()\n                for variable in variables:\n                    powers[variable] = 0\n                for variable in item:\n                    if variable != 1:\n(B)             yield from set(monomials_list_comm)\n        else:\n            monomials_list_non_comm = []\n            for item in product(variables, repeat=max_degree):\n                powers = dict()\n                for variable in variables:\n                    powers[variable] = 0\n                for variable in item:\n                    if variable != 1:\n                        powers[variable] += 1\n                if max(powers.values()) >= min_degree:\n                    monomials_list_non_comm.append(Mul(*item))\n            yield from set(monomials_list_non_comm)\n    else:\n        if any(min_degrees[i] > max_degrees[i] for i in range(n)):\n            raise ValueError('min_degrees[i] must be <= max_degrees[i] for all i')\n        power_lists = []\n        for var, min_d, max_d in zip(variables, min_degrees, max_degrees):\n            power_lists.append([var**i for i in range(min_d, max_d + 1)])\n(C)                 for variable in item:\n                    if variable != 1:\n                        powers[variable] += 1\n                if max(powers.values()) >= min_degree:\n                    monomials_list_comm.append(Mul(*item))\n            yield from set(monomials_list_comm)\n        else:\n            monomials_list_non_comm = []\n            for item in product(variables, repeat=max_degree):\n                powers = dict()\n                for variable in variables:\n                    powers[variable] = 0\n                for variable in item:\n                    if variable != 1:\n                        powers[variable] += 1\n                if max(powers.values()) >= min_degree:\n                    monomials_list_non_comm.append(Mul(*item))\n            yield from set(monomials_list_non_comm)\n    else:\n(D)                 for variable in variables:\n                    powers[variable] = 0\n                for variable in item:\n                    if variable != 1:\n                        powers[variable] += 1\n                if max(powers.values()) >= min_degree:\n                    monomials_list_non_comm.append(Mul(*item))\n            yield from set(monomials_list_non_comm)\n    else:\n        if any(min_degrees[i] > max_degrees[i] for i in range(n)):\n            raise ValueError('min_degrees[i] must be <= max_degrees[i] for all i')\n        power_lists = []\n        for var, min_d, max_d in zip(variables, min_degrees, max_degrees):\n            power_lists.append([var**i for i in range(min_d, max_d + 1)])\n        for powers in product(*power_lists):\n            yield Mul(*powers)\n\ndef monomial_count(V, N):\n    r\"\"\"", "answer": "C"}
{"uuid": "f8595955-c02a-4ef7-9c24-f170f1f0c4e8", "issue_statement": "detection of infinite solution request\n```python\r\n>>> solve_poly_system((x - 1,), x, y)\r\nTraceback (most recent call last):\r\n...\r\nNotImplementedError:\r\nonly zero-dimensional systems supported (finite number of solutions)\r\n>>> solve_poly_system((y - 1,), x, y)  <--- this is not handled correctly\r\n[(1,)]\r\n```\r\n```diff\r\ndiff --git a/sympy/solvers/polysys.py b/sympy/solvers/polysys.py\r\nindex b9809fd4e9..674322d4eb 100644\r\n--- a/sympy/solvers/polysys.py\r\n+++ b/sympy/solvers/polysys.py\r\n@@ -240,7 +240,7 @@ def _solve_reduced_system(system, gens, entry=False):\r\n \r\n         univariate = list(filter(_is_univariate, basis))\r\n \r\n-        if len(univariate) == 1:\r\n+        if len(univariate) == 1 and len(gens) == 1:\r\n             f = univariate.pop()\r\n         else:\r\n             raise NotImplementedError(filldedent('''\r\ndiff --git a/sympy/solvers/tests/test_polysys.py b/sympy/solvers/tests/test_polysys.py\r\nindex 58419f8762..9e674a6fe6 100644\r\n--- a/sympy/solvers/tests/test_polysys.py\r\n+++ b/sympy/solvers/tests/test_polysys.py\r\n@@ -48,6 +48,10 @@ def test_solve_poly_system():\r\n     raises(NotImplementedError, lambda: solve_poly_system(\r\n         [z, -2*x*y**2 + x + y**2*z, y**2*(-z - 4) + 2]))\r\n     raises(PolynomialError, lambda: solve_poly_system([1/x], x))\r\n+    raises(NotImplementedError, lambda: solve_poly_system(\r\n+        Poly(x - 1, x, y), (x, y)))\r\n+    raises(NotImplementedError, lambda: solve_poly_system(\r\n+        Poly(y - 1, x, y), (x, y)))\r\n \r\n \r\n def test_solve_biquadratic():\r\n```", "choices": "(A) \n        univariate = list(filter(_is_univariate, basis))\n\n        if len(univariate) == 1:\n            f = univariate.pop()\n        else:\n            raise NotImplementedError(filldedent('''\n                only zero-dimensional systems supported\n                (finite number of solutions)\n                '''))\n\n        gens = f.gens\n(B)                 return []\n            else:\n                return None\n\n        univariate = list(filter(_is_univariate, basis))\n\n        if len(univariate) == 1:\n            f = univariate.pop()\n        else:\n            raise NotImplementedError(filldedent('''\n                only zero-dimensional systems supported\n                (finite number of solutions)\n(C)             raise NotImplementedError(filldedent('''\n                only zero-dimensional systems supported\n                (finite number of solutions)\n                '''))\n\n        gens = f.gens\n        gen = gens[-1]\n\n        zeros = list(roots(f.ltrim(gen)).keys())\n\n        if not zeros:\n            return []\n(D)         if len(univariate) == 1:\n            f = univariate.pop()\n        else:\n            raise NotImplementedError(filldedent('''\n                only zero-dimensional systems supported\n                (finite number of solutions)\n                '''))\n\n        gens = f.gens\n        gen = gens[-1]\n\n        zeros = list(roots(f.ltrim(gen)).keys())", "answer": "A"}
{"uuid": "61c431cc-7645-4c10-8c37-cfeb7781d36d", "issue_statement": "simpify gives `Imaginary coordinates are not permitted.` with evaluate(False)\n## Issue\r\n`with evaluate(False)` crashes unexpectedly with `Point2D`\r\n\r\n## Code\r\n```python\r\nimport sympy as sp\r\nwith sp.evaluate(False):\r\n  sp.S('Point2D(Integer(1),Integer(2))')\r\n```\r\n\r\n## Error\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/avinash/.local/lib/python3.8/site-packages/sympy/core/sympify.py\", line 472, in sympify\r\n    expr = parse_expr(a, local_dict=locals, transformations=transformations, evaluate=evaluate)\r\n  File \"/home/avinash/.local/lib/python3.8/site-packages/sympy/parsing/sympy_parser.py\", line 1026, in parse_expr\r\n    raise e from ValueError(f\"Error from parse_expr with transformed code: {code!r}\")\r\n  File \"/home/avinash/.local/lib/python3.8/site-packages/sympy/parsing/sympy_parser.py\", line 1017, in parse_expr\r\n    rv = eval_expr(code, local_dict, global_dict)\r\n  File \"/home/avinash/.local/lib/python3.8/site-packages/sympy/parsing/sympy_parser.py\", line 911, in eval_expr\r\n    expr = eval(\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/avinash/.local/lib/python3.8/site-packages/sympy/geometry/point.py\", line 912, in __new__\r\n    args = Point(*args, **kwargs)\r\n  File \"/home/avinash/.local/lib/python3.8/site-packages/sympy/geometry/point.py\", line 153, in __new__\r\n    raise ValueError('Imaginary coordinates are not permitted.')\r\nValueError: Imaginary coordinates are not permitted.\r\n```\r\n\r\nHowever, it works without `with evaluate(False)`. Both of following commands work\r\n```python\r\nsp.S('Point2D(Integer(1),Integer(2))')\r\nsp.S('Point2D(Integer(1),Integer(2))', evaluate=False)\r\n```", "choices": "(A)             raise ValueError('Nonzero coordinates cannot be removed.')\n        if any(a.is_number and im(a) for a in coords):\n            raise ValueError('Imaginary coordinates are not permitted.')\n        if not all(isinstance(a, Expr) for a in coords):\n            raise TypeError('Coordinates must be valid SymPy expressions.')\n\n        # pad with zeros appropriately\n(B)             raise ValueError('Imaginary coordinates are not permitted.')\n        if not all(isinstance(a, Expr) for a in coords):\n            raise TypeError('Coordinates must be valid SymPy expressions.')\n\n        # pad with zeros appropriately\n        coords = coords[:dim] + (S.Zero,)*(dim - len(coords))\n\n(C)                         'warn' or 'ignore'.'''))\n        if any(coords[dim:]):\n            raise ValueError('Nonzero coordinates cannot be removed.')\n        if any(a.is_number and im(a) for a in coords):\n            raise ValueError('Imaginary coordinates are not permitted.')\n        if not all(isinstance(a, Expr) for a in coords):\n            raise TypeError('Coordinates must be valid SymPy expressions.')\n(D)                 raise ValueError(filldedent('''\n                        on_morph value should be 'error',\n                        'warn' or 'ignore'.'''))\n        if any(coords[dim:]):\n            raise ValueError('Nonzero coordinates cannot be removed.')\n        if any(a.is_number and im(a) for a in coords):\n            raise ValueError('Imaginary coordinates are not permitted.')", "answer": "C"}
{"uuid": "046633fd-3273-45fd-9fd4-e6eb7e791833", "issue_statement": "cse() has strange behaviour for MatrixSymbol indexing\nExample: \r\n```python\r\nimport sympy as sp\r\nfrom pprint import pprint\r\n\r\n\r\ndef sub_in_matrixsymbols(exp, matrices):\r\n    for matrix in matrices:\r\n        for i in range(matrix.shape[0]):\r\n            for j in range(matrix.shape[1]):\r\n                name = \"%s_%d_%d\" % (matrix.name, i, j)\r\n                sym = sp.symbols(name)\r\n                exp = exp.subs(sym, matrix[i, j])\r\n    return exp\r\n\r\n\r\ndef t44(name):\r\n    return sp.Matrix(4, 4, lambda i, j: sp.symbols('%s_%d_%d' % (name, i, j)))\r\n\r\n\r\n# Construct matrices of symbols that work with our\r\n# expressions. (MatrixSymbols does not.)\r\na = t44(\"a\")\r\nb = t44(\"b\")\r\n\r\n# Set up expression. This is a just a simple example.\r\ne = a * b\r\n\r\n# Put in matrixsymbols. (Gives array-input in codegen.)\r\ne2 = sub_in_matrixsymbols(e, [sp.MatrixSymbol(\"a\", 4, 4), sp.MatrixSymbol(\"b\", 4, 4)])\r\ncse_subs, cse_reduced = sp.cse(e2)\r\npprint((cse_subs, cse_reduced))\r\n\r\n# Codegen, etc..\r\nprint \"\\nccode:\"\r\nfor sym, expr in cse_subs:\r\n    constants, not_c, c_expr = sympy.printing.ccode(\r\n        expr,\r\n        human=False,\r\n        assign_to=sympy.printing.ccode(sym),\r\n    )\r\n    assert not constants, constants\r\n    assert not not_c, not_c\r\n    print \"%s\\n\" % c_expr\r\n\r\n```\r\n\r\nThis gives the following output:\r\n\r\n```\r\n([(x0, a),\r\n  (x1, x0[0, 0]),\r\n  (x2, b),\r\n  (x3, x2[0, 0]),\r\n  (x4, x0[0, 1]),\r\n  (x5, x2[1, 0]),\r\n  (x6, x0[0, 2]),\r\n  (x7, x2[2, 0]),\r\n  (x8, x0[0, 3]),\r\n  (x9, x2[3, 0]),\r\n  (x10, x2[0, 1]),\r\n  (x11, x2[1, 1]),\r\n  (x12, x2[2, 1]),\r\n  (x13, x2[3, 1]),\r\n  (x14, x2[0, 2]),\r\n  (x15, x2[1, 2]),\r\n  (x16, x2[2, 2]),\r\n  (x17, x2[3, 2]),\r\n  (x18, x2[0, 3]),\r\n  (x19, x2[1, 3]),\r\n  (x20, x2[2, 3]),\r\n  (x21, x2[3, 3]),\r\n  (x22, x0[1, 0]),\r\n  (x23, x0[1, 1]),\r\n  (x24, x0[1, 2]),\r\n  (x25, x0[1, 3]),\r\n  (x26, x0[2, 0]),\r\n  (x27, x0[2, 1]),\r\n  (x28, x0[2, 2]),\r\n  (x29, x0[2, 3]),\r\n  (x30, x0[3, 0]),\r\n  (x31, x0[3, 1]),\r\n  (x32, x0[3, 2]),\r\n  (x33, x0[3, 3])],\r\n [Matrix([\r\n[    x1*x3 + x4*x5 + x6*x7 + x8*x9,     x1*x10 + x11*x4 + x12*x6 + x13*x8,     x1*x14 + x15*x4 + x16*x6 + x17*x8,     x1*x18 + x19*x4 + x20*x6 + x21*x8],\r\n[x22*x3 + x23*x5 + x24*x7 + x25*x9, x10*x22 + x11*x23 + x12*x24 + x13*x25, x14*x22 + x15*x23 + x16*x24 + x17*x25, x18*x22 + x19*x23 + x20*x24 + x21*x25],\r\n[x26*x3 + x27*x5 + x28*x7 + x29*x9, x10*x26 + x11*x27 + x12*x28 + x13*x29, x14*x26 + x15*x27 + x16*x28 + x17*x29, x18*x26 + x19*x27 + x20*x28 + x21*x29],\r\n[x3*x30 + x31*x5 + x32*x7 + x33*x9, x10*x30 + x11*x31 + x12*x32 + x13*x33, x14*x30 + x15*x31 + x16*x32 + x17*x33, x18*x30 + x19*x31 + x20*x32 + x21*x33]])])\r\n\r\nccode:\r\nx0[0] = a[0];\r\nx0[1] = a[1];\r\nx0[2] = a[2];\r\nx0[3] = a[3];\r\nx0[4] = a[4];\r\nx0[5] = a[5];\r\nx0[6] = a[6];\r\nx0[7] = a[7];\r\nx0[8] = a[8];\r\nx0[9] = a[9];\r\nx0[10] = a[10];\r\nx0[11] = a[11];\r\nx0[12] = a[12];\r\nx0[13] = a[13];\r\nx0[14] = a[14];\r\nx0[15] = a[15];\r\nx1 = x0[0];\r\nx2[0] = b[0];\r\nx2[1] = b[1];\r\nx2[2] = b[2];\r\nx2[3] = b[3];\r\nx2[4] = b[4];\r\nx2[5] = b[5];\r\nx2[6] = b[6];\r\nx2[7] = b[7];\r\nx2[8] = b[8];\r\nx2[9] = b[9];\r\nx2[10] = b[10];\r\nx2[11] = b[11];\r\nx2[12] = b[12];\r\nx2[13] = b[13];\r\nx2[14] = b[14];\r\nx2[15] = b[15];\r\nx3 = x2[0];\r\nx4 = x0[1];\r\nx5 = x2[4];\r\nx6 = x0[2];\r\nx7 = x2[8];\r\nx8 = x0[3];\r\nx9 = x2[12];\r\nx10 = x2[1];\r\nx11 = x2[5];\r\nx12 = x2[9];\r\nx13 = x2[13];\r\nx14 = x2[2];\r\nx15 = x2[6];\r\nx16 = x2[10];\r\nx17 = x2[14];\r\nx18 = x2[3];\r\nx19 = x2[7];\r\nx20 = x2[11];\r\nx21 = x2[15];\r\nx22 = x0[4];\r\nx23 = x0[5];\r\nx24 = x0[6];\r\nx25 = x0[7];\r\nx26 = x0[8];\r\nx27 = x0[9];\r\nx28 = x0[10];\r\nx29 = x0[11];\r\nx30 = x0[12];\r\nx31 = x0[13];\r\nx32 = x0[14];\r\nx33 = x0[15];\r\n```\r\n\r\n`x0` and `x2` are just copies of the matrices `a` and `b`, respectively.", "choices": "(A)     ## Find repeated sub-expressions\n\n    to_eliminate = set()\n\n    seen_subexp = set()\n    excluded_symbols = set()\n\n    def _find_repeated(expr):\n        if not isinstance(expr, (Basic, Unevaluated)):\n            return\n\n        if isinstance(expr, RootOf):\n            return\n\n        if isinstance(expr, Basic) and (expr.is_Atom or expr.is_Order):\n            if expr.is_Symbol:\n                excluded_symbols.add(expr)\n            return\n\n        if iterable(expr):\n            args = expr\n\n        else:\n            if expr in seen_subexp:\n                for ign in ignore:\n                    if ign in expr.free_symbols:\n                        break\n                else:\n                    to_eliminate.add(expr)\n                    return\n(B)     def _find_repeated(expr):\n        if not isinstance(expr, (Basic, Unevaluated)):\n            return\n\n        if isinstance(expr, RootOf):\n            return\n\n        if isinstance(expr, Basic) and (expr.is_Atom or expr.is_Order):\n            if expr.is_Symbol:\n                excluded_symbols.add(expr)\n            return\n\n        if iterable(expr):\n            args = expr\n\n        else:\n            if expr in seen_subexp:\n                for ign in ignore:\n                    if ign in expr.free_symbols:\n                        break\n                else:\n                    to_eliminate.add(expr)\n                    return\n\n            seen_subexp.add(expr)\n\n            if expr in opt_subs:\n                expr = opt_subs[expr]\n\n            args = expr.args\n(C)         The symbols used to label the common subexpressions which are pulled\n        out.\n    opt_subs : dictionary of expression substitutions\n        The expressions to be substituted before any CSE action is performed.\n    order : string, 'none' or 'canonical'\n        The order by which Mul and Add arguments are processed. For large\n        expressions where speed is a concern, use the setting order='none'.\n    ignore : iterable of Symbols\n        Substitutions containing any Symbol from ``ignore`` will be ignored.\n    \"\"\"\n    from sympy.matrices.expressions import MatrixExpr, MatrixSymbol, MatMul, MatAdd\n    from sympy.polys.rootoftools import RootOf\n\n    if opt_subs is None:\n        opt_subs = dict()\n\n    ## Find repeated sub-expressions\n\n    to_eliminate = set()\n\n    seen_subexp = set()\n    excluded_symbols = set()\n\n    def _find_repeated(expr):\n        if not isinstance(expr, (Basic, Unevaluated)):\n            return\n\n        if isinstance(expr, RootOf):\n            return\n\n(D)         Substitutions containing any Symbol from ``ignore`` will be ignored.\n    \"\"\"\n    from sympy.matrices.expressions import MatrixExpr, MatrixSymbol, MatMul, MatAdd\n    from sympy.polys.rootoftools import RootOf\n\n    if opt_subs is None:\n        opt_subs = dict()\n\n    ## Find repeated sub-expressions\n\n    to_eliminate = set()\n\n    seen_subexp = set()\n    excluded_symbols = set()\n\n    def _find_repeated(expr):\n        if not isinstance(expr, (Basic, Unevaluated)):\n            return\n\n        if isinstance(expr, RootOf):\n            return\n\n        if isinstance(expr, Basic) and (expr.is_Atom or expr.is_Order):\n            if expr.is_Symbol:\n                excluded_symbols.add(expr)\n            return\n\n        if iterable(expr):\n            args = expr\n", "answer": "D"}
{"uuid": "626353d5-3c72-49d7-94de-9d48ed7b3c5f", "issue_statement": "sympy.Array([]) fails, while sympy.Matrix([]) works\nSymPy 1.4 does not allow to construct empty Array (see code below). Is this the intended behavior?\r\n\r\n```\r\n>>> import sympy\r\nKeyboardInterrupt\r\n>>> import sympy\r\n>>> from sympy import Array\r\n>>> sympy.__version__\r\n'1.4'\r\n>>> a = Array([])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/hcui7/miniconda3/envs/a/lib/python3.7/site-packages/sympy/tensor/array/dense_ndim_array.py\", line 130, in __new__\r\n    return cls._new(iterable, shape, **kwargs)\r\n  File \"/Users/hcui7/miniconda3/envs/a/lib/python3.7/site-packages/sympy/tensor/array/dense_ndim_array.py\", line 136, in _new\r\n    shape, flat_list = cls._handle_ndarray_creation_inputs(iterable, shape, **kwargs)\r\n  File \"/Users/hcui7/miniconda3/envs/a/lib/python3.7/site-packages/sympy/tensor/array/ndim_array.py\", line 142, in _handle_ndarray_creation_inputs\r\n    iterable, shape = cls._scan_iterable_shape(iterable)\r\n  File \"/Users/hcui7/miniconda3/envs/a/lib/python3.7/site-packages/sympy/tensor/array/ndim_array.py\", line 127, in _scan_iterable_shape\r\n    return f(iterable)\r\n  File \"/Users/hcui7/miniconda3/envs/a/lib/python3.7/site-packages/sympy/tensor/array/ndim_array.py\", line 120, in f\r\n    elems, shapes = zip(*[f(i) for i in pointer])\r\nValueError: not enough values to unpack (expected 2, got 0)\r\n```\r\n\r\n@czgdp1807", "choices": "(A)     >>> from sympy import NDimArray\n    >>> NDimArray([1,2,3]).kind\n    ArrayKind(NumberKind)\n\n    Although expressions representing an array may be not instance of\n    array class, it will have ``ArrayKind`` as well.\n\n    >>> from sympy import Integral\n    >>> from sympy.tensor.array import NDimArray\n    >>> from sympy.abc import x\n    >>> intA = Integral(NDimArray([1,2,3]), x)\n    >>> isinstance(intA, NDimArray)\n    False\n    >>> intA.kind\n    ArrayKind(NumberKind)\n\n    Use ``isinstance()`` to check for ``ArrayKind` without specifying\n    the element kind. Use ``is`` with specifying the element kind.\n\n    >>> from sympy.tensor.array import ArrayKind\n    >>> from sympy.core import NumberKind\n    >>> boolA = NDimArray([True, False])\n    >>> isinstance(boolA.kind, ArrayKind)\n    True\n    >>> boolA.kind is ArrayKind(NumberKind)\n    False\n\n    See Also\n    ========\n\n    shape : Function to return the shape of objects with ``MatrixKind``.\n\n    \"\"\"\n    def __new__(cls, element_kind=NumberKind):\n        obj = super().__new__(cls, element_kind)\n        obj.element_kind = element_kind\n        return obj\n\n    def __repr__(self):\n        return \"ArrayKind(%s)\" % self.element_kind\n\n    @classmethod\n    def _union(cls, kinds) -> 'ArrayKind':\n        elem_kinds = set(e.kind for e in kinds)\n        if len(elem_kinds) == 1:\n            elemkind, = elem_kinds\n        else:\n            elemkind = UndefinedKind\n        return ArrayKind(elemkind)\n\n\nclass NDimArray(Printable):\n    \"\"\"\n\n    Examples\n    ========\n\n    Create an N-dim array of zeros:\n\n    >>> from sympy import MutableDenseNDimArray\n    >>> a = MutableDenseNDimArray.zeros(2, 3, 4)\n    >>> a\n    [[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]\n\n    Create an N-dim array from a list;\n\n    >>> a = MutableDenseNDimArray([[2, 3], [4, 5]])\n    >>> a\n    [[2, 3], [4, 5]]\n\n    >>> b = MutableDenseNDimArray([[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11, 12]]])\n    >>> b\n    [[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11, 12]]]\n\n    Create an N-dim array from a flat list with dimension shape:\n\n    >>> a = MutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    >>> a\n    [[1, 2, 3], [4, 5, 6]]\n\n    Create an N-dim array from a matrix:\n\n    >>> from sympy import Matrix\n    >>> a = Matrix([[1,2],[3,4]])\n    >>> a\n    Matrix([\n    [1, 2],\n    [3, 4]])\n    >>> b = MutableDenseNDimArray(a)\n    >>> b\n    [[1, 2], [3, 4]]\n\n    Arithmetic operations on N-dim arrays\n\n    >>> a = MutableDenseNDimArray([1, 1, 1, 1], (2, 2))\n    >>> b = MutableDenseNDimArray([4, 4, 4, 4], (2, 2))\n    >>> c = a + b\n    >>> c\n    [[5, 5], [5, 5]]\n    >>> a - b\n    [[-3, -3], [-3, -3]]\n\n    \"\"\"\n\n    _diff_wrt = True\n    is_scalar = False\n\n    def __new__(cls, iterable, shape=None, **kwargs):\n        from sympy.tensor.array import ImmutableDenseNDimArray\n        return ImmutableDenseNDimArray(iterable, shape, **kwargs)\n\n    def _parse_index(self, index):\n        if isinstance(index, (SYMPY_INTS, Integer)):\n            raise ValueError(\"Only a tuple index is accepted\")\n\n        if self._loop_size == 0:\n            raise ValueError(\"Index not valide with an empty array\")\n\n        if len(index) != self._rank:\n            raise ValueError('Wrong number of array axes')\n\n        real_index = 0\n        # check if input index can exist in current indexing\n        for i in range(self._rank):\n            if (index[i] >= self.shape[i]) or (index[i] < -self.shape[i]):\n                raise ValueError('Index ' + str(index) + ' out of border')\n            if index[i] < 0:\n                real_index += 1\n            real_index = real_index*self.shape[i] + index[i]\n\n        return real_index\n\n    def _get_tuple_index(self, integer_index):\n        index = []\n        for i, sh in enumerate(reversed(self.shape)):\n            index.append(integer_index % sh)\n            integer_index //= sh\n        index.reverse()\n        return tuple(index)\n\n    def _check_symbolic_index(self, index):\n        # Check if any index is symbolic:\n        tuple_index = (index if isinstance(index, tuple) else (index,))\n        if any((isinstance(i, Expr) and (not i.is_number)) for i in tuple_index):\n            for i, nth_dim in zip(tuple_index, self.shape):\n                if ((i < 0) == True) or ((i >= nth_dim) == True):\n                    raise ValueError(\"index out of range\")\n            from sympy.tensor import Indexed\n            return Indexed(self, *tuple_index)\n        return None\n\n    def _setter_iterable_check(self, value):\n        from sympy.matrices.matrices import MatrixBase\n        if isinstance(value, (Iterable, MatrixBase, NDimArray)):\n            raise NotImplementedError\n\n    @classmethod\n    def _scan_iterable_shape(cls, iterable):\n        def f(pointer):\n            if not isinstance(pointer, Iterable):\n                return [pointer], ()\n\n            result = []\n            elems, shapes = zip(*[f(i) for i in pointer])\n            if len(set(shapes)) != 1:\n                raise ValueError(\"could not determine shape unambiguously\")\n            for i in elems:\n                result.extend(i)\n            return result, (len(shapes),)+shapes[0]\n\n        return f(iterable)\n\n    @classmethod\n    def _handle_ndarray_creation_inputs(cls, iterable=None, shape=None, **kwargs):\n        from sympy.matrices.matrices import MatrixBase\n        from sympy.tensor.array import SparseNDimArray\n\n        if shape is None:\n            if iterable is None:\n                shape = ()\n                iterable = ()\n            # Construction of a sparse array from a sparse array\n            elif isinstance(iterable, SparseNDimArray):\n                return iterable._shape, iterable._sparse_array\n\n            # Construct N-dim array from another N-dim array:\n            elif isinstance(iterable, NDimArray):\n                shape = iterable.shape\n\n            # Construct N-dim array from an iterable (numpy arrays included):\n            elif isinstance(iterable, Iterable):\n                iterable, shape = cls._scan_iterable_shape(iterable)\n\n            # Construct N-dim array from a Matrix:\n            elif isinstance(iterable, MatrixBase):\n                shape = iterable.shape\n\n            else:\n                shape = ()\n                iterable = (iterable,)\n\n        if isinstance(iterable, (Dict, dict)) and shape is not None:\n            new_dict = iterable.copy()\n            for k, v in new_dict.items():\n                if isinstance(k, (tuple, Tuple)):\n                    new_key = 0\n                    for i, idx in enumerate(k):\n                        new_key = new_key * shape[i] + idx\n                    iterable[new_key] = iterable[k]\n                    del iterable[k]\n\n        if isinstance(shape, (SYMPY_INTS, Integer)):\n            shape = (shape,)\n\n        if not all(isinstance(dim, (SYMPY_INTS, Integer)) for dim in shape):\n            raise TypeError(\"Shape should contain integers only.\")\n\n        return tuple(shape), iterable\n\n    def __len__(self):\n        \"\"\"Overload common function len(). Returns number of elements in array.\n\n        Examples\n        ========\n\n        >>> from sympy import MutableDenseNDimArray\n        >>> a = MutableDenseNDimArray.zeros(3, 3)\n        >>> a\n        [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n        >>> len(a)\n        9\n\n        \"\"\"\n        return self._loop_size\n\n    @property\n    def shape(self):\n        \"\"\"\n        Returns array shape (dimension).\n\n        Examples\n        ========\n\n        >>> from sympy import MutableDenseNDimArray\n        >>> a = MutableDenseNDimArray.zeros(3, 3)\n        >>> a.shape\n        (3, 3)\n\n        \"\"\"\n        return self._shape\n\n    def rank(self):\n        \"\"\"\n        Returns rank of array.\n\n        Examples\n        ========\n\n        >>> from sympy import MutableDenseNDimArray\n        >>> a = MutableDenseNDimArray.zeros(3,4,5,6,3)\n        >>> a.rank()\n        5\n\n        \"\"\"\n        return self._rank\n\n    def diff(self, *args, **kwargs):\n        \"\"\"\n        Calculate the derivative of each element in the array.\n\n        Examples\n        ========\n\n        >>> from sympy import ImmutableDenseNDimArray\n        >>> from sympy.abc import x, y\n        >>> M = ImmutableDenseNDimArray([[x, y], [1, x*y]])\n        >>> M.diff(x)\n        [[1, 0], [0, y]]\n\n        \"\"\"\n        from sympy.tensor.array.array_derivatives import ArrayDerivative\n        kwargs.setdefault('evaluate', True)\n        return ArrayDerivative(self.as_immutable(), *args, **kwargs)\n\n    def _eval_derivative(self, base):\n        # Types are (base: scalar, self: array)\n        return self.applyfunc(lambda x: base.diff(x))\n\n    def _eval_derivative_n_times(self, s, n):\n        return Basic._eval_derivative_n_times(self, s, n)\n\n    def applyfunc(self, f):\n        \"\"\"Apply a function to each element of the N-dim array.\n\n        Examples\n        ========\n\n        >>> from sympy import ImmutableDenseNDimArray\n        >>> m = ImmutableDenseNDimArray([i*2+j for i in range(2) for j in range(2)], (2, 2))\n        >>> m\n        [[0, 1], [2, 3]]\n        >>> m.applyfunc(lambda i: 2*i)\n        [[0, 2], [4, 6]]\n        \"\"\"\n        from sympy.tensor.array import SparseNDimArray\n        from sympy.tensor.array.arrayop import Flatten\n\n        if isinstance(self, SparseNDimArray) and f(S.Zero) == 0:\n            return type(self)({k: f(v) for k, v in self._sparse_array.items() if f(v) != 0}, self.shape)\n\n        return type(self)(map(f, Flatten(self)), self.shape)\n\n    def _sympystr(self, printer):\n        def f(sh, shape_left, i, j):\n            if len(shape_left) == 1:\n                return \"[\"+\", \".join([printer._print(self[self._get_tuple_index(e)]) for e in range(i, j)])+\"]\"\n\n            sh //= shape_left[0]\n            return \"[\" + \", \".join([f(sh, shape_left[1:], i+e*sh, i+(e+1)*sh) for e in range(shape_left[0])]) + \"]\" # + \"\\n\"*len(shape_left)\n\n        if self.rank() == 0:\n            return printer._print(self[()])\n\n        return f(self._loop_size, self.shape, 0, self._loop_size)\n\n    def tolist(self):\n        \"\"\"\n        Converting MutableDenseNDimArray to one-dim list\n\n        Examples\n        ========\n\n        >>> from sympy import MutableDenseNDimArray\n        >>> a = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n        >>> a\n        [[1, 2], [3, 4]]\n        >>> b = a.tolist()\n        >>> b\n        [[1, 2], [3, 4]]\n        \"\"\"\n\n        def f(sh, shape_left, i, j):\n            if len(shape_left) == 1:\n                return [self[self._get_tuple_index(e)] for e in range(i, j)]\n            result = []\n            sh //= shape_left[0]\n            for e in range(shape_left[0]):\n                result.append(f(sh, shape_left[1:], i+e*sh, i+(e+1)*sh))\n            return result\n\n        return f(self._loop_size, self.shape, 0, self._loop_size)\n\n    def __add__(self, other):\n        from sympy.tensor.array.arrayop import Flatten\n\n        if not isinstance(other, NDimArray):\n            return NotImplemented\n\n        if self.shape != other.shape:\n            raise ValueError(\"array shape mismatch\")\n        result_list = [i+j for i,j in zip(Flatten(self), Flatten(other))]\n\n        return type(self)(result_list, self.shape)\n\n    def __sub__(self, other):\n        from sympy.tensor.array.arrayop import Flatten\n\n        if not isinstance(other, NDimArray):\n            return NotImplemented\n\n        if self.shape != other.shape:\n            raise ValueError(\"array shape mismatch\")\n        result_list = [i-j for i,j in zip(Flatten(self), Flatten(other))]\n\n        return type(self)(result_list, self.shape)\n\n    def __mul__(self, other):\n        from sympy.matrices.matrices import MatrixBase\n        from sympy.tensor.array import SparseNDimArray\n        from sympy.tensor.array.arrayop import Flatten\n\n        if isinstance(other, (Iterable, NDimArray, MatrixBase)):\n            raise ValueError(\"scalar expected, use tensorproduct(...) for tensorial product\")\n\n        other = sympify(other)\n        if isinstance(self, SparseNDimArray):\n            if other.is_zero:\n                return type(self)({}, self.shape)\n            return type(self)({k: other*v for (k, v) in self._sparse_array.items()}, self.shape)\n\n        result_list = [i*other for i in Flatten(self)]\n        return type(self)(result_list, self.shape)\n\n    def __rmul__(self, other):\n        from sympy.matrices.matrices import MatrixBase\n        from sympy.tensor.array import SparseNDimArray\n        from sympy.tensor.array.arrayop import Flatten\n\n        if isinstance(other, (Iterable, NDimArray, MatrixBase)):\n            raise ValueError(\"scalar expected, use tensorproduct(...) for tensorial product\")\n\n        other = sympify(other)\n        if isinstance(self, SparseNDimArray):\n            if other.is_zero:\n                return type(self)({}, self.shape)\n            return type(self)({k: other*v for (k, v) in self._sparse_array.items()}, self.shape)\n\n        result_list = [other*i for i in Flatten(self)]\n        return type(self)(result_list, self.shape)\n\n    def __truediv__(self, other):\n        from sympy.matrices.matrices import MatrixBase\n        from sympy.tensor.array import SparseNDimArray\n        from sympy.tensor.array.arrayop import Flatten\n\n        if isinstance(other, (Iterable, NDimArray, MatrixBase)):\n            raise ValueError(\"scalar expected\")\n\n        other = sympify(other)\n        if isinstance(self, SparseNDimArray) and other != S.Zero:\n            return type(self)({k: v/other for (k, v) in self._sparse_array.items()}, self.shape)\n\n        result_list = [i/other for i in Flatten(self)]\n        return type(self)(result_list, self.shape)\n\n    def __rtruediv__(self, other):\n        raise NotImplementedError('unsupported operation on NDimArray')\n\n    def __neg__(self):\n        from sympy.tensor.array import SparseNDimArray\n        from sympy.tensor.array.arrayop import Flatten\n\n        if isinstance(self, SparseNDimArray):\n            return type(self)({k: -v for (k, v) in self._sparse_array.items()}, self.shape)\n\n        result_list = [-i for i in Flatten(self)]\n        return type(self)(result_list, self.shape)\n\n(B) \n    def _parse_index(self, index):\n        if isinstance(index, (SYMPY_INTS, Integer)):\n            raise ValueError(\"Only a tuple index is accepted\")\n\n        if self._loop_size == 0:\n            raise ValueError(\"Index not valide with an empty array\")\n\n        if len(index) != self._rank:\n            raise ValueError('Wrong number of array axes')\n\n        real_index = 0\n        # check if input index can exist in current indexing\n        for i in range(self._rank):\n            if (index[i] >= self.shape[i]) or (index[i] < -self.shape[i]):\n                raise ValueError('Index ' + str(index) + ' out of border')\n            if index[i] < 0:\n                real_index += 1\n            real_index = real_index*self.shape[i] + index[i]\n\n        return real_index\n\n    def _get_tuple_index(self, integer_index):\n        index = []\n        for i, sh in enumerate(reversed(self.shape)):\n            index.append(integer_index % sh)\n            integer_index //= sh\n        index.reverse()\n        return tuple(index)\n\n    def _check_symbolic_index(self, index):\n        # Check if any index is symbolic:\n        tuple_index = (index if isinstance(index, tuple) else (index,))\n        if any((isinstance(i, Expr) and (not i.is_number)) for i in tuple_index):\n            for i, nth_dim in zip(tuple_index, self.shape):\n                if ((i < 0) == True) or ((i >= nth_dim) == True):\n                    raise ValueError(\"index out of range\")\n            from sympy.tensor import Indexed\n            return Indexed(self, *tuple_index)\n        return None\n\n    def _setter_iterable_check(self, value):\n        from sympy.matrices.matrices import MatrixBase\n        if isinstance(value, (Iterable, MatrixBase, NDimArray)):\n            raise NotImplementedError\n\n    @classmethod\n    def _scan_iterable_shape(cls, iterable):\n        def f(pointer):\n            if not isinstance(pointer, Iterable):\n                return [pointer], ()\n\n            result = []\n            elems, shapes = zip(*[f(i) for i in pointer])\n            if len(set(shapes)) != 1:\n                raise ValueError(\"could not determine shape unambiguously\")\n            for i in elems:\n                result.extend(i)\n            return result, (len(shapes),)+shapes[0]\n\n        return f(iterable)\n\n    @classmethod\n    def _handle_ndarray_creation_inputs(cls, iterable=None, shape=None, **kwargs):\n        from sympy.matrices.matrices import MatrixBase\n        from sympy.tensor.array import SparseNDimArray\n\n        if shape is None:\n            if iterable is None:\n                shape = ()\n                iterable = ()\n            # Construction of a sparse array from a sparse array\n            elif isinstance(iterable, SparseNDimArray):\n                return iterable._shape, iterable._sparse_array\n\n            # Construct N-dim array from another N-dim array:\n            elif isinstance(iterable, NDimArray):\n                shape = iterable.shape\n\n            # Construct N-dim array from an iterable (numpy arrays included):\n            elif isinstance(iterable, Iterable):\n                iterable, shape = cls._scan_iterable_shape(iterable)\n\n            # Construct N-dim array from a Matrix:\n            elif isinstance(iterable, MatrixBase):\n                shape = iterable.shape\n\n            else:\n                shape = ()\n                iterable = (iterable,)\n\n        if isinstance(iterable, (Dict, dict)) and shape is not None:\n            new_dict = iterable.copy()\n            for k, v in new_dict.items():\n                if isinstance(k, (tuple, Tuple)):\n                    new_key = 0\n                    for i, idx in enumerate(k):\n                        new_key = new_key * shape[i] + idx\n                    iterable[new_key] = iterable[k]\n                    del iterable[k]\n\n        if isinstance(shape, (SYMPY_INTS, Integer)):\n            shape = (shape,)\n\n        if not all(isinstance(dim, (SYMPY_INTS, Integer)) for dim in shape):\n            raise TypeError(\"Shape should contain integers only.\")\n\n        return tuple(shape), iterable\n\n    def __len__(self):\n        \"\"\"Overload common function len(). Returns number of elements in array.\n\n        Examples\n        ========\n\n        >>> from sympy import MutableDenseNDimArray\n        >>> a = MutableDenseNDimArray.zeros(3, 3)\n        >>> a\n        [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n        >>> len(a)\n        9\n\n        \"\"\"\n        return self._loop_size\n\n    @property\n    def shape(self):\n        \"\"\"\n        Returns array shape (dimension).\n\n        Examples\n        ========\n\n        >>> from sympy import MutableDenseNDimArray\n        >>> a = MutableDenseNDimArray.zeros(3, 3)\n        >>> a.shape\n        (3, 3)\n\n        \"\"\"\n        return self._shape\n\n    def rank(self):\n        \"\"\"\n        Returns rank of array.\n\n        Examples\n        ========\n\n        >>> from sympy import MutableDenseNDimArray\n        >>> a = MutableDenseNDimArray.zeros(3,4,5,6,3)\n        >>> a.rank()\n        5\n\n        \"\"\"\n        return self._rank\n\n    def diff(self, *args, **kwargs):\n        \"\"\"\n        Calculate the derivative of each element in the array.\n\n        Examples\n        ========\n\n        >>> from sympy import ImmutableDenseNDimArray\n        >>> from sympy.abc import x, y\n        >>> M = ImmutableDenseNDimArray([[x, y], [1, x*y]])\n        >>> M.diff(x)\n        [[1, 0], [0, y]]\n\n        \"\"\"\n        from sympy.tensor.array.array_derivatives import ArrayDerivative\n        kwargs.setdefault('evaluate', True)\n        return ArrayDerivative(self.as_immutable(), *args, **kwargs)\n\n    def _eval_derivative(self, base):\n        # Types are (base: scalar, self: array)\n        return self.applyfunc(lambda x: base.diff(x))\n\n    def _eval_derivative_n_times(self, s, n):\n        return Basic._eval_derivative_n_times(self, s, n)\n\n    def applyfunc(self, f):\n        \"\"\"Apply a function to each element of the N-dim array.\n\n        Examples\n        ========\n\n        >>> from sympy import ImmutableDenseNDimArray\n        >>> m = ImmutableDenseNDimArray([i*2+j for i in range(2) for j in range(2)], (2, 2))\n        >>> m\n        [[0, 1], [2, 3]]\n        >>> m.applyfunc(lambda i: 2*i)\n        [[0, 2], [4, 6]]\n        \"\"\"\n        from sympy.tensor.array import SparseNDimArray\n        from sympy.tensor.array.arrayop import Flatten\n\n        if isinstance(self, SparseNDimArray) and f(S.Zero) == 0:\n            return type(self)({k: f(v) for k, v in self._sparse_array.items() if f(v) != 0}, self.shape)\n\n        return type(self)(map(f, Flatten(self)), self.shape)\n\n    def _sympystr(self, printer):\n        def f(sh, shape_left, i, j):\n            if len(shape_left) == 1:\n                return \"[\"+\", \".join([printer._print(self[self._get_tuple_index(e)]) for e in range(i, j)])+\"]\"\n\n            sh //= shape_left[0]\n            return \"[\" + \", \".join([f(sh, shape_left[1:], i+e*sh, i+(e+1)*sh) for e in range(shape_left[0])]) + \"]\" # + \"\\n\"*len(shape_left)\n\n        if self.rank() == 0:\n            return printer._print(self[()])\n\n        return f(self._loop_size, self.shape, 0, self._loop_size)\n\n    def tolist(self):\n        \"\"\"\n        Converting MutableDenseNDimArray to one-dim list\n\n        Examples\n        ========\n\n        >>> from sympy import MutableDenseNDimArray\n        >>> a = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n        >>> a\n        [[1, 2], [3, 4]]\n        >>> b = a.tolist()\n        >>> b\n        [[1, 2], [3, 4]]\n        \"\"\"\n\n        def f(sh, shape_left, i, j):\n            if len(shape_left) == 1:\n                return [self[self._get_tuple_index(e)] for e in range(i, j)]\n            result = []\n            sh //= shape_left[0]\n            for e in range(shape_left[0]):\n                result.append(f(sh, shape_left[1:], i+e*sh, i+(e+1)*sh))\n            return result\n\n        return f(self._loop_size, self.shape, 0, self._loop_size)\n\n    def __add__(self, other):\n        from sympy.tensor.array.arrayop import Flatten\n\n        if not isinstance(other, NDimArray):\n            return NotImplemented\n\n        if self.shape != other.shape:\n            raise ValueError(\"array shape mismatch\")\n        result_list = [i+j for i,j in zip(Flatten(self), Flatten(other))]\n\n        return type(self)(result_list, self.shape)\n\n    def __sub__(self, other):\n        from sympy.tensor.array.arrayop import Flatten\n\n        if not isinstance(other, NDimArray):\n            return NotImplemented\n\n        if self.shape != other.shape:\n            raise ValueError(\"array shape mismatch\")\n        result_list = [i-j for i,j in zip(Flatten(self), Flatten(other))]\n\n        return type(self)(result_list, self.shape)\n\n    def __mul__(self, other):\n        from sympy.matrices.matrices import MatrixBase\n        from sympy.tensor.array import SparseNDimArray\n        from sympy.tensor.array.arrayop import Flatten\n\n        if isinstance(other, (Iterable, NDimArray, MatrixBase)):\n            raise ValueError(\"scalar expected, use tensorproduct(...) for tensorial product\")\n\n        other = sympify(other)\n        if isinstance(self, SparseNDimArray):\n            if other.is_zero:\n                return type(self)({}, self.shape)\n            return type(self)({k: other*v for (k, v) in self._sparse_array.items()}, self.shape)\n\n        result_list = [i*other for i in Flatten(self)]\n        return type(self)(result_list, self.shape)\n\n    def __rmul__(self, other):\n        from sympy.matrices.matrices import MatrixBase\n        from sympy.tensor.array import SparseNDimArray\n        from sympy.tensor.array.arrayop import Flatten\n\n        if isinstance(other, (Iterable, NDimArray, MatrixBase)):\n            raise ValueError(\"scalar expected, use tensorproduct(...) for tensorial product\")\n\n        other = sympify(other)\n        if isinstance(self, SparseNDimArray):\n            if other.is_zero:\n                return type(self)({}, self.shape)\n            return type(self)({k: other*v for (k, v) in self._sparse_array.items()}, self.shape)\n\n        result_list = [other*i for i in Flatten(self)]\n        return type(self)(result_list, self.shape)\n\n    def __truediv__(self, other):\n        from sympy.matrices.matrices import MatrixBase\n        from sympy.tensor.array import SparseNDimArray\n        from sympy.tensor.array.arrayop import Flatten\n\n        if isinstance(other, (Iterable, NDimArray, MatrixBase)):\n            raise ValueError(\"scalar expected\")\n\n        other = sympify(other)\n        if isinstance(self, SparseNDimArray) and other != S.Zero:\n            return type(self)({k: v/other for (k, v) in self._sparse_array.items()}, self.shape)\n\n        result_list = [i/other for i in Flatten(self)]\n        return type(self)(result_list, self.shape)\n\n    def __rtruediv__(self, other):\n        raise NotImplementedError('unsupported operation on NDimArray')\n\n    def __neg__(self):\n        from sympy.tensor.array import SparseNDimArray\n        from sympy.tensor.array.arrayop import Flatten\n\n        if isinstance(self, SparseNDimArray):\n            return type(self)({k: -v for (k, v) in self._sparse_array.items()}, self.shape)\n\n        result_list = [-i for i in Flatten(self)]\n        return type(self)(result_list, self.shape)\n\n    def __iter__(self):\n        def iterator():\n            if self._shape:\n                for i in range(self._shape[0]):\n                    yield self[i]\n            else:\n                yield self[()]\n\n        return iterator()\n\n    def __eq__(self, other):\n        \"\"\"\n        NDimArray instances can be compared to each other.\n        Instances equal if they have same shape and data.\n\n        Examples\n        ========\n\n        >>> from sympy import MutableDenseNDimArray\n        >>> a = MutableDenseNDimArray.zeros(2, 3)\n        >>> b = MutableDenseNDimArray.zeros(2, 3)\n        >>> a == b\n        True\n        >>> c = a.reshape(3, 2)\n        >>> c == b\n        False\n        >>> a[0,0] = 1\n        >>> b[0,0] = 2\n        >>> a == b\n        False\n        \"\"\"\n        from sympy.tensor.array import SparseNDimArray\n        if not isinstance(other, NDimArray):\n            return False\n\n        if not self.shape == other.shape:\n            return False\n\n        if isinstance(self, SparseNDimArray) and isinstance(other, SparseNDimArray):\n            return dict(self._sparse_array) == dict(other._sparse_array)\n\n        return list(self) == list(other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def _eval_transpose(self):\n        if self.rank() != 2:\n            raise ValueError(\"array rank not 2\")\n        from .arrayop import permutedims\n        return permutedims(self, (1, 0))\n\n    def transpose(self):\n        return self._eval_transpose()\n\n    def _eval_conjugate(self):\n        from sympy.tensor.array.arrayop import Flatten\n\n        return self.func([i.conjugate() for i in Flatten(self)], self.shape)\n\n    def conjugate(self):\n        return self._eval_conjugate()\n\n    def _eval_adjoint(self):\n        return self.transpose().conjugate()\n\n    def adjoint(self):\n        return self._eval_adjoint()\n\n    def _slice_expand(self, s, dim):\n        if not isinstance(s, slice):\n                return (s,)\n        start, stop, step = s.indices(dim)\n        return [start + i*step for i in range((stop-start)//step)]\n\n    def _get_slice_data_for_array_access(self, index):\n        sl_factors = [self._slice_expand(i, dim) for (i, dim) in zip(index, self.shape)]\n        eindices = itertools.product(*sl_factors)\n        return sl_factors, eindices\n\n    def _get_slice_data_for_array_assignment(self, index, value):\n        if not isinstance(value, NDimArray):\n            value = type(self)(value)\n        sl_factors, eindices = self._get_slice_data_for_array_access(index)\n        slice_offsets = [min(i) if isinstance(i, list) else None for i in sl_factors]\n        # TODO: add checks for dimensions for `value`?\n        return value, eindices, slice_offsets\n\n    @classmethod\n    def _check_special_bounds(cls, flat_list, shape):\n        if shape == () and len(flat_list) != 1:\n            raise ValueError(\"arrays without shape need one scalar value\")\n        if shape == (0,) and len(flat_list) > 0:\n            raise ValueError(\"if array shape is (0,) there cannot be elements\")\n\n    def _check_index_for_getitem(self, index):\n        if isinstance(index, (SYMPY_INTS, Integer, slice)):\n            index = (index, )\n\n        if len(index) < self.rank():\n            index = tuple([i for i in index] + \\\n                          [slice(None) for i in range(len(index), self.rank())])\n\n        if len(index) > self.rank():\n            raise ValueError('Dimension of index greater than rank of array')\n\n        return index\n\n\nclass ImmutableNDimArray(NDimArray, Basic):", "answer": "C"}
{"uuid": "aa95b602-646c-4203-9ab9-6ddbd66911dd", "issue_statement": "display bug while using pretty_print with sympy.vector object in the terminal\nThe following code jumbles some of the outputs in the terminal, essentially by inserting the unit vector in the middle -\r\n```python\r\nfrom sympy import *\r\nfrom sympy.vector import CoordSys3D, Del\r\n\r\ninit_printing()\r\n\r\ndelop = Del()\r\nCC_ = CoordSys3D(\"C\")\r\nx,    y,    z    = CC_.x, CC_.y, CC_.z\r\nxhat, yhat, zhat = CC_.i, CC_.j, CC_.k\r\n\r\nt = symbols(\"t\")\r\nten = symbols(\"10\", positive=True)\r\neps, mu = 4*pi*ten**(-11), ten**(-5)\r\n\r\nBx = 2 * ten**(-4) * cos(ten**5 * t) * sin(ten**(-3) * y)\r\nvecB = Bx * xhat\r\nvecE = (1/eps) * Integral(delop.cross(vecB/mu).doit(), t)\r\n\r\npprint(vecB)\r\nprint()\r\npprint(vecE)\r\nprint()\r\npprint(vecE.doit())\r\n```\r\n\r\nOutput:\r\n```python\r\n\u239b     \u239by_C\u239e    \u239b  5  \u239e\u239e    \r\n\u239c2\u22c5sin\u239c\u2500\u2500\u2500\u239f i_C\u22c5cos\u239d10 \u22c5t\u23a0\u239f\r\n\u239c     \u239c  3\u239f           \u239f    \r\n\u239c     \u239d10 \u23a0           \u239f    \r\n\u239c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u239f    \r\n\u239c           4         \u239f    \r\n\u239d         10          \u23a0    \r\n\r\n\u239b     \u2320                           \u239e    \r\n\u239c     \u23ae       \u239by_C\u239e    \u239b  5  \u239e    \u239f k_C\r\n\u239c     \u23ae -2\u22c5cos\u239c\u2500\u2500\u2500\u239f\u22c5cos\u239d10 \u22c5t\u23a0    \u239f    \r\n\u239c     \u23ae       \u239c  3\u239f               \u239f    \r\n\u239c  11 \u23ae       \u239d10 \u23a0               \u239f    \r\n\u239c10  \u22c5\u23ae \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dt\u239f    \r\n\u239c     \u23ae             2             \u239f    \r\n\u239c     \u23ae           10              \u239f    \r\n\u239c     \u2321                           \u239f    \r\n\u239c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u239f    \r\n\u239d               4\u22c5\u03c0               \u23a0    \r\n\r\n\u239b   4    \u239b  5  \u239e    \u239by_C\u239e \u239e    \r\n\u239c-10 \u22c5sin\u239d10 \u22c5t\u23a0\u22c5cos\u239c\u2500\u2500\u2500\u239f k_C \u239f\r\n\u239c                   \u239c  3\u239f \u239f    \r\n\u239c                   \u239d10 \u23a0 \u239f    \r\n\u239c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u239f    \r\n\u239d           2\u22c5\u03c0           \u23a0    ```", "choices": "(A)         lengths = []\n        strs = ['']\n        flag = []\n        for i, partstr in enumerate(o1):\n            flag.append(0)\n            # XXX: What is this hack?\n            if '\\n' in partstr:\n                tempstr = partstr\n                tempstr = tempstr.replace(vectstrs[i], '')\n                if '\\N{right parenthesis extension}' in tempstr:   # If scalar is a fraction\n                    for paren in range(len(tempstr)):\n                        flag[i] = 1\n                        if tempstr[paren] == '\\N{right parenthesis extension}':\n                            tempstr = tempstr[:paren] + '\\N{right parenthesis extension}'\\\n                                         + ' '  + vectstrs[i] + tempstr[paren + 1:]\n                            break\n                elif '\\N{RIGHT PARENTHESIS LOWER HOOK}' in tempstr:\n                    flag[i] = 1\n                    tempstr = tempstr.replace('\\N{RIGHT PARENTHESIS LOWER HOOK}',\n                                        '\\N{RIGHT PARENTHESIS LOWER HOOK}'\n                                        + ' ' + vectstrs[i])\n                else:\n                    tempstr = tempstr.replace('\\N{RIGHT PARENTHESIS UPPER HOOK}',\n                                        '\\N{RIGHT PARENTHESIS UPPER HOOK}'\n(B)                     tempstr = tempstr.replace('\\N{RIGHT PARENTHESIS LOWER HOOK}',\n                                        '\\N{RIGHT PARENTHESIS LOWER HOOK}'\n                                        + ' ' + vectstrs[i])\n                else:\n                    tempstr = tempstr.replace('\\N{RIGHT PARENTHESIS UPPER HOOK}',\n                                        '\\N{RIGHT PARENTHESIS UPPER HOOK}'\n                                        + ' ' + vectstrs[i])\n                o1[i] = tempstr\n\n        o1 = [x.split('\\n') for x in o1]\n        n_newlines = max([len(x) for x in o1])  # Width of part in its pretty form\n\n        if 1 in flag:                           # If there was a fractional scalar\n            for i, parts in enumerate(o1):\n                if len(parts) == 1:             # If part has no newline\n                    parts.insert(0, ' ' * (len(parts[0])))\n                    flag[i] = 1\n\n        for i, parts in enumerate(o1):\n            lengths.append(len(parts[flag[i]]))\n            for j in range(n_newlines):\n                if j+1 <= len(parts):\n                    if j >= len(strs):\n                        strs.append(' ' * (sum(lengths[:-1]) +\n(C)                         if tempstr[paren] == '\\N{right parenthesis extension}':\n                            tempstr = tempstr[:paren] + '\\N{right parenthesis extension}'\\\n                                         + ' '  + vectstrs[i] + tempstr[paren + 1:]\n                            break\n                elif '\\N{RIGHT PARENTHESIS LOWER HOOK}' in tempstr:\n                    flag[i] = 1\n                    tempstr = tempstr.replace('\\N{RIGHT PARENTHESIS LOWER HOOK}',\n                                        '\\N{RIGHT PARENTHESIS LOWER HOOK}'\n                                        + ' ' + vectstrs[i])\n                else:\n                    tempstr = tempstr.replace('\\N{RIGHT PARENTHESIS UPPER HOOK}',\n                                        '\\N{RIGHT PARENTHESIS UPPER HOOK}'\n                                        + ' ' + vectstrs[i])\n                o1[i] = tempstr\n\n        o1 = [x.split('\\n') for x in o1]\n        n_newlines = max([len(x) for x in o1])  # Width of part in its pretty form\n\n        if 1 in flag:                           # If there was a fractional scalar\n            for i, parts in enumerate(o1):\n                if len(parts) == 1:             # If part has no newline\n                    parts.insert(0, ' ' * (len(parts[0])))\n                    flag[i] = 1\n\n(D)             if '\\n' in partstr:\n                tempstr = partstr\n                tempstr = tempstr.replace(vectstrs[i], '')\n                if '\\N{right parenthesis extension}' in tempstr:   # If scalar is a fraction\n                    for paren in range(len(tempstr)):\n                        flag[i] = 1\n                        if tempstr[paren] == '\\N{right parenthesis extension}':\n                            tempstr = tempstr[:paren] + '\\N{right parenthesis extension}'\\\n                                         + ' '  + vectstrs[i] + tempstr[paren + 1:]\n                            break\n                elif '\\N{RIGHT PARENTHESIS LOWER HOOK}' in tempstr:\n                    flag[i] = 1\n                    tempstr = tempstr.replace('\\N{RIGHT PARENTHESIS LOWER HOOK}',\n                                        '\\N{RIGHT PARENTHESIS LOWER HOOK}'\n                                        + ' ' + vectstrs[i])\n                else:\n                    tempstr = tempstr.replace('\\N{RIGHT PARENTHESIS UPPER HOOK}',\n                                        '\\N{RIGHT PARENTHESIS UPPER HOOK}'\n                                        + ' ' + vectstrs[i])\n                o1[i] = tempstr\n\n        o1 = [x.split('\\n') for x in o1]\n        n_newlines = max([len(x) for x in o1])  # Width of part in its pretty form\n", "answer": "D"}
{"uuid": "6263c120-0f73-4164-90ca-66dfa21dbb11", "issue_statement": "Python code printer not respecting tuple with one element\nHi,\r\n\r\nThanks for the recent updates in SymPy! I'm trying to update my code to use SymPy 1.10 but ran into an issue with the Python code printer. MWE:\r\n\r\n\r\n```python\r\nimport inspect\r\nfrom sympy import lambdify\r\n\r\ninspect.getsource(lambdify([], tuple([1])))\r\n```\r\nSymPy 1.9 and under outputs:\r\n```\r\n'def _lambdifygenerated():\\n    return (1,)\\n'\r\n```\r\n\r\nBut SymPy 1.10 gives\r\n\r\n```\r\n'def _lambdifygenerated():\\n    return (1)\\n'\r\n```\r\nNote the missing comma after `1` that causes an integer to be returned instead of a tuple. \r\n\r\nFor tuples with two or more elements, the generated code is correct:\r\n```python\r\ninspect.getsource(lambdify([], tuple([1, 2])))\r\n```\r\nIn SymPy  1.10 and under, outputs:\r\n\r\n```\r\n'def _lambdifygenerated():\\n    return (1, 2)\\n'\r\n```\r\nThis result is expected.\r\n\r\nNot sure if this is a regression. As this breaks my program which assumes the return type to always be a tuple, could you suggest a workaround from the code generation side? Thank you.", "choices": "(A)         return doprint(arg)\n    elif iterable(arg):\n        if isinstance(arg, list):\n            left, right = \"[]\"\n        elif isinstance(arg, tuple):\n            left, right = \"()\"\n        else:\n            raise NotImplementedError(\"unhandled type: %s, %s\" % (type(arg), arg))\n        return left +', '.join(_recursive_to_string(doprint, e) for e in arg) + right\n(B)             left, right = \"()\"\n        else:\n            raise NotImplementedError(\"unhandled type: %s, %s\" % (type(arg), arg))\n        return left +', '.join(_recursive_to_string(doprint, e) for e in arg) + right\n    elif isinstance(arg, str):\n        return arg\n    else:\n        return doprint(arg)\n\n(C)     from sympy.core.basic import Basic\n\n    if isinstance(arg, (Basic, MatrixOperations)):\n        return doprint(arg)\n    elif iterable(arg):\n        if isinstance(arg, list):\n            left, right = \"[]\"\n        elif isinstance(arg, tuple):\n            left, right = \"()\"\n(D)             left, right = \"[]\"\n        elif isinstance(arg, tuple):\n            left, right = \"()\"\n        else:\n            raise NotImplementedError(\"unhandled type: %s, %s\" % (type(arg), arg))\n        return left +', '.join(_recursive_to_string(doprint, e) for e in arg) + right\n    elif isinstance(arg, str):\n        return arg\n    else:", "answer": "A"}
{"uuid": "a87cc5a1-4efa-4e1a-a73a-a78d2ad3c4ea", "issue_statement": "SI._collect_factor_and_dimension() cannot properly detect that exponent is dimensionless\nHow to reproduce:\r\n\r\n```python\r\nfrom sympy import exp\r\nfrom sympy.physics import units\r\nfrom sympy.physics.units.systems.si import SI\r\n\r\nexpr = units.second / (units.ohm * units.farad)\r\ndim = SI._collect_factor_and_dimension(expr)[1]\r\n\r\nassert SI.get_dimension_system().is_dimensionless(dim)\r\n\r\nbuggy_expr = 100 + exp(expr)\r\nSI._collect_factor_and_dimension(buggy_expr)\r\n\r\n# results in ValueError: Dimension of \"exp(second/(farad*ohm))\" is Dimension(time/(capacitance*impedance)), but it should be Dimension(1)\r\n```", "choices": "(A)                 dim /= idim**count\n            return factor, dim\n        elif isinstance(expr, Function):\n            fds = [self._collect_factor_and_dimension(\n                arg) for arg in expr.args]\n            return (expr.func(*(f[0] for f in fds)),\n                    *(d[1] for d in fds))\n        elif isinstance(expr, Dimension):\n            return S.One, expr\n(B)             for independent, count in expr.variable_count:\n                ifactor, idim = self._collect_factor_and_dimension(independent)\n                factor /= ifactor**count\n                dim /= idim**count\n            return factor, dim\n        elif isinstance(expr, Function):\n            fds = [self._collect_factor_and_dimension(\n                arg) for arg in expr.args]\n            return (expr.func(*(f[0] for f in fds)),\n(C)             return (expr.func(*(f[0] for f in fds)),\n                    *(d[1] for d in fds))\n        elif isinstance(expr, Dimension):\n            return S.One, expr\n        else:\n            return expr, Dimension(1)\n\n    def get_units_non_prefixed(self) -> tSet[Quantity]:\n        \"\"\"\n(D)             fds = [self._collect_factor_and_dimension(\n                arg) for arg in expr.args]\n            return (expr.func(*(f[0] for f in fds)),\n                    *(d[1] for d in fds))\n        elif isinstance(expr, Dimension):\n            return S.One, expr\n        else:\n            return expr, Dimension(1)\n", "answer": "A"}
{"uuid": "1055769d-3d7b-4d86-8c98-53b9c0e9dfd6", "issue_statement": "Cannot parse Greek characters (and possibly others) in parse_mathematica\nThe old Mathematica parser `mathematica` in the package `sympy.parsing.mathematica` was able to parse e.g. Greek characters. Hence the following example works fine:\r\n```\r\nfrom sympy.parsing.mathematica import mathematica\r\nmathematica('\u03bb')\r\nOut[]: \r\n\u03bb\r\n```\r\n\r\nAs of SymPy v. 1.11, the `mathematica` function is deprecated, and is replaced by `parse_mathematica`. This function, however, seems unable to handle the simple example above:\r\n```\r\nfrom sympy.parsing.mathematica import parse_mathematica\r\nparse_mathematica('\u03bb')\r\nTraceback (most recent call last):\r\n...\r\nFile \"<string>\", line unknown\r\nSyntaxError: unable to create a single AST for the expression\r\n```\r\n\r\nThis appears to be due to a bug in `parse_mathematica`, which is why I have opened this issue.\r\n\r\nThanks in advance!\nCannot parse Greek characters (and possibly others) in parse_mathematica\nThe old Mathematica parser `mathematica` in the package `sympy.parsing.mathematica` was able to parse e.g. Greek characters. Hence the following example works fine:\r\n```\r\nfrom sympy.parsing.mathematica import mathematica\r\nmathematica('\u03bb')\r\nOut[]: \r\n\u03bb\r\n```\r\n\r\nAs of SymPy v. 1.11, the `mathematica` function is deprecated, and is replaced by `parse_mathematica`. This function, however, seems unable to handle the simple example above:\r\n```\r\nfrom sympy.parsing.mathematica import parse_mathematica\r\nparse_mathematica('\u03bb')\r\nTraceback (most recent call last):\r\n...\r\nFile \"<string>\", line unknown\r\nSyntaxError: unable to create a single AST for the expression\r\n```\r\n\r\nThis appears to be due to a bug in `parse_mathematica`, which is why I have opened this issue.\r\n\r\nThanks in advance!", "choices": "(A)             code_splits[i] = code_split\n\n        # Tokenize the input strings with a regular expression:\n        token_lists = [tokenizer.findall(i) if isinstance(i, str) else [i] for i in code_splits]\n        tokens = [j for i in token_lists for j in i]\n\n        # Remove newlines at the beginning\n(B)                     raise SyntaxError(\"mismatch in comment (*  *) code\")\n                code_split = code_split[:pos_comment_start] + code_split[pos_comment_end+2:]\n            code_splits[i] = code_split\n\n        # Tokenize the input strings with a regular expression:\n        token_lists = [tokenizer.findall(i) if isinstance(i, str) else [i] for i in code_splits]\n        tokens = [j for i in token_lists for j in i]\n(C)         tokens = [j for i in token_lists for j in i]\n\n        # Remove newlines at the beginning\n        while tokens and tokens[0] == \"\\n\":\n            tokens.pop(0)\n        # Remove newlines at the end\n        while tokens and tokens[-1] == \"\\n\":\n(D)         # Tokenize the input strings with a regular expression:\n        token_lists = [tokenizer.findall(i) if isinstance(i, str) else [i] for i in code_splits]\n        tokens = [j for i in token_lists for j in i]\n\n        # Remove newlines at the beginning\n        while tokens and tokens[0] == \"\\n\":\n            tokens.pop(0)", "answer": "A"}
{"uuid": "3203e113-8140-49c3-919a-4abb392a60f0", "issue_statement": "Bug in expand of TensorProduct + Workaround + Fix\n### Error description\r\nThe expansion of a TensorProduct object stops incomplete if summands in the tensor product factors have (scalar) factors, e.g.\r\n```\r\nfrom sympy import *\r\nfrom sympy.physics.quantum import *\r\nU = Operator('U')\r\nV = Operator('V')\r\nP = TensorProduct(2*U - V, U + V)\r\nprint(P) \r\n# (2*U - V)x(U + V)\r\nprint(P.expand(tensorproduct=True)) \r\n#result: 2*Ux(U + V) - Vx(U + V) #expansion has missed 2nd tensor factor and is incomplete\r\n```\r\nThis is clearly not the expected behaviour. It also effects other functions that rely on .expand(tensorproduct=True), as e.g. qapply() .\r\n\r\n### Work around\r\nRepeat .expand(tensorproduct=True) as may times as there are tensor factors, resp. until the expanded term does no longer change. This is however only reasonable in interactive session and not in algorithms.\r\n\r\n### Code Fix\r\n.expand relies on the method TensorProduct._eval_expand_tensorproduct(). The issue arises from an inprecise check in TensorProduct._eval_expand_tensorproduct() whether a recursive call is required; it fails when the creation of a TensorProduct object returns commutative (scalar) factors up front: in that case the constructor returns a Mul(c_factors, TensorProduct(..)).\r\nI thus propose the following  code fix in TensorProduct._eval_expand_tensorproduct() in quantum/tensorproduct.py.  I have marked the four lines to be added / modified:\r\n```\r\n    def _eval_expand_tensorproduct(self, **hints):\r\n                ...\r\n                for aa in args[i].args:\r\n                    tp = TensorProduct(*args[:i] + (aa,) + args[i + 1:])\r\n                    c_part, nc_part = tp.args_cnc() #added\r\n                    if len(nc_part)==1 and isinstance(nc_part[0], TensorProduct): #modified\r\n                        nc_part = (nc_part[0]._eval_expand_tensorproduct(), ) #modified\r\n                    add_args.append(Mul(*c_part)*Mul(*nc_part)) #modified\r\n                break\r\n                ...\r\n```\r\nThe fix splits of commutative (scalar) factors from the tp returned. The TensorProduct object will be the one nc factor in nc_part (see TensorProduct.__new__ constructor), if any. Note that the constructor will return 0 if a tensor factor is 0, so there is no guarantee that tp contains a TensorProduct object (e.g. TensorProduct(U-U, U+V).", "choices": "(A)             if isinstance(args[i], Add):\n                for aa in args[i].args:\n                    tp = TensorProduct(*args[:i] + (aa,) + args[i + 1:])\n                    if isinstance(tp, TensorProduct):\n                        tp = tp._eval_expand_tensorproduct()\n                    add_args.append(tp)\n                break\n\n        if add_args:\n            return Add(*add_args)\n        else:\n            return self\n(B)                     if isinstance(tp, TensorProduct):\n                        tp = tp._eval_expand_tensorproduct()\n                    add_args.append(tp)\n                break\n\n        if add_args:\n            return Add(*add_args)\n        else:\n            return self\n\n    def _eval_trace(self, **kwargs):\n        indices = kwargs.get('indices', None)\n(C)                 break\n\n        if add_args:\n            return Add(*add_args)\n        else:\n            return self\n\n    def _eval_trace(self, **kwargs):\n        indices = kwargs.get('indices', None)\n        exp = tensor_product_simp(self)\n\n        if indices is None or len(indices) == 0:\n(D)         args = self.args\n        add_args = []\n        for i in range(len(args)):\n            if isinstance(args[i], Add):\n                for aa in args[i].args:\n                    tp = TensorProduct(*args[:i] + (aa,) + args[i + 1:])\n                    if isinstance(tp, TensorProduct):\n                        tp = tp._eval_expand_tensorproduct()\n                    add_args.append(tp)\n                break\n\n        if add_args:", "answer": "A"}
{"uuid": "8aff0cac-6a50-4097-8c85-d74fb16b15e1", "issue_statement": "collect_factor_and_dimension does not detect equivalent dimensions in addition\nCode to reproduce:\r\n```python\r\nfrom sympy.physics import units\r\nfrom sympy.physics.units.systems.si import SI\r\n\r\nv1 = units.Quantity('v1')\r\nSI.set_quantity_dimension(v1, units.velocity)\r\nSI.set_quantity_scale_factor(v1, 2 * units.meter / units.second)\r\n\r\na1 = units.Quantity('a1')\r\nSI.set_quantity_dimension(a1, units.acceleration)\r\nSI.set_quantity_scale_factor(a1, -9.8 * units.meter / units.second**2)\r\n\r\nt1 = units.Quantity('t1')\r\nSI.set_quantity_dimension(t1, units.time)\r\nSI.set_quantity_scale_factor(t1, 5 * units.second)\r\n\r\nexpr1 = a1*t1 + v1\r\nSI._collect_factor_and_dimension(expr1)\r\n```\r\nResults in:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Python\\Python310\\lib\\site-packages\\sympy\\physics\\units\\unitsystem.py\", line 179, in _collect_factor_and_dimension\r\n    raise ValueError(\r\nValueError: Dimension of \"v1\" is Dimension(velocity), but it should be Dimension(acceleration*time)\r\n```", "choices": "(A)             for addend in expr.args[1:]:\n                addend_factor, addend_dim = \\\n                    self._collect_factor_and_dimension(addend)\n                if dim != addend_dim:\n                    raise ValueError(\n                        'Dimension of \"{}\" is {}, '\n                        'but it should be {}'.format(\n(B)         elif isinstance(expr, Add):\n            factor, dim = self._collect_factor_and_dimension(expr.args[0])\n            for addend in expr.args[1:]:\n                addend_factor, addend_dim = \\\n                    self._collect_factor_and_dimension(addend)\n                if dim != addend_dim:\n                    raise ValueError(\n(C)                     self._collect_factor_and_dimension(addend)\n                if dim != addend_dim:\n                    raise ValueError(\n                        'Dimension of \"{}\" is {}, '\n                        'but it should be {}'.format(\n                            addend, addend_dim, dim))\n                factor += addend_factor\n(D)                     raise ValueError(\n                        'Dimension of \"{}\" is {}, '\n                        'but it should be {}'.format(\n                            addend, addend_dim, dim))\n                factor += addend_factor\n            return factor, dim\n        elif isinstance(expr, Derivative):", "answer": "A"}
{"uuid": "72ccc1ee-f5c1-4517-8123-9a0c8c62c401", "issue_statement": "Bug with milli prefix\nWhat happened:\r\n```\r\nIn [1]: from sympy.physics.units import milli, W\r\nIn [2]: milli*W == 1\r\nOut[2]: True\r\nIn [3]: W*milli\r\nOut[3]: watt*Prefix(milli, m, -3, 10)\r\n```\r\nWhat I expected to happen: milli*W should evaluate to milli watts / mW\r\n\r\n`milli*W` or more generally `milli` times some unit evaluates to the number 1. I have tried this with Watts and Volts, I'm not sure what other cases this happens. I'm using sympy version 1.11.1-1 on Arch Linux with Python 3.10.9. If you cannot reproduce I would be happy to be of any assitance.", "choices": "(A)     def scale_factor(self):\n        return self._scale_factor\n\n    def _latex(self, printer):\n        if self._latex_repr is None:\n            return r'\\text{%s}' % self._abbrev\n        return self._latex_repr\n\n    @property\n    def base(self):\n        return self._base\n\n    def __str__(self):\n        return str(self._abbrev)\n\n    def __repr__(self):\n        if self.base == 10:\n            return \"Prefix(%r, %r, %r)\" % (\n                str(self.name), str(self.abbrev), self._exponent)\n        else:\n            return \"Prefix(%r, %r, %r, %r)\" % (\n                str(self.name), str(self.abbrev), self._exponent, self.base)\n\n    def __mul__(self, other):\n        from sympy.physics.units import Quantity\n        if not isinstance(other, (Quantity, Prefix)):\n            return super().__mul__(other)\n\n        fact = self.scale_factor * other.scale_factor\n\n        if fact == 1:\n            return 1\n        elif isinstance(other, Prefix):\n            # simplify prefix\n            for p in PREFIXES:\n                if PREFIXES[p].scale_factor == fact:\n                    return PREFIXES[p]\n            return fact\n\n        return self.scale_factor * other\n\n    def __truediv__(self, other):\n        if not hasattr(other, \"scale_factor\"):\n            return super().__truediv__(other)\n\n        fact = self.scale_factor / other.scale_factor\n\n        if fact == 1:\n            return 1\n        elif isinstance(other, Prefix):\n            for p in PREFIXES:\n                if PREFIXES[p].scale_factor == fact:\n                    return PREFIXES[p]\n            return fact\n\n        return self.scale_factor / other\n\n    def __rtruediv__(self, other):\n        if other == 1:\n            for p in PREFIXES:\n                if PREFIXES[p].scale_factor == 1 / self.scale_factor:\n                    return PREFIXES[p]\n        return other / self.scale_factor\n\n\ndef prefix_unit(unit, prefixes):\n    \"\"\"\n    Return a list of all units formed by unit and the given prefixes.\n\n    You can use the predefined PREFIXES or BIN_PREFIXES, but you can also\n    pass as argument a subdict of them if you do not want all prefixed units.\n\n        >>> from sympy.physics.units.prefixes import (PREFIXES,\n        ...                                                 prefix_unit)\n        >>> from sympy.physics.units import m\n        >>> pref = {\"m\": PREFIXES[\"m\"], \"c\": PREFIXES[\"c\"], \"d\": PREFIXES[\"d\"]}\n        >>> prefix_unit(m, pref)  # doctest: +SKIP\n        [millimeter, centimeter, decimeter]\n    \"\"\"\n\n    from sympy.physics.units.quantities import Quantity\n    from sympy.physics.units import UnitSystem\n\n    prefixed_units = []\n\n    for prefix_abbr, prefix in prefixes.items():\n        quantity = Quantity(\n                \"%s%s\" % (prefix.name, unit.name),\n                abbrev=(\"%s%s\" % (prefix.abbrev, unit.abbrev)),\n                is_prefixed=True,\n           )\n        UnitSystem._quantity_dimensional_equivalence_map_global[quantity] = unit\n        UnitSystem._quantity_scale_factors_global[quantity] = (prefix.scale_factor, unit)\n        prefixed_units.append(quantity)\n\n    return prefixed_units\n\n\nyotta = Prefix('yotta', 'Y', 24)\nzetta = Prefix('zetta', 'Z', 21)\nexa = Prefix('exa', 'E', 18)\npeta = Prefix('peta', 'P', 15)\ntera = Prefix('tera', 'T', 12)\ngiga = Prefix('giga', 'G', 9)\n(B) \"\"\"\nfrom sympy.core.expr import Expr\nfrom sympy.core.sympify import sympify\n\n\nclass Prefix(Expr):\n    \"\"\"\n    This class represent prefixes, with their name, symbol and factor.\n\n    Prefixes are used to create derived units from a given unit. They should\n    always be encapsulated into units.\n\n    The factor is constructed from a base (default is 10) to some power, and\n    it gives the total multiple or fraction. For example the kilometer km\n    is constructed from the meter (factor 1) and the kilo (10 to the power 3,\n    i.e. 1000). The base can be changed to allow e.g. binary prefixes.\n\n    A prefix multiplied by something will always return the product of this\n    other object times the factor, except if the other object:\n\n    - is a prefix and they can be combined into a new prefix;\n    - defines multiplication with prefixes (which is the case for the Unit\n      class).\n    \"\"\"\n    _op_priority = 13.0\n    is_commutative = True\n\n    def __new__(cls, name, abbrev, exponent, base=sympify(10), latex_repr=None):\n\n        name = sympify(name)\n        abbrev = sympify(abbrev)\n        exponent = sympify(exponent)\n        base = sympify(base)\n\n        obj = Expr.__new__(cls, name, abbrev, exponent, base)\n        obj._name = name\n        obj._abbrev = abbrev\n        obj._scale_factor = base**exponent\n        obj._exponent = exponent\n        obj._base = base\n        obj._latex_repr = latex_repr\n        return obj\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def abbrev(self):\n        return self._abbrev\n\n    @property\n    def scale_factor(self):\n        return self._scale_factor\n\n    def _latex(self, printer):\n        if self._latex_repr is None:\n            return r'\\text{%s}' % self._abbrev\n        return self._latex_repr\n\n    @property\n    def base(self):\n        return self._base\n\n    def __str__(self):\n        return str(self._abbrev)\n\n    def __repr__(self):\n        if self.base == 10:\n            return \"Prefix(%r, %r, %r)\" % (\n                str(self.name), str(self.abbrev), self._exponent)\n        else:\n            return \"Prefix(%r, %r, %r, %r)\" % (\n                str(self.name), str(self.abbrev), self._exponent, self.base)\n\n    def __mul__(self, other):\n        from sympy.physics.units import Quantity\n        if not isinstance(other, (Quantity, Prefix)):\n            return super().__mul__(other)\n\n        fact = self.scale_factor * other.scale_factor\n\n        if fact == 1:\n            return 1\n        elif isinstance(other, Prefix):\n            # simplify prefix\n            for p in PREFIXES:\n                if PREFIXES[p].scale_factor == fact:\n                    return PREFIXES[p]\n            return fact\n\n        return self.scale_factor * other\n\n    def __truediv__(self, other):\n        if not hasattr(other, \"scale_factor\"):\n            return super().__truediv__(other)\n\n        fact = self.scale_factor / other.scale_factor\n\n        if fact == 1:\n            return 1\n        elif isinstance(other, Prefix):\n            for p in PREFIXES:\n                if PREFIXES[p].scale_factor == fact:\n(C) \n    def __new__(cls, name, abbrev, exponent, base=sympify(10), latex_repr=None):\n\n        name = sympify(name)\n        abbrev = sympify(abbrev)\n        exponent = sympify(exponent)\n        base = sympify(base)\n\n        obj = Expr.__new__(cls, name, abbrev, exponent, base)\n        obj._name = name\n        obj._abbrev = abbrev\n        obj._scale_factor = base**exponent\n        obj._exponent = exponent\n        obj._base = base\n        obj._latex_repr = latex_repr\n        return obj\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def abbrev(self):\n        return self._abbrev\n\n    @property\n    def scale_factor(self):\n        return self._scale_factor\n\n    def _latex(self, printer):\n        if self._latex_repr is None:\n            return r'\\text{%s}' % self._abbrev\n        return self._latex_repr\n\n    @property\n    def base(self):\n        return self._base\n\n    def __str__(self):\n        return str(self._abbrev)\n\n    def __repr__(self):\n        if self.base == 10:\n            return \"Prefix(%r, %r, %r)\" % (\n                str(self.name), str(self.abbrev), self._exponent)\n        else:\n            return \"Prefix(%r, %r, %r, %r)\" % (\n                str(self.name), str(self.abbrev), self._exponent, self.base)\n\n    def __mul__(self, other):\n        from sympy.physics.units import Quantity\n        if not isinstance(other, (Quantity, Prefix)):\n            return super().__mul__(other)\n\n        fact = self.scale_factor * other.scale_factor\n\n        if fact == 1:\n            return 1\n        elif isinstance(other, Prefix):\n            # simplify prefix\n            for p in PREFIXES:\n                if PREFIXES[p].scale_factor == fact:\n                    return PREFIXES[p]\n            return fact\n\n        return self.scale_factor * other\n\n    def __truediv__(self, other):\n        if not hasattr(other, \"scale_factor\"):\n            return super().__truediv__(other)\n\n        fact = self.scale_factor / other.scale_factor\n\n        if fact == 1:\n            return 1\n        elif isinstance(other, Prefix):\n            for p in PREFIXES:\n                if PREFIXES[p].scale_factor == fact:\n                    return PREFIXES[p]\n            return fact\n\n        return self.scale_factor / other\n\n    def __rtruediv__(self, other):\n        if other == 1:\n            for p in PREFIXES:\n                if PREFIXES[p].scale_factor == 1 / self.scale_factor:\n                    return PREFIXES[p]\n        return other / self.scale_factor\n\n\ndef prefix_unit(unit, prefixes):\n    \"\"\"\n    Return a list of all units formed by unit and the given prefixes.\n\n    You can use the predefined PREFIXES or BIN_PREFIXES, but you can also\n    pass as argument a subdict of them if you do not want all prefixed units.\n\n        >>> from sympy.physics.units.prefixes import (PREFIXES,\n        ...                                                 prefix_unit)\n        >>> from sympy.physics.units import m\n        >>> pref = {\"m\": PREFIXES[\"m\"], \"c\": PREFIXES[\"c\"], \"d\": PREFIXES[\"d\"]}\n        >>> prefix_unit(m, pref)  # doctest: +SKIP\n        [millimeter, centimeter, decimeter]\n(D)             return super().__mul__(other)\n\n        fact = self.scale_factor * other.scale_factor\n\n        if fact == 1:\n            return 1\n        elif isinstance(other, Prefix):\n            # simplify prefix\n            for p in PREFIXES:\n                if PREFIXES[p].scale_factor == fact:\n                    return PREFIXES[p]\n            return fact\n\n        return self.scale_factor * other\n\n    def __truediv__(self, other):\n        if not hasattr(other, \"scale_factor\"):\n            return super().__truediv__(other)\n\n        fact = self.scale_factor / other.scale_factor\n\n        if fact == 1:\n            return 1\n        elif isinstance(other, Prefix):\n            for p in PREFIXES:\n                if PREFIXES[p].scale_factor == fact:\n                    return PREFIXES[p]\n            return fact\n\n        return self.scale_factor / other\n\n    def __rtruediv__(self, other):\n        if other == 1:\n            for p in PREFIXES:\n                if PREFIXES[p].scale_factor == 1 / self.scale_factor:\n                    return PREFIXES[p]\n        return other / self.scale_factor\n\n\ndef prefix_unit(unit, prefixes):\n    \"\"\"\n    Return a list of all units formed by unit and the given prefixes.\n\n    You can use the predefined PREFIXES or BIN_PREFIXES, but you can also\n    pass as argument a subdict of them if you do not want all prefixed units.\n\n        >>> from sympy.physics.units.prefixes import (PREFIXES,\n        ...                                                 prefix_unit)\n        >>> from sympy.physics.units import m\n        >>> pref = {\"m\": PREFIXES[\"m\"], \"c\": PREFIXES[\"c\"], \"d\": PREFIXES[\"d\"]}\n        >>> prefix_unit(m, pref)  # doctest: +SKIP\n        [millimeter, centimeter, decimeter]\n    \"\"\"\n\n    from sympy.physics.units.quantities import Quantity\n    from sympy.physics.units import UnitSystem\n\n    prefixed_units = []\n\n    for prefix_abbr, prefix in prefixes.items():\n        quantity = Quantity(\n                \"%s%s\" % (prefix.name, unit.name),\n                abbrev=(\"%s%s\" % (prefix.abbrev, unit.abbrev)),\n                is_prefixed=True,\n           )\n        UnitSystem._quantity_dimensional_equivalence_map_global[quantity] = unit\n        UnitSystem._quantity_scale_factors_global[quantity] = (prefix.scale_factor, unit)\n        prefixed_units.append(quantity)\n\n    return prefixed_units\n\n\nyotta = Prefix('yotta', 'Y', 24)\nzetta = Prefix('zetta', 'Z', 21)\nexa = Prefix('exa', 'E', 18)\npeta = Prefix('peta', 'P', 15)\ntera = Prefix('tera', 'T', 12)\ngiga = Prefix('giga', 'G', 9)\nmega = Prefix('mega', 'M', 6)\nkilo = Prefix('kilo', 'k', 3)\nhecto = Prefix('hecto', 'h', 2)\ndeca = Prefix('deca', 'da', 1)\ndeci = Prefix('deci', 'd', -1)\ncenti = Prefix('centi', 'c', -2)\nmilli = Prefix('milli', 'm', -3)\nmicro = Prefix('micro', 'mu', -6, latex_repr=r\"\\mu\")\nnano = Prefix('nano', 'n', -9)\npico = Prefix('pico', 'p', -12)\nfemto = Prefix('femto', 'f', -15)\natto = Prefix('atto', 'a', -18)\nzepto = Prefix('zepto', 'z', -21)\nyocto = Prefix('yocto', 'y', -24)\n\n\n# https://physics.nist.gov/cuu/Units/prefixes.html\nPREFIXES = {\n    'Y': yotta,\n    'Z': zetta,\n    'E': exa,\n    'P': peta,\n    'T': tera,\n    'G': giga,\n    'M': mega,\n    'k': kilo,", "answer": "B"}
