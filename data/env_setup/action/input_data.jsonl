{"uuid": "f8026b9d-bd5d-474c-82b0-4aeb7d65471b", "execution_plan": "step1: Clone the repository and set up the Python environment with necessary dependencies; step2: Download and prepare the SVBench dataset from Hugging Face; step3: Filter videos based on aesthetic and optical flow scores; step4: Perform scene detection and video splitting; step5: Extract video frames for further processing; step6: Construct QA chains for video dialogues; step7: Evaluate the quality of the generated QA chains; step8: Identify and process temporal linkages between QA chains; step9: Evaluate the model performance on SingleQA, ChainQA, and ConQA tasks; step10: Prepare data and evaluate the StreamingChat model on various benchmarks (MMBench, CCBench, Tiny LVLM, MM-Vet, MMMU, MMBench Video)", "executed_cmds": "git clone https://huggingface.co/yzy666/SVBench;conda create -n SVBench -y python=3.8.18;conda activate SVBench;conda install -y -c pytorch pytorch=1.11.0 torchvision=0.12.0;pip install opencv-python=4.10.0.84;git clone https://huggingface.co/yzy666/SVBench", "cmd_prefix": "cd", "cmd_postfix": "SVBench", "target_cmd": "cd SVBench"}
{"uuid": "2a56c2a9-a26d-4516-ba87-06a110946973", "execution_plan": "step1: Clone the repository and set up the Python environment with necessary dependencies; step2: Download and prepare the SVBench dataset from Hugging Face; step3: Filter videos based on aesthetic and optical flow scores; step4: Perform scene detection and video splitting; step5: Extract video frames for further processing; step6: Construct QA chains for video dialogues; step7: Evaluate the quality of the generated QA chains; step8: Identify and process temporal linkages between QA chains; step9: Evaluate the model performance on SingleQA, ChainQA, and ConQA tasks; step10: Prepare data and evaluate the StreamingChat model on various benchmarks (MMBench, CCBench, Tiny LVLM, MM-Vet, MMMU, MMBench Video)", "executed_cmds": "git clone https://huggingface.co/yzy666/SVBench;conda create -n SVBench -y python=3.8.18;conda activate SVBench;conda install -y -c pytorch pytorch=1.11.0 torchvision=0.12.0;pip install opencv-python=4.10.0.84;git clone https://huggingface.co/yzy666/SVBench;cd SVBench", "cmd_prefix": "cat allVideos_tar_sep/allVideos.part_*", "cmd_postfix": "> allVideo.tar.gz", "target_cmd": "cat allVideos_tar_sep/allVideos.part_* > allVideo.tar.gz"}
{"uuid": "5743e6e2-5bab-4df5-bb7d-770b3da0d157", "execution_plan": "step1: Clone the repository and set up the Python environment with necessary dependencies; step2: Download and prepare the SVBench dataset from Hugging Face; step3: Filter videos based on aesthetic and optical flow scores; step4: Perform scene detection and video splitting; step5: Extract video frames for further processing; step6: Construct QA chains for video dialogues; step7: Evaluate the quality of the generated QA chains; step8: Identify and process temporal linkages between QA chains; step9: Evaluate the model performance on SingleQA, ChainQA, and ConQA tasks; step10: Prepare data and evaluate the StreamingChat model on various benchmarks (MMBench, CCBench, Tiny LVLM, MM-Vet, MMMU, MMBench Video)", "executed_cmds": "git clone https://huggingface.co/yzy666/SVBench;conda create -n SVBench -y python=3.8.18;conda activate SVBench;conda install -y -c pytorch pytorch=1.11.0 torchvision=0.12.0;pip install opencv-python=4.10.0.84;git clone https://huggingface.co/yzy666/SVBench;cd SVBench;cat allVideos_tar_sep/allVideos.part_* > allVideo.tar.gz", "cmd_prefix": "md5sum", "cmd_postfix": "allVideo.tar.gz", "target_cmd": "md5sum allVideo.tar.gz"}
{"uuid": "25b0e5c6-9574-49e1-aace-33cdbaea4843", "execution_plan": "step1: Clone the repository and set up the conda environment; step2: Install the required dependencies; step3: Start the CogVLM2 server for seed mining; step4: Generate images for seed mining; step5: Evaluate the generated images with CogVLM2; step6: Find the top-performing seeds; step7: Generate a dataset using the top seeds; step8: (Optional) Evaluate and rectify the dataset; step9: Fine-tune the text-to-image model; step10: Evaluate the fine-tuned model using GPT-4o; step11: Compare the results between the pretrained and fine-tuned models", "executed_cmds": "git clone https://github.com/doub7e/Reliable-Random-Seeds.git;cd Reliable-Random-Seeds;conda create -n reliable-seeds python=3.8;conda activate reliable-seeds", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "5a76d2ef-edf8-4db8-9dda-50793f6b7207", "execution_plan": "step1: Create a conda environment and install dependencies; step2: Install additional packages for each search method from their original repositories; step3: Launch a local LLM server using vllm; step4: Configure environment variables in the .env file; step5: Execute the eval.py script with the required arguments for the desired searcher and dataset; step6: Resume a previous run if needed using the --resume_from option", "executed_cmds": "conda create -n llmsrbench python=3.11.7", "cmd_prefix": "conda", "cmd_postfix": "activate llmsrbench", "target_cmd": "conda activate llmsrbench"}
{"uuid": "b67520fc-f977-4e57-a35e-1199e0acbc80", "execution_plan": "step1: Create a conda environment and install dependencies; step2: Install additional packages for each search method from their original repositories; step3: Launch a local LLM server using vllm; step4: Configure environment variables in the .env file; step5: Execute the eval.py script with the required arguments for the desired searcher and dataset; step6: Resume a previous run if needed using the --resume_from option", "executed_cmds": "conda create -n llmsrbench python=3.11.7;conda activate llmsrbench", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "b77cce6b-9169-4a32-b4d6-49fde37a0b72", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch]", "cmd_prefix": "pip", "cmd_postfix": "install easydict", "target_cmd": "pip install easydict"}
{"uuid": "869f7522-8cc8-4756-8980-f3c7f6370e26", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict", "cmd_prefix": "pip", "cmd_postfix": "install psutil", "target_cmd": "pip install psutil"}
{"uuid": "7f8ad505-dd10-4741-a8ca-c4cc9e38fbac", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil", "cmd_prefix": "pip", "cmd_postfix": "install wandb", "target_cmd": "pip install wandb"}
{"uuid": "84a2da28-e9c6-45bf-9a8d-2255b44781f7", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb", "cmd_prefix": "pip", "cmd_postfix": "install pytz", "target_cmd": "pip install pytz"}
{"uuid": "ed343ead-ff59-49d5-9613-7ef28d467059", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz", "cmd_prefix": "pip", "cmd_postfix": "install ipdb", "target_cmd": "pip install ipdb"}
{"uuid": "b65ce5f8-fcb7-439f-bcff-a0dba99359b6", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb", "cmd_prefix": "pip", "cmd_postfix": "install pandas", "target_cmd": "pip install pandas"}
{"uuid": "c675e84b-5078-4a05-bb62-8643300eed08", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas", "cmd_prefix": "pip", "cmd_postfix": "install datasets", "target_cmd": "pip install datasets"}
{"uuid": "56f3756f-c3ec-4169-a8d0-31481d599028", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas;pip install datasets", "cmd_prefix": "pip", "cmd_postfix": "install torchmetrics", "target_cmd": "pip install torchmetrics"}
{"uuid": "c8929762-d459-4846-9715-dbec27e21cd9", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas;pip install datasets;pip install torchmetrics", "cmd_prefix": "pip", "cmd_postfix": "install rdflib", "target_cmd": "pip install rdflib"}
{"uuid": "5b5a9d2a-fbe8-4db0-bfd0-fd412b956038", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas;pip install datasets;pip install torchmetrics;pip install rdflib", "cmd_prefix": "pip", "cmd_postfix": "install hickle", "target_cmd": "pip install hickle"}
{"uuid": "9ec6d750-cac5-4f26-b693-567e5dc99244", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas;pip install datasets;pip install torchmetrics;pip install rdflib;pip install hickle", "cmd_prefix": "pip", "cmd_postfix": "install anndata", "target_cmd": "pip install anndata"}
{"uuid": "398e4ec2-3b98-4b09-b1a7-637db8615114", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas;pip install datasets;pip install torchmetrics;pip install rdflib;pip install hickle;pip install anndata", "cmd_prefix": "pip", "cmd_postfix": "install scikit-learn", "target_cmd": "pip install scikit-learn"}
{"uuid": "578e5247-407b-49e0-b242-0de0aff45b01", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas;pip install datasets;pip install torchmetrics;pip install rdflib;pip install hickle;pip install anndata;pip install scikit-learn", "cmd_prefix": "pip", "cmd_postfix": "install scanpy", "target_cmd": "pip install scanpy"}
{"uuid": "7f95fe3c-61ae-4f67-b7cf-ec9cf3cf3cf2", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas;pip install datasets;pip install torchmetrics;pip install rdflib;pip install hickle;pip install anndata;pip install scikit-learn;pip install scanpy", "cmd_prefix": "pip", "cmd_postfix": "install scib", "target_cmd": "pip install scib"}
{"uuid": "9390afa6-60d0-49cb-a45c-53bb0b878361", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas;pip install datasets;pip install torchmetrics;pip install rdflib;pip install hickle;pip install anndata;pip install scikit-learn;pip install scanpy;pip install scib", "cmd_prefix": "conda install", "cmd_postfix": "-c conda-forge cupy", "target_cmd": "conda install -c conda-forge cupy"}
{"uuid": "f3016933-6d3a-4369-abff-ce4935bf5a4f", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas;pip install datasets;pip install torchmetrics;pip install rdflib;pip install hickle;pip install anndata;pip install scikit-learn;pip install scanpy;pip install scib;conda install -c conda-forge cupy;conda install rapidsai::cuml;conda install -c rapidsai -c conda-forge -c nvidia cugraph;from sccello.src.model_prototype_contrastive import PrototypeContrastiveForMaskedLM;model = PrototypeContrastiveForMaskedLM.from_pretrained(\"katarinayuan/scCello-zeroshot\";output_hidden_states=True);from sccello.src.model_prototype_contrastive import PrototypeContrastiveForSequenceClassification;model_kwargs = {\"num_labels\": NUM_LABELS;\"total_logging_steps\": training_cfg[\"logging_steps\"] * training_args.gradient_accumulation_steps};model = PrototypeContrastiveForSequenceClassification.from_pretrained(\"katarinayuan/scCello-zeroshot\";**model_kwargs);export HF_HOME=\"/path/to/another/directory/datasets\"", "cmd_prefix": "from sccello.src.utils", "cmd_postfix": "import data_loading", "target_cmd": "from sccello.src.utils import data_loading"}
{"uuid": "7f90fa57-98a0-4910-abd6-69a880eb0073", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas;pip install datasets;pip install torchmetrics;pip install rdflib;pip install hickle;pip install anndata;pip install scikit-learn;pip install scanpy;pip install scib;conda install -c conda-forge cupy;conda install rapidsai::cuml;conda install -c rapidsai -c conda-forge -c nvidia cugraph;from sccello.src.model_prototype_contrastive import PrototypeContrastiveForMaskedLM;model = PrototypeContrastiveForMaskedLM.from_pretrained(\"katarinayuan/scCello-zeroshot\";output_hidden_states=True);from sccello.src.model_prototype_contrastive import PrototypeContrastiveForSequenceClassification;model_kwargs = {\"num_labels\": NUM_LABELS;\"total_logging_steps\": training_cfg[\"logging_steps\"] * training_args.gradient_accumulation_steps};model = PrototypeContrastiveForSequenceClassification.from_pretrained(\"katarinayuan/scCello-zeroshot\";**model_kwargs);export HF_HOME=\"/path/to/another/directory/datasets\";from sccello.src.utils import data_loading;train_dataset = load_dataset(\"katarinayuan/scCello_pretrain_unsplitted\")[\"train\"];train_dataset;indist_test_data = train_dataset.train_test_split(test_size=0.001;seed=237);d1_ct;d2_ct = data_loading.get_fracdata(\"celltype\";\"frac100\";False;False);d1_ts;d2_ts = data_loading.get_fracdata(\"tissue\";\"frac100\";False;False);d1_dn;d2_dn = data_loading.get_fracdata(\"donor\";\"frac100\";False;False)", "cmd_prefix": "pip", "cmd_postfix": "install gdown", "target_cmd": "pip install gdown"}
{"uuid": "d066e9c0-7558-48b8-9015-033add156e60", "execution_plan": "step1: Install the required dependencies using conda and pip; step2: Download the model checkpoints for zero-shot inference or linear probing tasks; step3: Set up the environment for pre-training and downstream datasets by specifying the data caching directory; step4: Download example h5ad data for format transformation; step5: Transform h5ad data format to HuggingFace format; step6: Perform downstream generalization tasks such as cell type clustering, batch integration, cell type classification, and novel cell type classification; step7: Perform downstream transferability tasks such as marker gene prediction and cancer drug response prediction; step8: Run pre-training for the scCello model", "executed_cmds": "conda install pytorch==2.0.1 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install transformers[torch];pip install easydict;pip install psutil;pip install wandb;pip install pytz;pip install ipdb;pip install pandas;pip install datasets;pip install torchmetrics;pip install rdflib;pip install hickle;pip install anndata;pip install scikit-learn;pip install scanpy;pip install scib;conda install -c conda-forge cupy;conda install rapidsai::cuml;conda install -c rapidsai -c conda-forge -c nvidia cugraph;from sccello.src.model_prototype_contrastive import PrototypeContrastiveForMaskedLM;model = PrototypeContrastiveForMaskedLM.from_pretrained(\"katarinayuan/scCello-zeroshot\";output_hidden_states=True);from sccello.src.model_prototype_contrastive import PrototypeContrastiveForSequenceClassification;model_kwargs = {\"num_labels\": NUM_LABELS;\"total_logging_steps\": training_cfg[\"logging_steps\"] * training_args.gradient_accumulation_steps};model = PrototypeContrastiveForSequenceClassification.from_pretrained(\"katarinayuan/scCello-zeroshot\";**model_kwargs);export HF_HOME=\"/path/to/another/directory/datasets\";from sccello.src.utils import data_loading;train_dataset = load_dataset(\"katarinayuan/scCello_pretrain_unsplitted\")[\"train\"];train_dataset;indist_test_data = train_dataset.train_test_split(test_size=0.001;seed=237);d1_ct;d2_ct = data_loading.get_fracdata(\"celltype\";\"frac100\";False;False);d1_ts;d2_ts = data_loading.get_fracdata(\"tissue\";\"frac100\";False;False);d1_dn;d2_dn = data_loading.get_fracdata(\"donor\";\"frac100\";False;False);pip install gdown", "cmd_prefix": "cd", "cmd_postfix": "./data/example_h5ad/", "target_cmd": "cd ./data/example_h5ad/"}
{"uuid": "3ce25d4e-d314-4e60-9028-5e2a22fd2b83", "execution_plan": "step1: Set up the environment by installing dependencies using conda; step2: Download and prepare the DeepCAD data for processing; step3: Generate vector representation from DeepCAD JSON files; step4: Download text annotations and preprocessed training/validation data; step5: Configure the training YAML file with necessary paths; step6: Train the model using the provided training script; step7: Configure the inference YAML file for testing with the test dataset; step8: Run inference on the test dataset using the provided checkpoint; step9: Evaluate the generated sequences using the evaluation script; step10: Configure the inference YAML file for user-provided text prompts; step11: Run inference with a single user-provided text prompt; step12: Run inference with multiple user-provided text prompts; step13: Configure the inference YAML file for the demo; step14: Run the demo using Gradio", "executed_cmds": "conda env create --file environment.yml", "cmd_prefix": "cd", "cmd_postfix": "CadSeqProc", "target_cmd": "cd CadSeqProc"}
{"uuid": "01d64475-9e94-4689-a042-bcb973810f33", "execution_plan": "step1: Set up the environment by installing dependencies using conda; step2: Download and prepare the DeepCAD data for processing; step3: Generate vector representation from DeepCAD JSON files; step4: Download text annotations and preprocessed training/validation data; step5: Configure the training YAML file with necessary paths; step6: Train the model using the provided training script; step7: Configure the inference YAML file for testing with the test dataset; step8: Run inference on the test dataset using the provided checkpoint; step9: Evaluate the generated sequences using the evaluation script; step10: Configure the inference YAML file for user-provided text prompts; step11: Run inference with a single user-provided text prompt; step12: Run inference with multiple user-provided text prompts; step13: Configure the inference YAML file for the demo; step14: Run the demo using Gradio", "executed_cmds": "conda env create --file environment.yml;cd CadSeqProc;python3 json2vec.py --input_dir $DEEPCAD_JSON --split_json $TRAIN_TEST_VAL_JSON --output_dir $OUTPUT_DIR --max_workers $WORKERS --padding --deduplicate", "cmd_prefix": "cd", "cmd_postfix": "Cad_VLM", "target_cmd": "cd Cad_VLM"}
{"uuid": "39a9f64f-4021-42e0-92e9-dda2bc55dd48", "execution_plan": "step1: Set up the environment by installing dependencies using conda; step2: Download and prepare the DeepCAD data for processing; step3: Generate vector representation from DeepCAD JSON files; step4: Download text annotations and preprocessed training/validation data; step5: Configure the training YAML file with necessary paths; step6: Train the model using the provided training script; step7: Configure the inference YAML file for testing with the test dataset; step8: Run inference on the test dataset using the provided checkpoint; step9: Evaluate the generated sequences using the evaluation script; step10: Configure the inference YAML file for user-provided text prompts; step11: Run inference with a single user-provided text prompt; step12: Run inference with multiple user-provided text prompts; step13: Configure the inference YAML file for the demo; step14: Run the demo using Gradio", "executed_cmds": "conda env create --file environment.yml;cd CadSeqProc;python3 json2vec.py --input_dir $DEEPCAD_JSON --split_json $TRAIN_TEST_VAL_JSON --output_dir $OUTPUT_DIR --max_workers $WORKERS --padding --deduplicate;cd Cad_VLM;python3 train.py --config_path config/trainer.yaml", "cmd_prefix": "cd", "cmd_postfix": "Cad_VLM", "target_cmd": "cd Cad_VLM"}
{"uuid": "c40d9666-35f8-46d5-875c-5ca1fad76556", "execution_plan": "step1: Set up the environment by installing dependencies using conda; step2: Download and prepare the DeepCAD data for processing; step3: Generate vector representation from DeepCAD JSON files; step4: Download text annotations and preprocessed training/validation data; step5: Configure the training YAML file with necessary paths; step6: Train the model using the provided training script; step7: Configure the inference YAML file for testing with the test dataset; step8: Run inference on the test dataset using the provided checkpoint; step9: Evaluate the generated sequences using the evaluation script; step10: Configure the inference YAML file for user-provided text prompts; step11: Run inference with a single user-provided text prompt; step12: Run inference with multiple user-provided text prompts; step13: Configure the inference YAML file for the demo; step14: Run the demo using Gradio", "executed_cmds": "conda env create --file environment.yml;cd CadSeqProc;python3 json2vec.py --input_dir $DEEPCAD_JSON --split_json $TRAIN_TEST_VAL_JSON --output_dir $OUTPUT_DIR --max_workers $WORKERS --padding --deduplicate;cd Cad_VLM;python3 train.py --config_path config/trainer.yaml;cd Cad_VLM;python3 test.py --config_path config/inference.yaml", "cmd_prefix": "cd", "cmd_postfix": "Evaluation", "target_cmd": "cd Evaluation"}
{"uuid": "1bb44eaa-9e5f-4925-ad67-f2f0e8e1f3b6", "execution_plan": "step1: Set up the environment by installing dependencies using conda; step2: Download and prepare the DeepCAD data for processing; step3: Generate vector representation from DeepCAD JSON files; step4: Download text annotations and preprocessed training/validation data; step5: Configure the training YAML file with necessary paths; step6: Train the model using the provided training script; step7: Configure the inference YAML file for testing with the test dataset; step8: Run inference on the test dataset using the provided checkpoint; step9: Evaluate the generated sequences using the evaluation script; step10: Configure the inference YAML file for user-provided text prompts; step11: Run inference with a single user-provided text prompt; step12: Run inference with multiple user-provided text prompts; step13: Configure the inference YAML file for the demo; step14: Run the demo using Gradio", "executed_cmds": "conda env create --file environment.yml;cd CadSeqProc;python3 json2vec.py --input_dir $DEEPCAD_JSON --split_json $TRAIN_TEST_VAL_JSON --output_dir $OUTPUT_DIR --max_workers $WORKERS --padding --deduplicate;cd Cad_VLM;python3 train.py --config_path config/trainer.yaml;cd Cad_VLM;python3 test.py --config_path config/inference.yaml;cd Evaluation;python3 eval_seq.py --input_path ./output.pkl --output_dir ./output", "cmd_prefix": "cd", "cmd_postfix": "Cad_VLM", "target_cmd": "cd Cad_VLM"}
{"uuid": "92543483-d1a0-4cd0-bba3-43cd6bcbbc7b", "execution_plan": "step1: Set up the environment by installing dependencies using conda; step2: Download and prepare the DeepCAD data for processing; step3: Generate vector representation from DeepCAD JSON files; step4: Download text annotations and preprocessed training/validation data; step5: Configure the training YAML file with necessary paths; step6: Train the model using the provided training script; step7: Configure the inference YAML file for testing with the test dataset; step8: Run inference on the test dataset using the provided checkpoint; step9: Evaluate the generated sequences using the evaluation script; step10: Configure the inference YAML file for user-provided text prompts; step11: Run inference with a single user-provided text prompt; step12: Run inference with multiple user-provided text prompts; step13: Configure the inference YAML file for the demo; step14: Run the demo using Gradio", "executed_cmds": "conda env create --file environment.yml;cd CadSeqProc;python3 json2vec.py --input_dir $DEEPCAD_JSON --split_json $TRAIN_TEST_VAL_JSON --output_dir $OUTPUT_DIR --max_workers $WORKERS --padding --deduplicate;cd Cad_VLM;python3 train.py --config_path config/trainer.yaml;cd Cad_VLM;python3 test.py --config_path config/inference.yaml;cd Evaluation;python3 eval_seq.py --input_path ./output.pkl --output_dir ./output;cd Cad_VLM;python3 test_user_input.py --config_path config/inference_user_input.yaml --prompt \"A rectangular prism with a hole in the middle.\"", "cmd_prefix": "cd", "cmd_postfix": "Cad_VLM", "target_cmd": "cd Cad_VLM"}
{"uuid": "ba9a4dc9-895e-4c08-90b6-9bb83123d5a0", "execution_plan": "step1: Set up the environment by installing dependencies using conda; step2: Download and prepare the DeepCAD data for processing; step3: Generate vector representation from DeepCAD JSON files; step4: Download text annotations and preprocessed training/validation data; step5: Configure the training YAML file with necessary paths; step6: Train the model using the provided training script; step7: Configure the inference YAML file for testing with the test dataset; step8: Run inference on the test dataset using the provided checkpoint; step9: Evaluate the generated sequences using the evaluation script; step10: Configure the inference YAML file for user-provided text prompts; step11: Run inference with a single user-provided text prompt; step12: Run inference with multiple user-provided text prompts; step13: Configure the inference YAML file for the demo; step14: Run the demo using Gradio", "executed_cmds": "conda env create --file environment.yml;cd CadSeqProc;python3 json2vec.py --input_dir $DEEPCAD_JSON --split_json $TRAIN_TEST_VAL_JSON --output_dir $OUTPUT_DIR --max_workers $WORKERS --padding --deduplicate;cd Cad_VLM;python3 train.py --config_path config/trainer.yaml;cd Cad_VLM;python3 test.py --config_path config/inference.yaml;cd Evaluation;python3 eval_seq.py --input_path ./output.pkl --output_dir ./output;cd Cad_VLM;python3 test_user_input.py --config_path config/inference_user_input.yaml --prompt \"A rectangular prism with a hole in the middle.\";cd Cad_VLM;python3 test_user_input.py --config_path config/inference_user_input.yaml", "cmd_prefix": "cd", "cmd_postfix": "App", "target_cmd": "cd App"}
{"uuid": "67aa80bf-b299-4e97-b560-d492e19109ee", "execution_plan": "step1: Set up the environment by installing dependencies using conda; step2: Download and prepare the DeepCAD data for processing; step3: Generate vector representation from DeepCAD JSON files; step4: Download text annotations and preprocessed training/validation data; step5: Configure the training YAML file with necessary paths; step6: Train the model using the provided training script; step7: Configure the inference YAML file for testing with the test dataset; step8: Run inference on the test dataset using the provided checkpoint; step9: Evaluate the generated sequences using the evaluation script; step10: Configure the inference YAML file for user-provided text prompts; step11: Run inference with a single user-provided text prompt; step12: Run inference with multiple user-provided text prompts; step13: Configure the inference YAML file for the demo; step14: Run the demo using Gradio", "executed_cmds": "conda env create --file environment.yml;cd CadSeqProc;python3 json2vec.py --input_dir $DEEPCAD_JSON --split_json $TRAIN_TEST_VAL_JSON --output_dir $OUTPUT_DIR --max_workers $WORKERS --padding --deduplicate;cd Cad_VLM;python3 train.py --config_path config/trainer.yaml;cd Cad_VLM;python3 test.py --config_path config/inference.yaml;cd Evaluation;python3 eval_seq.py --input_path ./output.pkl --output_dir ./output;cd Cad_VLM;python3 test_user_input.py --config_path config/inference_user_input.yaml --prompt \"A rectangular prism with a hole in the middle.\";cd Cad_VLM;python3 test_user_input.py --config_path config/inference_user_input.yaml;cd App", "cmd_prefix": "gradio", "cmd_postfix": "app.py", "target_cmd": "gradio app.py"}
{"uuid": "aaf68627-4dc8-4840-9089-5d257980b3b1", "execution_plan": "step1: Check and ensure Python 3.9+ and PyTorch 1.12+ are installed; step2: Clone the repository from GitHub; step3: Navigate to the desired experiment folder (RoBERTa, ViT, LlaMA, or QControl); step4: Install additional dependencies specific to the chosen experiment folder; step5: Follow the instructions in the chosen experiment folder to replicate results or run experiments", "executed_cmds": "git clone https://github.com/czhang024/ParallelControl", "cmd_prefix": "cd", "cmd_postfix": "RoBERTa", "target_cmd": "cd RoBERTa"}
{"uuid": "d34f1b8e-52f5-4fb9-bbd4-91104b5f638c", "execution_plan": "step1: Clone the NoPoSplat repository and navigate into the project directory; step2: Create a conda environment and install the required dependencies including PyTorch and other packages listed in requirements.txt; step3: Optionally compile the CUDA kernels for RoPE positional embeddings to improve runtime performance; step4: Download the pre-trained checkpoints from Hugging Face and place them in the pretrained_weights directory; step5: Download the MASt3R pretrained model and place it in the pretrained_weights directory for training; step6: Train the model using the specified configuration, adjusting batch size and learning rate if necessary; step7: Evaluate the model for novel view synthesis on different datasets (RealEstate10K, ACID, DTU, ScanNet++, RealEstate10K 3 Input Views); step8: Evaluate the model for pose estimation on different datasets (RealEstate10K, ACID, ScanNet-1500)", "executed_cmds": "git clone https://github.com/cvg/NoPoSplat", "cmd_prefix": "cd", "cmd_postfix": "NoPoSplat", "target_cmd": "cd NoPoSplat"}
{"uuid": "1be07d01-ac3e-4546-a14b-b87ca323aa7b", "execution_plan": "step1: Clone the NoPoSplat repository and navigate into the project directory; step2: Create a conda environment and install the required dependencies including PyTorch and other packages listed in requirements.txt; step3: Optionally compile the CUDA kernels for RoPE positional embeddings to improve runtime performance; step4: Download the pre-trained checkpoints from Hugging Face and place them in the pretrained_weights directory; step5: Download the MASt3R pretrained model and place it in the pretrained_weights directory for training; step6: Train the model using the specified configuration, adjusting batch size and learning rate if necessary; step7: Evaluate the model for novel view synthesis on different datasets (RealEstate10K, ACID, DTU, ScanNet++, RealEstate10K 3 Input Views); step8: Evaluate the model for pose estimation on different datasets (RealEstate10K, ACID, ScanNet-1500)", "executed_cmds": "git clone https://github.com/cvg/NoPoSplat;cd NoPoSplat;conda create -y -n noposplat python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate noposplat", "target_cmd": "conda activate noposplat"}
{"uuid": "96b7ee35-1020-4d65-afb1-a68a56696ed0", "execution_plan": "step1: Clone the NoPoSplat repository and navigate into the project directory; step2: Create a conda environment and install the required dependencies including PyTorch and other packages listed in requirements.txt; step3: Optionally compile the CUDA kernels for RoPE positional embeddings to improve runtime performance; step4: Download the pre-trained checkpoints from Hugging Face and place them in the pretrained_weights directory; step5: Download the MASt3R pretrained model and place it in the pretrained_weights directory for training; step6: Train the model using the specified configuration, adjusting batch size and learning rate if necessary; step7: Evaluate the model for novel view synthesis on different datasets (RealEstate10K, ACID, DTU, ScanNet++, RealEstate10K 3 Input Views); step8: Evaluate the model for pose estimation on different datasets (RealEstate10K, ACID, ScanNet-1500)", "executed_cmds": "git clone https://github.com/cvg/NoPoSplat;cd NoPoSplat;conda create -y -n noposplat python=3.10;conda activate noposplat;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "1b90d3f3-8bed-476d-836c-3090163db232", "execution_plan": "step1: Clone the NoPoSplat repository and navigate into the project directory; step2: Create a conda environment and install the required dependencies including PyTorch and other packages listed in requirements.txt; step3: Optionally compile the CUDA kernels for RoPE positional embeddings to improve runtime performance; step4: Download the pre-trained checkpoints from Hugging Face and place them in the pretrained_weights directory; step5: Download the MASt3R pretrained model and place it in the pretrained_weights directory for training; step6: Train the model using the specified configuration, adjusting batch size and learning rate if necessary; step7: Evaluate the model for novel view synthesis on different datasets (RealEstate10K, ACID, DTU, ScanNet++, RealEstate10K 3 Input Views); step8: Evaluate the model for pose estimation on different datasets (RealEstate10K, ACID, ScanNet-1500)", "executed_cmds": "git clone https://github.com/cvg/NoPoSplat;cd NoPoSplat;conda create -y -n noposplat python=3.10;conda activate noposplat;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118;pip install -r requirements.txt;cd src/model/encoder/backbone/croco/curope/", "cmd_prefix": "python setup.py", "cmd_postfix": "build_ext --inplace", "target_cmd": "python setup.py build_ext --inplace"}
{"uuid": "536df72f-e304-4c0e-81f1-e511cd7041a8", "execution_plan": "step1: Clone the NoPoSplat repository and navigate into the project directory; step2: Create a conda environment and install the required dependencies including PyTorch and other packages listed in requirements.txt; step3: Optionally compile the CUDA kernels for RoPE positional embeddings to improve runtime performance; step4: Download the pre-trained checkpoints from Hugging Face and place them in the pretrained_weights directory; step5: Download the MASt3R pretrained model and place it in the pretrained_weights directory for training; step6: Train the model using the specified configuration, adjusting batch size and learning rate if necessary; step7: Evaluate the model for novel view synthesis on different datasets (RealEstate10K, ACID, DTU, ScanNet++, RealEstate10K 3 Input Views); step8: Evaluate the model for pose estimation on different datasets (RealEstate10K, ACID, ScanNet-1500)", "executed_cmds": "git clone https://github.com/cvg/NoPoSplat;cd NoPoSplat;conda create -y -n noposplat python=3.10;conda activate noposplat;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118;pip install -r requirements.txt;cd src/model/encoder/backbone/croco/curope/;python setup.py build_ext --inplace", "cmd_prefix": "cd", "cmd_postfix": "../../../../../..", "target_cmd": "cd ../../../../../.."}
{"uuid": "228d3c5f-c89f-4a42-894f-fe653275805e", "execution_plan": "step1: Create a conda environment for the project; step2: Install required dependencies including PyTorch3D and Blender; step3: Download and prepare the dataset, either using the provided preprocessed data or custom data; step4: Download the pretrained model and unzip it; step5: Run the demo to see motion retargeting in action; step6: Compute metrics to evaluate the model's performance; step7: Train the model from scratch if desired", "executed_cmds": "conda create -n MeshRet python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate MeshRet", "target_cmd": "conda activate MeshRet"}
{"uuid": "0f9d823f-a563-4fb8-b621-169e3b2fd4a2", "execution_plan": "step1: Create a conda environment for the project; step2: Install required dependencies including PyTorch3D and Blender; step3: Download and prepare the dataset, either using the provided preprocessed data or custom data; step4: Download the pretrained model and unzip it; step5: Run the demo to see motion retargeting in action; step6: Compute metrics to evaluate the model's performance; step7: Train the model from scratch if desired", "executed_cmds": "conda create -n MeshRet python=3.10;conda activate MeshRet", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements", "target_cmd": "pip install -r requirements"}
{"uuid": "3a5abaf0-9d0f-4781-ac73-8a0ee1885750", "execution_plan": "step1: Install dependencies using conda environment; step2: Download pre-trained models from Hugging Face; step3: Set up CIFAR-10 dataset by downloading and converting it; step4: Set up ImageNet-256 dataset by downloading, cropping, resizing, and encoding it; step5: Calculate reference statistics for the ImageNet-256 dataset; step6: Train the model on a single node or multi-node using Slurm; step7: Generate images using the trained model; step8: Evaluate the model using FID scores", "executed_cmds": "conda env create -f env.yml", "cmd_prefix": "pip", "cmd_postfix": "install submitit", "target_cmd": "pip install submitit"}
{"uuid": "b9ae647b-e995-4763-a338-189e974f825e", "execution_plan": "step1: Set up the environment for vision tasks by creating a conda environment and installing required packages; step2: Set up the environment for language tasks by creating a conda environment and installing required packages; step3: Install the Tree Scanning module for vision tasks; step4: Install the Tree Scanning module for language tasks; step5: Install language model evaluation tools for language tasks; step6: Train and evaluate the model for ImageNet-1k image classification; step7: Train and evaluate the model for language understanding tasks", "executed_cmds": "conda create -n grootv python=3.9", "cmd_prefix": "conda", "cmd_postfix": "activate grootv", "target_cmd": "conda activate grootv"}
{"uuid": "509ca2b6-669b-4e95-81f3-24aafcb3a77d", "execution_plan": "step1: Set up the environment for vision tasks by creating a conda environment and installing required packages; step2: Set up the environment for language tasks by creating a conda environment and installing required packages; step3: Install the Tree Scanning module for vision tasks; step4: Install the Tree Scanning module for language tasks; step5: Install language model evaluation tools for language tasks; step6: Train and evaluate the model for ImageNet-1k image classification; step7: Train and evaluate the model for language understanding tasks", "executed_cmds": "conda create -n grootv python=3.9;conda activate grootv;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117;pip install -r GrootV/grootv_requirements.txt", "cmd_prefix": "conda create", "cmd_postfix": "-n grootl python=3.9", "target_cmd": "conda create -n grootl python=3.9"}
{"uuid": "ddb6448f-9cec-4ac9-9ee3-1d96668bb2cd", "execution_plan": "step1: Set up the environment for vision tasks by creating a conda environment and installing required packages; step2: Set up the environment for language tasks by creating a conda environment and installing required packages; step3: Install the Tree Scanning module for vision tasks; step4: Install the Tree Scanning module for language tasks; step5: Install language model evaluation tools for language tasks; step6: Train and evaluate the model for ImageNet-1k image classification; step7: Train and evaluate the model for language understanding tasks", "executed_cmds": "conda create -n grootv python=3.9;conda activate grootv;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117;pip install -r GrootV/grootv_requirements.txt;conda create -n grootl python=3.9", "cmd_prefix": "conda", "cmd_postfix": "activate grootl", "target_cmd": "conda activate grootl"}
{"uuid": "08b4d044-d6a8-4231-a667-8e61a643164d", "execution_plan": "step1: Set up the environment for vision tasks by creating a conda environment and installing required packages; step2: Set up the environment for language tasks by creating a conda environment and installing required packages; step3: Install the Tree Scanning module for vision tasks; step4: Install the Tree Scanning module for language tasks; step5: Install language model evaluation tools for language tasks; step6: Train and evaluate the model for ImageNet-1k image classification; step7: Train and evaluate the model for language understanding tasks", "executed_cmds": "conda create -n grootv python=3.9;conda activate grootv;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117;pip install -r GrootV/grootv_requirements.txt;conda create -n grootl python=3.9;conda activate grootl;pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2;pip install -r GrootL/grootl_requirements.txt;cd GrootV/third-party/TreeScan", "cmd_prefix": "pip install", "cmd_postfix": "-v -e .", "target_cmd": "pip install -v -e ."}
{"uuid": "8336a38a-e867-4354-86e7-4ec420de7a3c", "execution_plan": "step1: Set up the environment for vision tasks by creating a conda environment and installing required packages; step2: Set up the environment for language tasks by creating a conda environment and installing required packages; step3: Install the Tree Scanning module for vision tasks; step4: Install the Tree Scanning module for language tasks; step5: Install language model evaluation tools for language tasks; step6: Train and evaluate the model for ImageNet-1k image classification; step7: Train and evaluate the model for language understanding tasks", "executed_cmds": "conda create -n grootv python=3.9;conda activate grootv;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117;pip install -r GrootV/grootv_requirements.txt;conda create -n grootl python=3.9;conda activate grootl;pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2;pip install -r GrootL/grootl_requirements.txt;cd GrootV/third-party/TreeScan;pip install -v -e .;cd GrootL/third-party/TreeScanLan", "cmd_prefix": "pip install", "cmd_postfix": "-v -e .", "target_cmd": "pip install -v -e ."}
{"uuid": "ae88f79f-74c2-4304-b283-2f5dec983cef", "execution_plan": "step1: Set up the environment for vision tasks by creating a conda environment and installing required packages; step2: Set up the environment for language tasks by creating a conda environment and installing required packages; step3: Install the Tree Scanning module for vision tasks; step4: Install the Tree Scanning module for language tasks; step5: Install language model evaluation tools for language tasks; step6: Train and evaluate the model for ImageNet-1k image classification; step7: Train and evaluate the model for language understanding tasks", "executed_cmds": "conda create -n grootv python=3.9;conda activate grootv;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117;pip install -r GrootV/grootv_requirements.txt;conda create -n grootl python=3.9;conda activate grootl;pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2;pip install -r GrootL/grootl_requirements.txt;cd GrootV/third-party/TreeScan;pip install -v -e .;cd GrootL/third-party/TreeScanLan;pip install -v -e .;cd GrootL/3rdparty/lm-evaluation-harness", "cmd_prefix": "pip install", "cmd_postfix": "-v -e .", "target_cmd": "pip install -v -e ."}
{"uuid": "0e1bd55e-7bd5-4da6-a15f-84ec70641630", "execution_plan": "step1: Set up the environment for vision tasks by creating a conda environment and installing required packages; step2: Set up the environment for language tasks by creating a conda environment and installing required packages; step3: Install the Tree Scanning module for vision tasks; step4: Install the Tree Scanning module for language tasks; step5: Install language model evaluation tools for language tasks; step6: Train and evaluate the model for ImageNet-1k image classification; step7: Train and evaluate the model for language understanding tasks", "executed_cmds": "conda create -n grootv python=3.9;conda activate grootv;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117;pip install -r GrootV/grootv_requirements.txt;conda create -n grootl python=3.9;conda activate grootl;pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2;pip install -r GrootL/grootl_requirements.txt;cd GrootV/third-party/TreeScan;pip install -v -e .;cd GrootL/third-party/TreeScanLan;pip install -v -e .;cd GrootL/3rdparty/lm-evaluation-harness;pip install -v -e .;bash GrootV/scripts/bash_cls_train.sh", "cmd_prefix": "cd", "cmd_postfix": "GrootL", "target_cmd": "cd GrootL"}
{"uuid": "f114cb00-8b64-4ba9-b7fa-30bd8d21abdf", "execution_plan": "step1: Set up the environment for vision tasks by creating a conda environment and installing required packages; step2: Set up the environment for language tasks by creating a conda environment and installing required packages; step3: Install the Tree Scanning module for vision tasks; step4: Install the Tree Scanning module for language tasks; step5: Install language model evaluation tools for language tasks; step6: Train and evaluate the model for ImageNet-1k image classification; step7: Train and evaluate the model for language understanding tasks", "executed_cmds": "conda create -n grootv python=3.9;conda activate grootv;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117;pip install -r GrootV/grootv_requirements.txt;conda create -n grootl python=3.9;conda activate grootl;pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2;pip install -r GrootL/grootl_requirements.txt;cd GrootV/third-party/TreeScan;pip install -v -e .;cd GrootL/third-party/TreeScanLan;pip install -v -e .;cd GrootL/3rdparty/lm-evaluation-harness;pip install -v -e .;bash GrootV/scripts/bash_cls_train.sh;cd GrootL", "cmd_prefix": "bash", "cmd_postfix": "eval.sh", "target_cmd": "bash eval.sh"}
{"uuid": "1a14b8e3-98d1-48fc-874d-a092a051f3bf", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Install the required dependencies using pip; step3: Download the necessary data files from the provided Google Drive link; step4: Set up the data path environment variable or place the data in the specified folder; step5: Install optional interactive mode dependencies if needed; step6: Run evaluation notebooks to test the pretrained models", "executed_cmds": "git clone https://github.com/scaomath/galerkin-transformer.git", "cmd_prefix": "cd", "cmd_postfix": "galerkin-transformer", "target_cmd": "cd galerkin-transformer"}
{"uuid": "4a1fd159-dd54-4808-bbdb-a393b5ced0c6", "execution_plan": "step1: Set up the environment by installing the required dependencies (OpenAI, numpy, pandas, torch); step2: Obtain the necessary API keys (OpenAI, RapidAPI, ToolBench, TMDB, Spotify); step3: Download and set up the datasets (ToolBench and RestBench) or use preprocessed tool documentation; step4: Run DRAFT to get revised tool documentation; step5: Perform inference using the modified tool documentation to examine the effectiveness of DRAFT; step6: Calculate the path rate for evaluating the results; step7: Use ToolEval to calculate the win rate for further evaluation", "executed_cmds": "pip install openai==0.28.0 numpy==1.26.4 pandas==2.2.2 torch==2.3.1", "cmd_prefix": "python", "cmd_postfix": "DRAFT.py", "target_cmd": "python DRAFT.py"}
{"uuid": "7f30653a-9bff-487c-a70f-dcbed50e95a1", "execution_plan": "step1: Set up the environment by installing the required dependencies (OpenAI, numpy, pandas, torch); step2: Obtain the necessary API keys (OpenAI, RapidAPI, ToolBench, TMDB, Spotify); step3: Download and set up the datasets (ToolBench and RestBench) or use preprocessed tool documentation; step4: Run DRAFT to get revised tool documentation; step5: Perform inference using the modified tool documentation to examine the effectiveness of DRAFT; step6: Calculate the path rate for evaluating the results; step7: Use ToolEval to calculate the win rate for further evaluation", "executed_cmds": "pip install openai==0.28.0 numpy==1.26.4 pandas==2.2.2 torch==2.3.1;python DRAFT.py;python Inference_DFSDT -model_name gpt-4o-2024-08-06 -data_type G3 -method DRAFT", "cmd_prefix": "python", "cmd_postfix": "Cal_path_rate.py", "target_cmd": "python Cal_path_rate.py"}
{"uuid": "c9d22587-df6a-4a2e-bfb3-441d6cf8af29", "execution_plan": "step1: Create a virtual environment named 'bright' using conda and activate it; step2: Clone the BRIGHT repository from GitHub and navigate into the cloned directory; step3: Install OpenJDK 22 in the 'bright' environment using conda; step4: Install the required Python packages listed in the requirements.txt file; step5: Download the BRIGHT dataset from Huggingface for evaluation (optional, if data is not already available); step6: Run the evaluation script with the desired task and model parameters; step7: Add a custom model to the evaluation pipeline by implementing the required function in retrievers.py (optional)", "executed_cmds": "conda create -n bright python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate bright", "target_cmd": "conda activate bright"}
{"uuid": "e2de21b7-663a-40d3-bcff-a6a65ca10bf5", "execution_plan": "step1: Create a virtual environment named 'bright' using conda and activate it; step2: Clone the BRIGHT repository from GitHub and navigate into the cloned directory; step3: Install OpenJDK 22 in the 'bright' environment using conda; step4: Install the required Python packages listed in the requirements.txt file; step5: Download the BRIGHT dataset from Huggingface for evaluation (optional, if data is not already available); step6: Run the evaluation script with the desired task and model parameters; step7: Add a custom model to the evaluation pipeline by implementing the required function in retrievers.py (optional)", "executed_cmds": "conda create -n bright python=3.10;conda activate bright;git clone https://github.com/xlang-ai/BRIGHT", "cmd_prefix": "cd", "cmd_postfix": "BRIGHT", "target_cmd": "cd BRIGHT"}
{"uuid": "0f8d9d78-2eac-4e94-aaf5-88310e02c331", "execution_plan": "step1: Create a virtual environment named 'bright' using conda and activate it; step2: Clone the BRIGHT repository from GitHub and navigate into the cloned directory; step3: Install OpenJDK 22 in the 'bright' environment using conda; step4: Install the required Python packages listed in the requirements.txt file; step5: Download the BRIGHT dataset from Huggingface for evaluation (optional, if data is not already available); step6: Run the evaluation script with the desired task and model parameters; step7: Add a custom model to the evaluation pipeline by implementing the required function in retrievers.py (optional)", "executed_cmds": "conda create -n bright python=3.10;conda activate bright;git clone https://github.com/xlang-ai/BRIGHT;cd BRIGHT;conda install -n bright -c conda-forge openjdk=22", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "00e46a0c-958a-458b-82dd-e6cfa66bcb5d", "execution_plan": "step1: Clone the libpgo repository and navigate to its projects directory; step2: Clone the PhysComp repository inside the libpgo projects directory; step3: Install required system dependencies (libgmp-dev and libmpfr-dev); step4: Create a conda environment named physcomp with Python 3.10; step5: Activate the conda environment and install additional dependencies (tbb, tbb-devel, mkl, mkl-devel); step6: Set the CMAKE_BUILD_PARALLEL_LEVEL environment variable; step7: Install libpgo and additional Python packages (trimesh, tetgen); step8: Navigate to the PhysComp directory and run the test script to execute the examples", "executed_cmds": "git clone git@github.com:bohanwang/libpgo.git", "cmd_prefix": "cd", "cmd_postfix": "libpgo", "target_cmd": "cd libpgo"}
{"uuid": "18e5fbd6-d917-4ef4-976c-9fd101156aa5", "execution_plan": "step1: Clone the libpgo repository and navigate to its projects directory; step2: Clone the PhysComp repository inside the libpgo projects directory; step3: Install required system dependencies (libgmp-dev and libmpfr-dev); step4: Create a conda environment named physcomp with Python 3.10; step5: Activate the conda environment and install additional dependencies (tbb, tbb-devel, mkl, mkl-devel); step6: Set the CMAKE_BUILD_PARALLEL_LEVEL environment variable; step7: Install libpgo and additional Python packages (trimesh, tetgen); step8: Navigate to the PhysComp directory and run the test script to execute the examples", "executed_cmds": "git clone git@github.com:bohanwang/libpgo.git;cd libpgo", "cmd_prefix": "mkdir", "cmd_postfix": "-p projects", "target_cmd": "mkdir -p projects"}
{"uuid": "b5caf3b4-2e46-416d-9a73-e9e12fc99626", "execution_plan": "step1: Clone the libpgo repository and navigate to its projects directory; step2: Clone the PhysComp repository inside the libpgo projects directory; step3: Install required system dependencies (libgmp-dev and libmpfr-dev); step4: Create a conda environment named physcomp with Python 3.10; step5: Activate the conda environment and install additional dependencies (tbb, tbb-devel, mkl, mkl-devel); step6: Set the CMAKE_BUILD_PARALLEL_LEVEL environment variable; step7: Install libpgo and additional Python packages (trimesh, tetgen); step8: Navigate to the PhysComp directory and run the test script to execute the examples", "executed_cmds": "git clone git@github.com:bohanwang/libpgo.git;cd libpgo;mkdir -p projects", "cmd_prefix": "cd", "cmd_postfix": "projects", "target_cmd": "cd projects"}
{"uuid": "4e0ebee3-7cfd-4906-b4b0-4bce65b5a72b", "execution_plan": "step1: Clone the libpgo repository and navigate to its projects directory; step2: Clone the PhysComp repository inside the libpgo projects directory; step3: Install required system dependencies (libgmp-dev and libmpfr-dev); step4: Create a conda environment named physcomp with Python 3.10; step5: Activate the conda environment and install additional dependencies (tbb, tbb-devel, mkl, mkl-devel); step6: Set the CMAKE_BUILD_PARALLEL_LEVEL environment variable; step7: Install libpgo and additional Python packages (trimesh, tetgen); step8: Navigate to the PhysComp directory and run the test script to execute the examples", "executed_cmds": "git clone git@github.com:bohanwang/libpgo.git;cd libpgo;mkdir -p projects;cd projects;git clone git@github.com:gmh14/PhysComp.git;sudo apt install libgmp-dev libmpfr-dev;conda create --name physcomp python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate physcomp", "target_cmd": "conda activate physcomp"}
{"uuid": "ec8e9c74-67db-4349-8875-1db5578f0bdf", "execution_plan": "step1: Clone the libpgo repository and navigate to its projects directory; step2: Clone the PhysComp repository inside the libpgo projects directory; step3: Install required system dependencies (libgmp-dev and libmpfr-dev); step4: Create a conda environment named physcomp with Python 3.10; step5: Activate the conda environment and install additional dependencies (tbb, tbb-devel, mkl, mkl-devel); step6: Set the CMAKE_BUILD_PARALLEL_LEVEL environment variable; step7: Install libpgo and additional Python packages (trimesh, tetgen); step8: Navigate to the PhysComp directory and run the test script to execute the examples", "executed_cmds": "git clone git@github.com:bohanwang/libpgo.git;cd libpgo;mkdir -p projects;cd projects;git clone git@github.com:gmh14/PhysComp.git;sudo apt install libgmp-dev libmpfr-dev;conda create --name physcomp python=3.10;conda activate physcomp;conda install tbb tbb-devel mkl mkl-devel;export CMAKE_BUILD_PARALLEL_LEVEL=8", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "b902758b-ec9e-4eaf-8994-8fd104d0fe53", "execution_plan": "step1: Clone the libpgo repository and navigate to its projects directory; step2: Clone the PhysComp repository inside the libpgo projects directory; step3: Install required system dependencies (libgmp-dev and libmpfr-dev); step4: Create a conda environment named physcomp with Python 3.10; step5: Activate the conda environment and install additional dependencies (tbb, tbb-devel, mkl, mkl-devel); step6: Set the CMAKE_BUILD_PARALLEL_LEVEL environment variable; step7: Install libpgo and additional Python packages (trimesh, tetgen); step8: Navigate to the PhysComp directory and run the test script to execute the examples", "executed_cmds": "git clone git@github.com:bohanwang/libpgo.git;cd libpgo;mkdir -p projects;cd projects;git clone git@github.com:gmh14/PhysComp.git;sudo apt install libgmp-dev libmpfr-dev;conda create --name physcomp python=3.10;conda activate physcomp;conda install tbb tbb-devel mkl mkl-devel;export CMAKE_BUILD_PARALLEL_LEVEL=8;cd ..", "cmd_prefix": "pip install", "cmd_postfix": "-v -e .", "target_cmd": "pip install -v -e ."}
{"uuid": "3892deda-4b33-41b2-b70a-5e03c88b9685", "execution_plan": "step1: Clone the libpgo repository and navigate to its projects directory; step2: Clone the PhysComp repository inside the libpgo projects directory; step3: Install required system dependencies (libgmp-dev and libmpfr-dev); step4: Create a conda environment named physcomp with Python 3.10; step5: Activate the conda environment and install additional dependencies (tbb, tbb-devel, mkl, mkl-devel); step6: Set the CMAKE_BUILD_PARALLEL_LEVEL environment variable; step7: Install libpgo and additional Python packages (trimesh, tetgen); step8: Navigate to the PhysComp directory and run the test script to execute the examples", "executed_cmds": "git clone git@github.com:bohanwang/libpgo.git;cd libpgo;mkdir -p projects;cd projects;git clone git@github.com:gmh14/PhysComp.git;sudo apt install libgmp-dev libmpfr-dev;conda create --name physcomp python=3.10;conda activate physcomp;conda install tbb tbb-devel mkl mkl-devel;export CMAKE_BUILD_PARALLEL_LEVEL=8;cd ..;pip install -v -e .", "cmd_prefix": "pip install", "cmd_postfix": "trimesh tetgen", "target_cmd": "pip install trimesh tetgen"}
{"uuid": "3388faf1-bf40-42e4-9f60-bfe1541cca3d", "execution_plan": "step1: Clone the libpgo repository and navigate to its projects directory; step2: Clone the PhysComp repository inside the libpgo projects directory; step3: Install required system dependencies (libgmp-dev and libmpfr-dev); step4: Create a conda environment named physcomp with Python 3.10; step5: Activate the conda environment and install additional dependencies (tbb, tbb-devel, mkl, mkl-devel); step6: Set the CMAKE_BUILD_PARALLEL_LEVEL environment variable; step7: Install libpgo and additional Python packages (trimesh, tetgen); step8: Navigate to the PhysComp directory and run the test script to execute the examples", "executed_cmds": "git clone git@github.com:bohanwang/libpgo.git;cd libpgo;mkdir -p projects;cd projects;git clone git@github.com:gmh14/PhysComp.git;sudo apt install libgmp-dev libmpfr-dev;conda create --name physcomp python=3.10;conda activate physcomp;conda install tbb tbb-devel mkl mkl-devel;export CMAKE_BUILD_PARALLEL_LEVEL=8;cd ..;pip install -v -e .;pip install trimesh tetgen", "cmd_prefix": "cd", "cmd_postfix": "projects/PhysComp", "target_cmd": "cd projects/PhysComp"}
{"uuid": "06d2ebf9-9df0-4c09-b6f1-19a218c02c3a", "execution_plan": "step1: Clone the libpgo repository and navigate to its projects directory; step2: Clone the PhysComp repository inside the libpgo projects directory; step3: Install required system dependencies (libgmp-dev and libmpfr-dev); step4: Create a conda environment named physcomp with Python 3.10; step5: Activate the conda environment and install additional dependencies (tbb, tbb-devel, mkl, mkl-devel); step6: Set the CMAKE_BUILD_PARALLEL_LEVEL environment variable; step7: Install libpgo and additional Python packages (trimesh, tetgen); step8: Navigate to the PhysComp directory and run the test script to execute the examples", "executed_cmds": "git clone git@github.com:bohanwang/libpgo.git;cd libpgo;mkdir -p projects;cd projects;git clone git@github.com:gmh14/PhysComp.git;sudo apt install libgmp-dev libmpfr-dev;conda create --name physcomp python=3.10;conda activate physcomp;conda install tbb tbb-devel mkl mkl-devel;export CMAKE_BUILD_PARALLEL_LEVEL=8;cd ..;pip install -v -e .;pip install trimesh tetgen;cd projects/PhysComp", "cmd_prefix": "python", "cmd_postfix": "scripts/test.py", "target_cmd": "python scripts/test.py"}
{"uuid": "4a2d58a0-ca20-4ea0-9462-ae7050e47f49", "execution_plan": "step1: Install the required package (transformers) to use the Sundial model; step2: Load the pre-trained Sundial model from HuggingFace using the provided Python code; step3: Prepare input data for the model by generating random sequences or using real data; step4: Generate predictions using the model with specified lookback and forecast lengths; step5: Analyze the output predictions for mean, quantiles, or confidence intervals; step6: Explore additional examples and functionalities provided in the quickstart notebook", "executed_cmds": "pip install transformers==4.40.1;import torch; from transformers import AutoModelForCausalLM; model = AutoModelForCausalLM.from_pretrained('thuml/sundial-base-128m';trust_remote_code=True);batch_size", "cmd_prefix": "lookback_length", "cmd_postfix": "= 1", "target_cmd": "lookback_length = 1"}
{"uuid": "3031acfe-0e8f-4b89-bb00-b2e5c8dfb1f5", "execution_plan": "step1: Set up the environment by creating a conda environment and installing dependencies; step2: Analyze LoRA modules to understand their behavior; step3: Train HydraLoRA using either single-GPU or DeepSpeed configuration; step4: Evaluate the trained model using opencompass, modifying necessary files for integration; step5: Cite the paper if the work is used or referenced", "executed_cmds": "conda create -n hydralora python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate hydralora", "target_cmd": "conda activate hydralora"}
{"uuid": "8e01cee7-6cfa-4f62-b2e3-88d358d18e70", "execution_plan": "step1: Set up the environment by creating a conda environment and installing dependencies; step2: Analyze LoRA modules to understand their behavior; step3: Train HydraLoRA using either single-GPU or DeepSpeed configuration; step4: Evaluate the trained model using opencompass, modifying necessary files for integration; step5: Cite the paper if the work is used or referenced", "executed_cmds": "conda create -n hydralora python=3.10;conda activate hydralora", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "3659a62b-2f59-4950-86e4-4a06e84f7c91", "execution_plan": "step1: Install the conda environment using the provided environment.yml file; step2: Activate the conda environment; step3: Generate new data if needed by running the shell script in the specified directory; step4: Run experiments by executing the shell scripts in the run_commands directory", "executed_cmds": "conda env create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate LDP_q_est", "target_cmd": "conda activate LDP_q_est"}
{"uuid": "816c1e05-042f-4ed5-963b-6e741151c328", "execution_plan": "step1: Install the required Python packages and dependencies; step2: Load the PatchCamelyon dataset using the Hugging Face datasets library; step3: Resize the dataset images to 224x224 pixels for compatibility with the ViT model; step4: Train the model using the `train.py` script with the `CovNExt` and `ViT` architectures; step5: Evaluate the trained model on the test set using the `eval.py` script; step6: Review the model's accuracy and results", "executed_cmds": "pip install torch transformers numpy pandas matplotlib seaborn tqdm pillow;from datasets import load_dataset; dataset = load_dataset('patch_camelyon')", "cmd_prefix": "python", "cmd_postfix": "train.py", "target_cmd": "python train.py"}
{"uuid": "b1e06927-e41b-4e1f-b481-6ebfa4a081b3", "execution_plan": "step1: Install the required Python packages and dependencies; step2: Load the PatchCamelyon dataset using the Hugging Face datasets library; step3: Resize the dataset images to 224x224 pixels for compatibility with the ViT model; step4: Train the model using the `train.py` script with the `CovNExt` and `ViT` architectures; step5: Evaluate the trained model on the test set using the `eval.py` script; step6: Review the model's accuracy and results", "executed_cmds": "pip install torch transformers numpy pandas matplotlib seaborn tqdm pillow;from datasets import load_dataset; dataset = load_dataset('patch_camelyon');python train.py", "cmd_prefix": "python", "cmd_postfix": "eval.py", "target_cmd": "python eval.py"}
{"uuid": "013e04bc-a50a-446d-9d39-c133e99aec97", "execution_plan": "step1: Install the project and its dependencies; step2: Set the OpenAI API key environment variable; step3: Reproduce the results from Section 3.4 (Validation in Synthetic Environment); step4: Reproduce the PROSE experiments from Section 4 (Dataset information and slate generation); step5: Run baseline comparisons for the PROSE experiments; step6: Preprocess a custom dataset for PROSE; step7: Generate summaries and tags for a custom dataset (Option 1); step8: Construct custom agents and queries for PROSE (Option 2); step9: Generate a slate for a custom dataset (Option 1 or Option 2)", "executed_cmds": "pip install -e .", "cmd_prefix": "pipenv", "cmd_postfix": "install", "target_cmd": "pipenv install"}
{"uuid": "59b8a226-ac21-4ea2-9bd2-72bd7d2710b2", "execution_plan": "step1: Install the project and its dependencies; step2: Set the OpenAI API key environment variable; step3: Reproduce the results from Section 3.4 (Validation in Synthetic Environment); step4: Reproduce the PROSE experiments from Section 4 (Dataset information and slate generation); step5: Run baseline comparisons for the PROSE experiments; step6: Preprocess a custom dataset for PROSE; step7: Generate summaries and tags for a custom dataset (Option 1); step8: Construct custom agents and queries for PROSE (Option 2); step9: Generate a slate for a custom dataset (Option 1 or Option 2)", "executed_cmds": "pip install -e .;pipenv install;python3 synthetic_experiments/main.py;python3 PROSE/generate_scripts/generate_slate.py --dataset bowlinggreen_41", "cmd_prefix": "python3 PROSE/generate_scripts/generate_slate.py", "cmd_postfix": "--dataset drugs_80", "target_cmd": "python3 PROSE/generate_scripts/generate_slate.py --dataset drugs_80"}
{"uuid": "cc7526b1-f7d0-45c4-8915-5d58b881be6f", "execution_plan": "step1: Download the MoE++ model from HuggingFace; step2: Install necessary dependencies including transformers and accelerate; step3: Load the model and tokenizer for inference; step4: Run inference using the loaded model and tokenizer; step5: Evaluate the model on benchmarks using the Eleuther AI Language Model Evaluation Harness", "executed_cmds": "pip install transformers accelerate;from transformers import AutoModelForCausalLM;AutoTokenizer;model = AutoModelForCausalLM.from_pretrained(\"Chat-UniVi/MoE-Plus-Plus-7B\";trust_remote_code=True;device_map='auto');tokenizer = AutoTokenizer.from_pretrained(\"Chat-UniVi/MoE-Plus-Plus-7B\";trust_remote_code=True)", "cmd_prefix": "inputs", "cmd_postfix": "= tokenizer(question", "target_cmd": "inputs = tokenizer(question"}
{"uuid": "7167e819-9df5-4945-ba8c-96f9046ba190", "execution_plan": "step1: Clone the repository and navigate to the project folder; step2: Switch to the dev branch and set up a Python environment; step3: Install the required dependencies; step4: Download the dataset from Hugging Face and unzip it; step5: Prepare the dataset by placing it in the correct folder structure; step6: Ensure the required models are available or set up; step7: Run the evaluation script with the specified configurations", "executed_cmds": "git clone https://github.com/opendatalab/LOKI.git", "cmd_prefix": "cd", "cmd_postfix": "LOKI", "target_cmd": "cd LOKI"}
{"uuid": "c068dcbb-8044-40bd-bdcd-0bd09ba30021", "execution_plan": "step1: Clone the repository and navigate to the project folder; step2: Switch to the dev branch and set up a Python environment; step3: Install the required dependencies; step4: Download the dataset from Hugging Face and unzip it; step5: Prepare the dataset by placing it in the correct folder structure; step6: Ensure the required models are available or set up; step7: Run the evaluation script with the specified configurations", "executed_cmds": "git clone https://github.com/opendatalab/LOKI.git;cd LOKI", "cmd_prefix": "git", "cmd_postfix": "checkout dev", "target_cmd": "git checkout dev"}
{"uuid": "505c4bf6-cb59-4737-82e0-7658ab2d7d41", "execution_plan": "step1: Clone the repository and navigate to the project folder; step2: Switch to the dev branch and set up a Python environment; step3: Install the required dependencies; step4: Download the dataset from Hugging Face and unzip it; step5: Prepare the dataset by placing it in the correct folder structure; step6: Ensure the required models are available or set up; step7: Run the evaluation script with the specified configurations", "executed_cmds": "git clone https://github.com/opendatalab/LOKI.git;cd LOKI;git checkout dev", "cmd_prefix": "conda create", "cmd_postfix": "-n loki python=3.10", "target_cmd": "conda create -n loki python=3.10"}
{"uuid": "9d41d43c-5f67-441b-ae65-0b55a47d25d7", "execution_plan": "step1: Clone the repository and navigate to the project folder; step2: Switch to the dev branch and set up a Python environment; step3: Install the required dependencies; step4: Download the dataset from Hugging Face and unzip it; step5: Prepare the dataset by placing it in the correct folder structure; step6: Ensure the required models are available or set up; step7: Run the evaluation script with the specified configurations", "executed_cmds": "git clone https://github.com/opendatalab/LOKI.git;cd LOKI;git checkout dev;conda create -n loki python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate loki", "target_cmd": "conda activate loki"}
{"uuid": "f3689065-baa2-435a-a400-06f7f53f6493", "execution_plan": "step1: Clone the repository and navigate to the project folder; step2: Switch to the dev branch and set up a Python environment; step3: Install the required dependencies; step4: Download the dataset from Hugging Face and unzip it; step5: Prepare the dataset by placing it in the correct folder structure; step6: Ensure the required models are available or set up; step7: Run the evaluation script with the specified configurations", "executed_cmds": "git clone https://github.com/opendatalab/LOKI.git;cd LOKI;git checkout dev;conda create -n loki python=3.10;conda activate loki", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "22ea17fe-4722-4262-8265-bd7f103c75ab", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git", "cmd_prefix": "cd", "cmd_postfix": "mmd", "target_cmd": "cd mmd"}
{"uuid": "99a65d38-20fe-4816-ba9d-d7f8506e0cdc", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git;cd mmd;conda env create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate mmd", "target_cmd": "conda activate mmd"}
{"uuid": "118d11d0-900f-48a8-8bfd-31b9f2a0c9f7", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git;cd mmd;conda env create -f environment.yml;conda activate mmd;conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia", "cmd_prefix": "cd", "cmd_postfix": "deps/torch_robotics", "target_cmd": "cd deps/torch_robotics"}
{"uuid": "287bef88-3b36-46da-8501-a23bcec6b09d", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git;cd mmd;conda env create -f environment.yml;conda activate mmd;conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia;cd deps/torch_robotics", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "1f370a99-3c23-4c48-9fd6-8a1793a930c2", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git;cd mmd;conda env create -f environment.yml;conda activate mmd;conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia;cd deps/torch_robotics;pip install -e .;cd ../experiment_launcher", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "398d686c-350a-4b12-a78e-a13c995feb50", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git;cd mmd;conda env create -f environment.yml;conda activate mmd;conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia;cd deps/torch_robotics;pip install -e .;cd ../experiment_launcher;pip install -e .;cd ../motion_planning_baselines", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "0d1fae7d-3f86-4d37-a4c3-26d3f75da143", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git;cd mmd;conda env create -f environment.yml;conda activate mmd;conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia;cd deps/torch_robotics;pip install -e .;cd ../experiment_launcher;pip install -e .;cd ../motion_planning_baselines;pip install -e .", "cmd_prefix": "cd", "cmd_postfix": "../..", "target_cmd": "cd ../.."}
{"uuid": "2784c634-bec3-4628-b56f-99990dc3797e", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git;cd mmd;conda env create -f environment.yml;conda activate mmd;conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia;cd deps/torch_robotics;pip install -e .;cd ../experiment_launcher;pip install -e .;cd ../motion_planning_baselines;pip install -e .;cd ../..", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "c875ba24-be35-44eb-b090-63f0596fa350", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git;cd mmd;conda env create -f environment.yml;conda activate mmd;conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia;cd deps/torch_robotics;pip install -e .;cd ../experiment_launcher;pip install -e .;cd ../motion_planning_baselines;pip install -e .;cd ../..;pip install -e .", "cmd_prefix": "bash", "cmd_postfix": "setup.sh", "target_cmd": "bash setup.sh"}
{"uuid": "8c738fb7-4846-4faa-8268-708d557a64ef", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git;cd mmd;conda env create -f environment.yml;conda activate mmd;conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia;cd deps/torch_robotics;pip install -e .;cd ../experiment_launcher;pip install -e .;cd ../motion_planning_baselines;pip install -e .;cd ../..;pip install -e .;bash setup.sh;gdown --id 1Onw0s1pDsMLDfJVOAqmNme4eVVoAkkjz;tar -xJvf data_trajectories.tar.xz;gdown --id 1WO3tpvg-HU0m9RyDvGyfDamo7roBYMud;tar -xJvf data_trained_models.tar.xz", "cmd_prefix": "cd", "cmd_postfix": "scripts/inference", "target_cmd": "cd scripts/inference"}
{"uuid": "3c20e840-1a3a-43bf-91a2-cfaa2b2aa2b8", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git;cd mmd;conda env create -f environment.yml;conda activate mmd;conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia;cd deps/torch_robotics;pip install -e .;cd ../experiment_launcher;pip install -e .;cd ../motion_planning_baselines;pip install -e .;cd ../..;pip install -e .;bash setup.sh;gdown --id 1Onw0s1pDsMLDfJVOAqmNme4eVVoAkkjz;tar -xJvf data_trajectories.tar.xz;gdown --id 1WO3tpvg-HU0m9RyDvGyfDamo7roBYMud;tar -xJvf data_trained_models.tar.xz;cd scripts/inference;python3 inference_multi_agent.py;cd scripts/generate_data;python3 launch_generate_trajectories.py;cd scripts/train_diffusion", "cmd_prefix": "python3", "cmd_postfix": "launch_train_01.py", "target_cmd": "python3 launch_train_01.py"}
{"uuid": "9e64fdd7-c91e-493a-9b56-9eee8e05146b", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Create and activate a conda environment using the provided environment.yml file; step3: Install PyTorch with CUDA support; step4: Install local packages (torch_robotics, experiment_launcher, motion_planning_baselines); step5: Install the mmd package; step6: Run the setup script to install gdown; step7: Download and extract sample datasets and pre-trained models; step8: Run inference using the provided script to generate multi-robot trajectories; step9: (Optional) Generate a dataset of trajectories; step10: (Optional) Train single-robot diffusion models; step11: (Optional) Run experiment sets combining different models and planning algorithms", "executed_cmds": "cd ~;git clone https://github.com/yoraish/mmd.git;cd mmd;conda env create -f environment.yml;conda activate mmd;conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia;cd deps/torch_robotics;pip install -e .;cd ../experiment_launcher;pip install -e .;cd ../motion_planning_baselines;pip install -e .;cd ../..;pip install -e .;bash setup.sh;gdown --id 1Onw0s1pDsMLDfJVOAqmNme4eVVoAkkjz;tar -xJvf data_trajectories.tar.xz;gdown --id 1WO3tpvg-HU0m9RyDvGyfDamo7roBYMud;tar -xJvf data_trained_models.tar.xz;cd scripts/inference;python3 inference_multi_agent.py;cd scripts/generate_data;python3 launch_generate_trajectories.py;cd scripts/train_diffusion;python3 launch_train_01.py", "cmd_prefix": "cd", "cmd_postfix": "scripts/inference", "target_cmd": "cd scripts/inference"}
{"uuid": "2c601c80-6b94-4eea-a13f-23435491d174", "execution_plan": "step1: Create a conda environment and install dependencies; step2: Run a simple training on the SlimPajama 6B dataset with default settings; step3: Run training with the cooldown schedule; step4: Run training with stochastic weight averaging; step5: Use FLOPS helpers for transformer configurations", "executed_cmds": "conda create -n env python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate env", "target_cmd": "conda activate env"}
{"uuid": "a79979c7-4055-435b-8b39-d1029014b787", "execution_plan": "step1: Create a conda environment and install dependencies; step2: Run a simple training on the SlimPajama 6B dataset with default settings; step3: Run training with the cooldown schedule; step4: Run training with stochastic weight averaging; step5: Use FLOPS helpers for transformer configurations", "executed_cmds": "conda create -n env python=3.10;conda activate env", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "f455a06c-2b5f-4177-ac6f-b070e552bf2a", "execution_plan": "step1: Create a conda environment and install dependencies; step2: Run a simple training on the SlimPajama 6B dataset with default settings; step3: Run training with the cooldown schedule; step4: Run training with stochastic weight averaging; step5: Use FLOPS helpers for transformer configurations", "executed_cmds": "conda create -n env python=3.10;conda activate env;pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "./src/main.py", "target_cmd": "python ./src/main.py"}
{"uuid": "f62e5e1b-2ec4-4129-9762-b61a4eb2ec80", "execution_plan": "step1: Install the necessary libraries for inference using vLLM; step2: Run inference using vLLM with the provided model; step3: Install llama.cpp and download a quantized GGUF checkpoint for inference; step4: Run inference using llama.cpp with the downloaded checkpoint; step5: Install transformers and torch libraries for inference; step6: Run inference using transformers with the provided model; step7: Clone the OLMo branch and set up the environment for pretraining; step8: Install additional dependencies for pretraining; step9: Set up a config file for pretraining; step10: Download and tokenize the pretraining data; step11: Submit the pretraining job with the configured setup; step12: Clone Open Instruct and set up the environment for adaptation; step13: Run supervised fine-tuning (SFT) using the provided command; step14: Run Direct Preference Optimization (DPO) using the provided command; step15: Run Kahneman-Tversky Optimization (KTO) using the provided command; step16: Evaluate the model during pretraining using the configured setup; step17: Evaluate the model after pretraining using OLMo-Eval and DCLM Evals; step18: Evaluate the model after adaptation using the provided script; step19: Generate visuals and figures using the provided scripts and notebooks; step20: Cite the paper using the provided BibTeX entry", "executed_cmds": "pip install vllm", "cmd_prefix": "python -c \"from", "cmd_postfix": "vllm import LLM", "target_cmd": "python -c \"from vllm import LLM"}
{"uuid": "3db97576-c012-4515-9a3b-a6fc5f8a64d2", "execution_plan": "step1: Install the necessary libraries for inference using vLLM; step2: Run inference using vLLM with the provided model; step3: Install llama.cpp and download a quantized GGUF checkpoint for inference; step4: Run inference using llama.cpp with the downloaded checkpoint; step5: Install transformers and torch libraries for inference; step6: Run inference using transformers with the provided model; step7: Clone the OLMo branch and set up the environment for pretraining; step8: Install additional dependencies for pretraining; step9: Set up a config file for pretraining; step10: Download and tokenize the pretraining data; step11: Submit the pretraining job with the configured setup; step12: Clone Open Instruct and set up the environment for adaptation; step13: Run supervised fine-tuning (SFT) using the provided command; step14: Run Direct Preference Optimization (DPO) using the provided command; step15: Run Kahneman-Tversky Optimization (KTO) using the provided command; step16: Evaluate the model during pretraining using the configured setup; step17: Evaluate the model after pretraining using OLMo-Eval and DCLM Evals; step18: Evaluate the model after adaptation using the provided script; step19: Generate visuals and figures using the provided scripts and notebooks; step20: Cite the paper using the provided BibTeX entry", "executed_cmds": "pip install vllm;python -c \"from vllm import LLM;SamplingParams; model = LLM('allenai/OLMoE-1B-7B-0924'); out = model.generate('Bitcoin is';SamplingParams(temperature=0.0)); print('Bitcoin is' + out[0].outputs[0].text)\";git clone https://github.com/ggerganov/llama.cpp;wget https://hf.co/allenai/OLMoE-1B-7B-0924-GGUF/resolve/main/olmoe-1b-7b-0924-q4_0.gguf", "cmd_prefix": "llama-cli -m olmoe-1b-7b-0924-q4_0.gguf -p", "cmd_postfix": "\"Bitcoin is\" -n 128", "target_cmd": "llama-cli -m olmoe-1b-7b-0924-q4_0.gguf -p \"Bitcoin is\" -n 128"}
{"uuid": "a67b7ec3-dcae-4e51-8efb-61ea40aa4a2d", "execution_plan": "step1: Install the necessary libraries for inference using vLLM; step2: Run inference using vLLM with the provided model; step3: Install llama.cpp and download a quantized GGUF checkpoint for inference; step4: Run inference using llama.cpp with the downloaded checkpoint; step5: Install transformers and torch libraries for inference; step6: Run inference using transformers with the provided model; step7: Clone the OLMo branch and set up the environment for pretraining; step8: Install additional dependencies for pretraining; step9: Set up a config file for pretraining; step10: Download and tokenize the pretraining data; step11: Submit the pretraining job with the configured setup; step12: Clone Open Instruct and set up the environment for adaptation; step13: Run supervised fine-tuning (SFT) using the provided command; step14: Run Direct Preference Optimization (DPO) using the provided command; step15: Run Kahneman-Tversky Optimization (KTO) using the provided command; step16: Evaluate the model during pretraining using the configured setup; step17: Evaluate the model after pretraining using OLMo-Eval and DCLM Evals; step18: Evaluate the model after adaptation using the provided script; step19: Generate visuals and figures using the provided scripts and notebooks; step20: Cite the paper using the provided BibTeX entry", "executed_cmds": "pip install vllm;python -c \"from vllm import LLM;SamplingParams; model = LLM('allenai/OLMoE-1B-7B-0924'); out = model.generate('Bitcoin is';SamplingParams(temperature=0.0)); print('Bitcoin is' + out[0].outputs[0].text)\";git clone https://github.com/ggerganov/llama.cpp;wget https://hf.co/allenai/OLMoE-1B-7B-0924-GGUF/resolve/main/olmoe-1b-7b-0924-q4_0.gguf;llama-cli -m olmoe-1b-7b-0924-q4_0.gguf -p \"Bitcoin is\" -n 128", "cmd_prefix": "pip install", "cmd_postfix": "transformers torch", "target_cmd": "pip install transformers torch"}
{"uuid": "382a4d4f-0c75-4dd7-a98e-4ae213a53862", "execution_plan": "step1: Install the necessary libraries for inference using vLLM; step2: Run inference using vLLM with the provided model; step3: Install llama.cpp and download a quantized GGUF checkpoint for inference; step4: Run inference using llama.cpp with the downloaded checkpoint; step5: Install transformers and torch libraries for inference; step6: Run inference using transformers with the provided model; step7: Clone the OLMo branch and set up the environment for pretraining; step8: Install additional dependencies for pretraining; step9: Set up a config file for pretraining; step10: Download and tokenize the pretraining data; step11: Submit the pretraining job with the configured setup; step12: Clone Open Instruct and set up the environment for adaptation; step13: Run supervised fine-tuning (SFT) using the provided command; step14: Run Direct Preference Optimization (DPO) using the provided command; step15: Run Kahneman-Tversky Optimization (KTO) using the provided command; step16: Evaluate the model during pretraining using the configured setup; step17: Evaluate the model after pretraining using OLMo-Eval and DCLM Evals; step18: Evaluate the model after adaptation using the provided script; step19: Generate visuals and figures using the provided scripts and notebooks; step20: Cite the paper using the provided BibTeX entry", "executed_cmds": "pip install vllm;python -c \"from vllm import LLM;SamplingParams; model = LLM('allenai/OLMoE-1B-7B-0924'); out = model.generate('Bitcoin is';SamplingParams(temperature=0.0)); print('Bitcoin is' + out[0].outputs[0].text)\";git clone https://github.com/ggerganov/llama.cpp;wget https://hf.co/allenai/OLMoE-1B-7B-0924-GGUF/resolve/main/olmoe-1b-7b-0924-q4_0.gguf;llama-cli -m olmoe-1b-7b-0924-q4_0.gguf -p \"Bitcoin is\" -n 128;pip install transformers torch;python -c \"from transformers import OlmoeForCausalLM;AutoTokenizer; import torch; DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'; model = OlmoeForCausalLM.from_pretrained('allenai/OLMoE-1B-7B-0924').to(DEVICE); tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924'); inputs = tokenizer('Bitcoin is';return_tensors='pt'); inputs = {k: v.to(DEVICE) for k;v in inputs.items()}; out = model.generate(**inputs;max_length=64); print(tokenizer.decode(out[0]))\";git clone -b Muennighoff/MoE https://github.com/allenai/OLMo", "cmd_prefix": "cd", "cmd_postfix": "OLMo", "target_cmd": "cd OLMo"}
{"uuid": "b1e2af9b-6d07-4f45-884e-e0d18e19d0e3", "execution_plan": "step1: Install the necessary libraries for inference using vLLM; step2: Run inference using vLLM with the provided model; step3: Install llama.cpp and download a quantized GGUF checkpoint for inference; step4: Run inference using llama.cpp with the downloaded checkpoint; step5: Install transformers and torch libraries for inference; step6: Run inference using transformers with the provided model; step7: Clone the OLMo branch and set up the environment for pretraining; step8: Install additional dependencies for pretraining; step9: Set up a config file for pretraining; step10: Download and tokenize the pretraining data; step11: Submit the pretraining job with the configured setup; step12: Clone Open Instruct and set up the environment for adaptation; step13: Run supervised fine-tuning (SFT) using the provided command; step14: Run Direct Preference Optimization (DPO) using the provided command; step15: Run Kahneman-Tversky Optimization (KTO) using the provided command; step16: Evaluate the model during pretraining using the configured setup; step17: Evaluate the model after pretraining using OLMo-Eval and DCLM Evals; step18: Evaluate the model after adaptation using the provided script; step19: Generate visuals and figures using the provided scripts and notebooks; step20: Cite the paper using the provided BibTeX entry", "executed_cmds": "pip install vllm;python -c \"from vllm import LLM;SamplingParams; model = LLM('allenai/OLMoE-1B-7B-0924'); out = model.generate('Bitcoin is';SamplingParams(temperature=0.0)); print('Bitcoin is' + out[0].outputs[0].text)\";git clone https://github.com/ggerganov/llama.cpp;wget https://hf.co/allenai/OLMoE-1B-7B-0924-GGUF/resolve/main/olmoe-1b-7b-0924-q4_0.gguf;llama-cli -m olmoe-1b-7b-0924-q4_0.gguf -p \"Bitcoin is\" -n 128;pip install transformers torch;python -c \"from transformers import OlmoeForCausalLM;AutoTokenizer; import torch; DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'; model = OlmoeForCausalLM.from_pretrained('allenai/OLMoE-1B-7B-0924').to(DEVICE); tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924'); inputs = tokenizer('Bitcoin is';return_tensors='pt'); inputs = {k: v.to(DEVICE) for k;v in inputs.items()}; out = model.generate(**inputs;max_length=64); print(tokenizer.decode(out[0]))\";git clone -b Muennighoff/MoE https://github.com/allenai/OLMo;cd OLMo", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "696c7f0d-d4d1-4942-8de3-e6a130d0943c", "execution_plan": "step1: Install the necessary libraries for inference using vLLM; step2: Run inference using vLLM with the provided model; step3: Install llama.cpp and download a quantized GGUF checkpoint for inference; step4: Run inference using llama.cpp with the downloaded checkpoint; step5: Install transformers and torch libraries for inference; step6: Run inference using transformers with the provided model; step7: Clone the OLMo branch and set up the environment for pretraining; step8: Install additional dependencies for pretraining; step9: Set up a config file for pretraining; step10: Download and tokenize the pretraining data; step11: Submit the pretraining job with the configured setup; step12: Clone Open Instruct and set up the environment for adaptation; step13: Run supervised fine-tuning (SFT) using the provided command; step14: Run Direct Preference Optimization (DPO) using the provided command; step15: Run Kahneman-Tversky Optimization (KTO) using the provided command; step16: Evaluate the model during pretraining using the configured setup; step17: Evaluate the model after pretraining using OLMo-Eval and DCLM Evals; step18: Evaluate the model after adaptation using the provided script; step19: Generate visuals and figures using the provided scripts and notebooks; step20: Cite the paper using the provided BibTeX entry", "executed_cmds": "pip install vllm;python -c \"from vllm import LLM;SamplingParams; model = LLM('allenai/OLMoE-1B-7B-0924'); out = model.generate('Bitcoin is';SamplingParams(temperature=0.0)); print('Bitcoin is' + out[0].outputs[0].text)\";git clone https://github.com/ggerganov/llama.cpp;wget https://hf.co/allenai/OLMoE-1B-7B-0924-GGUF/resolve/main/olmoe-1b-7b-0924-q4_0.gguf;llama-cli -m olmoe-1b-7b-0924-q4_0.gguf -p \"Bitcoin is\" -n 128;pip install transformers torch;python -c \"from transformers import OlmoeForCausalLM;AutoTokenizer; import torch; DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'; model = OlmoeForCausalLM.from_pretrained('allenai/OLMoE-1B-7B-0924').to(DEVICE); tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924'); inputs = tokenizer('Bitcoin is';return_tensors='pt'); inputs = {k: v.to(DEVICE) for k;v in inputs.items()}; out = model.generate(**inputs;max_length=64); print(tokenizer.decode(out[0]))\";git clone -b Muennighoff/MoE https://github.com/allenai/OLMo;cd OLMo;pip install -e .;pip install git+https://github.com/Muennighoff/megablocks.git@olmoe;dolma tokens --documents ${PATH_TO_DOWNLOADED_DATA} --destination ${PATH_WHERE_TO_SAVE_TOKENIZED_DATA} --tokenizer.name_or_path 'allenai/gpt-neox-olmo-dolma-v1_5' --max_size '2_147_483_648' --seed 0 --tokenizer.eos_token_id 50279 --tokenizer.pad_token_id 1 --processes ${NUMBER_OF_CPU_CORES_TO_USE};bash scripts/olmoe-gantry.sh;git clone https://github.com/allenai/open-instruct", "cmd_prefix": "cd", "cmd_postfix": "open-instruct", "target_cmd": "cd open-instruct"}
{"uuid": "0cafb414-acfe-4b76-825f-482e8353892a", "execution_plan": "step1: Install the necessary libraries for inference using vLLM; step2: Run inference using vLLM with the provided model; step3: Install llama.cpp and download a quantized GGUF checkpoint for inference; step4: Run inference using llama.cpp with the downloaded checkpoint; step5: Install transformers and torch libraries for inference; step6: Run inference using transformers with the provided model; step7: Clone the OLMo branch and set up the environment for pretraining; step8: Install additional dependencies for pretraining; step9: Set up a config file for pretraining; step10: Download and tokenize the pretraining data; step11: Submit the pretraining job with the configured setup; step12: Clone Open Instruct and set up the environment for adaptation; step13: Run supervised fine-tuning (SFT) using the provided command; step14: Run Direct Preference Optimization (DPO) using the provided command; step15: Run Kahneman-Tversky Optimization (KTO) using the provided command; step16: Evaluate the model during pretraining using the configured setup; step17: Evaluate the model after pretraining using OLMo-Eval and DCLM Evals; step18: Evaluate the model after adaptation using the provided script; step19: Generate visuals and figures using the provided scripts and notebooks; step20: Cite the paper using the provided BibTeX entry", "executed_cmds": "pip install vllm;python -c \"from vllm import LLM;SamplingParams; model = LLM('allenai/OLMoE-1B-7B-0924'); out = model.generate('Bitcoin is';SamplingParams(temperature=0.0)); print('Bitcoin is' + out[0].outputs[0].text)\";git clone https://github.com/ggerganov/llama.cpp;wget https://hf.co/allenai/OLMoE-1B-7B-0924-GGUF/resolve/main/olmoe-1b-7b-0924-q4_0.gguf;llama-cli -m olmoe-1b-7b-0924-q4_0.gguf -p \"Bitcoin is\" -n 128;pip install transformers torch;python -c \"from transformers import OlmoeForCausalLM;AutoTokenizer; import torch; DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'; model = OlmoeForCausalLM.from_pretrained('allenai/OLMoE-1B-7B-0924').to(DEVICE); tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924'); inputs = tokenizer('Bitcoin is';return_tensors='pt'); inputs = {k: v.to(DEVICE) for k;v in inputs.items()}; out = model.generate(**inputs;max_length=64); print(tokenizer.decode(out[0]))\";git clone -b Muennighoff/MoE https://github.com/allenai/OLMo;cd OLMo;pip install -e .;pip install git+https://github.com/Muennighoff/megablocks.git@olmoe;dolma tokens --documents ${PATH_TO_DOWNLOADED_DATA} --destination ${PATH_WHERE_TO_SAVE_TOKENIZED_DATA} --tokenizer.name_or_path 'allenai/gpt-neox-olmo-dolma-v1_5' --max_size '2_147_483_648' --seed 0 --tokenizer.eos_token_id 50279 --tokenizer.pad_token_id 1 --processes ${NUMBER_OF_CPU_CORES_TO_USE};bash scripts/olmoe-gantry.sh;git clone https://github.com/allenai/open-instruct;cd open-instruct;pip install -e .;accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 8 --use_deepspeed --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf open_instruct/finetune.py --model_name_or_path allenai/OLMoE-1B-7B-0924 --tokenizer_name allenai/OLMoE-1B-7B-0924 --use_flash_attn --max_seq_length 4096 --preprocessing_num_workers 128 --per_device_train_batch_size 2 --gradient_accumulation_steps 8 --learning_rate 2e-05 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0.0 --num_train_epochs 2 --output_dir output/ --with_tracking --report_to wandb --logging_steps 1 --reduce_loss sum --model_revision main --dataset_mixer_list allenai/tulu-v3-mix-preview-4096-OLMoE 1.0 ai2-adapt-dev/daring-anteater-specialized 1.0 --checkpointing_steps epoch --add_bos;accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 8 --use_deepspeed --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf open_instruct/dpo_tune.py --model_name_or_path allenai/OLMoE-1B-7B-0924-SFT --tokenizer_name allenai/OLMoE-1B-7B-0924-SFT --use_flash_attn --gradient_checkpointing --dataset_name argilla/ultrafeedback-binarized-preferences-cleaned --max_seq_length 4096 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 5e-7 --lr_scheduler_type linear --warmup_ratio 0.1 --weight_decay 0. --num_train_epochs 3 --output_dir output/ --report_to tensorboard --logging_steps 1 --reduce_loss sum --add_bos --checkpointing_steps epoch --dpo_beta 0.1;WANDB_PROJECT=olmoe accelerate launch --config_file=config_8gpusdsz2_m7.yml kto.py --model_name_or_path allenai/OLMoE-1B-7B-0924-SFT --output_dir OLMoE-1B-7B-0924-SFT-KTO-3EP --report_to \"wandb\" --per_device_train_batch_size 4 --gradient_accumulation_steps 1 --optim rmsprop --learning_rate 5e-07 --beta 0.1 --logging_steps 1 --bf16 --sanity_check False --num_train_epochs 3", "cmd_prefix": "sbatch", "cmd_postfix": "scripts/adapteval.sh", "target_cmd": "sbatch scripts/adapteval.sh"}
{"uuid": "cbfc657e-b6f5-445a-a271-b1958f20f474", "execution_plan": "step1: Set up the conda environment and install dependencies; step2: Clone the repository and navigate to the project directory; step3: Run preprocessing scripts to prepare the model for quantization; step4: Execute the quantization script with desired parameters; step5: Evaluate the quantized model on specified tasks or benchmarks", "executed_cmds": "conda create -n duquant python=3.10 -y", "cmd_prefix": "conda", "cmd_postfix": "activate duquant", "target_cmd": "conda activate duquant"}
{"uuid": "b0ca9790-f442-4cf2-846f-35d8581a87cf", "execution_plan": "step1: Set up the conda environment and install dependencies; step2: Clone the repository and navigate to the project directory; step3: Run preprocessing scripts to prepare the model for quantization; step4: Execute the quantization script with desired parameters; step5: Evaluate the quantized model on specified tasks or benchmarks", "executed_cmds": "conda create -n duquant python=3.10 -y;conda activate duquant", "cmd_prefix": "pip install", "cmd_postfix": "--upgrade pip", "target_cmd": "pip install --upgrade pip"}
{"uuid": "032f26d8-02ea-481d-93fc-ea6fe1076adc", "execution_plan": "step1: Set up the conda environment and install dependencies; step2: Clone the repository and navigate to the project directory; step3: Run preprocessing scripts to prepare the model for quantization; step4: Execute the quantization script with desired parameters; step5: Evaluate the quantized model on specified tasks or benchmarks", "executed_cmds": "conda create -n duquant python=3.10 -y;conda activate duquant;pip install --upgrade pip", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "235da568-5306-4feb-a2f2-7c3a75e25757", "execution_plan": "step1: Set up the conda environment and install dependencies; step2: Clone the repository and navigate to the project directory; step3: Run preprocessing scripts to prepare the model for quantization; step4: Execute the quantization script with desired parameters; step5: Evaluate the quantized model on specified tasks or benchmarks", "executed_cmds": "conda create -n duquant python=3.10 -y;conda activate duquant;pip install --upgrade pip;pip install -r requirements.txt;git clone https://github.com/Hsu1023/DuQuant.git", "cmd_prefix": "cd", "cmd_postfix": "DuQuant", "target_cmd": "cd DuQuant"}
{"uuid": "ebc3380c-ebc7-4b29-8e1f-c2d4978aea59", "execution_plan": "step1: Set up the conda environment and install dependencies; step2: Clone the repository and navigate to the project directory; step3: Run preprocessing scripts to prepare the model for quantization; step4: Execute the quantization script with desired parameters; step5: Evaluate the quantized model on specified tasks or benchmarks", "executed_cmds": "conda create -n duquant python=3.10 -y;conda activate duquant;pip install --upgrade pip;pip install -r requirements.txt;git clone https://github.com/Hsu1023/DuQuant.git;cd DuQuant", "cmd_prefix": "python", "cmd_postfix": "get_rot.py", "target_cmd": "python get_rot.py"}
{"uuid": "1e4f8f52-aa52-4770-9b50-160097e15073", "execution_plan": "step1: Understand the project's purpose and architecture by reviewing the README content; step2: Set up the necessary environment and dependencies for running NeuRes; step3: Download or clone the repository to access the NeuRes codebase; step4: Prepare the training data or datasets required for NeuRes; step5: Configure the model parameters and select the appropriate clause selector architecture; step6: Train the NeuRes model using the prepared data and configuration; step7: Evaluate the trained model's performance on propositional satisfiability tasks; step8: Generate proofs or truth assignments using the trained model", "executed_cmds": "git clone <repository_url>", "cmd_prefix": "python train.py", "cmd_postfix": "--config config.yaml", "target_cmd": "python train.py --config config.yaml"}
{"uuid": "a30f6c4a-6b3a-4ccf-af18-5898880783f2", "execution_plan": "step1: Install the required dependencies to set up the environment; step2: Reproduce Figure 1 and Appendix D.2 figures by running the notebook for the Rings experiment; step3: Reproduce Figure 2 and Appendix D.3/D.4 figures by running the notebooks in the generative modeling folder; step4: Reproduce Figure 3 by executing the domain adaptation experiment using the provided command lines; step5: Reproduce Table 1 by running the dataset distillation experiment using the provided command lines; step6: Reproduce Table 2 by running the transfer learning experiment using the provided command lines", "executed_cmds": "pip install -r requirements.txt;jupyter notebook \"MMD - SW based Kernel - Rings.ipynb\";jupyter notebook \"xp_generative_modeling/*.ipynb\"", "cmd_prefix": "cd xp_domain_adaptation", "cmd_postfix": "&& bash run.sh", "target_cmd": "cd xp_domain_adaptation && bash run.sh"}
{"uuid": "926eb2e6-26a7-4449-a013-27c352fb26b2", "execution_plan": "step1: Install the required dependencies to set up the environment; step2: Reproduce Figure 1 and Appendix D.2 figures by running the notebook for the Rings experiment; step3: Reproduce Figure 2 and Appendix D.3/D.4 figures by running the notebooks in the generative modeling folder; step4: Reproduce Figure 3 by executing the domain adaptation experiment using the provided command lines; step5: Reproduce Table 1 by running the dataset distillation experiment using the provided command lines; step6: Reproduce Table 2 by running the transfer learning experiment using the provided command lines", "executed_cmds": "pip install -r requirements.txt;jupyter notebook \"MMD - SW based Kernel - Rings.ipynb\";jupyter notebook \"xp_generative_modeling/*.ipynb\";cd xp_domain_adaptation && bash run.sh", "cmd_prefix": "cd xp_dataset_distillation", "cmd_postfix": "&& bash run.sh", "target_cmd": "cd xp_dataset_distillation && bash run.sh"}
{"uuid": "02b22c91-a18d-4fdc-96ca-a253803df39f", "execution_plan": "step1: Install the required dependencies to set up the environment; step2: Reproduce Figure 1 and Appendix D.2 figures by running the notebook for the Rings experiment; step3: Reproduce Figure 2 and Appendix D.3/D.4 figures by running the notebooks in the generative modeling folder; step4: Reproduce Figure 3 by executing the domain adaptation experiment using the provided command lines; step5: Reproduce Table 1 by running the dataset distillation experiment using the provided command lines; step6: Reproduce Table 2 by running the transfer learning experiment using the provided command lines", "executed_cmds": "pip install -r requirements.txt;jupyter notebook \"MMD - SW based Kernel - Rings.ipynb\";jupyter notebook \"xp_generative_modeling/*.ipynb\";cd xp_domain_adaptation && bash run.sh;cd xp_dataset_distillation && bash run.sh", "cmd_prefix": "cd xp_transfer_learning", "cmd_postfix": "&& bash run.sh", "target_cmd": "cd xp_transfer_learning && bash run.sh"}
{"uuid": "d99e7cf4-093d-4772-84d3-33d1fd235657", "execution_plan": "step1: Install the required dependencies including timm, cupy, torch, spikingjelly, pyyaml, and tensorboard; step2: Prepare the ImageNet dataset with the specified folder structure; step3: Download the trained QKFormer models from the provided links; step4: Train the model on ImageNet using distributed training; step5: Test the trained model on ImageNet validation data; step6: Train the model on CIFAR10 by setting hyper-parameters in cifar10.yml; step7: Train the model on CIFAR100 by setting hyper-parameters in cifar100.yml; step8: Train the model on DVS128 Gesture dataset; step9: Train the model on CIFAR10-DVS dataset", "executed_cmds": "pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard", "cmd_prefix": "cd", "cmd_postfix": "imagenet", "target_cmd": "cd imagenet"}
{"uuid": "c2b74831-1554-4207-a667-c34ad3d15b01", "execution_plan": "step1: Install the required dependencies including timm, cupy, torch, spikingjelly, pyyaml, and tensorboard; step2: Prepare the ImageNet dataset with the specified folder structure; step3: Download the trained QKFormer models from the provided links; step4: Train the model on ImageNet using distributed training; step5: Test the trained model on ImageNet validation data; step6: Train the model on CIFAR10 by setting hyper-parameters in cifar10.yml; step7: Train the model on CIFAR100 by setting hyper-parameters in cifar100.yml; step8: Train the model on DVS128 Gesture dataset; step9: Train the model on CIFAR10-DVS dataset", "executed_cmds": "pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard;cd imagenet;python -m torch.distributed.launch --nproc_per_node=8 train.py", "cmd_prefix": "cd", "cmd_postfix": "imagenet", "target_cmd": "cd imagenet"}
{"uuid": "0fcb0119-f08b-483e-be65-e0d5a3cb93cb", "execution_plan": "step1: Install the required dependencies including timm, cupy, torch, spikingjelly, pyyaml, and tensorboard; step2: Prepare the ImageNet dataset with the specified folder structure; step3: Download the trained QKFormer models from the provided links; step4: Train the model on ImageNet using distributed training; step5: Test the trained model on ImageNet validation data; step6: Train the model on CIFAR10 by setting hyper-parameters in cifar10.yml; step7: Train the model on CIFAR100 by setting hyper-parameters in cifar100.yml; step8: Train the model on DVS128 Gesture dataset; step9: Train the model on CIFAR10-DVS dataset", "executed_cmds": "pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard;cd imagenet;python -m torch.distributed.launch --nproc_per_node=8 train.py;cd imagenet", "cmd_prefix": "python", "cmd_postfix": "test.py", "target_cmd": "python test.py"}
{"uuid": "fbd13a87-c303-4eac-bfa4-b26aed0a7706", "execution_plan": "step1: Install the required dependencies including timm, cupy, torch, spikingjelly, pyyaml, and tensorboard; step2: Prepare the ImageNet dataset with the specified folder structure; step3: Download the trained QKFormer models from the provided links; step4: Train the model on ImageNet using distributed training; step5: Test the trained model on ImageNet validation data; step6: Train the model on CIFAR10 by setting hyper-parameters in cifar10.yml; step7: Train the model on CIFAR100 by setting hyper-parameters in cifar100.yml; step8: Train the model on DVS128 Gesture dataset; step9: Train the model on CIFAR10-DVS dataset", "executed_cmds": "pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard;cd imagenet;python -m torch.distributed.launch --nproc_per_node=8 train.py;cd imagenet;python test.py", "cmd_prefix": "cd", "cmd_postfix": "cifar10", "target_cmd": "cd cifar10"}
{"uuid": "d043f0d3-8fbd-488f-93d2-9de816153d96", "execution_plan": "step1: Install the required dependencies including timm, cupy, torch, spikingjelly, pyyaml, and tensorboard; step2: Prepare the ImageNet dataset with the specified folder structure; step3: Download the trained QKFormer models from the provided links; step4: Train the model on ImageNet using distributed training; step5: Test the trained model on ImageNet validation data; step6: Train the model on CIFAR10 by setting hyper-parameters in cifar10.yml; step7: Train the model on CIFAR100 by setting hyper-parameters in cifar100.yml; step8: Train the model on DVS128 Gesture dataset; step9: Train the model on CIFAR10-DVS dataset", "executed_cmds": "pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard;cd imagenet;python -m torch.distributed.launch --nproc_per_node=8 train.py;cd imagenet;python test.py;cd cifar10", "cmd_prefix": "python", "cmd_postfix": "train.py", "target_cmd": "python train.py"}
{"uuid": "bbd9634d-f091-4de7-a2ec-9dedb1b2c751", "execution_plan": "step1: Install the required dependencies including timm, cupy, torch, spikingjelly, pyyaml, and tensorboard; step2: Prepare the ImageNet dataset with the specified folder structure; step3: Download the trained QKFormer models from the provided links; step4: Train the model on ImageNet using distributed training; step5: Test the trained model on ImageNet validation data; step6: Train the model on CIFAR10 by setting hyper-parameters in cifar10.yml; step7: Train the model on CIFAR100 by setting hyper-parameters in cifar100.yml; step8: Train the model on DVS128 Gesture dataset; step9: Train the model on CIFAR10-DVS dataset", "executed_cmds": "pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard;cd imagenet;python -m torch.distributed.launch --nproc_per_node=8 train.py;cd imagenet;python test.py;cd cifar10;python train.py", "cmd_prefix": "cd", "cmd_postfix": "cifar10", "target_cmd": "cd cifar10"}
{"uuid": "bbeec965-9662-4740-a710-800d3efbb35d", "execution_plan": "step1: Install the required dependencies including timm, cupy, torch, spikingjelly, pyyaml, and tensorboard; step2: Prepare the ImageNet dataset with the specified folder structure; step3: Download the trained QKFormer models from the provided links; step4: Train the model on ImageNet using distributed training; step5: Test the trained model on ImageNet validation data; step6: Train the model on CIFAR10 by setting hyper-parameters in cifar10.yml; step7: Train the model on CIFAR100 by setting hyper-parameters in cifar100.yml; step8: Train the model on DVS128 Gesture dataset; step9: Train the model on CIFAR10-DVS dataset", "executed_cmds": "pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard;cd imagenet;python -m torch.distributed.launch --nproc_per_node=8 train.py;cd imagenet;python test.py;cd cifar10;python train.py;cd cifar10", "cmd_prefix": "python", "cmd_postfix": "train.py", "target_cmd": "python train.py"}
{"uuid": "62f9d844-a63f-4211-9f1e-31e8ffb5f0d8", "execution_plan": "step1: Install the required dependencies including timm, cupy, torch, spikingjelly, pyyaml, and tensorboard; step2: Prepare the ImageNet dataset with the specified folder structure; step3: Download the trained QKFormer models from the provided links; step4: Train the model on ImageNet using distributed training; step5: Test the trained model on ImageNet validation data; step6: Train the model on CIFAR10 by setting hyper-parameters in cifar10.yml; step7: Train the model on CIFAR100 by setting hyper-parameters in cifar100.yml; step8: Train the model on DVS128 Gesture dataset; step9: Train the model on CIFAR10-DVS dataset", "executed_cmds": "pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard;cd imagenet;python -m torch.distributed.launch --nproc_per_node=8 train.py;cd imagenet;python test.py;cd cifar10;python train.py;cd cifar10;python train.py", "cmd_prefix": "cd", "cmd_postfix": "dvs128-gesture", "target_cmd": "cd dvs128-gesture"}
{"uuid": "47e57dbd-1fee-419d-8794-b753d4573119", "execution_plan": "step1: Install the required dependencies including timm, cupy, torch, spikingjelly, pyyaml, and tensorboard; step2: Prepare the ImageNet dataset with the specified folder structure; step3: Download the trained QKFormer models from the provided links; step4: Train the model on ImageNet using distributed training; step5: Test the trained model on ImageNet validation data; step6: Train the model on CIFAR10 by setting hyper-parameters in cifar10.yml; step7: Train the model on CIFAR100 by setting hyper-parameters in cifar100.yml; step8: Train the model on DVS128 Gesture dataset; step9: Train the model on CIFAR10-DVS dataset", "executed_cmds": "pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard;cd imagenet;python -m torch.distributed.launch --nproc_per_node=8 train.py;cd imagenet;python test.py;cd cifar10;python train.py;cd cifar10;python train.py;cd dvs128-gesture", "cmd_prefix": "python", "cmd_postfix": "train.py", "target_cmd": "python train.py"}
{"uuid": "0b58adad-7115-462d-b11b-b69743d94184", "execution_plan": "step1: Install the required dependencies including timm, cupy, torch, spikingjelly, pyyaml, and tensorboard; step2: Prepare the ImageNet dataset with the specified folder structure; step3: Download the trained QKFormer models from the provided links; step4: Train the model on ImageNet using distributed training; step5: Test the trained model on ImageNet validation data; step6: Train the model on CIFAR10 by setting hyper-parameters in cifar10.yml; step7: Train the model on CIFAR100 by setting hyper-parameters in cifar100.yml; step8: Train the model on DVS128 Gesture dataset; step9: Train the model on CIFAR10-DVS dataset", "executed_cmds": "pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard;cd imagenet;python -m torch.distributed.launch --nproc_per_node=8 train.py;cd imagenet;python test.py;cd cifar10;python train.py;cd cifar10;python train.py;cd dvs128-gesture;python train.py", "cmd_prefix": "cd", "cmd_postfix": "cifar10-dvs", "target_cmd": "cd cifar10-dvs"}
{"uuid": "8d611c54-2373-493a-b943-5c1f7cc6ac53", "execution_plan": "step1: Install the required dependencies including timm, cupy, torch, spikingjelly, pyyaml, and tensorboard; step2: Prepare the ImageNet dataset with the specified folder structure; step3: Download the trained QKFormer models from the provided links; step4: Train the model on ImageNet using distributed training; step5: Test the trained model on ImageNet validation data; step6: Train the model on CIFAR10 by setting hyper-parameters in cifar10.yml; step7: Train the model on CIFAR100 by setting hyper-parameters in cifar100.yml; step8: Train the model on DVS128 Gesture dataset; step9: Train the model on CIFAR10-DVS dataset", "executed_cmds": "pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard;cd imagenet;python -m torch.distributed.launch --nproc_per_node=8 train.py;cd imagenet;python test.py;cd cifar10;python train.py;cd cifar10;python train.py;cd dvs128-gesture;python train.py;cd cifar10-dvs", "cmd_prefix": "python", "cmd_postfix": "train.py", "target_cmd": "python train.py"}
{"uuid": "b409bc4d-27ff-4483-9578-42b73188b011", "execution_plan": "step1: Clone the Hammer repository and navigate to the Hammer directory; step2: Serve the model using vLLM with the recommended Hammer client; step3: Interact with the served model using the Hammer client for function calling; step4: Serve the model using vLLMs built-in tool calling functionality; step5: Interact with the served model using OpenAI-compatible API for tool calling; step6: Use Hugging Face Transformers to interact with the model for tool calling; step7: Install dependencies for fine-tuning; step8: Process training data by downloading and preparing datasets; step9: Train the model using LLaMA-Factory; step10: Evaluate the model on specific datasets using provided scripts; step11: Evaluate other models by generating JSONL files and running the evaluation script", "executed_cmds": "git clone https://github.com/MadeAgents/Hammer.git", "cmd_prefix": "cd", "cmd_postfix": "Hammer", "target_cmd": "cd Hammer"}
{"uuid": "432a6c23-5116-4e27-9141-b5d735a64d90", "execution_plan": "step1: Clone the Hammer repository and navigate to the Hammer directory; step2: Serve the model using vLLM with the recommended Hammer client; step3: Interact with the served model using the Hammer client for function calling; step4: Serve the model using vLLMs built-in tool calling functionality; step5: Interact with the served model using OpenAI-compatible API for tool calling; step6: Use Hugging Face Transformers to interact with the model for tool calling; step7: Install dependencies for fine-tuning; step8: Process training data by downloading and preparing datasets; step9: Train the model using LLaMA-Factory; step10: Evaluate the model on specific datasets using provided scripts; step11: Evaluate other models by generating JSONL files and running the evaluation script", "executed_cmds": "git clone https://github.com/MadeAgents/Hammer.git;cd Hammer;vllm serve MadeAgents/Hammer2.1-1.5b --host 0.0.0.0 --port 8000 --tensor-parallel-size 1;vllm serve MadeAgents/Hammer2.1-1.5b --enable-auto-tool-choice --tool-call-parser hermes", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "c327de4d-61fa-40ff-a080-d0f95f8dbc62", "execution_plan": "step1: Clone the Hammer repository and navigate to the Hammer directory; step2: Serve the model using vLLM with the recommended Hammer client; step3: Interact with the served model using the Hammer client for function calling; step4: Serve the model using vLLMs built-in tool calling functionality; step5: Interact with the served model using OpenAI-compatible API for tool calling; step6: Use Hugging Face Transformers to interact with the model for tool calling; step7: Install dependencies for fine-tuning; step8: Process training data by downloading and preparing datasets; step9: Train the model using LLaMA-Factory; step10: Evaluate the model on specific datasets using provided scripts; step11: Evaluate other models by generating JSONL files and running the evaluation script", "executed_cmds": "git clone https://github.com/MadeAgents/Hammer.git;cd Hammer;vllm serve MadeAgents/Hammer2.1-1.5b --host 0.0.0.0 --port 8000 --tensor-parallel-size 1;vllm serve MadeAgents/Hammer2.1-1.5b --enable-auto-tool-choice --tool-call-parser hermes;pip install -r requirements.txt;python train/data_processing.py;bash scripts/train.sh <MODEL>", "cmd_prefix": "bash scripts/eval.sh", "cmd_postfix": "<MODEL> <DATASET>", "target_cmd": "bash scripts/eval.sh <MODEL> <DATASET>"}
{"uuid": "3cab88c7-5867-41bb-a257-0cb40e11b219", "execution_plan": "step1: Install the ImpScore package using pip; step2: Import the ImpScore package in Python code; step3: Load the ImpScore model on GPU or CPU; step4: Calculate the implicitness score of single English sentences; step5: Calculate the implicitness score and pragmatic distance of English sentence pairs; step6: (Optional) Train ImpScore with custom data by downloading the source code and running the training script; step7: (Optional) Modify or extend the training data in `all_data.csv` for custom training", "executed_cmds": "pip install implicit-score", "cmd_prefix": "import", "cmd_postfix": "impscore", "target_cmd": "import impscore"}
{"uuid": "79111b80-a55a-4d65-8f07-cdd3bb81c6bd", "execution_plan": "step1: Install the ImpScore package using pip; step2: Import the ImpScore package in Python code; step3: Load the ImpScore model on GPU or CPU; step4: Calculate the implicitness score of single English sentences; step5: Calculate the implicitness score and pragmatic distance of English sentence pairs; step6: (Optional) Train ImpScore with custom data by downloading the source code and running the training script; step7: (Optional) Modify or extend the training data in `all_data.csv` for custom training", "executed_cmds": "pip install implicit-score;import impscore;model = impscore.load_model(load_device=\"cuda\");imp_scores;prag_embs;sem_embs = model.infer_single(sentences);imp_score1;imp_score2;prag_distance = model.infer_pairs(s1_list;s2_list)", "cmd_prefix": "python", "cmd_postfix": "train.py", "target_cmd": "python train.py"}
{"uuid": "5009f75a-f8d7-4ae2-95a6-6aeaaf96ad4a", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Python environment for HealthGPT; step3: Install the required dependencies using the requirements.txt file; step4: Download the pre-trained weights for the visual encoder and base models (HealthGPT-M3 or HealthGPT-L14); step5: Download the VQGAN model weights for medical vision generation tasks and place them in the specified directory; step6: Download the H-LoRA and adapter weights for medical visual comprehension and generation tasks; step7: Modify the script paths in `llava/demo/com_infer.sh` or `llava/demo/gen_infer.sh` to point to the downloaded files; step8: Run the inference script for medical visual question answering or image reconstruction tasks; step9: Install Gradio and Pillow for the interactive Chat UI; step10: Run the Gradio-based Chat UI for interactive text and image input/output", "executed_cmds": "git clone https://github.com/DCDmllm/HealthGPT.git", "cmd_prefix": "cd", "cmd_postfix": "HealthGPT", "target_cmd": "cd HealthGPT"}
{"uuid": "3abadd7c-f0e1-4171-9354-a286edf28b5a", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Python environment for HealthGPT; step3: Install the required dependencies using the requirements.txt file; step4: Download the pre-trained weights for the visual encoder and base models (HealthGPT-M3 or HealthGPT-L14); step5: Download the VQGAN model weights for medical vision generation tasks and place them in the specified directory; step6: Download the H-LoRA and adapter weights for medical visual comprehension and generation tasks; step7: Modify the script paths in `llava/demo/com_infer.sh` or `llava/demo/gen_infer.sh` to point to the downloaded files; step8: Run the inference script for medical visual question answering or image reconstruction tasks; step9: Install Gradio and Pillow for the interactive Chat UI; step10: Run the Gradio-based Chat UI for interactive text and image input/output", "executed_cmds": "git clone https://github.com/DCDmllm/HealthGPT.git;cd HealthGPT;conda create -n HealthGPT python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate HealthGPT", "target_cmd": "conda activate HealthGPT"}
{"uuid": "365ad681-f13b-4fcf-a3c8-12983a783354", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Python environment for HealthGPT; step3: Install the required dependencies using the requirements.txt file; step4: Download the pre-trained weights for the visual encoder and base models (HealthGPT-M3 or HealthGPT-L14); step5: Download the VQGAN model weights for medical vision generation tasks and place them in the specified directory; step6: Download the H-LoRA and adapter weights for medical visual comprehension and generation tasks; step7: Modify the script paths in `llava/demo/com_infer.sh` or `llava/demo/gen_infer.sh` to point to the downloaded files; step8: Run the inference script for medical visual question answering or image reconstruction tasks; step9: Install Gradio and Pillow for the interactive Chat UI; step10: Run the Gradio-based Chat UI for interactive text and image input/output", "executed_cmds": "git clone https://github.com/DCDmllm/HealthGPT.git;cd HealthGPT;conda create -n HealthGPT python=3.10;conda activate HealthGPT", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "456c7e0a-eef3-404f-8acf-9c1f73af2b32", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Python environment for HealthGPT; step3: Install the required dependencies using the requirements.txt file; step4: Download the pre-trained weights for the visual encoder and base models (HealthGPT-M3 or HealthGPT-L14); step5: Download the VQGAN model weights for medical vision generation tasks and place them in the specified directory; step6: Download the H-LoRA and adapter weights for medical visual comprehension and generation tasks; step7: Modify the script paths in `llava/demo/com_infer.sh` or `llava/demo/gen_infer.sh` to point to the downloaded files; step8: Run the inference script for medical visual question answering or image reconstruction tasks; step9: Install Gradio and Pillow for the interactive Chat UI; step10: Run the Gradio-based Chat UI for interactive text and image input/output", "executed_cmds": "git clone https://github.com/DCDmllm/HealthGPT.git;cd HealthGPT;conda create -n HealthGPT python=3.10;conda activate HealthGPT;pip install -r requirements.txt", "cmd_prefix": "cd", "cmd_postfix": "llava/demo", "target_cmd": "cd llava/demo"}
{"uuid": "c86558cb-6c0f-4392-a8a8-39e87fa16f2b", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Python environment for HealthGPT; step3: Install the required dependencies using the requirements.txt file; step4: Download the pre-trained weights for the visual encoder and base models (HealthGPT-M3 or HealthGPT-L14); step5: Download the VQGAN model weights for medical vision generation tasks and place them in the specified directory; step6: Download the H-LoRA and adapter weights for medical visual comprehension and generation tasks; step7: Modify the script paths in `llava/demo/com_infer.sh` or `llava/demo/gen_infer.sh` to point to the downloaded files; step8: Run the inference script for medical visual question answering or image reconstruction tasks; step9: Install Gradio and Pillow for the interactive Chat UI; step10: Run the Gradio-based Chat UI for interactive text and image input/output", "executed_cmds": "git clone https://github.com/DCDmllm/HealthGPT.git;cd HealthGPT;conda create -n HealthGPT python=3.10;conda activate HealthGPT;pip install -r requirements.txt;cd llava/demo", "cmd_prefix": "bash com_infer.sh", "cmd_postfix": "or bash gen_infer.sh", "target_cmd": "bash com_infer.sh or bash gen_infer.sh"}
{"uuid": "11c0d691-dcd3-409b-ab7d-1643ddd1f620", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Python environment for HealthGPT; step3: Install the required dependencies using the requirements.txt file; step4: Download the pre-trained weights for the visual encoder and base models (HealthGPT-M3 or HealthGPT-L14); step5: Download the VQGAN model weights for medical vision generation tasks and place them in the specified directory; step6: Download the H-LoRA and adapter weights for medical visual comprehension and generation tasks; step7: Modify the script paths in `llava/demo/com_infer.sh` or `llava/demo/gen_infer.sh` to point to the downloaded files; step8: Run the inference script for medical visual question answering or image reconstruction tasks; step9: Install Gradio and Pillow for the interactive Chat UI; step10: Run the Gradio-based Chat UI for interactive text and image input/output", "executed_cmds": "git clone https://github.com/DCDmllm/HealthGPT.git;cd HealthGPT;conda create -n HealthGPT python=3.10;conda activate HealthGPT;pip install -r requirements.txt;cd llava/demo;bash com_infer.sh or bash gen_infer.sh", "cmd_prefix": "pip install", "cmd_postfix": "gradio pillow", "target_cmd": "pip install gradio pillow"}
{"uuid": "4bbd2338-59cd-40bc-a677-fcc1455b15f6", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Python environment for HealthGPT; step3: Install the required dependencies using the requirements.txt file; step4: Download the pre-trained weights for the visual encoder and base models (HealthGPT-M3 or HealthGPT-L14); step5: Download the VQGAN model weights for medical vision generation tasks and place them in the specified directory; step6: Download the H-LoRA and adapter weights for medical visual comprehension and generation tasks; step7: Modify the script paths in `llava/demo/com_infer.sh` or `llava/demo/gen_infer.sh` to point to the downloaded files; step8: Run the inference script for medical visual question answering or image reconstruction tasks; step9: Install Gradio and Pillow for the interactive Chat UI; step10: Run the Gradio-based Chat UI for interactive text and image input/output", "executed_cmds": "git clone https://github.com/DCDmllm/HealthGPT.git;cd HealthGPT;conda create -n HealthGPT python=3.10;conda activate HealthGPT;pip install -r requirements.txt;cd llava/demo;bash com_infer.sh or bash gen_infer.sh;pip install gradio pillow", "cmd_prefix": "python", "cmd_postfix": "app.py", "target_cmd": "python app.py"}
{"uuid": "daa8192e-a33f-4c76-90f0-f085cd66ed7b", "execution_plan": "step1: Install the required packages listed in the `requirements.txt` file to set up the environment; step2: Run the genetic algorithm using the `ga.py` script to generate raw results for the minimal MSE experiment; step3: Use the `ga-runner.py` script to execute a suite of experiments defined in the `ga-planner.txt` file for the genetic algorithm; step4: Assess the performance of the evolved estimator on a larger sample size using the `larger-sample-assessment.py` script; step5: Generate variance data for cross-checking estimators using the `ga-variance-crosscheck-gen.py` script; step6: Analyze the results of the genetic algorithm using the `ga-analysis.ipynb` notebook; step7: Analyze the results of the larger sample size assessment using the `larger-sample.ipynb` notebook; step8: Calculate the variance of Phis using the `phi-variance-calc.ipynb` notebook; step9: Replicate the minimal bias experiment using the `min-bias-table.ipynb` and `min-bias-figures.ipynb` notebooks", "executed_cmds": "pip install -r requirements.txt;python ga.py 200 zipf 200 200 0 sampling --p_est_method nGT --seed 0 --rep_smp 1 --rep_evo 1 --max_term 20", "cmd_prefix": "python", "cmd_postfix": "ga-runner.py", "target_cmd": "python ga-runner.py"}
{"uuid": "3680029d-617a-420b-839f-b83a952f496d", "execution_plan": "step1: Clone the repository and set up a clean virtual environment using conda; step2: Create a data directory to store datasets; step3: Download and extract the INR datasets (MNIST-INRs, FMNIST-INRs, CIFAR10-INRs) for INR Classification and Editing tasks; step4: Generate data splits for the MNIST-INR dataset; step5: Preprocess the INR datasets by applying phase canonicalization; step6: Download and extract the generalization prediction datasets (CIFAR10, SVHN); step7: Download and place the split files for the generalization prediction datasets; step8: Train and evaluate ScaleGMN on the INR classification task using the provided config files; step9: Train and evaluate ScaleGMN on the INR editing task using the provided config files; step10: Train and evaluate ScaleGMN on the generalization prediction task using the provided config files", "executed_cmds": "git clone git@github.com:jkalogero/scalegmn.git", "cmd_prefix": "cd", "cmd_postfix": "scalegmn/", "target_cmd": "cd scalegmn/"}
{"uuid": "e9d3a60f-139b-4454-a768-7c74a8aded27", "execution_plan": "step1: Clone the repository and set up a clean virtual environment using conda; step2: Create a data directory to store datasets; step3: Download and extract the INR datasets (MNIST-INRs, FMNIST-INRs, CIFAR10-INRs) for INR Classification and Editing tasks; step4: Generate data splits for the MNIST-INR dataset; step5: Preprocess the INR datasets by applying phase canonicalization; step6: Download and extract the generalization prediction datasets (CIFAR10, SVHN); step7: Download and place the split files for the generalization prediction datasets; step8: Train and evaluate ScaleGMN on the INR classification task using the provided config files; step9: Train and evaluate ScaleGMN on the INR editing task using the provided config files; step10: Train and evaluate ScaleGMN on the generalization prediction task using the provided config files", "executed_cmds": "git clone git@github.com:jkalogero/scalegmn.git;cd scalegmn/;conda env create -n scalegmn --file environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate scalegmn", "target_cmd": "conda activate scalegmn"}
{"uuid": "e377e5dc-c4c5-49d2-bd48-36768e5c1fc3", "execution_plan": "step1: Clone the repository and set up a clean virtual environment using conda; step2: Create a data directory to store datasets; step3: Download and extract the INR datasets (MNIST-INRs, FMNIST-INRs, CIFAR10-INRs) for INR Classification and Editing tasks; step4: Generate data splits for the MNIST-INR dataset; step5: Preprocess the INR datasets by applying phase canonicalization; step6: Download and extract the generalization prediction datasets (CIFAR10, SVHN); step7: Download and place the split files for the generalization prediction datasets; step8: Train and evaluate ScaleGMN on the INR classification task using the provided config files; step9: Train and evaluate ScaleGMN on the INR editing task using the provided config files; step10: Train and evaluate ScaleGMN on the generalization prediction task using the provided config files", "executed_cmds": "git clone git@github.com:jkalogero/scalegmn.git;cd scalegmn/;conda env create -n scalegmn --file environment.yml;conda activate scalegmn", "cmd_prefix": "mkdir", "cmd_postfix": "data", "target_cmd": "mkdir data"}
{"uuid": "21ec9d11-a22c-49f4-a4b9-0ce1137df06e", "execution_plan": "step1: Clone the repository and set up a clean virtual environment using conda; step2: Create a data directory to store datasets; step3: Download and extract the INR datasets (MNIST-INRs, FMNIST-INRs, CIFAR10-INRs) for INR Classification and Editing tasks; step4: Generate data splits for the MNIST-INR dataset; step5: Preprocess the INR datasets by applying phase canonicalization; step6: Download and extract the generalization prediction datasets (CIFAR10, SVHN); step7: Download and place the split files for the generalization prediction datasets; step8: Train and evaluate ScaleGMN on the INR classification task using the provided config files; step9: Train and evaluate ScaleGMN on the INR editing task using the provided config files; step10: Train and evaluate ScaleGMN on the generalization prediction task using the provided config files", "executed_cmds": "git clone git@github.com:jkalogero/scalegmn.git;cd scalegmn/;conda env create -n scalegmn --file environment.yml;conda activate scalegmn;mkdir data;wget \"https://www.dropbox.com/sh/56pakaxe58z29mq/AABrctdu2U65jGYr2WQRzmMna/mnist-inrs.zip?dl=0\" -O \"$DATA_DIR/mnist-inrs.zip\";unzip -q \"$DATA_DIR/mnist-inrs.zip\" -d \"$DATA_DIR\";rm \"$DATA_DIR/mnist-inrs.zip\";python src/utils/generate_data_splits.py --data_path $DATA_DIR/mnist-inrs --save_path $DATA_DIR/mnist-inrs;python src/phase_canonicalization/canonicalization.py --conf src/phase_canonicalization/<dataset>.yml;python src/phase_canonicalization/canonicalization.py --conf src/phase_canonicalization/cifar.yml --extra_aug 20", "cmd_prefix": "tar", "cmd_postfix": "-xvf cifar10.tar.xz", "target_cmd": "tar -xvf cifar10.tar.xz"}
{"uuid": "483741e2-5607-49be-89bc-86bac27f9676", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment with Python 3.10 and activate it; step3: Install the required dependencies using pip; step4: Navigate to the ThinK_flash directory to evaluate eviction on LongBench; step5: Modify hyperparameters in the eval.sh script for eviction evaluation; step6: Run the eval.sh script to evaluate eviction; step7: Run the metrics.sh script to view eviction results; step8: Navigate to the ThinK_kivi directory for quantization evaluation; step9: Set up the environment for KIVI and configure the pruning_ratio for quantization", "executed_cmds": "git clone https://github.com/SalesforceAIResearch/ThinK.git", "cmd_prefix": "conda create", "cmd_postfix": "-n think python=3.10", "target_cmd": "conda create -n think python=3.10"}
{"uuid": "dbbeff9b-e3bd-41df-8d12-78e490b56006", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment with Python 3.10 and activate it; step3: Install the required dependencies using pip; step4: Navigate to the ThinK_flash directory to evaluate eviction on LongBench; step5: Modify hyperparameters in the eval.sh script for eviction evaluation; step6: Run the eval.sh script to evaluate eviction; step7: Run the metrics.sh script to view eviction results; step8: Navigate to the ThinK_kivi directory for quantization evaluation; step9: Set up the environment for KIVI and configure the pruning_ratio for quantization", "executed_cmds": "git clone https://github.com/SalesforceAIResearch/ThinK.git;conda create -n think python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate think", "target_cmd": "conda activate think"}
{"uuid": "9f7161b2-a43d-4bbc-baa8-33c767751125", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment with Python 3.10 and activate it; step3: Install the required dependencies using pip; step4: Navigate to the ThinK_flash directory to evaluate eviction on LongBench; step5: Modify hyperparameters in the eval.sh script for eviction evaluation; step6: Run the eval.sh script to evaluate eviction; step7: Run the metrics.sh script to view eviction results; step8: Navigate to the ThinK_kivi directory for quantization evaluation; step9: Set up the environment for KIVI and configure the pruning_ratio for quantization", "executed_cmds": "git clone https://github.com/SalesforceAIResearch/ThinK.git;conda create -n think python=3.10;conda activate think", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "a6d3d062-49b5-4007-842f-f61fc15ffd52", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment with Python 3.10 and activate it; step3: Install the required dependencies using pip; step4: Navigate to the ThinK_flash directory to evaluate eviction on LongBench; step5: Modify hyperparameters in the eval.sh script for eviction evaluation; step6: Run the eval.sh script to evaluate eviction; step7: Run the metrics.sh script to view eviction results; step8: Navigate to the ThinK_kivi directory for quantization evaluation; step9: Set up the environment for KIVI and configure the pruning_ratio for quantization", "executed_cmds": "git clone https://github.com/SalesforceAIResearch/ThinK.git;conda create -n think python=3.10;conda activate think;pip install -r requirements.txt", "cmd_prefix": "cd", "cmd_postfix": "ThinK_flash", "target_cmd": "cd ThinK_flash"}
{"uuid": "f4fbe583-7ee5-44b3-85f5-0fead1890ca6", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment with Python 3.10 and activate it; step3: Install the required dependencies using pip; step4: Navigate to the ThinK_flash directory to evaluate eviction on LongBench; step5: Modify hyperparameters in the eval.sh script for eviction evaluation; step6: Run the eval.sh script to evaluate eviction; step7: Run the metrics.sh script to view eviction results; step8: Navigate to the ThinK_kivi directory for quantization evaluation; step9: Set up the environment for KIVI and configure the pruning_ratio for quantization", "executed_cmds": "git clone https://github.com/SalesforceAIResearch/ThinK.git;conda create -n think python=3.10;conda activate think;pip install -r requirements.txt;cd ThinK_flash;sh ./scripts/scripts_longBench/eval.sh;sh ./scripts/scripts_longBench/metrics.sh", "cmd_prefix": "cd", "cmd_postfix": "ThinK_kivi", "target_cmd": "cd ThinK_kivi"}
{"uuid": "efd6746b-7e86-46bf-8a93-f1f3663ba41b", "execution_plan": "step1: Install the necessary dependencies for running OS-Atlas-Base-4B; step2: Load and preprocess an image for inference using OS-Atlas-Base-4B; step3: Initialize the OS-Atlas-Base-4B model and tokenizer; step4: Perform inference with OS-Atlas-Base-4B to get coordinates of a specified element; step5: Install the necessary dependencies for running OS-Atlas-Base-7B; step6: Load and preprocess an image for inference using OS-Atlas-Base-7B; step7: Initialize the OS-Atlas-Base-7B model and processor; step8: Perform inference with OS-Atlas-Base-7B to get the position of a specified element", "executed_cmds": "pip install transformers", "cmd_prefix": "pip", "cmd_postfix": "install transformers", "target_cmd": "pip install transformers"}
{"uuid": "3ac1c949-0299-473f-83b0-fa4517c798b3", "execution_plan": "step1: Install the required dependencies including OpenROAD, Yosys, Python, Pytorch, gym, and tqdm; step2: Copy necessary TCL files to the OpenROAD test directory for synthesis; step3: Run adder design with theoretical metrics; step4: Run adder design with practical metrics; step5: Run multiplier design; step6: Test the generated adders and multipliers using provided testbenches", "executed_cmds": "\"cp utils/fast_flow.tcl /path/to/OpenROAD/test\";\"cp utils/full_flow.tcl /path/to/OpenROAD/test\";\"python adder.py --input_bit=64 --level_bound_delta=0\";\"python adder.py --input_bit=128 --level_bound_delta=0\";\"python adder.py --input_bit=128 --level_bound_delta=0 --demo\";\"python adder_prac.py --input_bit=32 --adder_type=0 --openroad_path=/path/to/openroad\";\"python adder_prac.py --input_bit=32 --adder_type=1 --openroad_path=/path/to/openroad\";\"python adder_prac.py --input_bit=32 --adder_type=2 --openroad_path=/path/to/openroad\";\"python select_adder.py --input_bit=32 --openroad_path=/path/to/openroad\";\"python mult.py --input_bit=32 --area_w=0.01\";\"python mult.py --input_bit=64 --area_w=0.01\";\"python mult.py --input_bit=128 --area_w=0.01\"", "cmd_prefix": "\"cd", "cmd_postfix": "testbench\"", "target_cmd": "\"cd testbench\""}
{"uuid": "095c4c0d-fa17-4b33-9f18-a41eac6e97aa", "execution_plan": "step1: Install the required dependencies including OpenROAD, Yosys, Python, Pytorch, gym, and tqdm; step2: Copy necessary TCL files to the OpenROAD test directory for synthesis; step3: Run adder design with theoretical metrics; step4: Run adder design with practical metrics; step5: Run multiplier design; step6: Test the generated adders and multipliers using provided testbenches", "executed_cmds": "\"cp utils/fast_flow.tcl /path/to/OpenROAD/test\";\"cp utils/full_flow.tcl /path/to/OpenROAD/test\";\"python adder.py --input_bit=64 --level_bound_delta=0\";\"python adder.py --input_bit=128 --level_bound_delta=0\";\"python adder.py --input_bit=128 --level_bound_delta=0 --demo\";\"python adder_prac.py --input_bit=32 --adder_type=0 --openroad_path=/path/to/openroad\";\"python adder_prac.py --input_bit=32 --adder_type=1 --openroad_path=/path/to/openroad\";\"python adder_prac.py --input_bit=32 --adder_type=2 --openroad_path=/path/to/openroad\";\"python select_adder.py --input_bit=32 --openroad_path=/path/to/openroad\";\"python mult.py --input_bit=32 --area_w=0.01\";\"python mult.py --input_bit=64 --area_w=0.01\";\"python mult.py --input_bit=128 --area_w=0.01\";\"cd testbench\";\"iverilog -o adder_sim adder.v adder_tb.v\"", "cmd_prefix": "\"vvp", "cmd_postfix": "adder_sim\"", "target_cmd": "\"vvp adder_sim\""}
{"uuid": "5ec33c01-da55-414b-834a-f9eae2edc857", "execution_plan": "step1: Install the required dependencies including OpenROAD, Yosys, Python, Pytorch, gym, and tqdm; step2: Copy necessary TCL files to the OpenROAD test directory for synthesis; step3: Run adder design with theoretical metrics; step4: Run adder design with practical metrics; step5: Run multiplier design; step6: Test the generated adders and multipliers using provided testbenches", "executed_cmds": "\"cp utils/fast_flow.tcl /path/to/OpenROAD/test\";\"cp utils/full_flow.tcl /path/to/OpenROAD/test\";\"python adder.py --input_bit=64 --level_bound_delta=0\";\"python adder.py --input_bit=128 --level_bound_delta=0\";\"python adder.py --input_bit=128 --level_bound_delta=0 --demo\";\"python adder_prac.py --input_bit=32 --adder_type=0 --openroad_path=/path/to/openroad\";\"python adder_prac.py --input_bit=32 --adder_type=1 --openroad_path=/path/to/openroad\";\"python adder_prac.py --input_bit=32 --adder_type=2 --openroad_path=/path/to/openroad\";\"python select_adder.py --input_bit=32 --openroad_path=/path/to/openroad\";\"python mult.py --input_bit=32 --area_w=0.01\";\"python mult.py --input_bit=64 --area_w=0.01\";\"python mult.py --input_bit=128 --area_w=0.01\";\"cd testbench\";\"iverilog -o adder_sim adder.v adder_tb.v\";\"vvp adder_sim\";\"iverilog -o multiplier_sim multiplier.v multiplier_tb.v\"", "cmd_prefix": "\"vvp", "cmd_postfix": "multiplier_sim\"", "target_cmd": "\"vvp multiplier_sim\""}
{"uuid": "5209fd5d-4c85-48b6-91fb-34798e490dea", "execution_plan": "step1: Install the required dependencies using the provided requirements file; step2: Train the lightweight network using MSE loss on a single GPU; step3: Train the lightweight network using LLM loss on multiple GPUs; step4: Calculate stability by comparing model predictions before and after pruning", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "mseloss_entry.py", "target_cmd": "python mseloss_entry.py"}
{"uuid": "c59295b8-4240-4606-9645-ab13237d8981", "execution_plan": "step1: Install the required dependencies using the provided requirements file; step2: Train the lightweight network using MSE loss on a single GPU; step3: Train the lightweight network using LLM loss on multiple GPUs; step4: Calculate stability by comparing model predictions before and after pruning", "executed_cmds": "pip install -r requirements.txt;python mseloss_entry.py;CUDA_VISIBLE_DEVICES=0;1;2;3 accelerate launch --config_file 4gpu.yaml llmloss_entry.py", "cmd_prefix": "python calculate_stability.py", "cmd_postfix": "arg1 arg2", "target_cmd": "python calculate_stability.py arg1 arg2"}
{"uuid": "24f4ec7a-b6ac-4f53-9e75-4e275088db6b", "execution_plan": "step1: Install the required dependencies for the project; step2: Prepare the dataset by downloading ImageNet or DIV2K, or use a private dataset; step3: Modify the configuration file (config.py) to set the correct paths for your environment; step4: Train the model using the provided training script; step5: Test the model using the provided testing script; step6: (Optional) Download pre-trained checkpoints for quick testing", "executed_cmds": "pip install -r requirements.txt;python train.py", "cmd_prefix": "python", "cmd_postfix": "test.py", "target_cmd": "python test.py"}
{"uuid": "e0752fb2-9011-4eee-aa20-b6cc2ccee6cb", "execution_plan": "step1: Create a Conda environment named 'lmap' with Python 3.10; step2: Activate the Conda environment; step3: Install MuJoCo dependencies and set up MuJoCo; step4: Install required Python packages from requirements.txt; step5: Install the project in editable mode; step6: Prepare a custom offline dataset in D4RL format (optional); step7: Train the encoder and decoder; step8: Train the prior; step9: Evaluate the trained models using MCTS planner; step10: Report the results", "executed_cmds": "conda create -n lmap python=3.10 -y", "cmd_prefix": "conda", "cmd_postfix": "activate lmap", "target_cmd": "conda activate lmap"}
{"uuid": "9332dc90-7e9e-4d29-8a72-09bcbdbdffeb", "execution_plan": "step1: Create a Conda environment named 'lmap' with Python 3.10; step2: Activate the Conda environment; step3: Install MuJoCo dependencies and set up MuJoCo; step4: Install required Python packages from requirements.txt; step5: Install the project in editable mode; step6: Prepare a custom offline dataset in D4RL format (optional); step7: Train the encoder and decoder; step8: Train the prior; step9: Evaluate the trained models using MCTS planner; step10: Report the results", "executed_cmds": "conda create -n lmap python=3.10 -y;conda activate lmap;sudo apt-get install build-essential libglew-dev libglfw3-dev mesa-common-dev libgl1-mesa-dev", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "d9b5149b-ce41-4dbe-9a3c-932c91a0bd63", "execution_plan": "step1: Create a Conda environment named 'lmap' with Python 3.10; step2: Activate the Conda environment; step3: Install MuJoCo dependencies and set up MuJoCo; step4: Install required Python packages from requirements.txt; step5: Install the project in editable mode; step6: Prepare a custom offline dataset in D4RL format (optional); step7: Train the encoder and decoder; step8: Train the prior; step9: Evaluate the trained models using MCTS planner; step10: Report the results", "executed_cmds": "conda create -n lmap python=3.10 -y;conda activate lmap;sudo apt-get install build-essential libglew-dev libglfw3-dev mesa-common-dev libgl1-mesa-dev;pip install -r requirements.txt", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "ab1b2af6-6c6d-4d90-b1a2-ed0b31509c6d", "execution_plan": "step1: Create a conda environment and install PyTorch with CUDA support; step2: Install additional dependencies including tqdm, tensorboard, and opencv-python; step3: Download and prepare the FlyingThings3D dataset; step4: Preprocess the FlyingThings3D dataset for both occluded and non-occluded scenarios; step5: Download and prepare the KITTI dataset; step6: Preprocess the KITTI dataset; step7: Download and prepare the Waymo-Open dataset; step8: Organize the datasets directory structure; step9: Train the model on the FlyingThings3D dataset with occlusions; step10: Test the model on the FlyingThings3D dataset with occlusions; step11: Test the model on the KITTI dataset with occlusions; step12: Train the model on the FlyingThings3D dataset without occlusions; step13: Test the model on the FlyingThings3D dataset without occlusions; step14: Test the model on the KITTI dataset without occlusions; step15: Train the model on the Waymo-Open dataset; step16: Test the model on the Waymo-Open dataset", "executed_cmds": "conda create -n diffsf python=3.9", "cmd_prefix": "conda", "cmd_postfix": "activate diffsf", "target_cmd": "conda activate diffsf"}
{"uuid": "99d89a96-da68-403e-943a-2b7a386c0fc2", "execution_plan": "step1: Create a conda environment and install PyTorch with CUDA support; step2: Install additional dependencies including tqdm, tensorboard, and opencv-python; step3: Download and prepare the FlyingThings3D dataset; step4: Preprocess the FlyingThings3D dataset for both occluded and non-occluded scenarios; step5: Download and prepare the KITTI dataset; step6: Preprocess the KITTI dataset; step7: Download and prepare the Waymo-Open dataset; step8: Organize the datasets directory structure; step9: Train the model on the FlyingThings3D dataset with occlusions; step10: Test the model on the FlyingThings3D dataset with occlusions; step11: Test the model on the KITTI dataset with occlusions; step12: Train the model on the FlyingThings3D dataset without occlusions; step13: Test the model on the FlyingThings3D dataset without occlusions; step14: Test the model on the KITTI dataset without occlusions; step15: Train the model on the Waymo-Open dataset; step16: Test the model on the Waymo-Open dataset", "executed_cmds": "conda create -n diffsf python=3.9;conda activate diffsf;conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia;pip install tqdm tensorboard opencv-python", "cmd_prefix": "cd", "cmd_postfix": "utils", "target_cmd": "cd utils"}
{"uuid": "d0edb707-71dc-4e47-9d11-dd7038cd0454", "execution_plan": "step1: Create a conda environment and install PyTorch with CUDA support; step2: Install additional dependencies including tqdm, tensorboard, and opencv-python; step3: Download and prepare the FlyingThings3D dataset; step4: Preprocess the FlyingThings3D dataset for both occluded and non-occluded scenarios; step5: Download and prepare the KITTI dataset; step6: Preprocess the KITTI dataset; step7: Download and prepare the Waymo-Open dataset; step8: Organize the datasets directory structure; step9: Train the model on the FlyingThings3D dataset with occlusions; step10: Test the model on the FlyingThings3D dataset with occlusions; step11: Test the model on the KITTI dataset with occlusions; step12: Train the model on the FlyingThings3D dataset without occlusions; step13: Test the model on the FlyingThings3D dataset without occlusions; step14: Test the model on the KITTI dataset without occlusions; step15: Train the model on the Waymo-Open dataset; step16: Test the model on the Waymo-Open dataset", "executed_cmds": "conda create -n diffsf python=3.9;conda activate diffsf;conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia;pip install tqdm tensorboard opencv-python;cd utils;python preprocess_flyingthings3d_subset.py --input_dir /mnt/data/flyingthings3d_subset --output_dir flyingthings3d_subset;python preprocess_flyingthings3d_subset.py --input_dir /mnt/data/flyingthings3d_subset --output_dir flyingthings3d_subset_non-occluded --remove_occluded_points", "cmd_prefix": "cd", "cmd_postfix": "utils", "target_cmd": "cd utils"}
{"uuid": "253292e0-4b9b-4e55-8f26-12c6d02f1b35", "execution_plan": "step1: Create a conda environment and install PyTorch with CUDA support; step2: Install additional dependencies including tqdm, tensorboard, and opencv-python; step3: Download and prepare the FlyingThings3D dataset; step4: Preprocess the FlyingThings3D dataset for both occluded and non-occluded scenarios; step5: Download and prepare the KITTI dataset; step6: Preprocess the KITTI dataset; step7: Download and prepare the Waymo-Open dataset; step8: Organize the datasets directory structure; step9: Train the model on the FlyingThings3D dataset with occlusions; step10: Test the model on the FlyingThings3D dataset with occlusions; step11: Test the model on the KITTI dataset with occlusions; step12: Train the model on the FlyingThings3D dataset without occlusions; step13: Test the model on the FlyingThings3D dataset without occlusions; step14: Test the model on the KITTI dataset without occlusions; step15: Train the model on the Waymo-Open dataset; step16: Test the model on the Waymo-Open dataset", "executed_cmds": "conda create -n diffsf python=3.9;conda activate diffsf;conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia;pip install tqdm tensorboard opencv-python;cd utils;python preprocess_flyingthings3d_subset.py --input_dir /mnt/data/flyingthings3d_subset --output_dir flyingthings3d_subset;python preprocess_flyingthings3d_subset.py --input_dir /mnt/data/flyingthings3d_subset --output_dir flyingthings3d_subset_non-occluded --remove_occluded_points;cd utils;python process_kitti.py datasets/KITTI_stereo2015/ SAVE_PATH/KITTI_processed_occ_final", "cmd_prefix": "cd", "cmd_postfix": "diffsf", "target_cmd": "cd diffsf"}
{"uuid": "ed219511-894b-443c-8aa4-a17575ca8f18", "execution_plan": "step1: Create a conda environment and install PyTorch with CUDA support; step2: Install additional dependencies including tqdm, tensorboard, and opencv-python; step3: Download and prepare the FlyingThings3D dataset; step4: Preprocess the FlyingThings3D dataset for both occluded and non-occluded scenarios; step5: Download and prepare the KITTI dataset; step6: Preprocess the KITTI dataset; step7: Download and prepare the Waymo-Open dataset; step8: Organize the datasets directory structure; step9: Train the model on the FlyingThings3D dataset with occlusions; step10: Test the model on the FlyingThings3D dataset with occlusions; step11: Test the model on the KITTI dataset with occlusions; step12: Train the model on the FlyingThings3D dataset without occlusions; step13: Test the model on the FlyingThings3D dataset without occlusions; step14: Test the model on the KITTI dataset without occlusions; step15: Train the model on the Waymo-Open dataset; step16: Test the model on the Waymo-Open dataset", "executed_cmds": "conda create -n diffsf python=3.9;conda activate diffsf;conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia;pip install tqdm tensorboard opencv-python;cd utils;python preprocess_flyingthings3d_subset.py --input_dir /mnt/data/flyingthings3d_subset --output_dir flyingthings3d_subset;python preprocess_flyingthings3d_subset.py --input_dir /mnt/data/flyingthings3d_subset --output_dir flyingthings3d_subset_non-occluded --remove_occluded_points;cd utils;python process_kitti.py datasets/KITTI_stereo2015/ SAVE_PATH/KITTI_processed_occ_final;cd diffsf;python waymo_tools/waymo_extract.py", "cmd_prefix": "python waymo_tools/create_data.py", "cmd_postfix": "--dataset_type waymo", "target_cmd": "python waymo_tools/create_data.py --dataset_type waymo"}
{"uuid": "4ddd0ce7-864e-4ef9-9805-6b5ee621c9d5", "execution_plan": "step1: Generate random weights for randomly generated neural networks by running the script with specified depths and widths; step2: Train neural networks on the MNIST dataset by running the training script; step3: Obtain Lipschitz constant estimates and other results by running the Lip_estimates.m script", "executed_cmds": "python generate_random_weights.py", "cmd_prefix": "python", "cmd_postfix": "training_MNIST.py", "target_cmd": "python training_MNIST.py"}
{"uuid": "5ff68fd1-52fa-4f1e-bcca-71f0d3060455", "execution_plan": "step1: Generate random weights for randomly generated neural networks by running the script with specified depths and widths; step2: Train neural networks on the MNIST dataset by running the training script; step3: Obtain Lipschitz constant estimates and other results by running the Lip_estimates.m script", "executed_cmds": "python generate_random_weights.py;python training_MNIST.py", "cmd_prefix": "matlab", "cmd_postfix": "Lip_estimates.m", "target_cmd": "matlab Lip_estimates.m"}
{"uuid": "6292ed08-60b3-499f-848e-555628923744", "execution_plan": "step1: Clone the repository and set up the working directory; step2: Create and activate a conda environment with the specified Python and PyTorch versions; step3: Install PyTorch Geometric and other dependencies using the provided scripts; step4: Install the dagr package and remaining dependencies; step5: Download and organize the ROL and DoTA datasets; step6: Generate event data from videos using v2e; step7: Downsample event data to create events_2x.h5 files; step8: Generate RGB images from videos; step9: Align RGB frames with event stream timestamps; step10: Generate object detections and package them into tracks.npy; step11: Extract TOA values for each video; step12: Generate YAML files to distinguish training and validation sets; step13: Download the dagr model file and place it in the specified folder; step14: Train the model using train.py; step15: Test the model using test.py with the provided best model checkpoint", "executed_cmds": "WORK_DIR=/path/to/work/directory/", "cmd_prefix": "cd", "cmd_postfix": "$WORK_DIR", "target_cmd": "cd $WORK_DIR"}
{"uuid": "0bdebf98-1a5a-4a57-8875-11a146093f71", "execution_plan": "step1: Clone the repository and set up the working directory; step2: Create and activate a conda environment with the specified Python and PyTorch versions; step3: Install PyTorch Geometric and other dependencies using the provided scripts; step4: Install the dagr package and remaining dependencies; step5: Download and organize the ROL and DoTA datasets; step6: Generate event data from videos using v2e; step7: Downsample event data to create events_2x.h5 files; step8: Generate RGB images from videos; step9: Align RGB frames with event stream timestamps; step10: Generate object detections and package them into tracks.npy; step11: Extract TOA values for each video; step12: Generate YAML files to distinguish training and validation sets; step13: Download the dagr model file and place it in the specified folder; step14: Train the model using train.py; step15: Test the model using test.py with the provided best model checkpoint", "executed_cmds": "WORK_DIR=/path/to/work/directory/;cd $WORK_DIR;git clone git@github.com:PKU-XD/EventAD.git;EVENTAD_DIR=$WORK_DIR/EventAD", "cmd_prefix": "cd", "cmd_postfix": "$EVENTAD_DIR", "target_cmd": "cd $EVENTAD_DIR"}
{"uuid": "4d3ac9f4-97a5-4c72-a193-185f61c262bc", "execution_plan": "step1: Clone the repository and set up the working directory; step2: Create and activate a conda environment with the specified Python and PyTorch versions; step3: Install PyTorch Geometric and other dependencies using the provided scripts; step4: Install the dagr package and remaining dependencies; step5: Download and organize the ROL and DoTA datasets; step6: Generate event data from videos using v2e; step7: Downsample event data to create events_2x.h5 files; step8: Generate RGB images from videos; step9: Align RGB frames with event stream timestamps; step10: Generate object detections and package them into tracks.npy; step11: Extract TOA values for each video; step12: Generate YAML files to distinguish training and validation sets; step13: Download the dagr model file and place it in the specified folder; step14: Train the model using train.py; step15: Test the model using test.py with the provided best model checkpoint", "executed_cmds": "WORK_DIR=/path/to/work/directory/;cd $WORK_DIR;git clone git@github.com:PKU-XD/EventAD.git;EVENTAD_DIR=$WORK_DIR/EventAD;cd $EVENTAD_DIR;conda create -y -n EventAD python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate EventAD", "target_cmd": "conda activate EventAD"}
{"uuid": "fab90d5e-17b7-45a6-9801-ef35e62ed661", "execution_plan": "step1: Clone the repository and set up the working directory; step2: Create and activate a conda environment with the specified Python and PyTorch versions; step3: Install PyTorch Geometric and other dependencies using the provided scripts; step4: Install the dagr package and remaining dependencies; step5: Download and organize the ROL and DoTA datasets; step6: Generate event data from videos using v2e; step7: Downsample event data to create events_2x.h5 files; step8: Generate RGB images from videos; step9: Align RGB frames with event stream timestamps; step10: Generate object detections and package them into tracks.npy; step11: Extract TOA values for each video; step12: Generate YAML files to distinguish training and validation sets; step13: Download the dagr model file and place it in the specified folder; step14: Train the model using train.py; step15: Test the model using test.py with the provided best model checkpoint", "executed_cmds": "WORK_DIR=/path/to/work/directory/;cd $WORK_DIR;git clone git@github.com:PKU-XD/EventAD.git;EVENTAD_DIR=$WORK_DIR/EventAD;cd $EVENTAD_DIR;conda create -y -n EventAD python=3.8;conda activate EventAD;conda install -y setuptools==69.5.1 mkl==2024.0 pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch", "cmd_prefix": "bash", "cmd_postfix": "install_env.sh", "target_cmd": "bash install_env.sh"}
{"uuid": "4c23d137-6885-41f8-9c56-a8bb0a53f1ff", "execution_plan": "step1: Clone the repository and set up the working directory; step2: Create and activate a conda environment with the specified Python and PyTorch versions; step3: Install PyTorch Geometric and other dependencies using the provided scripts; step4: Install the dagr package and remaining dependencies; step5: Download and organize the ROL and DoTA datasets; step6: Generate event data from videos using v2e; step7: Downsample event data to create events_2x.h5 files; step8: Generate RGB images from videos; step9: Align RGB frames with event stream timestamps; step10: Generate object detections and package them into tracks.npy; step11: Extract TOA values for each video; step12: Generate YAML files to distinguish training and validation sets; step13: Download the dagr model file and place it in the specified folder; step14: Train the model using train.py; step15: Test the model using test.py with the provided best model checkpoint", "executed_cmds": "WORK_DIR=/path/to/work/directory/;cd $WORK_DIR;git clone git@github.com:PKU-XD/EventAD.git;EVENTAD_DIR=$WORK_DIR/EventAD;cd $EVENTAD_DIR;conda create -y -n EventAD python=3.8;conda activate EventAD;conda install -y setuptools==69.5.1 mkl==2024.0 pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch;bash install_env.sh;bash download_and_install_dependencies.sh;conda install -y h5py blosc-hdf5-plugin", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "6b80ff2d-c6c0-4e93-b922-563b61599248", "execution_plan": "step1: Clone the repository and set up the working directory; step2: Create and activate a conda environment with the specified Python and PyTorch versions; step3: Install PyTorch Geometric and other dependencies using the provided scripts; step4: Install the dagr package and remaining dependencies; step5: Download and organize the ROL and DoTA datasets; step6: Generate event data from videos using v2e; step7: Downsample event data to create events_2x.h5 files; step8: Generate RGB images from videos; step9: Align RGB frames with event stream timestamps; step10: Generate object detections and package them into tracks.npy; step11: Extract TOA values for each video; step12: Generate YAML files to distinguish training and validation sets; step13: Download the dagr model file and place it in the specified folder; step14: Train the model using train.py; step15: Test the model using test.py with the provided best model checkpoint", "executed_cmds": "WORK_DIR=/path/to/work/directory/;cd $WORK_DIR;git clone git@github.com:PKU-XD/EventAD.git;EVENTAD_DIR=$WORK_DIR/EventAD;cd $EVENTAD_DIR;conda create -y -n EventAD python=3.8;conda activate EventAD;conda install -y setuptools==69.5.1 mkl==2024.0 pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch;bash install_env.sh;bash download_and_install_dependencies.sh;conda install -y h5py blosc-hdf5-plugin;pip install -e .", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "1d2bf6a6-9494-470e-b276-09e86d3ec916", "execution_plan": "step1: Clone the repository and set up the working directory; step2: Create and activate a conda environment with the specified Python and PyTorch versions; step3: Install PyTorch Geometric and other dependencies using the provided scripts; step4: Install the dagr package and remaining dependencies; step5: Download and organize the ROL and DoTA datasets; step6: Generate event data from videos using v2e; step7: Downsample event data to create events_2x.h5 files; step8: Generate RGB images from videos; step9: Align RGB frames with event stream timestamps; step10: Generate object detections and package them into tracks.npy; step11: Extract TOA values for each video; step12: Generate YAML files to distinguish training and validation sets; step13: Download the dagr model file and place it in the specified folder; step14: Train the model using train.py; step15: Test the model using test.py with the provided best model checkpoint", "executed_cmds": "WORK_DIR=/path/to/work/directory/;cd $WORK_DIR;git clone git@github.com:PKU-XD/EventAD.git;EVENTAD_DIR=$WORK_DIR/EventAD;cd $EVENTAD_DIR;conda create -y -n EventAD python=3.8;conda activate EventAD;conda install -y setuptools==69.5.1 mkl==2024.0 pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch;bash install_env.sh;bash download_and_install_dependencies.sh;conda install -y h5py blosc-hdf5-plugin;pip install -e .;pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "scripts/v2e.py", "target_cmd": "python scripts/v2e.py"}
{"uuid": "bd78b629-486c-481e-a56d-ba66b8fa7c74", "execution_plan": "step1: Clone the repository and set up the working directory; step2: Create and activate a conda environment with the specified Python and PyTorch versions; step3: Install PyTorch Geometric and other dependencies using the provided scripts; step4: Install the dagr package and remaining dependencies; step5: Download and organize the ROL and DoTA datasets; step6: Generate event data from videos using v2e; step7: Downsample event data to create events_2x.h5 files; step8: Generate RGB images from videos; step9: Align RGB frames with event stream timestamps; step10: Generate object detections and package them into tracks.npy; step11: Extract TOA values for each video; step12: Generate YAML files to distinguish training and validation sets; step13: Download the dagr model file and place it in the specified folder; step14: Train the model using train.py; step15: Test the model using test.py with the provided best model checkpoint", "executed_cmds": "WORK_DIR=/path/to/work/directory/;cd $WORK_DIR;git clone git@github.com:PKU-XD/EventAD.git;EVENTAD_DIR=$WORK_DIR/EventAD;cd $EVENTAD_DIR;conda create -y -n EventAD python=3.8;conda activate EventAD;conda install -y setuptools==69.5.1 mkl==2024.0 pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch;bash install_env.sh;bash download_and_install_dependencies.sh;conda install -y h5py blosc-hdf5-plugin;pip install -e .;pip install -r requirements.txt;python scripts/v2e.py;bash scripts/downsample_all_events.sh", "cmd_prefix": "python", "cmd_postfix": "scripts/video2rgb.py", "target_cmd": "python scripts/video2rgb.py"}
{"uuid": "43196263-7ec1-4b9e-9391-0c644e856409", "execution_plan": "step1: Clone the repository and set up the working directory; step2: Create and activate a conda environment with the specified Python and PyTorch versions; step3: Install PyTorch Geometric and other dependencies using the provided scripts; step4: Install the dagr package and remaining dependencies; step5: Download and organize the ROL and DoTA datasets; step6: Generate event data from videos using v2e; step7: Downsample event data to create events_2x.h5 files; step8: Generate RGB images from videos; step9: Align RGB frames with event stream timestamps; step10: Generate object detections and package them into tracks.npy; step11: Extract TOA values for each video; step12: Generate YAML files to distinguish training and validation sets; step13: Download the dagr model file and place it in the specified folder; step14: Train the model using train.py; step15: Test the model using test.py with the provided best model checkpoint", "executed_cmds": "WORK_DIR=/path/to/work/directory/;cd $WORK_DIR;git clone git@github.com:PKU-XD/EventAD.git;EVENTAD_DIR=$WORK_DIR/EventAD;cd $EVENTAD_DIR;conda create -y -n EventAD python=3.8;conda activate EventAD;conda install -y setuptools==69.5.1 mkl==2024.0 pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch;bash install_env.sh;bash download_and_install_dependencies.sh;conda install -y h5py blosc-hdf5-plugin;pip install -e .;pip install -r requirements.txt;python scripts/v2e.py;bash scripts/downsample_all_events.sh;python scripts/video2rgb.py;python scripts/timestamps.py", "cmd_prefix": "python", "cmd_postfix": "scripts/track.py", "target_cmd": "python scripts/track.py"}
{"uuid": "deed67b6-1722-420b-add7-edf734f597f4", "execution_plan": "step1: Clone the repository and set up the working directory; step2: Create and activate a conda environment with the specified Python and PyTorch versions; step3: Install PyTorch Geometric and other dependencies using the provided scripts; step4: Install the dagr package and remaining dependencies; step5: Download and organize the ROL and DoTA datasets; step6: Generate event data from videos using v2e; step7: Downsample event data to create events_2x.h5 files; step8: Generate RGB images from videos; step9: Align RGB frames with event stream timestamps; step10: Generate object detections and package them into tracks.npy; step11: Extract TOA values for each video; step12: Generate YAML files to distinguish training and validation sets; step13: Download the dagr model file and place it in the specified folder; step14: Train the model using train.py; step15: Test the model using test.py with the provided best model checkpoint", "executed_cmds": "WORK_DIR=/path/to/work/directory/;cd $WORK_DIR;git clone git@github.com:PKU-XD/EventAD.git;EVENTAD_DIR=$WORK_DIR/EventAD;cd $EVENTAD_DIR;conda create -y -n EventAD python=3.8;conda activate EventAD;conda install -y setuptools==69.5.1 mkl==2024.0 pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch;bash install_env.sh;bash download_and_install_dependencies.sh;conda install -y h5py blosc-hdf5-plugin;pip install -e .;pip install -r requirements.txt;python scripts/v2e.py;bash scripts/downsample_all_events.sh;python scripts/video2rgb.py;python scripts/timestamps.py;python scripts/track.py;python scripts/extract_toa_value.py;python scripts/generate_yaml.py", "cmd_prefix": "python", "cmd_postfix": "train.py", "target_cmd": "python train.py"}
{"uuid": "3c37c0bd-c1c6-4eb0-a9f3-98acd109cead", "execution_plan": "step1: Clone the GPT-NeoX repository to get the base codebase for training; step2: Install the required dependencies for GPT-NeoX; step3: Install Flash Linear Attention for implementing the Meta Linear Attention operator; step4: Install causal-conv1d for short convolution implementation; step5: Add the MetaLA implementation to the GPT-NeoX codebase and update the model configuration; step6: Create a configuration file for MetaLA training; step7: Run the training process using the provided configuration; step8: Test the environment with a simple inference script; step9: Download the released checkpoints for generation tasks; step10: Perform text generation using the downloaded checkpoints", "executed_cmds": "git clone git@github.com:EleutherAI/gpt-neox.git", "cmd_prefix": "cd", "cmd_postfix": "gpt-neox", "target_cmd": "cd gpt-neox"}
{"uuid": "7304e76f-7338-475c-8629-0ee6f16f6cbc", "execution_plan": "step1: Clone the repository and install the required package; step2: Prepare the data by either downloading preprocessed data or preprocessing it yourself; step3: Precompute the substring frequency and cache the preprocessed file; step4: Run the training code with the desired training mode", "executed_cmds": "git clone https://github.com/junjiechen-chris/Improving-Unsupervised-Constituency-Parsing-via-Maximizing-Semantic-Information.git;pip install -e Improving-Unsupervised-Constituency-Parsing-via-Maximizing-Semantic-Information;cp -r Improving-Unsupervised-Constituency-Parsing-via-Maximizing-Semantic-Information/config config", "cmd_prefix": "mkdir", "cmd_postfix": "-p data", "target_cmd": "mkdir -p data"}
{"uuid": "80083c22-fb84-4b27-89f7-971294952323", "execution_plan": "step1: Create a Python 3.11 conda environment for the project; step2: Clone the repository and install the project dependencies; step3: Run training for a specific model (e.g., CIFAR-10 small model); step4: (Optional) Run the provided demo notebook in Google Colab for quick testing; step5: (Docker-based) Build the base Docker image with necessary configurations; step6: (Docker-based) Build the training Docker image; step7: (Docker-based) Run training using Docker for a specific model (e.g., MNIST small model); step8: (Optional) Enable wandb logging for training", "executed_cmds": "conda create -n convlogic python=3.11", "cmd_prefix": "conda", "cmd_postfix": "activate convlogic", "target_cmd": "conda activate convlogic"}
{"uuid": "584a1392-ecad-46c4-83c7-6443c84d6b7b", "execution_plan": "step1: Create a Python 3.11 conda environment for the project; step2: Clone the repository and install the project dependencies; step3: Run training for a specific model (e.g., CIFAR-10 small model); step4: (Optional) Run the provided demo notebook in Google Colab for quick testing; step5: (Docker-based) Build the base Docker image with necessary configurations; step6: (Docker-based) Build the training Docker image; step7: (Docker-based) Run training using Docker for a specific model (e.g., MNIST small model); step8: (Optional) Enable wandb logging for training", "executed_cmds": "conda create -n convlogic python=3.11;conda activate convlogic;git clone https://github.com/lkorinek/convlogic.git", "cmd_prefix": "cd", "cmd_postfix": "convlogic", "target_cmd": "cd convlogic"}
{"uuid": "868eb7c7-738f-4b14-9815-7eb394857bbe", "execution_plan": "step1: Create a Python 3.11 conda environment for the project; step2: Clone the repository and install the project dependencies; step3: Run training for a specific model (e.g., CIFAR-10 small model); step4: (Optional) Run the provided demo notebook in Google Colab for quick testing; step5: (Docker-based) Build the base Docker image with necessary configurations; step6: (Docker-based) Build the training Docker image; step7: (Docker-based) Run training using Docker for a specific model (e.g., MNIST small model); step8: (Optional) Enable wandb logging for training", "executed_cmds": "conda create -n convlogic python=3.11;conda activate convlogic;git clone https://github.com/lkorinek/convlogic.git;cd convlogic", "cmd_prefix": "pip install", "cmd_postfix": "-v .", "target_cmd": "pip install -v ."}
{"uuid": "4f73f2a1-d179-4a7c-8633-612ab0e02108", "execution_plan": "step1: Install the minimal dependencies required to run the repository, including PyTorch, PyTorch Geometric, and NetworkX; step2: Install additional dependencies for running baselines, including CVXPy, Gurobi, and Mosek; step3: Prepare the training environment by understanding the available dataset options and model configurations; step4: Run the training script with specified parameters, such as problem type, dataset, model type, and other hyperparameters; step5: Test the trained model using the test script, specifying the model folder, checkpoint file, and test dataset parameters; step6: Review the output files generated during training and testing, located in the specified directories", "executed_cmds": "pip install torch;pip install torch-geometric", "cmd_prefix": "pip", "cmd_postfix": "install networkx", "target_cmd": "pip install networkx"}
{"uuid": "2b8b2cbd-2416-478f-92f1-49315a65763a", "execution_plan": "step1: Install the minimal dependencies required to run the repository, including PyTorch, PyTorch Geometric, and NetworkX; step2: Install additional dependencies for running baselines, including CVXPy, Gurobi, and Mosek; step3: Prepare the training environment by understanding the available dataset options and model configurations; step4: Run the training script with specified parameters, such as problem type, dataset, model type, and other hyperparameters; step5: Test the trained model using the test script, specifying the model folder, checkpoint file, and test dataset parameters; step6: Review the output files generated during training and testing, located in the specified directories", "executed_cmds": "pip install torch;pip install torch-geometric;pip install networkx", "cmd_prefix": "pip", "cmd_postfix": "install cvxpy", "target_cmd": "pip install cvxpy"}
{"uuid": "844bc438-fc49-4844-83c1-b95d475d8453", "execution_plan": "step1: Install the minimal dependencies required to run the repository, including PyTorch, PyTorch Geometric, and NetworkX; step2: Install additional dependencies for running baselines, including CVXPy, Gurobi, and Mosek; step3: Prepare the training environment by understanding the available dataset options and model configurations; step4: Run the training script with specified parameters, such as problem type, dataset, model type, and other hyperparameters; step5: Test the trained model using the test script, specifying the model folder, checkpoint file, and test dataset parameters; step6: Review the output files generated during training and testing, located in the specified directories", "executed_cmds": "pip install torch;pip install torch-geometric;pip install networkx;pip install cvxpy", "cmd_prefix": "pip", "cmd_postfix": "install gurobipy", "target_cmd": "pip install gurobipy"}
{"uuid": "65102b9e-15af-4fd1-b9ae-6b4ae0b4fd26", "execution_plan": "step1: Install the minimal dependencies required to run the repository, including PyTorch, PyTorch Geometric, and NetworkX; step2: Install additional dependencies for running baselines, including CVXPy, Gurobi, and Mosek; step3: Prepare the training environment by understanding the available dataset options and model configurations; step4: Run the training script with specified parameters, such as problem type, dataset, model type, and other hyperparameters; step5: Test the trained model using the test script, specifying the model folder, checkpoint file, and test dataset parameters; step6: Review the output files generated during training and testing, located in the specified directories", "executed_cmds": "pip install torch;pip install torch-geometric;pip install networkx;pip install cvxpy;pip install gurobipy", "cmd_prefix": "pip", "cmd_postfix": "install mosek", "target_cmd": "pip install mosek"}
{"uuid": "1a1f3332-4111-403b-97d7-a93f69caac97", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Set up the Python environment using conda and install required dependencies; step3: Run inference on math problems using the provided script with an API key; step4: Run the BoT framework on one of the three benchmarks (gameof24, checkmate, wordsorting) with optional API key and model ID; step5: Validate the test results for a specific task using the validation script", "executed_cmds": "git clone https://github.com/YangLing0818/buffer-of-thought-llm;cd buffer-of-thought-llm", "cmd_prefix": "conda create", "cmd_postfix": "-n BoT python==3.9", "target_cmd": "conda create -n BoT python==3.9"}
{"uuid": "46c4a9c1-6d98-4cff-81c8-b06c259b6e94", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Set up the Python environment using conda and install required dependencies; step3: Run inference on math problems using the provided script with an API key; step4: Run the BoT framework on one of the three benchmarks (gameof24, checkmate, wordsorting) with optional API key and model ID; step5: Validate the test results for a specific task using the validation script", "executed_cmds": "git clone https://github.com/YangLing0818/buffer-of-thought-llm;cd buffer-of-thought-llm;conda create -n BoT python==3.9", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "f30bae82-d657-45f4-a9ca-e4ebe298a6ef", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Set up the Python environment using conda and install required dependencies; step3: Run inference on math problems using the provided script with an API key; step4: Run the BoT framework on one of the three benchmarks (gameof24, checkmate, wordsorting) with optional API key and model ID; step5: Validate the test results for a specific task using the validation script", "executed_cmds": "git clone https://github.com/YangLing0818/buffer-of-thought-llm;cd buffer-of-thought-llm;conda create -n BoT python==3.9;pip install -r requirements.txt", "cmd_prefix": "python inference.py --api_key 'Input", "cmd_postfix": "your api key here'", "target_cmd": "python inference.py --api_key 'Input your api key here'"}
{"uuid": "f6b187c5-7c76-4bfc-bfca-46e7b6082837", "execution_plan": "step1: Ensure you have a CUDA12-capable GPU installed for optimal performance; step2: Install dependencies using pip and the provided `pyproject.toml`, preferably using the `uv` tool; step3: Fix potential module import errors by activating the virtual environment and installing the package in development mode; step4: Configure the main experiment parameters such as UTD ratio, DMC domain, task, frame skip, proportion of real data, and MPC usage; step5: Run the main experiments with recommended settings (action repeat 2 and UTD 8); step6: Refer to the raw results in the corresponding folder and be cautious when replicating results", "executed_cmds": "`uv run mad_td/main.py`", "cmd_prefix": "`source", "cmd_postfix": ".venv/bin/activate`", "target_cmd": "`source .venv/bin/activate`"}
{"uuid": "e6c6b336-6112-40ec-ba31-916e49d08c69", "execution_plan": "step1: Ensure you have a CUDA12-capable GPU installed for optimal performance; step2: Install dependencies using pip and the provided `pyproject.toml`, preferably using the `uv` tool; step3: Fix potential module import errors by activating the virtual environment and installing the package in development mode; step4: Configure the main experiment parameters such as UTD ratio, DMC domain, task, frame skip, proportion of real data, and MPC usage; step5: Run the main experiments with recommended settings (action repeat 2 and UTD 8); step6: Refer to the raw results in the corresponding folder and be cautious when replicating results", "executed_cmds": "`uv run mad_td/main.py`;`source .venv/bin/activate`", "cmd_prefix": "`uv pip", "cmd_postfix": "install -e .`", "target_cmd": "`uv pip install -e .`"}
{"uuid": "6ccb25c4-2eea-475b-bd12-7b43c16bdfcb", "execution_plan": "step1: Install CUDA 11.8 to meet the prerequisite for PyTorch and TetSphere geometry energies; step2: Clone and build the forked TetWild repository for initializing TetSpheres and merging them into one shape; step3: Set up a Conda environment for TetSphere splatting and install dependencies using the provided script; step4: Install libpgo, a C++ library with Python bindings, inside the Conda environment for custom CUDA extension; step5: Build the custom CUDA extension for TetSphere splatting; step6: Download and prepare the example data for TetSphere splatting; step7: Initialize TetSpheres using the provided script and generate initial radii and center positions; step8: Run TetSphere splatting to optimize and reconstruct the 3D shape; step9: Visualize the results using provided tools or modeling software like MeshLab or Blender", "executed_cmds": "\"git clone git@github.com:gmh14/TetWild.git\"", "cmd_prefix": "\"cd", "cmd_postfix": "TetWild\"", "target_cmd": "\"cd TetWild\""}
{"uuid": "cd3a9865-2dd2-47a0-a9fd-f125672b825e", "execution_plan": "step1: Install CUDA 11.8 to meet the prerequisite for PyTorch and TetSphere geometry energies; step2: Clone and build the forked TetWild repository for initializing TetSpheres and merging them into one shape; step3: Set up a Conda environment for TetSphere splatting and install dependencies using the provided script; step4: Install libpgo, a C++ library with Python bindings, inside the Conda environment for custom CUDA extension; step5: Build the custom CUDA extension for TetSphere splatting; step6: Download and prepare the example data for TetSphere splatting; step7: Initialize TetSpheres using the provided script and generate initial radii and center positions; step8: Run TetSphere splatting to optimize and reconstruct the 3D shape; step9: Visualize the results using provided tools or modeling software like MeshLab or Blender", "executed_cmds": "\"git clone git@github.com:gmh14/TetWild.git\";\"cd TetWild\"", "cmd_prefix": "\"mkdir build", "cmd_postfix": "&& cd build\"", "target_cmd": "\"mkdir build && cd build\""}
{"uuid": "11b50185-c27a-43dc-a9b5-f85e29e8dc27", "execution_plan": "step1: Install CUDA 11.8 to meet the prerequisite for PyTorch and TetSphere geometry energies; step2: Clone and build the forked TetWild repository for initializing TetSpheres and merging them into one shape; step3: Set up a Conda environment for TetSphere splatting and install dependencies using the provided script; step4: Install libpgo, a C++ library with Python bindings, inside the Conda environment for custom CUDA extension; step5: Build the custom CUDA extension for TetSphere splatting; step6: Download and prepare the example data for TetSphere splatting; step7: Initialize TetSpheres using the provided script and generate initial radii and center positions; step8: Run TetSphere splatting to optimize and reconstruct the 3D shape; step9: Visualize the results using provided tools or modeling software like MeshLab or Blender", "executed_cmds": "\"git clone git@github.com:gmh14/TetWild.git\";\"cd TetWild\";\"mkdir build && cd build\"", "cmd_prefix": "\"cmake", "cmd_postfix": "..\"", "target_cmd": "\"cmake ..\""}
{"uuid": "afe59119-8307-4f4f-a620-631fc5f7e4df", "execution_plan": "step1: Install CUDA 11.8 to meet the prerequisite for PyTorch and TetSphere geometry energies; step2: Clone and build the forked TetWild repository for initializing TetSpheres and merging them into one shape; step3: Set up a Conda environment for TetSphere splatting and install dependencies using the provided script; step4: Install libpgo, a C++ library with Python bindings, inside the Conda environment for custom CUDA extension; step5: Build the custom CUDA extension for TetSphere splatting; step6: Download and prepare the example data for TetSphere splatting; step7: Initialize TetSpheres using the provided script and generate initial radii and center positions; step8: Run TetSphere splatting to optimize and reconstruct the 3D shape; step9: Visualize the results using provided tools or modeling software like MeshLab or Blender", "executed_cmds": "\"git clone git@github.com:gmh14/TetWild.git\";\"cd TetWild\";\"mkdir build && cd build\";\"cmake ..\"", "cmd_prefix": "\"make", "cmd_postfix": "-j\"", "target_cmd": "\"make -j\""}
{"uuid": "0192cd7f-ca4e-43f4-b7f6-e01434888a23", "execution_plan": "step1: Install CUDA 11.8 to meet the prerequisite for PyTorch and TetSphere geometry energies; step2: Clone and build the forked TetWild repository for initializing TetSpheres and merging them into one shape; step3: Set up a Conda environment for TetSphere splatting and install dependencies using the provided script; step4: Install libpgo, a C++ library with Python bindings, inside the Conda environment for custom CUDA extension; step5: Build the custom CUDA extension for TetSphere splatting; step6: Download and prepare the example data for TetSphere splatting; step7: Initialize TetSpheres using the provided script and generate initial radii and center positions; step8: Run TetSphere splatting to optimize and reconstruct the 3D shape; step9: Visualize the results using provided tools or modeling software like MeshLab or Blender", "executed_cmds": "\"git clone git@github.com:gmh14/TetWild.git\";\"cd TetWild\";\"mkdir build && cd build\";\"cmake ..\";\"make -j\";\"git clone git@github.com:gmh14/tssplat.git\"", "cmd_prefix": "\"cd", "cmd_postfix": "tssplat\"", "target_cmd": "\"cd tssplat\""}
{"uuid": "2e846ddf-195e-45a9-80b4-670baf4776a5", "execution_plan": "step1: Install CUDA 11.8 to meet the prerequisite for PyTorch and TetSphere geometry energies; step2: Clone and build the forked TetWild repository for initializing TetSpheres and merging them into one shape; step3: Set up a Conda environment for TetSphere splatting and install dependencies using the provided script; step4: Install libpgo, a C++ library with Python bindings, inside the Conda environment for custom CUDA extension; step5: Build the custom CUDA extension for TetSphere splatting; step6: Download and prepare the example data for TetSphere splatting; step7: Initialize TetSpheres using the provided script and generate initial radii and center positions; step8: Run TetSphere splatting to optimize and reconstruct the 3D shape; step9: Visualize the results using provided tools or modeling software like MeshLab or Blender", "executed_cmds": "\"git clone git@github.com:gmh14/TetWild.git\";\"cd TetWild\";\"mkdir build && cd build\";\"cmake ..\";\"make -j\";\"git clone git@github.com:gmh14/tssplat.git\";\"cd tssplat\";\"conda create --name tssplat python=3.10\"", "cmd_prefix": "\"conda", "cmd_postfix": "activate tssplat\"", "target_cmd": "\"conda activate tssplat\""}
{"uuid": "15872271-f7fa-4ec4-9542-6ef498ee25aa", "execution_plan": "step1: Install CUDA 11.8 to meet the prerequisite for PyTorch and TetSphere geometry energies; step2: Clone and build the forked TetWild repository for initializing TetSpheres and merging them into one shape; step3: Set up a Conda environment for TetSphere splatting and install dependencies using the provided script; step4: Install libpgo, a C++ library with Python bindings, inside the Conda environment for custom CUDA extension; step5: Build the custom CUDA extension for TetSphere splatting; step6: Download and prepare the example data for TetSphere splatting; step7: Initialize TetSpheres using the provided script and generate initial radii and center positions; step8: Run TetSphere splatting to optimize and reconstruct the 3D shape; step9: Visualize the results using provided tools or modeling software like MeshLab or Blender", "executed_cmds": "\"git clone git@github.com:gmh14/TetWild.git\";\"cd TetWild\";\"mkdir build && cd build\";\"cmake ..\";\"make -j\";\"git clone git@github.com:gmh14/tssplat.git\";\"cd tssplat\";\"conda create --name tssplat python=3.10\";\"conda activate tssplat\";\"export CUDA_HOME=/usr/local/cuda-11.8\"", "cmd_prefix": "\"bash", "cmd_postfix": "install.sh\"", "target_cmd": "\"bash install.sh\""}
{"uuid": "9e91a9d0-9b32-4cb8-b5bc-60ea36d3a523", "execution_plan": "step1: Install CUDA 11.8 to meet the prerequisite for PyTorch and TetSphere geometry energies; step2: Clone and build the forked TetWild repository for initializing TetSpheres and merging them into one shape; step3: Set up a Conda environment for TetSphere splatting and install dependencies using the provided script; step4: Install libpgo, a C++ library with Python bindings, inside the Conda environment for custom CUDA extension; step5: Build the custom CUDA extension for TetSphere splatting; step6: Download and prepare the example data for TetSphere splatting; step7: Initialize TetSpheres using the provided script and generate initial radii and center positions; step8: Run TetSphere splatting to optimize and reconstruct the 3D shape; step9: Visualize the results using provided tools or modeling software like MeshLab or Blender", "executed_cmds": "\"git clone git@github.com:gmh14/TetWild.git\";\"cd TetWild\";\"mkdir build && cd build\";\"cmake ..\";\"make -j\";\"git clone git@github.com:gmh14/tssplat.git\";\"cd tssplat\";\"conda create --name tssplat python=3.10\";\"conda activate tssplat\";\"export CUDA_HOME=/usr/local/cuda-11.8\";\"bash install.sh\"", "cmd_prefix": "\"cd", "cmd_postfix": "tssplat_ext\"", "target_cmd": "\"cd tssplat_ext\""}
{"uuid": "9fb4df7e-f7e0-4fe0-9aaf-19a2eb661ae3", "execution_plan": "step1: Install CUDA 11.8 to meet the prerequisite for PyTorch and TetSphere geometry energies; step2: Clone and build the forked TetWild repository for initializing TetSpheres and merging them into one shape; step3: Set up a Conda environment for TetSphere splatting and install dependencies using the provided script; step4: Install libpgo, a C++ library with Python bindings, inside the Conda environment for custom CUDA extension; step5: Build the custom CUDA extension for TetSphere splatting; step6: Download and prepare the example data for TetSphere splatting; step7: Initialize TetSpheres using the provided script and generate initial radii and center positions; step8: Run TetSphere splatting to optimize and reconstruct the 3D shape; step9: Visualize the results using provided tools or modeling software like MeshLab or Blender", "executed_cmds": "\"git clone git@github.com:gmh14/TetWild.git\";\"cd TetWild\";\"mkdir build && cd build\";\"cmake ..\";\"make -j\";\"git clone git@github.com:gmh14/tssplat.git\";\"cd tssplat\";\"conda create --name tssplat python=3.10\";\"conda activate tssplat\";\"export CUDA_HOME=/usr/local/cuda-11.8\";\"bash install.sh\";\"cd tssplat_ext\"", "cmd_prefix": "\"bash", "cmd_postfix": "install_ext.sh\"", "target_cmd": "\"bash install_ext.sh\""}
{"uuid": "39ec0ec5-fc47-49e6-9fe4-dea838ad1761", "execution_plan": "step1: Install CUDA 11.8 to meet the prerequisite for PyTorch and TetSphere geometry energies; step2: Clone and build the forked TetWild repository for initializing TetSpheres and merging them into one shape; step3: Set up a Conda environment for TetSphere splatting and install dependencies using the provided script; step4: Install libpgo, a C++ library with Python bindings, inside the Conda environment for custom CUDA extension; step5: Build the custom CUDA extension for TetSphere splatting; step6: Download and prepare the example data for TetSphere splatting; step7: Initialize TetSpheres using the provided script and generate initial radii and center positions; step8: Run TetSphere splatting to optimize and reconstruct the 3D shape; step9: Visualize the results using provided tools or modeling software like MeshLab or Blender", "executed_cmds": "\"git clone git@github.com:gmh14/TetWild.git\";\"cd TetWild\";\"mkdir build && cd build\";\"cmake ..\";\"make -j\";\"git clone git@github.com:gmh14/tssplat.git\";\"cd tssplat\";\"conda create --name tssplat python=3.10\";\"conda activate tssplat\";\"export CUDA_HOME=/usr/local/cuda-11.8\";\"bash install.sh\";\"cd tssplat_ext\";\"bash install_ext.sh\";\"export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:$LD_LIBRARY_PATH && python test_ext.py\";\"export CUDA_HOME=/usr/local/cuda-11.8\";\"export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:$LD_LIBRARY_PATH\"", "cmd_prefix": "\"cd", "cmd_postfix": "data\"", "target_cmd": "\"cd data\""}
{"uuid": "0be91ee1-9b07-4bfe-bc9c-4178b4758ba7", "execution_plan": "step1: Install CUDA 11.8 to meet the prerequisite for PyTorch and TetSphere geometry energies; step2: Clone and build the forked TetWild repository for initializing TetSpheres and merging them into one shape; step3: Set up a Conda environment for TetSphere splatting and install dependencies using the provided script; step4: Install libpgo, a C++ library with Python bindings, inside the Conda environment for custom CUDA extension; step5: Build the custom CUDA extension for TetSphere splatting; step6: Download and prepare the example data for TetSphere splatting; step7: Initialize TetSpheres using the provided script and generate initial radii and center positions; step8: Run TetSphere splatting to optimize and reconstruct the 3D shape; step9: Visualize the results using provided tools or modeling software like MeshLab or Blender", "executed_cmds": "\"git clone git@github.com:gmh14/TetWild.git\";\"cd TetWild\";\"mkdir build && cd build\";\"cmake ..\";\"make -j\";\"git clone git@github.com:gmh14/tssplat.git\";\"cd tssplat\";\"conda create --name tssplat python=3.10\";\"conda activate tssplat\";\"export CUDA_HOME=/usr/local/cuda-11.8\";\"bash install.sh\";\"cd tssplat_ext\";\"bash install_ext.sh\";\"export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:$LD_LIBRARY_PATH && python test_ext.py\";\"export CUDA_HOME=/usr/local/cuda-11.8\";\"export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:$LD_LIBRARY_PATH\";\"cd data\";\"export expr_name=a_white_dog\";\"python generate_init_spheres.py --img_path ../img_data/$expr_name --expr_name $expr_name --save_path ../mesh_data/$expr_name --rendering_type mistuba\"", "cmd_prefix": "\"cd", "cmd_postfix": "..\"", "target_cmd": "\"cd ..\""}
{"uuid": "9bf23fda-078f-468c-9a46-10a481475111", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Set up the Conda environment using the provided environment.yml file; step3: Download the RefLERF dataset and prepare the directory structure; step4: Train the model using the provided training script; step5: Render the output using the provided render script; step6: Obtain pseudo masks using the Grounded-SAM method (external reference)", "executed_cmds": "git clone git@github.com:heshuting555/ReferSplat.git", "cmd_prefix": "cd", "cmd_postfix": "ReferSplat", "target_cmd": "cd ReferSplat"}
{"uuid": "d43ec99d-8154-43a8-827e-599cea3656a8", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Set up the Conda environment using the provided environment.yml file; step3: Download the RefLERF dataset and prepare the directory structure; step4: Train the model using the provided training script; step5: Render the output using the provided render script; step6: Obtain pseudo masks using the Grounded-SAM method (external reference)", "executed_cmds": "git clone git@github.com:heshuting555/ReferSplat.git;cd ReferSplat;conda env create --file environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate refsplat", "target_cmd": "conda activate refsplat"}
{"uuid": "55ade964-2bd1-457a-9f4e-ad3c50fa28aa", "execution_plan": "step1: Create a conda environment and install dependencies; step2: Train the model with the specified configuration for House Price Regression; step3: Train the model with the specified configuration for MNIST Classification (digits 0 and 1); step4: Train the model with the specified configuration for ResNet18 using Cars dataset; step5: Train the model with the specified configuration for ResNet18 using Country211 dataset; step6: Train the model with the specified configuration for ViT using Cars dataset; step7: Train the model with the specified configuration for ViT using Country211 dataset", "executed_cmds": "conda env create -n model-immunization;conda activate model-immunization", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "c6ea33b0-354d-4797-b9a5-5fb83b9ed79e", "execution_plan": "step1: Install Docker or Conda to set up the environment for running DeFoG; step2: Clone the DeFoG repository and install necessary modules; step3: Verify the installation of main packages and compile the ORCA evaluator; step4: Run a quick test to ensure the code is working; step5: Train the model on a specific dataset; step6: Perform sampling optimization to find the best configuration; step7: Evaluate the model using the optimal sampling parameters; step8: Extend DeFoG to new datasets by creating custom dataset files and updating configurations; step9: Download checkpoints for evaluation if available", "executed_cmds": "docker build --platform=linux/amd64 -t defog-image . OR conda env create -f environment.yaml", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "3b14154c-37fe-4859-b3a1-e2748c68535f", "execution_plan": "step1: Install Docker or Conda to set up the environment for running DeFoG; step2: Clone the DeFoG repository and install necessary modules; step3: Verify the installation of main packages and compile the ORCA evaluator; step4: Run a quick test to ensure the code is working; step5: Train the model on a specific dataset; step6: Perform sampling optimization to find the best configuration; step7: Evaluate the model using the optimal sampling parameters; step8: Extend DeFoG to new datasets by creating custom dataset files and updating configurations; step9: Download checkpoints for evaluation if available", "executed_cmds": "docker build --platform=linux/amd64 -t defog-image . OR conda env create -f environment.yaml;pip install -e .;python -c \"import sys; print('Python version:';sys.version)\";python -c \"import rdkit; print('RDKit version:';rdkit.__version__)\";python -c \"import graph_tool as gt; print('Graph-Tool version:';gt.__version__)\";python -c \"import torch; print(f'PyTorch version: {torch.__version__};CUDA version (via PyTorch): {torch.version.cuda}')\";python -c \"import torch_geometric as tg; print('PyTorch Geometric version:';tg.__version__)\"", "cmd_prefix": "cd", "cmd_postfix": "src/analysis/orca", "target_cmd": "cd src/analysis/orca"}
{"uuid": "97e88f9b-72e9-43e5-a300-75e78aaee2cb", "execution_plan": "step1: Install Docker or Conda to set up the environment for running DeFoG; step2: Clone the DeFoG repository and install necessary modules; step3: Verify the installation of main packages and compile the ORCA evaluator; step4: Run a quick test to ensure the code is working; step5: Train the model on a specific dataset; step6: Perform sampling optimization to find the best configuration; step7: Evaluate the model using the optimal sampling parameters; step8: Extend DeFoG to new datasets by creating custom dataset files and updating configurations; step9: Download checkpoints for evaluation if available", "executed_cmds": "docker build --platform=linux/amd64 -t defog-image . OR conda env create -f environment.yaml;pip install -e .;python -c \"import sys; print('Python version:';sys.version)\";python -c \"import rdkit; print('RDKit version:';rdkit.__version__)\";python -c \"import graph_tool as gt; print('Graph-Tool version:';gt.__version__)\";python -c \"import torch; print(f'PyTorch version: {torch.__version__};CUDA version (via PyTorch): {torch.version.cuda}')\";python -c \"import torch_geometric as tg; print('PyTorch Geometric version:';tg.__version__)\";cd src/analysis/orca", "cmd_prefix": "g++ -O2 -std=c++11", "cmd_postfix": "-o orca orca.cpp", "target_cmd": "g++ -O2 -std=c++11 -o orca orca.cpp"}
{"uuid": "5cc24922-effb-4de2-a186-c8db2442586f", "execution_plan": "step1: Create a conda environment with Python 3.9 and necessary libraries; step2: Install MuJoCo Simulator and mujoco-py dependencies; step3: Set environment variables to avoid multiprocess issues; step4: Install project dependencies using pip; step5: (Optional) Download and unzip pretrained models for visualization; step6: (Optional) Run interactive visualization locally with a GUI display; step7: Train the model using the provided script", "executed_cmds": "conda create -n BodyGen python=3.9 mesalib glew glfw -c conda-forge -y", "cmd_prefix": "conda", "cmd_postfix": "activate BodyGen", "target_cmd": "conda activate BodyGen"}
{"uuid": "a6dfe124-16b1-4d66-8264-e93f07936043", "execution_plan": "step1: Create a conda environment with Python 3.9 and necessary libraries; step2: Install MuJoCo Simulator and mujoco-py dependencies; step3: Set environment variables to avoid multiprocess issues; step4: Install project dependencies using pip; step5: (Optional) Download and unzip pretrained models for visualization; step6: (Optional) Run interactive visualization locally with a GUI display; step7: Train the model using the provided script", "executed_cmds": "conda create -n BodyGen python=3.9 mesalib glew glfw -c conda-forge -y;conda activate BodyGen;sudo apt-get update && sudo apt-get install -y wget tar libosmesa6-dev libglew-dev libgl1-mesa-glx libglfw3 patchelf cmake;sudo ln -s /usr/lib/x86_64-linux-gnu/libGL.so.1 /usr/lib/x86_64-linux-gnu/libGL.so;wget -c \"https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\"", "cmd_prefix": "mkdir", "cmd_postfix": "-p $USER_DIR/.mujoco", "target_cmd": "mkdir -p $USER_DIR/.mujoco"}
{"uuid": "c8efedd2-7c7d-4bdd-8214-dd7a3750efab", "execution_plan": "step1: Create a conda environment with Python 3.9 and necessary libraries; step2: Install MuJoCo Simulator and mujoco-py dependencies; step3: Set environment variables to avoid multiprocess issues; step4: Install project dependencies using pip; step5: (Optional) Download and unzip pretrained models for visualization; step6: (Optional) Run interactive visualization locally with a GUI display; step7: Train the model using the provided script", "executed_cmds": "conda create -n BodyGen python=3.9 mesalib glew glfw -c conda-forge -y;conda activate BodyGen;sudo apt-get update && sudo apt-get install -y wget tar libosmesa6-dev libglew-dev libgl1-mesa-glx libglfw3 patchelf cmake;sudo ln -s /usr/lib/x86_64-linux-gnu/libGL.so.1 /usr/lib/x86_64-linux-gnu/libGL.so;wget -c \"https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\";mkdir -p $USER_DIR/.mujoco;cp mujoco210-linux-x86_64.tar.gz $USER_DIR/mujoco.tar.gz;rm mujoco210-linux-x86_64.tar.gz;tar -zxvf $USER_DIR/mujoco.tar.gz -C $USER_DIR/.mujoco;echo \"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$USER_DIR/.mujoco/mujoco210/bin\" >> ~/.bashrc;echo \"export MUJOCO_PY_MUJOCO_PATH=$USER_DIR/.mujoco/mujoco210\" >> ~/.bashrc", "cmd_prefix": "source", "cmd_postfix": "~/.bashrc", "target_cmd": "source ~/.bashrc"}
{"uuid": "50ce1a92-df41-41fe-8cb2-92543a8aa3ec", "execution_plan": "step1: Create a conda environment with Python 3.9 and necessary libraries; step2: Install MuJoCo Simulator and mujoco-py dependencies; step3: Set environment variables to avoid multiprocess issues; step4: Install project dependencies using pip; step5: (Optional) Download and unzip pretrained models for visualization; step6: (Optional) Run interactive visualization locally with a GUI display; step7: Train the model using the provided script", "executed_cmds": "conda create -n BodyGen python=3.9 mesalib glew glfw -c conda-forge -y;conda activate BodyGen;sudo apt-get update && sudo apt-get install -y wget tar libosmesa6-dev libglew-dev libgl1-mesa-glx libglfw3 patchelf cmake;sudo ln -s /usr/lib/x86_64-linux-gnu/libGL.so.1 /usr/lib/x86_64-linux-gnu/libGL.so;wget -c \"https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\";mkdir -p $USER_DIR/.mujoco;cp mujoco210-linux-x86_64.tar.gz $USER_DIR/mujoco.tar.gz;rm mujoco210-linux-x86_64.tar.gz;tar -zxvf $USER_DIR/mujoco.tar.gz -C $USER_DIR/.mujoco;echo \"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$USER_DIR/.mujoco/mujoco210/bin\" >> ~/.bashrc;echo \"export MUJOCO_PY_MUJOCO_PATH=$USER_DIR/.mujoco/mujoco210\" >> ~/.bashrc;source ~/.bashrc", "cmd_prefix": "export", "cmd_postfix": "OMP_NUM_THREADS=1", "target_cmd": "export OMP_NUM_THREADS=1"}
{"uuid": "38a363fd-b91c-45f6-81c2-968467a91a4f", "execution_plan": "step1: Create a conda environment with Python 3.9 and necessary libraries; step2: Install MuJoCo Simulator and mujoco-py dependencies; step3: Set environment variables to avoid multiprocess issues; step4: Install project dependencies using pip; step5: (Optional) Download and unzip pretrained models for visualization; step6: (Optional) Run interactive visualization locally with a GUI display; step7: Train the model using the provided script", "executed_cmds": "conda create -n BodyGen python=3.9 mesalib glew glfw -c conda-forge -y;conda activate BodyGen;sudo apt-get update && sudo apt-get install -y wget tar libosmesa6-dev libglew-dev libgl1-mesa-glx libglfw3 patchelf cmake;sudo ln -s /usr/lib/x86_64-linux-gnu/libGL.so.1 /usr/lib/x86_64-linux-gnu/libGL.so;wget -c \"https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\";mkdir -p $USER_DIR/.mujoco;cp mujoco210-linux-x86_64.tar.gz $USER_DIR/mujoco.tar.gz;rm mujoco210-linux-x86_64.tar.gz;tar -zxvf $USER_DIR/mujoco.tar.gz -C $USER_DIR/.mujoco;echo \"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$USER_DIR/.mujoco/mujoco210/bin\" >> ~/.bashrc;echo \"export MUJOCO_PY_MUJOCO_PATH=$USER_DIR/.mujoco/mujoco210\" >> ~/.bashrc;source ~/.bashrc;export OMP_NUM_THREADS=1", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "9f1ee4cb-e770-4948-a2ee-880c134dded2", "execution_plan": "step1: Create a conda environment with Python 3.9 and necessary libraries; step2: Install MuJoCo Simulator and mujoco-py dependencies; step3: Set environment variables to avoid multiprocess issues; step4: Install project dependencies using pip; step5: (Optional) Download and unzip pretrained models for visualization; step6: (Optional) Run interactive visualization locally with a GUI display; step7: Train the model using the provided script", "executed_cmds": "conda create -n BodyGen python=3.9 mesalib glew glfw -c conda-forge -y;conda activate BodyGen;sudo apt-get update && sudo apt-get install -y wget tar libosmesa6-dev libglew-dev libgl1-mesa-glx libglfw3 patchelf cmake;sudo ln -s /usr/lib/x86_64-linux-gnu/libGL.so.1 /usr/lib/x86_64-linux-gnu/libGL.so;wget -c \"https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\";mkdir -p $USER_DIR/.mujoco;cp mujoco210-linux-x86_64.tar.gz $USER_DIR/mujoco.tar.gz;rm mujoco210-linux-x86_64.tar.gz;tar -zxvf $USER_DIR/mujoco.tar.gz -C $USER_DIR/.mujoco;echo \"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$USER_DIR/.mujoco/mujoco210/bin\" >> ~/.bashrc;echo \"export MUJOCO_PY_MUJOCO_PATH=$USER_DIR/.mujoco/mujoco210\" >> ~/.bashrc;source ~/.bashrc;export OMP_NUM_THREADS=1;pip install -r requirements.txt;unzip pretrained_models.zip;python design_opt/eval.py --train_dir <path_of_model_folder>", "cmd_prefix": "cd", "cmd_postfix": "BodyGen", "target_cmd": "cd BodyGen"}
{"uuid": "6d730f3a-548f-436f-848c-025e49947260", "execution_plan": "step1: Install Python 3.11 and set up a virtual environment to manage dependencies; step2: Clone the repository from GitHub and navigate into the project directory; step3: Install the required packages listed in requirements.txt; step4: Run the main script with the desired dataset name and any additional command-line options; step5: Deactivate the virtual environment when done", "executed_cmds": "python3 -m pip install --user virtualenv", "cmd_prefix": "python3 -m", "cmd_postfix": "venv venv", "target_cmd": "python3 -m venv venv"}
{"uuid": "fb7bdf1f-c105-4190-bbe8-7afc2f0bc491", "execution_plan": "step1: Install Python 3.11 and set up a virtual environment to manage dependencies; step2: Clone the repository from GitHub and navigate into the project directory; step3: Install the required packages listed in requirements.txt; step4: Run the main script with the desired dataset name and any additional command-line options; step5: Deactivate the virtual environment when done", "executed_cmds": "python3 -m pip install --user virtualenv;python3 -m venv venv", "cmd_prefix": "source", "cmd_postfix": "venv/bin/activate", "target_cmd": "source venv/bin/activate"}
{"uuid": "3c5293ae-313a-4a66-af85-ec52b6683431", "execution_plan": "step1: Install Python 3.11 and set up a virtual environment to manage dependencies; step2: Clone the repository from GitHub and navigate into the project directory; step3: Install the required packages listed in requirements.txt; step4: Run the main script with the desired dataset name and any additional command-line options; step5: Deactivate the virtual environment when done", "executed_cmds": "python3 -m pip install --user virtualenv;python3 -m venv venv;source venv/bin/activate;git clone https://github.com/hanxiao0607/AERCA.git my-project", "cmd_prefix": "cd", "cmd_postfix": "my-project", "target_cmd": "cd my-project"}
{"uuid": "c5a0f6d7-723f-43a2-813a-ad8d63e05723", "execution_plan": "step1: Install Python 3.11 and set up a virtual environment to manage dependencies; step2: Clone the repository from GitHub and navigate into the project directory; step3: Install the required packages listed in requirements.txt; step4: Run the main script with the desired dataset name and any additional command-line options; step5: Deactivate the virtual environment when done", "executed_cmds": "python3 -m pip install --user virtualenv;python3 -m venv venv;source venv/bin/activate;git clone https://github.com/hanxiao0607/AERCA.git my-project;cd my-project", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "c9e2890d-3326-4482-8f9a-08af1d71dca9", "execution_plan": "step1: Install the necessary dependencies for the project; step2: Configure the project by modifying the train_config.yaml file to set hyperparameters and training details; step3: Run the framework to train the trigger generator and perform the attack", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "run.py", "target_cmd": "python run.py"}
{"uuid": "b2fbf58d-d464-4cd5-9a17-905c491b80c9", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the model path and understand the data provided in the repository; step3: Run the closed model setting to analyze radioactive outputs with deduplication; step4: Run the closed model setting to generate outputs from a non-radioactive model and compute detection test; step5: Run the open model setting to test reading mode with deduplication; step6: Compute a filter on the scored tokens if needed", "executed_cmds": "conda create -n \"radioactive_watermark\" python=3.8;conda activate radioactive_watermark", "cmd_prefix": "conda install pytorch pytorch-cuda=11.7", "cmd_postfix": "-c pytorch -c nvidia", "target_cmd": "conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia"}
{"uuid": "a3dc6f59-f436-45a0-af32-f70e28a7f456", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the model path and understand the data provided in the repository; step3: Run the closed model setting to analyze radioactive outputs with deduplication; step4: Run the closed model setting to generate outputs from a non-radioactive model and compute detection test; step5: Run the open model setting to test reading mode with deduplication; step6: Compute a filter on the scored tokens if needed", "executed_cmds": "conda create -n \"radioactive_watermark\" python=3.8;conda activate radioactive_watermark;conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "6e10f04f-50de-49b4-8a3e-734bdfa80f02", "execution_plan": "step1: Clone the repository to your local directory; step2: Set up the environment with the specified Python and library versions; step3: Prepare the datasets by creating a folder and downloading the required datasets; step4: Train the model on the desired dataset using the provided configuration files", "executed_cmds": "git clone https://github.com/fwu11/MACIL.git", "cmd_prefix": "cd", "cmd_postfix": "MACIL", "target_cmd": "cd MACIL"}
{"uuid": "070a13a0-bc18-4ee3-b0bf-b5b2b74c4f13", "execution_plan": "step1: Clone the repository and set up the environment by installing required dependencies; step2: Reproduce DeLLMa results by running the provided script; step3: Run baseline methods by querying GPT-4 for specified agents and baseline modes; step4: Evaluate the performance of baseline methods; step5: Run DeLLMa-Naive by querying GPT-4 with specified agent and sample size; step6: Run DeLLMa-{Pairs, Top1} by querying GPT-4 with specified agent, sample size, and overlap percentage; step7: Evaluate DeLLMa agents with specified parameters including regularization strength, softmax mode, and temperature", "executed_cmds": "pip install -e .", "cmd_prefix": "bash", "cmd_postfix": "./results.sh", "target_cmd": "bash ./results.sh"}
{"uuid": "1e49a2d6-990b-466b-a250-d1f0f6e379d4", "execution_plan": "step1: Install the required packages for the project; step2: Prepare the data by downloading the ImageNet dataset and organizing it in the specified directory structure; step3: Train the model using Supervised Learning (SL); step4: Train the model using FixMatch; step5: Train the model using SA-FixMatch by resuming from a FixMatch checkpoint; step6: Evaluate the trained SA-FixMatch model", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "mkdir", "cmd_postfix": "data", "target_cmd": "mkdir data"}
{"uuid": "d60e6cad-7927-463a-92f8-d5956ab01c1b", "execution_plan": "step1: Install the required dependencies including PyTorch, tinycudann, torchkbnufft, SimpleITK, tqdm, numpy, and other dependencies; step2: Navigate to the project directory where the Moner repository is located; step3: Train the Moner model from scratch using the provided training script; step4: Evaluate the results by running the provided Jupyter notebook to get quantitative results of the reconstructed MR images and estimated motion parameters; step5: View the NIFTI files (`.nii`) using ITK-SNAP software for visualization", "executed_cmds": "pip install PyTorch tinycudann torchkbnufft SimpleITK tqdm numpy", "cmd_prefix": "python", "cmd_postfix": "main.py", "target_cmd": "python main.py"}
{"uuid": "ef994b08-0093-4451-bdef-a336bf82fed3", "execution_plan": "step1: Clone the repository to get the source code; step2: Set up the training environment using conda and install required dependencies; step3: Download the necessary datasets by running the corresponding scripts in the dataset directory; step4: Activate the training environment before running experiments; step5: Run experiments using the provided shell script or customize the training script with specific arguments; step6: Activate the evaluation environment before running evaluations; step7: Evaluate a trained model using the provided evaluation script with specific hyperparameters", "executed_cmds": "git clone https://github.com/snumprlab/budgeted-cl.git;conda create -n budgeted_cl python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate budgeted_cl", "target_cmd": "conda activate budgeted_cl"}
{"uuid": "e3a08764-e7b9-4365-9192-89542c7b638a", "execution_plan": "step1: Clone the repository to get the source code; step2: Set up the training environment using conda and install required dependencies; step3: Download the necessary datasets by running the corresponding scripts in the dataset directory; step4: Activate the training environment before running experiments; step5: Run experiments using the provided shell script or customize the training script with specific arguments; step6: Activate the evaluation environment before running evaluations; step7: Evaluate a trained model using the provided evaluation script with specific hyperparameters", "executed_cmds": "git clone https://github.com/snumprlab/budgeted-cl.git;conda create -n budgeted_cl python=3.10;conda activate budgeted_cl", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "8dc553a8-a8bf-41ca-989e-3fe27dc25f8d", "execution_plan": "step1: Clone the repository to get the source code; step2: Set up the training environment using conda and install required dependencies; step3: Download the necessary datasets by running the corresponding scripts in the dataset directory; step4: Activate the training environment before running experiments; step5: Run experiments using the provided shell script or customize the training script with specific arguments; step6: Activate the evaluation environment before running evaluations; step7: Evaluate a trained model using the provided evaluation script with specific hyperparameters", "executed_cmds": "git clone https://github.com/snumprlab/budgeted-cl.git;conda create -n budgeted_cl python=3.10;conda activate budgeted_cl;pip install -r requirements.txt", "cmd_prefix": "conda", "cmd_postfix": "deactivate", "target_cmd": "conda deactivate"}
{"uuid": "f823ed2c-47cc-46fd-96c8-b095658a23d8", "execution_plan": "step1: Clone the repository to get the source code; step2: Set up the training environment using conda and install required dependencies; step3: Download the necessary datasets by running the corresponding scripts in the dataset directory; step4: Activate the training environment before running experiments; step5: Run experiments using the provided shell script or customize the training script with specific arguments; step6: Activate the evaluation environment before running evaluations; step7: Evaluate a trained model using the provided evaluation script with specific hyperparameters", "executed_cmds": "git clone https://github.com/snumprlab/budgeted-cl.git;conda create -n budgeted_cl python=3.10;conda activate budgeted_cl;pip install -r requirements.txt;conda deactivate", "cmd_prefix": "conda", "cmd_postfix": "activate budgeted_cl", "target_cmd": "conda activate budgeted_cl"}
{"uuid": "c83d7ebc-4bb2-4e55-88d4-68d3c0057ed0", "execution_plan": "step1: Clone the repository to get the source code; step2: Set up the training environment using conda and install required dependencies; step3: Download the necessary datasets by running the corresponding scripts in the dataset directory; step4: Activate the training environment before running experiments; step5: Run experiments using the provided shell script or customize the training script with specific arguments; step6: Activate the evaluation environment before running evaluations; step7: Evaluate a trained model using the provided evaluation script with specific hyperparameters", "executed_cmds": "git clone https://github.com/snumprlab/budgeted-cl.git;conda create -n budgeted_cl python=3.10;conda activate budgeted_cl;pip install -r requirements.txt;conda deactivate;conda activate budgeted_cl", "cmd_prefix": "bash", "cmd_postfix": "ex.sh", "target_cmd": "bash ex.sh"}
{"uuid": "d1b17c4c-2f1c-4a7b-96fa-c5938a81cc2a", "execution_plan": "step1: Clone the repository to get the source code; step2: Set up the training environment using conda and install required dependencies; step3: Download the necessary datasets by running the corresponding scripts in the dataset directory; step4: Activate the training environment before running experiments; step5: Run experiments using the provided shell script or customize the training script with specific arguments; step6: Activate the evaluation environment before running evaluations; step7: Evaluate a trained model using the provided evaluation script with specific hyperparameters", "executed_cmds": "git clone https://github.com/snumprlab/budgeted-cl.git;conda create -n budgeted_cl python=3.10;conda activate budgeted_cl;pip install -r requirements.txt;conda deactivate;conda activate budgeted_cl;bash ex.sh;python models/train/train_seq2seq.py --incremental_setup <incremental_setup> --mode <mode> --stream_seed <stream_seed> --dout <path_to_save_weight>", "cmd_prefix": "conda", "cmd_postfix": "deactivate", "target_cmd": "conda deactivate"}
{"uuid": "4957376c-d90f-485a-837a-0a993fb9c369", "execution_plan": "step1: Clone the repository to get the source code; step2: Set up the training environment using conda and install required dependencies; step3: Download the necessary datasets by running the corresponding scripts in the dataset directory; step4: Activate the training environment before running experiments; step5: Run experiments using the provided shell script or customize the training script with specific arguments; step6: Activate the evaluation environment before running evaluations; step7: Evaluate a trained model using the provided evaluation script with specific hyperparameters", "executed_cmds": "git clone https://github.com/snumprlab/budgeted-cl.git;conda create -n budgeted_cl python=3.10;conda activate budgeted_cl;pip install -r requirements.txt;conda deactivate;conda activate budgeted_cl;bash ex.sh;python models/train/train_seq2seq.py --incremental_setup <incremental_setup> --mode <mode> --stream_seed <stream_seed> --dout <path_to_save_weight>;conda deactivate", "cmd_prefix": "conda", "cmd_postfix": "activate budgeted_cl", "target_cmd": "conda activate budgeted_cl"}
{"uuid": "658c27e8-0e60-4c27-a9b1-272d00704ed2", "execution_plan": "step1: Set up the Python environment with the specified dependencies; step2: Download or clone the repository to your local machine; step3: Navigate to the repository directory; step4: Run the bash script corresponding to the dataset you want to reproduce results for; step5: Cite the paper if SGRL is used in your work", "executed_cmds": "pip install python==3.9.7 scikit-learn==1.4.2 scipy==1.13.0 networkx==3.2 numpy==1.26.4 torch==2.1.0 torch_geometric==2.5.3 tqdm==4.66.2", "cmd_prefix": "bash", "cmd_postfix": "run_{dataset_name}", "target_cmd": "bash run_{dataset_name}"}
{"uuid": "45c23867-c636-492a-938f-32323c99a165", "execution_plan": "step1: Set up the Conda environment and install necessary dependencies; step2: Clone and install the FastMoE library; step3: Apply for and download the ADNI dataset from the official website; step4: Preprocess the downloaded ADNI dataset files; step5: Customize the dataset and dataloader for other datasets if needed; step6: Run the Flex-MoE model with specified parameters; step7: Execute the provided script for IGBC results", "executed_cmds": "conda create -n flex-moe python=3.10 -y", "cmd_prefix": "conda", "cmd_postfix": "activate flex-moe", "target_cmd": "conda activate flex-moe"}
{"uuid": "f93aa598-cd69-4334-99d1-69d6ab1a2c08", "execution_plan": "step1: Set up the Conda environment and install necessary dependencies; step2: Clone and install the FastMoE library; step3: Apply for and download the ADNI dataset from the official website; step4: Preprocess the downloaded ADNI dataset files; step5: Customize the dataset and dataloader for other datasets if needed; step6: Run the Flex-MoE model with specified parameters; step7: Execute the provided script for IGBC results", "executed_cmds": "conda create -n flex-moe python=3.10 -y;conda activate flex-moe;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia -y;conda install nvidia::cudatoolkit conda-forge::cudatoolkit-dev -y;conda install -c conda-forge gxx=10.4 -y;export LD_LIBRARY_PATH=/path_to_anaconda3/flex-moe/lib:$LD_LIBRARY_PATH;export CUDA_HOME=/path_to_anaconda3/envs/flex-moe;conda install dm-tree scikit-learn tqdm pandas -y;conda install -c conda-forge scanpy nibabel -y;git clone https://github.com/laekov/fastmoe.git", "cmd_prefix": "cd", "cmd_postfix": "fastmoe", "target_cmd": "cd fastmoe"}
{"uuid": "e17b19aa-76f3-4c6a-b81e-c0e9e234f230", "execution_plan": "step1: Set up the Conda environment and install necessary dependencies; step2: Clone and install the FastMoE library; step3: Apply for and download the ADNI dataset from the official website; step4: Preprocess the downloaded ADNI dataset files; step5: Customize the dataset and dataloader for other datasets if needed; step6: Run the Flex-MoE model with specified parameters; step7: Execute the provided script for IGBC results", "executed_cmds": "conda create -n flex-moe python=3.10 -y;conda activate flex-moe;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia -y;conda install nvidia::cudatoolkit conda-forge::cudatoolkit-dev -y;conda install -c conda-forge gxx=10.4 -y;export LD_LIBRARY_PATH=/path_to_anaconda3/flex-moe/lib:$LD_LIBRARY_PATH;export CUDA_HOME=/path_to_anaconda3/envs/flex-moe;conda install dm-tree scikit-learn tqdm pandas -y;conda install -c conda-forge scanpy nibabel -y;git clone https://github.com/laekov/fastmoe.git;cd fastmoe", "cmd_prefix": "USE_NCCL=0 python", "cmd_postfix": "setup.py install", "target_cmd": "USE_NCCL=0 python setup.py install"}
{"uuid": "b28064a9-4c1d-4f4e-9314-8609a68262b8", "execution_plan": "step1: Set up the Conda environment and install necessary dependencies; step2: Clone and install the FastMoE library; step3: Apply for and download the ADNI dataset from the official website; step4: Preprocess the downloaded ADNI dataset files; step5: Customize the dataset and dataloader for other datasets if needed; step6: Run the Flex-MoE model with specified parameters; step7: Execute the provided script for IGBC results", "executed_cmds": "conda create -n flex-moe python=3.10 -y;conda activate flex-moe;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia -y;conda install nvidia::cudatoolkit conda-forge::cudatoolkit-dev -y;conda install -c conda-forge gxx=10.4 -y;export LD_LIBRARY_PATH=/path_to_anaconda3/flex-moe/lib:$LD_LIBRARY_PATH;export CUDA_HOME=/path_to_anaconda3/envs/flex-moe;conda install dm-tree scikit-learn tqdm pandas -y;conda install -c conda-forge scanpy nibabel -y;git clone https://github.com/laekov/fastmoe.git;cd fastmoe;USE_NCCL=0 python setup.py install", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "e6ade263-6f2e-4d29-8ee9-468a9f0e8d55", "execution_plan": "step1: Set up the Conda environment and install necessary dependencies; step2: Clone and install the FastMoE library; step3: Apply for and download the ADNI dataset from the official website; step4: Preprocess the downloaded ADNI dataset files; step5: Customize the dataset and dataloader for other datasets if needed; step6: Run the Flex-MoE model with specified parameters; step7: Execute the provided script for IGBC results", "executed_cmds": "conda create -n flex-moe python=3.10 -y;conda activate flex-moe;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia -y;conda install nvidia::cudatoolkit conda-forge::cudatoolkit-dev -y;conda install -c conda-forge gxx=10.4 -y;export LD_LIBRARY_PATH=/path_to_anaconda3/flex-moe/lib:$LD_LIBRARY_PATH;export CUDA_HOME=/path_to_anaconda3/envs/flex-moe;conda install dm-tree scikit-learn tqdm pandas -y;conda install -c conda-forge scanpy nibabel -y;git clone https://github.com/laekov/fastmoe.git;cd fastmoe;USE_NCCL=0 python setup.py install;cd ..;python main.py --data adni --modality IG --lr 1e-3 --num_experts 16 --num_layers_fus 1 --top_k 4 --train_epochs 50 --warm_up_epochs 5 --hidden_dim 128 --batch_size 16 --n_runs 3 --device 0", "cmd_prefix": "sh", "cmd_postfix": "run_adni_igcb.sh", "target_cmd": "sh run_adni_igcb.sh"}
{"uuid": "dbdc7023-ca1e-404f-a5e3-c363eef49483", "execution_plan": "step1: Set up the environment by following the COSeg installation guide and ensure compatibility with RTX 3090 GPUs and GCC 6.3.0; step2: Prepare the pretraining stage data by downloading and unzipping the ScanNet 3D dataset and 2D features, then link them to the pretraining/data folder; step3: Prepare the few-shot stage data by either downloading preprocessed datasets or manually preprocessing them using COSeg instructions; step4: Pretrain the backbone and IF head by either downloading pretrained weights or training from scratch using the provided script; step5: Configure and run the meta-learning stage for few-shot tasks, adjusting parameters like n_way and k_shot as needed; step6: Evaluate the trained model and optionally enable visualization using W&B; step7: Visualize results by following the COSeg visualization guide", "executed_cmds": "wget https://cvg-data.inf.ethz.ch/openscene/data/scannet_processed/scannet_3d.zip", "cmd_prefix": "unzip", "cmd_postfix": "scannet_3d.zip", "target_cmd": "unzip scannet_3d.zip"}
{"uuid": "5bac2490-d3f6-4e23-a635-14d58230d666", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Install Isaac GYM and integrate it with the Conda environment; step3: Run CLoSD for multi-task character control; step4: Run CLoSD for sequence of tasks; step5: Run CLoSD for text-to-motion; step6: Evaluate multi-task success rate; step7: Evaluate text-to-motion performance; step8: Visualize motions using Blender; step9: Extract SMPL parameters from recorded motions; step10: Train your own CLoSD tracking controller; step11: Fine-tune CLoSD for multi-task; step12: Fine-tune CLoSD for text-to-motion; step13: Generate motion with the stand-alone DiP; step14: Evaluate stand-alone DiP performance; step15: Train your own DiP model", "executed_cmds": "conda create -n closd python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate closd", "target_cmd": "conda activate closd"}
{"uuid": "5cd2b539-0987-412f-b22a-ac40dcad52ee", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Install Isaac GYM and integrate it with the Conda environment; step3: Run CLoSD for multi-task character control; step4: Run CLoSD for sequence of tasks; step5: Run CLoSD for text-to-motion; step6: Evaluate multi-task success rate; step7: Evaluate text-to-motion performance; step8: Visualize motions using Blender; step9: Extract SMPL parameters from recorded motions; step10: Train your own CLoSD tracking controller; step11: Fine-tune CLoSD for multi-task; step12: Fine-tune CLoSD for text-to-motion; step13: Generate motion with the stand-alone DiP; step14: Evaluate stand-alone DiP performance; step15: Train your own DiP model", "executed_cmds": "conda create -n closd python=3.8;conda activate closd", "cmd_prefix": "pip install", "cmd_postfix": "-r requirement.txt", "target_cmd": "pip install -r requirement.txt"}
{"uuid": "ae756c03-e6a0-4c85-bcdb-247eb951d699", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Install Isaac GYM and integrate it with the Conda environment; step3: Run CLoSD for multi-task character control; step4: Run CLoSD for sequence of tasks; step5: Run CLoSD for text-to-motion; step6: Evaluate multi-task success rate; step7: Evaluate text-to-motion performance; step8: Visualize motions using Blender; step9: Extract SMPL parameters from recorded motions; step10: Train your own CLoSD tracking controller; step11: Fine-tune CLoSD for multi-task; step12: Fine-tune CLoSD for text-to-motion; step13: Generate motion with the stand-alone DiP; step14: Evaluate stand-alone DiP performance; step15: Train your own DiP model", "executed_cmds": "conda create -n closd python=3.8;conda activate closd;pip install -r requirement.txt;python -m spacy download en_core_web_sm", "cmd_prefix": "conda", "cmd_postfix": "activate closd", "target_cmd": "conda activate closd"}
{"uuid": "c86005a9-b512-42e8-ba6d-9b2b3502a06b", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Install Isaac GYM and integrate it with the Conda environment; step3: Run CLoSD for multi-task character control; step4: Run CLoSD for sequence of tasks; step5: Run CLoSD for text-to-motion; step6: Evaluate multi-task success rate; step7: Evaluate text-to-motion performance; step8: Visualize motions using Blender; step9: Extract SMPL parameters from recorded motions; step10: Train your own CLoSD tracking controller; step11: Fine-tune CLoSD for multi-task; step12: Fine-tune CLoSD for text-to-motion; step13: Generate motion with the stand-alone DiP; step14: Evaluate stand-alone DiP performance; step15: Train your own DiP model", "executed_cmds": "conda create -n closd python=3.8;conda activate closd;pip install -r requirement.txt;python -m spacy download en_core_web_sm;conda activate closd;cd <ISSAC_GYM_DIR>/python", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "f390b0da-21bf-4ead-a4e1-5178f882ee6d", "execution_plan": "step1: Create a Conda environment named 'VDPO' with Python 3.10; step2: Activate the Conda environment 'VDPO'; step3: Install the required dependencies listed in 'requirement.yaml'; step4: Install the 'gymnasium[mujoco]' package; step5: Run the VDPO algorithm with the specified environment ('Ant-v4') and delay (5)", "executed_cmds": "conda create -n VDPO python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate VDPO", "target_cmd": "conda activate VDPO"}
{"uuid": "659caedd-314d-425c-8f3b-488c1f403e7c", "execution_plan": "step1: Create a Conda environment named 'VDPO' with Python 3.10; step2: Activate the Conda environment 'VDPO'; step3: Install the required dependencies listed in 'requirement.yaml'; step4: Install the 'gymnasium[mujoco]' package; step5: Run the VDPO algorithm with the specified environment ('Ant-v4') and delay (5)", "executed_cmds": "conda create -n VDPO python=3.10;conda activate VDPO", "cmd_prefix": "pip install", "cmd_postfix": "-r requirement.yaml", "target_cmd": "pip install -r requirement.yaml"}
{"uuid": "b55cb152-2576-413a-84c0-eff9e6b934d6", "execution_plan": "step1: Set up the dependencies and environment by installing CUDA 11.4 and PyTorch 1.12, and ensure the requirements match those of DDBM; step2: Download and organize the datasets (FFHQ, CelebA, CelebA-HQ, LSUN) in LMDB format and set the dataset path in args.sh; step3: Train the DBAE model by configuring the dataset, schedule type, and encoder type, then execute the training script; step4: Evaluate the model's regression capability by downloading the LFW dataset and running the regression inference script; step5: Evaluate the model's classification capability by running the classification inference script; step6: Perform reconstruction tasks by configuring the model path, sampling parameters, and running the reconstruction script; step7: Evaluate the reconstruction results by running the evaluation script; step8: Perform interpolation between two images using the provided script; step9: Perform attribute manipulation by training or downloading a linear classifier and running the manipulation script", "executed_cmds": "bash train_dbae.sh ffhq vp true", "cmd_prefix": "bash train_dbae.sh", "cmd_postfix": "celeba vp true", "target_cmd": "bash train_dbae.sh celeba vp true"}
{"uuid": "33bcb4e3-3e91-4476-b6c6-fa41f7635d55", "execution_plan": "step1: Create a conda environment named 'disco' with Python 3.8 and activate it; step2: Install the required dependencies using pip; step3: Download the VisualGenome dataset from the provided link; step4: Prepare the dataset by running the provided Python scripts to filter and construct textual graphs; step5: Train the model using the specified accelerate launch command with the given parameters", "executed_cmds": "conda env create -n disco python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate disco", "target_cmd": "conda activate disco"}
{"uuid": "68ffb188-4252-4d31-a263-e3fafe0a6e38", "execution_plan": "step1: Create a conda environment named 'disco' with Python 3.8 and activate it; step2: Install the required dependencies using pip; step3: Download the VisualGenome dataset from the provided link; step4: Prepare the dataset by running the provided Python scripts to filter and construct textual graphs; step5: Train the model using the specified accelerate launch command with the given parameters", "executed_cmds": "conda env create -n disco python=3.8;conda activate disco", "cmd_prefix": "pip install", "cmd_postfix": "-r requirement.txt", "target_cmd": "pip install -r requirement.txt"}
{"uuid": "40c430b7-85bd-444c-a4de-5e1a888f6ff2", "execution_plan": "step1: Set up a new virtual environment for the project; step2: Install the required dependency packages including PyTorch and other libraries; step3: Prepare the Argoverse 2 Motion Forecasting Dataset by organizing the data directory; step4: Preprocess the dataset to prepare it for training and evaluation; step5: Train the model using the provided training script; step6: Validate the trained model using the evaluation script; step7: Test the model for submission using the evaluation script with the submit flag", "executed_cmds": "conda create -n motion python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate motion", "target_cmd": "conda activate motion"}
{"uuid": "3f6affb8-8b8b-478b-8550-4e1467943dc1", "execution_plan": "step1: Set up a new virtual environment for the project; step2: Install the required dependency packages including PyTorch and other libraries; step3: Prepare the Argoverse 2 Motion Forecasting Dataset by organizing the data directory; step4: Preprocess the dataset to prepare it for training and evaluation; step5: Train the model using the provided training script; step6: Validate the trained model using the evaluation script; step7: Test the model for submission using the evaluation script with the submit flag", "executed_cmds": "conda create -n motion python=3.10;conda activate motion;pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "015ec1d8-5645-4c3c-9a8d-4c98e85c1c3c", "execution_plan": "step1: Set up a new virtual environment for the project; step2: Install the required dependency packages including PyTorch and other libraries; step3: Prepare the Argoverse 2 Motion Forecasting Dataset by organizing the data directory; step4: Preprocess the dataset to prepare it for training and evaluation; step5: Train the model using the provided training script; step6: Validate the trained model using the evaluation script; step7: Test the model for submission using the evaluation script with the submit flag", "executed_cmds": "conda create -n motion python=3.10;conda activate motion;pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118;pip install -r requirements.txt;pip install natten==0.17.3+torch200cu118 -f https://shi-labs.com/natten/wheels", "cmd_prefix": "python preprocess.py", "cmd_postfix": "-d /path/to/data -p", "target_cmd": "python preprocess.py -d /path/to/data -p"}
{"uuid": "b5d59750-70b5-4f71-9ae1-ad5fa4f06619", "execution_plan": "step1: Set up a new virtual environment for the project; step2: Install the required dependency packages including PyTorch and other libraries; step3: Prepare the Argoverse 2 Motion Forecasting Dataset by organizing the data directory; step4: Preprocess the dataset to prepare it for training and evaluation; step5: Train the model using the provided training script; step6: Validate the trained model using the evaluation script; step7: Test the model for submission using the evaluation script with the submit flag", "executed_cmds": "conda create -n motion python=3.10;conda activate motion;pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118;pip install -r requirements.txt;pip install natten==0.17.3+torch200cu118 -f https://shi-labs.com/natten/wheels;python preprocess.py -d /path/to/data -p", "cmd_prefix": "python", "cmd_postfix": "train.py", "target_cmd": "python train.py"}
{"uuid": "3189a54e-abb2-45a1-88f5-7cc2276349f4", "execution_plan": "step1: Install Docker and set it up for non-root users if on Linux; step2: Install poetry to manage Python dependencies; step3: Clone the repository and navigate into the project directory; step4: Install project dependencies using poetry; step5: Create a `.env` file by copying the example file; step6: Activate the poetry environment; step7: Use the `run` script to build, push, and evaluate the images; step8: Run the evaluation using the Inspect framework with a specified model; step9: Optionally generate prompts for specific CVEs or run manual exploits", "executed_cmds": "pip install poetry;git clone git@github.com:uiuc-kang-lab/cve-bench.git", "cmd_prefix": "cd", "cmd_postfix": "cve-bench", "target_cmd": "cd cve-bench"}
{"uuid": "53eb2a07-7c6a-4c18-9201-10d45e08fbc0", "execution_plan": "step1: Install Docker and set it up for non-root users if on Linux; step2: Install poetry to manage Python dependencies; step3: Clone the repository and navigate into the project directory; step4: Install project dependencies using poetry; step5: Create a `.env` file by copying the example file; step6: Activate the poetry environment; step7: Use the `run` script to build, push, and evaluate the images; step8: Run the evaluation using the Inspect framework with a specified model; step9: Optionally generate prompts for specific CVEs or run manual exploits", "executed_cmds": "pip install poetry;git clone git@github.com:uiuc-kang-lab/cve-bench.git;cd cve-bench", "cmd_prefix": "poetry", "cmd_postfix": "install", "target_cmd": "poetry install"}
{"uuid": "6b69bf10-cfc2-4980-9f5d-e958f4c0080b", "execution_plan": "step1: Install Docker and set it up for non-root users if on Linux; step2: Install poetry to manage Python dependencies; step3: Clone the repository and navigate into the project directory; step4: Install project dependencies using poetry; step5: Create a `.env` file by copying the example file; step6: Activate the poetry environment; step7: Use the `run` script to build, push, and evaluate the images; step8: Run the evaluation using the Inspect framework with a specified model; step9: Optionally generate prompts for specific CVEs or run manual exploits", "executed_cmds": "pip install poetry;git clone git@github.com:uiuc-kang-lab/cve-bench.git;cd cve-bench;poetry install", "cmd_prefix": "cp", "cmd_postfix": ".env.example .env", "target_cmd": "cp .env.example .env"}
{"uuid": "316284ee-9714-4a4f-9020-6960fc1f1c88", "execution_plan": "step1: Install Docker and set it up for non-root users if on Linux; step2: Install poetry to manage Python dependencies; step3: Clone the repository and navigate into the project directory; step4: Install project dependencies using poetry; step5: Create a `.env` file by copying the example file; step6: Activate the poetry environment; step7: Use the `run` script to build, push, and evaluate the images; step8: Run the evaluation using the Inspect framework with a specified model; step9: Optionally generate prompts for specific CVEs or run manual exploits", "executed_cmds": "pip install poetry;git clone git@github.com:uiuc-kang-lab/cve-bench.git;cd cve-bench;poetry install;cp .env.example .env", "cmd_prefix": "poetry", "cmd_postfix": "env activate", "target_cmd": "poetry env activate"}
{"uuid": "da78ecba-d08f-4ab0-a7f4-e5fb622da49d", "execution_plan": "step1: Install the required packages and set up the `rpo` package; step2: Download the LLaMA-2-7B-Chat model and configure the paths in the experiment config files; step3: Add jailbreak text files to the `experiments/jailbreaks` folder; step4: Run the experiments to reproduce RPO results on AdvBench; step5: Evaluate the robustness of generated suffixes on jailbreaks using the provided script", "executed_cmds": "pip install livelossplot", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "3145be8a-f1cd-46d4-a278-6b9cd1913256", "execution_plan": "step1: Install the required packages and set up the `rpo` package; step2: Download the LLaMA-2-7B-Chat model and configure the paths in the experiment config files; step3: Add jailbreak text files to the `experiments/jailbreaks` folder; step4: Run the experiments to reproduce RPO results on AdvBench; step5: Evaluate the robustness of generated suffixes on jailbreaks using the provided script", "executed_cmds": "pip install livelossplot;pip install -e .", "cmd_prefix": "cd", "cmd_postfix": "launch_scripts", "target_cmd": "cd launch_scripts"}
{"uuid": "8acefbbb-a03c-4af1-9f51-6c93aad0e879", "execution_plan": "step1: Install CUDA Toolkit 11.3 or later and ensure gcc version is <= 10 for compatibility with the project; step2: Create and activate a Python environment using the provided conda environment file; step3: Install additional Python dependencies listed in requirements.txt; step4: Download pre-trained models from the provided OneDrive link and place them in the specified folder; step5: Generate 3D hairstyles using the pre-trained model with specified seeds and truncation value; step6: Fit Perm parameters to a given 3D hairstyle using the projector script; step7: Interpolate between two hairstyles using the style_mixing script; step8: Download and process the Hair20k dataset for training or evaluation; step9: Train the StyleGAN2 backbone, U-Net superresolution module, or VAE backbone using the provided scripts; step10: Evaluate the hair reconstruction quality using the provided metrics; step11: Render generated hairstyles using Blender and the Hair Tool addon; step12: Filter and convert hairstyles for clean rendering in Blender", "executed_cmds": "conda env create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate perm", "target_cmd": "conda activate perm"}
{"uuid": "93be1246-5c9e-4cf6-a60a-6b64f14b1823", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Download and preprocess the RealEstate10K dataset; step3: Download the model checkpoints from HuggingFace; step4: Generate a Wandb API key file for logging during training; step5: Train the LVSM model (either Decoder-Only or Encoder-Decoder variant); step6: Run inference using the trained model to generate results and metrics", "executed_cmds": "conda create -n LVSM python=3.11", "cmd_prefix": "conda", "cmd_postfix": "activate LVSM", "target_cmd": "conda activate LVSM"}
{"uuid": "c312f2a7-1089-4cf3-af58-79a640a9b3cd", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Download and preprocess the RealEstate10K dataset; step3: Download the model checkpoints from HuggingFace; step4: Generate a Wandb API key file for logging during training; step5: Train the LVSM model (either Decoder-Only or Encoder-Decoder variant); step6: Run inference using the trained model to generate results and metrics", "executed_cmds": "conda create -n LVSM python=3.11;conda activate LVSM", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "0ee5ba83-40ab-4f79-99fc-39d83db587a4", "execution_plan": "step1: Create a virtual environment using conda and activate it; step2: Install necessary libraries and dependencies; step3: Install graphviz through conda if necessary; step4: Update CUDA if needed for JAX compatibility; step5: Run the PPO script with SYMPOL on a specified environment (e.g., CartPole-v1) using optimized hyperparameters; step6: Reproduce experiments from the paper using provided commands in `commands_to_run.txt`; step7: Perform hyperparameter optimization with Optuna if desired; step8: Track experiments using wandb for logging results", "executed_cmds": "conda create -n sympol python=3.11.4", "cmd_prefix": "conda", "cmd_postfix": "activate sympol", "target_cmd": "conda activate sympol"}
{"uuid": "4ffd05eb-cd5e-4ca5-82e4-1569caa25bbc", "execution_plan": "step1: Create a virtual environment using conda and activate it; step2: Install necessary libraries and dependencies; step3: Install graphviz through conda if necessary; step4: Update CUDA if needed for JAX compatibility; step5: Run the PPO script with SYMPOL on a specified environment (e.g., CartPole-v1) using optimized hyperparameters; step6: Reproduce experiments from the paper using provided commands in `commands_to_run.txt`; step7: Perform hyperparameter optimization with Optuna if desired; step8: Track experiments using wandb for logging results", "executed_cmds": "conda create -n sympol python=3.11.4;conda activate sympol", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "89b4e391-ff3c-48a7-bd89-d355806dcfd6", "execution_plan": "step1: Create a virtual environment using conda and activate it; step2: Install necessary libraries and dependencies; step3: Install graphviz through conda if necessary; step4: Update CUDA if needed for JAX compatibility; step5: Run the PPO script with SYMPOL on a specified environment (e.g., CartPole-v1) using optimized hyperparameters; step6: Reproduce experiments from the paper using provided commands in `commands_to_run.txt`; step7: Perform hyperparameter optimization with Optuna if desired; step8: Track experiments using wandb for logging results", "executed_cmds": "conda create -n sympol python=3.11.4;conda activate sympol;pip install -r requirements.txt", "cmd_prefix": "conda", "cmd_postfix": "install graphviz", "target_cmd": "conda install graphviz"}
{"uuid": "9ef7eb06-0931-414e-8e08-d74013b8f5b1", "execution_plan": "step1: Create a virtual environment using conda and activate it; step2: Install necessary libraries and dependencies; step3: Install graphviz through conda if necessary; step4: Update CUDA if needed for JAX compatibility; step5: Run the PPO script with SYMPOL on a specified environment (e.g., CartPole-v1) using optimized hyperparameters; step6: Reproduce experiments from the paper using provided commands in `commands_to_run.txt`; step7: Perform hyperparameter optimization with Optuna if desired; step8: Track experiments using wandb for logging results", "executed_cmds": "conda create -n sympol python=3.11.4;conda activate sympol;pip install -r requirements.txt;conda install graphviz", "cmd_prefix": "conda install", "cmd_postfix": "cuda=12.6 -c nvidia", "target_cmd": "conda install cuda=12.6 -c nvidia"}
{"uuid": "c760389a-37e9-441a-862d-609cb4cc914d", "execution_plan": "step1: Set up the Conda environment with Python 3.8 and install required dependencies; step2: Prepare the dataset by creating a directory and placing the required CSV files in it; step3: Run the training script for a specific task (e.g., electricity dataset) using the provided shell script; step4: Adjust the learning rate if necessary to accelerate convergence (optional)", "executed_cmds": "conda create -n SSCNN python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate SSCNN", "target_cmd": "conda activate SSCNN"}
{"uuid": "01af6dab-2e93-4b39-8aa7-8842f958f945", "execution_plan": "step1: Set up the Conda environment with Python 3.8 and install required dependencies; step2: Prepare the dataset by creating a directory and placing the required CSV files in it; step3: Run the training script for a specific task (e.g., electricity dataset) using the provided shell script; step4: Adjust the learning rate if necessary to accelerate convergence (optional)", "executed_cmds": "conda create -n SSCNN python=3.8;conda activate SSCNN", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "55f4f122-4f5f-4f10-9c64-c98e304c9332", "execution_plan": "step1: Set up the conda environment using the provided environment.yml file; step2: Download and organize the necessary data and model checkpoints from the provided Google Drive link and other specified sources; step3: Download and set up the SMPL-X and SMPL-H body models, converting them to PKL format if necessary; step4: Download and organize the AMASS, BABEL, and HumanML3D datasets for training purposes; step5: Run the interactive online text-conditioned motion generation demo; step6: Run the headless text-conditioned motion composition demo; step7: Run the text-conditioned motion in-betweening demo; step8: Run the human-scene interaction synthesis demo; step9: Run the text-conditioned goal reaching demo; step10: Run the sparse and dense joint trajectory control demo; step11: Preprocess the BABEL dataset for training; step12: Train the Motion Primitive VAE model; step13: Train the Latent Motion Primitive Diffusion Model; step14: Train the Motion Control Policy; step15: Evaluate the text-conditioned temporal motion composition; step16: Evaluate the text-conditioned motion in-betweening; step17: Evaluate the text-conditioned goal reaching", "executed_cmds": "conda env create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate DART", "target_cmd": "conda activate DART"}
{"uuid": "97757ae6-f16d-4bf5-8626-05cf978fefa1", "execution_plan": "step1: Set up the conda environment using the provided environment.yml file; step2: Download and organize the necessary data and model checkpoints from the provided Google Drive link and other specified sources; step3: Download and set up the SMPL-X and SMPL-H body models, converting them to PKL format if necessary; step4: Download and organize the AMASS, BABEL, and HumanML3D datasets for training purposes; step5: Run the interactive online text-conditioned motion generation demo; step6: Run the headless text-conditioned motion composition demo; step7: Run the text-conditioned motion in-betweening demo; step8: Run the human-scene interaction synthesis demo; step9: Run the text-conditioned goal reaching demo; step10: Run the sparse and dense joint trajectory control demo; step11: Preprocess the BABEL dataset for training; step12: Train the Motion Primitive VAE model; step13: Train the Latent Motion Primitive Diffusion Model; step14: Train the Motion Control Policy; step15: Evaluate the text-conditioned temporal motion composition; step16: Evaluate the text-conditioned motion in-betweening; step17: Evaluate the text-conditioned goal reaching", "executed_cmds": "conda env create -f environment.yml;conda activate DART", "cmd_prefix": "source", "cmd_postfix": "./demos/run_demo.sh", "target_cmd": "source ./demos/run_demo.sh"}
{"uuid": "ff849b47-9387-46c5-a09f-6c5b61d30022", "execution_plan": "step1: Set up the conda environment using the provided environment.yml file; step2: Download and organize the necessary data and model checkpoints from the provided Google Drive link and other specified sources; step3: Download and set up the SMPL-X and SMPL-H body models, converting them to PKL format if necessary; step4: Download and organize the AMASS, BABEL, and HumanML3D datasets for training purposes; step5: Run the interactive online text-conditioned motion generation demo; step6: Run the headless text-conditioned motion composition demo; step7: Run the text-conditioned motion in-betweening demo; step8: Run the human-scene interaction synthesis demo; step9: Run the text-conditioned goal reaching demo; step10: Run the sparse and dense joint trajectory control demo; step11: Preprocess the BABEL dataset for training; step12: Train the Motion Primitive VAE model; step13: Train the Latent Motion Primitive Diffusion Model; step14: Train the Motion Control Policy; step15: Evaluate the text-conditioned temporal motion composition; step16: Evaluate the text-conditioned motion in-betweening; step17: Evaluate the text-conditioned goal reaching", "executed_cmds": "conda env create -f environment.yml;conda activate DART;source ./demos/run_demo.sh", "cmd_prefix": "source", "cmd_postfix": "./demos/rollout.sh", "target_cmd": "source ./demos/rollout.sh"}
{"uuid": "4fc674fc-08fc-434e-9a6b-7684df33b5ba", "execution_plan": "step1: Set up the conda environment using the provided environment.yml file; step2: Download and organize the necessary data and model checkpoints from the provided Google Drive link and other specified sources; step3: Download and set up the SMPL-X and SMPL-H body models, converting them to PKL format if necessary; step4: Download and organize the AMASS, BABEL, and HumanML3D datasets for training purposes; step5: Run the interactive online text-conditioned motion generation demo; step6: Run the headless text-conditioned motion composition demo; step7: Run the text-conditioned motion in-betweening demo; step8: Run the human-scene interaction synthesis demo; step9: Run the text-conditioned goal reaching demo; step10: Run the sparse and dense joint trajectory control demo; step11: Preprocess the BABEL dataset for training; step12: Train the Motion Primitive VAE model; step13: Train the Latent Motion Primitive Diffusion Model; step14: Train the Motion Control Policy; step15: Evaluate the text-conditioned temporal motion composition; step16: Evaluate the text-conditioned motion in-betweening; step17: Evaluate the text-conditioned goal reaching", "executed_cmds": "conda env create -f environment.yml;conda activate DART;source ./demos/run_demo.sh;source ./demos/rollout.sh;source ./demos/inbetween_babel.sh", "cmd_prefix": "source", "cmd_postfix": "./demos/scene.sh", "target_cmd": "source ./demos/scene.sh"}
{"uuid": "2f4b3f98-e77c-4217-8649-8f96c26ca8ed", "execution_plan": "step1: Set up the conda environment using the provided environment.yml file; step2: Download and organize the necessary data and model checkpoints from the provided Google Drive link and other specified sources; step3: Download and set up the SMPL-X and SMPL-H body models, converting them to PKL format if necessary; step4: Download and organize the AMASS, BABEL, and HumanML3D datasets for training purposes; step5: Run the interactive online text-conditioned motion generation demo; step6: Run the headless text-conditioned motion composition demo; step7: Run the text-conditioned motion in-betweening demo; step8: Run the human-scene interaction synthesis demo; step9: Run the text-conditioned goal reaching demo; step10: Run the sparse and dense joint trajectory control demo; step11: Preprocess the BABEL dataset for training; step12: Train the Motion Primitive VAE model; step13: Train the Latent Motion Primitive Diffusion Model; step14: Train the Motion Control Policy; step15: Evaluate the text-conditioned temporal motion composition; step16: Evaluate the text-conditioned motion in-betweening; step17: Evaluate the text-conditioned goal reaching", "executed_cmds": "conda env create -f environment.yml;conda activate DART;source ./demos/run_demo.sh;source ./demos/rollout.sh;source ./demos/inbetween_babel.sh;source ./demos/scene.sh;source ./demos/goal_reach.sh", "cmd_prefix": "source", "cmd_postfix": "./demos/traj.sh", "target_cmd": "source ./demos/traj.sh"}
{"uuid": "1b655880-f635-43c2-8ee2-c687419a0b28", "execution_plan": "step1: Create a conda environment with Python 3.9.19 and activate it; step2: Install required packages from the requirements.txt file; step3: Download the pre-trained video model checkpoint for Libero environment and unzip it; step4: Download the random action dataset for Libero environment and place it in the correct directory; step5: Train the model using video-guided exploration for Libero environment; step6: Download pre-trained goal-conditioned policy models for evaluation; step7: Launch policy rollout to evaluate the pre-trained models", "executed_cmds": "conda create -n v2a_libero_release python=3.9.19", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "8dae617c-7766-4f4e-a4a0-f728a2385425", "execution_plan": "step1: Create a conda environment with Python 3.9.19 and activate it; step2: Install required packages from the requirements.txt file; step3: Download the pre-trained video model checkpoint for Libero environment and unzip it; step4: Download the random action dataset for Libero environment and place it in the correct directory; step5: Train the model using video-guided exploration for Libero environment; step6: Download pre-trained goal-conditioned policy models for evaluation; step7: Launch policy rollout to evaluate the pre-trained models", "executed_cmds": "conda create -n v2a_libero_release python=3.9.19;pip install -r requirements.txt", "cmd_prefix": "unzip libero-video-model.zip", "cmd_postfix": "-d .", "target_cmd": "unzip libero-video-model.zip -d ."}
{"uuid": "01d415e7-b30c-4c7b-a5d7-c1ceb3474012", "execution_plan": "step1: Create a conda environment with Python 3.9.19 and activate it; step2: Install required packages from the requirements.txt file; step3: Download the pre-trained video model checkpoint for Libero environment and unzip it; step4: Download the random action dataset for Libero environment and place it in the correct directory; step5: Train the model using video-guided exploration for Libero environment; step6: Download pre-trained goal-conditioned policy models for evaluation; step7: Launch policy rollout to evaluate the pre-trained models", "executed_cmds": "conda create -n v2a_libero_release python=3.9.19;pip install -r requirements.txt;unzip libero-video-model.zip -d .;export lb_dir=data_dir/scratch/libero/env_rand_samples", "cmd_prefix": "mkdir", "cmd_postfix": "-p $lb_dir", "target_cmd": "mkdir -p $lb_dir"}
{"uuid": "931eddc2-1349-4d49-878a-524a1baaa00c", "execution_plan": "step1: Create a conda environment with Python 3.9.19 and activate it; step2: Install required packages from the requirements.txt file; step3: Download the pre-trained video model checkpoint for Libero environment and unzip it; step4: Download the random action dataset for Libero environment and place it in the correct directory; step5: Train the model using video-guided exploration for Libero environment; step6: Download pre-trained goal-conditioned policy models for evaluation; step7: Launch policy rollout to evaluate the pre-trained models", "executed_cmds": "conda create -n v2a_libero_release python=3.9.19;pip install -r requirements.txt;unzip libero-video-model.zip -d .;export lb_dir=data_dir/scratch/libero/env_rand_samples;mkdir -p $lb_dir;mv lb_randsam_8tk_perTk500.hdf5 $lb_dir;sh scripts/train_libero_dp.sh", "cmd_prefix": "./diffuser/libero/plan_lb_list.sh", "cmd_postfix": "$1 $2", "target_cmd": "./diffuser/libero/plan_lb_list.sh $1 $2"}
{"uuid": "5456c855-47d2-40ba-86a5-6b15d9eb4cf7", "execution_plan": "step1: Create a Python virtual environment using Conda; step2: Install the remaining package dependencies; step3: Install Flash Attention 2; step4: Log into Hugging Face account; step5: Install Git LFS; step6: Apply for access to Meta-Llama-3-8B-Instruct; step7: Replace Huggingface and Wandb keys in the launch script; step8: Replace the BASE_DIR path in the launch script; step9: Train COPO on Meta-Llama-3-8B-Instruct", "executed_cmds": "conda create -n rlhf python=3.10 && conda activate rlhf", "cmd_prefix": "python -m", "cmd_postfix": "pip install .", "target_cmd": "python -m pip install ."}
{"uuid": "10363a54-9dbd-409a-b837-484ec494b502", "execution_plan": "step1: Create a Python virtual environment using Conda; step2: Install the remaining package dependencies; step3: Install Flash Attention 2; step4: Log into Hugging Face account; step5: Install Git LFS; step6: Apply for access to Meta-Llama-3-8B-Instruct; step7: Replace Huggingface and Wandb keys in the launch script; step8: Replace the BASE_DIR path in the launch script; step9: Train COPO on Meta-Llama-3-8B-Instruct", "executed_cmds": "conda create -n rlhf python=3.10 && conda activate rlhf;python -m pip install .;python -m pip install flash-attn==2.3.6 --no-build-isolation", "cmd_prefix": "huggingface-cli", "cmd_postfix": "login", "target_cmd": "huggingface-cli login"}
{"uuid": "d0dfae48-1892-4889-86a0-449336877573", "execution_plan": "step1: Create a Python virtual environment using Conda; step2: Install the remaining package dependencies; step3: Install Flash Attention 2; step4: Log into Hugging Face account; step5: Install Git LFS; step6: Apply for access to Meta-Llama-3-8B-Instruct; step7: Replace Huggingface and Wandb keys in the launch script; step8: Replace the BASE_DIR path in the launch script; step9: Train COPO on Meta-Llama-3-8B-Instruct", "executed_cmds": "conda create -n rlhf python=3.10 && conda activate rlhf;python -m pip install .;python -m pip install flash-attn==2.3.6 --no-build-isolation;huggingface-cli login", "cmd_prefix": "sudo apt-get", "cmd_postfix": "install git-lfs", "target_cmd": "sudo apt-get install git-lfs"}
{"uuid": "e28c26af-c8ea-4041-a6ed-c0071c07f667", "execution_plan": "step1: Create a Python virtual environment using Conda; step2: Install the remaining package dependencies; step3: Install Flash Attention 2; step4: Log into Hugging Face account; step5: Install Git LFS; step6: Apply for access to Meta-Llama-3-8B-Instruct; step7: Replace Huggingface and Wandb keys in the launch script; step8: Replace the BASE_DIR path in the launch script; step9: Train COPO on Meta-Llama-3-8B-Instruct", "executed_cmds": "conda create -n rlhf python=3.10 && conda activate rlhf;python -m pip install .;python -m pip install flash-attn==2.3.6 --no-build-isolation;huggingface-cli login;sudo apt-get install git-lfs", "cmd_prefix": "cd", "cmd_postfix": "COPO/recipes", "target_cmd": "cd COPO/recipes"}
{"uuid": "0a6822ec-4f2e-49ec-83d3-9c7b2824e629", "execution_plan": "step1: Train the model for 1D regression using the RBF dataset and NP model with Renyi divergence 0.7; step2: Evaluate the trained model for 1D regression using the RBF dataset and NP model with Renyi divergence 0.7; step3: Plot the results for 1D regression using the RBF dataset and NP model with Renyi divergence 0.7; step4: Generate simulation data for Lotka Volterra dataset (training set); step5: Generate simulation data for Lotka Volterra dataset (evaluation set); step6: Download and prepare real-world data for Hare Lynx dataset; step7: Train the model for Lotka Volterra dataset using the NP model with Renyi divergence 0.7", "executed_cmds": "python regression/main_gp.py --data_name=RBF --model_name=NP --mode=train --divergence=Renyi_0.7;python regression/main_gp.py --data_name=RBF --model_name=NP --mode=eval --divergence=Renyi_0.7;python regression/main_gp.py --data_name=RBF --model_name=NP --mode=plot --divergence=Renyi_0.7;python3 data/lotka_volterra.py --filename=train --num_batches=10000 --trajectory_all=0;python3 data/lotka_volterra.py --filename=eval --num_batches=1000 --trajectory_all=0", "cmd_prefix": "python3", "cmd_postfix": "data/hare_lynx.py", "target_cmd": "python3 data/hare_lynx.py"}
{"uuid": "86f59040-8858-4e29-b868-7194306c17a7", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Create a new Python 3.10 virtual environment named `venv`; step4: Activate the virtual environment; step5: Install the `robust-llm` project; step6: Install developer dependencies and pre-commit hooks (optional for development); step7: Configure and run experiments using Hydra; step8: Run experiments with `accelerate` for multi-GPU support (optional); step9: Run experiments with checkpointing for preemption tolerance (optional)", "executed_cmds": "git clone https://github.com/AlignmentResearch/robust-llm.git", "cmd_prefix": "cd", "cmd_postfix": "robust-llm", "target_cmd": "cd robust-llm"}
{"uuid": "3cc244f1-1c9e-4aaa-9141-8580cd91fb7b", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Create a new Python 3.10 virtual environment named `venv`; step4: Activate the virtual environment; step5: Install the `robust-llm` project; step6: Install developer dependencies and pre-commit hooks (optional for development); step7: Configure and run experiments using Hydra; step8: Run experiments with `accelerate` for multi-GPU support (optional); step9: Run experiments with checkpointing for preemption tolerance (optional)", "executed_cmds": "git clone https://github.com/AlignmentResearch/robust-llm.git;cd robust-llm", "cmd_prefix": "python -m", "cmd_postfix": "venv venv", "target_cmd": "python -m venv venv"}
{"uuid": "36a4f749-7685-4b49-a0ae-4e895cc7f2c4", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Create a new Python 3.10 virtual environment named `venv`; step4: Activate the virtual environment; step5: Install the `robust-llm` project; step6: Install developer dependencies and pre-commit hooks (optional for development); step7: Configure and run experiments using Hydra; step8: Run experiments with `accelerate` for multi-GPU support (optional); step9: Run experiments with checkpointing for preemption tolerance (optional)", "executed_cmds": "git clone https://github.com/AlignmentResearch/robust-llm.git;cd robust-llm;python -m venv venv", "cmd_prefix": "source", "cmd_postfix": "venv/bin/activate", "target_cmd": "source venv/bin/activate"}
{"uuid": "fc5dbe2e-30f7-4483-89ab-a14593ee8dfa", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Create a new Python 3.10 virtual environment named `venv`; step4: Activate the virtual environment; step5: Install the `robust-llm` project; step6: Install developer dependencies and pre-commit hooks (optional for development); step7: Configure and run experiments using Hydra; step8: Run experiments with `accelerate` for multi-GPU support (optional); step9: Run experiments with checkpointing for preemption tolerance (optional)", "executed_cmds": "git clone https://github.com/AlignmentResearch/robust-llm.git;cd robust-llm;python -m venv venv;source venv/bin/activate", "cmd_prefix": "pip", "cmd_postfix": "install .", "target_cmd": "pip install ."}
{"uuid": "8cc9f1b8-b258-4425-81f0-bbe2144958c6", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Create a new Python 3.10 virtual environment named `venv`; step4: Activate the virtual environment; step5: Install the `robust-llm` project; step6: Install developer dependencies and pre-commit hooks (optional for development); step7: Configure and run experiments using Hydra; step8: Run experiments with `accelerate` for multi-GPU support (optional); step9: Run experiments with checkpointing for preemption tolerance (optional)", "executed_cmds": "git clone https://github.com/AlignmentResearch/robust-llm.git;cd robust-llm;python -m venv venv;source venv/bin/activate;pip install .", "cmd_prefix": "pre-commit", "cmd_postfix": "install", "target_cmd": "pre-commit install"}
{"uuid": "2449943c-0f40-43df-a7b3-4904fdef6291", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Create a new Python 3.10 virtual environment named `venv`; step4: Activate the virtual environment; step5: Install the `robust-llm` project; step6: Install developer dependencies and pre-commit hooks (optional for development); step7: Configure and run experiments using Hydra; step8: Run experiments with `accelerate` for multi-GPU support (optional); step9: Run experiments with checkpointing for preemption tolerance (optional)", "executed_cmds": "git clone https://github.com/AlignmentResearch/robust-llm.git;cd robust-llm;python -m venv venv;source venv/bin/activate;pip install .;pre-commit install", "cmd_prefix": "pip install", "cmd_postfix": "-e '.[dev]'", "target_cmd": "pip install -e '.[dev]'"}
{"uuid": "7f9e345b-50ee-4a97-9783-ebed78bab7bb", "execution_plan": "step1: Install the required dependencies with specified versions; step2: Prepare the datasets by unzipping the `datasets.zip` file in the source code directory; step3: Run the `main.py` file for initial testing; step4: Set up WandB for detailed experiments by creating `configs` and `remote` folders; step5: Initiate a parameter sweep using the `sweep.py` script with WandB credentials and YAML configuration; step6: Execute the program using the `agents.py` script with the sweep ID, either in single process or parallel mode; step7: Evaluate the results on the WandB platform using the provided sweep URL", "executed_cmds": "pip install python==3.8.18 pytorch==1.8.1 torch_geometric==1.6.3 torch_sparse==0.6.12 torch_scatter==2.0.8 torch_cluster==1.5.9 torch_spline_conv==1.2.1 torchmetrics==0.2.0 wandb==0.12.16;unzip datasets.zip -d source code directory/", "cmd_prefix": "python", "cmd_postfix": "main.py", "target_cmd": "python main.py"}
{"uuid": "c8283678-a4f3-487a-a74e-63e6c61ba5d9", "execution_plan": "step1: Set up the conda environment for Brain-JEPA; step2: Install the required dependencies using pip; step3: Download the pre-trained model checkpoints and example fine-tuned model from the provided Google Drive link; step4: Run Brain-JEPA pretraining on multiple GPUs using the specified configuration file; step5: Evaluate the model using the provided downstream task script", "executed_cmds": "conda create -n brain-jepa python=3.8", "cmd_prefix": "pip install", "cmd_postfix": "-r requirement.txt", "target_cmd": "pip install -r requirement.txt"}
{"uuid": "d5d90588-4e56-49f7-ae8d-2b79ecd7961e", "execution_plan": "step1: Install the required packages using conda and pip; step2: Prepare the safety alignment dataset by downloading it and placing it in the specified directory; step3: Prepare the supervised fine-tuning data by running the provided scripts; step4: Obtain access to the Llama2-7B model and set up the Hugging Face token; step5: Run the safety alignment script to produce the aligned model; step6: Fine-tune the model using harmful data with the specified ratio", "executed_cmds": "conda env create -f booster.yml", "cmd_prefix": "pip install", "cmd_postfix": "-r booster_pip.txt", "target_cmd": "pip install -r booster_pip.txt"}
{"uuid": "35fb0678-87dd-4055-9914-fb0ceb0c38cc", "execution_plan": "step1: Install the required packages using conda and pip; step2: Prepare the safety alignment dataset by downloading it and placing it in the specified directory; step3: Prepare the supervised fine-tuning data by running the provided scripts; step4: Obtain access to the Llama2-7B model and set up the Hugging Face token; step5: Run the safety alignment script to produce the aligned model; step6: Fine-tune the model using harmful data with the specified ratio", "executed_cmds": "conda env create -f booster.yml;pip install -r booster_pip.txt;cd sst2 && python build_dataset.py;cd ../gsm8k && python build_dataset.py;cd ../ag_news && python build_dataset.py", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "b62d0663-b8f7-46c2-95bc-2ebd6c836126", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a virtual environment with Python 3.11, then install dependencies; step3: Install the `iterative_label_refinement` package in development mode; step4: Set up environment variables by copying and editing the `.env` file; step5: Ensure access to HuggingFace models and download additional datasets if needed (e.g., BIRD dataset); step6: Train the LM evaluator for simulating unreliable comparison feedback; step7: Run SFT+ILR experiments, including training the initial SFT model, performing label refinement, and running subsequent rounds; step8: Run SFT+DPO experiments, including training the initial SFT model, labeling preferences, and running subsequent rounds; step9: Run naive ILR experiments as a baseline comparison; step10: Run experiments with higher supervision quality (unreliable demonstrations + reliable feedback or reliable demonstrations + reliable feedback)", "executed_cmds": "git clone https://github.com/helloelwin/iterative-label-refinement.git;cd iterative-label-refinement", "cmd_prefix": "conda create", "cmd_postfix": "-n ilr python=3.11", "target_cmd": "conda create -n ilr python=3.11"}
{"uuid": "21bae271-2282-4f24-96a6-2fb023788ae5", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a virtual environment with Python 3.11, then install dependencies; step3: Install the `iterative_label_refinement` package in development mode; step4: Set up environment variables by copying and editing the `.env` file; step5: Ensure access to HuggingFace models and download additional datasets if needed (e.g., BIRD dataset); step6: Train the LM evaluator for simulating unreliable comparison feedback; step7: Run SFT+ILR experiments, including training the initial SFT model, performing label refinement, and running subsequent rounds; step8: Run SFT+DPO experiments, including training the initial SFT model, labeling preferences, and running subsequent rounds; step9: Run naive ILR experiments as a baseline comparison; step10: Run experiments with higher supervision quality (unreliable demonstrations + reliable feedback or reliable demonstrations + reliable feedback)", "executed_cmds": "git clone https://github.com/helloelwin/iterative-label-refinement.git;cd iterative-label-refinement;conda create -n ilr python=3.11", "cmd_prefix": "conda", "cmd_postfix": "activate ilr", "target_cmd": "conda activate ilr"}
{"uuid": "1ce36cdd-e49e-4cc5-a34e-aced9f702779", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a virtual environment with Python 3.11, then install dependencies; step3: Install the `iterative_label_refinement` package in development mode; step4: Set up environment variables by copying and editing the `.env` file; step5: Ensure access to HuggingFace models and download additional datasets if needed (e.g., BIRD dataset); step6: Train the LM evaluator for simulating unreliable comparison feedback; step7: Run SFT+ILR experiments, including training the initial SFT model, performing label refinement, and running subsequent rounds; step8: Run SFT+DPO experiments, including training the initial SFT model, labeling preferences, and running subsequent rounds; step9: Run naive ILR experiments as a baseline comparison; step10: Run experiments with higher supervision quality (unreliable demonstrations + reliable feedback or reliable demonstrations + reliable feedback)", "executed_cmds": "git clone https://github.com/helloelwin/iterative-label-refinement.git;cd iterative-label-refinement;conda create -n ilr python=3.11;conda activate ilr", "cmd_prefix": "pip install", "cmd_postfix": "-r requiremntes.txt", "target_cmd": "pip install -r requiremntes.txt"}
{"uuid": "57e8090e-b9e3-4cb8-8087-9ecea69c12ee", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a virtual environment with Python 3.11, then install dependencies; step3: Install the `iterative_label_refinement` package in development mode; step4: Set up environment variables by copying and editing the `.env` file; step5: Ensure access to HuggingFace models and download additional datasets if needed (e.g., BIRD dataset); step6: Train the LM evaluator for simulating unreliable comparison feedback; step7: Run SFT+ILR experiments, including training the initial SFT model, performing label refinement, and running subsequent rounds; step8: Run SFT+DPO experiments, including training the initial SFT model, labeling preferences, and running subsequent rounds; step9: Run naive ILR experiments as a baseline comparison; step10: Run experiments with higher supervision quality (unreliable demonstrations + reliable feedback or reliable demonstrations + reliable feedback)", "executed_cmds": "git clone https://github.com/helloelwin/iterative-label-refinement.git;cd iterative-label-refinement;conda create -n ilr python=3.11;conda activate ilr;pip install -r requiremntes.txt", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "5070fbbd-85a1-4733-9020-9e54affc49e5", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a virtual environment with Python 3.11, then install dependencies; step3: Install the `iterative_label_refinement` package in development mode; step4: Set up environment variables by copying and editing the `.env` file; step5: Ensure access to HuggingFace models and download additional datasets if needed (e.g., BIRD dataset); step6: Train the LM evaluator for simulating unreliable comparison feedback; step7: Run SFT+ILR experiments, including training the initial SFT model, performing label refinement, and running subsequent rounds; step8: Run SFT+DPO experiments, including training the initial SFT model, labeling preferences, and running subsequent rounds; step9: Run naive ILR experiments as a baseline comparison; step10: Run experiments with higher supervision quality (unreliable demonstrations + reliable feedback or reliable demonstrations + reliable feedback)", "executed_cmds": "git clone https://github.com/helloelwin/iterative-label-refinement.git;cd iterative-label-refinement;conda create -n ilr python=3.11;conda activate ilr;pip install -r requiremntes.txt;pip install -e .", "cmd_prefix": "cp", "cmd_postfix": ".env.example .env", "target_cmd": "cp .env.example .env"}
{"uuid": "d72b4a20-f03a-44a7-97e1-50ae6fd725a3", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a virtual environment with Python 3.11, then install dependencies; step3: Install the `iterative_label_refinement` package in development mode; step4: Set up environment variables by copying and editing the `.env` file; step5: Ensure access to HuggingFace models and download additional datasets if needed (e.g., BIRD dataset); step6: Train the LM evaluator for simulating unreliable comparison feedback; step7: Run SFT+ILR experiments, including training the initial SFT model, performing label refinement, and running subsequent rounds; step8: Run SFT+DPO experiments, including training the initial SFT model, labeling preferences, and running subsequent rounds; step9: Run naive ILR experiments as a baseline comparison; step10: Run experiments with higher supervision quality (unreliable demonstrations + reliable feedback or reliable demonstrations + reliable feedback)", "executed_cmds": "git clone https://github.com/helloelwin/iterative-label-refinement.git;cd iterative-label-refinement;conda create -n ilr python=3.11;conda activate ilr;pip install -r requiremntes.txt;pip install -e .;cp .env.example .env", "cmd_prefix": "vim", "cmd_postfix": ".env", "target_cmd": "vim .env"}
{"uuid": "14012417-62db-440c-b19b-4bb1e918cd6c", "execution_plan": "step1: Set up the environment by creating a Conda environment and installing required dependencies; step2: Generate responses and edits to ethics questions for the ethics2 and ethics3 datasets; step3: Compute gradients with respect to the modified outputs to construct operational embeddings; step4: Construct thesauruses and find disagreements between human and LLM interpretations; step5: Grade pairs on downstream success to evaluate candidate failures; step6: Generate data for inference-steering failures using summarize2 followup prompts; step7: Compute gradients for inference-steering data; step8: Find inference-steering candidates by comparing thesauruses; step9: Grade inference-steering candidates on downstream success", "executed_cmds": "conda create -n ted python=3.11", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "b900cbba-974c-4884-a341-1f8426f853ca", "execution_plan": "step1: Set up the environment by creating a Conda environment and installing required dependencies; step2: Generate responses and edits to ethics questions for the ethics2 and ethics3 datasets; step3: Compute gradients with respect to the modified outputs to construct operational embeddings; step4: Construct thesauruses and find disagreements between human and LLM interpretations; step5: Grade pairs on downstream success to evaluate candidate failures; step6: Generate data for inference-steering failures using summarize2 followup prompts; step7: Compute gradients for inference-steering data; step8: Find inference-steering candidates by comparing thesauruses; step9: Grade inference-steering candidates on downstream success", "executed_cmds": "conda create -n ted python=3.11;pip install -r requirements.txt", "cmd_prefix": "conda", "cmd_postfix": "activate ted", "target_cmd": "conda activate ted"}
{"uuid": "4d4f7138-ec3c-49cf-adf0-02ef1189ae1b", "execution_plan": "step1: Set up the environment by creating a Conda environment and installing required dependencies; step2: Generate responses and edits to ethics questions for the ethics2 and ethics3 datasets; step3: Compute gradients with respect to the modified outputs to construct operational embeddings; step4: Construct thesauruses and find disagreements between human and LLM interpretations; step5: Grade pairs on downstream success to evaluate candidate failures; step6: Generate data for inference-steering failures using summarize2 followup prompts; step7: Compute gradients for inference-steering data; step8: Find inference-steering candidates by comparing thesauruses; step9: Grade inference-steering candidates on downstream success", "executed_cmds": "conda create -n ted python=3.11;pip install -r requirements.txt;conda activate ted;python create_user_query_dataset.py --modal mistral --followup constitution7 --dataset ethics2;python create_user_query_dataset.py --modal mistral --followup constitution7 --dataset ethics3;python compute_grads.py --model mistral --dir <ethics3 dataset folder>;python compute_differences.py --model llama3-70B --grad-dir <ethics3 dataset folder> --followup-prompts constitution7 --similarity-threshold 0.95;python grade_outputs_parallel.py --modifiers <JSON file of candidates> --dataset <ethics2 outputs pickle file> --model gpt4 --followup-prompts constitution7;python create_user_query_dataset.py --modal mistral --followup summarize2 --dataset ethics3;python create_inference_responses_dataset.py --model mistral --infer-prompts infer1 --followup-promots summarize2;python compute_grads.py --model mistral --dir <ethics3", "cmd_prefix": "summarize2", "cmd_postfix": "dataset folder>", "target_cmd": "summarize2 dataset folder>"}
{"uuid": "7e96fee7-957d-4ae5-ba76-4db79d6e40a0", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a new conda environment and activate it; step3: Install PyTorch and other dependencies; step4: Download and extract the datasets to the `data` folder; step5: Train the model on a specific dataset (ScanNet, Replica, Tanksandtemples, or ScanNet++); step6: Extract the mesh after training; step7: Evaluate the extracted mesh; step8: Run with a custom dataset (optional); step9: Generate monocular cues for a custom dataset (optional)", "executed_cmds": "git clone https://github.com/zju3dv/ND-SDF.git", "cmd_prefix": "cd", "cmd_postfix": "ND-SDF", "target_cmd": "cd ND-SDF"}
{"uuid": "03f77ee2-3cff-4fbd-8440-3128ec6a5b47", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a new conda environment and activate it; step3: Install PyTorch and other dependencies; step4: Download and extract the datasets to the `data` folder; step5: Train the model on a specific dataset (ScanNet, Replica, Tanksandtemples, or ScanNet++); step6: Extract the mesh after training; step7: Evaluate the extracted mesh; step8: Run with a custom dataset (optional); step9: Generate monocular cues for a custom dataset (optional)", "executed_cmds": "git clone https://github.com/zju3dv/ND-SDF.git;cd ND-SDF", "cmd_prefix": "conda create", "cmd_postfix": "-n ndsdf python=3.8", "target_cmd": "conda create -n ndsdf python=3.8"}
{"uuid": "5c13ffba-6a87-4b8d-9109-c606c83c5904", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a new conda environment and activate it; step3: Install PyTorch and other dependencies; step4: Download and extract the datasets to the `data` folder; step5: Train the model on a specific dataset (ScanNet, Replica, Tanksandtemples, or ScanNet++); step6: Extract the mesh after training; step7: Evaluate the extracted mesh; step8: Run with a custom dataset (optional); step9: Generate monocular cues for a custom dataset (optional)", "executed_cmds": "git clone https://github.com/zju3dv/ND-SDF.git;cd ND-SDF;conda create -n ndsdf python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate ndsdf", "target_cmd": "conda activate ndsdf"}
{"uuid": "76cef15f-e941-4855-a246-d0b85a01d7f4", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a new conda environment and activate it; step3: Install PyTorch and other dependencies; step4: Download and extract the datasets to the `data` folder; step5: Train the model on a specific dataset (ScanNet, Replica, Tanksandtemples, or ScanNet++); step6: Extract the mesh after training; step7: Evaluate the extracted mesh; step8: Run with a custom dataset (optional); step9: Generate monocular cues for a custom dataset (optional)", "executed_cmds": "git clone https://github.com/zju3dv/ND-SDF.git;cd ND-SDF;conda create -n ndsdf python=3.8;conda activate ndsdf;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "b364e7ed-d51d-4c13-a982-b9e5b2cafa4c", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a new conda environment and activate it; step3: Install PyTorch and other dependencies; step4: Download and extract the datasets to the `data` folder; step5: Train the model on a specific dataset (ScanNet, Replica, Tanksandtemples, or ScanNet++); step6: Extract the mesh after training; step7: Evaluate the extracted mesh; step8: Run with a custom dataset (optional); step9: Generate monocular cues for a custom dataset (optional)", "executed_cmds": "git clone https://github.com/zju3dv/ND-SDF.git;cd ND-SDF;conda create -n ndsdf python=3.8;conda activate ndsdf;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html;pip install -r requirements.txt;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannet.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/replica.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/tnt.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannetpp.yaml --scan_id 1;python scripts/extract_mesh.py --conf <path_to_config_file> --checkpoint <path_to_checkpoint> --res 512 --textured", "cmd_prefix": "cd", "cmd_postfix": "evals/scannet_eval", "target_cmd": "cd evals/scannet_eval"}
{"uuid": "384ead80-0b88-4626-a0a6-963004a1b0b3", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a new conda environment and activate it; step3: Install PyTorch and other dependencies; step4: Download and extract the datasets to the `data` folder; step5: Train the model on a specific dataset (ScanNet, Replica, Tanksandtemples, or ScanNet++); step6: Extract the mesh after training; step7: Evaluate the extracted mesh; step8: Run with a custom dataset (optional); step9: Generate monocular cues for a custom dataset (optional)", "executed_cmds": "git clone https://github.com/zju3dv/ND-SDF.git;cd ND-SDF;conda create -n ndsdf python=3.8;conda activate ndsdf;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html;pip install -r requirements.txt;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannet.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/replica.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/tnt.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannetpp.yaml --scan_id 1;python scripts/extract_mesh.py --conf <path_to_config_file> --checkpoint <path_to_checkpoint> --res 512 --textured;cd evals/scannet_eval;python evaluate.py --exp_name <exp_name>", "cmd_prefix": "cd", "cmd_postfix": "evals/replica_eval", "target_cmd": "cd evals/replica_eval"}
{"uuid": "9adebc10-f6c9-40e8-abf8-fb55b43d1362", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a new conda environment and activate it; step3: Install PyTorch and other dependencies; step4: Download and extract the datasets to the `data` folder; step5: Train the model on a specific dataset (ScanNet, Replica, Tanksandtemples, or ScanNet++); step6: Extract the mesh after training; step7: Evaluate the extracted mesh; step8: Run with a custom dataset (optional); step9: Generate monocular cues for a custom dataset (optional)", "executed_cmds": "git clone https://github.com/zju3dv/ND-SDF.git;cd ND-SDF;conda create -n ndsdf python=3.8;conda activate ndsdf;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html;pip install -r requirements.txt;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannet.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/replica.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/tnt.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannetpp.yaml --scan_id 1;python scripts/extract_mesh.py --conf <path_to_config_file> --checkpoint <path_to_checkpoint> --res 512 --textured;cd evals/scannet_eval;python evaluate.py --exp_name <exp_name>;cd evals/replica_eval;python evaluate.py --exp_name <exp_name>", "cmd_prefix": "cd", "cmd_postfix": "evals/scannetpp_eval", "target_cmd": "cd evals/scannetpp_eval"}
{"uuid": "0907c4a7-0633-407e-bdfc-14c2ed90b21a", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a new conda environment and activate it; step3: Install PyTorch and other dependencies; step4: Download and extract the datasets to the `data` folder; step5: Train the model on a specific dataset (ScanNet, Replica, Tanksandtemples, or ScanNet++); step6: Extract the mesh after training; step7: Evaluate the extracted mesh; step8: Run with a custom dataset (optional); step9: Generate monocular cues for a custom dataset (optional)", "executed_cmds": "git clone https://github.com/zju3dv/ND-SDF.git;cd ND-SDF;conda create -n ndsdf python=3.8;conda activate ndsdf;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html;pip install -r requirements.txt;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannet.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/replica.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/tnt.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannetpp.yaml --scan_id 1;python scripts/extract_mesh.py --conf <path_to_config_file> --checkpoint <path_to_checkpoint> --res 512 --textured;cd evals/scannet_eval;python evaluate.py --exp_name <exp_name>;cd evals/replica_eval;python evaluate.py --exp_name <exp_name>;cd evals/scannetpp_eval;python evaluate.py --exp_name <exp_name>", "cmd_prefix": "cd", "cmd_postfix": "preprocess/datasets", "target_cmd": "cd preprocess/datasets"}
{"uuid": "8ffd460b-680b-4ef5-93ef-0c523ec52ba2", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a new conda environment and activate it; step3: Install PyTorch and other dependencies; step4: Download and extract the datasets to the `data` folder; step5: Train the model on a specific dataset (ScanNet, Replica, Tanksandtemples, or ScanNet++); step6: Extract the mesh after training; step7: Evaluate the extracted mesh; step8: Run with a custom dataset (optional); step9: Generate monocular cues for a custom dataset (optional)", "executed_cmds": "git clone https://github.com/zju3dv/ND-SDF.git;cd ND-SDF;conda create -n ndsdf python=3.8;conda activate ndsdf;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html;pip install -r requirements.txt;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannet.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/replica.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/tnt.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannetpp.yaml --scan_id 1;python scripts/extract_mesh.py --conf <path_to_config_file> --checkpoint <path_to_checkpoint> --res 512 --textured;cd evals/scannet_eval;python evaluate.py --exp_name <exp_name>;cd evals/replica_eval;python evaluate.py --exp_name <exp_name>;cd evals/scannetpp_eval;python evaluate.py --exp_name <exp_name>;cd preprocess/datasets;python convert.py -s <custom_dataset_dir>;python process_colmap_to_json.py -i <custom_dataset_dir>;torchrun --nproc_per_node=1 exp_runner.py --conf confs/custom.yaml --scan_id -1 --data_dir <custom_dataset_dir>", "cmd_prefix": "cd", "cmd_postfix": "preprocess/omnidata", "target_cmd": "cd preprocess/omnidata"}
{"uuid": "a232390e-11f1-44e2-9f9c-c96838ce7c65", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a new conda environment and activate it; step3: Install PyTorch and other dependencies; step4: Download and extract the datasets to the `data` folder; step5: Train the model on a specific dataset (ScanNet, Replica, Tanksandtemples, or ScanNet++); step6: Extract the mesh after training; step7: Evaluate the extracted mesh; step8: Run with a custom dataset (optional); step9: Generate monocular cues for a custom dataset (optional)", "executed_cmds": "git clone https://github.com/zju3dv/ND-SDF.git;cd ND-SDF;conda create -n ndsdf python=3.8;conda activate ndsdf;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 -f https://download.pytorch.org/whl/torch_stable.html;pip install -r requirements.txt;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannet.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/replica.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/tnt.yaml --scan_id 1;torchrun --nproc_per_node=1 exp_runner.py --conf confs/scannetpp.yaml --scan_id 1;python scripts/extract_mesh.py --conf <path_to_config_file> --checkpoint <path_to_checkpoint> --res 512 --textured;cd evals/scannet_eval;python evaluate.py --exp_name <exp_name>;cd evals/replica_eval;python evaluate.py --exp_name <exp_name>;cd evals/scannetpp_eval;python evaluate.py --exp_name <exp_name>;cd preprocess/datasets;python convert.py -s <custom_dataset_dir>;python process_colmap_to_json.py -i <custom_dataset_dir>;torchrun --nproc_per_node=1 exp_runner.py --conf confs/custom.yaml --scan_id -1 --data_dir <custom_dataset_dir>;cd preprocess/omnidata", "cmd_prefix": "sh", "cmd_postfix": "download.sh", "target_cmd": "sh download.sh"}
{"uuid": "ce133277-e5b6-4c8b-be71-0d4d898eae57", "execution_plan": "step1: Create a conda environment named 'xray' with Python 3.10 and install required dependencies including PyTorch, xformers, and other packages listed in requirements.txt; step2: Download the dataset from Huggingface, combine the split zip files, unzip them, and create a symbolic link to the dataset directory; step3: Preprocess the dataset by rendering the mesh to obtain images and camera parameters, then generate the X-Ray representation; step4: Train the diffusion model using the provided script; step5: Train the upsampler model using the provided script; step6: Evaluate the trained diffusion and upsampler models", "executed_cmds": "conda create -n xray python=3.10;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118;pip install -U xformers==v0.0.23.post1 --index-url https://download.pytorch.org/whl/cu118", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "928b4f3f-2e4f-49be-887f-d5c6c3b228d2", "execution_plan": "step1: Create a conda environment named 'xray' with Python 3.10 and install required dependencies including PyTorch, xformers, and other packages listed in requirements.txt; step2: Download the dataset from Huggingface, combine the split zip files, unzip them, and create a symbolic link to the dataset directory; step3: Preprocess the dataset by rendering the mesh to obtain images and camera parameters, then generate the X-Ray representation; step4: Train the diffusion model using the provided script; step5: Train the upsampler model using the provided script; step6: Evaluate the trained diffusion and upsampler models", "executed_cmds": "conda create -n xray python=3.10;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118;pip install -U xformers==v0.0.23.post1 --index-url https://download.pytorch.org/whl/cu118;pip install -r requirements.txt", "cmd_prefix": "cat 0*.zip", "cmd_postfix": "> Objaverse_XRay.zip", "target_cmd": "cat 0*.zip > Objaverse_XRay.zip"}
{"uuid": "b6f7da3d-fe56-4148-97fe-1d7b0f8b074b", "execution_plan": "step1: Create a conda environment named 'xray' with Python 3.10 and install required dependencies including PyTorch, xformers, and other packages listed in requirements.txt; step2: Download the dataset from Huggingface, combine the split zip files, unzip them, and create a symbolic link to the dataset directory; step3: Preprocess the dataset by rendering the mesh to obtain images and camera parameters, then generate the X-Ray representation; step4: Train the diffusion model using the provided script; step5: Train the upsampler model using the provided script; step6: Evaluate the trained diffusion and upsampler models", "executed_cmds": "conda create -n xray python=3.10;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118;pip install -U xformers==v0.0.23.post1 --index-url https://download.pytorch.org/whl/cu118;pip install -r requirements.txt;cat 0*.zip > Objaverse_XRay.zip", "cmd_prefix": "unzip", "cmd_postfix": "Objaverse_XRay.zip", "target_cmd": "unzip Objaverse_XRay.zip"}
{"uuid": "ec46b397-7636-402e-af20-3a1b5a2826a6", "execution_plan": "step1: Create a conda environment named 'xray' with Python 3.10 and install required dependencies including PyTorch, xformers, and other packages listed in requirements.txt; step2: Download the dataset from Huggingface, combine the split zip files, unzip them, and create a symbolic link to the dataset directory; step3: Preprocess the dataset by rendering the mesh to obtain images and camera parameters, then generate the X-Ray representation; step4: Train the diffusion model using the provided script; step5: Train the upsampler model using the provided script; step6: Evaluate the trained diffusion and upsampler models", "executed_cmds": "conda create -n xray python=3.10;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118;pip install -U xformers==v0.0.23.post1 --index-url https://download.pytorch.org/whl/cu118;pip install -r requirements.txt;cat 0*.zip > Objaverse_XRay.zip;unzip Objaverse_XRay.zip;ln -s /path/to/Objaverse_XRay Data/Objaverse_XRay", "cmd_prefix": "cd", "cmd_postfix": "preprocess/get_image", "target_cmd": "cd preprocess/get_image"}
{"uuid": "50cdf417-615f-44dd-bc32-e4ced00f4784", "execution_plan": "step1: Create a conda environment named 'xray' with Python 3.10 and install required dependencies including PyTorch, xformers, and other packages listed in requirements.txt; step2: Download the dataset from Huggingface, combine the split zip files, unzip them, and create a symbolic link to the dataset directory; step3: Preprocess the dataset by rendering the mesh to obtain images and camera parameters, then generate the X-Ray representation; step4: Train the diffusion model using the provided script; step5: Train the upsampler model using the provided script; step6: Evaluate the trained diffusion and upsampler models", "executed_cmds": "conda create -n xray python=3.10;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118;pip install -U xformers==v0.0.23.post1 --index-url https://download.pytorch.org/whl/cu118;pip install -r requirements.txt;cat 0*.zip > Objaverse_XRay.zip;unzip Objaverse_XRay.zip;ln -s /path/to/Objaverse_XRay Data/Objaverse_XRay;cd preprocess/get_image;bash custom/render_mesh.sh", "cmd_prefix": "cd", "cmd_postfix": "preprocess/get_xray", "target_cmd": "cd preprocess/get_xray"}
{"uuid": "176d4e25-fdc3-4eaa-9cb8-07fafc1e047e", "execution_plan": "step1: Create a conda environment named 'xray' with Python 3.10 and install required dependencies including PyTorch, xformers, and other packages listed in requirements.txt; step2: Download the dataset from Huggingface, combine the split zip files, unzip them, and create a symbolic link to the dataset directory; step3: Preprocess the dataset by rendering the mesh to obtain images and camera parameters, then generate the X-Ray representation; step4: Train the diffusion model using the provided script; step5: Train the upsampler model using the provided script; step6: Evaluate the trained diffusion and upsampler models", "executed_cmds": "conda create -n xray python=3.10;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118;pip install -U xformers==v0.0.23.post1 --index-url https://download.pytorch.org/whl/cu118;pip install -r requirements.txt;cat 0*.zip > Objaverse_XRay.zip;unzip Objaverse_XRay.zip;ln -s /path/to/Objaverse_XRay Data/Objaverse_XRay;cd preprocess/get_image;bash custom/render_mesh.sh;cd preprocess/get_xray", "cmd_prefix": "python", "cmd_postfix": "get_xray.py", "target_cmd": "python get_xray.py"}
{"uuid": "904bd9a0-8fcb-4cd6-ad5f-d643b106fdcf", "execution_plan": "step1: Understand the project and its requirements by reviewing the README and linked resources; step2: Set up the environment by following the requirements from the Separable PINN repository; step3: Train and test the model on the 3D Helmholtz equation; step4: Train and test the model on the 3D Klein-Gordon equation; step5: Train and test the model on the 4D Klein-Gordon equation; step6: Train and test the model on the 3D diffusion equation", "executed_cmds": "python helmholtz3d.py", "cmd_prefix": "python", "cmd_postfix": "klein_gordon3d.py", "target_cmd": "python klein_gordon3d.py"}
{"uuid": "f2c81284-1046-46e4-b947-e631fc5795cb", "execution_plan": "step1: Understand the project and its requirements by reviewing the README and linked resources; step2: Set up the environment by following the requirements from the Separable PINN repository; step3: Train and test the model on the 3D Helmholtz equation; step4: Train and test the model on the 3D Klein-Gordon equation; step5: Train and test the model on the 4D Klein-Gordon equation; step6: Train and test the model on the 3D diffusion equation", "executed_cmds": "python helmholtz3d.py;python klein_gordon3d.py", "cmd_prefix": "python", "cmd_postfix": "klein_gordon4d.py", "target_cmd": "python klein_gordon4d.py"}
{"uuid": "d5fc5d2c-2ed4-46fe-8d43-4341970e29e4", "execution_plan": "step1: Understand the project and its requirements by reviewing the README and linked resources; step2: Set up the environment by following the requirements from the Separable PINN repository; step3: Train and test the model on the 3D Helmholtz equation; step4: Train and test the model on the 3D Klein-Gordon equation; step5: Train and test the model on the 4D Klein-Gordon equation; step6: Train and test the model on the 3D diffusion equation", "executed_cmds": "python helmholtz3d.py;python klein_gordon3d.py;python klein_gordon4d.py", "cmd_prefix": "python", "cmd_postfix": "diffusion3d.py", "target_cmd": "python diffusion3d.py"}
{"uuid": "28534458-8b6f-4948-92de-f98724907c92", "execution_plan": "step1: Install PyTorch version 1.11.0 or higher; step2: Install key dependencies including monai, einops, transformers, and matplotlib; step3: Follow the inference demo guide to run a demo case; step4: Follow the training guide to train SegVol; step5: Use the pre-trained ViT as a model encoder if needed; step6: Download datasets from ModelScope or HuggingFace for training or inference", "executed_cmds": "pip install 'monai[all]==0.9.0';pip install einops==0.6.1;pip install transformers==4.18.0", "cmd_prefix": "pip", "cmd_postfix": "install matplotlib", "target_cmd": "pip install matplotlib"}
{"uuid": "c7f7c7da-90d3-4b28-a47d-75dda082f28e", "execution_plan": "step1: Install required packages and dependencies for FlexPrefill; step2: Install FlexPrefill using pip from the GitHub repository; step3: Run a basic test script to verify the installation and functionality of FlexPrefill with a specified model; step4: Use FlexPrefill sparse attention function in a custom script with specified parameters; step5: Patch a Hugging Face Transformers model to use FlexPrefill sparse attention and run inference; step6: Patch a vLLM model to use FlexPrefill sparse attention and run inference (experimental); step7: Install additional dependencies and download models for running experiments; step8: Download and preprocess datasets for experiments; step9: Run experiments using provided scripts and save results", "executed_cmds": "pip install torch==2.4.0 triton==3.0.0 transformers==4.44.0 flash_attn==2.6.3 vllm==0.5.4;pip install git+https://github.com/bytedance/FlexPrefill.git;python tests/test_llm.py --model meta-llama/Llama-3.1-8B-Instruct --pattern default;python tests/test_llm.py --model meta-llama/Llama-3.1-8B-Instruct --pattern flex_prefill", "cmd_prefix": "bash", "cmd_postfix": "install.sh", "target_cmd": "bash install.sh"}
{"uuid": "87299433-15f1-4666-b426-50bbb0986c41", "execution_plan": "step1: Install the required dependencies including jax, jaxopt, ott, and python-ternary; step2: Clone or download the repository to your local machine; step3: Navigate to the folder \"xps_interaction\" and run the notebook \"MD_mirror_interaction.ipynb\" to reproduce Figure 1; step4: Navigate to the folder \"xps_Gaussians\", run the script \"xps.sh\", and then run the notebook \"Results_Gaussian.ipynb\" to reproduce Figure 2; step5: Run the notebook \"MD - Dirichlet Posterior.ipynb\" to reproduce the experiment on the simplex from Appendix G", "executed_cmds": "pip install jax jaxopt", "cmd_prefix": "pip", "cmd_postfix": "install ott-jax", "target_cmd": "pip install ott-jax"}
{"uuid": "c77961cd-811e-47ed-ac47-642eb84d3bfa", "execution_plan": "step1: Set up the Conda environment and install necessary dependencies; step2: Train the teacher model using supervised fine-tuning (SFT) and DPO training; step3: Initialize the student model with supervised fine-tuning (SFT); step4: Perform DCKD by precomputing teacher log-probabilities and merging them into a DCKD dataset; step5: Run DCKD training on the student model; step6: Generate student responses for ADPA and form a preference dataset; step7: Compute advantage signals by comparing DPO teacher and reference teacher log-probabilities; step8: Merge logits for ADPA dataset; step9: Run ADPA training on the student model", "executed_cmds": "conda create -n handbook python=3.11", "cmd_prefix": "conda", "cmd_postfix": "activate handbook", "target_cmd": "conda activate handbook"}
{"uuid": "bb843bea-fd90-4088-831b-52256c314f80", "execution_plan": "step1: Create a virtual environment for the project using conda and install the required dependencies; step2: Prepare the actor model weights and seed prompts data by storing them in the specified directories; step3: Activate vLLM to accelerate MCT data generation by running the provided script; step4: Build MCT data by configuring the settings in `config/tree_generate.yaml` and `src/final_orm.py`, then run the MCT generation script; step5: Construct tuning data for the actor model using the generated MCT data and the provided data construction strategies; step6: Fine-tune the actor model using the constructed tuning data", "executed_cmds": "conda create -n stair python=3.11", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "dd4cc98d-2a04-4cdf-a356-021598390f09", "execution_plan": "step1: Clone the repository to get the code; step2: Set up the Python environment using conda and install dependencies; step3: Download LLM chat templates; step4: Log in to Weights & Biases for training visualization; step5: Train AutoDAN-Turbo-R using OpenAI API or Microsoft Azure API; step6: Test AutoDAN-Turbo-R with a malicious request using OpenAI API or Microsoft Azure API; step7: Train AutoDAN-Turbo-v1.0 using OpenAI API or Microsoft Azure API; step8: Test AutoDAN-Turbo-v1.0 with a malicious request using OpenAI API or Microsoft Azure API", "executed_cmds": "git clone https://github.com/SaFoLab-WISC/AutoDAN-Turbo.git", "cmd_prefix": "cd", "cmd_postfix": "AutoDAN-Turbo", "target_cmd": "cd AutoDAN-Turbo"}
{"uuid": "e1c2cea4-36bc-4d17-91da-fc9a972e2a3d", "execution_plan": "step1: Clone the repository to get the code; step2: Set up the Python environment using conda and install dependencies; step3: Download LLM chat templates; step4: Log in to Weights & Biases for training visualization; step5: Train AutoDAN-Turbo-R using OpenAI API or Microsoft Azure API; step6: Test AutoDAN-Turbo-R with a malicious request using OpenAI API or Microsoft Azure API; step7: Train AutoDAN-Turbo-v1.0 using OpenAI API or Microsoft Azure API; step8: Test AutoDAN-Turbo-v1.0 with a malicious request using OpenAI API or Microsoft Azure API", "executed_cmds": "git clone https://github.com/SaFoLab-WISC/AutoDAN-Turbo.git;cd AutoDAN-Turbo;conda create -n autodanturbo python==3.12;conda activate autodanturbo", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "842026f4-7f7e-4bdd-a736-eba674236f3d", "execution_plan": "step1: Clone the repository to get the code; step2: Set up the Python environment using conda and install dependencies; step3: Download LLM chat templates; step4: Log in to Weights & Biases for training visualization; step5: Train AutoDAN-Turbo-R using OpenAI API or Microsoft Azure API; step6: Test AutoDAN-Turbo-R with a malicious request using OpenAI API or Microsoft Azure API; step7: Train AutoDAN-Turbo-v1.0 using OpenAI API or Microsoft Azure API; step8: Test AutoDAN-Turbo-v1.0 with a malicious request using OpenAI API or Microsoft Azure API", "executed_cmds": "git clone https://github.com/SaFoLab-WISC/AutoDAN-Turbo.git;cd AutoDAN-Turbo;conda create -n autodanturbo python==3.12;conda activate autodanturbo;pip install -r requirements.txt", "cmd_prefix": "cd", "cmd_postfix": "llm", "target_cmd": "cd llm"}
{"uuid": "8af79cb6-262a-4f9a-99f1-4043dd183c71", "execution_plan": "step1: Clone the repository to get the code; step2: Set up the Python environment using conda and install dependencies; step3: Download LLM chat templates; step4: Log in to Weights & Biases for training visualization; step5: Train AutoDAN-Turbo-R using OpenAI API or Microsoft Azure API; step6: Test AutoDAN-Turbo-R with a malicious request using OpenAI API or Microsoft Azure API; step7: Train AutoDAN-Turbo-v1.0 using OpenAI API or Microsoft Azure API; step8: Test AutoDAN-Turbo-v1.0 with a malicious request using OpenAI API or Microsoft Azure API", "executed_cmds": "git clone https://github.com/SaFoLab-WISC/AutoDAN-Turbo.git;cd AutoDAN-Turbo;conda create -n autodanturbo python==3.12;conda activate autodanturbo;pip install -r requirements.txt;cd llm;git clone https://github.com/chujiezheng/chat_templates.git", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "ee68cf44-6106-4884-878f-4e316081ffc6", "execution_plan": "step1: Clone the repository to get the code; step2: Set up the Python environment using conda and install dependencies; step3: Download LLM chat templates; step4: Log in to Weights & Biases for training visualization; step5: Train AutoDAN-Turbo-R using OpenAI API or Microsoft Azure API; step6: Test AutoDAN-Turbo-R with a malicious request using OpenAI API or Microsoft Azure API; step7: Train AutoDAN-Turbo-v1.0 using OpenAI API or Microsoft Azure API; step8: Test AutoDAN-Turbo-v1.0 with a malicious request using OpenAI API or Microsoft Azure API", "executed_cmds": "git clone https://github.com/SaFoLab-WISC/AutoDAN-Turbo.git;cd AutoDAN-Turbo;conda create -n autodanturbo python==3.12;conda activate autodanturbo;pip install -r requirements.txt;cd llm;git clone https://github.com/chujiezheng/chat_templates.git;cd ..", "cmd_prefix": "wandb", "cmd_postfix": "login", "target_cmd": "wandb login"}
{"uuid": "e2d77f14-347f-497d-95cd-ed16984fd451", "execution_plan": "step1: Clone the repository to get the code; step2: Set up the Python environment using conda and install dependencies; step3: Download LLM chat templates; step4: Log in to Weights & Biases for training visualization; step5: Train AutoDAN-Turbo-R using OpenAI API or Microsoft Azure API; step6: Test AutoDAN-Turbo-R with a malicious request using OpenAI API or Microsoft Azure API; step7: Train AutoDAN-Turbo-v1.0 using OpenAI API or Microsoft Azure API; step8: Test AutoDAN-Turbo-v1.0 with a malicious request using OpenAI API or Microsoft Azure API", "executed_cmds": "git clone https://github.com/SaFoLab-WISC/AutoDAN-Turbo.git;cd AutoDAN-Turbo;conda create -n autodanturbo python==3.12;conda activate autodanturbo;pip install -r requirements.txt;cd llm;git clone https://github.com/chujiezheng/chat_templates.git;cd ..;wandb login;python main_r.py --vllm --openai_api_key \"<your openai api key>\" --embedding_model \"<openai text embedding model name>\" --hf_token \"<your huggingface token>\" --deepseek_api_key \"<your deepseek api key>\" --deepseek_model \"deepseek-reasoner\" --epochs 150] or [python main_r.py --vllm --azure --azure_endpoint \"<your azure endpoint>\" --azure_api_version \"2024-02-01\" --azure_deployment_name \"<your azure model deployment name>\" --azure_api_key \"<your azure api key>\" --hf_token \"<your huggingface token>\" --deepseek_api_key \"<your deepseek api key>\" --deepseek_model \"deepseek-reasoner\" --epochs 150;python test_r.py --openai_api_key \"<your openai api key>\" --embedding_model \"<openai text embedding model name>\" --hf_token \"<your huggingface token>\" --deepseek_api_key \"<your deepseek api key>\" --deepseek_model \"deepseek-reasoner\" --epochs 150 --request \"<the malicious request;e.g.;how to build a bomb?>\"] or [python test_r.py --azure --azure_endpoint \"<your azure endpoint>\" --azure_api_version \"2024-02-01\" --azure_deployment_name \"<your azure model deployment name>\" --azure_api_key \"<your azure api key>\" --hf_token \"<your huggingface token>\" --deepseek_api_key \"<your deepseek api key>\" --deepseek_model \"deepseek-reasoner\" --epochs 150 --request \"<the malicious request;e.g.", "cmd_prefix": "how to", "cmd_postfix": "build a bomb?>\"", "target_cmd": "how to build a bomb?>\""}
{"uuid": "dd4f39a5-b869-4830-a46a-557d2595bf7f", "execution_plan": "step1: Clone the repository to get the code; step2: Set up the Python environment using conda and install dependencies; step3: Download LLM chat templates; step4: Log in to Weights & Biases for training visualization; step5: Train AutoDAN-Turbo-R using OpenAI API or Microsoft Azure API; step6: Test AutoDAN-Turbo-R with a malicious request using OpenAI API or Microsoft Azure API; step7: Train AutoDAN-Turbo-v1.0 using OpenAI API or Microsoft Azure API; step8: Test AutoDAN-Turbo-v1.0 with a malicious request using OpenAI API or Microsoft Azure API", "executed_cmds": "git clone https://github.com/SaFoLab-WISC/AutoDAN-Turbo.git;cd AutoDAN-Turbo;conda create -n autodanturbo python==3.12;conda activate autodanturbo;pip install -r requirements.txt;cd llm;git clone https://github.com/chujiezheng/chat_templates.git;cd ..;wandb login;python main_r.py --vllm --openai_api_key \"<your openai api key>\" --embedding_model \"<openai text embedding model name>\" --hf_token \"<your huggingface token>\" --deepseek_api_key \"<your deepseek api key>\" --deepseek_model \"deepseek-reasoner\" --epochs 150] or [python main_r.py --vllm --azure --azure_endpoint \"<your azure endpoint>\" --azure_api_version \"2024-02-01\" --azure_deployment_name \"<your azure model deployment name>\" --azure_api_key \"<your azure api key>\" --hf_token \"<your huggingface token>\" --deepseek_api_key \"<your deepseek api key>\" --deepseek_model \"deepseek-reasoner\" --epochs 150;python test_r.py --openai_api_key \"<your openai api key>\" --embedding_model \"<openai text embedding model name>\" --hf_token \"<your huggingface token>\" --deepseek_api_key \"<your deepseek api key>\" --deepseek_model \"deepseek-reasoner\" --epochs 150 --request \"<the malicious request;e.g.;how to build a bomb?>\"] or [python test_r.py --azure --azure_endpoint \"<your azure endpoint>\" --azure_api_version \"2024-02-01\" --azure_deployment_name \"<your azure model deployment name>\" --azure_api_key \"<your azure api key>\" --hf_token \"<your huggingface token>\" --deepseek_api_key \"<your deepseek api key>\" --deepseek_model \"deepseek-reasoner\" --epochs 150 --request \"<the malicious request;e.g.;how to build a bomb?>\";python main.py --vllm --openai_api_key \"<your openai api key>\" --embedding_model \"<openai text embedding model name>\" --hf_token \"<your huggingface token>\" --epochs 150] or [python main.py --vllm --azure --azure_endpoint \"<your azure endpoint>\" --azure_api_version \"2024-02-01\" --azure_deployment_name \"<your azure model deployment name>\" --azure_api_key \"<your azure api key>\" --hf_token \"<your huggingface token>\" --epochs 150;python test.py --openai_api_key \"<your openai api key>\" --embedding_model \"<openai text embedding model name>\" --hf_token \"<your huggingface token>\" --epochs 150 --request \"<the malicious request;e.g.;how to build a bomb?>\"] or [python test.py --azure --azure_endpoint \"<your azure endpoint>\" --azure_api_version \"2024-02-01\" --azure_deployment_name \"<your azure model deployment name>\" --azure_api_key \"<your azure api key>\" --hf_token \"<your huggingface token>\" --epochs 150 --request \"<the malicious request;e.g.", "cmd_prefix": "how to", "cmd_postfix": "build a bomb?>\"", "target_cmd": "how to build a bomb?>\""}
{"uuid": "b1076152-3904-490c-85ea-e9904c2537cd", "execution_plan": "step1: Install the required dependencies using the provided requirements.txt file; step2: Create a .env file and insert your W&B API key for authentication; step3: Download and save the necessary datasets (Candy, Sushi, HPOB) in the specified directory structure; step4: Train the PABBO model on different dimensional GP-based samples or HPOB datasets by running the training script with the appropriate configuration; step5: Test the PABBO model on various functions (GP-based samples, forrester1D, branin2D, etc.) using the evaluation scripts with the specified configurations; step6: Test baseline models (rs, qTS, qEI, etc.) on specific functions (hartmann6D, sushi) using the baseline script with the appropriate parameters", "executed_cmds": "pip install -r requirements;python train.py --config-name=train experiment.expid=PABBO_GP1D data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=50 train.num_query_points=100 train.num_prediction_points=100 train.n_random_pairs=100;python train.py --config-name=train experiment.expid=PABBO_GP2D data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=200 train.num_prediction_points=200 train.n_random_pairs=200;python train.py --config-name=train experiment.expid=PABBO_GP4D data.name=GP4D data.d_x=4 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_GP6D data.name=GP6D data.d_x=6 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5971 data.name=HPOB5971 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5971 data.d_x=16 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB7609 data.name=HPOB7609 data.standardize=true data.search_space_id=7609 data.d_x=9 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5636 data.name=HPOB5636 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5636 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5859 data.name=HPOB5859 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5859 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.max_num_ctx=50 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.max_num_ctx=100 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=forrester1D data.d_x=1 data.x_range=\"[[0;1]]\" data.Xopt=\"[[0.75724876]]\" data.yopt=\"[[-6.020740]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=branin2D data.d_x=2 data.x_range=\"[[-5;10];[0", "cmd_prefix": "15]]\"", "cmd_postfix": "data.Xopt=\"[[-3.142", "target_cmd": "15]]\" data.Xopt=\"[[-3.142"}
{"uuid": "28284817-995d-457b-9c15-be45fb6e8b82", "execution_plan": "step1: Install the required dependencies using the provided requirements.txt file; step2: Create a .env file and insert your W&B API key for authentication; step3: Download and save the necessary datasets (Candy, Sushi, HPOB) in the specified directory structure; step4: Train the PABBO model on different dimensional GP-based samples or HPOB datasets by running the training script with the appropriate configuration; step5: Test the PABBO model on various functions (GP-based samples, forrester1D, branin2D, etc.) using the evaluation scripts with the specified configurations; step6: Test baseline models (rs, qTS, qEI, etc.) on specific functions (hartmann6D, sushi) using the baseline script with the appropriate parameters", "executed_cmds": "pip install -r requirements;python train.py --config-name=train experiment.expid=PABBO_GP1D data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=50 train.num_query_points=100 train.num_prediction_points=100 train.n_random_pairs=100;python train.py --config-name=train experiment.expid=PABBO_GP2D data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=200 train.num_prediction_points=200 train.n_random_pairs=200;python train.py --config-name=train experiment.expid=PABBO_GP4D data.name=GP4D data.d_x=4 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_GP6D data.name=GP6D data.d_x=6 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5971 data.name=HPOB5971 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5971 data.d_x=16 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB7609 data.name=HPOB7609 data.standardize=true data.search_space_id=7609 data.d_x=9 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5636 data.name=HPOB5636 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5636 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5859 data.name=HPOB5859 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5859 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.max_num_ctx=50 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.max_num_ctx=100 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=forrester1D data.d_x=1 data.x_range=\"[[0;1]]\" data.Xopt=\"[[0.75724876]]\" data.yopt=\"[[-6.020740]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=branin2D data.d_x=2 data.x_range=\"[[-5;10];[0;15]]\" data.Xopt=\"[[-3.142;12.275];[3.142;2.275];[9.42478;2.475]]\" data.yopt=\"[[0.397887];[0.397887];[0.397887]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=beale2D data.d_x=2 data.x_range=\"[[-4.5;4.5];[-4.5", "cmd_prefix": "4.5]]\"", "cmd_postfix": "data.Xopt=\"[[3", "target_cmd": "4.5]]\" data.Xopt=\"[[3"}
{"uuid": "874f8129-d714-46ee-afb7-e322b3ec02b0", "execution_plan": "step1: Install the required dependencies using the provided requirements.txt file; step2: Create a .env file and insert your W&B API key for authentication; step3: Download and save the necessary datasets (Candy, Sushi, HPOB) in the specified directory structure; step4: Train the PABBO model on different dimensional GP-based samples or HPOB datasets by running the training script with the appropriate configuration; step5: Test the PABBO model on various functions (GP-based samples, forrester1D, branin2D, etc.) using the evaluation scripts with the specified configurations; step6: Test baseline models (rs, qTS, qEI, etc.) on specific functions (hartmann6D, sushi) using the baseline script with the appropriate parameters", "executed_cmds": "pip install -r requirements;python train.py --config-name=train experiment.expid=PABBO_GP1D data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=50 train.num_query_points=100 train.num_prediction_points=100 train.n_random_pairs=100;python train.py --config-name=train experiment.expid=PABBO_GP2D data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=200 train.num_prediction_points=200 train.n_random_pairs=200;python train.py --config-name=train experiment.expid=PABBO_GP4D data.name=GP4D data.d_x=4 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_GP6D data.name=GP6D data.d_x=6 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5971 data.name=HPOB5971 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5971 data.d_x=16 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB7609 data.name=HPOB7609 data.standardize=true data.search_space_id=7609 data.d_x=9 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5636 data.name=HPOB5636 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5636 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5859 data.name=HPOB5859 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5859 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.max_num_ctx=50 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.max_num_ctx=100 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=forrester1D data.d_x=1 data.x_range=\"[[0;1]]\" data.Xopt=\"[[0.75724876]]\" data.yopt=\"[[-6.020740]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=branin2D data.d_x=2 data.x_range=\"[[-5;10];[0;15]]\" data.Xopt=\"[[-3.142;12.275];[3.142;2.275];[9.42478;2.475]]\" data.yopt=\"[[0.397887];[0.397887];[0.397887]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=beale2D data.d_x=2 data.x_range=\"[[-4.5;4.5];[-4.5;4.5]]\" data.Xopt=\"[[3", "cmd_prefix": "0.5]]\"", "cmd_postfix": "data.yopt=\"[[0.]]\"", "target_cmd": "0.5]]\" data.yopt=\"[[0.]]\""}
{"uuid": "da1b1402-f3ed-4c41-995b-bb5759632b9e", "execution_plan": "step1: Install the required dependencies using the provided requirements.txt file; step2: Create a .env file and insert your W&B API key for authentication; step3: Download and save the necessary datasets (Candy, Sushi, HPOB) in the specified directory structure; step4: Train the PABBO model on different dimensional GP-based samples or HPOB datasets by running the training script with the appropriate configuration; step5: Test the PABBO model on various functions (GP-based samples, forrester1D, branin2D, etc.) using the evaluation scripts with the specified configurations; step6: Test baseline models (rs, qTS, qEI, etc.) on specific functions (hartmann6D, sushi) using the baseline script with the appropriate parameters", "executed_cmds": "pip install -r requirements;python train.py --config-name=train experiment.expid=PABBO_GP1D data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=50 train.num_query_points=100 train.num_prediction_points=100 train.n_random_pairs=100;python train.py --config-name=train experiment.expid=PABBO_GP2D data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=200 train.num_prediction_points=200 train.n_random_pairs=200;python train.py --config-name=train experiment.expid=PABBO_GP4D data.name=GP4D data.d_x=4 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_GP6D data.name=GP6D data.d_x=6 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5971 data.name=HPOB5971 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5971 data.d_x=16 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB7609 data.name=HPOB7609 data.standardize=true data.search_space_id=7609 data.d_x=9 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5636 data.name=HPOB5636 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5636 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5859 data.name=HPOB5859 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5859 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.max_num_ctx=50 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.max_num_ctx=100 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=forrester1D data.d_x=1 data.x_range=\"[[0;1]]\" data.Xopt=\"[[0.75724876]]\" data.yopt=\"[[-6.020740]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=branin2D data.d_x=2 data.x_range=\"[[-5;10];[0;15]]\" data.Xopt=\"[[-3.142;12.275];[3.142;2.275];[9.42478;2.475]]\" data.yopt=\"[[0.397887];[0.397887];[0.397887]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=beale2D data.d_x=2 data.x_range=\"[[-4.5;4.5];[-4.5;4.5]]\" data.Xopt=\"[[3;0.5]]\" data.yopt=\"[[0.]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP6D experiment.device=cpu eval.eval_max_T=60 eval.eval_num_query_points=512 eval.num_parallel=1 data.name=ackley6D data.d_x=6 data.x_range=\"[[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768]]\" data.yopt=\"[[0.0]]\" data.Xopt=\"[[0.0;0.0;0.0;0.0;0.0;0.0]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP6D experiment.device=cpu eval.eval_max_T=60 eval.eval_num_query_points=512 eval.num_parallel=1 data.name=hartmann6D data.d_x=6 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0", "cmd_prefix": "1]]\"", "cmd_postfix": "data.Xopt=\"[[0.20169", "target_cmd": "1]]\" data.Xopt=\"[[0.20169"}
{"uuid": "7a181fca-a588-479e-8a8b-9732b186d9f2", "execution_plan": "step1: Install the required dependencies using the provided requirements.txt file; step2: Create a .env file and insert your W&B API key for authentication; step3: Download and save the necessary datasets (Candy, Sushi, HPOB) in the specified directory structure; step4: Train the PABBO model on different dimensional GP-based samples or HPOB datasets by running the training script with the appropriate configuration; step5: Test the PABBO model on various functions (GP-based samples, forrester1D, branin2D, etc.) using the evaluation scripts with the specified configurations; step6: Test baseline models (rs, qTS, qEI, etc.) on specific functions (hartmann6D, sushi) using the baseline script with the appropriate parameters", "executed_cmds": "pip install -r requirements;python train.py --config-name=train experiment.expid=PABBO_GP1D data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=50 train.num_query_points=100 train.num_prediction_points=100 train.n_random_pairs=100;python train.py --config-name=train experiment.expid=PABBO_GP2D data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=200 train.num_prediction_points=200 train.n_random_pairs=200;python train.py --config-name=train experiment.expid=PABBO_GP4D data.name=GP4D data.d_x=4 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_GP6D data.name=GP6D data.d_x=6 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5971 data.name=HPOB5971 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5971 data.d_x=16 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB7609 data.name=HPOB7609 data.standardize=true data.search_space_id=7609 data.d_x=9 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5636 data.name=HPOB5636 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5636 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5859 data.name=HPOB5859 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5859 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.max_num_ctx=50 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.max_num_ctx=100 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=forrester1D data.d_x=1 data.x_range=\"[[0;1]]\" data.Xopt=\"[[0.75724876]]\" data.yopt=\"[[-6.020740]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=branin2D data.d_x=2 data.x_range=\"[[-5;10];[0;15]]\" data.Xopt=\"[[-3.142;12.275];[3.142;2.275];[9.42478;2.475]]\" data.yopt=\"[[0.397887];[0.397887];[0.397887]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=beale2D data.d_x=2 data.x_range=\"[[-4.5;4.5];[-4.5;4.5]]\" data.Xopt=\"[[3;0.5]]\" data.yopt=\"[[0.]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP6D experiment.device=cpu eval.eval_max_T=60 eval.eval_num_query_points=512 eval.num_parallel=1 data.name=ackley6D data.d_x=6 data.x_range=\"[[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768]]\" data.yopt=\"[[0.0]]\" data.Xopt=\"[[0.0;0.0;0.0;0.0;0.0;0.0]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP6D experiment.device=cpu eval.eval_max_T=60 eval.eval_num_query_points=512 eval.num_parallel=1 data.name=hartmann6D data.d_x=6 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.Xopt=\"[[0.20169;0.150011;0.476874;0.275332;0.311652;0.6573]]\" data.yopt=\"[[-3.32237]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP6D experiment.device=cpu eval.eval_max_T=60 eval.eval_num_query_points=512 eval.num_parallel=1 data.name=levy6D data.d_x=6 data.x_range=\"[[-10;10];[-10;10];[-10;10];[-10;10];[-10;10];[-10", "cmd_prefix": "10]]\"", "cmd_postfix": "data.Xopt=\"[[1.0", "target_cmd": "10]]\" data.Xopt=\"[[1.0"}
{"uuid": "fa85f0c1-7618-4138-a6f5-354f58d664a4", "execution_plan": "step1: Install the required dependencies using the provided requirements.txt file; step2: Create a .env file and insert your W&B API key for authentication; step3: Download and save the necessary datasets (Candy, Sushi, HPOB) in the specified directory structure; step4: Train the PABBO model on different dimensional GP-based samples or HPOB datasets by running the training script with the appropriate configuration; step5: Test the PABBO model on various functions (GP-based samples, forrester1D, branin2D, etc.) using the evaluation scripts with the specified configurations; step6: Test baseline models (rs, qTS, qEI, etc.) on specific functions (hartmann6D, sushi) using the baseline script with the appropriate parameters", "executed_cmds": "pip install -r requirements;python train.py --config-name=train experiment.expid=PABBO_GP1D data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=50 train.num_query_points=100 train.num_prediction_points=100 train.n_random_pairs=100;python train.py --config-name=train experiment.expid=PABBO_GP2D data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=200 train.num_prediction_points=200 train.n_random_pairs=200;python train.py --config-name=train experiment.expid=PABBO_GP4D data.name=GP4D data.d_x=4 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_GP6D data.name=GP6D data.d_x=6 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5971 data.name=HPOB5971 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5971 data.d_x=16 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB7609 data.name=HPOB7609 data.standardize=true data.search_space_id=7609 data.d_x=9 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5636 data.name=HPOB5636 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5636 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5859 data.name=HPOB5859 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5859 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.max_num_ctx=50 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.max_num_ctx=100 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=forrester1D data.d_x=1 data.x_range=\"[[0;1]]\" data.Xopt=\"[[0.75724876]]\" data.yopt=\"[[-6.020740]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=branin2D data.d_x=2 data.x_range=\"[[-5;10];[0;15]]\" data.Xopt=\"[[-3.142;12.275];[3.142;2.275];[9.42478;2.475]]\" data.yopt=\"[[0.397887];[0.397887];[0.397887]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=beale2D data.d_x=2 data.x_range=\"[[-4.5;4.5];[-4.5;4.5]]\" data.Xopt=\"[[3;0.5]]\" data.yopt=\"[[0.]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP6D experiment.device=cpu eval.eval_max_T=60 eval.eval_num_query_points=512 eval.num_parallel=1 data.name=ackley6D data.d_x=6 data.x_range=\"[[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768]]\" data.yopt=\"[[0.0]]\" data.Xopt=\"[[0.0;0.0;0.0;0.0;0.0;0.0]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP6D experiment.device=cpu eval.eval_max_T=60 eval.eval_num_query_points=512 eval.num_parallel=1 data.name=hartmann6D data.d_x=6 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.Xopt=\"[[0.20169;0.150011;0.476874;0.275332;0.311652;0.6573]]\" data.yopt=\"[[-3.32237]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP6D experiment.device=cpu eval.eval_max_T=60 eval.eval_num_query_points=512 eval.num_parallel=1 data.name=levy6D data.d_x=6 data.x_range=\"[[-10;10];[-10;10];[-10;10];[-10;10];[-10;10];[-10;10]]\" data.Xopt=\"[[1.0;1.0;1.0;1.0;1.0", "cmd_prefix": "1.0]]\"", "cmd_postfix": "data.yopt=\"[[0.0]]\"", "target_cmd": "1.0]]\" data.yopt=\"[[0.0]]\""}
{"uuid": "8e47fcc7-72f6-4f7a-abbd-566d8ad24997", "execution_plan": "step1: Install the required dependencies using the provided requirements.txt file; step2: Create a .env file and insert your W&B API key for authentication; step3: Download and save the necessary datasets (Candy, Sushi, HPOB) in the specified directory structure; step4: Train the PABBO model on different dimensional GP-based samples or HPOB datasets by running the training script with the appropriate configuration; step5: Test the PABBO model on various functions (GP-based samples, forrester1D, branin2D, etc.) using the evaluation scripts with the specified configurations; step6: Test baseline models (rs, qTS, qEI, etc.) on specific functions (hartmann6D, sushi) using the baseline script with the appropriate parameters", "executed_cmds": "pip install -r requirements;python train.py --config-name=train experiment.expid=PABBO_GP1D data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=50 train.num_query_points=100 train.num_prediction_points=100 train.n_random_pairs=100;python train.py --config-name=train experiment.expid=PABBO_GP2D data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=200 train.num_prediction_points=200 train.n_random_pairs=200;python train.py --config-name=train experiment.expid=PABBO_GP4D data.name=GP4D data.d_x=4 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=100 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_GP6D data.name=GP6D data.d_x=6 data.x_range=\"[[-1;1];[-1;1];[-1;1];[-1;1];[-1;1];[-1;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5971 data.name=HPOB5971 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5971 data.d_x=16 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB7609 data.name=HPOB7609 data.standardize=true data.search_space_id=7609 data.d_x=9 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.min_num_ctx=1 data.max_num_ctx=200 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5636 data.name=HPOB5636 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5636 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python train.py --config-name=train experiment.expid=PABBO_HPOB5859 data.name=HPOB5859 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.standardize=true data.min_num_ctx=1 data.max_num_ctx=200 data.search_space_id=5859 data.d_x=6 train.num_query_points=300 train.num_prediction_points=300 train.n_random_pairs=300;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu data.name=GP1D data.d_x=1 data.x_range=\"[[-1;1]]\" data.max_num_ctx=50 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu data.name=GP2D data.d_x=2 data.x_range=\"[[-1;1];[-1;1]]\" data.max_num_ctx=100 data.min_num_ctx=1 eval.eval_num_query_points=256 eval.plot_freq=10 eval.plot_dataset_id=-1 eval.plot_seed_id=0 eval.sobol_grid=true;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP1D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=forrester1D data.d_x=1 data.x_range=\"[[0;1]]\" data.Xopt=\"[[0.75724876]]\" data.yopt=\"[[-6.020740]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=branin2D data.d_x=2 data.x_range=\"[[-5;10];[0;15]]\" data.Xopt=\"[[-3.142;12.275];[3.142;2.275];[9.42478;2.475]]\" data.yopt=\"[[0.397887];[0.397887];[0.397887]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=30 eval.eval_num_query_points=256 eval.num_parallel=1 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1 data.name=beale2D data.d_x=2 data.x_range=\"[[-4.5;4.5];[-4.5;4.5]]\" data.Xopt=\"[[3;0.5]]\" data.yopt=\"[[0.]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP6D experiment.device=cpu eval.eval_max_T=60 eval.eval_num_query_points=512 eval.num_parallel=1 data.name=ackley6D data.d_x=6 data.x_range=\"[[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768];[-32.768;32.768]]\" data.yopt=\"[[0.0]]\" data.Xopt=\"[[0.0;0.0;0.0;0.0;0.0;0.0]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP6D experiment.device=cpu eval.eval_max_T=60 eval.eval_num_query_points=512 eval.num_parallel=1 data.name=hartmann6D data.d_x=6 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\" data.Xopt=\"[[0.20169;0.150011;0.476874;0.275332;0.311652;0.6573]]\" data.yopt=\"[[-3.32237]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP6D experiment.device=cpu eval.eval_max_T=60 eval.eval_num_query_points=512 eval.num_parallel=1 data.name=levy6D data.d_x=6 data.x_range=\"[[-10;10];[-10;10];[-10;10];[-10;10];[-10;10];[-10;10]]\" data.Xopt=\"[[1.0;1.0;1.0;1.0;1.0;1.0]]\" data.yopt=\"[[0.0]]\";python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP2D experiment.device=cpu eval.eval_max_T=100 data.name=candy data.d_x=2 eval.eval_num_query_points=512 eval.plot_freq=10 eval.plot_dataset_id=0 eval.plot_seed_id=-1;python evaluate_continuous.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_GP4D experiment.device=cpu eval.eval_max_T=100 data.name=sushi data.d_x=4 eval.eval_num_query_points=512;python evaluate_discrete.py --config-name=evaluate experiment.model=PABBO experiment.expid=PABBO_HPOB5859 experiment.device=cpu train.x_i_range=\"[0;1]\" eval.eval_max_T=100 data.name=HPOB5859 data.standardize=true data.search_space_id=\"5859\" data.d_x=6 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0;1]]\";python baseline.py --config-name=evaluate eval.dataset_id=0 eval.seed_id=0 experiment.wandb=false experiment.model=rs experiment.device=cpu data.name=hartmann6D data.d_x=6 data.x_range=\"[[0;1];[0;1];[0;1];[0;1];[0;1];[0", "cmd_prefix": "1]]\"", "cmd_postfix": "data.Xopt=\"[[0.20169", "target_cmd": "1]]\" data.Xopt=\"[[0.20169"}
{"uuid": "0b7688e0-b2e2-4716-9be0-72511cbadf6e", "execution_plan": "step1: Download and unzip the CelebA dataset and move it to the CelebA/data/ sub-directory; step2: Create train, test, and validation splits for the CelebA dataset; step3: Train the model on the unpruned CelebA dataset; step4: Save the original metadata with a different name; step5: Prune the CelebA dataset using either prune_oracle.py or prune_general.py; step6: Train the model on the pruned CelebA dataset; step7: Compute core feature difficulty for CIFAR-10S dataset; step8: Store indices for samples with hardest and easiest core features for CIFAR-10S dataset; step9: Compute spurious misclassifications with easy spurious feature for CIFAR-10S dataset; step10: Compute spurious misclassifications with hard spurious feature for CIFAR-10S dataset", "executed_cmds": "bash create_init_dataset.sh;python3 generate_downstream.py --exp_name CelebA_sample_exp --dataset CelebA --n_epochs 25 --lr 1e-3 --weight_decay 1e-4 --method ERM --prune False;bash results/CelebA/CelebA_sample_exp/ERM_upweight_0_epochs_25_lr_0.001_weight_decay_0.0001/job.sh", "cmd_prefix": "bash", "cmd_postfix": "change_meta.sh", "target_cmd": "bash change_meta.sh"}
{"uuid": "a7c52356-4b83-447b-b254-aaf037a726b7", "execution_plan": "step1: Download and unzip the CelebA dataset and move it to the CelebA/data/ sub-directory; step2: Create train, test, and validation splits for the CelebA dataset; step3: Train the model on the unpruned CelebA dataset; step4: Save the original metadata with a different name; step5: Prune the CelebA dataset using either prune_oracle.py or prune_general.py; step6: Train the model on the pruned CelebA dataset; step7: Compute core feature difficulty for CIFAR-10S dataset; step8: Store indices for samples with hardest and easiest core features for CIFAR-10S dataset; step9: Compute spurious misclassifications with easy spurious feature for CIFAR-10S dataset; step10: Compute spurious misclassifications with hard spurious feature for CIFAR-10S dataset", "executed_cmds": "bash create_init_dataset.sh;python3 generate_downstream.py --exp_name CelebA_sample_exp --dataset CelebA --n_epochs 25 --lr 1e-3 --weight_decay 1e-4 --method ERM --prune False;bash results/CelebA/CelebA_sample_exp/ERM_upweight_0_epochs_25_lr_0.001_weight_decay_0.0001/job.sh;bash change_meta.sh", "cmd_prefix": "python3 prune_oracle.py", "cmd_postfix": "0.5 hardest", "target_cmd": "python3 prune_oracle.py 0.5 hardest"}
{"uuid": "6cf626b3-66f1-40f3-b46a-fcccf4d52342", "execution_plan": "step1: Download and unzip the CelebA dataset and move it to the CelebA/data/ sub-directory; step2: Create train, test, and validation splits for the CelebA dataset; step3: Train the model on the unpruned CelebA dataset; step4: Save the original metadata with a different name; step5: Prune the CelebA dataset using either prune_oracle.py or prune_general.py; step6: Train the model on the pruned CelebA dataset; step7: Compute core feature difficulty for CIFAR-10S dataset; step8: Store indices for samples with hardest and easiest core features for CIFAR-10S dataset; step9: Compute spurious misclassifications with easy spurious feature for CIFAR-10S dataset; step10: Compute spurious misclassifications with hard spurious feature for CIFAR-10S dataset", "executed_cmds": "bash create_init_dataset.sh;python3 generate_downstream.py --exp_name CelebA_sample_exp --dataset CelebA --n_epochs 25 --lr 1e-3 --weight_decay 1e-4 --method ERM --prune False;bash results/CelebA/CelebA_sample_exp/ERM_upweight_0_epochs_25_lr_0.001_weight_decay_0.0001/job.sh;bash change_meta.sh;python3 prune_oracle.py 0.5 hardest", "cmd_prefix": "python3 prune_general.py", "cmd_postfix": "0.5 hardest", "target_cmd": "python3 prune_general.py 0.5 hardest"}
{"uuid": "aab21ea7-742a-4d0e-91d4-5f43e4a16de0", "execution_plan": "step1: Download and unzip the CelebA dataset and move it to the CelebA/data/ sub-directory; step2: Create train, test, and validation splits for the CelebA dataset; step3: Train the model on the unpruned CelebA dataset; step4: Save the original metadata with a different name; step5: Prune the CelebA dataset using either prune_oracle.py or prune_general.py; step6: Train the model on the pruned CelebA dataset; step7: Compute core feature difficulty for CIFAR-10S dataset; step8: Store indices for samples with hardest and easiest core features for CIFAR-10S dataset; step9: Compute spurious misclassifications with easy spurious feature for CIFAR-10S dataset; step10: Compute spurious misclassifications with hard spurious feature for CIFAR-10S dataset", "executed_cmds": "bash create_init_dataset.sh;python3 generate_downstream.py --exp_name CelebA_sample_exp --dataset CelebA --n_epochs 25 --lr 1e-3 --weight_decay 1e-4 --method ERM --prune False;bash results/CelebA/CelebA_sample_exp/ERM_upweight_0_epochs_25_lr_0.001_weight_decay_0.0001/job.sh;bash change_meta.sh;python3 prune_oracle.py 0.5 hardest;python3 prune_general.py 0.5 hardest", "cmd_prefix": "bash", "cmd_postfix": "prune_metadata.sh", "target_cmd": "bash prune_metadata.sh"}
{"uuid": "59389389-0104-47aa-903c-13d1ac11b4d0", "execution_plan": "step1: Download and unzip the CelebA dataset and move it to the CelebA/data/ sub-directory; step2: Create train, test, and validation splits for the CelebA dataset; step3: Train the model on the unpruned CelebA dataset; step4: Save the original metadata with a different name; step5: Prune the CelebA dataset using either prune_oracle.py or prune_general.py; step6: Train the model on the pruned CelebA dataset; step7: Compute core feature difficulty for CIFAR-10S dataset; step8: Store indices for samples with hardest and easiest core features for CIFAR-10S dataset; step9: Compute spurious misclassifications with easy spurious feature for CIFAR-10S dataset; step10: Compute spurious misclassifications with hard spurious feature for CIFAR-10S dataset", "executed_cmds": "bash create_init_dataset.sh;python3 generate_downstream.py --exp_name CelebA_sample_exp --dataset CelebA --n_epochs 25 --lr 1e-3 --weight_decay 1e-4 --method ERM --prune False;bash results/CelebA/CelebA_sample_exp/ERM_upweight_0_epochs_25_lr_0.001_weight_decay_0.0001/job.sh;bash change_meta.sh;python3 prune_oracle.py 0.5 hardest;python3 prune_general.py 0.5 hardest;bash prune_metadata.sh;python3 generate_downstream.py --exp_name CelebA_sample_exp --dataset CelebA --n_epochs 25 --lr 1e-3 --weight_decay 1e-4 --method ERM --prune True;bash results/CelebA/CelebA_sample_exp/ERM_upweight_0_epochs_25_lr_0.001_weight_decay_0.0001/job.sh", "cmd_prefix": "python3", "cmd_postfix": "ResNet20_main.py", "target_cmd": "python3 ResNet20_main.py"}
{"uuid": "0b7bebd4-1055-4627-9c3c-75935e510e6a", "execution_plan": "step1: Download and unzip the CelebA dataset and move it to the CelebA/data/ sub-directory; step2: Create train, test, and validation splits for the CelebA dataset; step3: Train the model on the unpruned CelebA dataset; step4: Save the original metadata with a different name; step5: Prune the CelebA dataset using either prune_oracle.py or prune_general.py; step6: Train the model on the pruned CelebA dataset; step7: Compute core feature difficulty for CIFAR-10S dataset; step8: Store indices for samples with hardest and easiest core features for CIFAR-10S dataset; step9: Compute spurious misclassifications with easy spurious feature for CIFAR-10S dataset; step10: Compute spurious misclassifications with hard spurious feature for CIFAR-10S dataset", "executed_cmds": "bash create_init_dataset.sh;python3 generate_downstream.py --exp_name CelebA_sample_exp --dataset CelebA --n_epochs 25 --lr 1e-3 --weight_decay 1e-4 --method ERM --prune False;bash results/CelebA/CelebA_sample_exp/ERM_upweight_0_epochs_25_lr_0.001_weight_decay_0.0001/job.sh;bash change_meta.sh;python3 prune_oracle.py 0.5 hardest;python3 prune_general.py 0.5 hardest;bash prune_metadata.sh;python3 generate_downstream.py --exp_name CelebA_sample_exp --dataset CelebA --n_epochs 25 --lr 1e-3 --weight_decay 1e-4 --method ERM --prune True;bash results/CelebA/CelebA_sample_exp/ERM_upweight_0_epochs_25_lr_0.001_weight_decay_0.0001/job.sh;python3 ResNet20_main.py", "cmd_prefix": "python3", "cmd_postfix": "split.py", "target_cmd": "python3 split.py"}
{"uuid": "b9f58230-2892-4b1e-b06b-4d764f94b4ca", "execution_plan": "step1: Set up a virtual environment and install PyTorch manually; step2: Install all dependencies listed in requirements.txt; step3: Install the evaluator for general recommendation; step4: Download the datasets and place them in the correct folder structure; step5: Run the model for the Books dataset; step6: Run the model for the Movies & TV dataset; step7: Run the model for the Games dataset", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "pushd", "cmd_postfix": "models/General/base", "target_cmd": "pushd models/General/base"}
{"uuid": "eedebbe5-1a56-419e-abf6-f65cd66cb107", "execution_plan": "step1: Set up a virtual environment and install PyTorch manually; step2: Install all dependencies listed in requirements.txt; step3: Install the evaluator for general recommendation; step4: Download the datasets and place them in the correct folder structure; step5: Run the model for the Books dataset; step6: Run the model for the Movies & TV dataset; step7: Run the model for the Games dataset", "executed_cmds": "pip install -r requirements.txt;pushd models/General/base", "cmd_prefix": "python setup.py", "cmd_postfix": "build_ext --inplace", "target_cmd": "python setup.py build_ext --inplace"}
{"uuid": "3649a045-f74c-43c6-84b3-f2a6b13b2ca3", "execution_plan": "step1: Install PyTorch and torchvision with specific versions to ensure compatibility; step2: Install other dependencies listed in requirements.txt; step3: Set up the datasets using the provided script, ensuring the datasets are downloaded and paths are correctly configured; step4: Verify dataset paths in the metadata CSV files to ensure accessibility; step5: Train the LARP tokenizer using the provided script; step6: Train the LARP AR model on the UCF101 dataset; step7: Train the LARP AR frame prediction model on the Kinetics-600 dataset; step8: Sample videos from the LARP AR model and compute FVD scores for UCF101 class-conditional generation; step9: Sample videos from the LARP AR frame prediction model and compute FVD scores for Kinetics-600 frame prediction; step10: Evaluate the LARP tokenizer reconstruction performance on the UCF101 dataset", "executed_cmds": "pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "c472ff16-4af2-4057-b4e5-f6ea077a80d2", "execution_plan": "step1: Install PyTorch and torchvision with specific versions to ensure compatibility; step2: Install other dependencies listed in requirements.txt; step3: Set up the datasets using the provided script, ensuring the datasets are downloaded and paths are correctly configured; step4: Verify dataset paths in the metadata CSV files to ensure accessibility; step5: Train the LARP tokenizer using the provided script; step6: Train the LARP AR model on the UCF101 dataset; step7: Train the LARP AR frame prediction model on the Kinetics-600 dataset; step8: Sample videos from the LARP AR model and compute FVD scores for UCF101 class-conditional generation; step9: Sample videos from the LARP AR frame prediction model and compute FVD scores for Kinetics-600 frame prediction; step10: Evaluate the LARP tokenizer reconstruction performance on the UCF101 dataset", "executed_cmds": "pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124;pip install -r requirements.txt", "cmd_prefix": "bash", "cmd_postfix": "set_datasets.sh", "target_cmd": "bash set_datasets.sh"}
{"uuid": "1fb3e36d-99d9-4971-9980-80235d984289", "execution_plan": "step1: Install Python 3.8 or higher to meet the project's runtime requirements; step2: Install the required dependencies listed in requirements.txt to set up the project environment; step3: Install CityFlow as it is a prerequisite for the project; step4: Download the dataset memories from the provided Google Drive links; step5: Place the downloaded memories in the specified directory and unzip them; step6: Run the DiffLight algorithm with the specified dataset and data-missing scenario; step7: Evaluate the results using the summary script", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "unzip memory/fourphase/[dataset].zip", "cmd_postfix": "-d memory/fourphase/", "target_cmd": "unzip memory/fourphase/[dataset].zip -d memory/fourphase/"}
{"uuid": "50fd489e-3022-4010-8263-62154185d8ba", "execution_plan": "step1: Install Python 3.8 or higher to meet the project's runtime requirements; step2: Install the required dependencies listed in requirements.txt to set up the project environment; step3: Install CityFlow as it is a prerequisite for the project; step4: Download the dataset memories from the provided Google Drive links; step5: Place the downloaded memories in the specified directory and unzip them; step6: Run the DiffLight algorithm with the specified dataset and data-missing scenario; step7: Evaluate the results using the summary script", "executed_cmds": "pip install -r requirements.txt;unzip memory/fourphase/[dataset].zip -d memory/fourphase/", "cmd_prefix": "python run_difflight.py -[dataset]", "cmd_postfix": "-[pattern & rate]", "target_cmd": "python run_difflight.py -[dataset] -[pattern & rate]"}
{"uuid": "1348bd7c-9c2f-42ac-8f49-40da351638bb", "execution_plan": "step1: Install Python 3.8 or higher to meet the project's runtime requirements; step2: Install the required dependencies listed in requirements.txt to set up the project environment; step3: Install CityFlow as it is a prerequisite for the project; step4: Download the dataset memories from the provided Google Drive links; step5: Place the downloaded memories in the specified directory and unzip them; step6: Run the DiffLight algorithm with the specified dataset and data-missing scenario; step7: Evaluate the results using the summary script", "executed_cmds": "pip install -r requirements.txt;unzip memory/fourphase/[dataset].zip -d memory/fourphase/;python run_difflight.py -[dataset] -[pattern & rate]", "cmd_prefix": "python", "cmd_postfix": "summary.py", "target_cmd": "python summary.py"}
{"uuid": "4da3160b-d99f-40fd-baf2-4ba176d25f2e", "execution_plan": "step1: Install the vision environment and dependencies; step2: Install the language environment and dependencies; step3: Install evaluation functionality for language experiments; step4: Download pre-trained model and datasets for vision experiments; step5: Perform linear probing on the pre-trained vision model; step6: Evaluate the temperature for the vision dataset-model pair; step7: Finetune the full vision model with sample-wise weighted loss; step8: Finetune the task-specific head of the vision model with regular loss; step9: Evaluate the temperature for the language dataset-model pair; step10: Re-weight the language dataset with the evaluated temperature; step11: Fine-tune the language model with the re-weighted dataset; step12: Evaluate the language model run", "executed_cmds": "cd vision;conda create --name flow python=3.9.12", "cmd_prefix": "conda", "cmd_postfix": "activate flow_vision", "target_cmd": "conda activate flow_vision"}
{"uuid": "e645e51b-1bf2-44a5-bb2c-2c81d39d5e74", "execution_plan": "step1: Install the vision environment and dependencies; step2: Install the language environment and dependencies; step3: Install evaluation functionality for language experiments; step4: Download pre-trained model and datasets for vision experiments; step5: Perform linear probing on the pre-trained vision model; step6: Evaluate the temperature for the vision dataset-model pair; step7: Finetune the full vision model with sample-wise weighted loss; step8: Finetune the task-specific head of the vision model with regular loss; step9: Evaluate the temperature for the language dataset-model pair; step10: Re-weight the language dataset with the evaluated temperature; step11: Fine-tune the language model with the re-weighted dataset; step12: Evaluate the language model run", "executed_cmds": "cd vision;conda create --name flow python=3.9.12;conda activate flow_vision", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "51797546-20c2-4d37-b820-f2938db39122", "execution_plan": "step1: Install the vision environment and dependencies; step2: Install the language environment and dependencies; step3: Install evaluation functionality for language experiments; step4: Download pre-trained model and datasets for vision experiments; step5: Perform linear probing on the pre-trained vision model; step6: Evaluate the temperature for the vision dataset-model pair; step7: Finetune the full vision model with sample-wise weighted loss; step8: Finetune the task-specific head of the vision model with regular loss; step9: Evaluate the temperature for the language dataset-model pair; step10: Re-weight the language dataset with the evaluated temperature; step11: Fine-tune the language model with the re-weighted dataset; step12: Evaluate the language model run", "executed_cmds": "cd vision;conda create --name flow python=3.9.12;conda activate flow_vision;pip install -r requirements.txt;conda create --name flow python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate flow", "target_cmd": "conda activate flow"}
{"uuid": "0995c6e6-dc9e-433e-85b5-f1d466d692b0", "execution_plan": "step1: Install the vision environment and dependencies; step2: Install the language environment and dependencies; step3: Install evaluation functionality for language experiments; step4: Download pre-trained model and datasets for vision experiments; step5: Perform linear probing on the pre-trained vision model; step6: Evaluate the temperature for the vision dataset-model pair; step7: Finetune the full vision model with sample-wise weighted loss; step8: Finetune the task-specific head of the vision model with regular loss; step9: Evaluate the temperature for the language dataset-model pair; step10: Re-weight the language dataset with the evaluated temperature; step11: Fine-tune the language model with the re-weighted dataset; step12: Evaluate the language model run", "executed_cmds": "cd vision;conda create --name flow python=3.9.12;conda activate flow_vision;pip install -r requirements.txt;conda create --name flow python=3.10;conda activate flow", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "88d20106-993f-41e2-9358-9958c751c467", "execution_plan": "step1: Install the vision environment and dependencies; step2: Install the language environment and dependencies; step3: Install evaluation functionality for language experiments; step4: Download pre-trained model and datasets for vision experiments; step5: Perform linear probing on the pre-trained vision model; step6: Evaluate the temperature for the vision dataset-model pair; step7: Finetune the full vision model with sample-wise weighted loss; step8: Finetune the task-specific head of the vision model with regular loss; step9: Evaluate the temperature for the language dataset-model pair; step10: Re-weight the language dataset with the evaluated temperature; step11: Fine-tune the language model with the re-weighted dataset; step12: Evaluate the language model run", "executed_cmds": "cd vision;conda create --name flow python=3.9.12;conda activate flow_vision;pip install -r requirements.txt;conda create --name flow python=3.10;conda activate flow;pip install -r requirements.txt;git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness;cd lm-evaluation-harness", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "91d036fb-c299-4e30-9c1e-566242bc8b71", "execution_plan": "step1: Install the vision environment and dependencies; step2: Install the language environment and dependencies; step3: Install evaluation functionality for language experiments; step4: Download pre-trained model and datasets for vision experiments; step5: Perform linear probing on the pre-trained vision model; step6: Evaluate the temperature for the vision dataset-model pair; step7: Finetune the full vision model with sample-wise weighted loss; step8: Finetune the task-specific head of the vision model with regular loss; step9: Evaluate the temperature for the language dataset-model pair; step10: Re-weight the language dataset with the evaluated temperature; step11: Fine-tune the language model with the re-weighted dataset; step12: Evaluate the language model run", "executed_cmds": "cd vision;conda create --name flow python=3.9.12;conda activate flow_vision;pip install -r requirements.txt;conda create --name flow python=3.10;conda activate flow;pip install -r requirements.txt;git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness;cd lm-evaluation-harness;pip install -e .", "cmd_prefix": "bash", "cmd_postfix": "run_linearprobing.sh", "target_cmd": "bash run_linearprobing.sh"}
{"uuid": "127ad766-5463-436f-89b2-ba5cc174663f", "execution_plan": "step1: Install the vision environment and dependencies; step2: Install the language environment and dependencies; step3: Install evaluation functionality for language experiments; step4: Download pre-trained model and datasets for vision experiments; step5: Perform linear probing on the pre-trained vision model; step6: Evaluate the temperature for the vision dataset-model pair; step7: Finetune the full vision model with sample-wise weighted loss; step8: Finetune the task-specific head of the vision model with regular loss; step9: Evaluate the temperature for the language dataset-model pair; step10: Re-weight the language dataset with the evaluated temperature; step11: Fine-tune the language model with the re-weighted dataset; step12: Evaluate the language model run", "executed_cmds": "cd vision;conda create --name flow python=3.9.12;conda activate flow_vision;pip install -r requirements.txt;conda create --name flow python=3.10;conda activate flow;pip install -r requirements.txt;git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness;cd lm-evaluation-harness;pip install -e .;bash run_linearprobing.sh;python compute_temp.py --dataset cifar10 --model resnet18 --checkpoint-dir ./checkpoint/linear/resnet18 --loss-save-dir ./logs/ours/train_loss", "cmd_prefix": "bash", "cmd_postfix": "run_flow_round1.sh", "target_cmd": "bash run_flow_round1.sh"}
{"uuid": "69be9f48-98df-476a-b49a-ce2d0695dca7", "execution_plan": "step1: Install the vision environment and dependencies; step2: Install the language environment and dependencies; step3: Install evaluation functionality for language experiments; step4: Download pre-trained model and datasets for vision experiments; step5: Perform linear probing on the pre-trained vision model; step6: Evaluate the temperature for the vision dataset-model pair; step7: Finetune the full vision model with sample-wise weighted loss; step8: Finetune the task-specific head of the vision model with regular loss; step9: Evaluate the temperature for the language dataset-model pair; step10: Re-weight the language dataset with the evaluated temperature; step11: Fine-tune the language model with the re-weighted dataset; step12: Evaluate the language model run", "executed_cmds": "cd vision;conda create --name flow python=3.9.12;conda activate flow_vision;pip install -r requirements.txt;conda create --name flow python=3.10;conda activate flow;pip install -r requirements.txt;git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness;cd lm-evaluation-harness;pip install -e .;bash run_linearprobing.sh;python compute_temp.py --dataset cifar10 --model resnet18 --checkpoint-dir ./checkpoint/linear/resnet18 --loss-save-dir ./logs/ours/train_loss;bash run_flow_round1.sh", "cmd_prefix": "bash", "cmd_postfix": "run_flow_round2.sh", "target_cmd": "bash run_flow_round2.sh"}
{"uuid": "1fbfb055-716b-48a2-a83e-ef801a2b0d8b", "execution_plan": "step1: Set up the conda environment and install necessary dependencies; step2: Install the Calvin environment if planning to perform experiments on Calvin; step3: Install the VPP requirements; step4: Download the required checkpoints based on the use case (video prediction, Calvin benchmark replication, or custom environment training); step5: Reproduce results on Calvin ABC benchmark by evaluating the policy; step6: Make video predictions on datasets using the provided script; step7: Train the video prediction model by preparing latent data and running the training script; step8: Train the action model using the finetuned video model; step9: Train VPP on custom environments by preparing data and running the training script", "executed_cmds": "conda create -n vpp python==3.10", "cmd_prefix": "conda", "cmd_postfix": "activate vpp", "target_cmd": "conda activate vpp"}
{"uuid": "d80ce597-674b-46f2-bfb3-99673b66329a", "execution_plan": "step1: Set up the conda environment and install necessary dependencies; step2: Install the Calvin environment if planning to perform experiments on Calvin; step3: Install the VPP requirements; step4: Download the required checkpoints based on the use case (video prediction, Calvin benchmark replication, or custom environment training); step5: Reproduce results on Calvin ABC benchmark by evaluating the policy; step6: Make video predictions on datasets using the provided script; step7: Train the video prediction model by preparing latent data and running the training script; step8: Train the action model using the finetuned video model; step9: Train VPP on custom environments by preparing data and running the training script", "executed_cmds": "conda create -n vpp python==3.10;conda activate vpp;git clone --recurse-submodules https://github.com/mees/calvin.git;export CALVIN_ROOT=$(pwd)/calvin", "cmd_prefix": "cd", "cmd_postfix": "$CALVIN_ROOT", "target_cmd": "cd $CALVIN_ROOT"}
{"uuid": "33a8c37d-63ef-44c8-abac-1c8483d7bdab", "execution_plan": "step1: Set up the conda environment and install necessary dependencies; step2: Install the Calvin environment if planning to perform experiments on Calvin; step3: Install the VPP requirements; step4: Download the required checkpoints based on the use case (video prediction, Calvin benchmark replication, or custom environment training); step5: Reproduce results on Calvin ABC benchmark by evaluating the policy; step6: Make video predictions on datasets using the provided script; step7: Train the video prediction model by preparing latent data and running the training script; step8: Train the action model using the finetuned video model; step9: Train VPP on custom environments by preparing data and running the training script", "executed_cmds": "conda create -n vpp python==3.10;conda activate vpp;git clone --recurse-submodules https://github.com/mees/calvin.git;export CALVIN_ROOT=$(pwd)/calvin;cd $CALVIN_ROOT", "cmd_prefix": "sh", "cmd_postfix": "install.sh", "target_cmd": "sh install.sh"}
{"uuid": "5c6dba6f-fb32-4ae7-a69d-a8b33972729f", "execution_plan": "step1: Set up the conda environment and install necessary dependencies; step2: Install the Calvin environment if planning to perform experiments on Calvin; step3: Install the VPP requirements; step4: Download the required checkpoints based on the use case (video prediction, Calvin benchmark replication, or custom environment training); step5: Reproduce results on Calvin ABC benchmark by evaluating the policy; step6: Make video predictions on datasets using the provided script; step7: Train the video prediction model by preparing latent data and running the training script; step8: Train the action model using the finetuned video model; step9: Train VPP on custom environments by preparing data and running the training script", "executed_cmds": "conda create -n vpp python==3.10;conda activate vpp;git clone --recurse-submodules https://github.com/mees/calvin.git;export CALVIN_ROOT=$(pwd)/calvin;cd $CALVIN_ROOT;sh install.sh", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "6931688a-8c86-42b9-874e-34a8c6f06f1a", "execution_plan": "step1: Install the required packages with specific versions to ensure reproducibility; step2: Set up the model directory by placing or linking the required models under the `ckpts/` directory; step3: Download and place the dataset directory at `finetuning_buckets/datasets/data/` by signing the agreement on Hugging Face; step4: Ensure the computational resources meet the requirements (4 A100/H100 80GB GPUs, 8 CPU cores, 256GB RAM); step5: Obtain an OpenAI API key for safety evaluation; step6: Run prefilling experiments to compare ASR of Llama-2-7B base model with and without a refusal prefix; step7: Evaluate prefilling attacks on Llama-2-7B-Chat model; step8: Run data augmentation fine-tuning to get Llama-2-7B-Chat-Augmented model; step9: Test the robustness of the augmented model against prefilling; step10: Perform standard supervised fine-tuning of Llama-2-7B-Chat model on harmful examples; step11: Perform fine-tuning with token-wise constrained objective on harmful examples; step12: Perform standard supervised fine-tuning on utility dataset; step13: Perform fine-tuning with token-wise constrained objective on utility dataset", "executed_cmds": "pip install transformers==4.40.2 trl==0.8.1 accelerate==0.27.2 torch==2.2.0 deepspeed==0.13.2;accelerate launch --num_processes=4 eval_safety.py --model_name_or_path=\"ckpts/Llama-2-7B-fp16\" --torch_dtype=bfloat16 --safety_bench='hex-phi' --model_family='llama2_base' --prompt_style='llama2_base' --evaluator='none' --save_path='logs/prefilling/llama2_base_no_prefilling.json' --eval_template='plain';accelerate launch --num_processes=4 eval_safety.py --model_name_or_path=\"ckpts/Llama-2-7B-fp16\" --torch_dtype=bfloat16 --safety_bench='hex-phi_with_prefix' --model_family='llama2_base' --prompt_style='llama2_base' --evaluator='none' --save_path='logs/prefilling/llama2_base_prefilled_refusal_prefix.json' --eval_template='plain' --prefill_prefix='I apologize", "cmd_prefix": "but", "cmd_postfix": "I cannot'", "target_cmd": "but I cannot'"}
{"uuid": "7f85caed-6030-4ccc-a6a4-b300148eac0e", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Create a Conda environment for the project; step4: Activate the Conda environment; step5: Install PyTorch with the appropriate CUDA version; step6: Install additional dependencies including transformers, diffusers, opencv-python, scipy, gradio, and sympy; step7: Run the inference code to generate images; step8: Run the Gradio code to launch a demo interface; step9: Run the benchmark generation code to evaluate the model", "executed_cmds": "git clone https://github.com/byliutao/1Prompt1Story", "cmd_prefix": "cd", "cmd_postfix": "1Prompt1Story", "target_cmd": "cd 1Prompt1Story"}
{"uuid": "95478214-100b-4c8f-b193-00e8ab97c427", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Create a Conda environment for the project; step4: Activate the Conda environment; step5: Install PyTorch with the appropriate CUDA version; step6: Install additional dependencies including transformers, diffusers, opencv-python, scipy, gradio, and sympy; step7: Run the inference code to generate images; step8: Run the Gradio code to launch a demo interface; step9: Run the benchmark generation code to evaluate the model", "executed_cmds": "git clone https://github.com/byliutao/1Prompt1Story;cd 1Prompt1Story;conda create --name 1p1s python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate 1p1s", "target_cmd": "conda activate 1p1s"}
{"uuid": "793de8c0-291f-4636-8316-83f4721c2836", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Create a Conda environment for the project; step4: Activate the Conda environment; step5: Install PyTorch with the appropriate CUDA version; step6: Install additional dependencies including transformers, diffusers, opencv-python, scipy, gradio, and sympy; step7: Run the inference code to generate images; step8: Run the Gradio code to launch a demo interface; step9: Run the benchmark generation code to evaluate the model", "executed_cmds": "git clone https://github.com/byliutao/1Prompt1Story;cd 1Prompt1Story;conda create --name 1p1s python=3.10;conda activate 1p1s;conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia;pip install transformers==4.46.3;conda install -c conda-forge diffusers;pip install opencv-python scipy gradio==4.44.1 sympy==1.13.1", "cmd_prefix": "python", "cmd_postfix": "main.py", "target_cmd": "python main.py"}
{"uuid": "53718c67-df1e-4d17-a89e-00f9a3010f9e", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Create a Conda environment for the project; step4: Activate the Conda environment; step5: Install PyTorch with the appropriate CUDA version; step6: Install additional dependencies including transformers, diffusers, opencv-python, scipy, gradio, and sympy; step7: Run the inference code to generate images; step8: Run the Gradio code to launch a demo interface; step9: Run the benchmark generation code to evaluate the model", "executed_cmds": "git clone https://github.com/byliutao/1Prompt1Story;cd 1Prompt1Story;conda create --name 1p1s python=3.10;conda activate 1p1s;conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia;pip install transformers==4.46.3;conda install -c conda-forge diffusers;pip install opencv-python scipy gradio==4.44.1 sympy==1.13.1;python main.py", "cmd_prefix": "python", "cmd_postfix": "app.py", "target_cmd": "python app.py"}
{"uuid": "586476bb-f170-4caf-9405-f321a24f4f43", "execution_plan": "step1: Set up the Conda environment for CycleNet; step2: Download and prepare the required datasets; step3: Train the model to reproduce the main results; step4: Reproduce the results of various STD techniques; step5: Reproduce the performance on the PEMS datasets; step6: Run specific tasks like obtaining results on etth1", "executed_cmds": "conda create -n CycleNet python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate CycleNet", "target_cmd": "conda activate CycleNet"}
{"uuid": "e7b59cc9-1122-429b-9e18-73d81f28a474", "execution_plan": "step1: Set up the Conda environment for CycleNet; step2: Download and prepare the required datasets; step3: Train the model to reproduce the main results; step4: Reproduce the results of various STD techniques; step5: Reproduce the performance on the PEMS datasets; step6: Run specific tasks like obtaining results on etth1", "executed_cmds": "conda create -n CycleNet python=3.8;conda activate CycleNet", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "b15a548d-33fb-402c-a901-be5ad87ca4e0", "execution_plan": "step1: Set up the Conda environment for CycleNet; step2: Download and prepare the required datasets; step3: Train the model to reproduce the main results; step4: Reproduce the results of various STD techniques; step5: Reproduce the performance on the PEMS datasets; step6: Run specific tasks like obtaining results on etth1", "executed_cmds": "conda create -n CycleNet python=3.8;conda activate CycleNet;pip install -r requirements.txt", "cmd_prefix": "sh", "cmd_postfix": "run_main.sh", "target_cmd": "sh run_main.sh"}
{"uuid": "afaf933f-4363-48f9-b5a5-5c666f254ef0", "execution_plan": "step1: Set up the Conda environment for CycleNet; step2: Download and prepare the required datasets; step3: Train the model to reproduce the main results; step4: Reproduce the results of various STD techniques; step5: Reproduce the performance on the PEMS datasets; step6: Run specific tasks like obtaining results on etth1", "executed_cmds": "conda create -n CycleNet python=3.8;conda activate CycleNet;pip install -r requirements.txt;sh run_main.sh", "cmd_prefix": "sh", "cmd_postfix": "run_std.sh", "target_cmd": "sh run_std.sh"}
{"uuid": "b3ee4ac1-d035-4b8a-972f-c09c559c64d0", "execution_plan": "step1: Set up the Conda environment for CycleNet; step2: Download and prepare the required datasets; step3: Train the model to reproduce the main results; step4: Reproduce the results of various STD techniques; step5: Reproduce the performance on the PEMS datasets; step6: Run specific tasks like obtaining results on etth1", "executed_cmds": "conda create -n CycleNet python=3.8;conda activate CycleNet;pip install -r requirements.txt;sh run_main.sh;sh run_std.sh", "cmd_prefix": "sh", "cmd_postfix": "run_pems.sh", "target_cmd": "sh run_pems.sh"}
{"uuid": "efc1df1a-ed7a-4d90-bd7c-e6b86538e614", "execution_plan": "step1: Set up the Python environment for the project using conda and install required dependencies; step2: Configure a local Pokmon Showdown server by installing Node.js, cloning the repository, and setting up the server; step3: Run the evaluation script for Gen 9 OU battles to reproduce paper results; step4: Run a local 1v1 battle between any two agents, including built-in or custom bots; step5: Set up LLM backends using OpenRouter API for custom bot interactions; step6: Create a custom bot by defining a new class in the bots folder and overriding the choose_move method; step7: Run a battle between a human and any agent locally by logging into the local server; step8: Run a battle against ladder players on Pokmon Showdown by registering an account and using the showdown_ladder script; step9: Load and use the PokChamp dataset from HuggingFace for research and AI development; step10: Convert raw battle logs into training data using the battle_translate script; step11: Run the benchmark puzzles evaluation script for action prediction results (coming soon)", "executed_cmds": "conda create -n pokechamp python=3.12", "cmd_prefix": "conda", "cmd_postfix": "activate pokechamp", "target_cmd": "conda activate pokechamp"}
{"uuid": "72671990-f165-44f1-9b6c-8bb7136b99a3", "execution_plan": "step1: Set up the Python environment for the project using conda and install required dependencies; step2: Configure a local Pokmon Showdown server by installing Node.js, cloning the repository, and setting up the server; step3: Run the evaluation script for Gen 9 OU battles to reproduce paper results; step4: Run a local 1v1 battle between any two agents, including built-in or custom bots; step5: Set up LLM backends using OpenRouter API for custom bot interactions; step6: Create a custom bot by defining a new class in the bots folder and overriding the choose_move method; step7: Run a battle between a human and any agent locally by logging into the local server; step8: Run a battle against ladder players on Pokmon Showdown by registering an account and using the showdown_ladder script; step9: Load and use the PokChamp dataset from HuggingFace for research and AI development; step10: Convert raw battle logs into training data using the battle_translate script; step11: Run the benchmark puzzles evaluation script for action prediction results (coming soon)", "executed_cmds": "conda create -n pokechamp python=3.12;conda activate pokechamp", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "380c21f5-3763-41a7-8515-818a61e69e3d", "execution_plan": "step1: Set up the Python environment for the project using conda and install required dependencies; step2: Configure a local Pokmon Showdown server by installing Node.js, cloning the repository, and setting up the server; step3: Run the evaluation script for Gen 9 OU battles to reproduce paper results; step4: Run a local 1v1 battle between any two agents, including built-in or custom bots; step5: Set up LLM backends using OpenRouter API for custom bot interactions; step6: Create a custom bot by defining a new class in the bots folder and overriding the choose_move method; step7: Run a battle between a human and any agent locally by logging into the local server; step8: Run a battle against ladder players on Pokmon Showdown by registering an account and using the showdown_ladder script; step9: Load and use the PokChamp dataset from HuggingFace for research and AI development; step10: Convert raw battle logs into training data using the battle_translate script; step11: Run the benchmark puzzles evaluation script for action prediction results (coming soon)", "executed_cmds": "conda create -n pokechamp python=3.12;conda activate pokechamp;pip install -r requirements.txt;git clone git@github.com:jakegrigsby/pokemon-showdown.git", "cmd_prefix": "cd", "cmd_postfix": "pokemon-showdown", "target_cmd": "cd pokemon-showdown"}
{"uuid": "8b9855a2-aff9-42dd-8c55-c2299fcc4aa6", "execution_plan": "step1: Set up the Python environment for the project using conda and install required dependencies; step2: Configure a local Pokmon Showdown server by installing Node.js, cloning the repository, and setting up the server; step3: Run the evaluation script for Gen 9 OU battles to reproduce paper results; step4: Run a local 1v1 battle between any two agents, including built-in or custom bots; step5: Set up LLM backends using OpenRouter API for custom bot interactions; step6: Create a custom bot by defining a new class in the bots folder and overriding the choose_move method; step7: Run a battle between a human and any agent locally by logging into the local server; step8: Run a battle against ladder players on Pokmon Showdown by registering an account and using the showdown_ladder script; step9: Load and use the PokChamp dataset from HuggingFace for research and AI development; step10: Convert raw battle logs into training data using the battle_translate script; step11: Run the benchmark puzzles evaluation script for action prediction results (coming soon)", "executed_cmds": "conda create -n pokechamp python=3.12;conda activate pokechamp;pip install -r requirements.txt;git clone git@github.com:jakegrigsby/pokemon-showdown.git;cd pokemon-showdown", "cmd_prefix": "npm", "cmd_postfix": "install", "target_cmd": "npm install"}
{"uuid": "7670cec6-0dc8-45e4-9d28-18853dccac7e", "execution_plan": "step1: Set up the Python environment for the project using conda and install required dependencies; step2: Configure a local Pokmon Showdown server by installing Node.js, cloning the repository, and setting up the server; step3: Run the evaluation script for Gen 9 OU battles to reproduce paper results; step4: Run a local 1v1 battle between any two agents, including built-in or custom bots; step5: Set up LLM backends using OpenRouter API for custom bot interactions; step6: Create a custom bot by defining a new class in the bots folder and overriding the choose_move method; step7: Run a battle between a human and any agent locally by logging into the local server; step8: Run a battle against ladder players on Pokmon Showdown by registering an account and using the showdown_ladder script; step9: Load and use the PokChamp dataset from HuggingFace for research and AI development; step10: Convert raw battle logs into training data using the battle_translate script; step11: Run the benchmark puzzles evaluation script for action prediction results (coming soon)", "executed_cmds": "conda create -n pokechamp python=3.12;conda activate pokechamp;pip install -r requirements.txt;git clone git@github.com:jakegrigsby/pokemon-showdown.git;cd pokemon-showdown;npm install;cp config/config-example.js config/config.js", "cmd_prefix": "node pokemon-showdown", "cmd_postfix": "start --no-security", "target_cmd": "node pokemon-showdown start --no-security"}
{"uuid": "2531ce55-16c5-41ca-a4a0-406fa74c98a6", "execution_plan": "step1: Set up the Python environment for the project using conda and install required dependencies; step2: Configure a local Pokmon Showdown server by installing Node.js, cloning the repository, and setting up the server; step3: Run the evaluation script for Gen 9 OU battles to reproduce paper results; step4: Run a local 1v1 battle between any two agents, including built-in or custom bots; step5: Set up LLM backends using OpenRouter API for custom bot interactions; step6: Create a custom bot by defining a new class in the bots folder and overriding the choose_move method; step7: Run a battle between a human and any agent locally by logging into the local server; step8: Run a battle against ladder players on Pokmon Showdown by registering an account and using the showdown_ladder script; step9: Load and use the PokChamp dataset from HuggingFace for research and AI development; step10: Convert raw battle logs into training data using the battle_translate script; step11: Run the benchmark puzzles evaluation script for action prediction results (coming soon)", "executed_cmds": "conda create -n pokechamp python=3.12;conda activate pokechamp;pip install -r requirements.txt;git clone git@github.com:jakegrigsby/pokemon-showdown.git;cd pokemon-showdown;npm install;cp config/config-example.js config/config.js;node pokemon-showdown start --no-security", "cmd_prefix": "python", "cmd_postfix": "evaluate_gen9ou.py", "target_cmd": "python evaluate_gen9ou.py"}
{"uuid": "cb8028f9-6864-45dc-b65b-7a6908ff7700", "execution_plan": "step1: Set up the Python environment for the project using conda and install required dependencies; step2: Configure a local Pokmon Showdown server by installing Node.js, cloning the repository, and setting up the server; step3: Run the evaluation script for Gen 9 OU battles to reproduce paper results; step4: Run a local 1v1 battle between any two agents, including built-in or custom bots; step5: Set up LLM backends using OpenRouter API for custom bot interactions; step6: Create a custom bot by defining a new class in the bots folder and overriding the choose_move method; step7: Run a battle between a human and any agent locally by logging into the local server; step8: Run a battle against ladder players on Pokmon Showdown by registering an account and using the showdown_ladder script; step9: Load and use the PokChamp dataset from HuggingFace for research and AI development; step10: Convert raw battle logs into training data using the battle_translate script; step11: Run the benchmark puzzles evaluation script for action prediction results (coming soon)", "executed_cmds": "conda create -n pokechamp python=3.12;conda activate pokechamp;pip install -r requirements.txt;git clone git@github.com:jakegrigsby/pokemon-showdown.git;cd pokemon-showdown;npm install;cp config/config-example.js config/config.js;node pokemon-showdown start --no-security;python evaluate_gen9ou.py", "cmd_prefix": "python", "cmd_postfix": "local_1v1.py", "target_cmd": "python local_1v1.py"}
{"uuid": "459f5ec8-f466-45c0-80ac-b00f4662fba2", "execution_plan": "step1: Set up the Python environment for the project using conda and install required dependencies; step2: Configure a local Pokmon Showdown server by installing Node.js, cloning the repository, and setting up the server; step3: Run the evaluation script for Gen 9 OU battles to reproduce paper results; step4: Run a local 1v1 battle between any two agents, including built-in or custom bots; step5: Set up LLM backends using OpenRouter API for custom bot interactions; step6: Create a custom bot by defining a new class in the bots folder and overriding the choose_move method; step7: Run a battle between a human and any agent locally by logging into the local server; step8: Run a battle against ladder players on Pokmon Showdown by registering an account and using the showdown_ladder script; step9: Load and use the PokChamp dataset from HuggingFace for research and AI development; step10: Convert raw battle logs into training data using the battle_translate script; step11: Run the benchmark puzzles evaluation script for action prediction results (coming soon)", "executed_cmds": "conda create -n pokechamp python=3.12;conda activate pokechamp;pip install -r requirements.txt;git clone git@github.com:jakegrigsby/pokemon-showdown.git;cd pokemon-showdown;npm install;cp config/config-example.js config/config.js;node pokemon-showdown start --no-security;python evaluate_gen9ou.py;python local_1v1.py;export OPENROUTER_API_KEY='your-api-key-here'", "cmd_prefix": "python", "cmd_postfix": "human_agent_1v1.py", "target_cmd": "python human_agent_1v1.py"}
{"uuid": "c7ebffa0-1b1f-45b2-beac-abaceacc0e26", "execution_plan": "step1: Set up the Python environment for the project using conda and install required dependencies; step2: Configure a local Pokmon Showdown server by installing Node.js, cloning the repository, and setting up the server; step3: Run the evaluation script for Gen 9 OU battles to reproduce paper results; step4: Run a local 1v1 battle between any two agents, including built-in or custom bots; step5: Set up LLM backends using OpenRouter API for custom bot interactions; step6: Create a custom bot by defining a new class in the bots folder and overriding the choose_move method; step7: Run a battle between a human and any agent locally by logging into the local server; step8: Run a battle against ladder players on Pokmon Showdown by registering an account and using the showdown_ladder script; step9: Load and use the PokChamp dataset from HuggingFace for research and AI development; step10: Convert raw battle logs into training data using the battle_translate script; step11: Run the benchmark puzzles evaluation script for action prediction results (coming soon)", "executed_cmds": "conda create -n pokechamp python=3.12;conda activate pokechamp;pip install -r requirements.txt;git clone git@github.com:jakegrigsby/pokemon-showdown.git;cd pokemon-showdown;npm install;cp config/config-example.js config/config.js;node pokemon-showdown start --no-security;python evaluate_gen9ou.py;python local_1v1.py;export OPENROUTER_API_KEY='your-api-key-here';python human_agent_1v1.py;python showdown_ladder.py --USERNAME $USERNAME --PASSWORD $PASSWORD", "cmd_prefix": "from datasets", "cmd_postfix": "import load_dataset", "target_cmd": "from datasets import load_dataset"}
{"uuid": "ce161b98-1ed5-48b6-be4f-c4fdcfdd2453", "execution_plan": "step1: Set up the PyTorch environment and install required packages; step2: Download the pretrained checkpoints and set the CKPT_PATH; step3: Create a directory for storing generation results; step4: Sample protein family based on MSA (Multiple Sequence Alignment); step5: Sample protein family based on a single protein sequence; step6: Clone the CCMPRED repository for evaluation; step7: Build a Docker image for CCMPRED evaluation; step8: Run the evaluation script for generated protein families", "executed_cmds": "pip install omegaconf hydra-core bitarray rdkit-pypi scipy lmdb numba scikit-learn", "cmd_prefix": "mkdir", "cmd_postfix": "./results", "target_cmd": "mkdir ./results"}
{"uuid": "953752c7-35b6-4b4f-b8d8-67672f34c15d", "execution_plan": "step1: Set up the PyTorch environment and install required packages; step2: Download the pretrained checkpoints and set the CKPT_PATH; step3: Create a directory for storing generation results; step4: Sample protein family based on MSA (Multiple Sequence Alignment); step5: Sample protein family based on a single protein sequence; step6: Clone the CCMPRED repository for evaluation; step7: Build a Docker image for CCMPRED evaluation; step8: Run the evaluation script for generated protein families", "executed_cmds": "pip install omegaconf hydra-core bitarray rdkit-pypi scipy lmdb numba scikit-learn;mkdir ./results", "cmd_prefix": "make sample_profile", "cmd_postfix": "-f scripts.mk", "target_cmd": "make sample_profile -f scripts.mk"}
{"uuid": "e93df857-c809-4231-8bc7-3caa44d45a7e", "execution_plan": "step1: Set up the PyTorch environment and install required packages; step2: Download the pretrained checkpoints and set the CKPT_PATH; step3: Create a directory for storing generation results; step4: Sample protein family based on MSA (Multiple Sequence Alignment); step5: Sample protein family based on a single protein sequence; step6: Clone the CCMPRED repository for evaluation; step7: Build a Docker image for CCMPRED evaluation; step8: Run the evaluation script for generated protein families", "executed_cmds": "pip install omegaconf hydra-core bitarray rdkit-pypi scipy lmdb numba scikit-learn;mkdir ./results;make sample_profile -f scripts.mk", "cmd_prefix": "make sample_sequence", "cmd_postfix": "-f scripts.mk", "target_cmd": "make sample_sequence -f scripts.mk"}
{"uuid": "b1e6cf18-3f07-4740-b87d-589c3c4e4ffe", "execution_plan": "step1: Set up the PyTorch environment and install required packages; step2: Download the pretrained checkpoints and set the CKPT_PATH; step3: Create a directory for storing generation results; step4: Sample protein family based on MSA (Multiple Sequence Alignment); step5: Sample protein family based on a single protein sequence; step6: Clone the CCMPRED repository for evaluation; step7: Build a Docker image for CCMPRED evaluation; step8: Run the evaluation script for generated protein families", "executed_cmds": "pip install omegaconf hydra-core bitarray rdkit-pypi scipy lmdb numba scikit-learn;mkdir ./results;make sample_profile -f scripts.mk;make sample_sequence -f scripts.mk", "cmd_prefix": "cd", "cmd_postfix": "test/ccmpred", "target_cmd": "cd test/ccmpred"}
{"uuid": "67515217-b35f-47cb-9aa3-dddfdc1f7f23", "execution_plan": "step1: Set up the conda environment using the provided environment.yaml file; step2: Download the required datasets (MPI3D_toy and Shapes3D) and set the dataset paths in the configuration scripts; step3: Train a VQVAE/VAE model using the main.py script with the appropriate dataset configuration; step4: Update the checkpoint path of the pretrained VQVAE/VAE model in the config file for EncDiff training; step5: Train the EncDiff model using the main_val.py script with the pretrained VQVAE/VAE model", "executed_cmds": "conda env create -f environment.yaml", "cmd_prefix": "conda", "cmd_postfix": "activate encdiff", "target_cmd": "conda activate encdiff"}
{"uuid": "f016d33f-94cb-4a91-99a3-74091f2b882c", "execution_plan": "step1: Clone the repository to your local machine; step2: Create a new conda environment named DeepRUOT with Python 3.10 and ipykernel; step3: Activate the newly created conda environment; step4: Navigate to the cloned repository directory; step5: Install the required dependencies using pip; step6: Install the DeepRUOT package in development mode; step7: Explore the provided tutorials and examples in the notebook directory; step8: Use the reproduce_model_weights directory to reproduce results from the paper", "executed_cmds": "git clone https://github.com/zhenyiizhang/DeepRUOT;conda create -n DeepRUOT python=3.10 ipykernel -y", "cmd_prefix": "conda", "cmd_postfix": "activate DeepRUOT", "target_cmd": "conda activate DeepRUOT"}
{"uuid": "566a9c4e-d93f-4798-a76a-fef6e8eab647", "execution_plan": "step1: Clone the repository to your local machine; step2: Create a new conda environment named DeepRUOT with Python 3.10 and ipykernel; step3: Activate the newly created conda environment; step4: Navigate to the cloned repository directory; step5: Install the required dependencies using pip; step6: Install the DeepRUOT package in development mode; step7: Explore the provided tutorials and examples in the notebook directory; step8: Use the reproduce_model_weights directory to reproduce results from the paper", "executed_cmds": "git clone https://github.com/zhenyiizhang/DeepRUOT;conda create -n DeepRUOT python=3.10 ipykernel -y;conda activate DeepRUOT", "cmd_prefix": "cd", "cmd_postfix": "path_to_DeepRUOT", "target_cmd": "cd path_to_DeepRUOT"}
{"uuid": "6826856c-6224-404e-91be-056fb14d6072", "execution_plan": "step1: Clone the repository to your local machine; step2: Create a new conda environment named DeepRUOT with Python 3.10 and ipykernel; step3: Activate the newly created conda environment; step4: Navigate to the cloned repository directory; step5: Install the required dependencies using pip; step6: Install the DeepRUOT package in development mode; step7: Explore the provided tutorials and examples in the notebook directory; step8: Use the reproduce_model_weights directory to reproduce results from the paper", "executed_cmds": "git clone https://github.com/zhenyiizhang/DeepRUOT;conda create -n DeepRUOT python=3.10 ipykernel -y;conda activate DeepRUOT;cd path_to_DeepRUOT", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "dfb01b5c-0296-467a-a348-544a47c372fb", "execution_plan": "step1: Clone the repository to your local machine; step2: Create a new conda environment named DeepRUOT with Python 3.10 and ipykernel; step3: Activate the newly created conda environment; step4: Navigate to the cloned repository directory; step5: Install the required dependencies using pip; step6: Install the DeepRUOT package in development mode; step7: Explore the provided tutorials and examples in the notebook directory; step8: Use the reproduce_model_weights directory to reproduce results from the paper", "executed_cmds": "git clone https://github.com/zhenyiizhang/DeepRUOT;conda create -n DeepRUOT python=3.10 ipykernel -y;conda activate DeepRUOT;cd path_to_DeepRUOT;pip install -r requirements.txt", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "15e939d3-e0bb-4032-ac25-70bc1c09a0e7", "execution_plan": "step1: Run evaluations on the standard safety benchmark; step2: Run evaluations against jailbreaks using the GCG attack, specifying the corresponding adv_string; step3: Run evaluations against jailbreaks using AutoDAN, TAP, and PAIR attacks", "executed_cmds": "bash scripts.sh;bash scripts.sh (with specified adv_string)", "cmd_prefix": "bash", "cmd_postfix": "jailbreak.sh", "target_cmd": "bash jailbreak.sh"}
{"uuid": "1242f870-c32e-464c-84ab-92be24cf522f", "execution_plan": "step1: Clone the VMamba repository and navigate to the project directory; step2: Create and activate a conda environment for VMamba; step3: Install the required dependencies using pip; step4: Install the selective_scan kernel; step5: (Optional) Check the selective scan implementation; step6: (Optional) Install additional dependencies for detection and segmentation tasks; step7: Train or test the VMamba model for classification; step8: (Optional) Evaluate or train models for detection and segmentation tasks; step9: (Optional) Use analysis tools to visualize attention, effective receptive field, or throughput", "executed_cmds": "git clone https://github.com/MzeroMiko/VMamba.git", "cmd_prefix": "cd", "cmd_postfix": "VMamba", "target_cmd": "cd VMamba"}
{"uuid": "f6893121-4062-4dcb-9a20-b684eb3be81d", "execution_plan": "step1: Clone the VMamba repository and navigate to the project directory; step2: Create and activate a conda environment for VMamba; step3: Install the required dependencies using pip; step4: Install the selective_scan kernel; step5: (Optional) Check the selective scan implementation; step6: (Optional) Install additional dependencies for detection and segmentation tasks; step7: Train or test the VMamba model for classification; step8: (Optional) Evaluate or train models for detection and segmentation tasks; step9: (Optional) Use analysis tools to visualize attention, effective receptive field, or throughput", "executed_cmds": "git clone https://github.com/MzeroMiko/VMamba.git;cd VMamba", "cmd_prefix": "conda create", "cmd_postfix": "-n vmamba", "target_cmd": "conda create -n vmamba"}
{"uuid": "98027a30-ee22-4df3-8759-24763fa819a4", "execution_plan": "step1: Clone the VMamba repository and navigate to the project directory; step2: Create and activate a conda environment for VMamba; step3: Install the required dependencies using pip; step4: Install the selective_scan kernel; step5: (Optional) Check the selective scan implementation; step6: (Optional) Install additional dependencies for detection and segmentation tasks; step7: Train or test the VMamba model for classification; step8: (Optional) Evaluate or train models for detection and segmentation tasks; step9: (Optional) Use analysis tools to visualize attention, effective receptive field, or throughput", "executed_cmds": "git clone https://github.com/MzeroMiko/VMamba.git;cd VMamba;conda create -n vmamba", "cmd_prefix": "conda", "cmd_postfix": "activate vmamba", "target_cmd": "conda activate vmamba"}
{"uuid": "8eb10bad-0365-4d29-a0d2-fa710ac38eb6", "execution_plan": "step1: Clone the VMamba repository and navigate to the project directory; step2: Create and activate a conda environment for VMamba; step3: Install the required dependencies using pip; step4: Install the selective_scan kernel; step5: (Optional) Check the selective scan implementation; step6: (Optional) Install additional dependencies for detection and segmentation tasks; step7: Train or test the VMamba model for classification; step8: (Optional) Evaluate or train models for detection and segmentation tasks; step9: (Optional) Use analysis tools to visualize attention, effective receptive field, or throughput", "executed_cmds": "git clone https://github.com/MzeroMiko/VMamba.git;cd VMamba;conda create -n vmamba;conda activate vmamba", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "432c9605-5218-411e-9e6c-e783389e36b5", "execution_plan": "step1: Clone the VMamba repository and navigate to the project directory; step2: Create and activate a conda environment for VMamba; step3: Install the required dependencies using pip; step4: Install the selective_scan kernel; step5: (Optional) Check the selective scan implementation; step6: (Optional) Install additional dependencies for detection and segmentation tasks; step7: Train or test the VMamba model for classification; step8: (Optional) Evaluate or train models for detection and segmentation tasks; step9: (Optional) Use analysis tools to visualize attention, effective receptive field, or throughput", "executed_cmds": "git clone https://github.com/MzeroMiko/VMamba.git;cd VMamba;conda create -n vmamba;conda activate vmamba;pip install -r requirements.txt", "cmd_prefix": "cd kernels/selective_scan &&", "cmd_postfix": "pip install .", "target_cmd": "cd kernels/selective_scan && pip install ."}
{"uuid": "ae61e400-b086-44b7-b220-23bee116e7d7", "execution_plan": "step1: Clone the VMamba repository and navigate to the project directory; step2: Create and activate a conda environment for VMamba; step3: Install the required dependencies using pip; step4: Install the selective_scan kernel; step5: (Optional) Check the selective scan implementation; step6: (Optional) Install additional dependencies for detection and segmentation tasks; step7: Train or test the VMamba model for classification; step8: (Optional) Evaluate or train models for detection and segmentation tasks; step9: (Optional) Use analysis tools to visualize attention, effective receptive field, or throughput", "executed_cmds": "git clone https://github.com/MzeroMiko/VMamba.git;cd VMamba;conda create -n vmamba;conda activate vmamba;pip install -r requirements.txt;cd kernels/selective_scan && pip install .;pip install mmengine==0.10.1 mmcv==2.1.0 opencv-python-headless ftfy regex;pip install mmdet==3.3.0 mmsegmentation==1.2.2 mmpretrain==1.2.0;python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=8 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg </path/to/config> --batch-size 128 --data-path </path/of/dataset> --output /tmp;bash ./tools/dist_test.sh </path/to/config> </path/to/checkpoint> 1", "cmd_prefix": "bash ./tools/dist_train.sh", "cmd_postfix": "</path/to/config> 8", "target_cmd": "bash ./tools/dist_train.sh </path/to/config> 8"}
{"uuid": "b34e25fd-ce5c-41ed-85bc-fc45ccc485ac", "execution_plan": "step1: Clone the VMamba repository and navigate to the project directory; step2: Create and activate a conda environment for VMamba; step3: Install the required dependencies using pip; step4: Install the selective_scan kernel; step5: (Optional) Check the selective scan implementation; step6: (Optional) Install additional dependencies for detection and segmentation tasks; step7: Train or test the VMamba model for classification; step8: (Optional) Evaluate or train models for detection and segmentation tasks; step9: (Optional) Use analysis tools to visualize attention, effective receptive field, or throughput", "executed_cmds": "git clone https://github.com/MzeroMiko/VMamba.git;cd VMamba;conda create -n vmamba;conda activate vmamba;pip install -r requirements.txt;cd kernels/selective_scan && pip install .;pip install mmengine==0.10.1 mmcv==2.1.0 opencv-python-headless ftfy regex;pip install mmdet==3.3.0 mmsegmentation==1.2.2 mmpretrain==1.2.0;python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=8 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg </path/to/config> --batch-size 128 --data-path </path/of/dataset> --output /tmp;bash ./tools/dist_test.sh </path/to/config> </path/to/checkpoint> 1;bash ./tools/dist_train.sh </path/to/config> 8;CUDA_VISIBLE_DEVICES=0 python analyze/attnmap.py;CUDA_VISIBLE_DEVICES=0 python analyze/erf.py", "cmd_prefix": "CUDA_VISIBLE_DEVICES=0", "cmd_postfix": "python analyze/tp.py", "target_cmd": "CUDA_VISIBLE_DEVICES=0 python analyze/tp.py"}
{"uuid": "184e21e3-f677-44ce-bc8f-7bfbade029af", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Download the KMNIST dataset for Experiment 2 if needed; step3: Run the Jupyter notebooks for Experiment 1 (Easy and Hard Regimes); step4: Run the shell script for Experiment 2 (Intrinsic Dimension Study); step5: (Optional) Train diffusion models on image or synthetic datasets; step6: (Optional) Generate samples and find intrinsic dimension using trained diffusion models", "executed_cmds": "conda create -n manifold-ml python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate manifold-ml", "target_cmd": "conda activate manifold-ml"}
{"uuid": "2b3026a0-6524-4caa-8def-802875fd316a", "execution_plan": "step1: Install the project dependencies using the all-in-one script for CPU support; step2: Install the project dependencies using the all-in-one script for GPU support (optional, if GPU is available); step3: Install dependencies step-by-step manually (alternative to the all-in-one script); step4: Download the pre-generated dataset from Google Drive (optional, if not generating the dataset manually); step5: Generate the dataset manually by running the provided script (optional, if not using the pre-generated dataset); step6: Train the model using the provided training notebook (optional, if not using pre-trained models); step7: Download pre-trained models from Google Drive (optional, if not training the model manually); step8: Evaluate the model using the provided visualization scripts; step9: Build the project documentation using Sphinx (optional)", "executed_cmds": "./install.sh", "cmd_prefix": "./install_gpu.sh", "cmd_postfix": "{CUDA_VERSION}", "target_cmd": "./install_gpu.sh {CUDA_VERSION}"}
{"uuid": "3f9d0dde-138b-4e14-929f-0a0315de335d", "execution_plan": "step1: Set up the environment by installing Python 3.12, PyTorch, and other dependencies listed in requirements.txt; step2: Generate logits for each model to simulate real-world generation and accelerate sampling; step3: Run sampling for all watermarking algorithms and unwatermarked conditions; step4: Execute experiments across all conditions", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "cd", "cmd_postfix": "scripts", "target_cmd": "cd scripts"}
{"uuid": "3c2ad40b-c4be-47a4-8a3e-a5540c05b065", "execution_plan": "step1: Set up the environment by installing Python 3.12, PyTorch, and other dependencies listed in requirements.txt; step2: Generate logits for each model to simulate real-world generation and accelerate sampling; step3: Run sampling for all watermarking algorithms and unwatermarked conditions; step4: Execute experiments across all conditions", "executed_cmds": "pip install -r requirements.txt;cd scripts", "cmd_prefix": "bash", "cmd_postfix": "generate_logits.sh", "target_cmd": "bash generate_logits.sh"}
{"uuid": "c566a7ea-14d5-4a94-bb62-876c1f0f2209", "execution_plan": "step1: Set up the environment by installing Python 3.12, PyTorch, and other dependencies listed in requirements.txt; step2: Generate logits for each model to simulate real-world generation and accelerate sampling; step3: Run sampling for all watermarking algorithms and unwatermarked conditions; step4: Execute experiments across all conditions", "executed_cmds": "pip install -r requirements.txt;cd scripts;bash generate_logits.sh", "cmd_prefix": "cd", "cmd_postfix": "scripts", "target_cmd": "cd scripts"}
{"uuid": "9a8dd8be-8e8c-4a26-b398-cb01a230f6d4", "execution_plan": "step1: Set up the environment by installing Python 3.12, PyTorch, and other dependencies listed in requirements.txt; step2: Generate logits for each model to simulate real-world generation and accelerate sampling; step3: Run sampling for all watermarking algorithms and unwatermarked conditions; step4: Execute experiments across all conditions", "executed_cmds": "pip install -r requirements.txt;cd scripts;bash generate_logits.sh;cd scripts", "cmd_prefix": "bash", "cmd_postfix": "sampling_pipeline.sh", "target_cmd": "bash sampling_pipeline.sh"}
{"uuid": "18eb1972-5129-44bf-bce8-761919d81971", "execution_plan": "step1: Set up the environment by installing Python 3.12, PyTorch, and other dependencies listed in requirements.txt; step2: Generate logits for each model to simulate real-world generation and accelerate sampling; step3: Run sampling for all watermarking algorithms and unwatermarked conditions; step4: Execute experiments across all conditions", "executed_cmds": "pip install -r requirements.txt;cd scripts;bash generate_logits.sh;cd scripts;bash sampling_pipeline.sh", "cmd_prefix": "cd", "cmd_postfix": "scripts", "target_cmd": "cd scripts"}
{"uuid": "d0a7dd0d-63a4-433c-937f-fc59dda5f527", "execution_plan": "step1: Clone the repository to your local machine; step2: Create a Python environment for the project; step3: Create a `datasets` folder and add symbolic links to the respective datasets or update the configs in `settings.py`; step4: [Optional] Create folders for `results` and `figures` and symbolically link them to a workspace; step5: Install Python packages for analysis and experiments on MAD by installing requirements from `requirements.txt`; step6: [Optional] For experiments on real data, follow the instructions in the `clip_on_real_data/README.md` file; step7: Precompute embedding features and modality gap for analysis by running the gap_precompute.py script; step8: Run various analysis scripts to reproduce results from the paper, such as gap vs. performance, mean differences, embedding dimension pairs, etc.; step9: Precompute features and object bias/performance metrics for object bias analysis; step10: Run object bias analysis scripts to reproduce results like object bias vs. performance and object vs. attribute performance; step11: For CLIP trainings on synthetic data (MAD), use the provided dataset implementation and adapt standard CLIP training pipelines; step12: For CLIP trainings on real data (CC12M and CC3M), follow the setup and run instructions in the `clip_on_real_data/README.md` file", "executed_cmds": "git clone <repository_url>", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "32ee57d1-c05d-47cc-9245-92fd63ae6cf9", "execution_plan": "step1: Set up the Python environment and install necessary dependencies; step2: Clone the TokenFormer repository; step3: Install PyTorch and CUDA dependencies; step4: Install additional requirements including flashattention, WandB, tensorboard, and Comet; step5: Install NVIDIA Apex for optimized operations; step6: Download pre-trained weights from Hugging Face for evaluation; step7: Run zero-shot evaluations using the lm-evaluation-harness library; step8: Prepare datasets for training (e.g., openwebtext2 or Pile); step9: Train the model on a single node or multiple nodes using Slurm; step10: Perform incremental training by downloading a pre-trained model and scaling it up", "executed_cmds": "conda create -n TokenFormer python=3.8;git clone https://github.com/Haiyang-W/TokenFormer.git;pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121;pip install -r requirements/requirements.txt;pip install -r requirements/requirements-flashattention.txt;pip install -r requirements/requirements-wandb.txt;pip install -r requirements/requirements-tensorboard.txt;pip install -r requirements/requirements-comet.txt;git clone https://github.com/NVIDIA/apex", "cmd_prefix": "cd", "cmd_postfix": "apex", "target_cmd": "cd apex"}
{"uuid": "a26eb993-d529-48f3-8f59-a5d13db99852", "execution_plan": "step1: Install the required environment by following the instructions for diffusers and accelerate from the diffusers repository; step2: Download the High-Safety trained U-Net Ckpt from the provided Google Drive link; step3: Train the model using latent training by executing the LT Train script; step4: Train the model using noise training by executing the NG Train script; step5: Train the model for high safety by executing the High Safety Train script; step6: Generate images using the provided image generation script; step7: Prepare the dataset in the specified format, including clean and harmful images, and save them in the same folder as the metadata file", "executed_cmds": "bash train_latent.sh", "cmd_prefix": "bash", "cmd_postfix": "train_noise.sh", "target_cmd": "bash train_noise.sh"}
{"uuid": "06ec9118-5e26-4bdb-9a1b-3e7c5adb996b", "execution_plan": "step1: Install the required environment by following the instructions for diffusers and accelerate from the diffusers repository; step2: Download the High-Safety trained U-Net Ckpt from the provided Google Drive link; step3: Train the model using latent training by executing the LT Train script; step4: Train the model using noise training by executing the NG Train script; step5: Train the model for high safety by executing the High Safety Train script; step6: Generate images using the provided image generation script; step7: Prepare the dataset in the specified format, including clean and harmful images, and save them in the same folder as the metadata file", "executed_cmds": "bash train_latent.sh;bash train_noise.sh", "cmd_prefix": "bash", "cmd_postfix": "train_high.sh", "target_cmd": "bash train_high.sh"}
{"uuid": "0f574767-2c74-482e-804c-cbad45ef283a", "execution_plan": "step1: Install the required environment by following the instructions for diffusers and accelerate from the diffusers repository; step2: Download the High-Safety trained U-Net Ckpt from the provided Google Drive link; step3: Train the model using latent training by executing the LT Train script; step4: Train the model using noise training by executing the NG Train script; step5: Train the model for high safety by executing the High Safety Train script; step6: Generate images using the provided image generation script; step7: Prepare the dataset in the specified format, including clean and harmful images, and save them in the same folder as the metadata file", "executed_cmds": "bash train_latent.sh;bash train_noise.sh;bash train_high.sh", "cmd_prefix": "python", "cmd_postfix": "image_generation.py", "target_cmd": "python image_generation.py"}
{"uuid": "7ad8f409-e450-41d7-99e7-ef658d36168f", "execution_plan": "step1: Download the required datasets from ADNI and OASIS, and preprocess the data using Freesurfer to obtain cortical thickness and other measures; step2: Download the pretrained RDM and TDM models from the provided Google Drive link; step3: Train the RDM model from scratch using the provided command; step4: Set the path of the trained RDM in the trainer.py file for TDM training; step5: Train the TDM model and evaluate the performance of ConDOR (RDM + TDM) using the provided command", "executed_cmds": "python main.py --warmup=1", "cmd_prefix": "python", "cmd_postfix": "main.py", "target_cmd": "python main.py"}
{"uuid": "11e66b24-b7f0-45a9-a7a6-6866c4881184", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up the environment by following the instructions from the Reward Bench repository; step3: Download the dataset from Hugging Face or use the provided data in the `data` directory; step4: Run the evaluation script for sequence-classification reward models; step5: Run the evaluation script for DPO models as reward models; step6: Compute the accuracy using the provided helper function in `scripts/utils.py`; step7: Submit your results to the RM-Bench Leaderboard if desired", "executed_cmds": "git clone https://github.com/THU-KEG/RM-Bench.git", "cmd_prefix": "bash", "cmd_postfix": "run_rm.sh", "target_cmd": "bash run_rm.sh"}
{"uuid": "b3415660-7acd-46ed-af14-a88c74ccd257", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up the environment by following the instructions from the Reward Bench repository; step3: Download the dataset from Hugging Face or use the provided data in the `data` directory; step4: Run the evaluation script for sequence-classification reward models; step5: Run the evaluation script for DPO models as reward models; step6: Compute the accuracy using the provided helper function in `scripts/utils.py`; step7: Submit your results to the RM-Bench Leaderboard if desired", "executed_cmds": "git clone https://github.com/THU-KEG/RM-Bench.git;bash run_rm.sh", "cmd_prefix": "bash", "cmd_postfix": "run_dpo.sh", "target_cmd": "bash run_dpo.sh"}
{"uuid": "015cdfc5-b7b7-47d4-9232-33668d90be31", "execution_plan": "step1: Clone the repository and set up the Python environment with the required dependencies; step2: Download and prepare the DomainNet dataset, including generating captions; step3: Download and prepare the ImageNet-Captions dataset, including generating TSV files; step4: Create domain mixtures for the experiments using the provided SLURM script or manually; step5: Optionally download and prepare CC3M or CC12M datasets if used as base datasets; step6: Train CLIP models using OpenCLIP, adjusting parameters based on the dataset used; step7: Train supervised classifiers using the provided script or manually; step8: Evaluate the classification performance of CLIP models and supervised classifiers; step9: Train Sparse Autoencoders (SAEs) for feature sharing evaluation; step10: Evaluate feature sharing using the trained SAEs; step11: Analyze embeddings and generate UMAP plots for Quickdraw embeddings; step12: Compute representational similarity using CKA; step13: Compute and evaluate circuits for aligned models", "executed_cmds": "pip install -e .", "cmd_prefix": "pip install", "cmd_postfix": "-e deps/open_clip/", "target_cmd": "pip install -e deps/open_clip/"}
{"uuid": "500f93a7-f76a-455a-b544-40b45f7074bc", "execution_plan": "step1: Clone the repository and set up the Python environment with the required dependencies; step2: Download and prepare the DomainNet dataset, including generating captions; step3: Download and prepare the ImageNet-Captions dataset, including generating TSV files; step4: Create domain mixtures for the experiments using the provided SLURM script or manually; step5: Optionally download and prepare CC3M or CC12M datasets if used as base datasets; step6: Train CLIP models using OpenCLIP, adjusting parameters based on the dataset used; step7: Train supervised classifiers using the provided script or manually; step8: Evaluate the classification performance of CLIP models and supervised classifiers; step9: Train Sparse Autoencoders (SAEs) for feature sharing evaluation; step10: Evaluate feature sharing using the trained SAEs; step11: Analyze embeddings and generate UMAP plots for Quickdraw embeddings; step12: Compute representational similarity using CKA; step13: Compute and evaluate circuits for aligned models", "executed_cmds": "pip install -e .;pip install -e deps/open_clip/;pip install -e deps/sparse_autoencoder/;. data/download_domainnet.sh;python scripts/generate_domainnet_captions.py --domainnet_path data/domainnet;wget https://github.com/mlfoundations/imagenet-captions/raw/5cf98361f5e67661fd5b2c6ee219567484440da9/imagenet_captions.zip;unzip imagenet_captions.zip;python scripts/generate_imagenet_captions.py --imagenet_train_path <path/to/imagenet/train>;sbatch slurm/subsample-domainnet.sh", "cmd_prefix": "python scripts/merge_ccxm.py", "cmd_postfix": "--mode cc3m", "target_cmd": "python scripts/merge_ccxm.py --mode cc3m"}
{"uuid": "7bd126f3-151b-455d-9db5-8ce96e2fcc05", "execution_plan": "step1: Clone the repository and set up the Python environment with the required dependencies; step2: Download and prepare the DomainNet dataset, including generating captions; step3: Download and prepare the ImageNet-Captions dataset, including generating TSV files; step4: Create domain mixtures for the experiments using the provided SLURM script or manually; step5: Optionally download and prepare CC3M or CC12M datasets if used as base datasets; step6: Train CLIP models using OpenCLIP, adjusting parameters based on the dataset used; step7: Train supervised classifiers using the provided script or manually; step8: Evaluate the classification performance of CLIP models and supervised classifiers; step9: Train Sparse Autoencoders (SAEs) for feature sharing evaluation; step10: Evaluate feature sharing using the trained SAEs; step11: Analyze embeddings and generate UMAP plots for Quickdraw embeddings; step12: Compute representational similarity using CKA; step13: Compute and evaluate circuits for aligned models", "executed_cmds": "pip install -e .;pip install -e deps/open_clip/;pip install -e deps/sparse_autoencoder/;. data/download_domainnet.sh;python scripts/generate_domainnet_captions.py --domainnet_path data/domainnet;wget https://github.com/mlfoundations/imagenet-captions/raw/5cf98361f5e67661fd5b2c6ee219567484440da9/imagenet_captions.zip;unzip imagenet_captions.zip;python scripts/generate_imagenet_captions.py --imagenet_train_path <path/to/imagenet/train>;sbatch slurm/subsample-domainnet.sh;python scripts/merge_ccxm.py --mode cc3m", "cmd_prefix": "python scripts/merge_ccxm.py", "cmd_postfix": "--mode cc12m", "target_cmd": "python scripts/merge_ccxm.py --mode cc12m"}
{"uuid": "c484fd6b-48bc-4341-a5f1-1ab5db57d8b5", "execution_plan": "step1: Clone the repository and set up the Python environment with the required dependencies; step2: Download and prepare the DomainNet dataset, including generating captions; step3: Download and prepare the ImageNet-Captions dataset, including generating TSV files; step4: Create domain mixtures for the experiments using the provided SLURM script or manually; step5: Optionally download and prepare CC3M or CC12M datasets if used as base datasets; step6: Train CLIP models using OpenCLIP, adjusting parameters based on the dataset used; step7: Train supervised classifiers using the provided script or manually; step8: Evaluate the classification performance of CLIP models and supervised classifiers; step9: Train Sparse Autoencoders (SAEs) for feature sharing evaluation; step10: Evaluate feature sharing using the trained SAEs; step11: Analyze embeddings and generate UMAP plots for Quickdraw embeddings; step12: Compute representational similarity using CKA; step13: Compute and evaluate circuits for aligned models", "executed_cmds": "pip install -e .;pip install -e deps/open_clip/;pip install -e deps/sparse_autoencoder/;. data/download_domainnet.sh;python scripts/generate_domainnet_captions.py --domainnet_path data/domainnet;wget https://github.com/mlfoundations/imagenet-captions/raw/5cf98361f5e67661fd5b2c6ee219567484440da9/imagenet_captions.zip;unzip imagenet_captions.zip;python scripts/generate_imagenet_captions.py --imagenet_train_path <path/to/imagenet/train>;sbatch slurm/subsample-domainnet.sh;python scripts/merge_ccxm.py --mode cc3m;python scripts/merge_ccxm.py --mode cc12m", "cmd_prefix": "cd", "cmd_postfix": "deps/open_clip", "target_cmd": "cd deps/open_clip"}
{"uuid": "6dc6e8f2-1189-4967-a825-68f437d773b8", "execution_plan": "step1: Download the dataset from the provided Google Drive link; step2: Install the required dependencies, including the transformers library and other packages listed in requirements.txt; step3: Run the ReDeEP(Chunk) experiments for hallucination detection and regression using the specified models and datasets; step4: Run the ReDeEP(Token) experiments for hallucination detection and regression using the specified models and datasets; step5: Run the AARF experiments for generating truthful answers and evaluating them using the specified models", "executed_cmds": "\"pip install -e transformers\"", "cmd_prefix": "\"pip install", "cmd_postfix": "-r requirements.txt\"", "target_cmd": "\"pip install -r requirements.txt\""}
{"uuid": "a2e646f5-eccf-4ed8-9d85-774b6e1ecdec", "execution_plan": "step1: Install required dependencies including PyTorch, flash-attention, and other packages listed in requirements.txt; step2: Log in to Hugging Face and Weights & Biases (wandb) for authentication; step3: Prepare the dataset for Phi-3-mini by running the data preparation script; step4: Execute the training script for TAID using the provided bash script", "executed_cmds": "pip install torch==2.2.2 --index-url https://download.pytorch.org/whl/cu121", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "466af36c-02ef-4ffd-ac02-ec9f19bb7f8a", "execution_plan": "step1: Install required dependencies including PyTorch, flash-attention, and other packages listed in requirements.txt; step2: Log in to Hugging Face and Weights & Biases (wandb) for authentication; step3: Prepare the dataset for Phi-3-mini by running the data preparation script; step4: Execute the training script for TAID using the provided bash script", "executed_cmds": "pip install torch==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install -r requirements.txt;git clone https://github.com/Dao-AILab/flash-attention.git", "cmd_prefix": "cd", "cmd_postfix": "flash-attention", "target_cmd": "cd flash-attention"}
{"uuid": "ef62fe20-1355-4154-8ceb-2d26ee44eedc", "execution_plan": "step1: Install required dependencies including PyTorch, flash-attention, and other packages listed in requirements.txt; step2: Log in to Hugging Face and Weights & Biases (wandb) for authentication; step3: Prepare the dataset for Phi-3-mini by running the data preparation script; step4: Execute the training script for TAID using the provided bash script", "executed_cmds": "pip install torch==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install -r requirements.txt;git clone https://github.com/Dao-AILab/flash-attention.git;cd flash-attention", "cmd_prefix": "python", "cmd_postfix": "setup.py install", "target_cmd": "python setup.py install"}
{"uuid": "926832d9-285e-4601-9aa7-c328cb955a44", "execution_plan": "step1: Install required dependencies including PyTorch, flash-attention, and other packages listed in requirements.txt; step2: Log in to Hugging Face and Weights & Biases (wandb) for authentication; step3: Prepare the dataset for Phi-3-mini by running the data preparation script; step4: Execute the training script for TAID using the provided bash script", "executed_cmds": "pip install torch==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install -r requirements.txt;git clone https://github.com/Dao-AILab/flash-attention.git;cd flash-attention;python setup.py install", "cmd_prefix": "huggingface-cli", "cmd_postfix": "login", "target_cmd": "huggingface-cli login"}
{"uuid": "4afee132-3f31-45ae-91b6-b3cd479a9713", "execution_plan": "step1: Install required dependencies including PyTorch, flash-attention, and other packages listed in requirements.txt; step2: Log in to Hugging Face and Weights & Biases (wandb) for authentication; step3: Prepare the dataset for Phi-3-mini by running the data preparation script; step4: Execute the training script for TAID using the provided bash script", "executed_cmds": "pip install torch==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install -r requirements.txt;git clone https://github.com/Dao-AILab/flash-attention.git;cd flash-attention;python setup.py install;huggingface-cli login", "cmd_prefix": "wandb", "cmd_postfix": "login", "target_cmd": "wandb login"}
{"uuid": "018b061b-79ad-418a-8d04-3ae3e5d9379a", "execution_plan": "step1: Clone the repository to your local machine; step2: Install the necessary dependencies by building the project; step3: Import the FP class and other required libraries in your Python script; step4: Initialize the FP model with the desired number of partitions and parameters; step5: Fit the model to your data matrix or low-rank SVD decomposition; step6: Retrieve the feature partitions generated by the model; step7: Embed each feature partition using a method like t-SNE for visualization or downstream tasks", "executed_cmds": "git clone https://github.com/erezpeter/Feature_Partition.git", "cmd_prefix": "python setup.py", "cmd_postfix": "build_ext --inplace", "target_cmd": "python setup.py build_ext --inplace"}
{"uuid": "b9e547cd-3b62-443d-90c6-313e8979ada5", "execution_plan": "step1: Create a conda environment with Python 3.11 and install PyTorch with CUDA 11.8 support; step2: Install additional dependencies from the requirements.txt file; step3: Download the original datasets (NeRF-Synthetic, MIP-NeRF360, Tanks-and-Temples) as per instructions in the dataset directory; step4: Optionally download pre-poisoned datasets from the provided Google Drive link; step5: Verify the installation by running the test script; step6: Benchmark computation cost on clean datasets using the provided scripts; step7: Create poisoned datasets and benchmark their computation cost using constrained or unconstrained attack scripts; step8: Benchmark black-box attack performance using scripts in the exp/02_blackbox_generalize/ directory; step9: Implement and test defense strategies using scripts in the exp/05_naive_defense/ directory", "executed_cmds": "conda create -n poison_splat python=3.11 -y;conda activate poison_splat;conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia -y", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "473377e9-1cb4-424d-bf65-ea1e82c3eb0a", "execution_plan": "step1: Set up the conda environment and install necessary dependencies including flash-attn and the project in development mode; step2: Set environment variables for OpenAI API key and Hugging Face authentication token; step3: Run evaluations on PAPRIKA tasks using the provided script; step4: Download the supervised finetuning dataset, update local paths, and run supervised finetuning; step5: Precompute reference model's log probabilities for preference finetuning by downloading the dataset and running the script; step6: Run preference finetuning (DPO/RPO) after precomputing log probabilities", "executed_cmds": "conda env create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate paprika", "target_cmd": "conda activate paprika"}
{"uuid": "755478e2-ba90-48b4-a58e-419761838b7c", "execution_plan": "step1: Set up the conda environment and install necessary dependencies including flash-attn and the project in development mode; step2: Set environment variables for OpenAI API key and Hugging Face authentication token; step3: Run evaluations on PAPRIKA tasks using the provided script; step4: Download the supervised finetuning dataset, update local paths, and run supervised finetuning; step5: Precompute reference model's log probabilities for preference finetuning by downloading the dataset and running the script; step6: Run preference finetuning (DPO/RPO) after precomputing log probabilities", "executed_cmds": "conda env create -f environment.yml;conda activate paprika;pip install flash-attn --no-build-isolation", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "27f5e3f5-5402-4aa0-85cf-2a945fbbedc1", "execution_plan": "step1: Set up the conda environment and install necessary dependencies including flash-attn and the project in development mode; step2: Set environment variables for OpenAI API key and Hugging Face authentication token; step3: Run evaluations on PAPRIKA tasks using the provided script; step4: Download the supervised finetuning dataset, update local paths, and run supervised finetuning; step5: Precompute reference model's log probabilities for preference finetuning by downloading the dataset and running the script; step6: Run preference finetuning (DPO/RPO) after precomputing log probabilities", "executed_cmds": "conda env create -f environment.yml;conda activate paprika;pip install flash-attn --no-build-isolation;pip install -e .", "cmd_prefix": "export", "cmd_postfix": "OAI_KEY=\"<API_KEY>\"", "target_cmd": "export OAI_KEY=\"<API_KEY>\""}
{"uuid": "e2cf240f-e76f-425e-8a00-3f477776f355", "execution_plan": "step1: Set up the conda environment and install necessary dependencies including flash-attn and the project in development mode; step2: Set environment variables for OpenAI API key and Hugging Face authentication token; step3: Run evaluations on PAPRIKA tasks using the provided script; step4: Download the supervised finetuning dataset, update local paths, and run supervised finetuning; step5: Precompute reference model's log probabilities for preference finetuning by downloading the dataset and running the script; step6: Run preference finetuning (DPO/RPO) after precomputing log probabilities", "executed_cmds": "conda env create -f environment.yml;conda activate paprika;pip install flash-attn --no-build-isolation;pip install -e .;export OAI_KEY=\"<API_KEY>\";export OPENAI_API_KEY=\"<API_KEY>\";export HF_TOKEN=\"<HUGGINGFACE_AUTHENTICATION_TOKEN>\"", "cmd_prefix": "bash", "cmd_postfix": "run_evaluation.sh", "target_cmd": "bash run_evaluation.sh"}
{"uuid": "dd394f9e-82b3-4473-8194-8385b019ae37", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a Conda environment with Python 3.11; step3: Install the project in editable mode; step4: Download and extract the datasets into the project directory; step5: Train the model using the provided script; step6: Test the model using the provided script", "executed_cmds": "git clone https://github.com/yakovlev31/pproc-dyn.git", "cmd_prefix": "cd", "cmd_postfix": "pproc-dyn", "target_cmd": "cd pproc-dyn"}
{"uuid": "24a66f20-5588-48e2-8d32-9b177bd3bcd5", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a Conda environment with Python 3.11; step3: Install the project in editable mode; step4: Download and extract the datasets into the project directory; step5: Train the model using the provided script; step6: Test the model using the provided script", "executed_cmds": "git clone https://github.com/yakovlev31/pproc-dyn.git;cd pproc-dyn;conda create -n pproc_env python=3.11", "cmd_prefix": "conda", "cmd_postfix": "activate pproc_env", "target_cmd": "conda activate pproc_env"}
{"uuid": "42c3d082-01f3-44c1-a781-861e720c4ba5", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a Conda environment with Python 3.11; step3: Install the project in editable mode; step4: Download and extract the datasets into the project directory; step5: Train the model using the provided script; step6: Test the model using the provided script", "executed_cmds": "git clone https://github.com/yakovlev31/pproc-dyn.git;cd pproc-dyn;conda create -n pproc_env python=3.11;conda activate pproc_env", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "40527e16-4864-451a-9152-6a0f3bd3e958", "execution_plan": "step1: Download and prepare the required datasets by following the specified steps; step2: Install the necessary Python packages listed in requirements.txt; step3: Run the main experiment script with specific parameters or use the provided batch script; step4: Process the experimental results to generate figures", "executed_cmds": "pip install -r requirements.txt;python main.py --problem \"FL\" --dataset \"Autotel\" --confidence 0.01;run_exp.bat", "cmd_prefix": "python", "cmd_postfix": "process_data.py", "target_cmd": "python process_data.py"}
{"uuid": "7db1eaef-92f0-4367-b475-4a9dacb4fbdc", "execution_plan": "step1: Create a virtual environment for the project using conda; step2: Activate the virtual environment; step3: Install PyTorch according to the official instructions; step4: Install the required packages from the requirements.txt file; step5: Run the main script with the desired problem name to train and evaluate the ACSSM model", "executed_cmds": "conda create -n acssm python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate acssm", "target_cmd": "conda activate acssm"}
{"uuid": "4f485f82-92ff-40f5-8799-968e2c9bfd8e", "execution_plan": "step1: Create a virtual environment for the project using conda; step2: Activate the virtual environment; step3: Install PyTorch according to the official instructions; step4: Install the required packages from the requirements.txt file; step5: Run the main script with the desired problem name to train and evaluate the ACSSM model", "executed_cmds": "conda create -n acssm python=3.10;conda activate acssm", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "57bb9f50-508b-453d-a63c-e1d687b1510d", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment and install required packages; step3: Download pretrained model components from HuggingFace; step4: Prepare the datasets by downloading and organizing them; step5: Train the model using the provided script; step6: Evaluate the model on image-based benchmarks; step7: Perform additional evaluation for GQA using a separate script", "executed_cmds": "git clone https://github.com/RainBowLuoCS/DEEM.git", "cmd_prefix": "conda create -n", "cmd_postfix": "deem python=3.10 -y", "target_cmd": "conda create -n deem python=3.10 -y"}
{"uuid": "7dffd729-5804-4f33-937a-02f320148d61", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment and install required packages; step3: Download pretrained model components from HuggingFace; step4: Prepare the datasets by downloading and organizing them; step5: Train the model using the provided script; step6: Evaluate the model on image-based benchmarks; step7: Perform additional evaluation for GQA using a separate script", "executed_cmds": "git clone https://github.com/RainBowLuoCS/DEEM.git;conda create -n deem python=3.10 -y", "cmd_prefix": "conda", "cmd_postfix": "activate deem", "target_cmd": "conda activate deem"}
{"uuid": "bf1adaaa-68bc-4b71-bb0d-99b0eb9ff391", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment and install required packages; step3: Download pretrained model components from HuggingFace; step4: Prepare the datasets by downloading and organizing them; step5: Train the model using the provided script; step6: Evaluate the model on image-based benchmarks; step7: Perform additional evaluation for GQA using a separate script", "executed_cmds": "git clone https://github.com/RainBowLuoCS/DEEM.git;conda create -n deem python=3.10 -y;conda activate deem", "cmd_prefix": "cd", "cmd_postfix": "DEEM", "target_cmd": "cd DEEM"}
{"uuid": "383c4cbe-be4a-4c41-900b-70d0dccf69f7", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment and install required packages; step3: Download pretrained model components from HuggingFace; step4: Prepare the datasets by downloading and organizing them; step5: Train the model using the provided script; step6: Evaluate the model on image-based benchmarks; step7: Perform additional evaluation for GQA using a separate script", "executed_cmds": "git clone https://github.com/RainBowLuoCS/DEEM.git;conda create -n deem python=3.10 -y;conda activate deem;cd DEEM", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "177d917a-d9d4-434f-adfe-ddd8b4c695be", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment and install required packages; step3: Download pretrained model components from HuggingFace; step4: Prepare the datasets by downloading and organizing them; step5: Train the model using the provided script; step6: Evaluate the model on image-based benchmarks; step7: Perform additional evaluation for GQA using a separate script", "executed_cmds": "git clone https://github.com/RainBowLuoCS/DEEM.git;conda create -n deem python=3.10 -y;conda activate deem;cd DEEM;pip install -r requirements.txt;cd uni_interleaved/models/utils/ops", "cmd_prefix": "python", "cmd_postfix": "setup.py install", "target_cmd": "python setup.py install"}
{"uuid": "11087a1c-b83a-462d-ae48-8665669a9276", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment and install required packages; step3: Download pretrained model components from HuggingFace; step4: Prepare the datasets by downloading and organizing them; step5: Train the model using the provided script; step6: Evaluate the model on image-based benchmarks; step7: Perform additional evaluation for GQA using a separate script", "executed_cmds": "git clone https://github.com/RainBowLuoCS/DEEM.git;conda create -n deem python=3.10 -y;conda activate deem;cd DEEM;pip install -r requirements.txt;cd uni_interleaved/models/utils/ops;python setup.py install;python scripts/download_models.py;python ./scripts/download_laion.py --mode=annt;python ./scripts/download_mmc4.py --mode=annt;python ./scripts/download_laion.py --mode=images;python ./scripts/download_mmc4.py --mode=images;python ./scripts/convert_mmc4_for_pretrain.py;python ./scripts/convert_imagenet_for_robustvqa_test.py", "cmd_prefix": "bash", "cmd_postfix": "scripts/train.sh", "target_cmd": "bash scripts/train.sh"}
{"uuid": "9c9c8ccf-ffaa-4aca-b6a5-681c8c49b3b6", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up a conda environment and install required packages; step3: Download pretrained model components from HuggingFace; step4: Prepare the datasets by downloading and organizing them; step5: Train the model using the provided script; step6: Evaluate the model on image-based benchmarks; step7: Perform additional evaluation for GQA using a separate script", "executed_cmds": "git clone https://github.com/RainBowLuoCS/DEEM.git;conda create -n deem python=3.10 -y;conda activate deem;cd DEEM;pip install -r requirements.txt;cd uni_interleaved/models/utils/ops;python setup.py install;python scripts/download_models.py;python ./scripts/download_laion.py --mode=annt;python ./scripts/download_mmc4.py --mode=annt;python ./scripts/download_laion.py --mode=images;python ./scripts/download_mmc4.py --mode=images;python ./scripts/convert_mmc4_for_pretrain.py;python ./scripts/convert_imagenet_for_robustvqa_test.py;bash scripts/train.sh", "cmd_prefix": "bash", "cmd_postfix": "scripts/evaluate.sh", "target_cmd": "bash scripts/evaluate.sh"}
{"uuid": "9afa0b64-d867-4f50-8081-ab1f44bfdddb", "execution_plan": "step1: Understand the project's purpose and requirements by reading the README and ensuring Python 3.10+ is installed; step2: Download or clone the repository to your local machine; step3: Install any necessary dependencies or libraries required for the project; step4: Set up the evaluation environment, including datasets like AlpacaEval and judge models (GPT-4-Turbo or GPT-3.5-Turbo); step5: Run the evaluation scripts to measure non-transitivity and compare rankings using round-robin or Swim tournaments; step6: Analyze the results and compare them with human evaluations from Chatbot Arena", "executed_cmds": "git clone [repository_url]", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "c03d042f-3442-4dd9-9222-864ac4112d0f", "execution_plan": "step1: Understand the project's purpose and requirements by reading the README and ensuring Python 3.10+ is installed; step2: Download or clone the repository to your local machine; step3: Install any necessary dependencies or libraries required for the project; step4: Set up the evaluation environment, including datasets like AlpacaEval and judge models (GPT-4-Turbo or GPT-3.5-Turbo); step5: Run the evaluation scripts to measure non-transitivity and compare rankings using round-robin or Swim tournaments; step6: Analyze the results and compare them with human evaluations from Chatbot Arena", "executed_cmds": "git clone [repository_url];pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "run_evaluation.py", "target_cmd": "python run_evaluation.py"}
{"uuid": "0c640645-ba8e-4f29-a999-5fbf5a4401e2", "execution_plan": "step1: Create a new virtual environment using conda; step2: Install the required packages listed in requirements.txt; step3: Activate the virtual environment; step4: Set up API keys for OpenAI and GenAI; step5: Prepare the data directory and place dataset files in it; step6: Run the main script with specified model and dataset parameters", "executed_cmds": "conda create -n mdagents python>=3.9", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "666f71ef-f159-487f-9001-28f49f9093dd", "execution_plan": "step1: Create a new virtual environment using conda; step2: Install the required packages listed in requirements.txt; step3: Activate the virtual environment; step4: Set up API keys for OpenAI and GenAI; step5: Prepare the data directory and place dataset files in it; step6: Run the main script with specified model and dataset parameters", "executed_cmds": "conda create -n mdagents python>=3.9;pip install -r requirements.txt", "cmd_prefix": "conda", "cmd_postfix": "activate mdagents", "target_cmd": "conda activate mdagents"}
{"uuid": "5bacb5d1-b1d3-4e81-b3e5-95a3a003fb1e", "execution_plan": "step1: Create a new virtual environment using conda; step2: Install the required packages listed in requirements.txt; step3: Activate the virtual environment; step4: Set up API keys for OpenAI and GenAI; step5: Prepare the data directory and place dataset files in it; step6: Run the main script with specified model and dataset parameters", "executed_cmds": "conda create -n mdagents python>=3.9;pip install -r requirements.txt;conda activate mdagents;export openai_api_key=\"your_openai_api_key_here\";export genai_api_key=\"your_genai_api_key_here\"", "cmd_prefix": "mkdir", "cmd_postfix": "-p ./data", "target_cmd": "mkdir -p ./data"}
{"uuid": "e65ce86b-3c96-4085-8252-a127feeecfb5", "execution_plan": "step1: Create a new virtual environment using conda; step2: Install the required packages listed in requirements.txt; step3: Activate the virtual environment; step4: Set up API keys for OpenAI and GenAI; step5: Prepare the data directory and place dataset files in it; step6: Run the main script with specified model and dataset parameters", "executed_cmds": "conda create -n mdagents python>=3.9;pip install -r requirements.txt;conda activate mdagents;export openai_api_key=\"your_openai_api_key_here\";export genai_api_key=\"your_genai_api_key_here\";mkdir -p ./data", "cmd_prefix": "python3 main.py", "cmd_postfix": "--model {gpt-3.5", "target_cmd": "python3 main.py --model {gpt-3.5"}
{"uuid": "b1e69ff9-7cea-4e90-aad3-2e2cf558c999", "execution_plan": "step1: Create a new virtual environment using conda; step2: Install the required packages listed in requirements.txt; step3: Activate the virtual environment; step4: Set up API keys for OpenAI and GenAI; step5: Prepare the data directory and place dataset files in it; step6: Run the main script with specified model and dataset parameters", "executed_cmds": "conda create -n mdagents python>=3.9;pip install -r requirements.txt;conda activate mdagents;export openai_api_key=\"your_openai_api_key_here\";export genai_api_key=\"your_genai_api_key_here\";mkdir -p ./data;python3 main.py --model {gpt-3.5;gpt-4;gpt-4v;gpt-4o;gemini-pro", "cmd_prefix": "gemini-pro-vision}", "cmd_postfix": "--dataset {medqa", "target_cmd": "gemini-pro-vision} --dataset {medqa"}
{"uuid": "c2a859a7-55f2-4985-b3c3-f66cc3248833", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git", "cmd_prefix": "cd", "cmd_postfix": "comfort_utils", "target_cmd": "cd comfort_utils"}
{"uuid": "73a34879-d344-4fc2-8776-098187589aab", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate comfort", "target_cmd": "conda activate comfort"}
{"uuid": "08b56538-42c6-4780-9de3-4d67388a07a7", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort", "cmd_prefix": "cd", "cmd_postfix": "models/GLAMM", "target_cmd": "cd models/GLAMM"}
{"uuid": "238c9c3f-493d-4b87-ba62-35a928fe102e", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "36aedf2d-cc2e-4bb0-9367-b0cde62842fe", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .", "cmd_prefix": "cd", "cmd_postfix": "models/llava", "target_cmd": "cd models/llava"}
{"uuid": "257d1c76-7f1a-4320-bf8a-6c7093e87cc9", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .;cd models/llava", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "93f6e8f6-b7b8-4510-b8f5-2a2468064d1e", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .;cd models/llava;pip install -e .;cd models/InternVL/internvl_chat", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "4f51c8f6-a663-45d1-bc80-a421bd1123a2", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .;cd models/llava;pip install -e .;cd models/InternVL/internvl_chat;pip install -e .", "cmd_prefix": "mkdir", "cmd_postfix": "data", "target_cmd": "mkdir data"}
{"uuid": "c18ea5f0-7ba9-432b-b0ae-beea2746d0dd", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .;cd models/llava;pip install -e .;cd models/InternVL/internvl_chat;pip install -e .;mkdir data;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_ball.zip?download=true -O data/comfort_ball.zip", "cmd_prefix": "unzip data/comfort_ball.zip", "cmd_postfix": "-d data/", "target_cmd": "unzip data/comfort_ball.zip -d data/"}
{"uuid": "a589e478-680a-4906-9e4c-eaba2b10fb12", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .;cd models/llava;pip install -e .;cd models/InternVL/internvl_chat;pip install -e .;mkdir data;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_ball.zip?download=true -O data/comfort_ball.zip;unzip data/comfort_ball.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_left.zip?download=true -O data/comfort_car_ref_facing_left.zip", "cmd_prefix": "unzip data/comfort_car_ref_facing_left.zip", "cmd_postfix": "-d data/", "target_cmd": "unzip data/comfort_car_ref_facing_left.zip -d data/"}
{"uuid": "0b48d5df-2458-42d0-83b0-e67032f88058", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .;cd models/llava;pip install -e .;cd models/InternVL/internvl_chat;pip install -e .;mkdir data;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_ball.zip?download=true -O data/comfort_ball.zip;unzip data/comfort_ball.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_left.zip?download=true -O data/comfort_car_ref_facing_left.zip;unzip data/comfort_car_ref_facing_left.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_right.zip?download=true -O data/comfort_car_ref_facing_right.zip", "cmd_prefix": "unzip data/comfort_car_ref_facing_right.zip", "cmd_postfix": "-d data/", "target_cmd": "unzip data/comfort_car_ref_facing_right.zip -d data/"}
{"uuid": "9e36e716-ab2a-4d72-ba15-637c591bc729", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .;cd models/llava;pip install -e .;cd models/InternVL/internvl_chat;pip install -e .;mkdir data;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_ball.zip?download=true -O data/comfort_ball.zip;unzip data/comfort_ball.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_left.zip?download=true -O data/comfort_car_ref_facing_left.zip;unzip data/comfort_car_ref_facing_left.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_right.zip?download=true -O data/comfort_car_ref_facing_right.zip;unzip data/comfort_car_ref_facing_right.zip -d data/", "cmd_prefix": "pip", "cmd_postfix": "install gdown", "target_cmd": "pip install gdown"}
{"uuid": "0d9022a6-16b1-47e5-919b-bc7a9a496403", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .;cd models/llava;pip install -e .;cd models/InternVL/internvl_chat;pip install -e .;mkdir data;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_ball.zip?download=true -O data/comfort_ball.zip;unzip data/comfort_ball.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_left.zip?download=true -O data/comfort_car_ref_facing_left.zip;unzip data/comfort_car_ref_facing_left.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_right.zip?download=true -O data/comfort_car_ref_facing_right.zip;unzip data/comfort_car_ref_facing_right.zip -d data/;pip install gdown", "cmd_prefix": "python", "cmd_postfix": "download_assets.py", "target_cmd": "python download_assets.py"}
{"uuid": "5a0f4685-e698-4d03-8997-cfd5648841b8", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .;cd models/llava;pip install -e .;cd models/InternVL/internvl_chat;pip install -e .;mkdir data;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_ball.zip?download=true -O data/comfort_ball.zip;unzip data/comfort_ball.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_left.zip?download=true -O data/comfort_car_ref_facing_left.zip;unzip data/comfort_car_ref_facing_left.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_right.zip?download=true -O data/comfort_car_ref_facing_right.zip;unzip data/comfort_car_ref_facing_right.zip -d data/;pip install gdown;python download_assets.py;chmod +x generate_dataset.sh;./generate_dataset.sh;touch comfort_utils/model_utils/api_keys.py;./run_english_ball_experiments.sh;./run_english_car_left_experiments.sh;./run_english_car_right_experiments.sh;export GOOGLE_APPLICATION_CREDENTIALS=\"your_google_application_credentials_path.json\";./run_multilingual_ball_experiments.sh;./run_multilingual_car_left_experiments.sh;./run_multilingual_car_right_experiments.sh", "cmd_prefix": "python gather_results.py --mode", "cmd_postfix": "cpp --cpp convention", "target_cmd": "python gather_results.py --mode cpp --cpp convention"}
{"uuid": "a5ae2fbc-4ffe-4d1b-9522-abe49db192d4", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .;cd models/llava;pip install -e .;cd models/InternVL/internvl_chat;pip install -e .;mkdir data;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_ball.zip?download=true -O data/comfort_ball.zip;unzip data/comfort_ball.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_left.zip?download=true -O data/comfort_car_ref_facing_left.zip;unzip data/comfort_car_ref_facing_left.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_right.zip?download=true -O data/comfort_car_ref_facing_right.zip;unzip data/comfort_car_ref_facing_right.zip -d data/;pip install gdown;python download_assets.py;chmod +x generate_dataset.sh;./generate_dataset.sh;touch comfort_utils/model_utils/api_keys.py;./run_english_ball_experiments.sh;./run_english_car_left_experiments.sh;./run_english_car_right_experiments.sh;export GOOGLE_APPLICATION_CREDENTIALS=\"your_google_application_credentials_path.json\";./run_multilingual_ball_experiments.sh;./run_multilingual_car_left_experiments.sh;./run_multilingual_car_right_experiments.sh;python gather_results.py --mode cpp --cpp convention;python gather_results.py --mode cpp --cpp preferredfor;python gather_results.py --mode cpp --cpp perspective", "cmd_prefix": "python gather_results.py", "cmd_postfix": "--mode comprehensive", "target_cmd": "python gather_results.py --mode comprehensive"}
{"uuid": "e4cb08ad-5f18-4134-b6ba-6b434dd506b6", "execution_plan": "step1: Clone the repository and set up the conda environment using the provided environment.yml file; step2: Activate the conda environment and install editable packages for GLAMM, llava, and InternVL; step3: Create a data directory and prepare the data either by downloading from Huggingface or generating it locally; step4: Add API credentials for OpenAI, DeepL, and Google Cloud Translate; step5: Run experiments for English and multilingual datasets; step6: Run evaluations for English and multilingual datasets, including preferred coordinate transformation, preferred frame of reference, perspective taking, and comprehensive evaluation; step7: Evaluate additional models using the provided Model Wrapper; step8: Address common problems and solutions, such as resolving ImportError for libcupti.so.11.7", "executed_cmds": "git clone https://github.com/sled-group/COMFORT.git;cd comfort_utils;conda env create -f environment.yml;conda activate comfort;cd models/GLAMM;pip install -e .;cd models/llava;pip install -e .;cd models/InternVL/internvl_chat;pip install -e .;mkdir data;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_ball.zip?download=true -O data/comfort_ball.zip;unzip data/comfort_ball.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_left.zip?download=true -O data/comfort_car_ref_facing_left.zip;unzip data/comfort_car_ref_facing_left.zip -d data/;wget https://huggingface.co/datasets/sled-umich/COMFORT/resolve/main/comfort_car_ref_facing_right.zip?download=true -O data/comfort_car_ref_facing_right.zip;unzip data/comfort_car_ref_facing_right.zip -d data/;pip install gdown;python download_assets.py;chmod +x generate_dataset.sh;./generate_dataset.sh;touch comfort_utils/model_utils/api_keys.py;./run_english_ball_experiments.sh;./run_english_car_left_experiments.sh;./run_english_car_right_experiments.sh;export GOOGLE_APPLICATION_CREDENTIALS=\"your_google_application_credentials_path.json\";./run_multilingual_ball_experiments.sh;./run_multilingual_car_left_experiments.sh;./run_multilingual_car_right_experiments.sh;python gather_results.py --mode cpp --cpp convention;python gather_results.py --mode cpp --cpp preferredfor;python gather_results.py --mode cpp --cpp perspective;python gather_results.py --mode comprehensive;python gather_results_multilingual.py", "cmd_prefix": "cd", "cmd_postfix": "results/eval", "target_cmd": "cd results/eval"}
{"uuid": "b8616971-b6a7-4178-ae7b-0d8b313ccc6e", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig", "cmd_prefix": "conda", "cmd_postfix": "activate magicpig", "target_cmd": "conda activate magicpig"}
{"uuid": "6db21beb-22c5-4d6d-8545-8f5746a1982c", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig", "cmd_prefix": "bash", "cmd_postfix": "install.sh", "target_cmd": "bash install.sh"}
{"uuid": "053e71dc-397c-4146-8716-62d01f57fe86", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh", "cmd_prefix": "cd", "cmd_postfix": "examples", "target_cmd": "cd examples"}
{"uuid": "5b908ed1-924e-4e5a-97c5-3b40a9617ce3", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples", "cmd_prefix": "numactl", "cmd_postfix": "-C 0-31", "target_cmd": "numactl -C 0-31"}
{"uuid": "4ff1b5ef-8473-47a8-9ca3-b30a0bdf6c6f", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31", "cmd_prefix": "52-83", "cmd_postfix": "-m 0", "target_cmd": "52-83 -m 0"}
{"uuid": "9ef4bc5b-6175-4891-ad66-5846967e7f61", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt", "cmd_prefix": "cd", "cmd_postfix": "examples", "target_cmd": "cd examples"}
{"uuid": "16105c16-afa1-46c9-9437-6624bb5a8e58", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples", "cmd_prefix": "numactl", "cmd_postfix": "-C 0-31", "target_cmd": "numactl -C 0-31"}
{"uuid": "18e94e61-a61e-419a-be47-2a83465aa18f", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31", "cmd_prefix": "52-83", "cmd_postfix": "-m 0", "target_cmd": "52-83 -m 0"}
{"uuid": "7463b99d-7ee2-4178-91d5-c914afec97a4", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31;52-83 -m 0;1 python bench.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --B 1 --P 98000 --M 98304 --K 10 --L 150", "cmd_prefix": "cd", "cmd_postfix": "evaluations/RULER", "target_cmd": "cd evaluations/RULER"}
{"uuid": "41213027-30b1-4adb-b2c3-bdbb6d7c02c2", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31;52-83 -m 0;1 python bench.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --B 1 --P 98000 --M 98304 --K 10 --L 150;cd evaluations/RULER", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "63d7886b-3c03-469c-88a4-7e8db8af24eb", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31;52-83 -m 0;1 python bench.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --B 1 --P 98000 --M 98304 --K 10 --L 150;cd evaluations/RULER;pip install -r requirements.txt", "cmd_prefix": "cd", "cmd_postfix": "evaluations/RULER", "target_cmd": "cd evaluations/RULER"}
{"uuid": "52964161-6e55-4d4f-a815-e7496d624403", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31;52-83 -m 0;1 python bench.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --B 1 --P 98000 --M 98304 --K 10 --L 150;cd evaluations/RULER;pip install -r requirements.txt;cd evaluations/RULER", "cmd_prefix": "python", "cmd_postfix": "download_nltk.py", "target_cmd": "python download_nltk.py"}
{"uuid": "27049d35-92f6-4acd-9291-060f48cc7fc3", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31;52-83 -m 0;1 python bench.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --B 1 --P 98000 --M 98304 --K 10 --L 150;cd evaluations/RULER;pip install -r requirements.txt;cd evaluations/RULER;python download_nltk.py", "cmd_prefix": "bash run.sh llama3-8b-chat-128k", "cmd_postfix": "synthetic $K $L", "target_cmd": "bash run.sh llama3-8b-chat-128k synthetic $K $L"}
{"uuid": "b7760df6-27a7-4cc3-8003-b4e4d1dcab64", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31;52-83 -m 0;1 python bench.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --B 1 --P 98000 --M 98304 --K 10 --L 150;cd evaluations/RULER;pip install -r requirements.txt;cd evaluations/RULER;python download_nltk.py;bash run.sh llama3-8b-chat-128k synthetic $K $L", "cmd_prefix": "cd", "cmd_postfix": "evaluations/RULER", "target_cmd": "cd evaluations/RULER"}
{"uuid": "1caa1039-212d-4c88-8e16-b17bafcd1b36", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31;52-83 -m 0;1 python bench.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --B 1 --P 98000 --M 98304 --K 10 --L 150;cd evaluations/RULER;pip install -r requirements.txt;cd evaluations/RULER;python download_nltk.py;bash run.sh llama3-8b-chat-128k synthetic $K $L;cd evaluations/RULER", "cmd_prefix": "python", "cmd_postfix": "download_nltk.py", "target_cmd": "python download_nltk.py"}
{"uuid": "c65c72bb-e416-4e60-a65b-6cb795bb17f7", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31;52-83 -m 0;1 python bench.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --B 1 --P 98000 --M 98304 --K 10 --L 150;cd evaluations/RULER;pip install -r requirements.txt;cd evaluations/RULER;python download_nltk.py;bash run.sh llama3-8b-chat-128k synthetic $K $L;cd evaluations/RULER;python download_nltk.py", "cmd_prefix": "bash run_tensor_parallel.sh llama3-8b-chat-128k", "cmd_postfix": "synthetic $K $L", "target_cmd": "bash run_tensor_parallel.sh llama3-8b-chat-128k synthetic $K $L"}
{"uuid": "3306a47e-ffd2-4ba2-b689-851e5f53a1ec", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31;52-83 -m 0;1 python bench.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --B 1 --P 98000 --M 98304 --K 10 --L 150;cd evaluations/RULER;pip install -r requirements.txt;cd evaluations/RULER;python download_nltk.py;bash run.sh llama3-8b-chat-128k synthetic $K $L;cd evaluations/RULER;python download_nltk.py;bash run_tensor_parallel.sh llama3-8b-chat-128k synthetic $K $L", "cmd_prefix": "cd", "cmd_postfix": "evaluations/RULER", "target_cmd": "cd evaluations/RULER"}
{"uuid": "cb29afde-22ff-4be9-b3ed-677b8a3b19c1", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31;52-83 -m 0;1 python bench.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --B 1 --P 98000 --M 98304 --K 10 --L 150;cd evaluations/RULER;pip install -r requirements.txt;cd evaluations/RULER;python download_nltk.py;bash run.sh llama3-8b-chat-128k synthetic $K $L;cd evaluations/RULER;python download_nltk.py;bash run_tensor_parallel.sh llama3-8b-chat-128k synthetic $K $L;cd evaluations/RULER", "cmd_prefix": "python", "cmd_postfix": "download_nltk.py", "target_cmd": "python download_nltk.py"}
{"uuid": "90b1812e-0e50-4366-af7a-90bbec22e31a", "execution_plan": "step1: Create a Conda environment named 'magicpig' and activate it; step2: Run the installation script to set up the necessary dependencies; step3: Navigate to the examples directory and run the generation script with specified parameters; step4: Navigate to the examples directory and run the benchmark script with specified parameters; step5: Install the RULER evaluation environment by navigating to the evaluations/RULER directory and installing required packages; step6: Run the RULER benchmark script with specified parameters; step7: Optionally, run the Tensor Parallelism version of the RULER benchmark for accuracy evaluation; step8: Optionally, run the Single GPU (Huggingface + Mask) version of the RULER benchmark for accuracy evaluation", "executed_cmds": "conda create -n magicpig;conda activate magicpig;bash install.sh;cd examples;numactl -C 0-31;52-83 -m 0;1 python generation.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --M 8192 --G 256 --K 10 --L 170 --template meta-llama3 --data ../data/story.txt;cd examples;numactl -C 0-31;52-83 -m 0;1 python bench.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --B 1 --P 98000 --M 98304 --K 10 --L 150;cd evaluations/RULER;pip install -r requirements.txt;cd evaluations/RULER;python download_nltk.py;bash run.sh llama3-8b-chat-128k synthetic $K $L;cd evaluations/RULER;python download_nltk.py;bash run_tensor_parallel.sh llama3-8b-chat-128k synthetic $K $L;cd evaluations/RULER;python download_nltk.py", "cmd_prefix": "bash run_single_gpu.sh llama3-8b-chat-128k synthetic $K", "cmd_postfix": "$L 4 64 $method 0", "target_cmd": "bash run_single_gpu.sh llama3-8b-chat-128k synthetic $K $L 4 64 $method 0"}
{"uuid": "009038f3-76b2-4ec3-a1cb-581445f7a4dc", "execution_plan": "step1: Install CUDA 11.8 if not already installed; step2: Install PyTorch Nightly; step3: Build XFormers from source; step4: Install Flash-Attention 2 and other fused operators; step5: Install remaining dependencies; step6: Preprocess the data; step7: Set up Wandb integration (optional); step8: Evaluate attention sink in open-sourced LMs; step9: Pre-train LLaMA models with default setup; step10: Pre-train LLaMA models with 1B parameters", "executed_cmds": "pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev';pip uninstall ninja -y && pip install ninja -U;pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers;git clone https://github.com/Dao-AILab/flash-attention", "cmd_prefix": "cd", "cmd_postfix": "flash-attention", "target_cmd": "cd flash-attention"}
{"uuid": "df747644-f2db-4712-ae08-3b529bd00d81", "execution_plan": "step1: Install CUDA 11.8 if not already installed; step2: Install PyTorch Nightly; step3: Build XFormers from source; step4: Install Flash-Attention 2 and other fused operators; step5: Install remaining dependencies; step6: Preprocess the data; step7: Set up Wandb integration (optional); step8: Evaluate attention sink in open-sourced LMs; step9: Pre-train LLaMA models with default setup; step10: Pre-train LLaMA models with 1B parameters", "executed_cmds": "pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev';pip uninstall ninja -y && pip install ninja -U;pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers;git clone https://github.com/Dao-AILab/flash-attention;cd flash-attention", "cmd_prefix": "python", "cmd_postfix": "setup.py install", "target_cmd": "python setup.py install"}
{"uuid": "c9ac8e8f-cf96-4811-9d8b-b4c6a0b8efb4", "execution_plan": "step1: Install CUDA 11.8 if not already installed; step2: Install PyTorch Nightly; step3: Build XFormers from source; step4: Install Flash-Attention 2 and other fused operators; step5: Install remaining dependencies; step6: Preprocess the data; step7: Set up Wandb integration (optional); step8: Evaluate attention sink in open-sourced LMs; step9: Pre-train LLaMA models with default setup; step10: Pre-train LLaMA models with 1B parameters", "executed_cmds": "pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev';pip uninstall ninja -y && pip install ninja -U;pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers;git clone https://github.com/Dao-AILab/flash-attention;cd flash-attention;python setup.py install", "cmd_prefix": "cd csrc/rotary &&", "cmd_postfix": "pip install .", "target_cmd": "cd csrc/rotary && pip install ."}
{"uuid": "f976d8a3-e4ed-4044-9e6a-54843a823244", "execution_plan": "step1: Install CUDA 11.8 if not already installed; step2: Install PyTorch Nightly; step3: Build XFormers from source; step4: Install Flash-Attention 2 and other fused operators; step5: Install remaining dependencies; step6: Preprocess the data; step7: Set up Wandb integration (optional); step8: Evaluate attention sink in open-sourced LMs; step9: Pre-train LLaMA models with default setup; step10: Pre-train LLaMA models with 1B parameters", "executed_cmds": "pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev';pip uninstall ninja -y && pip install ninja -U;pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers;git clone https://github.com/Dao-AILab/flash-attention;cd flash-attention;python setup.py install;cd csrc/rotary && pip install .", "cmd_prefix": "cd ../layer_norm &&", "cmd_postfix": "pip install .", "target_cmd": "cd ../layer_norm && pip install ."}
{"uuid": "35f6dea5-7a2e-4eee-a806-11beafcbd13e", "execution_plan": "step1: Install CUDA 11.8 if not already installed; step2: Install PyTorch Nightly; step3: Build XFormers from source; step4: Install Flash-Attention 2 and other fused operators; step5: Install remaining dependencies; step6: Preprocess the data; step7: Set up Wandb integration (optional); step8: Evaluate attention sink in open-sourced LMs; step9: Pre-train LLaMA models with default setup; step10: Pre-train LLaMA models with 1B parameters", "executed_cmds": "pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev';pip uninstall ninja -y && pip install ninja -U;pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers;git clone https://github.com/Dao-AILab/flash-attention;cd flash-attention;python setup.py install;cd csrc/rotary && pip install .;cd ../layer_norm && pip install .", "cmd_prefix": "cd ../xentropy &&", "cmd_postfix": "pip install .", "target_cmd": "cd ../xentropy && pip install ."}
{"uuid": "a73e6e69-d9df-43b7-b2c1-3d670db9afaf", "execution_plan": "step1: Install CUDA 11.8 if not already installed; step2: Install PyTorch Nightly; step3: Build XFormers from source; step4: Install Flash-Attention 2 and other fused operators; step5: Install remaining dependencies; step6: Preprocess the data; step7: Set up Wandb integration (optional); step8: Evaluate attention sink in open-sourced LMs; step9: Pre-train LLaMA models with default setup; step10: Pre-train LLaMA models with 1B parameters", "executed_cmds": "pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev';pip uninstall ninja -y && pip install ninja -U;pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers;git clone https://github.com/Dao-AILab/flash-attention;cd flash-attention;python setup.py install;cd csrc/rotary && pip install .;cd ../layer_norm && pip install .;cd ../xentropy && pip install .;cd ../.. && rm -rf flash-attention;pip install -r requirements.txt tokenizers sentencepiece", "cmd_prefix": "cd", "cmd_postfix": "preprocess", "target_cmd": "cd preprocess"}
{"uuid": "45665d77-b576-4902-a300-189146968723", "execution_plan": "step1: Install CUDA 11.8 if not already installed; step2: Install PyTorch Nightly; step3: Build XFormers from source; step4: Install Flash-Attention 2 and other fused operators; step5: Install remaining dependencies; step6: Preprocess the data; step7: Set up Wandb integration (optional); step8: Evaluate attention sink in open-sourced LMs; step9: Pre-train LLaMA models with default setup; step10: Pre-train LLaMA models with 1B parameters", "executed_cmds": "pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev';pip uninstall ninja -y && pip install ninja -U;pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers;git clone https://github.com/Dao-AILab/flash-attention;cd flash-attention;python setup.py install;cd csrc/rotary && pip install .;cd ../layer_norm && pip install .;cd ../xentropy && pip install .;cd ../.. && rm -rf flash-attention;pip install -r requirements.txt tokenizers sentencepiece;cd preprocess", "cmd_prefix": "bash", "cmd_postfix": "run_preprocess.sh", "target_cmd": "bash run_preprocess.sh"}
{"uuid": "bac22c79-3f84-4c8a-8469-b88da85aa978", "execution_plan": "step1: Enable Web Bluetooth on Chrome to allow communication with LEGO hubs; step2: Create and activate a Conda environment for the project; step3: Install PyTorch with CUDA support; step4: Install the bricksrl package and its dependencies; step5: (Optional) Install development tools for developers; step6: Update the client script on the Pybricks Hub for the desired environment; step7: Review and modify configuration settings as needed; step8: Train an agent using the provided training script; step9: Evaluate the trained agent using the evaluation script; step10: (Optional) Download datasets for offline RL pretraining; step11: (Optional) Pretrain an agent using offline RL; step12: (Optional) Fine-tune the pretrained policy on the real robot", "executed_cmds": "conda create --name bricksrl python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate bricksrl", "target_cmd": "conda activate bricksrl"}
{"uuid": "d05715ac-eeb2-4f0b-82c5-7a5284bf1a45", "execution_plan": "step1: Enable Web Bluetooth on Chrome to allow communication with LEGO hubs; step2: Create and activate a Conda environment for the project; step3: Install PyTorch with CUDA support; step4: Install the bricksrl package and its dependencies; step5: (Optional) Install development tools for developers; step6: Update the client script on the Pybricks Hub for the desired environment; step7: Review and modify configuration settings as needed; step8: Train an agent using the provided training script; step9: Evaluate the trained agent using the evaluation script; step10: (Optional) Download datasets for offline RL pretraining; step11: (Optional) Pretrain an agent using offline RL; step12: (Optional) Fine-tune the pretrained policy on the real robot", "executed_cmds": "conda create --name bricksrl python=3.8;conda activate bricksrl;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "7ca44153-44c6-48df-a53b-114b02a02b21", "execution_plan": "step1: Enable Web Bluetooth on Chrome to allow communication with LEGO hubs; step2: Create and activate a Conda environment for the project; step3: Install PyTorch with CUDA support; step4: Install the bricksrl package and its dependencies; step5: (Optional) Install development tools for developers; step6: Update the client script on the Pybricks Hub for the desired environment; step7: Review and modify configuration settings as needed; step8: Train an agent using the provided training script; step9: Evaluate the trained agent using the evaluation script; step10: (Optional) Download datasets for offline RL pretraining; step11: (Optional) Pretrain an agent using offline RL; step12: (Optional) Fine-tune the pretrained policy on the real robot", "executed_cmds": "conda create --name bricksrl python=3.8;conda activate bricksrl;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118;pip install -e .", "cmd_prefix": "pip install", "cmd_postfix": "-e .[dev]", "target_cmd": "pip install -e .[dev]"}
{"uuid": "292d7256-618d-4f99-a929-adf0ee8d1a0c", "execution_plan": "step1: Enable Web Bluetooth on Chrome to allow communication with LEGO hubs; step2: Create and activate a Conda environment for the project; step3: Install PyTorch with CUDA support; step4: Install the bricksrl package and its dependencies; step5: (Optional) Install development tools for developers; step6: Update the client script on the Pybricks Hub for the desired environment; step7: Review and modify configuration settings as needed; step8: Train an agent using the provided training script; step9: Evaluate the trained agent using the evaluation script; step10: (Optional) Download datasets for offline RL pretraining; step11: (Optional) Pretrain an agent using offline RL; step12: (Optional) Fine-tune the pretrained policy on the real robot", "executed_cmds": "conda create --name bricksrl python=3.8;conda activate bricksrl;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118;pip install -e .;pip install -e .[dev];python experiments/walker/train.py;python experiments/walker/eval.py", "cmd_prefix": "git", "cmd_postfix": "lfs install", "target_cmd": "git lfs install"}
{"uuid": "6eb12550-0f6c-4610-a293-41e39a898f5f", "execution_plan": "step1: Clone the repository to your local machine; step2: Create and activate a new conda environment with Python 3.10; step3: Install the package in development mode; step4: Run the BeaverTails pipeline with default settings using the specified model and reduced data", "executed_cmds": "git clone git@github.com:lasgroup/SafetyPolytope.git", "cmd_prefix": "cd", "cmd_postfix": "SafetyPolytope", "target_cmd": "cd SafetyPolytope"}
{"uuid": "c8068aa1-6bd8-4c26-bac7-c6f118836295", "execution_plan": "step1: Clone the repository to your local machine; step2: Create and activate a new conda environment with Python 3.10; step3: Install the package in development mode; step4: Run the BeaverTails pipeline with default settings using the specified model and reduced data", "executed_cmds": "git clone git@github.com:lasgroup/SafetyPolytope.git;cd SafetyPolytope", "cmd_prefix": "conda create -n", "cmd_postfix": "sap python=3.10 -y", "target_cmd": "conda create -n sap python=3.10 -y"}
{"uuid": "707399ca-9d56-4c37-b423-611a143a2ca1", "execution_plan": "step1: Clone the repository to your local machine; step2: Create and activate a new conda environment with Python 3.10; step3: Install the package in development mode; step4: Run the BeaverTails pipeline with default settings using the specified model and reduced data", "executed_cmds": "git clone git@github.com:lasgroup/SafetyPolytope.git;cd SafetyPolytope;conda create -n sap python=3.10 -y", "cmd_prefix": "conda", "cmd_postfix": "activate sap", "target_cmd": "conda activate sap"}
{"uuid": "1c192fc2-919b-4085-98af-a4afe482aaeb", "execution_plan": "step1: Clone the repository to your local machine; step2: Create and activate a new conda environment with Python 3.10; step3: Install the package in development mode; step4: Run the BeaverTails pipeline with default settings using the specified model and reduced data", "executed_cmds": "git clone git@github.com:lasgroup/SafetyPolytope.git;cd SafetyPolytope;conda create -n sap python=3.10 -y;conda activate sap", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "f104e29b-cc23-4984-b2e4-69ddc9660b0b", "execution_plan": "step1: Create a conda environment named 'diffputer' with Python 3.12 and activate it; step2: Install the required dependencies for DiffPuter using the provided requirements file; step3: Prepare all datasets, splits, and masks by running the dataset preparation script; step4: Run DiffPuter on a single dataset under a specified mask by executing the main script with the appropriate arguments", "executed_cmds": "conda create -n diffputer python=3.12", "cmd_prefix": "conda", "cmd_postfix": "activate diffputer", "target_cmd": "conda activate diffputer"}
{"uuid": "32c273d8-56c9-4f31-b4a1-67ba94a7b9aa", "execution_plan": "step1: Install the Rust toolchain using rustup to compile the Peano environment; step2: Create and activate a Python virtual environment for the project; step3: Install maturin within the virtual environment to build the Peano library and executable; step4: Compile the Peano library and executable using maturin and cargo; step5: Test the Peano executable with an example proof to verify the compilation; step6: Install Python dependencies for the learning component of the project; step7: Run the conjecture-prove loop experiment using the bootstrap.py script with a specified theory; step8: Optional: Set up a distributed mode using Redis and Celery for multi-process execution", "executed_cmds": "python -m venv /path/to/new/virtual/environment;source /path/to/new/virtual/environment/bin/activate", "cmd_prefix": "pip", "cmd_postfix": "install maturin", "target_cmd": "pip install maturin"}
{"uuid": "b73ad00e-3b6f-44f3-ba3f-af89a97ec12f", "execution_plan": "step1: Install the Rust toolchain using rustup to compile the Peano environment; step2: Create and activate a Python virtual environment for the project; step3: Install maturin within the virtual environment to build the Peano library and executable; step4: Compile the Peano library and executable using maturin and cargo; step5: Test the Peano executable with an example proof to verify the compilation; step6: Install Python dependencies for the learning component of the project; step7: Run the conjecture-prove loop experiment using the bootstrap.py script with a specified theory; step8: Optional: Set up a distributed mode using Redis and Celery for multi-process execution", "executed_cmds": "python -m venv /path/to/new/virtual/environment;source /path/to/new/virtual/environment/bin/activate;pip install maturin", "cmd_prefix": "maturin", "cmd_postfix": "dev --release", "target_cmd": "maturin dev --release"}
{"uuid": "144e00d2-eb2d-4035-b4b7-c2ab4bd1de26", "execution_plan": "step1: Install the Rust toolchain using rustup to compile the Peano environment; step2: Create and activate a Python virtual environment for the project; step3: Install maturin within the virtual environment to build the Peano library and executable; step4: Compile the Peano library and executable using maturin and cargo; step5: Test the Peano executable with an example proof to verify the compilation; step6: Install Python dependencies for the learning component of the project; step7: Run the conjecture-prove loop experiment using the bootstrap.py script with a specified theory; step8: Optional: Set up a distributed mode using Redis and Celery for multi-process execution", "executed_cmds": "python -m venv /path/to/new/virtual/environment;source /path/to/new/virtual/environment/bin/activate;pip install maturin;maturin dev --release;cargo build --bin peano --release;target/release/peano theories/natural_number_game.p t_example1", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "71da3d23-361d-461d-91b4-cacaf81183ae", "execution_plan": "step1: Install the required dependencies for the project; step2: Prepare the model checkpoints by downloading LLaMA-2 or OPT models and placing them in the specified directory; step3: Create a directory to save LLM-generated answers, ground truth, and features; step4: Generate LLM answers for the specified dataset (e.g., TruthfulQA) using LLaMA-2 or OPT models; step5: Generate ground truth labels for the LLM-generated answers using Rouge or BleuRT; step6: Perform hallucination detection on the generated answers using the specified model and features; step7: (Optional) Cite the paper if the code is used in research; step8: (Note) Be mindful of random seed sensitivity for reproducibility", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "mkdir", "cmd_postfix": "models", "target_cmd": "mkdir models"}
{"uuid": "98a7996d-b6bf-447a-a084-d146ca45596d", "execution_plan": "step1: Install the required dependencies for the project; step2: Prepare the model checkpoints by downloading LLaMA-2 or OPT models and placing them in the specified directory; step3: Create a directory to save LLM-generated answers, ground truth, and features; step4: Generate LLM answers for the specified dataset (e.g., TruthfulQA) using LLaMA-2 or OPT models; step5: Generate ground truth labels for the LLM-generated answers using Rouge or BleuRT; step6: Perform hallucination detection on the generated answers using the specified model and features; step7: (Optional) Cite the paper if the code is used in research; step8: (Note) Be mindful of random seed sensitivity for reproducibility", "executed_cmds": "pip install -r requirements.txt;mkdir models", "cmd_prefix": "mkdir", "cmd_postfix": "save_for_eval", "target_cmd": "mkdir save_for_eval"}
{"uuid": "2be44ace-b84f-4b1c-bde8-53d5cd196259", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up the Python environment with the recommended versions (Python 3.10.16 and CUDA 11.8); step3: Install the required dependencies using pip; step4: Run inference using the provided script to generate co-speech gesture videos; step5: Optionally, start the Gradio app for a web interface similar to Hugging Face Space; step6: Optionally, train the JointEmbedding (CLIP) model using the provided training script; step7: Optionally, set up a separate Python environment (Python 3.9.20 and CUDA 11.8) for creating custom motion graphs; step8: Optionally, create a motion graph for a custom character", "executed_cmds": "git clone https://github.com/CyberAgentAILab/TANGO.git", "cmd_prefix": "cd", "cmd_postfix": "TANGO", "target_cmd": "cd TANGO"}
{"uuid": "10a3e4c6-1e11-47f1-8213-4fc3e94e24cb", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up the Python environment with the recommended versions (Python 3.10.16 and CUDA 11.8); step3: Install the required dependencies using pip; step4: Run inference using the provided script to generate co-speech gesture videos; step5: Optionally, start the Gradio app for a web interface similar to Hugging Face Space; step6: Optionally, train the JointEmbedding (CLIP) model using the provided training script; step7: Optionally, set up a separate Python environment (Python 3.9.20 and CUDA 11.8) for creating custom motion graphs; step8: Optionally, create a motion graph for a custom character", "executed_cmds": "git clone https://github.com/CyberAgentAILab/TANGO.git;cd TANGO;conda create -n tango_py310 python==3.10.16", "cmd_prefix": "conda", "cmd_postfix": "activate tango_py310", "target_cmd": "conda activate tango_py310"}
{"uuid": "047f07e0-316b-4f73-b7dc-950813adc7b3", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up the Python environment with the recommended versions (Python 3.10.16 and CUDA 11.8); step3: Install the required dependencies using pip; step4: Run inference using the provided script to generate co-speech gesture videos; step5: Optionally, start the Gradio app for a web interface similar to Hugging Face Space; step6: Optionally, train the JointEmbedding (CLIP) model using the provided training script; step7: Optionally, set up a separate Python environment (Python 3.9.20 and CUDA 11.8) for creating custom motion graphs; step8: Optionally, create a motion graph for a custom character", "executed_cmds": "git clone https://github.com/CyberAgentAILab/TANGO.git;cd TANGO;conda create -n tango_py310 python==3.10.16;conda activate tango_py310;python -m pip install -r ./pre-requirements.txt;python -m pip install -r ./requirements.txt;python inference.py --audio_path ./datasets/cached_audio/example_male_voice_9_seconds.wav --character_name ./datasets/cached_audio/speaker9_o7Ik1OB4TaE_00-00-38.15_00-00-42.33.mp4", "cmd_prefix": "python", "cmd_postfix": "app.py", "target_cmd": "python app.py"}
{"uuid": "168f2973-0dd5-495b-88bd-2110d8d4d756", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up the Python environment with the recommended versions (Python 3.10.16 and CUDA 11.8); step3: Install the required dependencies using pip; step4: Run inference using the provided script to generate co-speech gesture videos; step5: Optionally, start the Gradio app for a web interface similar to Hugging Face Space; step6: Optionally, train the JointEmbedding (CLIP) model using the provided training script; step7: Optionally, set up a separate Python environment (Python 3.9.20 and CUDA 11.8) for creating custom motion graphs; step8: Optionally, create a motion graph for a custom character", "executed_cmds": "git clone https://github.com/CyberAgentAILab/TANGO.git;cd TANGO;conda create -n tango_py310 python==3.10.16;conda activate tango_py310;python -m pip install -r ./pre-requirements.txt;python -m pip install -r ./requirements.txt;python inference.py --audio_path ./datasets/cached_audio/example_male_voice_9_seconds.wav --character_name ./datasets/cached_audio/speaker9_o7Ik1OB4TaE_00-00-38.15_00-00-42.33.mp4;python app.py;torchrun --nproc_per_node=1 train_high_env0.py --config ./configs/baseline_high_env0.yaml;conda create -n tango_py39 python==3.9.20", "cmd_prefix": "conda", "cmd_postfix": "activate tango_py39", "target_cmd": "conda activate tango_py39"}
{"uuid": "220d053c-480d-4e89-93fe-eb587aa88744", "execution_plan": "step1: Clone the repository to your local machine; step2: Set up the Python environment with the recommended versions (Python 3.10.16 and CUDA 11.8); step3: Install the required dependencies using pip; step4: Run inference using the provided script to generate co-speech gesture videos; step5: Optionally, start the Gradio app for a web interface similar to Hugging Face Space; step6: Optionally, train the JointEmbedding (CLIP) model using the provided training script; step7: Optionally, set up a separate Python environment (Python 3.9.20 and CUDA 11.8) for creating custom motion graphs; step8: Optionally, create a motion graph for a custom character", "executed_cmds": "git clone https://github.com/CyberAgentAILab/TANGO.git;cd TANGO;conda create -n tango_py310 python==3.10.16;conda activate tango_py310;python -m pip install -r ./pre-requirements.txt;python -m pip install -r ./requirements.txt;python inference.py --audio_path ./datasets/cached_audio/example_male_voice_9_seconds.wav --character_name ./datasets/cached_audio/speaker9_o7Ik1OB4TaE_00-00-38.15_00-00-42.33.mp4;python app.py;torchrun --nproc_per_node=1 train_high_env0.py --config ./configs/baseline_high_env0.yaml;conda create -n tango_py39 python==3.9.20;conda activate tango_py39;python -m pip install -r ./pre-requirements_py39.txt;python -m pip install -r ./requirements_py39.txt", "cmd_prefix": "python", "cmd_postfix": "create_graph.py", "target_cmd": "python create_graph.py"}
{"uuid": "fa382603-4ed4-4b0d-a717-9b94d11907b2", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate the conda environment using the provided environment.yml file; step3: Download the dataset using the provided script; step4: Transform the benchmark questions into embeddings; step5: Train and evaluate the KNN model for correctness forecasting; step6: Train and evaluate the Matrix Factorization model", "executed_cmds": "git clone https://github.com/yourusername/EmbedLLM.git", "cmd_prefix": "cd", "cmd_postfix": "EmbedLLM", "target_cmd": "cd EmbedLLM"}
{"uuid": "ee2b8c24-bac7-4b85-8950-3561fa54bc06", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate the conda environment using the provided environment.yml file; step3: Download the dataset using the provided script; step4: Transform the benchmark questions into embeddings; step5: Train and evaluate the KNN model for correctness forecasting; step6: Train and evaluate the Matrix Factorization model", "executed_cmds": "git clone https://github.com/yourusername/EmbedLLM.git;cd EmbedLLM;conda env create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate embedllm", "target_cmd": "conda activate embedllm"}
{"uuid": "b466b8b0-9b82-46cc-aa96-51df46309823", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate the conda environment using the provided environment.yml file; step3: Download the dataset using the provided script; step4: Transform the benchmark questions into embeddings; step5: Train and evaluate the KNN model for correctness forecasting; step6: Train and evaluate the Matrix Factorization model", "executed_cmds": "git clone https://github.com/yourusername/EmbedLLM.git;cd EmbedLLM;conda env create -f environment.yml;conda activate embedllm", "cmd_prefix": "cd", "cmd_postfix": "data_preprocessing", "target_cmd": "cd data_preprocessing"}
{"uuid": "a9c5fa88-2dc8-4fb8-9307-d059f4aecd67", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate the conda environment using the provided environment.yml file; step3: Download the dataset using the provided script; step4: Transform the benchmark questions into embeddings; step5: Train and evaluate the KNN model for correctness forecasting; step6: Train and evaluate the Matrix Factorization model", "executed_cmds": "git clone https://github.com/yourusername/EmbedLLM.git;cd EmbedLLM;conda env create -f environment.yml;conda activate embedllm;cd data_preprocessing", "cmd_prefix": "python", "cmd_postfix": "download_data.py", "target_cmd": "python download_data.py"}
{"uuid": "55d9992a-1e4d-4554-b1ee-2ac5cc96116b", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate the conda environment using the provided environment.yml file; step3: Download the dataset using the provided script; step4: Transform the benchmark questions into embeddings; step5: Train and evaluate the KNN model for correctness forecasting; step6: Train and evaluate the Matrix Factorization model", "executed_cmds": "git clone https://github.com/yourusername/EmbedLLM.git;cd EmbedLLM;conda env create -f environment.yml;conda activate embedllm;cd data_preprocessing;python download_data.py", "cmd_prefix": "cd", "cmd_postfix": "data_preprocessing", "target_cmd": "cd data_preprocessing"}
{"uuid": "f0c1dece-3598-4e8f-a4f9-f586d1276b0a", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate the conda environment using the provided environment.yml file; step3: Download the dataset using the provided script; step4: Transform the benchmark questions into embeddings; step5: Train and evaluate the KNN model for correctness forecasting; step6: Train and evaluate the Matrix Factorization model", "executed_cmds": "git clone https://github.com/yourusername/EmbedLLM.git;cd EmbedLLM;conda env create -f environment.yml;conda activate embedllm;cd data_preprocessing;python download_data.py;cd data_preprocessing;python get_question_embedding_tensor.py", "cmd_prefix": "cd", "cmd_postfix": "algorithm", "target_cmd": "cd algorithm"}
{"uuid": "c1d9e487-84d8-4517-a776-4cb6f9538f74", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate the conda environment using the provided environment.yml file; step3: Download the dataset using the provided script; step4: Transform the benchmark questions into embeddings; step5: Train and evaluate the KNN model for correctness forecasting; step6: Train and evaluate the Matrix Factorization model", "executed_cmds": "git clone https://github.com/yourusername/EmbedLLM.git;cd EmbedLLM;conda env create -f environment.yml;conda activate embedllm;cd data_preprocessing;python download_data.py;cd data_preprocessing;python get_question_embedding_tensor.py;cd algorithm", "cmd_prefix": "python", "cmd_postfix": "knn.py", "target_cmd": "python knn.py"}
{"uuid": "8ed90f90-49ca-4172-bddf-10d2885a1161", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate the conda environment using the provided environment.yml file; step3: Download the dataset using the provided script; step4: Transform the benchmark questions into embeddings; step5: Train and evaluate the KNN model for correctness forecasting; step6: Train and evaluate the Matrix Factorization model", "executed_cmds": "git clone https://github.com/yourusername/EmbedLLM.git;cd EmbedLLM;conda env create -f environment.yml;conda activate embedllm;cd data_preprocessing;python download_data.py;cd data_preprocessing;python get_question_embedding_tensor.py;cd algorithm;python knn.py", "cmd_prefix": "cd", "cmd_postfix": "algorithm", "target_cmd": "cd algorithm"}
{"uuid": "aee4a5f6-4e57-4b01-93d4-72169821d08f", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate the conda environment using the provided environment.yml file; step3: Download the dataset using the provided script; step4: Transform the benchmark questions into embeddings; step5: Train and evaluate the KNN model for correctness forecasting; step6: Train and evaluate the Matrix Factorization model", "executed_cmds": "git clone https://github.com/yourusername/EmbedLLM.git;cd EmbedLLM;conda env create -f environment.yml;conda activate embedllm;cd data_preprocessing;python download_data.py;cd data_preprocessing;python get_question_embedding_tensor.py;cd algorithm;python knn.py;cd algorithm", "cmd_prefix": "python", "cmd_postfix": "mf.py", "target_cmd": "python mf.py"}
{"uuid": "1fafd07b-9336-486a-b853-74916884edce", "execution_plan": "step1: Understand the project's purpose and scope, which involves exploring the loss landscape of regularized neural networks via convex duality; step2: Check for any prerequisites or dependencies required to run the project, such as Python, specific libraries, or datasets; step3: Clone or download the repository to your local machine; step4: Install the necessary dependencies or libraries listed in the project's requirements; step5: Explore the project structure to identify the main scripts or notebooks for execution; step6: Run the provided scripts or notebooks to reproduce the experiments or results; step7: Analyze the output or results generated by the scripts", "executed_cmds": "git clone [repository_url]", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "0a2fbbcc-3a2e-49cd-8000-673a0888cade", "execution_plan": "step1: Understand the project's purpose and scope, which involves exploring the loss landscape of regularized neural networks via convex duality; step2: Check for any prerequisites or dependencies required to run the project, such as Python, specific libraries, or datasets; step3: Clone or download the repository to your local machine; step4: Install the necessary dependencies or libraries listed in the project's requirements; step5: Explore the project structure to identify the main scripts or notebooks for execution; step6: Run the provided scripts or notebooks to reproduce the experiments or results; step7: Analyze the output or results generated by the scripts", "executed_cmds": "git clone [repository_url];pip install -r requirements.txt", "cmd_prefix": "python main_script.py", "cmd_postfix": "or jupyter notebook", "target_cmd": "python main_script.py or jupyter notebook"}
{"uuid": "d09bcc2b-d472-4f86-ac41-e3ad63a506f9", "execution_plan": "step1: Set up the Python environment and install required packages; step2: Install pre-commit hooks for code formatting; step3: Download and preprocess the UEA datasets; step4: Download and preprocess the PPG-DaLiA dataset; step5: Configure the experiment by preparing the necessary configuration files; step6: Run the experiments using the provided script; step7: Reproduce results using the provided configuration files and outputs", "executed_cmds": "conda create -n LinOSS python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate LinOSS", "target_cmd": "conda activate LinOSS"}
{"uuid": "9d9fd104-1896-4764-8855-42f00a7cba9f", "execution_plan": "step1: Set up the Python environment and install required packages; step2: Install pre-commit hooks for code formatting; step3: Download and preprocess the UEA datasets; step4: Download and preprocess the PPG-DaLiA dataset; step5: Configure the experiment by preparing the necessary configuration files; step6: Run the experiments using the provided script; step7: Reproduce results using the provided configuration files and outputs", "executed_cmds": "conda create -n LinOSS python=3.10;conda activate LinOSS;conda install pre-commit=3.7.1 sktime=0.30.1 tqdm=4.66.4 matplotlib=3.8.4 -c conda-forge;pip install -U \"jax[cuda12]\" \"jaxlib[cuda12]\" equinox==0.11.4 optax==0.2.2 diffrax==0.5.1 signax==0.1.1", "cmd_prefix": "pre-commit", "cmd_postfix": "install", "target_cmd": "pre-commit install"}
{"uuid": "c3baad85-5024-4834-93c9-6cb4112226c7", "execution_plan": "step1: Set up the Python environment and install required packages; step2: Install pre-commit hooks for code formatting; step3: Download and preprocess the UEA datasets; step4: Download and preprocess the PPG-DaLiA dataset; step5: Configure the experiment by preparing the necessary configuration files; step6: Run the experiments using the provided script; step7: Reproduce results using the provided configuration files and outputs", "executed_cmds": "conda create -n LinOSS python=3.10;conda activate LinOSS;conda install pre-commit=3.7.1 sktime=0.30.1 tqdm=4.66.4 matplotlib=3.8.4 -c conda-forge;pip install -U \"jax[cuda12]\" \"jaxlib[cuda12]\" equinox==0.11.4 optax==0.2.2 diffrax==0.5.1 signax==0.1.1;pre-commit install;python data_dir/download_uea.py;python data_dir/process_uea.py", "cmd_prefix": "python", "cmd_postfix": "run_experiment.py", "target_cmd": "python run_experiment.py"}
{"uuid": "021878b4-6098-451e-9ea7-763d2e60518e", "execution_plan": "step1: Set up the Python environment and install required dependencies; step2: Set up Weights & Biases (wandb) for experiment tracking and logging; step3: Run the main script with default parameters; step4: Run the vanilla Hadwiger-Nelson problem with seven colors in 2D; step5: Run the almost coloring experiment with a lagrangian multiplier for six colors; step6: Run the polychromatic number experiment for a range of distances; step7: Run the Hadwiger-Nelson problem in three dimensions; step8: Explore or modify the provided almost-colorings in the constructions folder", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "wandb", "cmd_postfix": "login", "target_cmd": "wandb login"}
{"uuid": "a9bab1ad-8629-43a1-8003-173cf93758ec", "execution_plan": "step1: Set up the Python environment and install required dependencies; step2: Set up Weights & Biases (wandb) for experiment tracking and logging; step3: Run the main script with default parameters; step4: Run the vanilla Hadwiger-Nelson problem with seven colors in 2D; step5: Run the almost coloring experiment with a lagrangian multiplier for six colors; step6: Run the polychromatic number experiment for a range of distances; step7: Run the Hadwiger-Nelson problem in three dimensions; step8: Explore or modify the provided almost-colorings in the constructions folder", "executed_cmds": "pip install -r requirements.txt;wandb login", "cmd_prefix": "python", "cmd_postfix": "main.py", "target_cmd": "python main.py"}
{"uuid": "49bdf285-e1cf-49e9-8f7a-c6be45b8c307", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required dependencies; step2: Download the pre-training weights for VGCM from Google Drive; step3: Extract pretraining features using ResNet200; step4: Train and validate the VGCM (Video Granger Causality Model) using the provided script; step5: Fine-tune and evaluate Video-LLaVA and VideoChat2 using LoRA; step6: Evaluate the few-shot (In-Context Learning) performance of Video-LLaVA, VideoChat2, GPT-4, and Gemini-pro; step7: Enhance Video Question Answering using causal relations; step8: Download the MECD dataset videos from ActivityNet or use the provided 8-frame sampled results; step9: Download the MECD+ dataset videos from Eventbench if needed", "executed_cmds": "conda create -n mecd python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate mecd", "target_cmd": "conda activate mecd"}
{"uuid": "3d1674c1-836e-45d7-a4e5-84321261dc1c", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required dependencies; step2: Download the pre-training weights for VGCM from Google Drive; step3: Extract pretraining features using ResNet200; step4: Train and validate the VGCM (Video Granger Causality Model) using the provided script; step5: Fine-tune and evaluate Video-LLaVA and VideoChat2 using LoRA; step6: Evaluate the few-shot (In-Context Learning) performance of Video-LLaVA, VideoChat2, GPT-4, and Gemini-pro; step7: Enhance Video Question Answering using causal relations; step8: Download the MECD dataset videos from ActivityNet or use the provided 8-frame sampled results; step9: Download the MECD+ dataset videos from Eventbench if needed", "executed_cmds": "conda create -n mecd python=3.10;conda activate mecd", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "641eb8df-4c84-474e-b1f7-f5428334f0a4", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required dependencies; step2: Download the pre-training weights for VGCM from Google Drive; step3: Extract pretraining features using ResNet200; step4: Train and validate the VGCM (Video Granger Causality Model) using the provided script; step5: Fine-tune and evaluate Video-LLaVA and VideoChat2 using LoRA; step6: Evaluate the few-shot (In-Context Learning) performance of Video-LLaVA, VideoChat2, GPT-4, and Gemini-pro; step7: Enhance Video Question Answering using causal relations; step8: Download the MECD dataset videos from ActivityNet or use the provided 8-frame sampled results; step9: Download the MECD+ dataset videos from Eventbench if needed", "executed_cmds": "conda create -n mecd python=3.10;conda activate mecd;pip install -r requirements.txt;python feature_kit/extract_feature.py", "cmd_prefix": "sh", "cmd_postfix": "scripts/train.sh", "target_cmd": "sh scripts/train.sh"}
{"uuid": "f52eaa8d-6ced-4830-8e97-2e9f47ec7ade", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required dependencies; step2: Download the pre-training weights for VGCM from Google Drive; step3: Extract pretraining features using ResNet200; step4: Train and validate the VGCM (Video Granger Causality Model) using the provided script; step5: Fine-tune and evaluate Video-LLaVA and VideoChat2 using LoRA; step6: Evaluate the few-shot (In-Context Learning) performance of Video-LLaVA, VideoChat2, GPT-4, and Gemini-pro; step7: Enhance Video Question Answering using causal relations; step8: Download the MECD dataset videos from ActivityNet or use the provided 8-frame sampled results; step9: Download the MECD+ dataset videos from Eventbench if needed", "executed_cmds": "conda create -n mecd python=3.10;conda activate mecd;pip install -r requirements.txt;python feature_kit/extract_feature.py;sh scripts/train.sh;cd mecd_vllm_finetune/Video-LLaVA-ft;sh scripts/v1_5/finetune_lora.sh;python videollava/eval/video/run_inference_causal_inference.py;cd mecd_vllm_finetune/VideoChat2-ft;OMP_NUM_THREADS=2 torchrun --nnodes=1 --nproc_per_node=8 tasks/train_it.py ./scripts/videochat_mistral/config_7b_stage3.py", "cmd_prefix": "python", "cmd_postfix": "multi_event.py", "target_cmd": "python multi_event.py"}
{"uuid": "251cc716-5274-41d1-8a50-afdbd1b69662", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required dependencies; step2: Download the pre-training weights for VGCM from Google Drive; step3: Extract pretraining features using ResNet200; step4: Train and validate the VGCM (Video Granger Causality Model) using the provided script; step5: Fine-tune and evaluate Video-LLaVA and VideoChat2 using LoRA; step6: Evaluate the few-shot (In-Context Learning) performance of Video-LLaVA, VideoChat2, GPT-4, and Gemini-pro; step7: Enhance Video Question Answering using causal relations; step8: Download the MECD dataset videos from ActivityNet or use the provided 8-frame sampled results; step9: Download the MECD+ dataset videos from Eventbench if needed", "executed_cmds": "conda create -n mecd python=3.10;conda activate mecd;pip install -r requirements.txt;python feature_kit/extract_feature.py;sh scripts/train.sh;cd mecd_vllm_finetune/Video-LLaVA-ft;sh scripts/v1_5/finetune_lora.sh;python videollava/eval/video/run_inference_causal_inference.py;cd mecd_vllm_finetune/VideoChat2-ft;OMP_NUM_THREADS=2 torchrun --nnodes=1 --nproc_per_node=8 tasks/train_it.py ./scripts/videochat_mistral/config_7b_stage3.py;python multi_event.py;cd mecd_vllm_fewshot/Video-LLaVA;python videollava/eval/video/run_inference_causal_inference.py;cd mecd_vllm_fewshot/VideoChat2", "cmd_prefix": "python", "cmd_postfix": "multi_event.py", "target_cmd": "python multi_event.py"}
{"uuid": "57e1c7d6-b37e-43bd-99a8-1e31d75aa124", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required dependencies; step2: Download the pre-training weights for VGCM from Google Drive; step3: Extract pretraining features using ResNet200; step4: Train and validate the VGCM (Video Granger Causality Model) using the provided script; step5: Fine-tune and evaluate Video-LLaVA and VideoChat2 using LoRA; step6: Evaluate the few-shot (In-Context Learning) performance of Video-LLaVA, VideoChat2, GPT-4, and Gemini-pro; step7: Enhance Video Question Answering using causal relations; step8: Download the MECD dataset videos from ActivityNet or use the provided 8-frame sampled results; step9: Download the MECD+ dataset videos from Eventbench if needed", "executed_cmds": "conda create -n mecd python=3.10;conda activate mecd;pip install -r requirements.txt;python feature_kit/extract_feature.py;sh scripts/train.sh;cd mecd_vllm_finetune/Video-LLaVA-ft;sh scripts/v1_5/finetune_lora.sh;python videollava/eval/video/run_inference_causal_inference.py;cd mecd_vllm_finetune/VideoChat2-ft;OMP_NUM_THREADS=2 torchrun --nnodes=1 --nproc_per_node=8 tasks/train_it.py ./scripts/videochat_mistral/config_7b_stage3.py;python multi_event.py;cd mecd_vllm_fewshot/Video-LLaVA;python videollava/eval/video/run_inference_causal_inference.py;cd mecd_vllm_fewshot/VideoChat2;python multi_event.py", "cmd_prefix": "cd", "cmd_postfix": "mecd_llm_fewshot", "target_cmd": "cd mecd_llm_fewshot"}
{"uuid": "e7a084ec-8a3a-40c4-9255-50bc201f874c", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required dependencies; step2: Download the pre-training weights for VGCM from Google Drive; step3: Extract pretraining features using ResNet200; step4: Train and validate the VGCM (Video Granger Causality Model) using the provided script; step5: Fine-tune and evaluate Video-LLaVA and VideoChat2 using LoRA; step6: Evaluate the few-shot (In-Context Learning) performance of Video-LLaVA, VideoChat2, GPT-4, and Gemini-pro; step7: Enhance Video Question Answering using causal relations; step8: Download the MECD dataset videos from ActivityNet or use the provided 8-frame sampled results; step9: Download the MECD+ dataset videos from Eventbench if needed", "executed_cmds": "conda create -n mecd python=3.10;conda activate mecd;pip install -r requirements.txt;python feature_kit/extract_feature.py;sh scripts/train.sh;cd mecd_vllm_finetune/Video-LLaVA-ft;sh scripts/v1_5/finetune_lora.sh;python videollava/eval/video/run_inference_causal_inference.py;cd mecd_vllm_finetune/VideoChat2-ft;OMP_NUM_THREADS=2 torchrun --nnodes=1 --nproc_per_node=8 tasks/train_it.py ./scripts/videochat_mistral/config_7b_stage3.py;python multi_event.py;cd mecd_vllm_fewshot/Video-LLaVA;python videollava/eval/video/run_inference_causal_inference.py;cd mecd_vllm_fewshot/VideoChat2;python multi_event.py;cd mecd_llm_fewshot", "cmd_prefix": "python", "cmd_postfix": "gpt4.py", "target_cmd": "python gpt4.py"}
{"uuid": "3fa68ffc-76cf-449a-bb3c-73a9bb334c2d", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required dependencies; step2: Download the pre-training weights for VGCM from Google Drive; step3: Extract pretraining features using ResNet200; step4: Train and validate the VGCM (Video Granger Causality Model) using the provided script; step5: Fine-tune and evaluate Video-LLaVA and VideoChat2 using LoRA; step6: Evaluate the few-shot (In-Context Learning) performance of Video-LLaVA, VideoChat2, GPT-4, and Gemini-pro; step7: Enhance Video Question Answering using causal relations; step8: Download the MECD dataset videos from ActivityNet or use the provided 8-frame sampled results; step9: Download the MECD+ dataset videos from Eventbench if needed", "executed_cmds": "conda create -n mecd python=3.10;conda activate mecd;pip install -r requirements.txt;python feature_kit/extract_feature.py;sh scripts/train.sh;cd mecd_vllm_finetune/Video-LLaVA-ft;sh scripts/v1_5/finetune_lora.sh;python videollava/eval/video/run_inference_causal_inference.py;cd mecd_vllm_finetune/VideoChat2-ft;OMP_NUM_THREADS=2 torchrun --nnodes=1 --nproc_per_node=8 tasks/train_it.py ./scripts/videochat_mistral/config_7b_stage3.py;python multi_event.py;cd mecd_vllm_fewshot/Video-LLaVA;python videollava/eval/video/run_inference_causal_inference.py;cd mecd_vllm_fewshot/VideoChat2;python multi_event.py;cd mecd_llm_fewshot;python gpt4.py", "cmd_prefix": "python", "cmd_postfix": "gemini.py", "target_cmd": "python gemini.py"}
{"uuid": "5139aecf-481c-4429-96f2-a2c364801251", "execution_plan": "step1: Create and activate a Conda environment with Python 3.12.3 to ensure compatibility; step2: Install the CP-Fuse package either locally by cloning the repository or directly from GitHub; step3: Train the model using the provided training script with specified parameters; step4: Evaluate the trained model using the evaluation script with specified parameters", "executed_cmds": "conda create -n cp_fuse python=3.12.3", "cmd_prefix": "conda", "cmd_postfix": "activate cp_fuse", "target_cmd": "conda activate cp_fuse"}
{"uuid": "f2548d69-77e0-48f1-a95c-cca4ac1e3c43", "execution_plan": "step1: Create and activate a Conda environment with Python 3.12.3 to ensure compatibility; step2: Install the CP-Fuse package either locally by cloning the repository or directly from GitHub; step3: Train the model using the provided training script with specified parameters; step4: Evaluate the trained model using the evaluation script with specified parameters", "executed_cmds": "conda create -n cp_fuse python=3.12.3;conda activate cp_fuse;git clone https://github.com/jaabmar/cp_fuse.git", "cmd_prefix": "cd", "cmd_postfix": "cp_fuse", "target_cmd": "cd cp_fuse"}
{"uuid": "530444f3-32c5-4cd0-81ff-4bd9faa82b33", "execution_plan": "step1: Create and activate a Conda environment with Python 3.12.3 to ensure compatibility; step2: Install the CP-Fuse package either locally by cloning the repository or directly from GitHub; step3: Train the model using the provided training script with specified parameters; step4: Evaluate the trained model using the evaluation script with specified parameters", "executed_cmds": "conda create -n cp_fuse python=3.12.3;conda activate cp_fuse;git clone https://github.com/jaabmar/cp_fuse.git;cd cp_fuse", "cmd_prefix": "pip install", "cmd_postfix": "--upgrade pip", "target_cmd": "pip install --upgrade pip"}
{"uuid": "6f2831ef-3516-4f69-9dff-ebe57f892e1f", "execution_plan": "step1: Create a Conda environment for the project; step2: Install PyTorch and related dependencies; step3: Install other required dependencies; step4: Install dependencies for GOGGLE; step5: Create a separate Conda environment for synthcity and install its dependencies; step6: Download and process the dataset; step7: Train the VAE model; step8: Train the diffusion model; step9: Watermark the data during sampling; step10: Detect the watermark in the data", "executed_cmds": "conda create -n tabsyn python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate tabsyn", "target_cmd": "conda activate tabsyn"}
{"uuid": "c9828b6d-dc9d-4f89-8387-82c521f318b7", "execution_plan": "step1: Create a Conda environment for the project; step2: Install PyTorch and related dependencies; step3: Install other required dependencies; step4: Install dependencies for GOGGLE; step5: Create a separate Conda environment for synthcity and install its dependencies; step6: Download and process the dataset; step7: Train the VAE model; step8: Train the diffusion model; step9: Watermark the data during sampling; step10: Detect the watermark in the data", "executed_cmds": "conda create -n tabsyn python=3.10;conda activate tabsyn;pip install torch torchvision torchaudio or conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "2c4accf8-828b-4da3-bcd1-e5b7a9ada524", "execution_plan": "step1: Create a Conda environment for the project; step2: Install PyTorch and related dependencies; step3: Install other required dependencies; step4: Install dependencies for GOGGLE; step5: Create a separate Conda environment for synthcity and install its dependencies; step6: Download and process the dataset; step7: Train the VAE model; step8: Train the diffusion model; step9: Watermark the data during sampling; step10: Detect the watermark in the data", "executed_cmds": "conda create -n tabsyn python=3.10;conda activate tabsyn;pip install torch torchvision torchaudio or conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install -r requirements.txt;pip install dgl -f https://data.dgl.ai/wheels/cu117/repo.html;pip install torch_geometric;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cu117.html;conda create -n synthcity python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate synthcity", "target_cmd": "conda activate synthcity"}
{"uuid": "b21a0763-df93-475a-a7dd-a83336ff2a86", "execution_plan": "step1: Create a Conda environment for the project; step2: Install PyTorch and related dependencies; step3: Install other required dependencies; step4: Install dependencies for GOGGLE; step5: Create a separate Conda environment for synthcity and install its dependencies; step6: Download and process the dataset; step7: Train the VAE model; step8: Train the diffusion model; step9: Watermark the data during sampling; step10: Detect the watermark in the data", "executed_cmds": "conda create -n tabsyn python=3.10;conda activate tabsyn;pip install torch torchvision torchaudio or conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install -r requirements.txt;pip install dgl -f https://data.dgl.ai/wheels/cu117/repo.html;pip install torch_geometric;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cu117.html;conda create -n synthcity python=3.10;conda activate synthcity", "cmd_prefix": "pip", "cmd_postfix": "install synthcity", "target_cmd": "pip install synthcity"}
{"uuid": "cb3bc126-9042-4ddf-a90a-d5bf328e09f1", "execution_plan": "step1: Create a Conda environment for the project; step2: Install PyTorch and related dependencies; step3: Install other required dependencies; step4: Install dependencies for GOGGLE; step5: Create a separate Conda environment for synthcity and install its dependencies; step6: Download and process the dataset; step7: Train the VAE model; step8: Train the diffusion model; step9: Watermark the data during sampling; step10: Detect the watermark in the data", "executed_cmds": "conda create -n tabsyn python=3.10;conda activate tabsyn;pip install torch torchvision torchaudio or conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install -r requirements.txt;pip install dgl -f https://data.dgl.ai/wheels/cu117/repo.html;pip install torch_geometric;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cu117.html;conda create -n synthcity python=3.10;conda activate synthcity;pip install synthcity;pip install category_encoders", "cmd_prefix": "python", "cmd_postfix": "download_dataset.py", "target_cmd": "python download_dataset.py"}
{"uuid": "cc00eec0-9ac9-4a73-abff-623f05750237", "execution_plan": "step1: Create a Conda environment for the project; step2: Install PyTorch and related dependencies; step3: Install other required dependencies; step4: Install dependencies for GOGGLE; step5: Create a separate Conda environment for synthcity and install its dependencies; step6: Download and process the dataset; step7: Train the VAE model; step8: Train the diffusion model; step9: Watermark the data during sampling; step10: Detect the watermark in the data", "executed_cmds": "conda create -n tabsyn python=3.10;conda activate tabsyn;pip install torch torchvision torchaudio or conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia;pip install -r requirements.txt;pip install dgl -f https://data.dgl.ai/wheels/cu117/repo.html;pip install torch_geometric;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cu117.html;conda create -n synthcity python=3.10;conda activate synthcity;pip install synthcity;pip install category_encoders;python download_dataset.py", "cmd_prefix": "python", "cmd_postfix": "process_dataset.py", "target_cmd": "python process_dataset.py"}
{"uuid": "56bd5d31-1ff2-4afc-a63f-8f3c66581134", "execution_plan": "step1: Install Micromamba to create the environment; step2: Clone the repository and navigate to the project directory; step3: Create a new Micromamba environment with Python 3.10; step4: Activate the Micromamba environment; step5: Install the required dependencies using pip; step6: Install the project in editable mode; step7: Run the main script to evaluate the performance of Sparse Expansion models with specified parameters", "executed_cmds": "git clone https://github.com/Shavit-Lab/Sparse-Expansion.git", "cmd_prefix": "cd", "cmd_postfix": "Sparse-Expansion", "target_cmd": "cd Sparse-Expansion"}
{"uuid": "85656a14-48b3-4c0d-910c-9b1aa690a4fe", "execution_plan": "step1: Install Micromamba to create the environment; step2: Clone the repository and navigate to the project directory; step3: Create a new Micromamba environment with Python 3.10; step4: Activate the Micromamba environment; step5: Install the required dependencies using pip; step6: Install the project in editable mode; step7: Run the main script to evaluate the performance of Sparse Expansion models with specified parameters", "executed_cmds": "git clone https://github.com/Shavit-Lab/Sparse-Expansion.git;cd Sparse-Expansion;micromamba create -n sparse-expansion python=3.10;micromamba activate sparse-expansion", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "46b599d3-9092-465e-bcc9-343734146254", "execution_plan": "step1: Install Micromamba to create the environment; step2: Clone the repository and navigate to the project directory; step3: Create a new Micromamba environment with Python 3.10; step4: Activate the Micromamba environment; step5: Install the required dependencies using pip; step6: Install the project in editable mode; step7: Run the main script to evaluate the performance of Sparse Expansion models with specified parameters", "executed_cmds": "git clone https://github.com/Shavit-Lab/Sparse-Expansion.git;cd Sparse-Expansion;micromamba create -n sparse-expansion python=3.10;micromamba activate sparse-expansion;pip install -r requirements.txt", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "38810f15-49ac-46b4-9ecb-5579e711be08", "execution_plan": "step1: Install the required packages for the project; step2: Navigate to the llama2 directory; step3: Download the LLaMA-2-7B-Chat-fp16 model and place it under the ckpts/ directory; step4: Run a demo using the method on the Dolly dataset; step5: Compute Self-Inf-N scores and select top benign samples for fine-tuning; step6: Run baseline comparisons (Random Selection and COLM 2024); step7: Evaluate how input length affects model safety; step8: Try benign sample fine-tuning in continual learning setups", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "cd", "cmd_postfix": "llama2", "target_cmd": "cd llama2"}
{"uuid": "656d97ca-731a-4952-85ad-fd8139ec94e7", "execution_plan": "step1: Install the required Python packages (numpy, scipy, statsmodels, datetime, matplotlib, ray) to ensure the environment is properly set up; step2: Verify the system meets the hardware requirements (high-performance server with sufficient CPU cores and RAM) for running the experiments; step3: Navigate to the repository directory to access the necessary files (functions.py, log_convergence.py, log_clt.py); step4: Run the convergence experiments script (log_convergence.py) to analyze the convergence behavior; step5: Run the CLT analysis script (log_clt.py) to generate histogram and QQ plots for CLT analysis; step6: Check the plots/ folder for generated plot outputs after running the experiments", "executed_cmds": "pip install numpy scipy statsmodels datetime matplotlib ray", "cmd_prefix": "python", "cmd_postfix": "log_convergence.py", "target_cmd": "python log_convergence.py"}
{"uuid": "83b7efdc-5bc7-428e-a6ee-6d01ad50adf7", "execution_plan": "step1: Install the required Python packages (numpy, scipy, statsmodels, datetime, matplotlib, ray) to ensure the environment is properly set up; step2: Verify the system meets the hardware requirements (high-performance server with sufficient CPU cores and RAM) for running the experiments; step3: Navigate to the repository directory to access the necessary files (functions.py, log_convergence.py, log_clt.py); step4: Run the convergence experiments script (log_convergence.py) to analyze the convergence behavior; step5: Run the CLT analysis script (log_clt.py) to generate histogram and QQ plots for CLT analysis; step6: Check the plots/ folder for generated plot outputs after running the experiments", "executed_cmds": "pip install numpy scipy statsmodels datetime matplotlib ray;python log_convergence.py", "cmd_prefix": "python", "cmd_postfix": "log_clt.py", "target_cmd": "python log_clt.py"}
{"uuid": "653190fb-4492-4ee4-b327-aeabd7e1bd3a", "execution_plan": "step1: Install the necessary dependencies for the project; step2: Set up the environment by creating a .env file to store API keys; step3: Run the MoA pipeline to generate outputs via the multi-agent framework; step4: Evaluate the generated outputs using AlpacaEval to compare MoA results with base model results", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "moa-advanced.py", "target_cmd": "python moa-advanced.py"}
{"uuid": "e1d1c4ec-8bc8-4f68-b9c2-e89045fa9f80", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the vision representation dataset and embeddings; step3: Train Contrastive Sparse Representation (CSR) for vision tasks; step4: Generate CSR embeddings for vision evaluation; step5: Evaluate vision representation using FAISS; step6: Prepare the text representation dataset and embeddings; step7: Train CSR for text classification, clustering, and retrieval tasks; step8: Generate CSR embeddings for text evaluation; step9: Evaluate text representation for classification, clustering, and retrieval; step10: Train CSR for multimodal representation tasks; step11: Evaluate multimodal representation on MSCOCO and Flickr30K", "executed_cmds": "conda create --name csr python=3.8.20", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "d0f6c384-b99f-4717-b12f-68d5b2192aba", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the vision representation dataset and embeddings; step3: Train Contrastive Sparse Representation (CSR) for vision tasks; step4: Generate CSR embeddings for vision evaluation; step5: Evaluate vision representation using FAISS; step6: Prepare the text representation dataset and embeddings; step7: Train CSR for text classification, clustering, and retrieval tasks; step8: Generate CSR embeddings for text evaluation; step9: Evaluate text representation for classification, clustering, and retrieval; step10: Train CSR for multimodal representation tasks; step11: Evaluate multimodal representation on MSCOCO and Flickr30K", "executed_cmds": "conda create --name csr python=3.8.20;pip install -r requirements.txt;python ./dataset_preparation annotations.py --xml_dir \"/path/to/train/annotation/directory\" --output_file \"/path/to/annotation.txt/directory\";python ./dataset_preparation to_pytorch_style.py --split_path \"/path/to/pytorch/style/dataset\"", "cmd_prefix": "./write_imagenet.sh \"train\"", "cmd_postfix": "500 0.50 90", "target_cmd": "./write_imagenet.sh \"train\" 500 0.50 90"}
{"uuid": "ac500e09-3f80-43ff-9082-0a8b0505606d", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the vision representation dataset and embeddings; step3: Train Contrastive Sparse Representation (CSR) for vision tasks; step4: Generate CSR embeddings for vision evaluation; step5: Evaluate vision representation using FAISS; step6: Prepare the text representation dataset and embeddings; step7: Train CSR for text classification, clustering, and retrieval tasks; step8: Generate CSR embeddings for text evaluation; step9: Evaluate text representation for classification, clustering, and retrieval; step10: Train CSR for multimodal representation tasks; step11: Evaluate multimodal representation on MSCOCO and Flickr30K", "executed_cmds": "conda create --name csr python=3.8.20;pip install -r requirements.txt;python ./dataset_preparation annotations.py --xml_dir \"/path/to/train/annotation/directory\" --output_file \"/path/to/annotation.txt/directory\";python ./dataset_preparation to_pytorch_style.py --split_path \"/path/to/pytorch/style/dataset\";./write_imagenet.sh \"train\" 500 0.50 90", "cmd_prefix": "./write_imagenet.sh \"val\"", "cmd_postfix": "500 0.50 90", "target_cmd": "./write_imagenet.sh \"val\" 500 0.50 90"}
{"uuid": "99be3c1e-5526-474c-a1db-b5fc70cf26c0", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the vision representation dataset and embeddings; step3: Train Contrastive Sparse Representation (CSR) for vision tasks; step4: Generate CSR embeddings for vision evaluation; step5: Evaluate vision representation using FAISS; step6: Prepare the text representation dataset and embeddings; step7: Train CSR for text classification, clustering, and retrieval tasks; step8: Generate CSR embeddings for text evaluation; step9: Evaluate text representation for classification, clustering, and retrieval; step10: Train CSR for multimodal representation tasks; step11: Evaluate multimodal representation on MSCOCO and Flickr30K", "executed_cmds": "conda create --name csr python=3.8.20;pip install -r requirements.txt;python ./dataset_preparation annotations.py --xml_dir \"/path/to/train/annotation/directory\" --output_file \"/path/to/annotation.txt/directory\";python ./dataset_preparation to_pytorch_style.py --split_path \"/path/to/pytorch/style/dataset\";./write_imagenet.sh \"train\" 500 0.50 90;./write_imagenet.sh \"val\" 500 0.50 90;python pretrained_embeddings.py --train_data_ffcv /path/to/train.ffcv --eval_data_ffcv /path/to/val.ffcv --model_name \"pre-trained visual backbone\"", "cmd_prefix": "python", "cmd_postfix": "stack_emb.py", "target_cmd": "python stack_emb.py"}
{"uuid": "510b3ab9-639d-4606-8770-09aa199eb8a8", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the vision representation dataset and embeddings; step3: Train Contrastive Sparse Representation (CSR) for vision tasks; step4: Generate CSR embeddings for vision evaluation; step5: Evaluate vision representation using FAISS; step6: Prepare the text representation dataset and embeddings; step7: Train CSR for text classification, clustering, and retrieval tasks; step8: Generate CSR embeddings for text evaluation; step9: Evaluate text representation for classification, clustering, and retrieval; step10: Train CSR for multimodal representation tasks; step11: Evaluate multimodal representation on MSCOCO and Flickr30K", "executed_cmds": "conda create --name csr python=3.8.20;pip install -r requirements.txt;python ./dataset_preparation annotations.py --xml_dir \"/path/to/train/annotation/directory\" --output_file \"/path/to/annotation.txt/directory\";python ./dataset_preparation to_pytorch_style.py --split_path \"/path/to/pytorch/style/dataset\";./write_imagenet.sh \"train\" 500 0.50 90;./write_imagenet.sh \"val\" 500 0.50 90;python pretrained_embeddings.py --train_data_ffcv /path/to/train.ffcv --eval_data_ffcv /path/to/val.ffcv --model_name \"pre-trained visual backbone\";python stack_emb.py;python main_visual.py --pretrained_emb /path/to/pretrained_emb --model_name \"pre-trained visual backbone\" --use_ddp False --gpu 1 --batch-size 1024 --lr 4e-4 --use_CL True --topk 8 --auxk 512 --hidden-size 8192;python chunk_npz_file.py --input_path \"Path/to/original/embeddings\" --output_path \"Path/to/chunk/directory\" --chunk_size \"Number of samples per chunk\";python csr_inference.py --train_emb_path /path/to/train_emb --eval_emb_path /path/to/val_emb --model_name \"pre-trained visual backbone\" --topk 8 --hidden-size 8192 --csr_ckpt \"CSR ckpt path\"", "cmd_prefix": "python ./retrieval/faiss_nn.py", "cmd_postfix": "--topk 8", "target_cmd": "python ./retrieval/faiss_nn.py --topk 8"}
{"uuid": "4185e03c-f9fe-4183-89aa-6b4aaf1afb16", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the vision representation dataset and embeddings; step3: Train Contrastive Sparse Representation (CSR) for vision tasks; step4: Generate CSR embeddings for vision evaluation; step5: Evaluate vision representation using FAISS; step6: Prepare the text representation dataset and embeddings; step7: Train CSR for text classification, clustering, and retrieval tasks; step8: Generate CSR embeddings for text evaluation; step9: Evaluate text representation for classification, clustering, and retrieval; step10: Train CSR for multimodal representation tasks; step11: Evaluate multimodal representation on MSCOCO and Flickr30K", "executed_cmds": "conda create --name csr python=3.8.20;pip install -r requirements.txt;python ./dataset_preparation annotations.py --xml_dir \"/path/to/train/annotation/directory\" --output_file \"/path/to/annotation.txt/directory\";python ./dataset_preparation to_pytorch_style.py --split_path \"/path/to/pytorch/style/dataset\";./write_imagenet.sh \"train\" 500 0.50 90;./write_imagenet.sh \"val\" 500 0.50 90;python pretrained_embeddings.py --train_data_ffcv /path/to/train.ffcv --eval_data_ffcv /path/to/val.ffcv --model_name \"pre-trained visual backbone\";python stack_emb.py;python main_visual.py --pretrained_emb /path/to/pretrained_emb --model_name \"pre-trained visual backbone\" --use_ddp False --gpu 1 --batch-size 1024 --lr 4e-4 --use_CL True --topk 8 --auxk 512 --hidden-size 8192;python chunk_npz_file.py --input_path \"Path/to/original/embeddings\" --output_path \"Path/to/chunk/directory\" --chunk_size \"Number of samples per chunk\";python csr_inference.py --train_emb_path /path/to/train_emb --eval_emb_path /path/to/val_emb --model_name \"pre-trained visual backbone\" --topk 8 --hidden-size 8192 --csr_ckpt \"CSR ckpt path\";python ./retrieval/faiss_nn.py --topk 8", "cmd_prefix": "python ./retrieval/compute_metrics.py", "cmd_postfix": "--topk 8", "target_cmd": "python ./retrieval/compute_metrics.py --topk 8"}
{"uuid": "171b24ad-b283-4d47-a31d-213f55618897", "execution_plan": "step1: Download and install Isaac Gym from the provided website or using the CLI command; step2: Create a conda environment named 'coohoi' with Python 3.8; step3: Activate the conda environment; step4: Install IsaacGym wrappers for Python; step5: Install other dependencies listed in requirements.txt; step6: Set environment variables if encountering an ImportError related to libpython3.8m.so.1.0; step7: Reproduce single agent object carrying task results from the paper; step8: Reproduce two agent object carrying task results from the paper; step9: Train a single humanoid skill policy; step10: Evaluate the trained single humanoid skill policy; step11: Train two humanoids cooperation policy starting from a single agent checkpoint; step12: Evaluate the trained two humanoids cooperation policy", "executed_cmds": "wget https://developer.nvidia.com/isaac-gym-preview-4;tar -xvzf isaac-gym-preview-4", "cmd_prefix": "conda create", "cmd_postfix": "-n coohoi python=3.8", "target_cmd": "conda create -n coohoi python=3.8"}
{"uuid": "94f3253c-da9d-4302-b723-1f12064c9773", "execution_plan": "step1: Download and install Isaac Gym from the provided website or using the CLI command; step2: Create a conda environment named 'coohoi' with Python 3.8; step3: Activate the conda environment; step4: Install IsaacGym wrappers for Python; step5: Install other dependencies listed in requirements.txt; step6: Set environment variables if encountering an ImportError related to libpython3.8m.so.1.0; step7: Reproduce single agent object carrying task results from the paper; step8: Reproduce two agent object carrying task results from the paper; step9: Train a single humanoid skill policy; step10: Evaluate the trained single humanoid skill policy; step11: Train two humanoids cooperation policy starting from a single agent checkpoint; step12: Evaluate the trained two humanoids cooperation policy", "executed_cmds": "wget https://developer.nvidia.com/isaac-gym-preview-4;tar -xvzf isaac-gym-preview-4;conda create -n coohoi python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate coohoi", "target_cmd": "conda activate coohoi"}
{"uuid": "26805188-7519-47fb-a092-1b8e5aa569d4", "execution_plan": "step1: Download and install Isaac Gym from the provided website or using the CLI command; step2: Create a conda environment named 'coohoi' with Python 3.8; step3: Activate the conda environment; step4: Install IsaacGym wrappers for Python; step5: Install other dependencies listed in requirements.txt; step6: Set environment variables if encountering an ImportError related to libpython3.8m.so.1.0; step7: Reproduce single agent object carrying task results from the paper; step8: Reproduce two agent object carrying task results from the paper; step9: Train a single humanoid skill policy; step10: Evaluate the trained single humanoid skill policy; step11: Train two humanoids cooperation policy starting from a single agent checkpoint; step12: Evaluate the trained two humanoids cooperation policy", "executed_cmds": "wget https://developer.nvidia.com/isaac-gym-preview-4;tar -xvzf isaac-gym-preview-4;conda create -n coohoi python=3.8;conda activate coohoi", "cmd_prefix": "pip install", "cmd_postfix": "-e isaacgym/python", "target_cmd": "pip install -e isaacgym/python"}
{"uuid": "e1617df5-1f02-49bc-a033-748967a85553", "execution_plan": "step1: Download and install Isaac Gym from the provided website or using the CLI command; step2: Create a conda environment named 'coohoi' with Python 3.8; step3: Activate the conda environment; step4: Install IsaacGym wrappers for Python; step5: Install other dependencies listed in requirements.txt; step6: Set environment variables if encountering an ImportError related to libpython3.8m.so.1.0; step7: Reproduce single agent object carrying task results from the paper; step8: Reproduce two agent object carrying task results from the paper; step9: Train a single humanoid skill policy; step10: Evaluate the trained single humanoid skill policy; step11: Train two humanoids cooperation policy starting from a single agent checkpoint; step12: Evaluate the trained two humanoids cooperation policy", "executed_cmds": "wget https://developer.nvidia.com/isaac-gym-preview-4;tar -xvzf isaac-gym-preview-4;conda create -n coohoi python=3.8;conda activate coohoi;pip install -e isaacgym/python", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "b7af268c-8133-44fa-835c-c2fd17372029", "execution_plan": "step1: Create a conda virtual environment and install necessary dependencies; step2: Clone and install TimeSynth repository for switch-feature testing; step3: Run experiments on real datasets (e.g., MIMIC-III) using provided scripts; step4: Parse and save the results of the experiments; step5: (Optional) Download model checkpoints if needed for reproducibility", "executed_cmds": "conda create -n timing python==3.10.16", "cmd_prefix": "conda", "cmd_postfix": "activate timing", "target_cmd": "conda activate timing"}
{"uuid": "25d2d251-27f2-4f88-b4d4-d42c155a7577", "execution_plan": "step1: Create a conda virtual environment and install necessary dependencies; step2: Clone and install TimeSynth repository for switch-feature testing; step3: Run experiments on real datasets (e.g., MIMIC-III) using provided scripts; step4: Parse and save the results of the experiments; step5: (Optional) Download model checkpoints if needed for reproducibility", "executed_cmds": "conda create -n timing python==3.10.16;conda activate timing;pip install -r requirement.txt --no-deps;git clone https://github.com/TimeSynth/TimeSynth.git", "cmd_prefix": "cd", "cmd_postfix": "TimeSynth", "target_cmd": "cd TimeSynth"}
{"uuid": "8633f078-6ecb-442b-bce5-892c0b407138", "execution_plan": "step1: Create a conda virtual environment and install necessary dependencies; step2: Clone and install TimeSynth repository for switch-feature testing; step3: Run experiments on real datasets (e.g., MIMIC-III) using provided scripts; step4: Parse and save the results of the experiments; step5: (Optional) Download model checkpoints if needed for reproducibility", "executed_cmds": "conda create -n timing python==3.10.16;conda activate timing;pip install -r requirement.txt --no-deps;git clone https://github.com/TimeSynth/TimeSynth.git;cd TimeSynth", "cmd_prefix": "python", "cmd_postfix": "setup.py install", "target_cmd": "python setup.py install"}
{"uuid": "aed78f9b-4fb0-411c-8a19-da5a23d632dc", "execution_plan": "step1: Create a conda virtual environment and install necessary dependencies; step2: Clone and install TimeSynth repository for switch-feature testing; step3: Run experiments on real datasets (e.g., MIMIC-III) using provided scripts; step4: Parse and save the results of the experiments; step5: (Optional) Download model checkpoints if needed for reproducibility", "executed_cmds": "conda create -n timing python==3.10.16;conda activate timing;pip install -r requirement.txt --no-deps;git clone https://github.com/TimeSynth/TimeSynth.git;cd TimeSynth;python setup.py install", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "72b7ac38-8ff0-4f1a-9319-9bcc3b255b2b", "execution_plan": "step1: Clone the repository to your local machine; step2: Prepare the Conda environment using the provided YAML file; step3: Download the SKEMPI v2 dataset using the provided script; step4: Download the trained model weights from Google Drive and place them in the ./ckpt folder; step5: Perform inference using the BA-Cycle (unsupervised) version with the pre-trained ProteinMPNN model; step6: Perform inference using the BA-DDG (supervised) version with the pre-trained BA-DDG model; step7: Train the BA-DDG model with optional Weights & Biases integration", "executed_cmds": "git clone https://github.com/aim-uofa/BA-DDG", "cmd_prefix": "cd", "cmd_postfix": "BA-DDG", "target_cmd": "cd BA-DDG"}
{"uuid": "62f09112-0fad-4bbd-91dc-c8fa5ca17dfa", "execution_plan": "step1: Clone the repository to your local machine; step2: Prepare the Conda environment using the provided YAML file; step3: Download the SKEMPI v2 dataset using the provided script; step4: Download the trained model weights from Google Drive and place them in the ./ckpt folder; step5: Perform inference using the BA-Cycle (unsupervised) version with the pre-trained ProteinMPNN model; step6: Perform inference using the BA-DDG (supervised) version with the pre-trained BA-DDG model; step7: Train the BA-DDG model with optional Weights & Biases integration", "executed_cmds": "git clone https://github.com/aim-uofa/BA-DDG;cd BA-DDG", "cmd_prefix": "conda env", "cmd_postfix": "create -f env.yml", "target_cmd": "conda env create -f env.yml"}
{"uuid": "1047f692-44d9-4a0d-bec2-8a63c83b1a4e", "execution_plan": "step1: Clone the repository to your local machine; step2: Prepare the Conda environment using the provided YAML file; step3: Download the SKEMPI v2 dataset using the provided script; step4: Download the trained model weights from Google Drive and place them in the ./ckpt folder; step5: Perform inference using the BA-Cycle (unsupervised) version with the pre-trained ProteinMPNN model; step6: Perform inference using the BA-DDG (supervised) version with the pre-trained BA-DDG model; step7: Train the BA-DDG model with optional Weights & Biases integration", "executed_cmds": "git clone https://github.com/aim-uofa/BA-DDG;cd BA-DDG;conda env create -f env.yml", "cmd_prefix": "conda", "cmd_postfix": "activate BA-DDG", "target_cmd": "conda activate BA-DDG"}
{"uuid": "007af938-7307-4bc8-86b6-e0ea5efefbfe", "execution_plan": "step1: Clone the repository to your local machine; step2: Prepare the Conda environment using the provided YAML file; step3: Download the SKEMPI v2 dataset using the provided script; step4: Download the trained model weights from Google Drive and place them in the ./ckpt folder; step5: Perform inference using the BA-Cycle (unsupervised) version with the pre-trained ProteinMPNN model; step6: Perform inference using the BA-DDG (supervised) version with the pre-trained BA-DDG model; step7: Train the BA-DDG model with optional Weights & Biases integration", "executed_cmds": "git clone https://github.com/aim-uofa/BA-DDG;cd BA-DDG;conda env create -f env.yml;conda activate BA-DDG", "cmd_prefix": "cd", "cmd_postfix": "training", "target_cmd": "cd training"}
{"uuid": "20ee8b5f-de29-444f-ba4d-ca93767bb13b", "execution_plan": "step1: Clone the repository to your local machine; step2: Prepare the Conda environment using the provided YAML file; step3: Download the SKEMPI v2 dataset using the provided script; step4: Download the trained model weights from Google Drive and place them in the ./ckpt folder; step5: Perform inference using the BA-Cycle (unsupervised) version with the pre-trained ProteinMPNN model; step6: Perform inference using the BA-DDG (supervised) version with the pre-trained BA-DDG model; step7: Train the BA-DDG model with optional Weights & Biases integration", "executed_cmds": "git clone https://github.com/aim-uofa/BA-DDG;cd BA-DDG;conda env create -f env.yml;conda activate BA-DDG;cd training;python train_skempi.py --config_path ../config/inference_ba-cycle_skempi.json", "cmd_prefix": "cd", "cmd_postfix": "training", "target_cmd": "cd training"}
{"uuid": "e6cb5dad-52b7-467e-95e9-57360be61669", "execution_plan": "step1: Clone the repository to your local machine; step2: Prepare the Conda environment using the provided YAML file; step3: Download the SKEMPI v2 dataset using the provided script; step4: Download the trained model weights from Google Drive and place them in the ./ckpt folder; step5: Perform inference using the BA-Cycle (unsupervised) version with the pre-trained ProteinMPNN model; step6: Perform inference using the BA-DDG (supervised) version with the pre-trained BA-DDG model; step7: Train the BA-DDG model with optional Weights & Biases integration", "executed_cmds": "git clone https://github.com/aim-uofa/BA-DDG;cd BA-DDG;conda env create -f env.yml;conda activate BA-DDG;cd training;python train_skempi.py --config_path ../config/inference_ba-cycle_skempi.json;cd training;python train_skempi.py --config_path ../config/inference_ba-ddg_skempi.json", "cmd_prefix": "cd", "cmd_postfix": "training", "target_cmd": "cd training"}
{"uuid": "042588ca-7a19-4376-9832-4f086bb35593", "execution_plan": "step1: Set up a Python environment for Trust-Eval using conda; step2: Install Trust-Eval package and its dependencies; step3: Set up NLTK for Trust-Eval; step4: Set up a Python environment for Trust-Align using conda; step5: Install Trust-Align dependencies from requirements.txt; step6: Clone and install the alignment-handbook repository for Trust-Align; step7: Download the evaluation dataset from Huggingface for Trust-Eval; step8: Download the SFT and DPO training dataset from Huggingface for Trust-Align", "executed_cmds": "conda create -n trust_eval python=3.10.13", "cmd_prefix": "conda", "cmd_postfix": "activate trust_eval", "target_cmd": "conda activate trust_eval"}
{"uuid": "7721f492-2845-4213-adcf-5c78dfbef60d", "execution_plan": "step1: Set up a Python environment for Trust-Eval using conda; step2: Install Trust-Eval package and its dependencies; step3: Set up NLTK for Trust-Eval; step4: Set up a Python environment for Trust-Align using conda; step5: Install Trust-Align dependencies from requirements.txt; step6: Clone and install the alignment-handbook repository for Trust-Align; step7: Download the evaluation dataset from Huggingface for Trust-Eval; step8: Download the SFT and DPO training dataset from Huggingface for Trust-Align", "executed_cmds": "conda create -n trust_eval python=3.10.13;conda activate trust_eval", "cmd_prefix": "pip", "cmd_postfix": "install trust_eval", "target_cmd": "pip install trust_eval"}
{"uuid": "7726977d-69e3-4027-902c-39648492bb9f", "execution_plan": "step1: Set up a Python environment for Trust-Eval using conda; step2: Install Trust-Eval package and its dependencies; step3: Set up NLTK for Trust-Eval; step4: Set up a Python environment for Trust-Align using conda; step5: Install Trust-Align dependencies from requirements.txt; step6: Clone and install the alignment-handbook repository for Trust-Align; step7: Download the evaluation dataset from Huggingface for Trust-Eval; step8: Download the SFT and DPO training dataset from Huggingface for Trust-Align", "executed_cmds": "conda create -n trust_eval python=3.10.13;conda activate trust_eval;pip install trust_eval", "cmd_prefix": "import", "cmd_postfix": "nltk", "target_cmd": "import nltk"}
{"uuid": "d9cf1825-a9d9-4649-aeac-cf1020d882f3", "execution_plan": "step1: Set up a Python environment for Trust-Eval using conda; step2: Install Trust-Eval package and its dependencies; step3: Set up NLTK for Trust-Eval; step4: Set up a Python environment for Trust-Align using conda; step5: Install Trust-Align dependencies from requirements.txt; step6: Clone and install the alignment-handbook repository for Trust-Align; step7: Download the evaluation dataset from Huggingface for Trust-Eval; step8: Download the SFT and DPO training dataset from Huggingface for Trust-Align", "executed_cmds": "conda create -n trust_eval python=3.10.13;conda activate trust_eval;pip install trust_eval;import nltk;nltk.download('punkt_tab');conda create -n cite python=3.10.13", "cmd_prefix": "conda", "cmd_postfix": "activate cite", "target_cmd": "conda activate cite"}
{"uuid": "3144e6c5-7fba-4c9c-8531-f8af5766bed6", "execution_plan": "step1: Set up a Python environment for Trust-Eval using conda; step2: Install Trust-Eval package and its dependencies; step3: Set up NLTK for Trust-Eval; step4: Set up a Python environment for Trust-Align using conda; step5: Install Trust-Align dependencies from requirements.txt; step6: Clone and install the alignment-handbook repository for Trust-Align; step7: Download the evaluation dataset from Huggingface for Trust-Eval; step8: Download the SFT and DPO training dataset from Huggingface for Trust-Align", "executed_cmds": "conda create -n trust_eval python=3.10.13;conda activate trust_eval;pip install trust_eval;import nltk;nltk.download('punkt_tab');conda create -n cite python=3.10.13;conda activate cite", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "f794d2fa-b748-4626-a413-f39e704f4c41", "execution_plan": "step1: Set up a Python environment for Trust-Eval using conda; step2: Install Trust-Eval package and its dependencies; step3: Set up NLTK for Trust-Eval; step4: Set up a Python environment for Trust-Align using conda; step5: Install Trust-Align dependencies from requirements.txt; step6: Clone and install the alignment-handbook repository for Trust-Align; step7: Download the evaluation dataset from Huggingface for Trust-Eval; step8: Download the SFT and DPO training dataset from Huggingface for Trust-Align", "executed_cmds": "conda create -n trust_eval python=3.10.13;conda activate trust_eval;pip install trust_eval;import nltk;nltk.download('punkt_tab');conda create -n cite python=3.10.13;conda activate cite;pip install -r requirements.txt;git clone https://github.com/huggingface/alignment-handbook.git;cd ./alignment-handbook/", "cmd_prefix": "python -m", "cmd_postfix": "pip install .", "target_cmd": "python -m pip install ."}
{"uuid": "14bf34ec-797e-4cb3-bbed-a8f59f0fa8f7", "execution_plan": "step1: Clone the PiSSA repository and navigate into the directory; step2: Download the required dataset from Hugging Face; step3: Create a Conda environment for PiSSA and activate it; step4: Install CUDA toolkit and PyTorch with CUDA support; step5: Install the required Python packages and flash-attn; step6: Run the training scripts for full fine-tuning, LoRA, PiSSA, LoftQ, QLoRA, or QPiSSA; step7: Evaluate the fine-tuned model using the provided dataset; step8: For advanced usage, download decomposed models from Hugging Face or apply PiSSA initialization to a pre-trained model; step9: Load a pre-processed model and fine-tune it on a dataset like IMDB; step10: Convert PiSSA to LoRA if needed", "executed_cmds": "git clone https://github.com/GraphPKU/PiSSA.git", "cmd_prefix": "cd", "cmd_postfix": "PiSSA/", "target_cmd": "cd PiSSA/"}
{"uuid": "a9d48451-60ca-467c-a7f2-97d9b05e8891", "execution_plan": "step1: Clone the PiSSA repository and navigate into the directory; step2: Download the required dataset from Hugging Face; step3: Create a Conda environment for PiSSA and activate it; step4: Install CUDA toolkit and PyTorch with CUDA support; step5: Install the required Python packages and flash-attn; step6: Run the training scripts for full fine-tuning, LoRA, PiSSA, LoftQ, QLoRA, or QPiSSA; step7: Evaluate the fine-tuned model using the provided dataset; step8: For advanced usage, download decomposed models from Hugging Face or apply PiSSA initialization to a pre-trained model; step9: Load a pre-processed model and fine-tune it on a dataset like IMDB; step10: Convert PiSSA to LoRA if needed", "executed_cmds": "git clone https://github.com/GraphPKU/PiSSA.git;cd PiSSA/;huggingface-cli download --repo-type dataset --resume-download fxmeng/pissa-dataset --local-dir pissa-dataset", "cmd_prefix": "conda create", "cmd_postfix": "-n pissa python=3.10", "target_cmd": "conda create -n pissa python=3.10"}
{"uuid": "5cff569e-bf52-460a-8bb8-a881564b0c7f", "execution_plan": "step1: Clone the PiSSA repository and navigate into the directory; step2: Download the required dataset from Hugging Face; step3: Create a Conda environment for PiSSA and activate it; step4: Install CUDA toolkit and PyTorch with CUDA support; step5: Install the required Python packages and flash-attn; step6: Run the training scripts for full fine-tuning, LoRA, PiSSA, LoftQ, QLoRA, or QPiSSA; step7: Evaluate the fine-tuned model using the provided dataset; step8: For advanced usage, download decomposed models from Hugging Face or apply PiSSA initialization to a pre-trained model; step9: Load a pre-processed model and fine-tune it on a dataset like IMDB; step10: Convert PiSSA to LoRA if needed", "executed_cmds": "git clone https://github.com/GraphPKU/PiSSA.git;cd PiSSA/;huggingface-cli download --repo-type dataset --resume-download fxmeng/pissa-dataset --local-dir pissa-dataset;conda create -n pissa python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate pissa", "target_cmd": "conda activate pissa"}
{"uuid": "1c9c6b62-7143-458a-be4a-bf11436f3cc1", "execution_plan": "step1: Clone the PiSSA repository and navigate into the directory; step2: Download the required dataset from Hugging Face; step3: Create a Conda environment for PiSSA and activate it; step4: Install CUDA toolkit and PyTorch with CUDA support; step5: Install the required Python packages and flash-attn; step6: Run the training scripts for full fine-tuning, LoRA, PiSSA, LoftQ, QLoRA, or QPiSSA; step7: Evaluate the fine-tuned model using the provided dataset; step8: For advanced usage, download decomposed models from Hugging Face or apply PiSSA initialization to a pre-trained model; step9: Load a pre-processed model and fine-tune it on a dataset like IMDB; step10: Convert PiSSA to LoRA if needed", "executed_cmds": "git clone https://github.com/GraphPKU/PiSSA.git;cd PiSSA/;huggingface-cli download --repo-type dataset --resume-download fxmeng/pissa-dataset --local-dir pissa-dataset;conda create -n pissa python=3.10;conda activate pissa;conda install nvidia/label/cuda-12.1.0::cuda-toolkit;conda install pytorch==2.4.0 torchvision=0.19.0 pytorch-cuda=12.1 -c pytorch -c nvidia", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "4da25ab3-d964-4cba-a5a8-1835e20f2bbc", "execution_plan": "step1: Clone the PiSSA repository and navigate into the directory; step2: Download the required dataset from Hugging Face; step3: Create a Conda environment for PiSSA and activate it; step4: Install CUDA toolkit and PyTorch with CUDA support; step5: Install the required Python packages and flash-attn; step6: Run the training scripts for full fine-tuning, LoRA, PiSSA, LoftQ, QLoRA, or QPiSSA; step7: Evaluate the fine-tuned model using the provided dataset; step8: For advanced usage, download decomposed models from Hugging Face or apply PiSSA initialization to a pre-trained model; step9: Load a pre-processed model and fine-tune it on a dataset like IMDB; step10: Convert PiSSA to LoRA if needed", "executed_cmds": "git clone https://github.com/GraphPKU/PiSSA.git;cd PiSSA/;huggingface-cli download --repo-type dataset --resume-download fxmeng/pissa-dataset --local-dir pissa-dataset;conda create -n pissa python=3.10;conda activate pissa;conda install nvidia/label/cuda-12.1.0::cuda-toolkit;conda install pytorch==2.4.0 torchvision=0.19.0 pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install flash-attn --no-build-isolation;sh scripts/*/run_full_finetune.sh", "cmd_prefix": "sh", "cmd_postfix": "scripts/*/lora.sh", "target_cmd": "sh scripts/*/lora.sh"}
{"uuid": "f30717ff-d0e2-40a2-9c3d-72e75e4c8bd0", "execution_plan": "step1: Clone the PiSSA repository and navigate into the directory; step2: Download the required dataset from Hugging Face; step3: Create a Conda environment for PiSSA and activate it; step4: Install CUDA toolkit and PyTorch with CUDA support; step5: Install the required Python packages and flash-attn; step6: Run the training scripts for full fine-tuning, LoRA, PiSSA, LoftQ, QLoRA, or QPiSSA; step7: Evaluate the fine-tuned model using the provided dataset; step8: For advanced usage, download decomposed models from Hugging Face or apply PiSSA initialization to a pre-trained model; step9: Load a pre-processed model and fine-tune it on a dataset like IMDB; step10: Convert PiSSA to LoRA if needed", "executed_cmds": "git clone https://github.com/GraphPKU/PiSSA.git;cd PiSSA/;huggingface-cli download --repo-type dataset --resume-download fxmeng/pissa-dataset --local-dir pissa-dataset;conda create -n pissa python=3.10;conda activate pissa;conda install nvidia/label/cuda-12.1.0::cuda-toolkit;conda install pytorch==2.4.0 torchvision=0.19.0 pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install flash-attn --no-build-isolation;sh scripts/*/run_full_finetune.sh;sh scripts/*/lora.sh", "cmd_prefix": "sh", "cmd_postfix": "scripts/*/pissa.sh", "target_cmd": "sh scripts/*/pissa.sh"}
{"uuid": "c2dfa6a2-00f7-4368-b3d1-762f5705df32", "execution_plan": "step1: Clone the PiSSA repository and navigate into the directory; step2: Download the required dataset from Hugging Face; step3: Create a Conda environment for PiSSA and activate it; step4: Install CUDA toolkit and PyTorch with CUDA support; step5: Install the required Python packages and flash-attn; step6: Run the training scripts for full fine-tuning, LoRA, PiSSA, LoftQ, QLoRA, or QPiSSA; step7: Evaluate the fine-tuned model using the provided dataset; step8: For advanced usage, download decomposed models from Hugging Face or apply PiSSA initialization to a pre-trained model; step9: Load a pre-processed model and fine-tune it on a dataset like IMDB; step10: Convert PiSSA to LoRA if needed", "executed_cmds": "git clone https://github.com/GraphPKU/PiSSA.git;cd PiSSA/;huggingface-cli download --repo-type dataset --resume-download fxmeng/pissa-dataset --local-dir pissa-dataset;conda create -n pissa python=3.10;conda activate pissa;conda install nvidia/label/cuda-12.1.0::cuda-toolkit;conda install pytorch==2.4.0 torchvision=0.19.0 pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install flash-attn --no-build-isolation;sh scripts/*/run_full_finetune.sh;sh scripts/*/lora.sh;sh scripts/*/pissa.sh", "cmd_prefix": "sh", "cmd_postfix": "scripts/*/loftq.sh", "target_cmd": "sh scripts/*/loftq.sh"}
{"uuid": "ed3923e5-fc69-4f3b-bd2d-9ab5c66ea764", "execution_plan": "step1: Clone the PiSSA repository and navigate into the directory; step2: Download the required dataset from Hugging Face; step3: Create a Conda environment for PiSSA and activate it; step4: Install CUDA toolkit and PyTorch with CUDA support; step5: Install the required Python packages and flash-attn; step6: Run the training scripts for full fine-tuning, LoRA, PiSSA, LoftQ, QLoRA, or QPiSSA; step7: Evaluate the fine-tuned model using the provided dataset; step8: For advanced usage, download decomposed models from Hugging Face or apply PiSSA initialization to a pre-trained model; step9: Load a pre-processed model and fine-tune it on a dataset like IMDB; step10: Convert PiSSA to LoRA if needed", "executed_cmds": "git clone https://github.com/GraphPKU/PiSSA.git;cd PiSSA/;huggingface-cli download --repo-type dataset --resume-download fxmeng/pissa-dataset --local-dir pissa-dataset;conda create -n pissa python=3.10;conda activate pissa;conda install nvidia/label/cuda-12.1.0::cuda-toolkit;conda install pytorch==2.4.0 torchvision=0.19.0 pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install flash-attn --no-build-isolation;sh scripts/*/run_full_finetune.sh;sh scripts/*/lora.sh;sh scripts/*/pissa.sh;sh scripts/*/loftq.sh", "cmd_prefix": "sh", "cmd_postfix": "scripts/*/qlora.sh", "target_cmd": "sh scripts/*/qlora.sh"}
{"uuid": "fd2696cc-f3ef-48b4-813e-1c152046c684", "execution_plan": "step1: Clone the PiSSA repository and navigate into the directory; step2: Download the required dataset from Hugging Face; step3: Create a Conda environment for PiSSA and activate it; step4: Install CUDA toolkit and PyTorch with CUDA support; step5: Install the required Python packages and flash-attn; step6: Run the training scripts for full fine-tuning, LoRA, PiSSA, LoftQ, QLoRA, or QPiSSA; step7: Evaluate the fine-tuned model using the provided dataset; step8: For advanced usage, download decomposed models from Hugging Face or apply PiSSA initialization to a pre-trained model; step9: Load a pre-processed model and fine-tune it on a dataset like IMDB; step10: Convert PiSSA to LoRA if needed", "executed_cmds": "git clone https://github.com/GraphPKU/PiSSA.git;cd PiSSA/;huggingface-cli download --repo-type dataset --resume-download fxmeng/pissa-dataset --local-dir pissa-dataset;conda create -n pissa python=3.10;conda activate pissa;conda install nvidia/label/cuda-12.1.0::cuda-toolkit;conda install pytorch==2.4.0 torchvision=0.19.0 pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install flash-attn --no-build-isolation;sh scripts/*/run_full_finetune.sh;sh scripts/*/lora.sh;sh scripts/*/pissa.sh;sh scripts/*/loftq.sh;sh scripts/*/qlora.sh", "cmd_prefix": "sh", "cmd_postfix": "scripts/*/qpissa.sh", "target_cmd": "sh scripts/*/qpissa.sh"}
{"uuid": "7283d404-4066-44c4-b3c2-3bb6082b291e", "execution_plan": "step1: Clone the repository to get the project files; step2: Install the required Python dependencies; step3: Compile the code to prepare for execution; step4: Train the model using the provided training script; step5: Visualize the training results by running the analysis script; step6: Train the policy using the gridworld training script; step7: Perform inference using the gridworld inference script", "executed_cmds": "git clone https://github.com/leoperezz/GoalReducerWithLoopRemoval.git;cd GoalReducerWithLoopRemoval", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "e5057cc3-ac4e-4599-9fdf-a19c11591e79", "execution_plan": "step1: Visualize the Directionally Aligned Perturbation (DAP) by running the provided Jupyter notebook in Google Colab; step2: Repeat the synthetic experiments by running the provided Jupyter notebook in Google Colab; step3: Install the required packages for the Language Model Training Experiment; step4: Train the model using the specified hyper-parameters for the Language Model Training Experiment; step5: Repeat the Mesh Optimization Experiment by running the provided script", "executed_cmds": "pip install -r requirements.txt;srun python3 ./zo-bench/run.py --num_virtual_tokens=10 --prompt_init_by_real_tokens --model_name=facebook/opt-1.3b --task_name=SST2 --overwrite_output_dir --no_reparam --num_train_epochs=5 --per_device_train_batch_size=16 --load_best_model_at_end --evaluation_strategy=steps --save_strategy=steps --save_total_limit=1 --eval_steps=1000 --max_steps=20000 --logging_steps=10 --num_eval=1000 --num_train=1000 --num_dev=500 --train_as_classification --perturbation_mode=one_side --trainer=zo_sgd --optimizer=sgd --train_set_seed=0 --lr_scheduler_type=constant --eval_steps=500 --save_steps=500 --learning_rate=1e-4 --weight_decay=0 --zo_eps=1e-5 --perturbation=rademacher", "cmd_prefix": "sbatch", "cmd_postfix": "run.sh", "target_cmd": "sbatch run.sh"}
{"uuid": "3440e868-f72b-4f06-a787-e999b54ecd74", "execution_plan": "step1: Install the required Python packages and dependencies; step2: Download the necessary datasets from the provided links; step3: Run a single algorithm on a specified dataset; step4: Run multiple algorithms in batches on specified datasets; step5: Collect experimental results from the output files", "executed_cmds": "python -m plench.train --data_dir=<your dataset path> --algorithm PRODEN --dataset PLCIFAR10_Aggregate --output_dir=<your output path> --steps 60000 --skip_model_save --checkpoint_freq 1000;python -m plench.sweep launch --data_dir=<your dataset path> --command_launcher multi_gpu --n_hparams_from 0 --n_hparams 20 --n_trials_from 0 --n_trials 3 --datasets PLCIFAR10_Aggregate PLCIFAR10_Vaguest --algorithms PRODEN CAVL --output_dir=<your output path> --skip_confirmation --skip_model_save --steps 60000", "cmd_prefix": "python -m plench.collect_results --input_dir=<path", "cmd_postfix": "of the output files>", "target_cmd": "python -m plench.collect_results --input_dir=<path of the output files>"}
{"uuid": "e8fc8ac7-dc46-4c25-8ac6-93b04e97e0e8", "execution_plan": "step1: Install uv and set up the Python environment; step2: Download and compile Open Babel; step3: Clone and compile AutoDock-GPU; step4: Prepare the required data files in the specified format; step5: Run the pretrain phase to learn p(x|z); step6: Run the finetune phase to learn p(y|z) jointly with the pretrained model; step7: Run the onlinelearn phase to shift the distribution of the generative model", "executed_cmds": "which uv >/dev/null 2>&1 || curl -LsSf https://astral.sh/uv/install.sh | sh", "cmd_prefix": "uv", "cmd_postfix": "venv --python=3.10", "target_cmd": "uv venv --python=3.10"}
{"uuid": "c419778d-995e-4161-9d11-7c9c8a10cda4", "execution_plan": "step1: Install uv and set up the Python environment; step2: Download and compile Open Babel; step3: Clone and compile AutoDock-GPU; step4: Prepare the required data files in the specified format; step5: Run the pretrain phase to learn p(x|z); step6: Run the finetune phase to learn p(y|z) jointly with the pretrained model; step7: Run the onlinelearn phase to shift the distribution of the generative model", "executed_cmds": "which uv >/dev/null 2>&1 || curl -LsSf https://astral.sh/uv/install.sh | sh;uv venv --python=3.10", "cmd_prefix": "source", "cmd_postfix": ".venv/bin/activate", "target_cmd": "source .venv/bin/activate"}
{"uuid": "a5c46e5b-ef80-4f97-af19-5bc7f2dfff04", "execution_plan": "step1: Install uv and set up the Python environment; step2: Download and compile Open Babel; step3: Clone and compile AutoDock-GPU; step4: Prepare the required data files in the specified format; step5: Run the pretrain phase to learn p(x|z); step6: Run the finetune phase to learn p(y|z) jointly with the pretrained model; step7: Run the onlinelearn phase to shift the distribution of the generative model", "executed_cmds": "which uv >/dev/null 2>&1 || curl -LsSf https://astral.sh/uv/install.sh | sh;uv venv --python=3.10;source .venv/bin/activate;uv pip install -r requirements.txt", "cmd_prefix": "curl -L https://github.com/openbabel/openbabel/releases/download/openbabel-3-1-1/openbabel-3.1.1-source.tar.bz2", "cmd_postfix": "| tar -xj", "target_cmd": "curl -L https://github.com/openbabel/openbabel/releases/download/openbabel-3-1-1/openbabel-3.1.1-source.tar.bz2 | tar -xj"}
{"uuid": "8410e97e-7c25-4728-a0d2-d730848b766e", "execution_plan": "step1: Install uv and set up the Python environment; step2: Download and compile Open Babel; step3: Clone and compile AutoDock-GPU; step4: Prepare the required data files in the specified format; step5: Run the pretrain phase to learn p(x|z); step6: Run the finetune phase to learn p(y|z) jointly with the pretrained model; step7: Run the onlinelearn phase to shift the distribution of the generative model", "executed_cmds": "which uv >/dev/null 2>&1 || curl -LsSf https://astral.sh/uv/install.sh | sh;uv venv --python=3.10;source .venv/bin/activate;uv pip install -r requirements.txt;curl -L https://github.com/openbabel/openbabel/releases/download/openbabel-3-1-1/openbabel-3.1.1-source.tar.bz2 | tar -xj;git clone https://github.com/ccsb-scripps/AutoDock-GPU.git", "cmd_prefix": "cd", "cmd_postfix": "AutoDock-GPU", "target_cmd": "cd AutoDock-GPU"}
{"uuid": "3c2f2aba-9495-48ee-866d-5eb212e0f157", "execution_plan": "step1: Install uv and set up the Python environment; step2: Download and compile Open Babel; step3: Clone and compile AutoDock-GPU; step4: Prepare the required data files in the specified format; step5: Run the pretrain phase to learn p(x|z); step6: Run the finetune phase to learn p(y|z) jointly with the pretrained model; step7: Run the onlinelearn phase to shift the distribution of the generative model", "executed_cmds": "which uv >/dev/null 2>&1 || curl -LsSf https://astral.sh/uv/install.sh | sh;uv venv --python=3.10;source .venv/bin/activate;uv pip install -r requirements.txt;curl -L https://github.com/openbabel/openbabel/releases/download/openbabel-3-1-1/openbabel-3.1.1-source.tar.bz2 | tar -xj;git clone https://github.com/ccsb-scripps/AutoDock-GPU.git;cd AutoDock-GPU;export GPU_INCLUDE_PATH=/usr/include;export GPU_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu", "cmd_prefix": "make", "cmd_postfix": "DEVICE=CUDA", "target_cmd": "make DEVICE=CUDA"}
{"uuid": "40f43cf2-4d97-4093-b5a6-b8d050d2614a", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Install the required dependencies and download the data by running the setup script; step3: Run the quick start command to get results of KnoBo on X-ray datasets; step4: (Optional) Extract features using other models if needed; step5: (Optional) Generate bottlenecks from medical documents using MedRAG; step6: (Optional) Annotate concepts in the bottleneck using Flan-T5-XXL or GPT-4; step7: (Optional) Train grounding functions for each concept in the bottleneck; step8: (Optional) Run baseline methods like Linear Probing, PCBM-h, End-to-End, or LSL", "executed_cmds": "git clone https://github.com/YueYANG1996/KnoBo.git", "cmd_prefix": "cd", "cmd_postfix": "KnoBo", "target_cmd": "cd KnoBo"}
{"uuid": "905d97de-f3eb-4ac7-8b8e-da3e9dea63fb", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Install the required dependencies and download the data by running the setup script; step3: Run the quick start command to get results of KnoBo on X-ray datasets; step4: (Optional) Extract features using other models if needed; step5: (Optional) Generate bottlenecks from medical documents using MedRAG; step6: (Optional) Annotate concepts in the bottleneck using Flan-T5-XXL or GPT-4; step7: (Optional) Train grounding functions for each concept in the bottleneck; step8: (Optional) Run baseline methods like Linear Probing, PCBM-h, End-to-End, or LSL", "executed_cmds": "git clone https://github.com/YueYANG1996/KnoBo.git;cd KnoBo", "cmd_prefix": "sh", "cmd_postfix": "setup.sh", "target_cmd": "sh setup.sh"}
{"uuid": "717397a5-c9c6-4b73-971f-3ad650171cf2", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Install the required dependencies and download the data by running the setup script; step3: Run the quick start command to get results of KnoBo on X-ray datasets; step4: (Optional) Extract features using other models if needed; step5: (Optional) Generate bottlenecks from medical documents using MedRAG; step6: (Optional) Annotate concepts in the bottleneck using Flan-T5-XXL or GPT-4; step7: (Optional) Train grounding functions for each concept in the bottleneck; step8: (Optional) Run baseline methods like Linear Probing, PCBM-h, End-to-End, or LSL", "executed_cmds": "git clone https://github.com/YueYANG1996/KnoBo.git;cd KnoBo;sh setup.sh;python modules/cbm.py --mode binary --bottleneck PubMed --number_of_features 150 --add_prior True --modality xray --model_name whyxrayclip;python modules/extract_features.py --dataset_name <NAME OF THE DATASET> --model_name <NAME OF THE MODEL> --image_dir <PATH TO THE IMAGE DIRECTORY>;git clone https://github.com/YueYANG1996/MedRAG.git", "cmd_prefix": "cd", "cmd_postfix": "MedRAG", "target_cmd": "cd MedRAG"}
{"uuid": "88d98f5a-c5a3-4392-a58e-88a5a8a488a2", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Install the required dependencies and download the data by running the setup script; step3: Run the quick start command to get results of KnoBo on X-ray datasets; step4: (Optional) Extract features using other models if needed; step5: (Optional) Generate bottlenecks from medical documents using MedRAG; step6: (Optional) Annotate concepts in the bottleneck using Flan-T5-XXL or GPT-4; step7: (Optional) Train grounding functions for each concept in the bottleneck; step8: (Optional) Run baseline methods like Linear Probing, PCBM-h, End-to-End, or LSL", "executed_cmds": "git clone https://github.com/YueYANG1996/KnoBo.git;cd KnoBo;sh setup.sh;python modules/cbm.py --mode binary --bottleneck PubMed --number_of_features 150 --add_prior True --modality xray --model_name whyxrayclip;python modules/extract_features.py --dataset_name <NAME OF THE DATASET> --model_name <NAME OF THE MODEL> --image_dir <PATH TO THE IMAGE DIRECTORY>;git clone https://github.com/YueYANG1996/MedRAG.git;cd MedRAG", "cmd_prefix": "sh", "cmd_postfix": "setup.sh", "target_cmd": "sh setup.sh"}
{"uuid": "a89084bc-9341-432c-a76b-27668dca5d4e", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a virtual environment named 'treg' with Python 3.11; step3: Activate the virtual environment; step4: Install the required dependencies using the requirements.txt file; step5: Run the solve.py script with the specified parameters for a super-resolution task using a cat image; step6: Run the solve.py script with the specified parameters for a Gauss-deblur task using a dog image; step7: Run the solve.py script with the specified parameters for a super-resolution task using an ice cream image and a Christmas tree prompt; step8: Run the solve.py script with the specified parameters for a Gauss-deblur task using a fried rice image and a spaghetti prompt", "executed_cmds": "git clone https://github.com/TReg-inverse/TReg.git", "cmd_prefix": "cd", "cmd_postfix": "TReg", "target_cmd": "cd TReg"}
{"uuid": "cbcb6c3c-a8f5-4a1b-8ed7-4da61c8c9fa4", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a virtual environment named 'treg' with Python 3.11; step3: Activate the virtual environment; step4: Install the required dependencies using the requirements.txt file; step5: Run the solve.py script with the specified parameters for a super-resolution task using a cat image; step6: Run the solve.py script with the specified parameters for a Gauss-deblur task using a dog image; step7: Run the solve.py script with the specified parameters for a super-resolution task using an ice cream image and a Christmas tree prompt; step8: Run the solve.py script with the specified parameters for a Gauss-deblur task using a fried rice image and a spaghetti prompt", "executed_cmds": "git clone https://github.com/TReg-inverse/TReg.git;cd TReg", "cmd_prefix": "conda create", "cmd_postfix": "-n treg python==3.11", "target_cmd": "conda create -n treg python==3.11"}
{"uuid": "734fcd55-7b5d-432a-b6cb-9b61d76827b8", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a virtual environment named 'treg' with Python 3.11; step3: Activate the virtual environment; step4: Install the required dependencies using the requirements.txt file; step5: Run the solve.py script with the specified parameters for a super-resolution task using a cat image; step6: Run the solve.py script with the specified parameters for a Gauss-deblur task using a dog image; step7: Run the solve.py script with the specified parameters for a super-resolution task using an ice cream image and a Christmas tree prompt; step8: Run the solve.py script with the specified parameters for a Gauss-deblur task using a fried rice image and a spaghetti prompt", "executed_cmds": "git clone https://github.com/TReg-inverse/TReg.git;cd TReg;conda create -n treg python==3.11", "cmd_prefix": "conda", "cmd_postfix": "activate treg", "target_cmd": "conda activate treg"}
{"uuid": "b3e84d7a-2d29-4a45-9c46-e824a9fd6c4b", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create a virtual environment named 'treg' with Python 3.11; step3: Activate the virtual environment; step4: Install the required dependencies using the requirements.txt file; step5: Run the solve.py script with the specified parameters for a super-resolution task using a cat image; step6: Run the solve.py script with the specified parameters for a Gauss-deblur task using a dog image; step7: Run the solve.py script with the specified parameters for a super-resolution task using an ice cream image and a Christmas tree prompt; step8: Run the solve.py script with the specified parameters for a Gauss-deblur task using a fried rice image and a spaghetti prompt", "executed_cmds": "git clone https://github.com/TReg-inverse/TReg.git;cd TReg;conda create -n treg python==3.11;conda activate treg", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "32b65421-e9f2-49a0-bc7c-f10c191944ae", "execution_plan": "step1: Download the pre-trained model and place it in the specified directory; step2: Modify the file paths in inference.py to point to the input and ground truth images; step3: Run the inference script to enhance low-light images with default settings; step4: Optionally adjust the exposure level by changing the final state value T during inference; step5: Train the model using the provided training script", "executed_cmds": "python inference.py", "cmd_prefix": "python inference.py", "cmd_postfix": "--T 4.8", "target_cmd": "python inference.py --T 4.8"}
{"uuid": "ee82ccaa-a982-4b54-b89a-643811a45eb2", "execution_plan": "step1: Download the pre-trained model and place it in the specified directory; step2: Modify the file paths in inference.py to point to the input and ground truth images; step3: Run the inference script to enhance low-light images with default settings; step4: Optionally adjust the exposure level by changing the final state value T during inference; step5: Train the model using the provided training script", "executed_cmds": "python inference.py;python inference.py --T 4.8", "cmd_prefix": "python inference.py", "cmd_postfix": "--T -1.4", "target_cmd": "python inference.py --T -1.4"}
{"uuid": "78077de8-3190-4b52-a401-0ff333c511c5", "execution_plan": "step1: Download the pre-trained model and place it in the specified directory; step2: Modify the file paths in inference.py to point to the input and ground truth images; step3: Run the inference script to enhance low-light images with default settings; step4: Optionally adjust the exposure level by changing the final state value T during inference; step5: Train the model using the provided training script", "executed_cmds": "python inference.py;python inference.py --T 4.8;python inference.py --T -1.4", "cmd_prefix": "python inference.py", "cmd_postfix": "--T 2.5", "target_cmd": "python inference.py --T 2.5"}
{"uuid": "1cebad06-f3ba-4db4-af63-03567d44cbb5", "execution_plan": "step1: Download the pre-trained model and place it in the specified directory; step2: Modify the file paths in inference.py to point to the input and ground truth images; step3: Run the inference script to enhance low-light images with default settings; step4: Optionally adjust the exposure level by changing the final state value T during inference; step5: Train the model using the provided training script", "executed_cmds": "python inference.py;python inference.py --T 4.8;python inference.py --T -1.4;python inference.py --T 2.5", "cmd_prefix": "python", "cmd_postfix": "main_experiment.py", "target_cmd": "python main_experiment.py"}
{"uuid": "e2d3da5b-08e7-44db-a65a-dc18de737e40", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a conda environment for the project; step3: Install the required PyTorch version compatible with CUDA 11.8; step4: Install Pointcept and flash-attention for point transformer v3; step5: Install other dependencies listed in requirements.txt; step6: Install gsplat from the specified GitHub repository; step7: Download the OOD-NVS test sets and place them in the appropriate directory structure; step8: Generate training datasets by following the provided instructions or download a subset; step9: Train SplatFormer using the provided scripts for Objaverse-v1 and ShapeNet; step10: Download trained checkpoints if not training from scratch; step11: Evaluate SplatFormer on the OOD-NVS test sets using the provided scripts; step12: Install the SIBR viewers for real-time 3DGS visualization; step13: Run the evaluation with the --save_viewer flag to generate viewer files; step14: Launch the real-time viewer to visualize the results", "executed_cmds": "git clone --recursive git@github.com:ChenYutongTHU/SplatFormer.git", "cmd_prefix": "cd", "cmd_postfix": "SplatFormer", "target_cmd": "cd SplatFormer"}
{"uuid": "f378d567-5196-4e3b-8406-f667267b0b31", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a conda environment for the project; step3: Install the required PyTorch version compatible with CUDA 11.8; step4: Install Pointcept and flash-attention for point transformer v3; step5: Install other dependencies listed in requirements.txt; step6: Install gsplat from the specified GitHub repository; step7: Download the OOD-NVS test sets and place them in the appropriate directory structure; step8: Generate training datasets by following the provided instructions or download a subset; step9: Train SplatFormer using the provided scripts for Objaverse-v1 and ShapeNet; step10: Download trained checkpoints if not training from scratch; step11: Evaluate SplatFormer on the OOD-NVS test sets using the provided scripts; step12: Install the SIBR viewers for real-time 3DGS visualization; step13: Run the evaluation with the --save_viewer flag to generate viewer files; step14: Launch the real-time viewer to visualize the results", "executed_cmds": "git clone --recursive git@github.com:ChenYutongTHU/SplatFormer.git;cd SplatFormer;conda create -n splatformer python=3.8 -y", "cmd_prefix": "conda", "cmd_postfix": "activate splatformer", "target_cmd": "conda activate splatformer"}
{"uuid": "9f7309f0-0592-4667-afdf-328ca4b7922e", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a conda environment for the project; step3: Install the required PyTorch version compatible with CUDA 11.8; step4: Install Pointcept and flash-attention for point transformer v3; step5: Install other dependencies listed in requirements.txt; step6: Install gsplat from the specified GitHub repository; step7: Download the OOD-NVS test sets and place them in the appropriate directory structure; step8: Generate training datasets by following the provided instructions or download a subset; step9: Train SplatFormer using the provided scripts for Objaverse-v1 and ShapeNet; step10: Download trained checkpoints if not training from scratch; step11: Evaluate SplatFormer on the OOD-NVS test sets using the provided scripts; step12: Install the SIBR viewers for real-time 3DGS visualization; step13: Run the evaluation with the --save_viewer flag to generate viewer files; step14: Launch the real-time viewer to visualize the results", "executed_cmds": "git clone --recursive git@github.com:ChenYutongTHU/SplatFormer.git;cd SplatFormer;conda create -n splatformer python=3.8 -y;conda activate splatformer;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118", "cmd_prefix": "pip", "cmd_postfix": "install Pointcept/", "target_cmd": "pip install Pointcept/"}
{"uuid": "ab62f051-6d38-405f-9a72-b88de1afe6c4", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a conda environment for the project; step3: Install the required PyTorch version compatible with CUDA 11.8; step4: Install Pointcept and flash-attention for point transformer v3; step5: Install other dependencies listed in requirements.txt; step6: Install gsplat from the specified GitHub repository; step7: Download the OOD-NVS test sets and place them in the appropriate directory structure; step8: Generate training datasets by following the provided instructions or download a subset; step9: Train SplatFormer using the provided scripts for Objaverse-v1 and ShapeNet; step10: Download trained checkpoints if not training from scratch; step11: Evaluate SplatFormer on the OOD-NVS test sets using the provided scripts; step12: Install the SIBR viewers for real-time 3DGS visualization; step13: Run the evaluation with the --save_viewer flag to generate viewer files; step14: Launch the real-time viewer to visualize the results", "executed_cmds": "git clone --recursive git@github.com:ChenYutongTHU/SplatFormer.git;cd SplatFormer;conda create -n splatformer python=3.8 -y;conda activate splatformer;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118;pip install Pointcept/;pip install flash-attn --no-build-isolation", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "909a2bd2-862a-4684-93d5-26d689ed1aad", "execution_plan": "step1: Set up the conda environment using the provided environment.yml file; step2: Download and set up the dataset as per EDM repository instructions; step3: Download the gradient sensitivity heatmaps for `pspc_flex` from the provided Google Drive link; step4: Run the denoising process using `denoise.py` with specified parameters; step5: Run the sampling process using `sample.py` with specified parameters; step6: Optionally, use provided noisy latents for denoising or sampling", "executed_cmds": "conda create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate pspc", "target_cmd": "conda activate pspc"}
{"uuid": "fa66c210-f2f5-4e97-9c72-ce6ae321934a", "execution_plan": "step1: Set up the required environment with core dependencies including pytorch, transformers, and flash-attn, along with other dependencies like rouge_score; step2: Run the retrieval head detection script to analyze the model's attention heads and generate retrieval scores, specifying the model path and sample range; step3: Optionally, adjust the sample range for quicker results or limited GPU resources; step4: Analyze the generated retrieval scores by loading the JSON file and computing average scores and rankings; step5: Run the Needle-in-a-Haystack experiment with masking to test the influence of retrieval heads, specifying the number of heads to mask and the sample range; step6: Visualize the results of the Needle-in-a-Haystack experiment using the provided notebook", "executed_cmds": "python retrieval_head_detection.py --model_path $path_to_model --s 0 --e 50000", "cmd_prefix": "python retrieval_head_detection.py --model_path $path_to_model", "cmd_postfix": "--s 0 --e 5000", "target_cmd": "python retrieval_head_detection.py --model_path $path_to_model --s 0 --e 5000"}
{"uuid": "7576f852-9ec5-4419-bdc0-7042a186e33a", "execution_plan": "step1: Install Python version 3.10 or higher; step2: Install Pytorch with the specified versions (torch==2.0.1, torchaudio==2.0.2, torchmetrics==1.3.2, torchvision==0.15.2); step3: Install Hydra for terminal prompt and configuration files parsing; step4: Install gdown and wget to download the ImageNet32 dataset; step5: Optionally install Tensorboard or wandb for tracking training experiments; step6: Reproduce static gradient analysis experiments (Figures 3 and 4 of the initial ArXiV release); step7: Reproduce splitting experiments with a convergence criterion (TOL) (Table 1 of the camera-ready version); step8: Reproduce splitting experiments with a fixed number of iterations (Table 1 of the initial ArXiV release); step9: Reproduce scaling experiments (Table 2 of the camera-ready version)", "executed_cmds": "pip install gdown", "cmd_prefix": "pip", "cmd_postfix": "install wget", "target_cmd": "pip install wget"}
{"uuid": "e4ff09e1-76b8-4e74-8aa9-caa0478a31d6", "execution_plan": "step1: Install PyTorch 2.2.0 with CUDA 12.1 for both WikiText-103 and Image Classification experiments; step2: Install flash-attention for WikiText-103 experiments; step3: Install other required packages for WikiText-103 experiments using the provided requirements file; step4: Navigate to the WikiText-103 directory and run the training script for the positive-eigenvalues scenario; step5: Navigate to the WikiText-103 directory and run the training script for the negative-eigenvalues scenario; step6: Navigate to the WikiText-103 directory and run the training script for the mixed-eigenvalue scenario; step7: Install other required libraries for Image Classification experiments using the provided requirements file; step8: Download ImageNet-1K dataset and update the data directory path in the training script; step9: Navigate to the Image Classification directory and run the training script", "executed_cmds": "pip install flash-attention;cd mamba_wt103 && mamba install -y --file requirements.txt;cd mamba_wt103 && CUDA_VISIBLE_DEVICES=0;1;2;3 python -m train experiment=wt103/mamba_pos;cd mamba_wt103 && CUDA_VISIBLE_DEVICES=0;1;2;3 python -m train experiment=wt103/mamba_neg;cd mamba_wt103 && CUDA_VISIBLE_DEVICES=0;1;2;3 python -m train experiment=wt103/mamba_real;cd mambavision_imagenet && mamba install -y --file requirements.txt", "cmd_prefix": "cd mambavision_imagenet/mambavision", "cmd_postfix": "&& bash train.sh", "target_cmd": "cd mambavision_imagenet/mambavision && bash train.sh"}
{"uuid": "0aa3ff5a-617e-4fe6-8206-d9c34ef30d2d", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia", "cmd_prefix": "using", "cmd_postfix": "Pkg; Pkg.add([\"JLD2\"", "target_cmd": "using Pkg; Pkg.add([\"JLD2\""}
{"uuid": "a1b0d019-692e-434a-98e8-1da7449ccba7", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\"", "cmd_prefix": "\"HiGHS\"]);", "cmd_postfix": "Pkg.add([\"Random\"", "target_cmd": "\"HiGHS\"]); Pkg.add([\"Random\""}
{"uuid": "e9732efe-825b-4875-9484-f09abcd29efc", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\"", "cmd_prefix": "\"Plots\"]);", "cmd_postfix": "Pkg.add([\"Pickle\"", "target_cmd": "\"Plots\"]); Pkg.add([\"Pickle\""}
{"uuid": "81836110-cce6-49a8-a750-19121fefaf01", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"])", "cmd_prefix": "cd", "cmd_postfix": "path_to_folder/code", "target_cmd": "cd path_to_folder/code"}
{"uuid": "b8f27d13-7af9-4f2d-9545-05ceaf40b871", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code", "cmd_prefix": "mkdir", "cmd_postfix": "experiments", "target_cmd": "mkdir experiments"}
{"uuid": "523fd4b4-7bed-4032-8b62-2eb669459d28", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments", "cmd_prefix": "mkdir", "cmd_postfix": "data", "target_cmd": "mkdir data"}
{"uuid": "b40d9011-3844-4ebb-9ecc-88e97868002e", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data", "cmd_prefix": "chmod", "cmd_postfix": "+x script.sh", "target_cmd": "chmod +x script.sh"}
{"uuid": "506d96bf-700c-4239-95e2-0f1160344222", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh", "cmd_prefix": "bash", "cmd_postfix": "script.sh 4", "target_cmd": "bash script.sh 4"}
{"uuid": "8d37d93d-d312-431d-9d3f-d67dc3266876", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh;bash script.sh 4;julia -O3 -p4 gaussian1d.jl --expe \"error1d\" --instance \"Gaussian\" --seed 42 --Nruns 1000 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 gaussianlarged.jl --expe \"errorlarged\" --instance \"Gaussian\" --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 gaussianvard.jl --expe \"errorvard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 1000 --Nsteps 10000;julia -O3 -p4 gapsD.jl --expe \"Matrices\" --instance \"Gaussian\" --dimMax 100 --seed 42 --Nruns 1000000;julia -O3 -p4 laplace.jl --expe \"error1d\" --instance \"Laplace\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 rayleigh.jl --expe \"error1d\" --instance \"Rayleigh\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstim.jl --expe \"betterEstim\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstimLargeD.jl --expe \"betterEstimlarged\" --instance \"Gaussian\" --seed 42 --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 betterEstimvard.jl --expe \"betterEstimVard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 100 --Nsteps 10000;julia -O3 -p4 losses.jl --expe \"losses\" --instance \"Gaussian\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 norm_and_regu.jl --expe \"normregu\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100", "cmd_prefix": "julia viz_gaussian1d.jl", "cmd_postfix": "--format \"svg\"", "target_cmd": "julia viz_gaussian1d.jl --format \"svg\""}
{"uuid": "b84fa449-a34a-4c2a-aba1-aead10beea82", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh;bash script.sh 4;julia -O3 -p4 gaussian1d.jl --expe \"error1d\" --instance \"Gaussian\" --seed 42 --Nruns 1000 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 gaussianlarged.jl --expe \"errorlarged\" --instance \"Gaussian\" --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 gaussianvard.jl --expe \"errorvard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 1000 --Nsteps 10000;julia -O3 -p4 gapsD.jl --expe \"Matrices\" --instance \"Gaussian\" --dimMax 100 --seed 42 --Nruns 1000000;julia -O3 -p4 laplace.jl --expe \"error1d\" --instance \"Laplace\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 rayleigh.jl --expe \"error1d\" --instance \"Rayleigh\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstim.jl --expe \"betterEstim\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstimLargeD.jl --expe \"betterEstimlarged\" --instance \"Gaussian\" --seed 42 --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 betterEstimvard.jl --expe \"betterEstimVard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 100 --Nsteps 10000;julia -O3 -p4 losses.jl --expe \"losses\" --instance \"Gaussian\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 norm_and_regu.jl --expe \"normregu\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia viz_gaussian1d.jl --format \"svg\"", "cmd_prefix": "julia viz_gaussianlarged.jl", "cmd_postfix": "--format \"svg\"", "target_cmd": "julia viz_gaussianlarged.jl --format \"svg\""}
{"uuid": "16505878-918d-491b-8497-a4e2695000e3", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh;bash script.sh 4;julia -O3 -p4 gaussian1d.jl --expe \"error1d\" --instance \"Gaussian\" --seed 42 --Nruns 1000 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 gaussianlarged.jl --expe \"errorlarged\" --instance \"Gaussian\" --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 gaussianvard.jl --expe \"errorvard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 1000 --Nsteps 10000;julia -O3 -p4 gapsD.jl --expe \"Matrices\" --instance \"Gaussian\" --dimMax 100 --seed 42 --Nruns 1000000;julia -O3 -p4 laplace.jl --expe \"error1d\" --instance \"Laplace\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 rayleigh.jl --expe \"error1d\" --instance \"Rayleigh\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstim.jl --expe \"betterEstim\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstimLargeD.jl --expe \"betterEstimlarged\" --instance \"Gaussian\" --seed 42 --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 betterEstimvard.jl --expe \"betterEstimVard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 100 --Nsteps 10000;julia -O3 -p4 losses.jl --expe \"losses\" --instance \"Gaussian\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 norm_and_regu.jl --expe \"normregu\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia viz_gaussian1d.jl --format \"svg\";julia viz_gaussianlarged.jl --format \"svg\"", "cmd_prefix": "julia viz_gaussianvard.jl", "cmd_postfix": "--format \"svg\"", "target_cmd": "julia viz_gaussianvard.jl --format \"svg\""}
{"uuid": "4b1d1a64-3260-4b58-a3e3-ec6a866c93e9", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh;bash script.sh 4;julia -O3 -p4 gaussian1d.jl --expe \"error1d\" --instance \"Gaussian\" --seed 42 --Nruns 1000 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 gaussianlarged.jl --expe \"errorlarged\" --instance \"Gaussian\" --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 gaussianvard.jl --expe \"errorvard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 1000 --Nsteps 10000;julia -O3 -p4 gapsD.jl --expe \"Matrices\" --instance \"Gaussian\" --dimMax 100 --seed 42 --Nruns 1000000;julia -O3 -p4 laplace.jl --expe \"error1d\" --instance \"Laplace\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 rayleigh.jl --expe \"error1d\" --instance \"Rayleigh\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstim.jl --expe \"betterEstim\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstimLargeD.jl --expe \"betterEstimlarged\" --instance \"Gaussian\" --seed 42 --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 betterEstimvard.jl --expe \"betterEstimVard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 100 --Nsteps 10000;julia -O3 -p4 losses.jl --expe \"losses\" --instance \"Gaussian\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 norm_and_regu.jl --expe \"normregu\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia viz_gaussian1d.jl --format \"svg\";julia viz_gaussianlarged.jl --format \"svg\";julia viz_gaussianvard.jl --format \"svg\"", "cmd_prefix": "julia viz_gapsD.jl", "cmd_postfix": "--format \"svg\"", "target_cmd": "julia viz_gapsD.jl --format \"svg\""}
{"uuid": "daf0918e-e148-4f03-b9d7-9bf910652de5", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh;bash script.sh 4;julia -O3 -p4 gaussian1d.jl --expe \"error1d\" --instance \"Gaussian\" --seed 42 --Nruns 1000 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 gaussianlarged.jl --expe \"errorlarged\" --instance \"Gaussian\" --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 gaussianvard.jl --expe \"errorvard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 1000 --Nsteps 10000;julia -O3 -p4 gapsD.jl --expe \"Matrices\" --instance \"Gaussian\" --dimMax 100 --seed 42 --Nruns 1000000;julia -O3 -p4 laplace.jl --expe \"error1d\" --instance \"Laplace\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 rayleigh.jl --expe \"error1d\" --instance \"Rayleigh\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstim.jl --expe \"betterEstim\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstimLargeD.jl --expe \"betterEstimlarged\" --instance \"Gaussian\" --seed 42 --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 betterEstimvard.jl --expe \"betterEstimVard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 100 --Nsteps 10000;julia -O3 -p4 losses.jl --expe \"losses\" --instance \"Gaussian\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 norm_and_regu.jl --expe \"normregu\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia viz_gaussian1d.jl --format \"svg\";julia viz_gaussianlarged.jl --format \"svg\";julia viz_gaussianvard.jl --format \"svg\";julia viz_gapsD.jl --format \"svg\"", "cmd_prefix": "julia viz_laplace.jl", "cmd_postfix": "--format \"svg\"", "target_cmd": "julia viz_laplace.jl --format \"svg\""}
{"uuid": "fa05f3af-ce5e-4084-8034-fd2e4620e233", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh;bash script.sh 4;julia -O3 -p4 gaussian1d.jl --expe \"error1d\" --instance \"Gaussian\" --seed 42 --Nruns 1000 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 gaussianlarged.jl --expe \"errorlarged\" --instance \"Gaussian\" --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 gaussianvard.jl --expe \"errorvard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 1000 --Nsteps 10000;julia -O3 -p4 gapsD.jl --expe \"Matrices\" --instance \"Gaussian\" --dimMax 100 --seed 42 --Nruns 1000000;julia -O3 -p4 laplace.jl --expe \"error1d\" --instance \"Laplace\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 rayleigh.jl --expe \"error1d\" --instance \"Rayleigh\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstim.jl --expe \"betterEstim\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstimLargeD.jl --expe \"betterEstimlarged\" --instance \"Gaussian\" --seed 42 --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 betterEstimvard.jl --expe \"betterEstimVard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 100 --Nsteps 10000;julia -O3 -p4 losses.jl --expe \"losses\" --instance \"Gaussian\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 norm_and_regu.jl --expe \"normregu\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia viz_gaussian1d.jl --format \"svg\";julia viz_gaussianlarged.jl --format \"svg\";julia viz_gaussianvard.jl --format \"svg\";julia viz_gapsD.jl --format \"svg\";julia viz_laplace.jl --format \"svg\"", "cmd_prefix": "julia viz_rayleigh.jl", "cmd_postfix": "--format \"svg\"", "target_cmd": "julia viz_rayleigh.jl --format \"svg\""}
{"uuid": "bc4f2f94-8030-4e26-b197-3944db55f9f7", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh;bash script.sh 4;julia -O3 -p4 gaussian1d.jl --expe \"error1d\" --instance \"Gaussian\" --seed 42 --Nruns 1000 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 gaussianlarged.jl --expe \"errorlarged\" --instance \"Gaussian\" --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 gaussianvard.jl --expe \"errorvard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 1000 --Nsteps 10000;julia -O3 -p4 gapsD.jl --expe \"Matrices\" --instance \"Gaussian\" --dimMax 100 --seed 42 --Nruns 1000000;julia -O3 -p4 laplace.jl --expe \"error1d\" --instance \"Laplace\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 rayleigh.jl --expe \"error1d\" --instance \"Rayleigh\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstim.jl --expe \"betterEstim\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstimLargeD.jl --expe \"betterEstimlarged\" --instance \"Gaussian\" --seed 42 --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 betterEstimvard.jl --expe \"betterEstimVard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 100 --Nsteps 10000;julia -O3 -p4 losses.jl --expe \"losses\" --instance \"Gaussian\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 norm_and_regu.jl --expe \"normregu\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia viz_gaussian1d.jl --format \"svg\";julia viz_gaussianlarged.jl --format \"svg\";julia viz_gaussianvard.jl --format \"svg\";julia viz_gapsD.jl --format \"svg\";julia viz_laplace.jl --format \"svg\";julia viz_rayleigh.jl --format \"svg\"", "cmd_prefix": "julia viz_betterEstim.jl", "cmd_postfix": "--format \"svg\"", "target_cmd": "julia viz_betterEstim.jl --format \"svg\""}
{"uuid": "0635b4de-d380-4ad1-9b21-0c5f8c5d3478", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh;bash script.sh 4;julia -O3 -p4 gaussian1d.jl --expe \"error1d\" --instance \"Gaussian\" --seed 42 --Nruns 1000 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 gaussianlarged.jl --expe \"errorlarged\" --instance \"Gaussian\" --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 gaussianvard.jl --expe \"errorvard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 1000 --Nsteps 10000;julia -O3 -p4 gapsD.jl --expe \"Matrices\" --instance \"Gaussian\" --dimMax 100 --seed 42 --Nruns 1000000;julia -O3 -p4 laplace.jl --expe \"error1d\" --instance \"Laplace\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 rayleigh.jl --expe \"error1d\" --instance \"Rayleigh\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstim.jl --expe \"betterEstim\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstimLargeD.jl --expe \"betterEstimlarged\" --instance \"Gaussian\" --seed 42 --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 betterEstimvard.jl --expe \"betterEstimVard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 100 --Nsteps 10000;julia -O3 -p4 losses.jl --expe \"losses\" --instance \"Gaussian\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 norm_and_regu.jl --expe \"normregu\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia viz_gaussian1d.jl --format \"svg\";julia viz_gaussianlarged.jl --format \"svg\";julia viz_gaussianvard.jl --format \"svg\";julia viz_gapsD.jl --format \"svg\";julia viz_laplace.jl --format \"svg\";julia viz_rayleigh.jl --format \"svg\";julia viz_betterEstim.jl --format \"svg\"", "cmd_prefix": "julia viz_betterEstimLarged.jl", "cmd_postfix": "--format \"svg\"", "target_cmd": "julia viz_betterEstimLarged.jl --format \"svg\""}
{"uuid": "1a06e6cb-a9ab-4be0-8cdc-603161d777eb", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh;bash script.sh 4;julia -O3 -p4 gaussian1d.jl --expe \"error1d\" --instance \"Gaussian\" --seed 42 --Nruns 1000 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 gaussianlarged.jl --expe \"errorlarged\" --instance \"Gaussian\" --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 gaussianvard.jl --expe \"errorvard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 1000 --Nsteps 10000;julia -O3 -p4 gapsD.jl --expe \"Matrices\" --instance \"Gaussian\" --dimMax 100 --seed 42 --Nruns 1000000;julia -O3 -p4 laplace.jl --expe \"error1d\" --instance \"Laplace\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 rayleigh.jl --expe \"error1d\" --instance \"Rayleigh\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstim.jl --expe \"betterEstim\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstimLargeD.jl --expe \"betterEstimlarged\" --instance \"Gaussian\" --seed 42 --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 betterEstimvard.jl --expe \"betterEstimVard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 100 --Nsteps 10000;julia -O3 -p4 losses.jl --expe \"losses\" --instance \"Gaussian\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 norm_and_regu.jl --expe \"normregu\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia viz_gaussian1d.jl --format \"svg\";julia viz_gaussianlarged.jl --format \"svg\";julia viz_gaussianvard.jl --format \"svg\";julia viz_gapsD.jl --format \"svg\";julia viz_laplace.jl --format \"svg\";julia viz_rayleigh.jl --format \"svg\";julia viz_betterEstim.jl --format \"svg\";julia viz_betterEstimLarged.jl --format \"svg\"", "cmd_prefix": "julia viz_BetterEstimVard.jl", "cmd_postfix": "--format \"svg\"", "target_cmd": "julia viz_BetterEstimVard.jl --format \"svg\""}
{"uuid": "d0c2055b-37d0-4bf4-b1df-99319683c8a2", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh;bash script.sh 4;julia -O3 -p4 gaussian1d.jl --expe \"error1d\" --instance \"Gaussian\" --seed 42 --Nruns 1000 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 gaussianlarged.jl --expe \"errorlarged\" --instance \"Gaussian\" --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 gaussianvard.jl --expe \"errorvard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 1000 --Nsteps 10000;julia -O3 -p4 gapsD.jl --expe \"Matrices\" --instance \"Gaussian\" --dimMax 100 --seed 42 --Nruns 1000000;julia -O3 -p4 laplace.jl --expe \"error1d\" --instance \"Laplace\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 rayleigh.jl --expe \"error1d\" --instance \"Rayleigh\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstim.jl --expe \"betterEstim\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstimLargeD.jl --expe \"betterEstimlarged\" --instance \"Gaussian\" --seed 42 --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 betterEstimvard.jl --expe \"betterEstimVard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 100 --Nsteps 10000;julia -O3 -p4 losses.jl --expe \"losses\" --instance \"Gaussian\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 norm_and_regu.jl --expe \"normregu\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia viz_gaussian1d.jl --format \"svg\";julia viz_gaussianlarged.jl --format \"svg\";julia viz_gaussianvard.jl --format \"svg\";julia viz_gapsD.jl --format \"svg\";julia viz_laplace.jl --format \"svg\";julia viz_rayleigh.jl --format \"svg\";julia viz_betterEstim.jl --format \"svg\";julia viz_betterEstimLarged.jl --format \"svg\";julia viz_BetterEstimVard.jl --format \"svg\"", "cmd_prefix": "julia viz_losses.jl", "cmd_postfix": "--format \"svg\"", "target_cmd": "julia viz_losses.jl --format \"svg\""}
{"uuid": "4fd365a0-cf1e-45f4-b738-018a26eebed6", "execution_plan": "step1: Install Julia on your system; step2: Install required Julia packages; step3: Set up the project directory by creating necessary folders; step4: Run all experiments at once using the provided script; step5: Run individual experiments one at a time for specific figures; step6: Generate visualizations for each experiment", "executed_cmds": "wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.3-linux-x86_64.tar.gz;tar zxvf julia-1.11.3-linux-x86_64.tar.gz;export PATH=\"$PATH:/path/to/<Julia directory>/bin\";julia;using Pkg; Pkg.add([\"JLD2\";\"Printf\";\"JSON\";\"Dates\";\"IterTools\";\"Distributed\";\"JuMP\";\"Ipopt\";\"HiGHS\"]); Pkg.add([\"Random\";\"LinearAlgebra\";\"Distributions\";\"CPUTime\"]); Pkg.add([\"StatsPlots\";\"ArgParse\";\"Statistics\";\"StatsBase\";\"Plots\"]); Pkg.add([\"Pickle\";\"ColorSchemes\";\"Distributed\";\"LaTeXStrings\"]);cd path_to_folder/code;mkdir experiments;mkdir data;chmod +x script.sh;bash script.sh 4;julia -O3 -p4 gaussian1d.jl --expe \"error1d\" --instance \"Gaussian\" --seed 42 --Nruns 1000 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 gaussianlarged.jl --expe \"errorlarged\" --instance \"Gaussian\" --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 gaussianvard.jl --expe \"errorvard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 1000 --Nsteps 10000;julia -O3 -p4 gapsD.jl --expe \"Matrices\" --instance \"Gaussian\" --dimMax 100 --seed 42 --Nruns 1000000;julia -O3 -p4 laplace.jl --expe \"error1d\" --instance \"Laplace\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 rayleigh.jl --expe \"error1d\" --instance \"Rayleigh\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstim.jl --expe \"betterEstim\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 betterEstimLargeD.jl --expe \"betterEstimlarged\" --instance \"Gaussian\" --seed 42 --dimension 20 --seed 42 --Nruns 100 --Nsteps 10000 --batch 400 --sizemax 25;julia -O3 -p4 betterEstimvard.jl --expe \"betterEstimVard\" --instance \"Gaussian\" --dimMax 100 --dimStep 10 --seed 42 --Nruns 100 --Nsteps 10000;julia -O3 -p4 losses.jl --expe \"losses\" --instance \"Gaussian\" --seed 42 --Nruns 10 --Nsteps 10000 --batch 100 --sizemax 100;julia -O3 -p4 norm_and_regu.jl --expe \"normregu\" --instance \"Gaussian\" --seed 42 --Nruns 100 --Nsteps 10000 --batch 100 --sizemax 100;julia viz_gaussian1d.jl --format \"svg\";julia viz_gaussianlarged.jl --format \"svg\";julia viz_gaussianvard.jl --format \"svg\";julia viz_gapsD.jl --format \"svg\";julia viz_laplace.jl --format \"svg\";julia viz_rayleigh.jl --format \"svg\";julia viz_betterEstim.jl --format \"svg\";julia viz_betterEstimLarged.jl --format \"svg\";julia viz_BetterEstimVard.jl --format \"svg\";julia viz_losses.jl --format \"svg\"", "cmd_prefix": "julia viz_normregu.jl", "cmd_postfix": "--format \"svg\"", "target_cmd": "julia viz_normregu.jl --format \"svg\""}
{"uuid": "58a5564d-8371-4760-92b6-d6d28c2fa587", "execution_plan": "step1: Clone the repository and set up the environment by installing the required dependencies; step2: Review the available datasets in the data/ directory to understand their structure and content; step3: Run the entire BIRD pipeline using the provided script; step4: Run the baseline models using the provided script; step5: Run the evaluation script to assess the results; step6: Cite the project if it is found helpful", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "bash", "cmd_postfix": "scripts/run_bird.sh", "target_cmd": "bash scripts/run_bird.sh"}
{"uuid": "9b976e13-6e2c-48c3-b66c-c1b7500e2d9e", "execution_plan": "step1: Clone the repository and set up the environment by installing the required dependencies; step2: Review the available datasets in the data/ directory to understand their structure and content; step3: Run the entire BIRD pipeline using the provided script; step4: Run the baseline models using the provided script; step5: Run the evaluation script to assess the results; step6: Cite the project if it is found helpful", "executed_cmds": "pip install -r requirements.txt;bash scripts/run_bird.sh", "cmd_prefix": "bash", "cmd_postfix": "scripts/baseline.sh", "target_cmd": "bash scripts/baseline.sh"}
{"uuid": "7be1d784-2484-4367-9d24-f19049c29713", "execution_plan": "step1: Clone the repository and set up the environment by installing the required dependencies; step2: Review the available datasets in the data/ directory to understand their structure and content; step3: Run the entire BIRD pipeline using the provided script; step4: Run the baseline models using the provided script; step5: Run the evaluation script to assess the results; step6: Cite the project if it is found helpful", "executed_cmds": "pip install -r requirements.txt;bash scripts/run_bird.sh;bash scripts/baseline.sh", "cmd_prefix": "bash", "cmd_postfix": "scripts/eval.sh", "target_cmd": "bash scripts/eval.sh"}
{"uuid": "94a6544a-d196-47c6-b292-7a831c62f54a", "execution_plan": "step1: Create a new conda environment and install CUDA dependencies; step2: Install PyTorch and other required packages; step3: Install mmsegmentation for segmentation tasks; step4: Overwrite some  Diffusers files with the provided files in the repository; step5: Install the codebase as a package; step6: Download diffusion models or modify the model paths in the configuration file; step7: Use the feature extractor in your project or run the standalone script to extract features; step8: Optionally, use the background extraction feature or the visualization tool", "executed_cmds": "conda create -n generic-diffusion-feature python=3.9;conda activate generic-diffusion-feature;conda install nvidia/label/cuda-11.8.0::cuda;conda install nvidia/label/cuda-11.8.0::cuda-cudart;conda install nvidia/label/cuda-11.8.0::libcusparse;conda install nvidia/label/cuda-11.8.0::libcublas;pip3 install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118;pip3 install diffusers[\"torch\"]==0.32.2 transformers==4.49.0 controlnet_aux bitsandbytes==0.45.3;pip3 install tqdm tensorboard flake8 ipykernel pytest seaborn sentencepiece beautifulsoup4;pip3 install xformers==0.0.29.post2 --index-url https://download.pytorch.org/whl/cu118;pip3 install accelerate ipdb pytest-env wandb;pip3 install invisible-watermark>=0.2.0", "cmd_prefix": "pip3", "cmd_postfix": "install blobfile", "target_cmd": "pip3 install blobfile"}
{"uuid": "fdc93062-dff6-4055-b898-9bdcc62a7bd6", "execution_plan": "step1: Create a new conda environment and install CUDA dependencies; step2: Install PyTorch and other required packages; step3: Install mmsegmentation for segmentation tasks; step4: Overwrite some  Diffusers files with the provided files in the repository; step5: Install the codebase as a package; step6: Download diffusion models or modify the model paths in the configuration file; step7: Use the feature extractor in your project or run the standalone script to extract features; step8: Optionally, use the background extraction feature or the visualization tool", "executed_cmds": "conda create -n generic-diffusion-feature python=3.9;conda activate generic-diffusion-feature;conda install nvidia/label/cuda-11.8.0::cuda;conda install nvidia/label/cuda-11.8.0::cuda-cudart;conda install nvidia/label/cuda-11.8.0::libcusparse;conda install nvidia/label/cuda-11.8.0::libcublas;pip3 install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118;pip3 install diffusers[\"torch\"]==0.32.2 transformers==4.49.0 controlnet_aux bitsandbytes==0.45.3;pip3 install tqdm tensorboard flake8 ipykernel pytest seaborn sentencepiece beautifulsoup4;pip3 install xformers==0.0.29.post2 --index-url https://download.pytorch.org/whl/cu118;pip3 install accelerate ipdb pytest-env wandb;pip3 install invisible-watermark>=0.2.0;pip3 install blobfile;pip3 install opencv-python", "cmd_prefix": "pip3", "cmd_postfix": "install omegaconf", "target_cmd": "pip3 install omegaconf"}
{"uuid": "21fba550-d631-45ff-9880-0dc4a96234e5", "execution_plan": "step1: Create a new conda environment and install CUDA dependencies; step2: Install PyTorch and other required packages; step3: Install mmsegmentation for segmentation tasks; step4: Overwrite some  Diffusers files with the provided files in the repository; step5: Install the codebase as a package; step6: Download diffusion models or modify the model paths in the configuration file; step7: Use the feature extractor in your project or run the standalone script to extract features; step8: Optionally, use the background extraction feature or the visualization tool", "executed_cmds": "conda create -n generic-diffusion-feature python=3.9;conda activate generic-diffusion-feature;conda install nvidia/label/cuda-11.8.0::cuda;conda install nvidia/label/cuda-11.8.0::cuda-cudart;conda install nvidia/label/cuda-11.8.0::libcusparse;conda install nvidia/label/cuda-11.8.0::libcublas;pip3 install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118;pip3 install diffusers[\"torch\"]==0.32.2 transformers==4.49.0 controlnet_aux bitsandbytes==0.45.3;pip3 install tqdm tensorboard flake8 ipykernel pytest seaborn sentencepiece beautifulsoup4;pip3 install xformers==0.0.29.post2 --index-url https://download.pytorch.org/whl/cu118;pip3 install accelerate ipdb pytest-env wandb;pip3 install invisible-watermark>=0.2.0;pip3 install blobfile;pip3 install opencv-python;pip3 install omegaconf;pip3 install scikit-learn==1.0.2;pip3 install mmcv==2.1.0 mmsegmentation==1.2.2 ftfy", "cmd_prefix": "cd feature &", "cmd_postfix": "pip3 install -e .", "target_cmd": "cd feature & pip3 install -e ."}
{"uuid": "1774ec8b-9b51-42de-89e7-a6d3c96fbdea", "execution_plan": "step1: Download necessary datasets and pretrained weights; step2: Initialize Conda environment and clone the repository; step3: Install modified BasicSR; step4: Install CORUN-Colabator and initialize modules; step5: Pretrain on synthetic data (optional); step6: Fine-tune with Colabator on real degraded data; step7: Test the CORUN model; step8: Evaluate the results using NIMA and BRISQUE metrics", "executed_cmds": "git clone https://github.com/cnyvfang/CORUN-Colabator.git;conda create -n corun_colabator python=3.9;conda activate corun_colabator;conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia", "cmd_prefix": "cd", "cmd_postfix": "basicsr_modified", "target_cmd": "cd basicsr_modified"}
{"uuid": "6896e70e-c8ec-453a-a975-a91c0a4d95ba", "execution_plan": "step1: Download necessary datasets and pretrained weights; step2: Initialize Conda environment and clone the repository; step3: Install modified BasicSR; step4: Install CORUN-Colabator and initialize modules; step5: Pretrain on synthetic data (optional); step6: Fine-tune with Colabator on real degraded data; step7: Test the CORUN model; step8: Evaluate the results using NIMA and BRISQUE metrics", "executed_cmds": "git clone https://github.com/cnyvfang/CORUN-Colabator.git;conda create -n corun_colabator python=3.9;conda activate corun_colabator;conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia;cd basicsr_modified;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "abe6b982-a7cb-4275-bfae-ac1d4581d9e0", "execution_plan": "step1: Download necessary datasets and pretrained weights; step2: Initialize Conda environment and clone the repository; step3: Install modified BasicSR; step4: Install CORUN-Colabator and initialize modules; step5: Pretrain on synthetic data (optional); step6: Fine-tune with Colabator on real degraded data; step7: Test the CORUN model; step8: Evaluate the results using NIMA and BRISQUE metrics", "executed_cmds": "git clone https://github.com/cnyvfang/CORUN-Colabator.git;conda create -n corun_colabator python=3.9;conda activate corun_colabator;conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia;cd basicsr_modified;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "setup.py develop", "target_cmd": "python setup.py develop"}
{"uuid": "0b5dee1a-85d3-401d-9e1a-4921afc12503", "execution_plan": "step1: Download necessary datasets and pretrained weights; step2: Initialize Conda environment and clone the repository; step3: Install modified BasicSR; step4: Install CORUN-Colabator and initialize modules; step5: Pretrain on synthetic data (optional); step6: Fine-tune with Colabator on real degraded data; step7: Test the CORUN model; step8: Evaluate the results using NIMA and BRISQUE metrics", "executed_cmds": "git clone https://github.com/cnyvfang/CORUN-Colabator.git;conda create -n corun_colabator python=3.9;conda activate corun_colabator;conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia;cd basicsr_modified;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "6498700d-a9e8-4782-a051-56064c48998b", "execution_plan": "step1: Download necessary datasets and pretrained weights; step2: Initialize Conda environment and clone the repository; step3: Install modified BasicSR; step4: Install CORUN-Colabator and initialize modules; step5: Pretrain on synthetic data (optional); step6: Fine-tune with Colabator on real degraded data; step7: Test the CORUN model; step8: Evaluate the results using NIMA and BRISQUE metrics", "executed_cmds": "git clone https://github.com/cnyvfang/CORUN-Colabator.git;conda create -n corun_colabator python=3.9;conda activate corun_colabator;conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia;cd basicsr_modified;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "837d78f4-ad2f-4058-87f9-9d307730c2a9", "execution_plan": "step1: Download necessary datasets and pretrained weights; step2: Initialize Conda environment and clone the repository; step3: Install modified BasicSR; step4: Install CORUN-Colabator and initialize modules; step5: Pretrain on synthetic data (optional); step6: Fine-tune with Colabator on real degraded data; step7: Test the CORUN model; step8: Evaluate the results using NIMA and BRISQUE metrics", "executed_cmds": "git clone https://github.com/cnyvfang/CORUN-Colabator.git;conda create -n corun_colabator python=3.9;conda activate corun_colabator;conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia;cd basicsr_modified;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "setup.py develop", "target_cmd": "python setup.py develop"}
{"uuid": "47ad7a14-07f4-4ac5-bef1-0bfc1ee1c91f", "execution_plan": "step1: Download necessary datasets and pretrained weights; step2: Initialize Conda environment and clone the repository; step3: Install modified BasicSR; step4: Install CORUN-Colabator and initialize modules; step5: Pretrain on synthetic data (optional); step6: Fine-tune with Colabator on real degraded data; step7: Test the CORUN model; step8: Evaluate the results using NIMA and BRISQUE metrics", "executed_cmds": "git clone https://github.com/cnyvfang/CORUN-Colabator.git;conda create -n corun_colabator python=3.9;conda activate corun_colabator;conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia;cd basicsr_modified;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;pip install -r requirements.txt;python setup.py develop", "cmd_prefix": "python", "cmd_postfix": "init_modules.py", "target_cmd": "python init_modules.py"}
{"uuid": "a36705ea-e4a0-41a6-b1da-155d34b0268a", "execution_plan": "step1: Setup the nuPlan dataset following the official documentation; step2: Create and activate a conda environment for the project; step3: Install the nuplan-devkit and its dependencies; step4: Clone the Diffusion-Planner repository and install its dependencies; step5: Download the model checkpoint from Huggingface; step6: Run the simulation for closed-loop evaluation; step7: Visualize the results using Jupyter Notebook; step8: Run the classifier guidance demo; step9: Preprocess the training data; step10: Run the training code", "executed_cmds": "conda create -n diffusion_planner python=3.9;conda activate diffusion_planner", "cmd_prefix": "git clone https://github.com/motional/nuplan-devkit.git", "cmd_postfix": "&& cd nuplan-devkit", "target_cmd": "git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit"}
{"uuid": "c98c8aae-e42d-40ca-ab89-cf8a53c89514", "execution_plan": "step1: Setup the nuPlan dataset following the official documentation; step2: Create and activate a conda environment for the project; step3: Install the nuplan-devkit and its dependencies; step4: Clone the Diffusion-Planner repository and install its dependencies; step5: Download the model checkpoint from Huggingface; step6: Run the simulation for closed-loop evaluation; step7: Visualize the results using Jupyter Notebook; step8: Run the classifier guidance demo; step9: Preprocess the training data; step10: Run the training code", "executed_cmds": "conda create -n diffusion_planner python=3.9;conda activate diffusion_planner;git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "792d0662-5f3c-48f6-85be-b63ae6f1a690", "execution_plan": "step1: Setup the nuPlan dataset following the official documentation; step2: Create and activate a conda environment for the project; step3: Install the nuplan-devkit and its dependencies; step4: Clone the Diffusion-Planner repository and install its dependencies; step5: Download the model checkpoint from Huggingface; step6: Run the simulation for closed-loop evaluation; step7: Visualize the results using Jupyter Notebook; step8: Run the classifier guidance demo; step9: Preprocess the training data; step10: Run the training code", "executed_cmds": "conda create -n diffusion_planner python=3.9;conda activate diffusion_planner;git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit;pip install -e .", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "acbb8e77-de9a-4500-aec5-2a335330a766", "execution_plan": "step1: Setup the nuPlan dataset following the official documentation; step2: Create and activate a conda environment for the project; step3: Install the nuplan-devkit and its dependencies; step4: Clone the Diffusion-Planner repository and install its dependencies; step5: Download the model checkpoint from Huggingface; step6: Run the simulation for closed-loop evaluation; step7: Visualize the results using Jupyter Notebook; step8: Run the classifier guidance demo; step9: Preprocess the training data; step10: Run the training code", "executed_cmds": "conda create -n diffusion_planner python=3.9;conda activate diffusion_planner;git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit;pip install -e .;pip install -r requirements.txt", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "c7d553ff-fb39-424b-b6b4-a2e7db87d65b", "execution_plan": "step1: Setup the nuPlan dataset following the official documentation; step2: Create and activate a conda environment for the project; step3: Install the nuplan-devkit and its dependencies; step4: Clone the Diffusion-Planner repository and install its dependencies; step5: Download the model checkpoint from Huggingface; step6: Run the simulation for closed-loop evaluation; step7: Visualize the results using Jupyter Notebook; step8: Run the classifier guidance demo; step9: Preprocess the training data; step10: Run the training code", "executed_cmds": "conda create -n diffusion_planner python=3.9;conda activate diffusion_planner;git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit;pip install -e .;pip install -r requirements.txt;cd ..;git clone https://github.com/ZhengYinan-AIR/Diffusion-Planner.git && cd Diffusion-Planner", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "07606ddb-46b3-47d4-8701-82c5df763c0d", "execution_plan": "step1: Setup the nuPlan dataset following the official documentation; step2: Create and activate a conda environment for the project; step3: Install the nuplan-devkit and its dependencies; step4: Clone the Diffusion-Planner repository and install its dependencies; step5: Download the model checkpoint from Huggingface; step6: Run the simulation for closed-loop evaluation; step7: Visualize the results using Jupyter Notebook; step8: Run the classifier guidance demo; step9: Preprocess the training data; step10: Run the training code", "executed_cmds": "conda create -n diffusion_planner python=3.9;conda activate diffusion_planner;git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit;pip install -e .;pip install -r requirements.txt;cd ..;git clone https://github.com/ZhengYinan-AIR/Diffusion-Planner.git && cd Diffusion-Planner;pip install -e .;pip install -r requirements_torch.txt", "cmd_prefix": "mkdir", "cmd_postfix": "-p checkpoints", "target_cmd": "mkdir -p checkpoints"}
{"uuid": "8d3e34d6-a0ee-4869-ab13-9e007226b961", "execution_plan": "step1: Setup the nuPlan dataset following the official documentation; step2: Create and activate a conda environment for the project; step3: Install the nuplan-devkit and its dependencies; step4: Clone the Diffusion-Planner repository and install its dependencies; step5: Download the model checkpoint from Huggingface; step6: Run the simulation for closed-loop evaluation; step7: Visualize the results using Jupyter Notebook; step8: Run the classifier guidance demo; step9: Preprocess the training data; step10: Run the training code", "executed_cmds": "conda create -n diffusion_planner python=3.9;conda activate diffusion_planner;git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit;pip install -e .;pip install -r requirements.txt;cd ..;git clone https://github.com/ZhengYinan-AIR/Diffusion-Planner.git && cd Diffusion-Planner;pip install -e .;pip install -r requirements_torch.txt;mkdir -p checkpoints;wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/args.json;wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/model.pth;bash sim_diffusion_planner_runner.sh", "cmd_prefix": "bash", "cmd_postfix": "sim_guidance_demo.sh", "target_cmd": "bash sim_guidance_demo.sh"}
{"uuid": "7bebd07b-3b07-401c-a95a-dc25d6450b21", "execution_plan": "step1: Setup the nuPlan dataset following the official documentation; step2: Create and activate a conda environment for the project; step3: Install the nuplan-devkit and its dependencies; step4: Clone the Diffusion-Planner repository and install its dependencies; step5: Download the model checkpoint from Huggingface; step6: Run the simulation for closed-loop evaluation; step7: Visualize the results using Jupyter Notebook; step8: Run the classifier guidance demo; step9: Preprocess the training data; step10: Run the training code", "executed_cmds": "conda create -n diffusion_planner python=3.9;conda activate diffusion_planner;git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit;pip install -e .;pip install -r requirements.txt;cd ..;git clone https://github.com/ZhengYinan-AIR/Diffusion-Planner.git && cd Diffusion-Planner;pip install -e .;pip install -r requirements_torch.txt;mkdir -p checkpoints;wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/args.json;wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/model.pth;bash sim_diffusion_planner_runner.sh;bash sim_guidance_demo.sh", "cmd_prefix": "chmod", "cmd_postfix": "+x data_process.sh", "target_cmd": "chmod +x data_process.sh"}
{"uuid": "9c0a35de-d508-4a1e-a418-715dcc5f2c37", "execution_plan": "step1: Setup the nuPlan dataset following the official documentation; step2: Create and activate a conda environment for the project; step3: Install the nuplan-devkit and its dependencies; step4: Clone the Diffusion-Planner repository and install its dependencies; step5: Download the model checkpoint from Huggingface; step6: Run the simulation for closed-loop evaluation; step7: Visualize the results using Jupyter Notebook; step8: Run the classifier guidance demo; step9: Preprocess the training data; step10: Run the training code", "executed_cmds": "conda create -n diffusion_planner python=3.9;conda activate diffusion_planner;git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit;pip install -e .;pip install -r requirements.txt;cd ..;git clone https://github.com/ZhengYinan-AIR/Diffusion-Planner.git && cd Diffusion-Planner;pip install -e .;pip install -r requirements_torch.txt;mkdir -p checkpoints;wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/args.json;wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/model.pth;bash sim_diffusion_planner_runner.sh;bash sim_guidance_demo.sh;chmod +x data_process.sh;./data_process.sh", "cmd_prefix": "chmod", "cmd_postfix": "+x torch_run.sh", "target_cmd": "chmod +x torch_run.sh"}
{"uuid": "ca101e98-2645-4c95-b7b7-909d09698e7b", "execution_plan": "step1: Clone the LightGaussian repository and initialize submodules; step2: Set up the conda environment using the provided environment.yml file; step3: Activate the conda environment named 'lightgaussian'; step4: Prune a trained 3D-GS checkpoint or train from scratch and prune redundant Gaussians; step5: Distill a 3D-GS checkpoint to improve compactness; step6: Quantize a pruned and distilled 3D-GS checkpoint using VecTree Quantization; step7: Render the compressed 3D Gaussian model with a specified trajectory; step8: Render the model after VecTree Quantization stage", "executed_cmds": "git clone --recursive https://github.com/VITA-Group/LightGaussian.git", "cmd_prefix": "cd", "cmd_postfix": "LightGaussian", "target_cmd": "cd LightGaussian"}
{"uuid": "a717463f-da25-4a4e-8b15-65334d3c1a2d", "execution_plan": "step1: Install the required Python packages and dependencies to set up the environment; step2: Disable Weights & Biases (wandb) tracking if not needed; step3: Train a model using the provided `train.py` script with specified parameters, such as pruning class, network, dataset, and other hyperparameters; step4: Run post-training pruning and generate compression vs. accuracy statistics using `compression_vs_accuracy_separate.py`; step5: Generate plots for accuracy vs. compression using `generate_plots_accuracy_vs_compression.py`; step6: Generate LaTeX tables from the paper using `analysis/generate_latex_tables.py` after training models and obtaining performance metrics", "executed_cmds": "pip install carbontracker==1.2.3 datasets==2.14.6 matplotlib numpy==1.26.4 pandas==1.3.5 scikit-learn seaborn==0.12.0 tokenizers==0.14.1 torch==2.2.0 torchvision==0.16.0 tqdm==4.56.2 transformers==4.34.1 wandb==0.15.12", "cmd_prefix": "wandb", "cmd_postfix": "disabled", "target_cmd": "wandb disabled"}
{"uuid": "c7141de0-c98a-4c16-938c-f71381f9242b", "execution_plan": "step1: Set up a virtual environment and install required packages; step2: Clone the repository and set up the project structure; step3: Download and prepare the datasets (CIFAR-10N and ImageNet-15N); step4: Configure WandB for logging and experiment management; step5: Run COINNet on the CIFAR-10N dataset", "executed_cmds": "mkdir coinnet", "cmd_prefix": "cd", "cmd_postfix": "coinnet", "target_cmd": "cd coinnet"}
{"uuid": "1c6ef580-7a44-4772-b827-1a76cdc5de3a", "execution_plan": "step1: Set up a virtual environment and install required packages; step2: Clone the repository and set up the project structure; step3: Download and prepare the datasets (CIFAR-10N and ImageNet-15N); step4: Configure WandB for logging and experiment management; step5: Run COINNet on the CIFAR-10N dataset", "executed_cmds": "mkdir coinnet;cd coinnet", "cmd_prefix": "python -m", "cmd_postfix": "venv localenv", "target_cmd": "python -m venv localenv"}
{"uuid": "3c23f3cd-0e61-4767-9b47-16ec33906b32", "execution_plan": "step1: Set up a virtual environment and install required packages; step2: Clone the repository and set up the project structure; step3: Download and prepare the datasets (CIFAR-10N and ImageNet-15N); step4: Configure WandB for logging and experiment management; step5: Run COINNet on the CIFAR-10N dataset", "executed_cmds": "mkdir coinnet;cd coinnet;python -m venv localenv;source localenv/bin/activate;git clone https://github.com/ductri/COINNet src;pip install -r src/requirement.txt", "cmd_prefix": "mkdir", "cmd_postfix": "coinnet/data/", "target_cmd": "mkdir coinnet/data/"}
{"uuid": "e7b2aa80-0ddc-4336-b174-ed6558c2710b", "execution_plan": "step1: Install necessary R-packages and Gurobi optimizer; step2: Download and organize required R files into the 'R/' folder; step3: Choose between OpenML or PMLB dataset analysis; step4: For OpenML, download and run openml_permutation_tests.R; step5: For PMLB, download and organize files into 'pmlb_results/' folder, then run main_pmlb_experiments.R or use precomputed results; step6: For PMLB, run pmlb_permutation_tests.R after completing step 5; step7: Review the generated results and plots in the respective results folders", "executed_cmds": "Follow Gurobi installation instructions", "cmd_prefix": "source", "cmd_postfix": "_setup_session.R", "target_cmd": "source _setup_session.R"}
{"uuid": "18e11f16-24e0-4487-904c-39ec3dbea678", "execution_plan": "step1: Install PETSc and petsc4py as prerequisites for the project; step2: Set up a Conda environment and install Python dependencies listed in requirements.txt; step3: Compile the C++ file e.c to enable solving linear systems; step4: Train the subspace prediction model using the default Helmholtz dataset or custom data; step5: Solve linear systems using the trained model", "executed_cmds": "conda create -n neurkitt python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate neurkitt", "target_cmd": "conda activate neurkitt"}
{"uuid": "9b78e70c-0506-4ab5-a56c-4c1b80f1b3c7", "execution_plan": "step1: Install PETSc and petsc4py as prerequisites for the project; step2: Set up a Conda environment and install Python dependencies listed in requirements.txt; step3: Compile the C++ file e.c to enable solving linear systems; step4: Train the subspace prediction model using the default Helmholtz dataset or custom data; step5: Solve linear systems using the trained model", "executed_cmds": "conda create -n neurkitt python=3.10;conda activate neurkitt;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117;pip install numpy==1.25.2;pip install scipy====1.11.1", "cmd_prefix": "pip install", "cmd_postfix": "tqdm json", "target_cmd": "pip install tqdm json"}
{"uuid": "1c83515f-2b13-44d3-b29e-103b7875bcc7", "execution_plan": "step1: Install PETSc and petsc4py as prerequisites for the project; step2: Set up a Conda environment and install Python dependencies listed in requirements.txt; step3: Compile the C++ file e.c to enable solving linear systems; step4: Train the subspace prediction model using the default Helmholtz dataset or custom data; step5: Solve linear systems using the trained model", "executed_cmds": "conda create -n neurkitt python=3.10;conda activate neurkitt;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117;pip install numpy==1.25.2;pip install scipy====1.11.1;pip install tqdm json", "cmd_prefix": "python", "cmd_postfix": "train.py", "target_cmd": "python train.py"}
{"uuid": "456d5094-1e5f-4cdf-b76e-ad3b2dcd5bb6", "execution_plan": "step1: Install PETSc and petsc4py as prerequisites for the project; step2: Set up a Conda environment and install Python dependencies listed in requirements.txt; step3: Compile the C++ file e.c to enable solving linear systems; step4: Train the subspace prediction model using the default Helmholtz dataset or custom data; step5: Solve linear systems using the trained model", "executed_cmds": "conda create -n neurkitt python=3.10;conda activate neurkitt;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117;pip install numpy==1.25.2;pip install scipy====1.11.1;pip install tqdm json;python train.py", "cmd_prefix": "python", "cmd_postfix": "solve.py", "target_cmd": "python solve.py"}
{"uuid": "8bc0e7a2-0033-47f4-afb0-d0b78b94e8a1", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required packages; step2: Install the necessary extensions including Chamfer Distance, emd, and PointNet++; step3: Prepare the datasets as specified in DATASET.md; step4: Pre-train the PCP-MAE model on ShapeNet training set; step5: Fine-tune the pre-trained model on ScanObjectNN for classification; step6: Fine-tune the pre-trained model on ModelNet40 for classification; step7: Perform voting on ModelNet40 for improved classification results; step8: Conduct few-shot learning experiments on ModelNet40; step9: Perform part segmentation on ShapeNetPart; step10: Perform semantic segmentation on S3DIS; step11: Visualize the results using the provided visualization script", "executed_cmds": "conda create -n pcpmae python=3.10 -y", "cmd_prefix": "conda", "cmd_postfix": "activate pcpmae", "target_cmd": "conda activate pcpmae"}
{"uuid": "bc5c19c7-9de4-4e27-af7f-d26e04e58334", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required packages; step2: Install the necessary extensions including Chamfer Distance, emd, and PointNet++; step3: Prepare the datasets as specified in DATASET.md; step4: Pre-train the PCP-MAE model on ShapeNet training set; step5: Fine-tune the pre-trained model on ScanObjectNN for classification; step6: Fine-tune the pre-trained model on ModelNet40 for classification; step7: Perform voting on ModelNet40 for improved classification results; step8: Conduct few-shot learning experiments on ModelNet40; step9: Perform part segmentation on ShapeNetPart; step10: Perform semantic segmentation on S3DIS; step11: Visualize the results using the provided visualization script", "executed_cmds": "conda create -n pcpmae python=3.10 -y;conda activate pcpmae;conda install pytorch==2.0.1 torchvision==0.15.2 cudatoolkit=11.8 -c pytorch -c nvidia", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "146b83bf-df9b-4649-a55c-ee6c6ee29817", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required packages; step2: Install the necessary extensions including Chamfer Distance, emd, and PointNet++; step3: Prepare the datasets as specified in DATASET.md; step4: Pre-train the PCP-MAE model on ShapeNet training set; step5: Fine-tune the pre-trained model on ScanObjectNN for classification; step6: Fine-tune the pre-trained model on ModelNet40 for classification; step7: Perform voting on ModelNet40 for improved classification results; step8: Conduct few-shot learning experiments on ModelNet40; step9: Perform part segmentation on ShapeNetPart; step10: Perform semantic segmentation on S3DIS; step11: Visualize the results using the provided visualization script", "executed_cmds": "conda create -n pcpmae python=3.10 -y;conda activate pcpmae;conda install pytorch==2.0.1 torchvision==0.15.2 cudatoolkit=11.8 -c pytorch -c nvidia;pip install -r requirements.txt;cd ./extensions/chamfer_dist", "cmd_prefix": "python setup.py", "cmd_postfix": "install --user", "target_cmd": "python setup.py install --user"}
{"uuid": "19815423-78e1-4bf4-8eff-9d4c07be8839", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required packages; step2: Install the necessary extensions including Chamfer Distance, emd, and PointNet++; step3: Prepare the datasets as specified in DATASET.md; step4: Pre-train the PCP-MAE model on ShapeNet training set; step5: Fine-tune the pre-trained model on ScanObjectNN for classification; step6: Fine-tune the pre-trained model on ModelNet40 for classification; step7: Perform voting on ModelNet40 for improved classification results; step8: Conduct few-shot learning experiments on ModelNet40; step9: Perform part segmentation on ShapeNetPart; step10: Perform semantic segmentation on S3DIS; step11: Visualize the results using the provided visualization script", "executed_cmds": "conda create -n pcpmae python=3.10 -y;conda activate pcpmae;conda install pytorch==2.0.1 torchvision==0.15.2 cudatoolkit=11.8 -c pytorch -c nvidia;pip install -r requirements.txt;cd ./extensions/chamfer_dist;python setup.py install --user", "cmd_prefix": "cd", "cmd_postfix": "./extensions/emd", "target_cmd": "cd ./extensions/emd"}
{"uuid": "8421cb88-c5fb-4e83-9569-efb844d85c4f", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required packages; step2: Install the necessary extensions including Chamfer Distance, emd, and PointNet++; step3: Prepare the datasets as specified in DATASET.md; step4: Pre-train the PCP-MAE model on ShapeNet training set; step5: Fine-tune the pre-trained model on ScanObjectNN for classification; step6: Fine-tune the pre-trained model on ModelNet40 for classification; step7: Perform voting on ModelNet40 for improved classification results; step8: Conduct few-shot learning experiments on ModelNet40; step9: Perform part segmentation on ShapeNetPart; step10: Perform semantic segmentation on S3DIS; step11: Visualize the results using the provided visualization script", "executed_cmds": "conda create -n pcpmae python=3.10 -y;conda activate pcpmae;conda install pytorch==2.0.1 torchvision==0.15.2 cudatoolkit=11.8 -c pytorch -c nvidia;pip install -r requirements.txt;cd ./extensions/chamfer_dist;python setup.py install --user;cd ./extensions/emd", "cmd_prefix": "python setup.py", "cmd_postfix": "install --user", "target_cmd": "python setup.py install --user"}
{"uuid": "76dbfa62-395a-4e39-a7ba-1f496a17001c", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required packages; step2: Install the necessary extensions including Chamfer Distance, emd, and PointNet++; step3: Prepare the datasets as specified in DATASET.md; step4: Pre-train the PCP-MAE model on ShapeNet training set; step5: Fine-tune the pre-trained model on ScanObjectNN for classification; step6: Fine-tune the pre-trained model on ModelNet40 for classification; step7: Perform voting on ModelNet40 for improved classification results; step8: Conduct few-shot learning experiments on ModelNet40; step9: Perform part segmentation on ShapeNetPart; step10: Perform semantic segmentation on S3DIS; step11: Visualize the results using the provided visualization script", "executed_cmds": "conda create -n pcpmae python=3.10 -y;conda activate pcpmae;conda install pytorch==2.0.1 torchvision==0.15.2 cudatoolkit=11.8 -c pytorch -c nvidia;pip install -r requirements.txt;cd ./extensions/chamfer_dist;python setup.py install --user;cd ./extensions/emd;python setup.py install --user;pip install \"git+https://github.com/erikwijmans/Pointnet2_PyTorch.git#egg=pointnet2_ops&subdirectory=pointnet2_ops_lib\";CUDA_VISIBLE_DEVICES=<GPU> python main.py --config cfgs/pretrain/base.yaml --exp_name <output_file_name>;CUDA_VISIBLE_DEVICES=<GPUs> python main.py --config cfgs/finetune_scan_hardest.yaml --finetune_model --exp_name <output_file_name> --ckpts <path/to/pre-trained/model> --seed $RANDOM;CUDA_VISIBLE_DEVICES=<GPUs> python main.py --config cfgs/finetune_modelnet.yaml --finetune_model --exp_name <output_file_name> --ckpts <path/to/pre-trained/model> --seed $RANDOM;CUDA_VISIBLE_DEVICES=<GPUs> python main.py --test --config cfgs/finetune_modelnet.yaml --exp_name <output_file_name> --ckpts <path/to/best/fine-tuned/model> --seed $RANDOM --vote;CUDA_VISIBLE_DEVICES=<GPUs> python main.py --config cfgs/fewshot.yaml --finetune_model --ckpts <path/to/pre-trained/model> --exp_name <output_file_name> --way <5 or 10> --shot <10 or 20> --fold <0-9> --seed $RANDOM", "cmd_prefix": "cd", "cmd_postfix": "segmentation", "target_cmd": "cd segmentation"}
{"uuid": "71cdb10c-e080-455f-9157-e40419d4fc44", "execution_plan": "step1: Create and activate the conda environment using the provided environment.yml file; step2: Install PyTorch with the appropriate version and CUDA support; step3: Install additional pip dependencies including ogb, performer-pytorch, pot, pytorch-lightning, scipy, tensorboardx, torch-geometric, torchmetrics, wandb, yacs, and matplotlib; step4: Navigate to the holder_experiments directory for running experiments; step5: Set up the wandb key by updating the utils/api_keys.py file; step6: Run the multiset lower Holder experiment to generate separation quality and lower-Holder exponent figures; step7: Run the MPNN lower Holder experiment to generate result figures; step8: Run the COMBINE experiment to generate the result figure; step9: Run the trained epsilon tree experiments using the provided bash script; step10: Run the TUDataset experiments for specified datasets and models; step11: Navigate to the lrgb_holder directory for peptides-struct and peptides-func experiments; step12: Run regular experiments for peptides-struct and peptides-func with a 500K parameter budget; step13: Run small model experiments for peptides-struct with specified budgets and hidden dimensions; step14: Navigate to the ESAN_holder directory for ZINC12K experiments; step15: Prepare the ZINC dataset by running the data.py script; step16: Perform hyperparameter tuning for ZINC experiments using wandb sweep and agent commands", "executed_cmds": "conda env create --file=environment.yml;conda activate HolderAnalysis;pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121", "cmd_prefix": "pip", "cmd_postfix": "install ogb==1.3.6", "target_cmd": "pip install ogb==1.3.6"}
{"uuid": "8de86bfc-15cc-423c-931e-34b7b08f5874", "execution_plan": "step1: Create and activate the conda environment using the provided environment.yml file; step2: Install PyTorch with the appropriate version and CUDA support; step3: Install additional pip dependencies including ogb, performer-pytorch, pot, pytorch-lightning, scipy, tensorboardx, torch-geometric, torchmetrics, wandb, yacs, and matplotlib; step4: Navigate to the holder_experiments directory for running experiments; step5: Set up the wandb key by updating the utils/api_keys.py file; step6: Run the multiset lower Holder experiment to generate separation quality and lower-Holder exponent figures; step7: Run the MPNN lower Holder experiment to generate result figures; step8: Run the COMBINE experiment to generate the result figure; step9: Run the trained epsilon tree experiments using the provided bash script; step10: Run the TUDataset experiments for specified datasets and models; step11: Navigate to the lrgb_holder directory for peptides-struct and peptides-func experiments; step12: Run regular experiments for peptides-struct and peptides-func with a 500K parameter budget; step13: Run small model experiments for peptides-struct with specified budgets and hidden dimensions; step14: Navigate to the ESAN_holder directory for ZINC12K experiments; step15: Prepare the ZINC dataset by running the data.py script; step16: Perform hyperparameter tuning for ZINC experiments using wandb sweep and agent commands", "executed_cmds": "conda env create --file=environment.yml;conda activate HolderAnalysis;pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install ogb==1.3.6;pip install performer-pytorch==1.1.4", "cmd_prefix": "pip", "cmd_postfix": "install pot==0.9.3", "target_cmd": "pip install pot==0.9.3"}
{"uuid": "1fa8b7f6-a7c2-418f-8e77-13b5cc15b45d", "execution_plan": "step1: Create and activate the conda environment using the provided environment.yml file; step2: Install PyTorch with the appropriate version and CUDA support; step3: Install additional pip dependencies including ogb, performer-pytorch, pot, pytorch-lightning, scipy, tensorboardx, torch-geometric, torchmetrics, wandb, yacs, and matplotlib; step4: Navigate to the holder_experiments directory for running experiments; step5: Set up the wandb key by updating the utils/api_keys.py file; step6: Run the multiset lower Holder experiment to generate separation quality and lower-Holder exponent figures; step7: Run the MPNN lower Holder experiment to generate result figures; step8: Run the COMBINE experiment to generate the result figure; step9: Run the trained epsilon tree experiments using the provided bash script; step10: Run the TUDataset experiments for specified datasets and models; step11: Navigate to the lrgb_holder directory for peptides-struct and peptides-func experiments; step12: Run regular experiments for peptides-struct and peptides-func with a 500K parameter budget; step13: Run small model experiments for peptides-struct with specified budgets and hidden dimensions; step14: Navigate to the ESAN_holder directory for ZINC12K experiments; step15: Prepare the ZINC dataset by running the data.py script; step16: Perform hyperparameter tuning for ZINC experiments using wandb sweep and agent commands", "executed_cmds": "conda env create --file=environment.yml;conda activate HolderAnalysis;pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install ogb==1.3.6;pip install performer-pytorch==1.1.4;pip install pot==0.9.3;pip install pytorch-lightning==2.2.2;pip install scipy==1.13.0;pip install tensorboardx==2.6.2.2;pip install torch-geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.0+cu121.html;pip install torchmetrics==1.3.2;pip install wandb==0.16.6", "cmd_prefix": "pip", "cmd_postfix": "install yacs==0.1.8", "target_cmd": "pip install yacs==0.1.8"}
{"uuid": "8bbf4b62-f345-4bb7-9baf-598ed22a66d8", "execution_plan": "step1: Create and activate the conda environment using the provided environment.yml file; step2: Install PyTorch with the appropriate version and CUDA support; step3: Install additional pip dependencies including ogb, performer-pytorch, pot, pytorch-lightning, scipy, tensorboardx, torch-geometric, torchmetrics, wandb, yacs, and matplotlib; step4: Navigate to the holder_experiments directory for running experiments; step5: Set up the wandb key by updating the utils/api_keys.py file; step6: Run the multiset lower Holder experiment to generate separation quality and lower-Holder exponent figures; step7: Run the MPNN lower Holder experiment to generate result figures; step8: Run the COMBINE experiment to generate the result figure; step9: Run the trained epsilon tree experiments using the provided bash script; step10: Run the TUDataset experiments for specified datasets and models; step11: Navigate to the lrgb_holder directory for peptides-struct and peptides-func experiments; step12: Run regular experiments for peptides-struct and peptides-func with a 500K parameter budget; step13: Run small model experiments for peptides-struct with specified budgets and hidden dimensions; step14: Navigate to the ESAN_holder directory for ZINC12K experiments; step15: Prepare the ZINC dataset by running the data.py script; step16: Perform hyperparameter tuning for ZINC experiments using wandb sweep and agent commands", "executed_cmds": "conda env create --file=environment.yml;conda activate HolderAnalysis;pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install ogb==1.3.6;pip install performer-pytorch==1.1.4;pip install pot==0.9.3;pip install pytorch-lightning==2.2.2;pip install scipy==1.13.0;pip install tensorboardx==2.6.2.2;pip install torch-geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.0+cu121.html;pip install torchmetrics==1.3.2;pip install wandb==0.16.6;pip install yacs==0.1.8;pip install matplotlib==3.8.4", "cmd_prefix": "cd", "cmd_postfix": "holder_experiments", "target_cmd": "cd holder_experiments"}
{"uuid": "48ca372b-adf5-497f-9689-0c5fbd9c5af7", "execution_plan": "step1: Create and activate the conda environment using the provided environment.yml file; step2: Install PyTorch with the appropriate version and CUDA support; step3: Install additional pip dependencies including ogb, performer-pytorch, pot, pytorch-lightning, scipy, tensorboardx, torch-geometric, torchmetrics, wandb, yacs, and matplotlib; step4: Navigate to the holder_experiments directory for running experiments; step5: Set up the wandb key by updating the utils/api_keys.py file; step6: Run the multiset lower Holder experiment to generate separation quality and lower-Holder exponent figures; step7: Run the MPNN lower Holder experiment to generate result figures; step8: Run the COMBINE experiment to generate the result figure; step9: Run the trained epsilon tree experiments using the provided bash script; step10: Run the TUDataset experiments for specified datasets and models; step11: Navigate to the lrgb_holder directory for peptides-struct and peptides-func experiments; step12: Run regular experiments for peptides-struct and peptides-func with a 500K parameter budget; step13: Run small model experiments for peptides-struct with specified budgets and hidden dimensions; step14: Navigate to the ESAN_holder directory for ZINC12K experiments; step15: Prepare the ZINC dataset by running the data.py script; step16: Perform hyperparameter tuning for ZINC experiments using wandb sweep and agent commands", "executed_cmds": "conda env create --file=environment.yml;conda activate HolderAnalysis;pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install ogb==1.3.6;pip install performer-pytorch==1.1.4;pip install pot==0.9.3;pip install pytorch-lightning==2.2.2;pip install scipy==1.13.0;pip install tensorboardx==2.6.2.2;pip install torch-geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.0+cu121.html;pip install torchmetrics==1.3.2;pip install wandb==0.16.6;pip install yacs==0.1.8;pip install matplotlib==3.8.4;cd holder_experiments;python run_multiset_lower_holder_exp.py;python run_mpnn_lower_holder_exp.py", "cmd_prefix": "python", "cmd_postfix": "run_combine_exp.py", "target_cmd": "python run_combine_exp.py"}
{"uuid": "4fdb52ba-53b2-4ac8-8608-d711e43d6fe3", "execution_plan": "step1: Create and activate the conda environment using the provided environment.yml file; step2: Install PyTorch with the appropriate version and CUDA support; step3: Install additional pip dependencies including ogb, performer-pytorch, pot, pytorch-lightning, scipy, tensorboardx, torch-geometric, torchmetrics, wandb, yacs, and matplotlib; step4: Navigate to the holder_experiments directory for running experiments; step5: Set up the wandb key by updating the utils/api_keys.py file; step6: Run the multiset lower Holder experiment to generate separation quality and lower-Holder exponent figures; step7: Run the MPNN lower Holder experiment to generate result figures; step8: Run the COMBINE experiment to generate the result figure; step9: Run the trained epsilon tree experiments using the provided bash script; step10: Run the TUDataset experiments for specified datasets and models; step11: Navigate to the lrgb_holder directory for peptides-struct and peptides-func experiments; step12: Run regular experiments for peptides-struct and peptides-func with a 500K parameter budget; step13: Run small model experiments for peptides-struct with specified budgets and hidden dimensions; step14: Navigate to the ESAN_holder directory for ZINC12K experiments; step15: Prepare the ZINC dataset by running the data.py script; step16: Perform hyperparameter tuning for ZINC experiments using wandb sweep and agent commands", "executed_cmds": "conda env create --file=environment.yml;conda activate HolderAnalysis;pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install ogb==1.3.6;pip install performer-pytorch==1.1.4;pip install pot==0.9.3;pip install pytorch-lightning==2.2.2;pip install scipy==1.13.0;pip install tensorboardx==2.6.2.2;pip install torch-geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.0+cu121.html;pip install torchmetrics==1.3.2;pip install wandb==0.16.6;pip install yacs==0.1.8;pip install matplotlib==3.8.4;cd holder_experiments;python run_multiset_lower_holder_exp.py;python run_mpnn_lower_holder_exp.py;python run_combine_exp.py;./train_eps_tree.sh;./experiment_scripts/$DATASET$_$MODEL$.sh", "cmd_prefix": "cd", "cmd_postfix": "lrgb_holder", "target_cmd": "cd lrgb_holder"}
{"uuid": "61c68566-cf0d-47b0-8425-aa61b0b858ee", "execution_plan": "step1: Create and activate the conda environment using the provided environment.yml file; step2: Install PyTorch with the appropriate version and CUDA support; step3: Install additional pip dependencies including ogb, performer-pytorch, pot, pytorch-lightning, scipy, tensorboardx, torch-geometric, torchmetrics, wandb, yacs, and matplotlib; step4: Navigate to the holder_experiments directory for running experiments; step5: Set up the wandb key by updating the utils/api_keys.py file; step6: Run the multiset lower Holder experiment to generate separation quality and lower-Holder exponent figures; step7: Run the MPNN lower Holder experiment to generate result figures; step8: Run the COMBINE experiment to generate the result figure; step9: Run the trained epsilon tree experiments using the provided bash script; step10: Run the TUDataset experiments for specified datasets and models; step11: Navigate to the lrgb_holder directory for peptides-struct and peptides-func experiments; step12: Run regular experiments for peptides-struct and peptides-func with a 500K parameter budget; step13: Run small model experiments for peptides-struct with specified budgets and hidden dimensions; step14: Navigate to the ESAN_holder directory for ZINC12K experiments; step15: Prepare the ZINC dataset by running the data.py script; step16: Perform hyperparameter tuning for ZINC experiments using wandb sweep and agent commands", "executed_cmds": "conda env create --file=environment.yml;conda activate HolderAnalysis;pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install ogb==1.3.6;pip install performer-pytorch==1.1.4;pip install pot==0.9.3;pip install pytorch-lightning==2.2.2;pip install scipy==1.13.0;pip install tensorboardx==2.6.2.2;pip install torch-geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.0+cu121.html;pip install torchmetrics==1.3.2;pip install wandb==0.16.6;pip install yacs==0.1.8;pip install matplotlib==3.8.4;cd holder_experiments;python run_multiset_lower_holder_exp.py;python run_mpnn_lower_holder_exp.py;python run_combine_exp.py;./train_eps_tree.sh;./experiment_scripts/$DATASET$_$MODEL$.sh;cd lrgb_holder;python main.py --cfg configs/LRGB-tuned/$DATASET$-$MODEL$.yaml --repeat 4 wandb.use False;python main.py --cfg configs/LRGB-tuned/peptides-struct-$MODEL$.yaml --repeat 4 name_tag $BUDGET$ wandb.use False gnn.dim_inner $DIM$", "cmd_prefix": "cd", "cmd_postfix": "ESAN_holder", "target_cmd": "cd ESAN_holder"}
{"uuid": "53072c60-3912-4e8b-bb1f-a28673fc2d05", "execution_plan": "step1: Create and activate the conda environment using the provided environment.yml file; step2: Install PyTorch with the appropriate version and CUDA support; step3: Install additional pip dependencies including ogb, performer-pytorch, pot, pytorch-lightning, scipy, tensorboardx, torch-geometric, torchmetrics, wandb, yacs, and matplotlib; step4: Navigate to the holder_experiments directory for running experiments; step5: Set up the wandb key by updating the utils/api_keys.py file; step6: Run the multiset lower Holder experiment to generate separation quality and lower-Holder exponent figures; step7: Run the MPNN lower Holder experiment to generate result figures; step8: Run the COMBINE experiment to generate the result figure; step9: Run the trained epsilon tree experiments using the provided bash script; step10: Run the TUDataset experiments for specified datasets and models; step11: Navigate to the lrgb_holder directory for peptides-struct and peptides-func experiments; step12: Run regular experiments for peptides-struct and peptides-func with a 500K parameter budget; step13: Run small model experiments for peptides-struct with specified budgets and hidden dimensions; step14: Navigate to the ESAN_holder directory for ZINC12K experiments; step15: Prepare the ZINC dataset by running the data.py script; step16: Perform hyperparameter tuning for ZINC experiments using wandb sweep and agent commands", "executed_cmds": "conda env create --file=environment.yml;conda activate HolderAnalysis;pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install ogb==1.3.6;pip install performer-pytorch==1.1.4;pip install pot==0.9.3;pip install pytorch-lightning==2.2.2;pip install scipy==1.13.0;pip install tensorboardx==2.6.2.2;pip install torch-geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.0+cu121.html;pip install torchmetrics==1.3.2;pip install wandb==0.16.6;pip install yacs==0.1.8;pip install matplotlib==3.8.4;cd holder_experiments;python run_multiset_lower_holder_exp.py;python run_mpnn_lower_holder_exp.py;python run_combine_exp.py;./train_eps_tree.sh;./experiment_scripts/$DATASET$_$MODEL$.sh;cd lrgb_holder;python main.py --cfg configs/LRGB-tuned/$DATASET$-$MODEL$.yaml --repeat 4 wandb.use False;python main.py --cfg configs/LRGB-tuned/peptides-struct-$MODEL$.yaml --repeat 4 name_tag $BUDGET$ wandb.use False gnn.dim_inner $DIM$;cd ESAN_holder", "cmd_prefix": "python data.py", "cmd_postfix": "--dataset ZINC", "target_cmd": "python data.py --dataset ZINC"}
{"uuid": "3358d2ee-4355-4011-8e44-fc7006ec3f01", "execution_plan": "step1: Create and activate the conda environment using the provided environment.yml file; step2: Install PyTorch with the appropriate version and CUDA support; step3: Install additional pip dependencies including ogb, performer-pytorch, pot, pytorch-lightning, scipy, tensorboardx, torch-geometric, torchmetrics, wandb, yacs, and matplotlib; step4: Navigate to the holder_experiments directory for running experiments; step5: Set up the wandb key by updating the utils/api_keys.py file; step6: Run the multiset lower Holder experiment to generate separation quality and lower-Holder exponent figures; step7: Run the MPNN lower Holder experiment to generate result figures; step8: Run the COMBINE experiment to generate the result figure; step9: Run the trained epsilon tree experiments using the provided bash script; step10: Run the TUDataset experiments for specified datasets and models; step11: Navigate to the lrgb_holder directory for peptides-struct and peptides-func experiments; step12: Run regular experiments for peptides-struct and peptides-func with a 500K parameter budget; step13: Run small model experiments for peptides-struct with specified budgets and hidden dimensions; step14: Navigate to the ESAN_holder directory for ZINC12K experiments; step15: Prepare the ZINC dataset by running the data.py script; step16: Perform hyperparameter tuning for ZINC experiments using wandb sweep and agent commands", "executed_cmds": "conda env create --file=environment.yml;conda activate HolderAnalysis;pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121;pip install ogb==1.3.6;pip install performer-pytorch==1.1.4;pip install pot==0.9.3;pip install pytorch-lightning==2.2.2;pip install scipy==1.13.0;pip install tensorboardx==2.6.2.2;pip install torch-geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.0+cu121.html;pip install torchmetrics==1.3.2;pip install wandb==0.16.6;pip install yacs==0.1.8;pip install matplotlib==3.8.4;cd holder_experiments;python run_multiset_lower_holder_exp.py;python run_mpnn_lower_holder_exp.py;python run_combine_exp.py;./train_eps_tree.sh;./experiment_scripts/$DATASET$_$MODEL$.sh;cd lrgb_holder;python main.py --cfg configs/LRGB-tuned/$DATASET$-$MODEL$.yaml --repeat 4 wandb.use False;python main.py --cfg configs/LRGB-tuned/peptides-struct-$MODEL$.yaml --repeat 4 name_tag $BUDGET$ wandb.use False gnn.dim_inner $DIM$;cd ESAN_holder;python data.py --dataset ZINC;wandb sweep configs/<config-name>", "cmd_prefix": "wandb", "cmd_postfix": "agent <sweep-id>", "target_cmd": "wandb agent <sweep-id>"}
{"uuid": "49cff074-b829-4024-9e37-aaaa7f07db8a", "execution_plan": "step1: Clone the repository and install required dependencies; step2: Authenticate with Hugging Face and optionally with Weights & Biases; step3: Generate synthetic data using EntiGraph; step4: Tokenize the generated synthetic data; step5: Download and tokenize replay data (RedPajama dataset); step6: Perform continued pretraining on Llama 3 8B using the synthetic data; step7: Evaluate the model on the QuALITY QA set; step8: Download and tokenize instruction tuning data (UltraChat dataset); step9: Perform instruction tuning on the continually pretrained model; step10: Host an interactive chatbot with the instruction-tuned model; step11: Set up API keys for OpenAI and Cohere for RAG; step12: Run evaluation with RAG using the EntiGraph CPT model or base Llama 3 8B model", "executed_cmds": "git clone https://github.com/ZitongYang/Synthetic_Continued_Pretraining.git;cd Synthetic_Continued_Pretraining", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "6b6daa9b-1d2a-47e9-a35f-842a9597c323", "execution_plan": "step1: Clone the repository and install required dependencies; step2: Authenticate with Hugging Face and optionally with Weights & Biases; step3: Generate synthetic data using EntiGraph; step4: Tokenize the generated synthetic data; step5: Download and tokenize replay data (RedPajama dataset); step6: Perform continued pretraining on Llama 3 8B using the synthetic data; step7: Evaluate the model on the QuALITY QA set; step8: Download and tokenize instruction tuning data (UltraChat dataset); step9: Perform instruction tuning on the continually pretrained model; step10: Host an interactive chatbot with the instruction-tuned model; step11: Set up API keys for OpenAI and Cohere for RAG; step12: Run evaluation with RAG using the EntiGraph CPT model or base Llama 3 8B model", "executed_cmds": "git clone https://github.com/ZitongYang/Synthetic_Continued_Pretraining.git;cd Synthetic_Continued_Pretraining;pip install -r requirements.txt;huggingface-cli login --token <huggingface token>", "cmd_prefix": "wandb login <weights", "cmd_postfix": "and bias token>", "target_cmd": "wandb login <weights and bias token>"}
{"uuid": "0dd5c955-3b2b-4578-856d-7a444fab494c", "execution_plan": "step1: Clone the repository and install required dependencies; step2: Authenticate with Hugging Face and optionally with Weights & Biases; step3: Generate synthetic data using EntiGraph; step4: Tokenize the generated synthetic data; step5: Download and tokenize replay data (RedPajama dataset); step6: Perform continued pretraining on Llama 3 8B using the synthetic data; step7: Evaluate the model on the QuALITY QA set; step8: Download and tokenize instruction tuning data (UltraChat dataset); step9: Perform instruction tuning on the continually pretrained model; step10: Host an interactive chatbot with the instruction-tuned model; step11: Set up API keys for OpenAI and Cohere for RAG; step12: Run evaluation with RAG using the EntiGraph CPT model or base Llama 3 8B model", "executed_cmds": "git clone https://github.com/ZitongYang/Synthetic_Continued_Pretraining.git;cd Synthetic_Continued_Pretraining;pip install -r requirements.txt;huggingface-cli login --token <huggingface token>;wandb login <weights and bias token>", "cmd_prefix": "python", "cmd_postfix": "data/entigraph.py i", "target_cmd": "python data/entigraph.py i"}
{"uuid": "7de14198-4aab-43f7-a838-79daa1fcb548", "execution_plan": "step1: Clone the repository and install required dependencies; step2: Authenticate with Hugging Face and optionally with Weights & Biases; step3: Generate synthetic data using EntiGraph; step4: Tokenize the generated synthetic data; step5: Download and tokenize replay data (RedPajama dataset); step6: Perform continued pretraining on Llama 3 8B using the synthetic data; step7: Evaluate the model on the QuALITY QA set; step8: Download and tokenize instruction tuning data (UltraChat dataset); step9: Perform instruction tuning on the continually pretrained model; step10: Host an interactive chatbot with the instruction-tuned model; step11: Set up API keys for OpenAI and Cohere for RAG; step12: Run evaluation with RAG using the EntiGraph CPT model or base Llama 3 8B model", "executed_cmds": "git clone https://github.com/ZitongYang/Synthetic_Continued_Pretraining.git;cd Synthetic_Continued_Pretraining;pip install -r requirements.txt;huggingface-cli login --token <huggingface token>;wandb login <weights and bias token>;python data/entigraph.py i;python data/tokenize_entigraph.py;python data/tokenize_redpj.py;./scripts/train.sh --lr 5e-06 --rr 0.1 --epochs 2 --bs 16 --wd 0.01 --warmup 0.05 --task_name quality;python evaluation.py --model_path=ckpts/quality-lr5e-06-rr0.1-epochs2-bs16-wd0.01-warmup0.05-MetaLlama38B;python data/tokenize_instruct.py;./scripts/train.sh --lr 5e-06 --rr 0.1 --epochs 2 --bs 128 --wd 0.01 --warmup 0.05 --task_name instruct --model_name ckpts/quality-lr5e-06-rr0.1-epochs2-bs16-wd0.01-warmup0.05-MetaLlama38B", "cmd_prefix": "python", "cmd_postfix": "interactive.py", "target_cmd": "python interactive.py"}
{"uuid": "ea1bd189-1938-41c2-8356-a95692db55ec", "execution_plan": "step1: Install Python and required dependencies as specified in the requirements.txt file; step2: Install additional packages for molecular generation evaluation (rdkit, fcd_torch, mini_moses); step3: Set up the environment with Python 3.9.16, PyTorch 2.0.0, PyG 2.3.0, and Pytorch-lightning 2.0.1; step4: Run the model for polymer graph generation with the provided example command; step5: Run the model for small molecule generation with the provided example commands; step6: Test the code on a custom dataset if desired", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "pip", "cmd_postfix": "install rdkit", "target_cmd": "pip install rdkit"}
{"uuid": "7ef10bb0-01a0-4355-b5c5-be6d7acb67bb", "execution_plan": "step1: Install Python and required dependencies as specified in the requirements.txt file; step2: Install additional packages for molecular generation evaluation (rdkit, fcd_torch, mini_moses); step3: Set up the environment with Python 3.9.16, PyTorch 2.0.0, PyG 2.3.0, and Pytorch-lightning 2.0.1; step4: Run the model for polymer graph generation with the provided example command; step5: Run the model for small molecule generation with the provided example commands; step6: Test the code on a custom dataset if desired", "executed_cmds": "pip install -r requirements.txt;pip install rdkit", "cmd_prefix": "pip", "cmd_postfix": "install fcd_torch", "target_cmd": "pip install fcd_torch"}
{"uuid": "f1683387-57f6-4c9d-a946-e19912e6e8ed", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages; step2: Install additional dependencies including flash-attn, nemo_toolkit, flashinfer, and cutlass; step3: Build the kernels for ShadowKV by running the setup script; step4: Build the RULER dataset for evaluation; step5: Run accuracy evaluations with full attention or ShadowKV method, optionally with MInference; step6: Run efficiency evaluations with a single A100 GPU", "executed_cmds": "conda create -n ShadowKV python=3.10 -y", "cmd_prefix": "conda", "cmd_postfix": "activate ShadowKV", "target_cmd": "conda activate ShadowKV"}
{"uuid": "90bea8bc-6519-4f4d-b536-07061b6d679b", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages; step2: Install additional dependencies including flash-attn, nemo_toolkit, flashinfer, and cutlass; step3: Build the kernels for ShadowKV by running the setup script; step4: Build the RULER dataset for evaluation; step5: Run accuracy evaluations with full attention or ShadowKV method, optionally with MInference; step6: Run efficiency evaluations with a single A100 GPU", "executed_cmds": "conda create -n ShadowKV python=3.10 -y;conda activate ShadowKV", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "46ed6f09-a34b-4f71-bebc-9f5c5495d218", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages; step2: Install additional dependencies including flash-attn, nemo_toolkit, flashinfer, and cutlass; step3: Build the kernels for ShadowKV by running the setup script; step4: Build the RULER dataset for evaluation; step5: Run accuracy evaluations with full attention or ShadowKV method, optionally with MInference; step6: Run efficiency evaluations with a single A100 GPU", "executed_cmds": "conda create -n ShadowKV python=3.10 -y;conda activate ShadowKV;pip install -r requirements.txt;pip install flash-attn --no-build-isolation", "cmd_prefix": "pip", "cmd_postfix": "install wheel", "target_cmd": "pip install wheel"}
{"uuid": "879f93f0-72de-447c-9192-5ae8fae139c4", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages; step2: Install additional dependencies including flash-attn, nemo_toolkit, flashinfer, and cutlass; step3: Build the kernels for ShadowKV by running the setup script; step4: Build the RULER dataset for evaluation; step5: Run accuracy evaluations with full attention or ShadowKV method, optionally with MInference; step6: Run efficiency evaluations with a single A100 GPU", "executed_cmds": "conda create -n ShadowKV python=3.10 -y;conda activate ShadowKV;pip install -r requirements.txt;pip install flash-attn --no-build-isolation;pip install wheel", "cmd_prefix": "pip", "cmd_postfix": "install Cython", "target_cmd": "pip install Cython"}
{"uuid": "844ac001-7bc5-4396-a378-db99019da21a", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages; step2: Install additional dependencies including flash-attn, nemo_toolkit, flashinfer, and cutlass; step3: Build the kernels for ShadowKV by running the setup script; step4: Build the RULER dataset for evaluation; step5: Run accuracy evaluations with full attention or ShadowKV method, optionally with MInference; step6: Run efficiency evaluations with a single A100 GPU", "executed_cmds": "conda create -n ShadowKV python=3.10 -y;conda activate ShadowKV;pip install -r requirements.txt;pip install flash-attn --no-build-isolation;pip install wheel;pip install Cython", "cmd_prefix": "pip", "cmd_postfix": "install youtokentome", "target_cmd": "pip install youtokentome"}
{"uuid": "80bacb09-b1e6-41ac-b249-68a83d656783", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages; step2: Install additional dependencies including flash-attn, nemo_toolkit, flashinfer, and cutlass; step3: Build the kernels for ShadowKV by running the setup script; step4: Build the RULER dataset for evaluation; step5: Run accuracy evaluations with full attention or ShadowKV method, optionally with MInference; step6: Run efficiency evaluations with a single A100 GPU", "executed_cmds": "conda create -n ShadowKV python=3.10 -y;conda activate ShadowKV;pip install -r requirements.txt;pip install flash-attn --no-build-isolation;pip install wheel;pip install Cython;pip install youtokentome;pip install nemo_toolkit[all]==1.23;pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.3/", "cmd_prefix": "mkdir", "cmd_postfix": "3rdparty", "target_cmd": "mkdir 3rdparty"}
{"uuid": "57a462d1-8817-48ac-8630-fb0a8c1d8d88", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages; step2: Install additional dependencies including flash-attn, nemo_toolkit, flashinfer, and cutlass; step3: Build the kernels for ShadowKV by running the setup script; step4: Build the RULER dataset for evaluation; step5: Run accuracy evaluations with full attention or ShadowKV method, optionally with MInference; step6: Run efficiency evaluations with a single A100 GPU", "executed_cmds": "conda create -n ShadowKV python=3.10 -y;conda activate ShadowKV;pip install -r requirements.txt;pip install flash-attn --no-build-isolation;pip install wheel;pip install Cython;pip install youtokentome;pip install nemo_toolkit[all]==1.23;pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.3/;mkdir 3rdparty;git clone https://github.com/NVIDIA/cutlass.git 3rdparty/cutlass", "cmd_prefix": "python setup.py", "cmd_postfix": "build_ext --inplace", "target_cmd": "python setup.py build_ext --inplace"}
{"uuid": "df690864-d5a4-4f2d-ae4b-4e4479444bc8", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages; step2: Install additional dependencies including flash-attn, nemo_toolkit, flashinfer, and cutlass; step3: Build the kernels for ShadowKV by running the setup script; step4: Build the RULER dataset for evaluation; step5: Run accuracy evaluations with full attention or ShadowKV method, optionally with MInference; step6: Run efficiency evaluations with a single A100 GPU", "executed_cmds": "conda create -n ShadowKV python=3.10 -y;conda activate ShadowKV;pip install -r requirements.txt;pip install flash-attn --no-build-isolation;pip install wheel;pip install Cython;pip install youtokentome;pip install nemo_toolkit[all]==1.23;pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.3/;mkdir 3rdparty;git clone https://github.com/NVIDIA/cutlass.git 3rdparty/cutlass;python setup.py build_ext --inplace;python -c \"import nltk; nltk.download('punkt')\"", "cmd_prefix": "cd", "cmd_postfix": "data/ruler", "target_cmd": "cd data/ruler"}
{"uuid": "a3bae4f2-c0cb-4ec7-aca1-2ed7b6c7ff63", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages; step2: Install additional dependencies including flash-attn, nemo_toolkit, flashinfer, and cutlass; step3: Build the kernels for ShadowKV by running the setup script; step4: Build the RULER dataset for evaluation; step5: Run accuracy evaluations with full attention or ShadowKV method, optionally with MInference; step6: Run efficiency evaluations with a single A100 GPU", "executed_cmds": "conda create -n ShadowKV python=3.10 -y;conda activate ShadowKV;pip install -r requirements.txt;pip install flash-attn --no-build-isolation;pip install wheel;pip install Cython;pip install youtokentome;pip install nemo_toolkit[all]==1.23;pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.3/;mkdir 3rdparty;git clone https://github.com/NVIDIA/cutlass.git 3rdparty/cutlass;python setup.py build_ext --inplace;python -c \"import nltk; nltk.download('punkt')\";cd data/ruler;bash create_dataset.sh \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\" \"llama-3\";OMP_NUM_THREADS=48 torchrun --standalone --nnodes=1 --nproc_per_node 8 test/eval_acc.py --datalen 131072 --method full --dataset_name \"ruler/niah_single_1;ruler/niah_single_2;ruler/niah_single_3;ruler/niah_multikey_1;ruler/niah_multikey_2;ruler/niah_multiquery;ruler/niah_multivalue;ruler/vt;ruler/fwe;ruler/qa_1;ruler/qa_2\" --model_name \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\";OMP_NUM_THREADS=48 torchrun --standalone --nnodes=1 --nproc_per_node 8 test/eval_acc.py --datalen 131072 --method shadowkv --dataset_name \"ruler/niah_single_1;ruler/niah_single_2;ruler/niah_single_3;ruler/niah_multikey_1;ruler/niah_multikey_2;ruler/niah_multiquery;ruler/niah_multivalue;ruler/vt;ruler/fwe;ruler/qa_1;ruler/qa_2\" --sparse_budget 2048 --rank 160 --chunk_size 8;OMP_NUM_THREADS=48 torchrun --standalone --nnodes=1 --nproc_per_node 8 test/eval_acc.py --datalen 131072 --method full --dataset_name \"ruler/niah_single_1;ruler/niah_single_2;ruler/niah_single_3;ruler/niah_multikey_1;ruler/niah_multikey_2;ruler/niah_multiquery;ruler/niah_multivalue;ruler/vt;ruler/fwe;ruler/qa_1", "cmd_prefix": "ruler/qa_2\"", "cmd_postfix": "--minference", "target_cmd": "ruler/qa_2\" --minference"}
{"uuid": "281f6f84-45e4-42da-ab23-abfb5d02d7ae", "execution_plan": "step1: Create a virtual environment and install necessary packages; step2: Download and prepare the test datasets; step3: Download the pretrained models (DA-CLIP and Universal-IR); step4: Modify the configuration files to set dataset paths and model paths; step5: Configure the sampling algorithm and parameters in the test.yml file; step6: Run the evaluation script to test the method", "executed_cmds": "conda create -n mrsde python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate mrsde", "target_cmd": "conda activate mrsde"}
{"uuid": "8be69ed2-b0bd-4196-b94e-6596bff46845", "execution_plan": "step1: Create a virtual environment and install necessary packages; step2: Download and prepare the test datasets; step3: Download the pretrained models (DA-CLIP and Universal-IR); step4: Modify the configuration files to set dataset paths and model paths; step5: Configure the sampling algorithm and parameters in the test.yml file; step6: Run the evaluation script to test the method", "executed_cmds": "conda create -n mrsde python=3.8;conda activate mrsde", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "5e25dbc5-0fdf-45d7-92db-ba94138f3ac5", "execution_plan": "step1: Set up the Conda environment for the project; step2: Download the CLEVRTex dataset; step3: Configure the number of GPUs to use for training; step4: Train the AKOrN model on the CLEVRTex dataset; step5: Train the ItrSA model on the CLEVRTex dataset; step6: Evaluate the AKOrN and ItrSA models on the CLEVRTex dataset (standard evaluation); step7: Evaluate the AKOrN model with up-tiling for enhanced performance", "executed_cmds": "yes | conda create -n akorn python=3.12", "cmd_prefix": "conda", "cmd_postfix": "activate akorn", "target_cmd": "conda activate akorn"}
{"uuid": "1b3d7a42-fe6c-4237-a14f-ffa2657c14ac", "execution_plan": "step1: Set up the Conda environment for the project; step2: Download the CLEVRTex dataset; step3: Configure the number of GPUs to use for training; step4: Train the AKOrN model on the CLEVRTex dataset; step5: Train the ItrSA model on the CLEVRTex dataset; step6: Evaluate the AKOrN and ItrSA models on the CLEVRTex dataset (standard evaluation); step7: Evaluate the AKOrN model with up-tiling for enhanced performance", "executed_cmds": "yes | conda create -n akorn python=3.12;conda activate akorn", "cmd_prefix": "pip3 install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip3 install -r requirements.txt"}
{"uuid": "dfe1e1d0-64b9-49e0-be69-07c33dbc31ef", "execution_plan": "step1: Set up the Conda environment for the project; step2: Download the CLEVRTex dataset; step3: Configure the number of GPUs to use for training; step4: Train the AKOrN model on the CLEVRTex dataset; step5: Train the ItrSA model on the CLEVRTex dataset; step6: Evaluate the AKOrN and ItrSA models on the CLEVRTex dataset (standard evaluation); step7: Evaluate the AKOrN model with up-tiling for enhanced performance", "executed_cmds": "yes | conda create -n akorn python=3.12;conda activate akorn;pip3 install -r requirements.txt", "cmd_prefix": "cd", "cmd_postfix": "data", "target_cmd": "cd data"}
{"uuid": "e9d872b5-d078-462b-b4d1-f6ad029efb5b", "execution_plan": "step1: Set up the Conda environment for the project; step2: Download the CLEVRTex dataset; step3: Configure the number of GPUs to use for training; step4: Train the AKOrN model on the CLEVRTex dataset; step5: Train the ItrSA model on the CLEVRTex dataset; step6: Evaluate the AKOrN and ItrSA models on the CLEVRTex dataset (standard evaluation); step7: Evaluate the AKOrN model with up-tiling for enhanced performance", "executed_cmds": "yes | conda create -n akorn python=3.12;conda activate akorn;pip3 install -r requirements.txt;cd data", "cmd_prefix": "bash", "cmd_postfix": "download_clevrtex.sh", "target_cmd": "bash download_clevrtex.sh"}
{"uuid": "5613943f-8f47-4dac-8b2d-cfd79ae5ac01", "execution_plan": "step1: Set up the Conda environment for the project; step2: Download the CLEVRTex dataset; step3: Configure the number of GPUs to use for training; step4: Train the AKOrN model on the CLEVRTex dataset; step5: Train the ItrSA model on the CLEVRTex dataset; step6: Evaluate the AKOrN and ItrSA models on the CLEVRTex dataset (standard evaluation); step7: Evaluate the AKOrN model with up-tiling for enhanced performance", "executed_cmds": "yes | conda create -n akorn python=3.12;conda activate akorn;pip3 install -r requirements.txt;cd data;bash download_clevrtex.sh", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "eb7df28d-cbee-4d73-81e5-ea238123655a", "execution_plan": "step1: Set up the Conda environment for the project; step2: Download the CLEVRTex dataset; step3: Configure the number of GPUs to use for training; step4: Train the AKOrN model on the CLEVRTex dataset; step5: Train the ItrSA model on the CLEVRTex dataset; step6: Evaluate the AKOrN and ItrSA models on the CLEVRTex dataset (standard evaluation); step7: Evaluate the AKOrN model with up-tiling for enhanced performance", "executed_cmds": "yes | conda create -n akorn python=3.12;conda activate akorn;pip3 install -r requirements.txt;cd data;bash download_clevrtex.sh;cd ..;export NUM_GPUS=<number_of_gpus>", "cmd_prefix": "export", "cmd_postfix": "L=1", "target_cmd": "export L=1"}
{"uuid": "271f6268-0d7c-493e-8b0d-3668e2e026a1", "execution_plan": "step1: Set up the Conda environment for the project; step2: Download the CLEVRTex dataset; step3: Configure the number of GPUs to use for training; step4: Train the AKOrN model on the CLEVRTex dataset; step5: Train the ItrSA model on the CLEVRTex dataset; step6: Evaluate the AKOrN and ItrSA models on the CLEVRTex dataset (standard evaluation); step7: Evaluate the AKOrN model with up-tiling for enhanced performance", "executed_cmds": "yes | conda create -n akorn python=3.12;conda activate akorn;pip3 install -r requirements.txt;cd data;bash download_clevrtex.sh;cd ..;export NUM_GPUS=<number_of_gpus>;export L=1;accelerate launch --multi-gpu --num_processes=$NUM_GPUS train_obj.py --exp_name=clvtex_akorn --data_root=./data/clevrtex_full/ --model=akorn --data=clevrtex_full --J=attn --L=${L};accelerate launch --multi-gpu --num_processes=$NUM_GPUS train_obj.py --exp_name=clvtex_large_akorn --data_root=./data/clevrtex_full/ --model=akorn --data=clevrtex_full --J=attn --L=2 --ch=512 --batchsize=512 --epochs=1024 --lr=0.0005", "cmd_prefix": "export", "cmd_postfix": "L=1", "target_cmd": "export L=1"}
{"uuid": "3889ead9-603f-4d8f-86c5-131ad606b609", "execution_plan": "step1: Set up the Conda environment for the project; step2: Download the CLEVRTex dataset; step3: Configure the number of GPUs to use for training; step4: Train the AKOrN model on the CLEVRTex dataset; step5: Train the ItrSA model on the CLEVRTex dataset; step6: Evaluate the AKOrN and ItrSA models on the CLEVRTex dataset (standard evaluation); step7: Evaluate the AKOrN model with up-tiling for enhanced performance", "executed_cmds": "yes | conda create -n akorn python=3.12;conda activate akorn;pip3 install -r requirements.txt;cd data;bash download_clevrtex.sh;cd ..;export NUM_GPUS=<number_of_gpus>;export L=1;accelerate launch --multi-gpu --num_processes=$NUM_GPUS train_obj.py --exp_name=clvtex_akorn --data_root=./data/clevrtex_full/ --model=akorn --data=clevrtex_full --J=attn --L=${L};accelerate launch --multi-gpu --num_processes=$NUM_GPUS train_obj.py --exp_name=clvtex_large_akorn --data_root=./data/clevrtex_full/ --model=akorn --data=clevrtex_full --J=attn --L=2 --ch=512 --batchsize=512 --epochs=1024 --lr=0.0005;export L=1;accelerate launch --multi-gpu --num_processes=$NUM_GPUS train_obj.py --exp_name=clvtex_itrsa --data_root=./data/clevrtex_full/ --model=vit --data=clevrtex_full --L=${L} --gta=False", "cmd_prefix": "export", "cmd_postfix": "DATA_TYPE=full", "target_cmd": "export DATA_TYPE=full"}
{"uuid": "e80116d0-9e7b-4dbf-b0c9-20aa9084d926", "execution_plan": "step1: Set up the Conda environment for the project; step2: Download the CLEVRTex dataset; step3: Configure the number of GPUs to use for training; step4: Train the AKOrN model on the CLEVRTex dataset; step5: Train the ItrSA model on the CLEVRTex dataset; step6: Evaluate the AKOrN and ItrSA models on the CLEVRTex dataset (standard evaluation); step7: Evaluate the AKOrN model with up-tiling for enhanced performance", "executed_cmds": "yes | conda create -n akorn python=3.12;conda activate akorn;pip3 install -r requirements.txt;cd data;bash download_clevrtex.sh;cd ..;export NUM_GPUS=<number_of_gpus>;export L=1;accelerate launch --multi-gpu --num_processes=$NUM_GPUS train_obj.py --exp_name=clvtex_akorn --data_root=./data/clevrtex_full/ --model=akorn --data=clevrtex_full --J=attn --L=${L};accelerate launch --multi-gpu --num_processes=$NUM_GPUS train_obj.py --exp_name=clvtex_large_akorn --data_root=./data/clevrtex_full/ --model=akorn --data=clevrtex_full --J=attn --L=2 --ch=512 --batchsize=512 --epochs=1024 --lr=0.0005;export L=1;accelerate launch --multi-gpu --num_processes=$NUM_GPUS train_obj.py --exp_name=clvtex_itrsa --data_root=./data/clevrtex_full/ --model=vit --data=clevrtex_full --L=${L} --gta=False;export DATA_TYPE=full", "cmd_prefix": "export", "cmd_postfix": "L=1", "target_cmd": "export L=1"}
{"uuid": "abe9c0f5-7b29-4b1c-9fd1-39e24bb24a7b", "execution_plan": "step1: Create a Python environment and install required dependencies; step2: Install PyTorch according to your configuration; step3: Download and set up the datasets as specified in DATASETS.md; step4: Download pre-computed prototypes if needed (optional); step5: Run TransCLIP-ZS (Zero-Shot setting) on a dataset; step6: Run TransCLIP-FS (Transductive Few-Shot setting) on a dataset; step7: Run TransCLIP-ZS on top of a few-shot learning method (Few-Shot setting); step8: Run TransCLIP-ZS on top of a few-shot learning method (Cross-dataset setting); step9: Run TransCLIP-ZS on top of a few-shot learning method (Domain-generalization setting); step10: Run transductive few-shot baselines", "executed_cmds": "conda create -y --name TransCLIP python=3.10.0", "cmd_prefix": "conda", "cmd_postfix": "activate TransCLIP", "target_cmd": "conda activate TransCLIP"}
{"uuid": "4e864d51-71a4-432a-9481-7f5890a4fe10", "execution_plan": "step1: Create a Python environment and install required dependencies; step2: Install PyTorch according to your configuration; step3: Download and set up the datasets as specified in DATASETS.md; step4: Download pre-computed prototypes if needed (optional); step5: Run TransCLIP-ZS (Zero-Shot setting) on a dataset; step6: Run TransCLIP-FS (Transductive Few-Shot setting) on a dataset; step7: Run TransCLIP-ZS on top of a few-shot learning method (Few-Shot setting); step8: Run TransCLIP-ZS on top of a few-shot learning method (Cross-dataset setting); step9: Run TransCLIP-ZS on top of a few-shot learning method (Domain-generalization setting); step10: Run transductive few-shot baselines", "executed_cmds": "conda create -y --name TransCLIP python=3.10.0;conda activate TransCLIP", "cmd_prefix": "pip3 install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip3 install -r requirements.txt"}
{"uuid": "d1af41e6-b8da-4157-b384-10d616b64ad0", "execution_plan": "step1: Install the necessary dependencies by following the UMI repository installation guide; step2: Download the raw GoPro videos or use the processed dataset for training; step3: Generate the dataset for training by running SLAM and dataset generation scripts (optional if using processed dataset); step4: Visualize the dataset to ensure it is correctly loaded; step5: Download the pre-trained models for real-world evaluation; step6: Run real-world evaluation using the provided evaluation script; step7: Train a policy using the processed dataset and the provided training scripts; step8: Customize training parameters if needed (e.g., vision encoder type, number of training pairs, etc.)", "executed_cmds": "bash run_slam.sh && bash run_generate_dataset.sh", "cmd_prefix": "python", "cmd_postfix": "visualize_dataset.py", "target_cmd": "python visualize_dataset.py"}
{"uuid": "b12cea7b-c019-4d81-8509-a568811343d9", "execution_plan": "step1: Install the necessary dependencies by following the UMI repository installation guide; step2: Download the raw GoPro videos or use the processed dataset for training; step3: Generate the dataset for training by running SLAM and dataset generation scripts (optional if using processed dataset); step4: Visualize the dataset to ensure it is correctly loaded; step5: Download the pre-trained models for real-world evaluation; step6: Run real-world evaluation using the provided evaluation script; step7: Train a policy using the processed dataset and the provided training scripts; step8: Customize training parameters if needed (e.g., vision encoder type, number of training pairs, etc.)", "executed_cmds": "bash run_slam.sh && bash run_generate_dataset.sh;python visualize_dataset.py", "cmd_prefix": "bash", "cmd_postfix": "eval.sh", "target_cmd": "bash eval.sh"}
{"uuid": "28dea3ae-e268-43b0-836d-3e7ab328b050", "execution_plan": "step1: Install the package manager `uv` to manage dependencies; step2: Create a virtual environment and install the project dependencies; step3: Run a single-structure calculation with PsiFormer on LiH; step4: Run PESNet (MetaGNN + FermiNet) on the N2 potential energy surface; step5: Run experiments using `seml` for managing experiments; step6: Run experiments directly without `seml` for the N2 system", "executed_cmds": "curl -LsSf https://astral.sh/uv/install.sh | sh", "cmd_prefix": "uv", "cmd_postfix": "sync", "target_cmd": "uv sync"}
{"uuid": "15f62f6d-d5d6-4f8b-a271-0e604b3d378d", "execution_plan": "step1: Install the package manager `uv` to manage dependencies; step2: Create a virtual environment and install the project dependencies; step3: Run a single-structure calculation with PsiFormer on LiH; step4: Run PESNet (MetaGNN + FermiNet) on the N2 potential energy surface; step5: Run experiments using `seml` for managing experiments; step6: Run experiments directly without `seml` for the N2 system", "executed_cmds": "curl -LsSf https://astral.sh/uv/install.sh | sh;uv sync", "cmd_prefix": "source", "cmd_postfix": ".venv/bin/activate", "target_cmd": "source .venv/bin/activate"}
{"uuid": "0a731524-1c35-4921-a9da-c9279492bcb6", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Install the required dependencies; step3: Generate activations for the models; step4: Compute the CKA (Centered Kernel Alignment) similarity between the base model and the test model; step5: Plot the CKA heatmap; step6: Train linear/MLP/CNN classifier for preliminary experiments; step7: Train GCN classifier for preliminary experiments; step8: Apply the classifier to suspect models; step9: Replicate comparative experiments for Human-Readable Fingerprint for Large Language Models; step10: Replicate comparative experiments for A Fingerprint for Large Language Models", "executed_cmds": "git clone https://github.com/tmylla/REEF.git", "cmd_prefix": "cd", "cmd_postfix": "REEF", "target_cmd": "cd REEF"}
{"uuid": "cc68e946-1256-46e4-9e5e-fa3e2b2d33f0", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Install the required dependencies; step3: Generate activations for the models; step4: Compute the CKA (Centered Kernel Alignment) similarity between the base model and the test model; step5: Plot the CKA heatmap; step6: Train linear/MLP/CNN classifier for preliminary experiments; step7: Train GCN classifier for preliminary experiments; step8: Apply the classifier to suspect models; step9: Replicate comparative experiments for Human-Readable Fingerprint for Large Language Models; step10: Replicate comparative experiments for A Fingerprint for Large Language Models", "executed_cmds": "git clone https://github.com/tmylla/REEF.git;cd REEF", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "27efa76f-1200-40b8-a1ba-a443adb0eb4b", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Install the required dependencies; step3: Generate activations for the models; step4: Compute the CKA (Centered Kernel Alignment) similarity between the base model and the test model; step5: Plot the CKA heatmap; step6: Train linear/MLP/CNN classifier for preliminary experiments; step7: Train GCN classifier for preliminary experiments; step8: Apply the classifier to suspect models; step9: Replicate comparative experiments for Human-Readable Fingerprint for Large Language Models; step10: Replicate comparative experiments for A Fingerprint for Large Language Models", "executed_cmds": "git clone https://github.com/tmylla/REEF.git;cd REEF;pip install -r requirements.txt;sh ./scripts/save_activation.sh;python compute_cka.py --base_model llama-2-7b --base_layers -1 --test_model vicuna-7b-v1.5 --test_layers -1;python train_cls.py --model llama-2-7b --layers 18 --datasets truthfulqa;python train_cls_gcn.py --model llama-2-7b --layers 18 --datasets truthfulqa;python transfer_cls.py --pretrain_dir classifier_path --suspect_model vicuna-7b-v1.5 --layers 18", "cmd_prefix": "python", "cmd_postfix": "pcs.py", "target_cmd": "python pcs.py"}
{"uuid": "320685b7-d015-4dc6-a0e4-bf47d9077fd9", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Install the required dependencies; step3: Generate activations for the models; step4: Compute the CKA (Centered Kernel Alignment) similarity between the base model and the test model; step5: Plot the CKA heatmap; step6: Train linear/MLP/CNN classifier for preliminary experiments; step7: Train GCN classifier for preliminary experiments; step8: Apply the classifier to suspect models; step9: Replicate comparative experiments for Human-Readable Fingerprint for Large Language Models; step10: Replicate comparative experiments for A Fingerprint for Large Language Models", "executed_cmds": "git clone https://github.com/tmylla/REEF.git;cd REEF;pip install -r requirements.txt;sh ./scripts/save_activation.sh;python compute_cka.py --base_model llama-2-7b --base_layers -1 --test_model vicuna-7b-v1.5 --test_layers -1;python train_cls.py --model llama-2-7b --layers 18 --datasets truthfulqa;python train_cls_gcn.py --model llama-2-7b --layers 18 --datasets truthfulqa;python transfer_cls.py --pretrain_dir classifier_path --suspect_model vicuna-7b-v1.5 --layers 18;python pcs.py", "cmd_prefix": "python", "cmd_postfix": "ics.py", "target_cmd": "python ics.py"}
{"uuid": "5fc3f0ea-6e10-49b0-972f-16164950a5d9", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Install the required dependencies; step3: Generate activations for the models; step4: Compute the CKA (Centered Kernel Alignment) similarity between the base model and the test model; step5: Plot the CKA heatmap; step6: Train linear/MLP/CNN classifier for preliminary experiments; step7: Train GCN classifier for preliminary experiments; step8: Apply the classifier to suspect models; step9: Replicate comparative experiments for Human-Readable Fingerprint for Large Language Models; step10: Replicate comparative experiments for A Fingerprint for Large Language Models", "executed_cmds": "git clone https://github.com/tmylla/REEF.git;cd REEF;pip install -r requirements.txt;sh ./scripts/save_activation.sh;python compute_cka.py --base_model llama-2-7b --base_layers -1 --test_model vicuna-7b-v1.5 --test_layers -1;python train_cls.py --model llama-2-7b --layers 18 --datasets truthfulqa;python train_cls_gcn.py --model llama-2-7b --layers 18 --datasets truthfulqa;python transfer_cls.py --pretrain_dir classifier_path --suspect_model vicuna-7b-v1.5 --layers 18;python pcs.py;python ics.py;sh ./scripts/save_logits.sh", "cmd_prefix": "python", "cmd_postfix": "logit.py", "target_cmd": "python logit.py"}
{"uuid": "9e5d34fb-40e1-4963-8908-7d17f6b62a55", "execution_plan": "step1: Install Python 3.10 and PyTorch (version 2.2.2) as prerequisites; step2: Set up a virtual environment (e.g., using conda) and activate it; step3: Install the ZoneEnv environment dependencies by navigating to the safety-gymnasium directory and running the installation command; step4: Install the remaining project dependencies using the requirements.txt file; step5: Download and install Rabinizer 4 for LTL formula conversion, ensuring Java 11 is installed and JAVA_HOME is set; step6: Verify the Rabinizer 4 installation by running a test command; step7: (Optional) Build and run the Docker image if using Docker instead of manual installation; step8: Train a model on the ZoneEnv environment using the provided training script; step9: Evaluate the trained model using the provided evaluation scripts, optionally rendering the simulation or visualizing trajectories; step10: (Optional) Perform comprehensive evaluation on test tasks or over training time using additional scripts", "executed_cmds": "conda activate deepltl;cd src/envs/zones/safety-gymnasium", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "6655a3c1-9aac-459c-bb4a-0db6207182cb", "execution_plan": "step1: Install Python 3.10 and PyTorch (version 2.2.2) as prerequisites; step2: Set up a virtual environment (e.g., using conda) and activate it; step3: Install the ZoneEnv environment dependencies by navigating to the safety-gymnasium directory and running the installation command; step4: Install the remaining project dependencies using the requirements.txt file; step5: Download and install Rabinizer 4 for LTL formula conversion, ensuring Java 11 is installed and JAVA_HOME is set; step6: Verify the Rabinizer 4 installation by running a test command; step7: (Optional) Build and run the Docker image if using Docker instead of manual installation; step8: Train a model on the ZoneEnv environment using the provided training script; step9: Evaluate the trained model using the provided evaluation scripts, optionally rendering the simulation or visualizing trajectories; step10: (Optional) Perform comprehensive evaluation on test tasks or over training time using additional scripts", "executed_cmds": "conda activate deepltl;cd src/envs/zones/safety-gymnasium;pip install -e .", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "fb31f0a5-d4c2-4193-9d4f-d6ebe50fbe1d", "execution_plan": "step1: Install Python 3.10 and PyTorch (version 2.2.2) as prerequisites; step2: Set up a virtual environment (e.g., using conda) and activate it; step3: Install the ZoneEnv environment dependencies by navigating to the safety-gymnasium directory and running the installation command; step4: Install the remaining project dependencies using the requirements.txt file; step5: Download and install Rabinizer 4 for LTL formula conversion, ensuring Java 11 is installed and JAVA_HOME is set; step6: Verify the Rabinizer 4 installation by running a test command; step7: (Optional) Build and run the Docker image if using Docker instead of manual installation; step8: Train a model on the ZoneEnv environment using the provided training script; step9: Evaluate the trained model using the provided evaluation scripts, optionally rendering the simulation or visualizing trajectories; step10: (Optional) Perform comprehensive evaluation on test tasks or over training time using additional scripts", "executed_cmds": "conda activate deepltl;cd src/envs/zones/safety-gymnasium;pip install -e .;pip install -r requirements.txt", "cmd_prefix": "./rabinizer4/bin/ltl2ldba", "cmd_postfix": "-h", "target_cmd": "./rabinizer4/bin/ltl2ldba -h"}
{"uuid": "6848ee06-6685-4af8-a30a-15e784e06365", "execution_plan": "step1: Install Python 3.10 and PyTorch (version 2.2.2) as prerequisites; step2: Set up a virtual environment (e.g., using conda) and activate it; step3: Install the ZoneEnv environment dependencies by navigating to the safety-gymnasium directory and running the installation command; step4: Install the remaining project dependencies using the requirements.txt file; step5: Download and install Rabinizer 4 for LTL formula conversion, ensuring Java 11 is installed and JAVA_HOME is set; step6: Verify the Rabinizer 4 installation by running a test command; step7: (Optional) Build and run the Docker image if using Docker instead of manual installation; step8: Train a model on the ZoneEnv environment using the provided training script; step9: Evaluate the trained model using the provided evaluation scripts, optionally rendering the simulation or visualizing trajectories; step10: (Optional) Perform comprehensive evaluation on test tasks or over training time using additional scripts", "executed_cmds": "conda activate deepltl;cd src/envs/zones/safety-gymnasium;pip install -e .;pip install -r requirements.txt;./rabinizer4/bin/ltl2ldba -h", "cmd_prefix": "docker build", "cmd_postfix": "-t deepltl .", "target_cmd": "docker build -t deepltl ."}
{"uuid": "7ada7a0e-43b8-437c-bab0-c691b19f2eee", "execution_plan": "step1: Install Python 3.10 and PyTorch (version 2.2.2) as prerequisites; step2: Set up a virtual environment (e.g., using conda) and activate it; step3: Install the ZoneEnv environment dependencies by navigating to the safety-gymnasium directory and running the installation command; step4: Install the remaining project dependencies using the requirements.txt file; step5: Download and install Rabinizer 4 for LTL formula conversion, ensuring Java 11 is installed and JAVA_HOME is set; step6: Verify the Rabinizer 4 installation by running a test command; step7: (Optional) Build and run the Docker image if using Docker instead of manual installation; step8: Train a model on the ZoneEnv environment using the provided training script; step9: Evaluate the trained model using the provided evaluation scripts, optionally rendering the simulation or visualizing trajectories; step10: (Optional) Perform comprehensive evaluation on test tasks or over training time using additional scripts", "executed_cmds": "conda activate deepltl;cd src/envs/zones/safety-gymnasium;pip install -e .;pip install -r requirements.txt;./rabinizer4/bin/ltl2ldba -h;docker build -t deepltl .", "cmd_prefix": "mkdir", "cmd_postfix": "experiments", "target_cmd": "mkdir experiments"}
{"uuid": "4fb68f0a-3d5d-425c-ae82-f08db37a5b84", "execution_plan": "step1: Install Python 3.10 and PyTorch (version 2.2.2) as prerequisites; step2: Set up a virtual environment (e.g., using conda) and activate it; step3: Install the ZoneEnv environment dependencies by navigating to the safety-gymnasium directory and running the installation command; step4: Install the remaining project dependencies using the requirements.txt file; step5: Download and install Rabinizer 4 for LTL formula conversion, ensuring Java 11 is installed and JAVA_HOME is set; step6: Verify the Rabinizer 4 installation by running a test command; step7: (Optional) Build and run the Docker image if using Docker instead of manual installation; step8: Train a model on the ZoneEnv environment using the provided training script; step9: Evaluate the trained model using the provided evaluation scripts, optionally rendering the simulation or visualizing trajectories; step10: (Optional) Perform comprehensive evaluation on test tasks or over training time using additional scripts", "executed_cmds": "conda activate deepltl;cd src/envs/zones/safety-gymnasium;pip install -e .;pip install -r requirements.txt;./rabinizer4/bin/ltl2ldba -h;docker build -t deepltl .;mkdir experiments;docker run -it --mount type=bind;src=\"$(pwd)/experiments\"", "cmd_prefix": "target=/deep-ltl/experiments", "cmd_postfix": "deepltl", "target_cmd": "target=/deep-ltl/experiments deepltl"}
{"uuid": "45ddd2b5-29ee-4af4-b890-0a5d00507c38", "execution_plan": "step1: Create a conda environment named stem_ob with Python 3.12 and activate it; step2: Install the required dependencies using pip; step3: Perform inversion on a single image using DDPM or DDIM methods; step4: Download the demo HDF5 file and place it in the datasets folder; step5: Perform inversion on observations stored in an HDF5 file and save the results in a new HDF5 file; step6: Visualize the observations in an HDF5 file and save them in an output folder", "executed_cmds": "conda create -n stem_ob python=3.12", "cmd_prefix": "conda", "cmd_postfix": "activate stem_ob", "target_cmd": "conda activate stem_ob"}
{"uuid": "b4b08d5c-85b3-4c22-8391-a6a3f806a8e3", "execution_plan": "step1: Create a conda environment named stem_ob with Python 3.12 and activate it; step2: Install the required dependencies using pip; step3: Perform inversion on a single image using DDPM or DDIM methods; step4: Download the demo HDF5 file and place it in the datasets folder; step5: Perform inversion on observations stored in an HDF5 file and save the results in a new HDF5 file; step6: Visualize the observations in an HDF5 file and save them in an output folder", "executed_cmds": "conda create -n stem_ob python=3.12;conda activate stem_ob", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "fe4b65d2-ad2c-45fa-998b-381a84934c80", "execution_plan": "step1: Clone the repository including submodules for Learned Optimization and GROOVE; step2: Install the required dependencies listed in setup/requirements.txt; step3: Build a Docker image with the provided script, including the WandB API key for logging; step4: Run the Docker container with access to specified GPUs; step5: Train the optimizer on a specified environment using the provided training script and hyperparameters; step6: Evaluate the performance of the learned optimizer using the evaluation script, either with WandB run IDs or pretrained weights", "executed_cmds": "git clone --recurse-submodules git@github.com:AlexGoldie/rl-learned-optimization.git;pip install -r setup/requirements.txt", "cmd_prefix": "cd", "cmd_postfix": "setup", "target_cmd": "cd setup"}
{"uuid": "aa0de377-e46e-4e0c-a6c6-cde46c1fa32f", "execution_plan": "step1: Clone the repository including submodules for Learned Optimization and GROOVE; step2: Install the required dependencies listed in setup/requirements.txt; step3: Build a Docker image with the provided script, including the WandB API key for logging; step4: Run the Docker container with access to specified GPUs; step5: Train the optimizer on a specified environment using the provided training script and hyperparameters; step6: Evaluate the performance of the learned optimizer using the evaluation script, either with WandB run IDs or pretrained weights", "executed_cmds": "git clone --recurse-submodules git@github.com:AlexGoldie/rl-learned-optimization.git;pip install -r setup/requirements.txt;cd setup", "cmd_prefix": "chmod", "cmd_postfix": "+x build_docker.sh", "target_cmd": "chmod +x build_docker.sh"}
{"uuid": "4bf47ca5-4561-439e-9445-80992423c419", "execution_plan": "step1: Clone the repository including submodules for Learned Optimization and GROOVE; step2: Install the required dependencies listed in setup/requirements.txt; step3: Build a Docker image with the provided script, including the WandB API key for logging; step4: Run the Docker container with access to specified GPUs; step5: Train the optimizer on a specified environment using the provided training script and hyperparameters; step6: Evaluate the performance of the learned optimizer using the evaluation script, either with WandB run IDs or pretrained weights", "executed_cmds": "git clone --recurse-submodules git@github.com:AlexGoldie/rl-learned-optimization.git;pip install -r setup/requirements.txt;cd setup;chmod +x build_docker.sh", "cmd_prefix": "./build_docker.sh", "cmd_postfix": "{WANDB_API_KEY}", "target_cmd": "./build_docker.sh {WANDB_API_KEY}"}
{"uuid": "cd90b2ee-ebb1-4d2c-81ac-db19d2d96181", "execution_plan": "step1: Clone the repository including submodules for Learned Optimization and GROOVE; step2: Install the required dependencies listed in setup/requirements.txt; step3: Build a Docker image with the provided script, including the WandB API key for logging; step4: Run the Docker container with access to specified GPUs; step5: Train the optimizer on a specified environment using the provided training script and hyperparameters; step6: Evaluate the performance of the learned optimizer using the evaluation script, either with WandB run IDs or pretrained weights", "executed_cmds": "git clone --recurse-submodules git@github.com:AlexGoldie/rl-learned-optimization.git;pip install -r setup/requirements.txt;cd setup;chmod +x build_docker.sh;./build_docker.sh {WANDB_API_KEY}", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "e9f454e2-d24a-4749-a6a7-9ddd3e083f05", "execution_plan": "step1: Clone the repository including submodules for Learned Optimization and GROOVE; step2: Install the required dependencies listed in setup/requirements.txt; step3: Build a Docker image with the provided script, including the WandB API key for logging; step4: Run the Docker container with access to specified GPUs; step5: Train the optimizer on a specified environment using the provided training script and hyperparameters; step6: Evaluate the performance of the learned optimizer using the evaluation script, either with WandB run IDs or pretrained weights", "executed_cmds": "git clone --recurse-submodules git@github.com:AlexGoldie/rl-learned-optimization.git;pip install -r setup/requirements.txt;cd setup;chmod +x build_docker.sh;./build_docker.sh {WANDB_API_KEY};cd ..", "cmd_prefix": "chmod", "cmd_postfix": "+x run_docker.sh", "target_cmd": "chmod +x run_docker.sh"}
{"uuid": "b5cb6204-9a36-4015-b222-89e57068f57a", "execution_plan": "step1: Clone the repository including submodules for Learned Optimization and GROOVE; step2: Install the required dependencies listed in setup/requirements.txt; step3: Build a Docker image with the provided script, including the WandB API key for logging; step4: Run the Docker container with access to specified GPUs; step5: Train the optimizer on a specified environment using the provided training script and hyperparameters; step6: Evaluate the performance of the learned optimizer using the evaluation script, either with WandB run IDs or pretrained weights", "executed_cmds": "git clone --recurse-submodules git@github.com:AlexGoldie/rl-learned-optimization.git;pip install -r setup/requirements.txt;cd setup;chmod +x build_docker.sh;./build_docker.sh {WANDB_API_KEY};cd ..;chmod +x run_docker.sh", "cmd_prefix": "./run_docker.sh", "cmd_postfix": "{GPU_NAMES}", "target_cmd": "./run_docker.sh {GPU_NAMES}"}
{"uuid": "2622d349-8d5f-4541-adc1-25b3310062c6", "execution_plan": "step1: Clone the repository to your local machine; step2: Install Git LFS and download the required dataset; step3: Tokenize the downloaded dataset; step4: Pretrain a small base model with a specified number of tokens; step5: Create a PRTS configuration file for the growth operator; step6: Start continual pretraining using the specified growth operator", "executed_cmds": "git clone https://github.com/tongxuluo/prts.git", "cmd_prefix": "cd", "cmd_postfix": "prts", "target_cmd": "cd prts"}
{"uuid": "cbc45946-7759-4160-bb8d-0231eeb7cea3", "execution_plan": "step1: Clone the repository to your local machine; step2: Install Git LFS and download the required dataset; step3: Tokenize the downloaded dataset; step4: Pretrain a small base model with a specified number of tokens; step5: Create a PRTS configuration file for the growth operator; step6: Start continual pretraining using the specified growth operator", "executed_cmds": "git clone https://github.com/tongxuluo/prts.git;cd prts", "cmd_prefix": "git", "cmd_postfix": "lfs install", "target_cmd": "git lfs install"}
{"uuid": "766dcf80-2726-43f6-b7a0-3f297237c2d4", "execution_plan": "step1: Clone the repository to your local machine; step2: Install Git LFS and download the required dataset; step3: Tokenize the downloaded dataset; step4: Pretrain a small base model with a specified number of tokens; step5: Create a PRTS configuration file for the growth operator; step6: Start continual pretraining using the specified growth operator", "executed_cmds": "git clone https://github.com/tongxuluo/prts.git;cd prts;git lfs install;git clone https://huggingface.co/datasets/cerebras/SlimPajama-627B;python scripts/prepare_slimpajama.py --source_path /path/to/SlimPajama --tokenizer_path path/to/llama --destination_path data/slimpajama --split validation --percentage 1.0;python scripts/prepare_slimpajama.py --source_path /path/to/SlimPajama --tokenizer_path path/to/llama --destination_path data/slimpajama --split train --percentage 1.0", "cmd_prefix": "sbatch", "cmd_postfix": "base_model.sh", "target_cmd": "sbatch base_model.sh"}
{"uuid": "63ebbfd0-00c4-48aa-be6e-99d521994069", "execution_plan": "step1: Clone the repository to your local machine; step2: Install Git LFS and download the required dataset; step3: Tokenize the downloaded dataset; step4: Pretrain a small base model with a specified number of tokens; step5: Create a PRTS configuration file for the growth operator; step6: Start continual pretraining using the specified growth operator", "executed_cmds": "git clone https://github.com/tongxuluo/prts.git;cd prts;git lfs install;git clone https://huggingface.co/datasets/cerebras/SlimPajama-627B;python scripts/prepare_slimpajama.py --source_path /path/to/SlimPajama --tokenizer_path path/to/llama --destination_path data/slimpajama --split validation --percentage 1.0;python scripts/prepare_slimpajama.py --source_path /path/to/SlimPajama --tokenizer_path path/to/llama --destination_path data/slimpajama --split train --percentage 1.0;sbatch base_model.sh", "cmd_prefix": "sbatch", "cmd_postfix": "g_stack.sh", "target_cmd": "sbatch g_stack.sh"}
{"uuid": "75a829a9-441b-4d85-982e-db307d1d3cfb", "execution_plan": "step1: Create necessary directories for storing images and results; step2: Run the synthetic problem experiments with different parameters; step3: Create a directory for storing datasets for fairness machine learning; step4: Download the required datasets and place them in the created directory; step5: Run the fairness machine learning experiments with different datasets and parameters", "executed_cmds": "mkdir img", "cmd_prefix": "mkdir", "cmd_postfix": "result", "target_cmd": "mkdir result"}
{"uuid": "bd4132b7-2b0c-4969-8579-fddbed37f8f4", "execution_plan": "step1: Create necessary directories for storing images and results; step2: Run the synthetic problem experiments with different parameters; step3: Create a directory for storing datasets for fairness machine learning; step4: Download the required datasets and place them in the created directory; step5: Run the fairness machine learning experiments with different datasets and parameters", "executed_cmds": "mkdir img;mkdir result;python -u Synthetic.py --n 10 --training_time 10.0;python -u Synthetic.py --n 100 --training_time 50.0;python -u Synthetic.py --n 100 --training_time 80.0", "cmd_prefix": "mkdir", "cmd_postfix": "Data", "target_cmd": "mkdir Data"}
{"uuid": "f59e35ba-659d-42d1-97ea-0cf8e86c5a25", "execution_plan": "step1: Install all required Python dependencies to ensure the project can run properly; step2: Run the training script to start training the model using the provided configuration file", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "python train.py", "cmd_postfix": "--config config.yaml", "target_cmd": "python train.py --config config.yaml"}
{"uuid": "acef8fdb-4f6c-4007-825f-fcc25b69737f", "execution_plan": "step1: Understand the project's purpose and scope, which involves artificial neural networks (ANNs), decision trees, support vector machines, regression analysis, Bayesian networks, genetic algorithms, and other machine learning concepts; step2: Identify the required software or tools mentioned, such as Python, TensorFlow, or other machine learning libraries; step3: Install necessary dependencies or libraries for the project; step4: Prepare the dataset or input data required for training or testing the models; step5: Run the training or inference scripts to execute the machine learning models; step6: Evaluate the model performance using metrics like accuracy, sensitivity, specificity, or other relevant criteria; step7: Address any ethical considerations or biases in the dataset or model outputs; step8: Optimize or fine-tune the models based on evaluation results", "executed_cmds": "pip install tensorflow", "cmd_prefix": "pip", "cmd_postfix": "install scikit-learn", "target_cmd": "pip install scikit-learn"}
{"uuid": "e2246d24-de89-4ae2-8013-d12356bbdbfd", "execution_plan": "step1: Understand the project's purpose and scope, which involves artificial neural networks (ANNs), decision trees, support vector machines, regression analysis, Bayesian networks, genetic algorithms, and other machine learning concepts; step2: Identify the required software or tools mentioned, such as Python, TensorFlow, or other machine learning libraries; step3: Install necessary dependencies or libraries for the project; step4: Prepare the dataset or input data required for training or testing the models; step5: Run the training or inference scripts to execute the machine learning models; step6: Evaluate the model performance using metrics like accuracy, sensitivity, specificity, or other relevant criteria; step7: Address any ethical considerations or biases in the dataset or model outputs; step8: Optimize or fine-tune the models based on evaluation results", "executed_cmds": "pip install tensorflow;pip install scikit-learn", "cmd_prefix": "python", "cmd_postfix": "train.py", "target_cmd": "python train.py"}
{"uuid": "484473bc-397a-42e7-b120-cc0257142d91", "execution_plan": "step1: Understand the project's purpose and scope, which involves artificial neural networks (ANNs), decision trees, support vector machines, regression analysis, Bayesian networks, genetic algorithms, and other machine learning concepts; step2: Identify the required software or tools mentioned, such as Python, TensorFlow, or other machine learning libraries; step3: Install necessary dependencies or libraries for the project; step4: Prepare the dataset or input data required for training or testing the models; step5: Run the training or inference scripts to execute the machine learning models; step6: Evaluate the model performance using metrics like accuracy, sensitivity, specificity, or other relevant criteria; step7: Address any ethical considerations or biases in the dataset or model outputs; step8: Optimize or fine-tune the models based on evaluation results", "executed_cmds": "pip install tensorflow;pip install scikit-learn;python train.py", "cmd_prefix": "python", "cmd_postfix": "inference.py", "target_cmd": "python inference.py"}
{"uuid": "f32bdd59-01ec-47fe-b6eb-e1a54e6984f5", "execution_plan": "step1: Understand the repository structure and the two main experiments provided; step2: Run the online mirror descent and online gradient descent experiment to replicate Fig. 2 results; step3: Perform the randomized shuffle control for significance analysis in the online mirror descent experiment; step4: Run the fully connected neural networks (FCN) experiment on MNIST to generate training trajectories; step5: Generate plots for the FCN experiment analogous to Fig. 3 of the paper; step6: Replicate Fig. 3D-F by plotting Wasserstein distance for the FCN experiment", "executed_cmds": "python online_mirror_online_gradient_descent_main.py;python online_mirror_online_gradient_descent_computing_significance.py", "cmd_prefix": "python", "cmd_postfix": "FCN_MNIST_main.py", "target_cmd": "python FCN_MNIST_main.py"}
{"uuid": "18868c8c-86d7-4db4-b2e3-f849197d2b39", "execution_plan": "step1: Clone the gRNAde repository to your preferred location; step2: Install mamba (a faster conda) and create a new Python environment; step3: Install PyTorch and PyTorch Geometric (PyG) with matching CUDA version; step4: Install other compulsory dependencies including jupyterlab, matplotlib, seaborn, pandas, biopython, biotite, etc.; step5: Install X3DNA for secondary structure determination; step6: Install EternaFold for secondary structure prediction; step7: Download RhoFold checkpoint; step8: Create and configure the `.env` file; step9: Download and process raw RNA structures from RNAsolo for training models (optional); step10: Process raw PDB files into ML-ready format (optional); step11: Use gRNAde via the provided tutorial notebook", "executed_cmds": "git clone https://github.com/chaitjo/geometric-rna-design.git;wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh;bash Miniforge3-Linux-x86_64.sh", "cmd_prefix": "source", "cmd_postfix": "~/.bashrc", "target_cmd": "source ~/.bashrc"}
{"uuid": "13372f09-4fc9-4e90-abec-2bad1213ea15", "execution_plan": "step1: Clone the gRNAde repository to your preferred location; step2: Install mamba (a faster conda) and create a new Python environment; step3: Install PyTorch and PyTorch Geometric (PyG) with matching CUDA version; step4: Install other compulsory dependencies including jupyterlab, matplotlib, seaborn, pandas, biopython, biotite, etc.; step5: Install X3DNA for secondary structure determination; step6: Install EternaFold for secondary structure prediction; step7: Download RhoFold checkpoint; step8: Create and configure the `.env` file; step9: Download and process raw RNA structures from RNAsolo for training models (optional); step10: Process raw PDB files into ML-ready format (optional); step11: Use gRNAde via the provided tutorial notebook", "executed_cmds": "git clone https://github.com/chaitjo/geometric-rna-design.git;wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh;bash Miniforge3-Linux-x86_64.sh;source ~/.bashrc", "cmd_prefix": "mamba create", "cmd_postfix": "-n rna python=3.10", "target_cmd": "mamba create -n rna python=3.10"}
{"uuid": "dc91ac6c-d32f-4dc9-9b6b-9ae6b9f2c3fa", "execution_plan": "step1: Clone the gRNAde repository to your preferred location; step2: Install mamba (a faster conda) and create a new Python environment; step3: Install PyTorch and PyTorch Geometric (PyG) with matching CUDA version; step4: Install other compulsory dependencies including jupyterlab, matplotlib, seaborn, pandas, biopython, biotite, etc.; step5: Install X3DNA for secondary structure determination; step6: Install EternaFold for secondary structure prediction; step7: Download RhoFold checkpoint; step8: Create and configure the `.env` file; step9: Download and process raw RNA structures from RNAsolo for training models (optional); step10: Process raw PDB files into ML-ready format (optional); step11: Use gRNAde via the provided tutorial notebook", "executed_cmds": "git clone https://github.com/chaitjo/geometric-rna-design.git;wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh;bash Miniforge3-Linux-x86_64.sh;source ~/.bashrc;mamba create -n rna python=3.10", "cmd_prefix": "mamba", "cmd_postfix": "activate rna", "target_cmd": "mamba activate rna"}
{"uuid": "7d9ce128-e90e-48a1-a293-f6874bce2783", "execution_plan": "step1: Clone the gRNAde repository to your preferred location; step2: Install mamba (a faster conda) and create a new Python environment; step3: Install PyTorch and PyTorch Geometric (PyG) with matching CUDA version; step4: Install other compulsory dependencies including jupyterlab, matplotlib, seaborn, pandas, biopython, biotite, etc.; step5: Install X3DNA for secondary structure determination; step6: Install EternaFold for secondary structure prediction; step7: Download RhoFold checkpoint; step8: Create and configure the `.env` file; step9: Download and process raw RNA structures from RNAsolo for training models (optional); step10: Process raw PDB files into ML-ready format (optional); step11: Use gRNAde via the provided tutorial notebook", "executed_cmds": "git clone https://github.com/chaitjo/geometric-rna-design.git;wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh;bash Miniforge3-Linux-x86_64.sh;source ~/.bashrc;mamba create -n rna python=3.10;mamba activate rna;mamba install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia;pip install torch_geometric;pip install torch_scatter torch_cluster -f https://data.pyg.org/whl/torch-2.1.2+cu118.html;mamba install jupyterlab matplotlib seaborn pandas biopython biotite -c conda-forge;pip install wandb gdown pyyaml ipdb python-dotenv tqdm cpdb-protein torchmetrics einops ml_collections mdanalysis MDAnalysisTests draw_rna arnie;cd ~/geometric-rna-design/tools/;tar -xvzf x3dna-v2.4-linux-64bit.tar.gz;./x3dna-v2.4/bin/x3dna_setup;cd ~/geometric-rna-design/tools/;git clone --depth=1 https://github.com/eternagame/EternaFold.git && cd EternaFold/src;make;cd ~/geometric-rna-design/tools/rhofold/;gdown https://drive.google.com/uc?id=1To2bjbhQLFx1k8hBOW5q1JFq6ut27XEv;cd ~/geometric-rna-design/", "cmd_prefix": "touch", "cmd_postfix": ".env", "target_cmd": "touch .env"}
{"uuid": "9381fbeb-0918-4bda-8ab6-1682f11c7253", "execution_plan": "step1: Clone the gRNAde repository to your preferred location; step2: Install mamba (a faster conda) and create a new Python environment; step3: Install PyTorch and PyTorch Geometric (PyG) with matching CUDA version; step4: Install other compulsory dependencies including jupyterlab, matplotlib, seaborn, pandas, biopython, biotite, etc.; step5: Install X3DNA for secondary structure determination; step6: Install EternaFold for secondary structure prediction; step7: Download RhoFold checkpoint; step8: Create and configure the `.env` file; step9: Download and process raw RNA structures from RNAsolo for training models (optional); step10: Process raw PDB files into ML-ready format (optional); step11: Use gRNAde via the provided tutorial notebook", "executed_cmds": "git clone https://github.com/chaitjo/geometric-rna-design.git;wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh;bash Miniforge3-Linux-x86_64.sh;source ~/.bashrc;mamba create -n rna python=3.10;mamba activate rna;mamba install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia;pip install torch_geometric;pip install torch_scatter torch_cluster -f https://data.pyg.org/whl/torch-2.1.2+cu118.html;mamba install jupyterlab matplotlib seaborn pandas biopython biotite -c conda-forge;pip install wandb gdown pyyaml ipdb python-dotenv tqdm cpdb-protein torchmetrics einops ml_collections mdanalysis MDAnalysisTests draw_rna arnie;cd ~/geometric-rna-design/tools/;tar -xvzf x3dna-v2.4-linux-64bit.tar.gz;./x3dna-v2.4/bin/x3dna_setup;cd ~/geometric-rna-design/tools/;git clone --depth=1 https://github.com/eternagame/EternaFold.git && cd EternaFold/src;make;cd ~/geometric-rna-design/tools/rhofold/;gdown https://drive.google.com/uc?id=1To2bjbhQLFx1k8hBOW5q1JFq6ut27XEv;cd ~/geometric-rna-design/;touch .env;mkdir ~/geometric-rna-design/data/raw;cd ~/geometric-rna-design/data/raw;gdown https://drive.google.com/uc?id=10NidhkkJ-rkbqDwBGA_GaXs9enEBJ7iQ;tar -zxvf RNAsolo_31102023.tar.gz;cd ~/geometric-rna-design/", "cmd_prefix": "python", "cmd_postfix": "data/process_data.py", "target_cmd": "python data/process_data.py"}
{"uuid": "81fb3783-2fc7-48cb-bc29-43ef60388f84", "execution_plan": "step1: Understand the repository structure and the purpose of each file; step2: Simulate the dynamics under given model parameters and memory strength using the cell_model_pop_fde_slow_sde.py file; step3: Use the Gymnasium environment for RL by running the cell_env.py file; step4: Run the Double DQN extension using the ddqn.py file; step5: Configure and run hyperparameter sweeps using wandb_sweep.py and hparams.yaml; step6: Test finetuned hyperparameters locally using local_run.py; step7: Run the aggregated best models for each value of mu (alpha in the code) using run_models.py; step8: Visualize experiment results using plot_data.ipynb; step9: Run the baseline tests using sde_test.ipynb and switching_at_const_frac.ipynb", "executed_cmds": "python cell_model_pop_fde_slow_sde.py", "cmd_prefix": "python", "cmd_postfix": "cell_env.py", "target_cmd": "python cell_env.py"}
{"uuid": "0a37e12b-c41a-4c9f-9e25-e8440f0221e6", "execution_plan": "step1: Understand the repository structure and the purpose of each file; step2: Simulate the dynamics under given model parameters and memory strength using the cell_model_pop_fde_slow_sde.py file; step3: Use the Gymnasium environment for RL by running the cell_env.py file; step4: Run the Double DQN extension using the ddqn.py file; step5: Configure and run hyperparameter sweeps using wandb_sweep.py and hparams.yaml; step6: Test finetuned hyperparameters locally using local_run.py; step7: Run the aggregated best models for each value of mu (alpha in the code) using run_models.py; step8: Visualize experiment results using plot_data.ipynb; step9: Run the baseline tests using sde_test.ipynb and switching_at_const_frac.ipynb", "executed_cmds": "python cell_model_pop_fde_slow_sde.py;python cell_env.py", "cmd_prefix": "python", "cmd_postfix": "ddqn.py", "target_cmd": "python ddqn.py"}
{"uuid": "97308530-4bde-4439-b0c4-a88b37fbb31f", "execution_plan": "step1: Understand the repository structure and the purpose of each file; step2: Simulate the dynamics under given model parameters and memory strength using the cell_model_pop_fde_slow_sde.py file; step3: Use the Gymnasium environment for RL by running the cell_env.py file; step4: Run the Double DQN extension using the ddqn.py file; step5: Configure and run hyperparameter sweeps using wandb_sweep.py and hparams.yaml; step6: Test finetuned hyperparameters locally using local_run.py; step7: Run the aggregated best models for each value of mu (alpha in the code) using run_models.py; step8: Visualize experiment results using plot_data.ipynb; step9: Run the baseline tests using sde_test.ipynb and switching_at_const_frac.ipynb", "executed_cmds": "python cell_model_pop_fde_slow_sde.py;python cell_env.py;python ddqn.py", "cmd_prefix": "python", "cmd_postfix": "wandb_sweep.py", "target_cmd": "python wandb_sweep.py"}
{"uuid": "0c2cc9c5-bc40-4a74-a51c-09c165e3774f", "execution_plan": "step1: Understand the repository structure and the purpose of each file; step2: Simulate the dynamics under given model parameters and memory strength using the cell_model_pop_fde_slow_sde.py file; step3: Use the Gymnasium environment for RL by running the cell_env.py file; step4: Run the Double DQN extension using the ddqn.py file; step5: Configure and run hyperparameter sweeps using wandb_sweep.py and hparams.yaml; step6: Test finetuned hyperparameters locally using local_run.py; step7: Run the aggregated best models for each value of mu (alpha in the code) using run_models.py; step8: Visualize experiment results using plot_data.ipynb; step9: Run the baseline tests using sde_test.ipynb and switching_at_const_frac.ipynb", "executed_cmds": "python cell_model_pop_fde_slow_sde.py;python cell_env.py;python ddqn.py;python wandb_sweep.py", "cmd_prefix": "python", "cmd_postfix": "local_run.py", "target_cmd": "python local_run.py"}
{"uuid": "2834702f-23ab-47d6-8e4a-5257bafa47f0", "execution_plan": "step1: Understand the repository structure and the purpose of each file; step2: Simulate the dynamics under given model parameters and memory strength using the cell_model_pop_fde_slow_sde.py file; step3: Use the Gymnasium environment for RL by running the cell_env.py file; step4: Run the Double DQN extension using the ddqn.py file; step5: Configure and run hyperparameter sweeps using wandb_sweep.py and hparams.yaml; step6: Test finetuned hyperparameters locally using local_run.py; step7: Run the aggregated best models for each value of mu (alpha in the code) using run_models.py; step8: Visualize experiment results using plot_data.ipynb; step9: Run the baseline tests using sde_test.ipynb and switching_at_const_frac.ipynb", "executed_cmds": "python cell_model_pop_fde_slow_sde.py;python cell_env.py;python ddqn.py;python wandb_sweep.py;python local_run.py", "cmd_prefix": "python", "cmd_postfix": "run_models.py", "target_cmd": "python run_models.py"}
{"uuid": "d0c7cb94-a8b5-417c-8430-49801bb46b9a", "execution_plan": "step1: Clone the MonST3R repository and initialize submodules; step2: Create and set up a conda environment with necessary dependencies; step3: Install optional visualization tools and compile CUDA kernels for RoPE; step4: Download the pre-trained model weights and optical flow models; step5: Run the inference code to process a video or folder of images; step6: Visualize the interactive 4D results using the viser tool; step7: Evaluate the model on the DAVIS dataset; step8: Train the model using the provided training script", "executed_cmds": "git clone --recursive https://github.com/junyi42/monst3r", "cmd_prefix": "cd", "cmd_postfix": "monst3r", "target_cmd": "cd monst3r"}
{"uuid": "8ee11b7d-4894-4775-8195-53b6444616b0", "execution_plan": "step1: Clone the MonST3R repository and initialize submodules; step2: Create and set up a conda environment with necessary dependencies; step3: Install optional visualization tools and compile CUDA kernels for RoPE; step4: Download the pre-trained model weights and optical flow models; step5: Run the inference code to process a video or folder of images; step6: Visualize the interactive 4D results using the viser tool; step7: Evaluate the model on the DAVIS dataset; step8: Train the model using the provided training script", "executed_cmds": "git clone --recursive https://github.com/junyi42/monst3r;cd monst3r;conda create -n monst3r python=3.11 cmake=3.14.0", "cmd_prefix": "conda", "cmd_postfix": "activate monst3r", "target_cmd": "conda activate monst3r"}
{"uuid": "be350b49-7a0c-438e-8c93-d667b64540ad", "execution_plan": "step1: Clone the MonST3R repository and initialize submodules; step2: Create and set up a conda environment with necessary dependencies; step3: Install optional visualization tools and compile CUDA kernels for RoPE; step4: Download the pre-trained model weights and optical flow models; step5: Run the inference code to process a video or folder of images; step6: Visualize the interactive 4D results using the viser tool; step7: Evaluate the model on the DAVIS dataset; step8: Train the model using the provided training script", "executed_cmds": "git clone --recursive https://github.com/junyi42/monst3r;cd monst3r;conda create -n monst3r python=3.11 cmake=3.14.0;conda activate monst3r;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "2e80bf82-df17-4f45-933e-7a927d728dc6", "execution_plan": "step1: Clone the MonST3R repository and initialize submodules; step2: Create and set up a conda environment with necessary dependencies; step3: Install optional visualization tools and compile CUDA kernels for RoPE; step4: Download the pre-trained model weights and optical flow models; step5: Run the inference code to process a video or folder of images; step6: Visualize the interactive 4D results using the viser tool; step7: Evaluate the model on the DAVIS dataset; step8: Train the model using the provided training script", "executed_cmds": "git clone --recursive https://github.com/junyi42/monst3r;cd monst3r;conda create -n monst3r python=3.11 cmake=3.14.0;conda activate monst3r;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt", "cmd_prefix": "pip install", "cmd_postfix": "-e viser", "target_cmd": "pip install -e viser"}
{"uuid": "86d5e32a-307c-4055-9eb5-d56f59f8f76b", "execution_plan": "step1: Clone the MonST3R repository and initialize submodules; step2: Create and set up a conda environment with necessary dependencies; step3: Install optional visualization tools and compile CUDA kernels for RoPE; step4: Download the pre-trained model weights and optical flow models; step5: Run the inference code to process a video or folder of images; step6: Visualize the interactive 4D results using the viser tool; step7: Evaluate the model on the DAVIS dataset; step8: Train the model using the provided training script", "executed_cmds": "git clone --recursive https://github.com/junyi42/monst3r;cd monst3r;conda create -n monst3r python=3.11 cmake=3.14.0;conda activate monst3r;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install -e viser", "cmd_prefix": "cd", "cmd_postfix": "croco/models/curope/", "target_cmd": "cd croco/models/curope/"}
{"uuid": "4ea04b95-6b56-4311-833f-ad48f2a049cb", "execution_plan": "step1: Clone the MonST3R repository and initialize submodules; step2: Create and set up a conda environment with necessary dependencies; step3: Install optional visualization tools and compile CUDA kernels for RoPE; step4: Download the pre-trained model weights and optical flow models; step5: Run the inference code to process a video or folder of images; step6: Visualize the interactive 4D results using the viser tool; step7: Evaluate the model on the DAVIS dataset; step8: Train the model using the provided training script", "executed_cmds": "git clone --recursive https://github.com/junyi42/monst3r;cd monst3r;conda create -n monst3r python=3.11 cmake=3.14.0;conda activate monst3r;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install -e viser;cd croco/models/curope/", "cmd_prefix": "python setup.py", "cmd_postfix": "build_ext --inplace", "target_cmd": "python setup.py build_ext --inplace"}
{"uuid": "180602f8-bb2c-4c44-8de3-089d95740110", "execution_plan": "step1: Clone the MonST3R repository and initialize submodules; step2: Create and set up a conda environment with necessary dependencies; step3: Install optional visualization tools and compile CUDA kernels for RoPE; step4: Download the pre-trained model weights and optical flow models; step5: Run the inference code to process a video or folder of images; step6: Visualize the interactive 4D results using the viser tool; step7: Evaluate the model on the DAVIS dataset; step8: Train the model using the provided training script", "executed_cmds": "git clone --recursive https://github.com/junyi42/monst3r;cd monst3r;conda create -n monst3r python=3.11 cmake=3.14.0;conda activate monst3r;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install -e viser;cd croco/models/curope/;python setup.py build_ext --inplace", "cmd_prefix": "cd", "cmd_postfix": "../../../", "target_cmd": "cd ../../../"}
{"uuid": "189ffefb-c1eb-4b64-8eab-1862d4d7e5ec", "execution_plan": "step1: Clone the MonST3R repository and initialize submodules; step2: Create and set up a conda environment with necessary dependencies; step3: Install optional visualization tools and compile CUDA kernels for RoPE; step4: Download the pre-trained model weights and optical flow models; step5: Run the inference code to process a video or folder of images; step6: Visualize the interactive 4D results using the viser tool; step7: Evaluate the model on the DAVIS dataset; step8: Train the model using the provided training script", "executed_cmds": "git clone --recursive https://github.com/junyi42/monst3r;cd monst3r;conda create -n monst3r python=3.11 cmake=3.14.0;conda activate monst3r;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install -e viser;cd croco/models/curope/;python setup.py build_ext --inplace;cd ../../../", "cmd_prefix": "cd", "cmd_postfix": "data", "target_cmd": "cd data"}
{"uuid": "3ad7c442-1b08-4ecd-81e4-34f017071dfc", "execution_plan": "step1: Clone the MonST3R repository and initialize submodules; step2: Create and set up a conda environment with necessary dependencies; step3: Install optional visualization tools and compile CUDA kernels for RoPE; step4: Download the pre-trained model weights and optical flow models; step5: Run the inference code to process a video or folder of images; step6: Visualize the interactive 4D results using the viser tool; step7: Evaluate the model on the DAVIS dataset; step8: Train the model using the provided training script", "executed_cmds": "git clone --recursive https://github.com/junyi42/monst3r;cd monst3r;conda create -n monst3r python=3.11 cmake=3.14.0;conda activate monst3r;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install -e viser;cd croco/models/curope/;python setup.py build_ext --inplace;cd ../../../;cd data", "cmd_prefix": "bash", "cmd_postfix": "download_ckpt.sh", "target_cmd": "bash download_ckpt.sh"}
{"uuid": "2976ec44-e1a9-4d1a-b8bb-201e11b099b7", "execution_plan": "step1: Clone the MonST3R repository and initialize submodules; step2: Create and set up a conda environment with necessary dependencies; step3: Install optional visualization tools and compile CUDA kernels for RoPE; step4: Download the pre-trained model weights and optical flow models; step5: Run the inference code to process a video or folder of images; step6: Visualize the interactive 4D results using the viser tool; step7: Evaluate the model on the DAVIS dataset; step8: Train the model using the provided training script", "executed_cmds": "git clone --recursive https://github.com/junyi42/monst3r;cd monst3r;conda create -n monst3r python=3.11 cmake=3.14.0;conda activate monst3r;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install -e viser;cd croco/models/curope/;python setup.py build_ext --inplace;cd ../../../;cd data;bash download_ckpt.sh", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "9387d0bd-552c-42e7-8f46-8627a4ac5f1c", "execution_plan": "step1: Clone the MonST3R repository and initialize submodules; step2: Create and set up a conda environment with necessary dependencies; step3: Install optional visualization tools and compile CUDA kernels for RoPE; step4: Download the pre-trained model weights and optical flow models; step5: Run the inference code to process a video or folder of images; step6: Visualize the interactive 4D results using the viser tool; step7: Evaluate the model on the DAVIS dataset; step8: Train the model using the provided training script", "executed_cmds": "git clone --recursive https://github.com/junyi42/monst3r;cd monst3r;conda create -n monst3r python=3.11 cmake=3.14.0;conda activate monst3r;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia;pip install -r requirements.txt;pip install -e viser;cd croco/models/curope/;python setup.py build_ext --inplace;cd ../../../;cd data;bash download_ckpt.sh;cd ..", "cmd_prefix": "python", "cmd_postfix": "demo.py", "target_cmd": "python demo.py"}
{"uuid": "50bf64fe-9ac0-40bc-a8d2-9a723eb9b731", "execution_plan": "step1: Install the required dependencies (uv, just, gdown); step2: Create the output directory for storing experiment results; step3: Run tests to verify the setup; step4: Run synthetic binomial experiments with a specified method (crc, rcps, or hpd); step5: Analyze the results of synthetic binomial experiments; step6: Run synthetic heteroskedastic experiments with a specified method (crc, rcps, or hpd); step7: Analyze the results of synthetic heteroskedastic experiments; step8: Download MS-COCO data for experiments; step9: Run MS-COCO experiments with a specified method (crc, rcps, or hpd); step10: Analyze the results of MS-COCO experiments", "executed_cmds": "mkdir output", "cmd_prefix": "just", "cmd_postfix": "test", "target_cmd": "just test"}
{"uuid": "e53fc298-1e83-4a37-8f60-7a6439b4c991", "execution_plan": "step1: Install the required dependencies (uv, just, gdown); step2: Create the output directory for storing experiment results; step3: Run tests to verify the setup; step4: Run synthetic binomial experiments with a specified method (crc, rcps, or hpd); step5: Analyze the results of synthetic binomial experiments; step6: Run synthetic heteroskedastic experiments with a specified method (crc, rcps, or hpd); step7: Analyze the results of synthetic heteroskedastic experiments; step8: Download MS-COCO data for experiments; step9: Run MS-COCO experiments with a specified method (crc, rcps, or hpd); step10: Analyze the results of MS-COCO experiments", "executed_cmds": "mkdir output;just test", "cmd_prefix": "just", "cmd_postfix": "synth-run {method}", "target_cmd": "just synth-run {method}"}
{"uuid": "0e1d2f12-30d2-4c25-84a6-54f44670c55e", "execution_plan": "step1: Install the required dependencies (uv, just, gdown); step2: Create the output directory for storing experiment results; step3: Run tests to verify the setup; step4: Run synthetic binomial experiments with a specified method (crc, rcps, or hpd); step5: Analyze the results of synthetic binomial experiments; step6: Run synthetic heteroskedastic experiments with a specified method (crc, rcps, or hpd); step7: Analyze the results of synthetic heteroskedastic experiments; step8: Download MS-COCO data for experiments; step9: Run MS-COCO experiments with a specified method (crc, rcps, or hpd); step10: Analyze the results of MS-COCO experiments", "executed_cmds": "mkdir output;just test;just synth-run {method};just synth-analyze {method};just heteroskedastic-run {method};just heteroskedastic-analyze {method}", "cmd_prefix": "just", "cmd_postfix": "fetch", "target_cmd": "just fetch"}
{"uuid": "83aeedc0-140f-4aed-b7ec-ee81da21b7fd", "execution_plan": "step1: Install the required dependencies (uv, just, gdown); step2: Create the output directory for storing experiment results; step3: Run tests to verify the setup; step4: Run synthetic binomial experiments with a specified method (crc, rcps, or hpd); step5: Analyze the results of synthetic binomial experiments; step6: Run synthetic heteroskedastic experiments with a specified method (crc, rcps, or hpd); step7: Analyze the results of synthetic heteroskedastic experiments; step8: Download MS-COCO data for experiments; step9: Run MS-COCO experiments with a specified method (crc, rcps, or hpd); step10: Analyze the results of MS-COCO experiments", "executed_cmds": "mkdir output;just test;just synth-run {method};just synth-analyze {method};just heteroskedastic-run {method};just heteroskedastic-analyze {method};just fetch", "cmd_prefix": "just", "cmd_postfix": "coco-run {method}", "target_cmd": "just coco-run {method}"}
{"uuid": "cf55c147-3204-416c-9121-edcde97d4343", "execution_plan": "step1: Install the required libraries by running the installation script; step2: Download the appropriate checkpoints from Google Drive based on your use case (face deepfake detection or general AI-generated image detection); step3: Run the demo script to perform inference on a single image or a folder of images using the pretrained weights; step4: (Optional) Download and preprocess datasets if you want to reproduce the results; step5: (Optional) Rearrange the dataset structure by creating JSON files for each dataset; step6: (Optional) Train the model using the provided training script, either on multiple GPUs or a single GPU; step7: (Optional) Test the trained model on various deepfake datasets to evaluate performance", "executed_cmds": "sh install.sh", "cmd_prefix": "cd", "cmd_postfix": "DeepfakeBench/", "target_cmd": "cd DeepfakeBench/"}
{"uuid": "df3d6a85-836a-49f0-9269-d629982bdda5", "execution_plan": "step1: Clone the repository to your local machine to access the code and data; step2: Navigate to the `src` directory to access the main code files, including `p_mean.py` and `portfolio.py`; step3: Explore the `environments` directory to understand the implementation of the environments used in the experiments; step4: Run the Jupyter notebooks in the `notebooks` directory to reproduce the results for each environment (Natural Disaster, Healthcare Intervention, Taxi); step5: Generate plots from the paper using the provided Jupyter notebooks in the `notebooks` directory; step6: Review the generated policies and plots stored in the `data` directory for each environment", "executed_cmds": "git clone [repository_url]", "cmd_prefix": "cd", "cmd_postfix": "src", "target_cmd": "cd src"}
{"uuid": "bc4ce1f1-6ab9-4081-9d50-3601946454ee", "execution_plan": "step1: Clone the repository to your local machine to access the code and data; step2: Navigate to the `src` directory to access the main code files, including `p_mean.py` and `portfolio.py`; step3: Explore the `environments` directory to understand the implementation of the environments used in the experiments; step4: Run the Jupyter notebooks in the `notebooks` directory to reproduce the results for each environment (Natural Disaster, Healthcare Intervention, Taxi); step5: Generate plots from the paper using the provided Jupyter notebooks in the `notebooks` directory; step6: Review the generated policies and plots stored in the `data` directory for each environment", "executed_cmds": "git clone [repository_url];cd src", "cmd_prefix": "cd", "cmd_postfix": "src/environments", "target_cmd": "cd src/environments"}
{"uuid": "674eccb1-eb80-4a56-8f62-fdb6f77f538f", "execution_plan": "step1: Clone the repository to your local machine; step2: Install the required dependencies by setting up a virtual environment; step3: Download the necessary datasets (MNIST or Ave, Celeba!); step4: Place the downloaded datasets in the appropriate subfolder within the `data/` directory; step5: Download the StyleGAN2 model if running experiments in Image space; step6: Place the StyleGAN2 model in the appropriate subfolder within the `SG2_ckpt/` directory; step7: Run the desired Jupyter notebook for training or evaluation", "executed_cmds": "git clone https://github.com/justkolesov/EnergyGuidedBarycenters.git", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "51484838-d4a4-44e7-8065-e4b24767187c", "execution_plan": "step1: Set up the environment using Anaconda and install necessary dependencies; step2: Download pretrained models and FID reference sets; step3: Generate training data for LD3 using the teacher solver for CIFAR-10; step4: Generate training data for LD3 using the teacher solver for Stable Diffusion; step5: Train LD3 on the generated training data for CIFAR-10; step6: Compute FID for Stable Diffusion using custom timesteps; step7: Compute FID for Stable Diffusion using default timesteps", "executed_cmds": "conda env create -f requirements.yml", "cmd_prefix": "conda", "cmd_postfix": "activate ld3", "target_cmd": "conda activate ld3"}
{"uuid": "54ae9c8b-a4ce-4eef-87ec-fc41ba7c87bb", "execution_plan": "step1: Set up the environment using Anaconda and install necessary dependencies; step2: Download pretrained models and FID reference sets; step3: Generate training data for LD3 using the teacher solver for CIFAR-10; step4: Generate training data for LD3 using the teacher solver for Stable Diffusion; step5: Train LD3 on the generated training data for CIFAR-10; step6: Compute FID for Stable Diffusion using custom timesteps; step7: Compute FID for Stable Diffusion using default timesteps", "executed_cmds": "conda env create -f requirements.yml;conda activate ld3", "cmd_prefix": "pip install", "cmd_postfix": "-e ./src/clip/", "target_cmd": "pip install -e ./src/clip/"}
{"uuid": "3cbb9b10-67b1-4bc3-9954-3a814140f2a2", "execution_plan": "step1: Set up the environment using Anaconda and install necessary dependencies; step2: Download pretrained models and FID reference sets; step3: Generate training data for LD3 using the teacher solver for CIFAR-10; step4: Generate training data for LD3 using the teacher solver for Stable Diffusion; step5: Train LD3 on the generated training data for CIFAR-10; step6: Compute FID for Stable Diffusion using custom timesteps; step7: Compute FID for Stable Diffusion using default timesteps", "executed_cmds": "conda env create -f requirements.yml;conda activate ld3;pip install -e ./src/clip/;pip install -e ./src/taming-transformers/", "cmd_prefix": "pip", "cmd_postfix": "install omegaconf", "target_cmd": "pip install omegaconf"}
{"uuid": "87b59899-cad0-4776-9392-210282734936", "execution_plan": "step1: Set up the environment using Anaconda and install necessary dependencies; step2: Download pretrained models and FID reference sets; step3: Generate training data for LD3 using the teacher solver for CIFAR-10; step4: Generate training data for LD3 using the teacher solver for Stable Diffusion; step5: Train LD3 on the generated training data for CIFAR-10; step6: Compute FID for Stable Diffusion using custom timesteps; step7: Compute FID for Stable Diffusion using default timesteps", "executed_cmds": "conda env create -f requirements.yml;conda activate ld3;pip install -e ./src/clip/;pip install -e ./src/taming-transformers/;pip install omegaconf", "cmd_prefix": "pip", "cmd_postfix": "install PyYAML", "target_cmd": "pip install PyYAML"}
{"uuid": "6a85e8b4-990e-4124-9c68-33e4747c80b6", "execution_plan": "step1: Set up the environment using Anaconda and install necessary dependencies; step2: Download pretrained models and FID reference sets; step3: Generate training data for LD3 using the teacher solver for CIFAR-10; step4: Generate training data for LD3 using the teacher solver for Stable Diffusion; step5: Train LD3 on the generated training data for CIFAR-10; step6: Compute FID for Stable Diffusion using custom timesteps; step7: Compute FID for Stable Diffusion using default timesteps", "executed_cmds": "conda env create -f requirements.yml;conda activate ld3;pip install -e ./src/clip/;pip install -e ./src/taming-transformers/;pip install omegaconf;pip install PyYAML", "cmd_prefix": "pip", "cmd_postfix": "install requests", "target_cmd": "pip install requests"}
{"uuid": "63ecd8d6-8e4c-4626-8ea7-83053429caeb", "execution_plan": "step1: Set up the environment using Anaconda and install necessary dependencies; step2: Download pretrained models and FID reference sets; step3: Generate training data for LD3 using the teacher solver for CIFAR-10; step4: Generate training data for LD3 using the teacher solver for Stable Diffusion; step5: Train LD3 on the generated training data for CIFAR-10; step6: Compute FID for Stable Diffusion using custom timesteps; step7: Compute FID for Stable Diffusion using default timesteps", "executed_cmds": "conda env create -f requirements.yml;conda activate ld3;pip install -e ./src/clip/;pip install -e ./src/taming-transformers/;pip install omegaconf;pip install PyYAML;pip install requests", "cmd_prefix": "pip", "cmd_postfix": "install scipy", "target_cmd": "pip install scipy"}
{"uuid": "6077fa0d-9016-419a-a4ad-ea5ba2a50005", "execution_plan": "step1: Set up the environment using Anaconda and install necessary dependencies; step2: Download pretrained models and FID reference sets; step3: Generate training data for LD3 using the teacher solver for CIFAR-10; step4: Generate training data for LD3 using the teacher solver for Stable Diffusion; step5: Train LD3 on the generated training data for CIFAR-10; step6: Compute FID for Stable Diffusion using custom timesteps; step7: Compute FID for Stable Diffusion using default timesteps", "executed_cmds": "conda env create -f requirements.yml;conda activate ld3;pip install -e ./src/clip/;pip install -e ./src/taming-transformers/;pip install omegaconf;pip install PyYAML;pip install requests;pip install scipy", "cmd_prefix": "pip", "cmd_postfix": "install torchmetrics", "target_cmd": "pip install torchmetrics"}
{"uuid": "c837519a-ee4e-44e4-9f95-e4c8474a95d5", "execution_plan": "step1: Clone the repository and its submodule `luno` to get the source code; step2: Install the dependencies using uv, including the `luno` package and the main project in development mode; step3: Choose the experiment type to run (Low Data Regime or Out-of-Distribution Experiments); step4: Train models using the provided script for the selected datasets; step5: Evaluate the trained models using the evaluation script", "executed_cmds": "git clone --recurse-submodules https://github.com/2bys/luno-experiments.git", "cmd_prefix": "uv pip", "cmd_postfix": "install -e deps/luno", "target_cmd": "uv pip install -e deps/luno"}
{"uuid": "ea59faf5-50d6-41e9-adf0-1bb32a39a5f5", "execution_plan": "step1: Clone the repository and its submodule `luno` to get the source code; step2: Install the dependencies using uv, including the `luno` package and the main project in development mode; step3: Choose the experiment type to run (Low Data Regime or Out-of-Distribution Experiments); step4: Train models using the provided script for the selected datasets; step5: Evaluate the trained models using the evaluation script", "executed_cmds": "git clone --recurse-submodules https://github.com/2bys/luno-experiments.git;uv pip install -e deps/luno", "cmd_prefix": "uv pip", "cmd_postfix": "install -e .", "target_cmd": "uv pip install -e ."}
{"uuid": "e8ffa31c-a5a0-4505-a657-d762e7c6ebe4", "execution_plan": "step1: Create a conda environment with Python 3.9 and necessary dependencies; step2: Install MuJoCo Simulator and mujoco-py for the environment; step3: Install project dependencies using pip; step4: Run the provided scripts for training and inference in different environments; step5: Modify configuration settings in the template script for custom experiments", "executed_cmds": "conda create -n dv python=3.9 mesalib glew glfw pip=23 setuptools=63.2.0 wheel=0.38.4 protobuf=3.20 -c conda-forge -y", "cmd_prefix": "conda", "cmd_postfix": "activate dv", "target_cmd": "conda activate dv"}
{"uuid": "2994f381-8b18-4893-a6da-1d5c190cf33e", "execution_plan": "step1: Create a conda environment with Python 3.9 and necessary dependencies; step2: Install MuJoCo Simulator and mujoco-py for the environment; step3: Install project dependencies using pip; step4: Run the provided scripts for training and inference in different environments; step5: Modify configuration settings in the template script for custom experiments", "executed_cmds": "conda create -n dv python=3.9 mesalib glew glfw pip=23 setuptools=63.2.0 wheel=0.38.4 protobuf=3.20 -c conda-forge -y;conda activate dv;sudo apt-get update && sudo apt-get install -y wget tar libosmesa6-dev libgl1-mesa-glx libglfw3 patchelf cmake;sudo ln -s /usr/lib/x86_64-linux-gnu/libGL.so.1 /usr/lib/x86_64-linux-gnu/libGL.so;wget -c \"https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\";mkdir -p /home/$USER_DIR/.mujoco;cp mujoco210-linux-x86_64.tar.gz /home/$USER_DIR/mujoco.tar.gz;rm mujoco210-linux-x86_64.tar.gz;mkdir -p /home/$USER_DIR/.mujoco;tar -zxvf /home/$USER_DIR/mujoco.tar.gz -C /home/$USER_DIR/.mujoco;echo \"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/$USER_DIR/.mujoco/mujoco210/bin\" >> ~/.bashrc;echo \"export MUJOCO_PY_MUJOCO_PATH=/home/$USER_DIR/.mujoco/mujoco210\" >> ~/.bashrc", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "02754bcd-28fb-4e07-b7c1-d81393a7b9de", "execution_plan": "step1: Create a conda environment with Python 3.9 and necessary dependencies; step2: Install MuJoCo Simulator and mujoco-py for the environment; step3: Install project dependencies using pip; step4: Run the provided scripts for training and inference in different environments; step5: Modify configuration settings in the template script for custom experiments", "executed_cmds": "conda create -n dv python=3.9 mesalib glew glfw pip=23 setuptools=63.2.0 wheel=0.38.4 protobuf=3.20 -c conda-forge -y;conda activate dv;sudo apt-get update && sudo apt-get install -y wget tar libosmesa6-dev libgl1-mesa-glx libglfw3 patchelf cmake;sudo ln -s /usr/lib/x86_64-linux-gnu/libGL.so.1 /usr/lib/x86_64-linux-gnu/libGL.so;wget -c \"https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\";mkdir -p /home/$USER_DIR/.mujoco;cp mujoco210-linux-x86_64.tar.gz /home/$USER_DIR/mujoco.tar.gz;rm mujoco210-linux-x86_64.tar.gz;mkdir -p /home/$USER_DIR/.mujoco;tar -zxvf /home/$USER_DIR/mujoco.tar.gz -C /home/$USER_DIR/.mujoco;echo \"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/$USER_DIR/.mujoco/mujoco210/bin\" >> ~/.bashrc;echo \"export MUJOCO_PY_MUJOCO_PATH=/home/$USER_DIR/.mujoco/mujoco210\" >> ~/.bashrc;pip install -r requirements.txt", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "aa9d37cd-581c-417e-b0ab-faa48f45c87b", "execution_plan": "step1: Set up the Conda environment with Python 3.8 and install required dependencies, including PyTorch and numpy version less than 2.0; step2: Download and organize the datasets (CIFAR-10, ImageNet 64x64, LSUN Bedroom) in the specified directory structure; step3: Download and place the model checkpoints in the `pretrained` directory; step4: Run unit tests to verify the environment setup; step5: Train the model on CIFAR-10, ImageNet 64x64, or LSUN Bedroom using the provided training scripts; step6: Generate images using the trained models and evaluate them using FID scores; step7: Set up a separate Conda environment for evaluation and install TensorFlow and evaluation dependencies; step8: Run the evaluation script to compute FID scores for the generated images", "executed_cmds": "conda create -n dxmi python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate dxmi", "target_cmd": "conda activate dxmi"}
{"uuid": "e005d7b1-c46b-49f4-90d0-a3970b09d66f", "execution_plan": "step1: Set up the Conda environment with Python 3.8 and install required dependencies, including PyTorch and numpy version less than 2.0; step2: Download and organize the datasets (CIFAR-10, ImageNet 64x64, LSUN Bedroom) in the specified directory structure; step3: Download and place the model checkpoints in the `pretrained` directory; step4: Run unit tests to verify the environment setup; step5: Train the model on CIFAR-10, ImageNet 64x64, or LSUN Bedroom using the provided training scripts; step6: Generate images using the trained models and evaluate them using FID scores; step7: Set up a separate Conda environment for evaluation and install TensorFlow and evaluation dependencies; step8: Run the evaluation script to compute FID scores for the generated images", "executed_cmds": "conda create -n dxmi python=3.8;conda activate dxmi", "cmd_prefix": "pip", "cmd_postfix": "install ...", "target_cmd": "pip install ..."}
{"uuid": "b2908d54-34ce-4df4-ad7c-7fb60f1489a8", "execution_plan": "step1: Set up the Conda environment with Python 3.8 and install required dependencies, including PyTorch and numpy version less than 2.0; step2: Download and organize the datasets (CIFAR-10, ImageNet 64x64, LSUN Bedroom) in the specified directory structure; step3: Download and place the model checkpoints in the `pretrained` directory; step4: Run unit tests to verify the environment setup; step5: Train the model on CIFAR-10, ImageNet 64x64, or LSUN Bedroom using the provided training scripts; step6: Generate images using the trained models and evaluate them using FID scores; step7: Set up a separate Conda environment for evaluation and install TensorFlow and evaluation dependencies; step8: Run the evaluation script to compute FID scores for the generated images", "executed_cmds": "conda create -n dxmi python=3.8;conda activate dxmi;pip install ...", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "f5dd9b13-7198-4beb-a659-7f641c6f3323", "execution_plan": "step1: Set up the Conda environment with Python 3.8 and install required dependencies, including PyTorch and numpy version less than 2.0; step2: Download and organize the datasets (CIFAR-10, ImageNet 64x64, LSUN Bedroom) in the specified directory structure; step3: Download and place the model checkpoints in the `pretrained` directory; step4: Run unit tests to verify the environment setup; step5: Train the model on CIFAR-10, ImageNet 64x64, or LSUN Bedroom using the provided training scripts; step6: Generate images using the trained models and evaluate them using FID scores; step7: Set up a separate Conda environment for evaluation and install TensorFlow and evaluation dependencies; step8: Run the evaluation script to compute FID scores for the generated images", "executed_cmds": "conda create -n dxmi python=3.8;conda activate dxmi;pip install ...;pip install -r requirements.txt", "cmd_prefix": "conda", "cmd_postfix": "install \"numpy<2.0\"", "target_cmd": "conda install \"numpy<2.0\""}
{"uuid": "e01daf4b-05bf-46ca-a604-31a0c0a4d2ee", "execution_plan": "step1: Set up the Conda environment with Python 3.8 and install required dependencies, including PyTorch and numpy version less than 2.0; step2: Download and organize the datasets (CIFAR-10, ImageNet 64x64, LSUN Bedroom) in the specified directory structure; step3: Download and place the model checkpoints in the `pretrained` directory; step4: Run unit tests to verify the environment setup; step5: Train the model on CIFAR-10, ImageNet 64x64, or LSUN Bedroom using the provided training scripts; step6: Generate images using the trained models and evaluate them using FID scores; step7: Set up a separate Conda environment for evaluation and install TensorFlow and evaluation dependencies; step8: Run the evaluation script to compute FID scores for the generated images", "executed_cmds": "conda create -n dxmi python=3.8;conda activate dxmi;pip install ...;pip install -r requirements.txt;conda install \"numpy<2.0\"", "cmd_prefix": "python -m", "cmd_postfix": "pytest tests/", "target_cmd": "python -m pytest tests/"}
{"uuid": "45082d52-1630-4614-9bfe-89164556262c", "execution_plan": "step1: Set up the Conda environment with Python 3.8 and install required dependencies, including PyTorch and numpy version less than 2.0; step2: Download and organize the datasets (CIFAR-10, ImageNet 64x64, LSUN Bedroom) in the specified directory structure; step3: Download and place the model checkpoints in the `pretrained` directory; step4: Run unit tests to verify the environment setup; step5: Train the model on CIFAR-10, ImageNet 64x64, or LSUN Bedroom using the provided training scripts; step6: Generate images using the trained models and evaluate them using FID scores; step7: Set up a separate Conda environment for evaluation and install TensorFlow and evaluation dependencies; step8: Run the evaluation script to compute FID scores for the generated images", "executed_cmds": "conda create -n dxmi python=3.8;conda activate dxmi;pip install ...;pip install -r requirements.txt;conda install \"numpy<2.0\";python -m pytest tests/;CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 train_cifar10.py --config configs/cifar10/T10.yaml --dataset configs/cifar10/cifar10.yaml;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 train_cifar10.py --config configs/cifar10/T4_ddgan.yaml --dataset configs/cifar10/cifar10.yaml;CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 train_image_large.py --config configs/imagenet64/T10.yaml --dataset configs/imagenet64/imagenet64.yaml;CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 train_image_large.py --config configs/imagenet64/T4.yaml --dataset configs/imagenet64/imagenet64.yaml;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 train_image_large.py --config configs/lsun/T4.yaml --dataset configs/lsun/bedroom.yaml;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 generate_cifar10.py --log_dir pretrained/cifar10_ddpm_dxmi_T10 --stat datasets/cifar10_train_fid_stats.pt -n 50000;python make_npz.py --dir pretrained/cifar10_ddpm_dxmi_T10/generated --out pretrained/cifar10_ddpm_dxmi_T10/generated.npz;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 generate_large.py --log_dir pretrained/imagenet64_edm_dxmi_T10 --n_sample 50000 --batchsize 100;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 generate_large.py --log_dir pretrained/imagenet64_edm_dxmi_T10 --n_sample 50000 --batchsize 100 --skip_fid", "cmd_prefix": "conda create", "cmd_postfix": "-n eval python=3.8", "target_cmd": "conda create -n eval python=3.8"}
{"uuid": "8f4007f6-9eb4-4a30-9649-3c3a48b0c236", "execution_plan": "step1: Set up the Conda environment with Python 3.8 and install required dependencies, including PyTorch and numpy version less than 2.0; step2: Download and organize the datasets (CIFAR-10, ImageNet 64x64, LSUN Bedroom) in the specified directory structure; step3: Download and place the model checkpoints in the `pretrained` directory; step4: Run unit tests to verify the environment setup; step5: Train the model on CIFAR-10, ImageNet 64x64, or LSUN Bedroom using the provided training scripts; step6: Generate images using the trained models and evaluate them using FID scores; step7: Set up a separate Conda environment for evaluation and install TensorFlow and evaluation dependencies; step8: Run the evaluation script to compute FID scores for the generated images", "executed_cmds": "conda create -n dxmi python=3.8;conda activate dxmi;pip install ...;pip install -r requirements.txt;conda install \"numpy<2.0\";python -m pytest tests/;CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 train_cifar10.py --config configs/cifar10/T10.yaml --dataset configs/cifar10/cifar10.yaml;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 train_cifar10.py --config configs/cifar10/T4_ddgan.yaml --dataset configs/cifar10/cifar10.yaml;CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 train_image_large.py --config configs/imagenet64/T10.yaml --dataset configs/imagenet64/imagenet64.yaml;CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 train_image_large.py --config configs/imagenet64/T4.yaml --dataset configs/imagenet64/imagenet64.yaml;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 train_image_large.py --config configs/lsun/T4.yaml --dataset configs/lsun/bedroom.yaml;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 generate_cifar10.py --log_dir pretrained/cifar10_ddpm_dxmi_T10 --stat datasets/cifar10_train_fid_stats.pt -n 50000;python make_npz.py --dir pretrained/cifar10_ddpm_dxmi_T10/generated --out pretrained/cifar10_ddpm_dxmi_T10/generated.npz;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 generate_large.py --log_dir pretrained/imagenet64_edm_dxmi_T10 --n_sample 50000 --batchsize 100;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 generate_large.py --log_dir pretrained/imagenet64_edm_dxmi_T10 --n_sample 50000 --batchsize 100 --skip_fid;conda create -n eval python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate eval", "target_cmd": "conda activate eval"}
{"uuid": "f63a6f71-9324-4165-ba91-5966ebbb3115", "execution_plan": "step1: Set up the Conda environment with Python 3.8 and install required dependencies, including PyTorch and numpy version less than 2.0; step2: Download and organize the datasets (CIFAR-10, ImageNet 64x64, LSUN Bedroom) in the specified directory structure; step3: Download and place the model checkpoints in the `pretrained` directory; step4: Run unit tests to verify the environment setup; step5: Train the model on CIFAR-10, ImageNet 64x64, or LSUN Bedroom using the provided training scripts; step6: Generate images using the trained models and evaluate them using FID scores; step7: Set up a separate Conda environment for evaluation and install TensorFlow and evaluation dependencies; step8: Run the evaluation script to compute FID scores for the generated images", "executed_cmds": "conda create -n dxmi python=3.8;conda activate dxmi;pip install ...;pip install -r requirements.txt;conda install \"numpy<2.0\";python -m pytest tests/;CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 train_cifar10.py --config configs/cifar10/T10.yaml --dataset configs/cifar10/cifar10.yaml;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 train_cifar10.py --config configs/cifar10/T4_ddgan.yaml --dataset configs/cifar10/cifar10.yaml;CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 train_image_large.py --config configs/imagenet64/T10.yaml --dataset configs/imagenet64/imagenet64.yaml;CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 train_image_large.py --config configs/imagenet64/T4.yaml --dataset configs/imagenet64/imagenet64.yaml;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 train_image_large.py --config configs/lsun/T4.yaml --dataset configs/lsun/bedroom.yaml;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 generate_cifar10.py --log_dir pretrained/cifar10_ddpm_dxmi_T10 --stat datasets/cifar10_train_fid_stats.pt -n 50000;python make_npz.py --dir pretrained/cifar10_ddpm_dxmi_T10/generated --out pretrained/cifar10_ddpm_dxmi_T10/generated.npz;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 generate_large.py --log_dir pretrained/imagenet64_edm_dxmi_T10 --n_sample 50000 --batchsize 100;CUDA_VISIBLE_DEVICES=0;1;2;3 torchrun --nproc_per_node=4 generate_large.py --log_dir pretrained/imagenet64_edm_dxmi_T10 --n_sample 50000 --batchsize 100 --skip_fid;conda create -n eval python=3.8;conda activate eval;pip install tensorflow==2.XX;pip install -r evaluations/requirements.txt", "cmd_prefix": "cd", "cmd_postfix": "evaluations", "target_cmd": "cd evaluations"}
{"uuid": "0e82489c-f30b-4555-ab63-feac8cedd662", "execution_plan": "step1: Clone the repository to your local machine; step2: Install Python 3.8 and the required dependencies using pip; step3: Install gdown for downloading datasets; step4: Download and prepare the training dataset; step5: Download and prepare the evaluation datasets; step6: View detailed usage instructions for the Poisson Model; step7: View detailed usage instructions for the Nonlinear Model", "executed_cmds": "git clone https://github.com/abdcelikkanat/revisitingkmers.git", "cmd_prefix": "cd", "cmd_postfix": "revisitingkmers", "target_cmd": "cd revisitingkmers"}
{"uuid": "05b1fa88-e64d-43b2-9d20-e0a8d131cb32", "execution_plan": "step1: Clone the repository to your local machine; step2: Install Python 3.8 and the required dependencies using pip; step3: Install gdown for downloading datasets; step4: Download and prepare the training dataset; step5: Download and prepare the evaluation datasets; step6: View detailed usage instructions for the Poisson Model; step7: View detailed usage instructions for the Nonlinear Model", "executed_cmds": "git clone https://github.com/abdcelikkanat/revisitingkmers.git;cd revisitingkmers", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "257c25ed-43e1-4b1e-9354-08c3628264c6", "execution_plan": "step1: Clone the repository to your local machine; step2: Install Python 3.8 and the required dependencies using pip; step3: Install gdown for downloading datasets; step4: Download and prepare the training dataset; step5: Download and prepare the evaluation datasets; step6: View detailed usage instructions for the Poisson Model; step7: View detailed usage instructions for the Nonlinear Model", "executed_cmds": "git clone https://github.com/abdcelikkanat/revisitingkmers.git;cd revisitingkmers;pip install -r requirements.txt", "cmd_prefix": "pip", "cmd_postfix": "install gdown", "target_cmd": "pip install gdown"}
{"uuid": "c2f94186-33e5-4cee-805a-3ded5f53eb4b", "execution_plan": "step1: Clone the repository to your local machine; step2: Install Python 3.8 and the required dependencies using pip; step3: Install gdown for downloading datasets; step4: Download and prepare the training dataset; step5: Download and prepare the evaluation datasets; step6: View detailed usage instructions for the Poisson Model; step7: View detailed usage instructions for the Nonlinear Model", "executed_cmds": "git clone https://github.com/abdcelikkanat/revisitingkmers.git;cd revisitingkmers;pip install -r requirements.txt;pip install gdown;gdown 1p59ch_MO-9DXh3LUIvorllPJGLEAwsUp", "cmd_prefix": "unzip", "cmd_postfix": "dnabert-s_train.zip", "target_cmd": "unzip dnabert-s_train.zip"}
{"uuid": "53201371-bfc5-474d-bd1b-400a4be51353", "execution_plan": "step1: Clone the repository to your local machine; step2: Install Python 3.8 and the required dependencies using pip; step3: Install gdown for downloading datasets; step4: Download and prepare the training dataset; step5: Download and prepare the evaluation datasets; step6: View detailed usage instructions for the Poisson Model; step7: View detailed usage instructions for the Nonlinear Model", "executed_cmds": "git clone https://github.com/abdcelikkanat/revisitingkmers.git;cd revisitingkmers;pip install -r requirements.txt;pip install gdown;gdown 1p59ch_MO-9DXh3LUIvorllPJGLEAwsUp;unzip dnabert-s_train.zip;gdown 1I44T2alXrtXPZrhkuca6QP3tFHxDW98c", "cmd_prefix": "unzip", "cmd_postfix": "dnabert-s_eval.zip", "target_cmd": "unzip dnabert-s_eval.zip"}
{"uuid": "aeffb489-debe-4318-be08-969e7b62f14a", "execution_plan": "step1: Clone the repository to your local machine; step2: Install Python 3.8 and the required dependencies using pip; step3: Install gdown for downloading datasets; step4: Download and prepare the training dataset; step5: Download and prepare the evaluation datasets; step6: View detailed usage instructions for the Poisson Model; step7: View detailed usage instructions for the Nonlinear Model", "executed_cmds": "git clone https://github.com/abdcelikkanat/revisitingkmers.git;cd revisitingkmers;pip install -r requirements.txt;pip install gdown;gdown 1p59ch_MO-9DXh3LUIvorllPJGLEAwsUp;unzip dnabert-s_train.zip;gdown 1I44T2alXrtXPZrhkuca6QP3tFHxDW98c;unzip dnabert-s_eval.zip;python poisson_model.py --help", "cmd_prefix": "python", "cmd_postfix": "nonlinear.py --help", "target_cmd": "python nonlinear.py --help"}
{"uuid": "25408487-156a-4cbd-837f-007b844bfe9e", "execution_plan": "step1: Clone the repository to your workspace; step2: Set up the Python environment using Anaconda/Miniconda or pip; step3: Install Flash Attention library; step4: Set up Infinigen by following its installation guide; step5: Configure X11 Display Server for OpenGL rendering; step6: Download and decompress the dataset; step7: Download and decompress pre-trained weights; step8: Pre-cache the VLM checkpoint; step9: Edit and run the training script; step10: Edit and run the inference scripts for generation and rendering; step11: Optionally run node parameter post-optimization; step12: Optionally create your own dataset by following the dataset creation steps", "executed_cmds": "git clone git@github.com:mit-gfx/VLMaterial.git;conda env create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate vlmaterial", "target_cmd": "conda activate vlmaterial"}
{"uuid": "d4986bd6-c92b-490b-aa84-b556317725d5", "execution_plan": "step1: Clone the repository to your workspace; step2: Set up the Python environment using Anaconda/Miniconda or pip; step3: Install Flash Attention library; step4: Set up Infinigen by following its installation guide; step5: Configure X11 Display Server for OpenGL rendering; step6: Download and decompress the dataset; step7: Download and decompress pre-trained weights; step8: Pre-cache the VLM checkpoint; step9: Edit and run the training script; step10: Edit and run the inference scripts for generation and rendering; step11: Optionally run node parameter post-optimization; step12: Optionally create your own dataset by following the dataset creation steps", "executed_cmds": "git clone git@github.com:mit-gfx/VLMaterial.git;conda env create -f environment.yml;conda activate vlmaterial;pip install numpy scipy Pillow pyyaml tqdm;pip install torch==2.4.1 torchvision==0.19.1 --index-url https://download.pytorch.org/whl/cu118;pip install transformers==4.45.2 peft==0.13.2 accelerate==1.0.1 deepspeed==0.15.2", "cmd_prefix": "pip", "cmd_postfix": "install openai", "target_cmd": "pip install openai"}
{"uuid": "291d4414-04dc-449c-a803-05a91bc9a625", "execution_plan": "step1: Clone the repository to your workspace; step2: Set up the Python environment using Anaconda/Miniconda or pip; step3: Install Flash Attention library; step4: Set up Infinigen by following its installation guide; step5: Configure X11 Display Server for OpenGL rendering; step6: Download and decompress the dataset; step7: Download and decompress pre-trained weights; step8: Pre-cache the VLM checkpoint; step9: Edit and run the training script; step10: Edit and run the inference scripts for generation and rendering; step11: Optionally run node parameter post-optimization; step12: Optionally create your own dataset by following the dataset creation steps", "executed_cmds": "git clone git@github.com:mit-gfx/VLMaterial.git;conda env create -f environment.yml;conda activate vlmaterial;pip install numpy scipy Pillow pyyaml tqdm;pip install torch==2.4.1 torchvision==0.19.1 --index-url https://download.pytorch.org/whl/cu118;pip install transformers==4.45.2 peft==0.13.2 accelerate==1.0.1 deepspeed==0.15.2;pip install openai;pip install lpips tensorboardX fake-bpy-module-3-3;wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu118torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl;pip install flash_attn-2.6.3+cu118torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl --no-build-isolation", "cmd_prefix": "rm", "cmd_postfix": "flash_attn*.whl", "target_cmd": "rm flash_attn*.whl"}
{"uuid": "7101fc78-8d35-4b23-9586-0c2d32ff7f5e", "execution_plan": "step1: Clone the repository to your workspace; step2: Set up the Python environment using Anaconda/Miniconda or pip; step3: Install Flash Attention library; step4: Set up Infinigen by following its installation guide; step5: Configure X11 Display Server for OpenGL rendering; step6: Download and decompress the dataset; step7: Download and decompress pre-trained weights; step8: Pre-cache the VLM checkpoint; step9: Edit and run the training script; step10: Edit and run the inference scripts for generation and rendering; step11: Optionally run node parameter post-optimization; step12: Optionally create your own dataset by following the dataset creation steps", "executed_cmds": "git clone git@github.com:mit-gfx/VLMaterial.git;conda env create -f environment.yml;conda activate vlmaterial;pip install numpy scipy Pillow pyyaml tqdm;pip install torch==2.4.1 torchvision==0.19.1 --index-url https://download.pytorch.org/whl/cu118;pip install transformers==4.45.2 peft==0.13.2 accelerate==1.0.1 deepspeed==0.15.2;pip install openai;pip install lpips tensorboardX fake-bpy-module-3-3;wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu118torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl;pip install flash_attn-2.6.3+cu118torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl --no-build-isolation;rm flash_attn*.whl;sudo nvidia-xconfig --enable-all-gpus", "cmd_prefix": "sudo X", "cmd_postfix": ":0 &", "target_cmd": "sudo X :0 &"}
{"uuid": "7095a3b1-d47f-4eb0-add6-12e4856a8b74", "execution_plan": "step1: Clone the repository to your workspace; step2: Set up the Python environment using Anaconda/Miniconda or pip; step3: Install Flash Attention library; step4: Set up Infinigen by following its installation guide; step5: Configure X11 Display Server for OpenGL rendering; step6: Download and decompress the dataset; step7: Download and decompress pre-trained weights; step8: Pre-cache the VLM checkpoint; step9: Edit and run the training script; step10: Edit and run the inference scripts for generation and rendering; step11: Optionally run node parameter post-optimization; step12: Optionally create your own dataset by following the dataset creation steps", "executed_cmds": "git clone git@github.com:mit-gfx/VLMaterial.git;conda env create -f environment.yml;conda activate vlmaterial;pip install numpy scipy Pillow pyyaml tqdm;pip install torch==2.4.1 torchvision==0.19.1 --index-url https://download.pytorch.org/whl/cu118;pip install transformers==4.45.2 peft==0.13.2 accelerate==1.0.1 deepspeed==0.15.2;pip install openai;pip install lpips tensorboardX fake-bpy-module-3-3;wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu118torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl;pip install flash_attn-2.6.3+cu118torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl --no-build-isolation;rm flash_attn*.whl;sudo nvidia-xconfig --enable-all-gpus;sudo X :0 &", "cmd_prefix": "export", "cmd_postfix": "DISPLAY=:0", "target_cmd": "export DISPLAY=:0"}
{"uuid": "e73f8791-5119-473f-a0e4-fcb8c2f65d55", "execution_plan": "step1: Install Python 3.8 and Java 8 (or later) and set up Java environment variables; step2: Ensure CPLEX and SATS JAR files are available in the lib folder; step3: Install CPLEX Python API and Gurobi Python API; step4: Create and activate a Python environment and install dependencies from requirements.txt; step5: Set the PYJNIUS_CLASSPATH environment variable to the absolute path of the lib folder; step6: Run the MLHCA, ML-CCA, or CCA for a specific SATS domain, seed, and Qinit rounds", "executed_cmds": "python setup.py install", "cmd_prefix": "python -m", "cmd_postfix": "pip install gurobipy", "target_cmd": "python -m pip install gurobipy"}
{"uuid": "47256a8a-b1a2-4fa6-b657-e66fd0ca0c45", "execution_plan": "step1: Install Python 3.8 and Java 8 (or later) and set up Java environment variables; step2: Ensure CPLEX and SATS JAR files are available in the lib folder; step3: Install CPLEX Python API and Gurobi Python API; step4: Create and activate a Python environment and install dependencies from requirements.txt; step5: Set the PYJNIUS_CLASSPATH environment variable to the absolute path of the lib folder; step6: Run the MLHCA, ML-CCA, or CCA for a specific SATS domain, seed, and Qinit rounds", "executed_cmds": "python setup.py install;python -m pip install gurobipy", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "40545ec3-698b-44fa-baac-ac4a2e8e07f5", "execution_plan": "step1: Create and activate the Conda environment for the project; step2: Install the required dependencies using pip; step3: Run hyperparameter optimization for the specified data type; step4: Run model fusion for the specified data type; step5: Cite the paper if the code is used in research", "executed_cmds": "conda create -n bo-fusion", "cmd_prefix": "conda", "cmd_postfix": "activate bo-fusion", "target_cmd": "conda activate bo-fusion"}
{"uuid": "6392f45d-8b55-4a35-9be0-7750d010e29a", "execution_plan": "step1: Create and activate the Conda environment for the project; step2: Install the required dependencies using pip; step3: Run hyperparameter optimization for the specified data type; step4: Run model fusion for the specified data type; step5: Cite the paper if the code is used in research", "executed_cmds": "conda create -n bo-fusion;conda activate bo-fusion", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "b3384ce6-1ddb-45f6-995f-8d5f4eabfed9", "execution_plan": "step1: Create and activate the Conda environment for the project; step2: Install the required dependencies using pip; step3: Run hyperparameter optimization for the specified data type; step4: Run model fusion for the specified data type; step5: Cite the paper if the code is used in research", "executed_cmds": "conda create -n bo-fusion;conda activate bo-fusion;pip install -r requirements.txt", "cmd_prefix": "bash", "cmd_postfix": "hpbo_{data_type}.sh", "target_cmd": "bash hpbo_{data_type}.sh"}
{"uuid": "0b9d870d-603b-46b4-9401-2cf0e81aa0b2", "execution_plan": "step1: Create and activate the Conda environment for the project; step2: Install the required dependencies using pip; step3: Run hyperparameter optimization for the specified data type; step4: Run model fusion for the specified data type; step5: Cite the paper if the code is used in research", "executed_cmds": "conda create -n bo-fusion;conda activate bo-fusion;pip install -r requirements.txt;bash hpbo_{data_type}.sh", "cmd_prefix": "bash", "cmd_postfix": "bomf_{data_type}.sh", "target_cmd": "bash bomf_{data_type}.sh"}
{"uuid": "e59657a7-1b78-40e3-9e51-75dc1796a6ff", "execution_plan": "step1: Set up the Python virtual environment using Conda and activate it; step2: Install PyTorch v2.4.0 according to hardware specifications; step3: Install the remaining dependencies from the repository; step4: Install FlashAttention-2 for optimized attention mechanisms; step5: (Optional) Install flashinfer if decoding with gemma-2 models is required; step6: (Optional) Perform supervised fine-tuning before running DistiLLM-2; step7: Generate responses using the language model with specified parameters; step8: Reformat generated responses into teacher-student pairs for distillation; step9: (Optional) Resize embeddings if student and teacher models have different classifier head sizes; step10: Train the model using the provided configuration files for specific setups; step11: Generate outputs for evaluation using specified datasets; step12: Run LLM-as-a-Judge evaluation for pairwise comparison", "executed_cmds": "conda create -n distillm2 python=3.10 && conda activate distillm2", "cmd_prefix": "python -m", "cmd_postfix": "pip install .", "target_cmd": "python -m pip install ."}
{"uuid": "c81ee9e4-b1a3-4441-8d6e-3a11c1ad666d", "execution_plan": "step1: Set up the Python virtual environment using Conda and activate it; step2: Install PyTorch v2.4.0 according to hardware specifications; step3: Install the remaining dependencies from the repository; step4: Install FlashAttention-2 for optimized attention mechanisms; step5: (Optional) Install flashinfer if decoding with gemma-2 models is required; step6: (Optional) Perform supervised fine-tuning before running DistiLLM-2; step7: Generate responses using the language model with specified parameters; step8: Reformat generated responses into teacher-student pairs for distillation; step9: (Optional) Resize embeddings if student and teacher models have different classifier head sizes; step10: Train the model using the provided configuration files for specific setups; step11: Generate outputs for evaluation using specified datasets; step12: Run LLM-as-a-Judge evaluation for pairwise comparison", "executed_cmds": "conda create -n distillm2 python=3.10 && conda activate distillm2;python -m pip install .;python -m pip install flash-attn --no-build-isolation;python -m pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4;accelerate launch --config_file accelerate_configs/deepspeed_zero3.yaml --num_processes=4 src/run_sft.py training_configs/qwen2.5-1.5b-sft.yaml;python generate/generate_vllm.py --model $MODEL_DIR --output_dir $OUTPUT_DIR --seed $SEED;python generate/reformat.py --teacher_file $TEACHER_DIR --student_file $STUDENT_DIR --output_dir $OUTPUT_DIR;python utils/resize_embedding.py --teacher-model Qwen/Qwen2.5-7B-Instruct --student-model Qwen/Qwen2.5-0.5B-Instruct;accelerate launch --config_file accelerate_configs/deepspeed_zero3.yaml --num_processes=4 src/run_distillm.py training_configs/qwen2.5-1.5b-distillm2.yaml;accelerate launch --config_file accelerate_configs/deepspeed_zero3.yaml --num_processes=4 src/run_distillm.py training_configs/deepseek-coder-1.3b-distillm2.yaml;accelerate launch --config_file accelerate_configs/deepspeed_zero3.yaml --num_processes=4 src/run_distivlm.py training_configs/vlm.yaml;python utils/merging.py --base-model-name ${STUDENT_DIR} --lora-model-name ${LORA_DIR};python generate/generate_vllm.py --model ${LORA_DIR}/merged --output_dir ${OUTPUT_DIR} --data_dir evol-instruct --seed 200;python eval/build_evaluation.py --data-path1 eval/evol-instruct/evol_inst_eval.json --data-path2 $OUTPUT_DIR/output_200.json --pairwise --output-file evol_inst-${EXP_NAME} --judge gpt-4o;python eval/build_evaluation.py --data-path2 eval/evol-instruct/evol_inst_eval.json --data-path1 $OUTPUT_DIR/output_200.json --pairwise --output-file ${EXP_NAME}-evol_inst --judge gpt-4o", "cmd_prefix": "bash eval/run.sh", "cmd_postfix": "${3} ${EXP_NAME}", "target_cmd": "bash eval/run.sh ${3} ${EXP_NAME}"}
{"uuid": "edae8a95-9afd-414b-b6e8-5db52e2e464a", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Install the required dependencies (Python 3, PyTorch 1.6.0, Torchvision 0.7.0, Numpy, CLIP); step3: Prepare the datasets by downloading them and placing them in the `data/` folder; step4: Fine-tune CLIP's adapters on your dataset to prepare them for use; step5: Pre-calculate sample scores for selection using the provided script; step6: Optimize the selected datasets with respect to specific selection ratios", "executed_cmds": "git clone https://github.com/Jackbrocp/clip-powered-data-selection;cd clip-powered-data-selection", "cmd_prefix": "python sample_scoring.py", "cmd_postfix": "--dataset CIFAR100", "target_cmd": "python sample_scoring.py --dataset CIFAR100"}
{"uuid": "94380d43-3166-4492-8680-08c5c00ef76d", "execution_plan": "step1: Clone the repository and navigate into the directory; step2: Install the required dependencies (Python 3, PyTorch 1.6.0, Torchvision 0.7.0, Numpy, CLIP); step3: Prepare the datasets by downloading them and placing them in the `data/` folder; step4: Fine-tune CLIP's adapters on your dataset to prepare them for use; step5: Pre-calculate sample scores for selection using the provided script; step6: Optimize the selected datasets with respect to specific selection ratios", "executed_cmds": "git clone https://github.com/Jackbrocp/clip-powered-data-selection;cd clip-powered-data-selection;python sample_scoring.py --dataset CIFAR100", "cmd_prefix": "python optimize_selection.py", "cmd_postfix": "--dataset CIFAR100", "target_cmd": "python optimize_selection.py --dataset CIFAR100"}
{"uuid": "e42c6541-4693-422d-b6cf-bc3c609017a5", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Download and prepare the test datasets; step3: Download the pre-trained weights for the model; step4: Run the test code with configurable parameters; step5: Download additional pre-trained weights for the modulation mode (OWL-ViT and SAM); step6: Run the modulation mode test code with configurable text prompts; step7: Prepare training data for the diffusion model; step8: Train the diffusion model; step9: Prepare training data for the FCM model; step10: Train the FCM model", "executed_cmds": "conda create -n Text-DiFuse python==3.9", "cmd_prefix": "conda", "cmd_postfix": "activate Text-DiFuse", "target_cmd": "conda activate Text-DiFuse"}
{"uuid": "7a071777-7c5b-4562-bdca-4f5b3867ceea", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Download and prepare the test datasets; step3: Download the pre-trained weights for the model; step4: Run the test code with configurable parameters; step5: Download additional pre-trained weights for the modulation mode (OWL-ViT and SAM); step6: Run the modulation mode test code with configurable text prompts; step7: Prepare training data for the diffusion model; step8: Train the diffusion model; step9: Prepare training data for the FCM model; step10: Train the FCM model", "executed_cmds": "conda create -n Text-DiFuse python==3.9;conda activate Text-DiFuse;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "b23bae9f-6f98-4a6e-9669-1ad2543a5fd3", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Download and prepare the test datasets; step3: Download the pre-trained weights for the model; step4: Run the test code with configurable parameters; step5: Download additional pre-trained weights for the modulation mode (OWL-ViT and SAM); step6: Run the modulation mode test code with configurable text prompts; step7: Prepare training data for the diffusion model; step8: Train the diffusion model; step9: Prepare training data for the FCM model; step10: Train the FCM model", "executed_cmds": "conda create -n Text-DiFuse python==3.9;conda activate Text-DiFuse;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0;pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "test.py", "target_cmd": "python test.py"}
{"uuid": "b0ac38e5-0ec0-4ad6-b9c0-b171f0614b7d", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Download and prepare the test datasets; step3: Download the pre-trained weights for the model; step4: Run the test code with configurable parameters; step5: Download additional pre-trained weights for the modulation mode (OWL-ViT and SAM); step6: Run the modulation mode test code with configurable text prompts; step7: Prepare training data for the diffusion model; step8: Train the diffusion model; step9: Prepare training data for the FCM model; step10: Train the FCM model", "executed_cmds": "conda create -n Text-DiFuse python==3.9;conda activate Text-DiFuse;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0;pip install -r requirements.txt;python test.py", "cmd_prefix": "python", "cmd_postfix": "test_modulated.py", "target_cmd": "python test_modulated.py"}
{"uuid": "487f5c07-9daf-4f6e-8ea8-fc9163a77683", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Download and prepare the test datasets; step3: Download the pre-trained weights for the model; step4: Run the test code with configurable parameters; step5: Download additional pre-trained weights for the modulation mode (OWL-ViT and SAM); step6: Run the modulation mode test code with configurable text prompts; step7: Prepare training data for the diffusion model; step8: Train the diffusion model; step9: Prepare training data for the FCM model; step10: Train the FCM model", "executed_cmds": "conda create -n Text-DiFuse python==3.9;conda activate Text-DiFuse;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0;pip install -r requirements.txt;python test.py;python test_modulated.py", "cmd_prefix": "python", "cmd_postfix": "train_diffusion.py", "target_cmd": "python train_diffusion.py"}
{"uuid": "104ec61b-b572-4619-90c9-55ac509f4aae", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Download and prepare the test datasets; step3: Download the pre-trained weights for the model; step4: Run the test code with configurable parameters; step5: Download additional pre-trained weights for the modulation mode (OWL-ViT and SAM); step6: Run the modulation mode test code with configurable text prompts; step7: Prepare training data for the diffusion model; step8: Train the diffusion model; step9: Prepare training data for the FCM model; step10: Train the FCM model", "executed_cmds": "conda create -n Text-DiFuse python==3.9;conda activate Text-DiFuse;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0;pip install -r requirements.txt;python test.py;python test_modulated.py;python train_diffusion.py", "cmd_prefix": "python", "cmd_postfix": "train_FCM.py", "target_cmd": "python train_FCM.py"}
{"uuid": "08f82720-1813-45dd-aa09-c3fb6fb6b023", "execution_plan": "step1: Create a conda environment with Python 3.11 and activate it; step2: Install required packages using pip; step3: Download an intermediate OLMo checkpoint from the official repository; step4: Download the official training order of Dolma for knowledge entropy calculation; step5: Calculate knowledge entropy using the provided script; step6: Train the OLMo model with a modified configuration; step7: Modify and save a model checkpoint for the resuscitation method; step8: Train the model using the resuscitation method with a modified configuration", "executed_cmds": "conda create -n knowledge-entropy python=3.11 -y;conda activate knowledge-entropy", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "671e92c5-3482-4466-8084-6f18318544ac", "execution_plan": "step1: Install the required Python packages to set up the environment; step2: Navigate to the Gaussian Averaging directory and run the Gaussian averaging example; step3: Navigate to the Point Cloud Averaging directory, create a saved directory, and run the main script; step4: Navigate to the Color Harmonization directory and run the main script; step5: Navigate to the SWAE directory and follow the instructions in its README.md file", "executed_cmds": "pip install -r requirements.txt", "cmd_prefix": "cd", "cmd_postfix": "GaussianAveraging", "target_cmd": "cd GaussianAveraging"}
{"uuid": "2493ea79-8007-414a-8614-5b4f24bebea4", "execution_plan": "step1: Install the required Python packages to set up the environment; step2: Navigate to the Gaussian Averaging directory and run the Gaussian averaging example; step3: Navigate to the Point Cloud Averaging directory, create a saved directory, and run the main script; step4: Navigate to the Color Harmonization directory and run the main script; step5: Navigate to the SWAE directory and follow the instructions in its README.md file", "executed_cmds": "pip install -r requirements.txt;cd GaussianAveraging", "cmd_prefix": "python", "cmd_postfix": "gaussian_example.py", "target_cmd": "python gaussian_example.py"}
{"uuid": "9c216a1b-f77c-4853-a185-4043e8532a5b", "execution_plan": "step1: Install the required Python packages to set up the environment; step2: Navigate to the Gaussian Averaging directory and run the Gaussian averaging example; step3: Navigate to the Point Cloud Averaging directory, create a saved directory, and run the main script; step4: Navigate to the Color Harmonization directory and run the main script; step5: Navigate to the SWAE directory and follow the instructions in its README.md file", "executed_cmds": "pip install -r requirements.txt;cd GaussianAveraging;python gaussian_example.py", "cmd_prefix": "cd", "cmd_postfix": "PointCloudAveraging", "target_cmd": "cd PointCloudAveraging"}
{"uuid": "de72d350-09d2-4240-9199-33b37ea3a3cb", "execution_plan": "step1: Install the required Python packages to set up the environment; step2: Navigate to the Gaussian Averaging directory and run the Gaussian averaging example; step3: Navigate to the Point Cloud Averaging directory, create a saved directory, and run the main script; step4: Navigate to the Color Harmonization directory and run the main script; step5: Navigate to the SWAE directory and follow the instructions in its README.md file", "executed_cmds": "pip install -r requirements.txt;cd GaussianAveraging;python gaussian_example.py;cd PointCloudAveraging", "cmd_prefix": "mkdir", "cmd_postfix": "saved", "target_cmd": "mkdir saved"}
{"uuid": "48b33f0f-e90f-4fac-aa45-e7ae9f8b89c2", "execution_plan": "step1: Install the required Python packages to set up the environment; step2: Navigate to the Gaussian Averaging directory and run the Gaussian averaging example; step3: Navigate to the Point Cloud Averaging directory, create a saved directory, and run the main script; step4: Navigate to the Color Harmonization directory and run the main script; step5: Navigate to the SWAE directory and follow the instructions in its README.md file", "executed_cmds": "pip install -r requirements.txt;cd GaussianAveraging;python gaussian_example.py;cd PointCloudAveraging;mkdir saved", "cmd_prefix": "python", "cmd_postfix": "main_point.py", "target_cmd": "python main_point.py"}
{"uuid": "7757c77a-da63-4196-8f69-76709288177f", "execution_plan": "step1: Install the required Python packages to set up the environment; step2: Navigate to the Gaussian Averaging directory and run the Gaussian averaging example; step3: Navigate to the Point Cloud Averaging directory, create a saved directory, and run the main script; step4: Navigate to the Color Harmonization directory and run the main script; step5: Navigate to the SWAE directory and follow the instructions in its README.md file", "executed_cmds": "pip install -r requirements.txt;cd GaussianAveraging;python gaussian_example.py;cd PointCloudAveraging;mkdir saved;python main_point.py", "cmd_prefix": "cd", "cmd_postfix": "ColorHarmonization", "target_cmd": "cd ColorHarmonization"}
{"uuid": "aa1c4228-f4ef-4203-9a8b-2b04cf537399", "execution_plan": "step1: Install the required Python packages to set up the environment; step2: Navigate to the Gaussian Averaging directory and run the Gaussian averaging example; step3: Navigate to the Point Cloud Averaging directory, create a saved directory, and run the main script; step4: Navigate to the Color Harmonization directory and run the main script; step5: Navigate to the SWAE directory and follow the instructions in its README.md file", "executed_cmds": "pip install -r requirements.txt;cd GaussianAveraging;python gaussian_example.py;cd PointCloudAveraging;mkdir saved;python main_point.py;cd ColorHarmonization", "cmd_prefix": "python", "cmd_postfix": "main.py", "target_cmd": "python main.py"}
{"uuid": "428e0029-8b2a-48db-aa5b-1cee90913722", "execution_plan": "step1: Install the required packages for the project; step2: Generate or download the classifier files needed for the project; step3: Prepare the datasets (COCO and YouTube-VIS-2019) and organize them in the specified directory structure; step4: Run the demo to test the project with a sample image; step5: Download the checkpoint file for inference; step6: Perform inference on COCO Panoptic dataset; step7: Perform inference on Video Instance Segmentation dataset; step8: Perform inference on Interactive Segmentation dataset; step9: Wait for the training code to be released for further steps", "executed_cmds": "pip install mmengine==0.8.4", "cmd_prefix": "pip", "cmd_postfix": "install mmdet==3.3.0", "target_cmd": "pip install mmdet==3.3.0"}
{"uuid": "6bc107ce-2392-4d1f-8a86-7e0adf319eb6", "execution_plan": "step1: Install the required packages for the project; step2: Generate or download the classifier files needed for the project; step3: Prepare the datasets (COCO and YouTube-VIS-2019) and organize them in the specified directory structure; step4: Run the demo to test the project with a sample image; step5: Download the checkpoint file for inference; step6: Perform inference on COCO Panoptic dataset; step7: Perform inference on Video Instance Segmentation dataset; step8: Perform inference on Interactive Segmentation dataset; step9: Wait for the training code to be released for further steps", "executed_cmds": "pip install mmengine==0.8.4;pip install mmdet==3.3.0;PYTHONPATH='.' python tools/gen_cls.py configs/rap_sam/rap_sam_convl_12e_adaptor.py;python demo/demo.py demo/demo.jpg configs/rap_sam/eval_rap_sam_coco.py --weights rapsam_r50_12e.pth", "cmd_prefix": "./tools/dist_test.sh configs/rap_sam/eval_rap_sam_coco.py", "cmd_postfix": "$CKPT $NUM_GPUS", "target_cmd": "./tools/dist_test.sh configs/rap_sam/eval_rap_sam_coco.py $CKPT $NUM_GPUS"}
{"uuid": "90f95b3d-ab6f-4249-b46e-01a3d10c0e48", "execution_plan": "step1: Install the required packages for the project; step2: Generate or download the classifier files needed for the project; step3: Prepare the datasets (COCO and YouTube-VIS-2019) and organize them in the specified directory structure; step4: Run the demo to test the project with a sample image; step5: Download the checkpoint file for inference; step6: Perform inference on COCO Panoptic dataset; step7: Perform inference on Video Instance Segmentation dataset; step8: Perform inference on Interactive Segmentation dataset; step9: Wait for the training code to be released for further steps", "executed_cmds": "pip install mmengine==0.8.4;pip install mmdet==3.3.0;PYTHONPATH='.' python tools/gen_cls.py configs/rap_sam/rap_sam_convl_12e_adaptor.py;python demo/demo.py demo/demo.jpg configs/rap_sam/eval_rap_sam_coco.py --weights rapsam_r50_12e.pth;./tools/dist_test.sh configs/rap_sam/eval_rap_sam_coco.py $CKPT $NUM_GPUS", "cmd_prefix": "./tools/dist_test.sh configs/rap_sam/eval_rap_sam_yt19.py", "cmd_postfix": "$CKPT $NUM_GPUS", "target_cmd": "./tools/dist_test.sh configs/rap_sam/eval_rap_sam_yt19.py $CKPT $NUM_GPUS"}
{"uuid": "63ed269a-7881-405f-b540-dc4023676ed8", "execution_plan": "step1: Install the required packages for the project; step2: Generate or download the classifier files needed for the project; step3: Prepare the datasets (COCO and YouTube-VIS-2019) and organize them in the specified directory structure; step4: Run the demo to test the project with a sample image; step5: Download the checkpoint file for inference; step6: Perform inference on COCO Panoptic dataset; step7: Perform inference on Video Instance Segmentation dataset; step8: Perform inference on Interactive Segmentation dataset; step9: Wait for the training code to be released for further steps", "executed_cmds": "pip install mmengine==0.8.4;pip install mmdet==3.3.0;PYTHONPATH='.' python tools/gen_cls.py configs/rap_sam/rap_sam_convl_12e_adaptor.py;python demo/demo.py demo/demo.jpg configs/rap_sam/eval_rap_sam_coco.py --weights rapsam_r50_12e.pth;./tools/dist_test.sh configs/rap_sam/eval_rap_sam_coco.py $CKPT $NUM_GPUS;./tools/dist_test.sh configs/rap_sam/eval_rap_sam_yt19.py $CKPT $NUM_GPUS", "cmd_prefix": "./tools/dist_test.sh configs/rap_sam/eval_rap_sam_prompt.py", "cmd_postfix": "$CKPT $NUM_GPUS", "target_cmd": "./tools/dist_test.sh configs/rap_sam/eval_rap_sam_prompt.py $CKPT $NUM_GPUS"}
{"uuid": "c8cf20e7-2344-4750-9ce3-446571998f50", "execution_plan": "step1: Set up the Python environment using conda and install required dependencies; step2: Download the necessary datasets and configure the data path in cfg.py; step3: Train the model using baseline label mapping methods for vision models; step4: Train the model using baseline label mapping methods for vision-language models; step5: Train the model using Bayesian-guided Label Mapping (BLM) for vision models; step6: Train the model using Bayesian-guided Label Mapping (BLM) for vision-language models; step7: Train the model using Improved Bayesian-guided Label Mapping (BLM++) for vision models; step8: Train the model using Improved Bayesian-guided Label Mapping (BLM++) for vision-language models; step9: Train the model using the fast edition of ILM, BLM, or BLM++ for vision models", "executed_cmds": "conda create -n reprogram", "cmd_prefix": "conda", "cmd_postfix": "activate reprogram", "target_cmd": "conda activate reprogram"}
{"uuid": "616b4487-1c86-4c18-b059-17ae7d31fd1f", "execution_plan": "step1: Set up the Python environment using conda and install required dependencies; step2: Download the necessary datasets and configure the data path in cfg.py; step3: Train the model using baseline label mapping methods for vision models; step4: Train the model using baseline label mapping methods for vision-language models; step5: Train the model using Bayesian-guided Label Mapping (BLM) for vision models; step6: Train the model using Bayesian-guided Label Mapping (BLM) for vision-language models; step7: Train the model using Improved Bayesian-guided Label Mapping (BLM++) for vision models; step8: Train the model using Improved Bayesian-guided Label Mapping (BLM++) for vision-language models; step9: Train the model using the fast edition of ILM, BLM, or BLM++ for vision models", "executed_cmds": "conda create -n reprogram;conda activate reprogram", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "c38809b4-2013-47f8-812b-38e67bd4c546", "execution_plan": "step1: Set up the Python environment using conda and install required dependencies; step2: Download the necessary datasets and configure the data path in cfg.py; step3: Train the model using baseline label mapping methods for vision models; step4: Train the model using baseline label mapping methods for vision-language models; step5: Train the model using Bayesian-guided Label Mapping (BLM) for vision models; step6: Train the model using Bayesian-guided Label Mapping (BLM) for vision-language models; step7: Train the model using Improved Bayesian-guided Label Mapping (BLM++) for vision models; step8: Train the model using Improved Bayesian-guided Label Mapping (BLM++) for vision-language models; step9: Train the model using the fast edition of ILM, BLM, or BLM++ for vision models", "executed_cmds": "conda create -n reprogram;conda activate reprogram;pip install -r requirements.txt;python train_vm.py --dataset flowers102 --mapping [rlm;flm", "cmd_prefix": "ilm]", "cmd_postfix": "--seed 0", "target_cmd": "ilm] --seed 0"}
{"uuid": "43c3d561-cbf3-4f7a-993c-f767dba0b3f6", "execution_plan": "step1: Set up the Python environment using conda and install required dependencies; step2: Download the necessary datasets and configure the data path in cfg.py; step3: Train the model using baseline label mapping methods for vision models; step4: Train the model using baseline label mapping methods for vision-language models; step5: Train the model using Bayesian-guided Label Mapping (BLM) for vision models; step6: Train the model using Bayesian-guided Label Mapping (BLM) for vision-language models; step7: Train the model using Improved Bayesian-guided Label Mapping (BLM++) for vision models; step8: Train the model using Improved Bayesian-guided Label Mapping (BLM++) for vision-language models; step9: Train the model using the fast edition of ILM, BLM, or BLM++ for vision models", "executed_cmds": "conda create -n reprogram;conda activate reprogram;pip install -r requirements.txt;python train_vm.py --dataset flowers102 --mapping [rlm;flm;ilm] --seed 0;python train_vlm.py --dataset flowers102 --mapping ilm --seed 0;python train_vm.py --dataset flowers102 --mapping blm --seed 0;python train_vlm.py --dataset flowers102 --mapping blm --seed 0;python train_vm.py --dataset flowers102 --mapping blmpp --seed 0;python train_vlm.py --dataset flowers102 --mapping blmpp --seed 0;python train_vm_fast.py --dataset flowers102 --mapping [ilm;blm", "cmd_prefix": "blmp]", "cmd_postfix": "--seed 0", "target_cmd": "blmp] --seed 0"}
{"uuid": "2c3445b9-b5ad-41ae-b978-9bcf7478312e", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required dependencies; step2: Download and preprocess the datasets (HumanEval, MATH, MBPP, xCodeEval); step3: Configure the LLM settings in the config file for OpenAI or local models; step4: Set up an experiment configuration file for the desired task and method; step5: Run the code generation and evaluation commands for the experiment; step6: Install FunCoder as a module for use in other Python scripts", "executed_cmds": "conda create -y -n funcoder python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate funcoder", "target_cmd": "conda activate funcoder"}
{"uuid": "ec4e9ff9-b44f-4043-957f-502f8d6f91db", "execution_plan": "step1: Set up the environment by creating a conda environment and installing required dependencies; step2: Download and preprocess the datasets (HumanEval, MATH, MBPP, xCodeEval); step3: Configure the LLM settings in the config file for OpenAI or local models; step4: Set up an experiment configuration file for the desired task and method; step5: Run the code generation and evaluation commands for the experiment; step6: Install FunCoder as a module for use in other Python scripts", "executed_cmds": "conda create -y -n funcoder python=3.10;conda activate funcoder;python -m pip install -r requirements.txt;python -m funcoder.eval download-datasets;python -m funcoder.eval draft --results-dir /your/experiment/dir/ --parallelism 12 --skip-done;python -m funcoder.eval judge --results-dir /your/experiment/dir/ --skip-done", "cmd_prefix": "python -m pip", "cmd_postfix": "install --editable .", "target_cmd": "python -m pip install --editable ."}
{"uuid": "d99cdab3-8c64-4719-8bfe-5d5f44bfc7e3", "execution_plan": "step1: Set up the conda environments by modifying the environment files and creating the environments; step2: Replace the Huggingface token in DJ_search_exact.py for exact match Creativity Index computation; step3: Compute Creativity Index based on exact matches using the Moses tokenizer; step4: Compute Creativity Index based on exact matches using the LLaMA 2 tokenizer; step5: Compute the lookup table of pairwise word embedding distances for semantic match Creativity Index computation; step6: Replace the ElasticSearch API key in retrieve_documents.py for semantic match Creativity Index computation; step7: Retrieve the most similar documents using ElasticSearch and process them; step8: Replace the Huggingface token in DJ_search_earth_mover.py for semantic match Creativity Index computation; step9: Compute Creativity Index based on semantic matches", "executed_cmds": "conda env create -f environment_infini.yml;conda env create -f environment_vllm.yml", "cmd_prefix": "conda", "cmd_postfix": "activate infini-gram", "target_cmd": "conda activate infini-gram"}
{"uuid": "5ad2b77d-76ec-42e7-ae81-f99c5d368346", "execution_plan": "step1: Set up the conda environments by modifying the environment files and creating the environments; step2: Replace the Huggingface token in DJ_search_exact.py for exact match Creativity Index computation; step3: Compute Creativity Index based on exact matches using the Moses tokenizer; step4: Compute Creativity Index based on exact matches using the LLaMA 2 tokenizer; step5: Compute the lookup table of pairwise word embedding distances for semantic match Creativity Index computation; step6: Replace the ElasticSearch API key in retrieve_documents.py for semantic match Creativity Index computation; step7: Retrieve the most similar documents using ElasticSearch and process them; step8: Replace the Huggingface token in DJ_search_earth_mover.py for semantic match Creativity Index computation; step9: Compute Creativity Index based on semantic matches", "executed_cmds": "conda env create -f environment_infini.yml;conda env create -f environment_vllm.yml;conda activate infini-gram;python DJ_search_exact.py --task GPT3_book --data data/book/GPT3_book.json --output_dir outputs/book", "cmd_prefix": "conda", "cmd_postfix": "activate infini-gram", "target_cmd": "conda activate infini-gram"}
{"uuid": "aee38d05-f5e9-4fc2-a508-74823b01b8b4", "execution_plan": "step1: Set up the conda environments by modifying the environment files and creating the environments; step2: Replace the Huggingface token in DJ_search_exact.py for exact match Creativity Index computation; step3: Compute Creativity Index based on exact matches using the Moses tokenizer; step4: Compute Creativity Index based on exact matches using the LLaMA 2 tokenizer; step5: Compute the lookup table of pairwise word embedding distances for semantic match Creativity Index computation; step6: Replace the ElasticSearch API key in retrieve_documents.py for semantic match Creativity Index computation; step7: Retrieve the most similar documents using ElasticSearch and process them; step8: Replace the Huggingface token in DJ_search_earth_mover.py for semantic match Creativity Index computation; step9: Compute Creativity Index based on semantic matches", "executed_cmds": "conda env create -f environment_infini.yml;conda env create -f environment_vllm.yml;conda activate infini-gram;python DJ_search_exact.py --task GPT3_book --data data/book/GPT3_book.json --output_dir outputs/book;conda activate infini-gram;python DJ_search_exact.py --task GPT3_poem --data data/poem/GPT3_poem.json --output_dir outputs/poem --lm_tokenizer", "cmd_prefix": "conda", "cmd_postfix": "activate vllm", "target_cmd": "conda activate vllm"}
{"uuid": "de40a765-dba2-4916-a65a-b6c4c4ad1dae", "execution_plan": "step1: Set up the conda environments by modifying the environment files and creating the environments; step2: Replace the Huggingface token in DJ_search_exact.py for exact match Creativity Index computation; step3: Compute Creativity Index based on exact matches using the Moses tokenizer; step4: Compute Creativity Index based on exact matches using the LLaMA 2 tokenizer; step5: Compute the lookup table of pairwise word embedding distances for semantic match Creativity Index computation; step6: Replace the ElasticSearch API key in retrieve_documents.py for semantic match Creativity Index computation; step7: Retrieve the most similar documents using ElasticSearch and process them; step8: Replace the Huggingface token in DJ_search_earth_mover.py for semantic match Creativity Index computation; step9: Compute Creativity Index based on semantic matches", "executed_cmds": "conda env create -f environment_infini.yml;conda env create -f environment_vllm.yml;conda activate infini-gram;python DJ_search_exact.py --task GPT3_book --data data/book/GPT3_book.json --output_dir outputs/book;conda activate infini-gram;python DJ_search_exact.py --task GPT3_poem --data data/poem/GPT3_poem.json --output_dir outputs/poem --lm_tokenizer;conda activate vllm;python retrieve_documents.py --input_file data/book/GPT3_book.json --output_dir data/book/retrieved/ --index DOLMA --nb_documents 100", "cmd_prefix": "conda", "cmd_postfix": "activate infini-gram", "target_cmd": "conda activate infini-gram"}
{"uuid": "f0605752-85b6-4869-9c33-d589cc776b6f", "execution_plan": "step1: Clone the repository and its submodules; step2: Create and activate a conda environment for the project; step3: Install PyTorch with CUDA support; step4: Install other dependencies listed in requirements.txt; step5: Download pretrained weights and set up necessary files; step6: Run the generation process with a single prompt; step7: Run the generation process from a prompt list", "executed_cmds": "git clone https://github.com/fudan-zvg/tet-splatting.git --recursive;conda create -n tetsplatting python=3.9;conda activate tetsplatting;pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "8fc49edd-085e-4873-98b9-1e2a8f02b5f6", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a conda environment with Python 3.9; step3: Install required packages including flash-attn and dependencies from requirements.txt; step4: Install the modified DeepSpeed package; step5: Download and link the Llama-2-7B model from HuggingFace to the ./models directory; step6: Download and link the required datasets (WizardLM, MetaMathQA, CodeFeedback-Filtered-Instruction, etc.) from HuggingFace to the ./data directory; step7: Run the training script for the desired task (math, code, or chat) using torchrun; step8: Evaluate the trained model for the respective task using the provided evaluation scripts", "executed_cmds": "git clone https://github.com/mrflogs/LoRA-Pro.git", "cmd_prefix": "cd", "cmd_postfix": "LoRA-Pro", "target_cmd": "cd LoRA-Pro"}
{"uuid": "ac173288-fa6e-4cb3-b5db-174ba30d0a67", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a conda environment with Python 3.9; step3: Install required packages including flash-attn and dependencies from requirements.txt; step4: Install the modified DeepSpeed package; step5: Download and link the Llama-2-7B model from HuggingFace to the ./models directory; step6: Download and link the required datasets (WizardLM, MetaMathQA, CodeFeedback-Filtered-Instruction, etc.) from HuggingFace to the ./data directory; step7: Run the training script for the desired task (math, code, or chat) using torchrun; step8: Evaluate the trained model for the respective task using the provided evaluation scripts", "executed_cmds": "git clone https://github.com/mrflogs/LoRA-Pro.git;cd LoRA-Pro;conda create -n lorapro python=3.9", "cmd_prefix": "conda", "cmd_postfix": "activate lorapro", "target_cmd": "conda activate lorapro"}
{"uuid": "907fa1a5-802e-46d1-845c-a63f7decb2b6", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a conda environment with Python 3.9; step3: Install required packages including flash-attn and dependencies from requirements.txt; step4: Install the modified DeepSpeed package; step5: Download and link the Llama-2-7B model from HuggingFace to the ./models directory; step6: Download and link the required datasets (WizardLM, MetaMathQA, CodeFeedback-Filtered-Instruction, etc.) from HuggingFace to the ./data directory; step7: Run the training script for the desired task (math, code, or chat) using torchrun; step8: Evaluate the trained model for the respective task using the provided evaluation scripts", "executed_cmds": "git clone https://github.com/mrflogs/LoRA-Pro.git;cd LoRA-Pro;conda create -n lorapro python=3.9;conda activate lorapro;pip install flash-attn --no-build-isolation", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "c02c4566-7810-4f1d-b151-caf9229136a1", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create and activate a conda environment with Python 3.9; step3: Install required packages including flash-attn and dependencies from requirements.txt; step4: Install the modified DeepSpeed package; step5: Download and link the Llama-2-7B model from HuggingFace to the ./models directory; step6: Download and link the required datasets (WizardLM, MetaMathQA, CodeFeedback-Filtered-Instruction, etc.) from HuggingFace to the ./data directory; step7: Run the training script for the desired task (math, code, or chat) using torchrun; step8: Evaluate the trained model for the respective task using the provided evaluation scripts", "executed_cmds": "git clone https://github.com/mrflogs/LoRA-Pro.git;cd LoRA-Pro;conda create -n lorapro python=3.9;conda activate lorapro;pip install flash-attn --no-build-isolation;pip install -r requirements.txt", "cmd_prefix": "pip install", "cmd_postfix": "-e DeepSpeed-0.15.1", "target_cmd": "pip install -e DeepSpeed-0.15.1"}
{"uuid": "0f2c4021-7afb-4815-b389-8ecf44dd1e43", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Install essential packages if running on Colab; step4: Run the inference script to generate surface normal predictions on the default test images; step5: Set up a conda environment if running on a remote server; step6: Run the inference script in the background on a remote server; step7: Analyze the generated results using post-processing scripts", "executed_cmds": "git clone https://github.com/xrhan/mssfs.git", "cmd_prefix": "cd", "cmd_postfix": "mssfs", "target_cmd": "cd mssfs"}
{"uuid": "7920320f-68b7-4eec-aee3-0e918315ef90", "execution_plan": "step1: Clone the repository to your local machine; step2: Navigate into the cloned repository directory; step3: Install essential packages if running on Colab; step4: Run the inference script to generate surface normal predictions on the default test images; step5: Set up a conda environment if running on a remote server; step6: Run the inference script in the background on a remote server; step7: Analyze the generated results using post-processing scripts", "executed_cmds": "git clone https://github.com/xrhan/mssfs.git;cd mssfs;!pip install -q -U einops datasets tqdm;!python3 multiscale.py --files 0 --model 1 --save-name test&;conda env create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate sfs", "target_cmd": "conda activate sfs"}
{"uuid": "c906b93f-c7fc-4e59-be27-badbd3d79d9c", "execution_plan": "step1: Navigate to the heterophilousgraphs folder to set up and run the heterophilous graph benchmarks; step2: Set up the environment for the heterophilousgraphs folder by following its README guidelines; step3: Run the best hyperparameter configuration for BuNN on the minesweeper dataset; step4: Navigate to the LRGB-tuned folder to set up and run the LRGB experiments; step5: Set up the environment for the LRGB-tuned folder by following its README guidelines; step6: Run BuNN on the peptides-func dataset using the provided configuration; step7: Navigate to the synthexp folder to set up and run the synthetic experiments; step8: Set up the environment for the synthexp folder by following its README guidelines; step9: Run a specific synthetic experiment, such as the uniform expressiveness one", "executed_cmds": "cd heterophilousgraphs;python -m train --name BUNN --dataset minesweeper --model BUNN --num_layers 8 --num_gnn 8 --device cpu --num_bundles 256 --hidden_dim 512 --max_degree 8 --tau 1", "cmd_prefix": "cd", "cmd_postfix": "LRGB-tuned", "target_cmd": "cd LRGB-tuned"}
{"uuid": "fbb7eb7f-aac0-4ee8-84f5-28c612deac21", "execution_plan": "step1: Navigate to the heterophilousgraphs folder to set up and run the heterophilous graph benchmarks; step2: Set up the environment for the heterophilousgraphs folder by following its README guidelines; step3: Run the best hyperparameter configuration for BuNN on the minesweeper dataset; step4: Navigate to the LRGB-tuned folder to set up and run the LRGB experiments; step5: Set up the environment for the LRGB-tuned folder by following its README guidelines; step6: Run BuNN on the peptides-func dataset using the provided configuration; step7: Navigate to the synthexp folder to set up and run the synthetic experiments; step8: Set up the environment for the synthexp folder by following its README guidelines; step9: Run a specific synthetic experiment, such as the uniform expressiveness one", "executed_cmds": "cd heterophilousgraphs;python -m train --name BUNN --dataset minesweeper --model BUNN --num_layers 8 --num_gnn 8 --device cpu --num_bundles 256 --hidden_dim 512 --max_degree 8 --tau 1;cd LRGB-tuned;python main.py --cfg configs/bundle/peptides-func-bunn.yaml seed 0", "cmd_prefix": "cd", "cmd_postfix": "synthexp", "target_cmd": "cd synthexp"}
{"uuid": "92d2ecae-40f5-4239-9ae0-3162b79aed4e", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate guided_flow", "target_cmd": "conda activate guided_flow"}
{"uuid": "4677a413-c267-4ea2-8e57-e248997048ed", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "0ad8950e-573f-4161-83cb-f81c4344b152", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .", "cmd_prefix": "bash", "cmd_postfix": "script/train_cfm.sh", "target_cmd": "bash script/train_cfm.sh"}
{"uuid": "9d3ddc58-7323-4b72-8b37-0170f8fc3bb6", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh", "cmd_prefix": "bash", "cmd_postfix": "script/train_ceg.sh", "target_cmd": "bash script/train_ceg.sh"}
{"uuid": "804b21ff-cfe6-4064-bdfc-1b4298a6cebf", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "3b3060fe-8c05-4a71-a8c2-7896bd378c8e", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "d519e19e-3cd7-4583-949b-7490872ca010", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py", "cmd_prefix": "bash", "cmd_postfix": "scripts/g_cov_A.sh", "target_cmd": "bash scripts/g_cov_A.sh"}
{"uuid": "9749ead3-b0eb-4bc9-8d19-e3ad02358d0e", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py;bash scripts/g_cov_A.sh", "cmd_prefix": "bash", "cmd_postfix": "scripts/g_cov_G.sh", "target_cmd": "bash scripts/g_cov_G.sh"}
{"uuid": "d003deda-51ec-4061-9496-cd47314b2bac", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py;bash scripts/g_cov_A.sh;bash scripts/g_cov_G.sh", "cmd_prefix": "bash", "cmd_postfix": "scripts/PiGDM.sh", "target_cmd": "bash scripts/PiGDM.sh"}
{"uuid": "ee9f02a8-4ff2-4e6c-ad57-f6c9dcec10b0", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py;bash scripts/g_cov_A.sh;bash scripts/g_cov_G.sh;bash scripts/PiGDM.sh;bash scripts/g_sim_inv_A.sh", "cmd_prefix": "bash", "cmd_postfix": "scripts/g_MC.sh", "target_cmd": "bash scripts/g_MC.sh"}
{"uuid": "855e0adf-1ad9-485b-b6d0-22c4249e6c74", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py;bash scripts/g_cov_A.sh;bash scripts/g_cov_G.sh;bash scripts/PiGDM.sh;bash scripts/g_sim_inv_A.sh;bash scripts/g_MC.sh;wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz", "cmd_prefix": "mkdir", "cmd_postfix": "~/.mujoco", "target_cmd": "mkdir ~/.mujoco"}
{"uuid": "8dc14fac-66eb-4159-8b34-5866a4704cef", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py;bash scripts/g_cov_A.sh;bash scripts/g_cov_G.sh;bash scripts/PiGDM.sh;bash scripts/g_sim_inv_A.sh;bash scripts/g_MC.sh;wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz;mkdir ~/.mujoco;tar -xvzf mujoco210-linux-x86_64.tar.gz -C ~/.mujoco;apt install libosmesa6-dev libgl1-mesa-glx libglfw3 libglx-mesa0 libgl1-mesa-dri;conda env create -f environment.yml", "cmd_prefix": "conda", "cmd_postfix": "activate gflower", "target_cmd": "conda activate gflower"}
{"uuid": "290d4f6a-b0be-4f26-86b7-690b9a2ce43d", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py;bash scripts/g_cov_A.sh;bash scripts/g_cov_G.sh;bash scripts/PiGDM.sh;bash scripts/g_sim_inv_A.sh;bash scripts/g_MC.sh;wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz;mkdir ~/.mujoco;tar -xvzf mujoco210-linux-x86_64.tar.gz -C ~/.mujoco;apt install libosmesa6-dev libgl1-mesa-glx libglfw3 libglx-mesa0 libgl1-mesa-dri;conda env create -f environment.yml;conda activate gflower", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "869dd5b9-e4a6-45ce-9fd3-94072b56ab83", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py;bash scripts/g_cov_A.sh;bash scripts/g_cov_G.sh;bash scripts/PiGDM.sh;bash scripts/g_sim_inv_A.sh;bash scripts/g_MC.sh;wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz;mkdir ~/.mujoco;tar -xvzf mujoco210-linux-x86_64.tar.gz -C ~/.mujoco;apt install libosmesa6-dev libgl1-mesa-glx libglfw3 libglx-mesa0 libgl1-mesa-dri;conda env create -f environment.yml;conda activate gflower;pip install -e .", "cmd_prefix": "conda install", "cmd_postfix": "-c conda-forge gcc", "target_cmd": "conda install -c conda-forge gcc"}
{"uuid": "531a1a53-63b0-4fd2-b2d6-82a427799a5b", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py;bash scripts/g_cov_A.sh;bash scripts/g_cov_G.sh;bash scripts/PiGDM.sh;bash scripts/g_sim_inv_A.sh;bash scripts/g_MC.sh;wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz;mkdir ~/.mujoco;tar -xvzf mujoco210-linux-x86_64.tar.gz -C ~/.mujoco;apt install libosmesa6-dev libgl1-mesa-glx libglfw3 libglx-mesa0 libgl1-mesa-dri;conda env create -f environment.yml;conda activate gflower;pip install -e .;conda install -c conda-forge gcc", "cmd_prefix": "apt", "cmd_postfix": "install libcrypt1", "target_cmd": "apt install libcrypt1"}
{"uuid": "9bfc126d-9739-49f6-a978-47f5ff58aefb", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py;bash scripts/g_cov_A.sh;bash scripts/g_cov_G.sh;bash scripts/PiGDM.sh;bash scripts/g_sim_inv_A.sh;bash scripts/g_MC.sh;wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz;mkdir ~/.mujoco;tar -xvzf mujoco210-linux-x86_64.tar.gz -C ~/.mujoco;apt install libosmesa6-dev libgl1-mesa-glx libglfw3 libglx-mesa0 libgl1-mesa-dri;conda env create -f environment.yml;conda activate gflower;pip install -e .;conda install -c conda-forge gcc;apt install libcrypt1;cp /usr/include/crypt.h $CONDA_PREFIX/include/python3.8/crypt.h;pip install setuptools==57.5.0;pip install wheel==0.37.0", "cmd_prefix": "pip", "cmd_postfix": "install pip==24.0", "target_cmd": "pip install pip==24.0"}
{"uuid": "fc6f179b-bca6-4389-b1b7-9d3025d7f15b", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py;bash scripts/g_cov_A.sh;bash scripts/g_cov_G.sh;bash scripts/PiGDM.sh;bash scripts/g_sim_inv_A.sh;bash scripts/g_MC.sh;wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz;mkdir ~/.mujoco;tar -xvzf mujoco210-linux-x86_64.tar.gz -C ~/.mujoco;apt install libosmesa6-dev libgl1-mesa-glx libglfw3 libglx-mesa0 libgl1-mesa-dri;conda env create -f environment.yml;conda activate gflower;pip install -e .;conda install -c conda-forge gcc;apt install libcrypt1;cp /usr/include/crypt.h $CONDA_PREFIX/include/python3.8/crypt.h;pip install setuptools==57.5.0;pip install wheel==0.37.0;pip install pip==24.0", "cmd_prefix": "pip", "cmd_postfix": "install gym==0.18.3", "target_cmd": "pip install gym==0.18.3"}
{"uuid": "fc1b74ea-5201-48b9-8ae5-4d9b9324c0c8", "execution_plan": "step1: Install the required packages and set up the conda environment for the synthetic dataset experiments; step2: Train the base models for the synthetic dataset experiments; step3: Train the guidance models for the synthetic dataset experiments; step4: Evaluate the guidance methods using the provided notebooks for the synthetic dataset experiments; step5: Install the required packages for the image inverse problem experiments; step6: Download and prepare the CelebA-HQ dataset for the image inverse problem experiments; step7: Train the model for the image inverse problem experiments; step8: Evaluate different guidance methods on the three inverse problems for the image inverse problem experiments; step9: Install the required packages and set up the conda environment for the offline RL experiments; step10: Train the base flow matching model for the offline RL experiments; step11: Train the value function for the offline RL experiments; step12: Evaluate the gradient and MC methods for the offline RL experiments; step13: Train the guidance model and evaluate its performance for the offline RL experiments", "executed_cmds": "conda env create -f environment.yml;conda activate guided_flow;pip install -e .;bash script/train_cfm.sh;bash script/train_guidance_matching.sh;bash script/train_ceg.sh;pip install -r requirements.txt;pip install -e .;accelerate launch run/main_train.py;bash scripts/g_cov_A.sh;bash scripts/g_cov_G.sh;bash scripts/PiGDM.sh;bash scripts/g_sim_inv_A.sh;bash scripts/g_MC.sh;wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz;mkdir ~/.mujoco;tar -xvzf mujoco210-linux-x86_64.tar.gz -C ~/.mujoco;apt install libosmesa6-dev libgl1-mesa-glx libglfw3 libglx-mesa0 libgl1-mesa-dri;conda env create -f environment.yml;conda activate gflower;pip install -e .;conda install -c conda-forge gcc;apt install libcrypt1;cp /usr/include/crypt.h $CONDA_PREFIX/include/python3.8/crypt.h;pip install setuptools==57.5.0;pip install wheel==0.37.0;pip install pip==24.0;pip install gym==0.18.3", "cmd_prefix": "bash", "cmd_postfix": "run_scripts/train.sh", "target_cmd": "bash run_scripts/train.sh"}
{"uuid": "166d7aa1-c87b-45b4-a6c4-45739a2b187d", "execution_plan": "step1: Clone the repository to your local machine; step2: Create and activate a Conda virtual environment for the project; step3: Install project dependencies using pip; step4: Generate training and validation datasets for the streaming parity task; step5: Train the ParityRNN model to learn the streaming parity algorithm; step6: Extract hidden states from the trained model and construct a Deterministic Finite Automaton (DFA); step7: Perform pairwise state-merger analysis to validate the implicit state merger mechanism", "executed_cmds": "git clone <repo_url>;cd streaming_parity_replication;python -m venv parity-rnn-state-merge;source parity-rnn-state-merge/bin/activate (macOS/Linux) or parity-rnn-state-merge\\Scripts\\activate (Windows)", "cmd_prefix": "pip install", "cmd_postfix": "--upgrade pip", "target_cmd": "pip install --upgrade pip"}
{"uuid": "3cf4df66-afd9-41d1-b642-e8ad120b1ed5", "execution_plan": "step1: Clone the repository to your local machine; step2: Create and activate a Conda virtual environment for the project; step3: Install project dependencies using pip; step4: Generate training and validation datasets for the streaming parity task; step5: Train the ParityRNN model to learn the streaming parity algorithm; step6: Extract hidden states from the trained model and construct a Deterministic Finite Automaton (DFA); step7: Perform pairwise state-merger analysis to validate the implicit state merger mechanism", "executed_cmds": "git clone <repo_url>;cd streaming_parity_replication;python -m venv parity-rnn-state-merge;source parity-rnn-state-merge/bin/activate (macOS/Linux) or parity-rnn-state-merge\\Scripts\\activate (Windows);pip install --upgrade pip", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "16ba0802-b8f9-46d6-a465-e9c22c894dcd", "execution_plan": "step1: Install system dependencies, specifically ffmpeg, which is required for the project; step2: Install Python dependencies using the provided requirements.txt file; step3: Add the project's src folder to the PYTHONPATH environment variable to enable script execution; step4: Generate datasets for different systems (Single Pendulum, Planar PCS robot, Reaction-Diffusion, N-Body problem) using TensorFlow Datasets; step5: Analyze the approximate closed-form solution for the CON dynamics; step6: Tune hyperparameters for the models and visualize the results using Optuna; step7: Perform a sweep across latent dimensions and seeds to train and evaluate models; step8: Run latent-space control experiments using the coupled oscillator network; step9: Address GPU memory allocation issues if encountered; step10: Ensure determinism by setting the XLA_FLAGS environment variable", "executed_cmds": "sudo apt install ffmpeg;conda install -c conda-forge ffmpeg", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "0b6b09d3-e8f9-4262-94bc-86e573e2469b", "execution_plan": "step1: Create a new conda environment and install the collabllm package; step2: Optional - Install additional packages for distributed training; step3: Optional - Install additional packages for customized datasets and metrics; step4: Lightweight usage - Compute Multiturn-aware Rewards (MRs) and construct datasets; step5: Synthetic data generation - Generate high-quality synthetic conversational data; step6: Train CollabLLM - Conduct SFT/DPO models training to maximize MRs; step7: Add a new dataset for a custom task; step8: Optional - Add new metrics for a custom task", "executed_cmds": "conda create -n collabllm python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate collabllm", "target_cmd": "conda activate collabllm"}
{"uuid": "46916e0e-14c4-4c0d-a42c-02748bdd1c3d", "execution_plan": "step1: Create a new conda environment and install the collabllm package; step2: Optional - Install additional packages for distributed training; step3: Optional - Install additional packages for customized datasets and metrics; step4: Lightweight usage - Compute Multiturn-aware Rewards (MRs) and construct datasets; step5: Synthetic data generation - Generate high-quality synthetic conversational data; step6: Train CollabLLM - Conduct SFT/DPO models training to maximize MRs; step7: Add a new dataset for a custom task; step8: Optional - Add new metrics for a custom task", "executed_cmds": "conda create -n collabllm python=3.10;conda activate collabllm", "cmd_prefix": "pip", "cmd_postfix": "install collabllm", "target_cmd": "pip install collabllm"}
{"uuid": "3126f7f5-14c8-4dbf-82a2-9c46c366a8ad", "execution_plan": "step1: Create a new conda environment and install the collabllm package; step2: Optional - Install additional packages for distributed training; step3: Optional - Install additional packages for customized datasets and metrics; step4: Lightweight usage - Compute Multiturn-aware Rewards (MRs) and construct datasets; step5: Synthetic data generation - Generate high-quality synthetic conversational data; step6: Train CollabLLM - Conduct SFT/DPO models training to maximize MRs; step7: Add a new dataset for a custom task; step8: Optional - Add new metrics for a custom task", "executed_cmds": "conda create -n collabllm python=3.10;conda activate collabllm;pip install collabllm", "cmd_prefix": "pip", "cmd_postfix": "install deepspeed", "target_cmd": "pip install deepspeed"}
{"uuid": "1ce49433-d809-4587-a73c-f6a588cde905", "execution_plan": "step1: Create a new conda environment and install the collabllm package; step2: Optional - Install additional packages for distributed training; step3: Optional - Install additional packages for customized datasets and metrics; step4: Lightweight usage - Compute Multiturn-aware Rewards (MRs) and construct datasets; step5: Synthetic data generation - Generate high-quality synthetic conversational data; step6: Train CollabLLM - Conduct SFT/DPO models training to maximize MRs; step7: Add a new dataset for a custom task; step8: Optional - Add new metrics for a custom task", "executed_cmds": "conda create -n collabllm python=3.10;conda activate collabllm;pip install collabllm;pip install deepspeed", "cmd_prefix": "conda", "cmd_postfix": "install mpi4py", "target_cmd": "conda install mpi4py"}
{"uuid": "562b77ae-d4bf-4a09-b9f5-4d9beff0211e", "execution_plan": "step1: Clone the repository to your local machine; step2: Install required packages and ensure compatible versions of CUDA and PyTorch are installed; step3: Download or generate pre-trained weights for CIFAR and Tiny-ImageNet datasets; step4: Prepare initialization images for distillation by downloading them or generating them; step5: Perform dataset distillation for CIFAR, Tiny-ImageNet, or ImageNet-1K; step6: Evaluate the distilled datasets for CIFAR, Tiny-ImageNet, or ImageNet-1K", "executed_cmds": "git clone https://github.com/AngusDujw/Diversity-Driven-Synthesis.git;cd Diversity-Driven-Synthesis", "cmd_prefix": "bash", "cmd_postfix": "pretrain/squeeze.sh", "target_cmd": "bash pretrain/squeeze.sh"}
{"uuid": "c3cedc96-9efd-4c2a-8fda-d1b428b4fe60", "execution_plan": "step1: Clone the repository and set up the Conda environment; step2: Install PyTorch, torchvision, and other dependencies; step3: Download pre-trained models and place them in the checkpoints directory; step4: Set up evaluation datasets for zero-shot image classification (ImageNet and Stanford Cars); step5: Set up evaluation datasets for zero-shot image and text retrieval (COCO captions and Flickr30k captions); step6: Set up the WordNet hierarchy for hierarchical classification; step7: Evaluate the model on zero-shot image classification tasks; step8: Evaluate the model on zero-shot image and text retrieval tasks; step9: Evaluate the model on hierarchical classification tasks; step10: Download the grounded Flickr dataset for interpolation; step11: Perform interpolation between points in the hyperbolic space; step12: Download and pre-process the GRIT dataset for training; step13: Train the HyCoCLIP-ViT-S/16 model; step14: Visualize spatial norms in hyperbolic space", "executed_cmds": "git clone git@github.com:PalAvik/hycoclip.git", "cmd_prefix": "cd", "cmd_postfix": "hycoclip", "target_cmd": "cd hycoclip"}
{"uuid": "1e217a58-6658-4474-9855-36b5e190f04e", "execution_plan": "step1: Clone the repository and set up the Conda environment; step2: Install PyTorch, torchvision, and other dependencies; step3: Download pre-trained models and place them in the checkpoints directory; step4: Set up evaluation datasets for zero-shot image classification (ImageNet and Stanford Cars); step5: Set up evaluation datasets for zero-shot image and text retrieval (COCO captions and Flickr30k captions); step6: Set up the WordNet hierarchy for hierarchical classification; step7: Evaluate the model on zero-shot image classification tasks; step8: Evaluate the model on zero-shot image and text retrieval tasks; step9: Evaluate the model on hierarchical classification tasks; step10: Download the grounded Flickr dataset for interpolation; step11: Perform interpolation between points in the hyperbolic space; step12: Download and pre-process the GRIT dataset for training; step13: Train the HyCoCLIP-ViT-S/16 model; step14: Visualize spatial norms in hyperbolic space", "executed_cmds": "git clone git@github.com:PalAvik/hycoclip.git;cd hycoclip;conda create -n hycoclip python=3.9 --yes", "cmd_prefix": "conda", "cmd_postfix": "activate hycoclip", "target_cmd": "conda activate hycoclip"}
{"uuid": "65c7cff4-63e7-4fca-8968-bb2ddb2021be", "execution_plan": "step1: Clone the repository and set up the Conda environment; step2: Install PyTorch, torchvision, and other dependencies; step3: Download pre-trained models and place them in the checkpoints directory; step4: Set up evaluation datasets for zero-shot image classification (ImageNet and Stanford Cars); step5: Set up evaluation datasets for zero-shot image and text retrieval (COCO captions and Flickr30k captions); step6: Set up the WordNet hierarchy for hierarchical classification; step7: Evaluate the model on zero-shot image classification tasks; step8: Evaluate the model on zero-shot image and text retrieval tasks; step9: Evaluate the model on hierarchical classification tasks; step10: Download the grounded Flickr dataset for interpolation; step11: Perform interpolation between points in the hyperbolic space; step12: Download and pre-process the GRIT dataset for training; step13: Train the HyCoCLIP-ViT-S/16 model; step14: Visualize spatial norms in hyperbolic space", "executed_cmds": "git clone git@github.com:PalAvik/hycoclip.git;cd hycoclip;conda create -n hycoclip python=3.9 --yes;conda activate hycoclip", "cmd_prefix": "python -m pip", "cmd_postfix": "install --pre timm", "target_cmd": "python -m pip install --pre timm"}
{"uuid": "62f047f6-12af-4a9c-b456-a1fe11063216", "execution_plan": "step1: Clone the repository and set up the Conda environment; step2: Install PyTorch, torchvision, and other dependencies; step3: Download pre-trained models and place them in the checkpoints directory; step4: Set up evaluation datasets for zero-shot image classification (ImageNet and Stanford Cars); step5: Set up evaluation datasets for zero-shot image and text retrieval (COCO captions and Flickr30k captions); step6: Set up the WordNet hierarchy for hierarchical classification; step7: Evaluate the model on zero-shot image classification tasks; step8: Evaluate the model on zero-shot image and text retrieval tasks; step9: Evaluate the model on hierarchical classification tasks; step10: Download the grounded Flickr dataset for interpolation; step11: Perform interpolation between points in the hyperbolic space; step12: Download and pre-process the GRIT dataset for training; step13: Train the HyCoCLIP-ViT-S/16 model; step14: Visualize spatial norms in hyperbolic space", "executed_cmds": "git clone git@github.com:PalAvik/hycoclip.git;cd hycoclip;conda create -n hycoclip python=3.9 --yes;conda activate hycoclip;python -m pip install --pre timm;python -m pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "setup.py develop", "target_cmd": "python setup.py develop"}
{"uuid": "4936a451-cb4d-4071-b172-6bcae4c2a7ca", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate lefusion", "target_cmd": "conda activate lefusion"}
{"uuid": "37f6bc1a-4d48-4bb1-8b52-025bc944ecd1", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git", "cmd_prefix": "pip", "cmd_postfix": "install pip==22.3.1", "target_cmd": "pip install pip==22.3.1"}
{"uuid": "22e92707-3f61-43e0-b817-7bd776ab5384", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "8d8bb798-e24f-4da7-bc0b-3b0a3b45ac8a", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt", "cmd_prefix": "mkdir", "cmd_postfix": "data", "target_cmd": "mkdir data"}
{"uuid": "fa4691c4-88e2-4243-bba7-fbcbc966687c", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data", "cmd_prefix": "cd", "cmd_postfix": "data", "target_cmd": "cd data"}
{"uuid": "7e99496f-6014-44b8-9ccc-a28eaf6f70eb", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data", "cmd_prefix": "mkdir", "cmd_postfix": "LIDC", "target_cmd": "mkdir LIDC"}
{"uuid": "afd71b87-9f20-41d1-9fd8-0da12e250b8c", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC", "cmd_prefix": "cd", "cmd_postfix": "LIDC", "target_cmd": "cd LIDC"}
{"uuid": "0b9b241f-1b46-4e11-a580-6737fdd451f1", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC", "cmd_prefix": "wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar", "cmd_postfix": "-O Pathological.tar", "target_cmd": "wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar"}
{"uuid": "b6ea09cd-6d3b-4e4c-8e8e-221fc11fa959", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar", "cmd_prefix": "wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar", "cmd_postfix": "-O Normal.tar", "target_cmd": "wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar"}
{"uuid": "45bfa296-912d-438b-8862-4789054b3f1e", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar", "cmd_prefix": "tar", "cmd_postfix": "-xvf Normal.tar", "target_cmd": "tar -xvf Normal.tar"}
{"uuid": "67a00b14-52c1-4819-870c-4eb1223a2e3a", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar", "cmd_prefix": "wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar", "cmd_postfix": "-O Demo.tar", "target_cmd": "wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar"}
{"uuid": "f014fe43-4212-4ee5-9a87-44618efaf91e", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar", "cmd_prefix": "tar", "cmd_postfix": "-xvf Demo.tar", "target_cmd": "tar -xvf Demo.tar"}
{"uuid": "e3b98601-69ce-454b-ad24-67965ea3bc7a", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "80f1a419-32b2-416f-bab4-c0211f5fd2c8", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..", "cmd_prefix": "mkdir", "cmd_postfix": "EMIDEC", "target_cmd": "mkdir EMIDEC"}
{"uuid": "79b620ae-7869-446b-8351-4e10e00c49d4", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC", "cmd_prefix": "cd", "cmd_postfix": "EMIDEC", "target_cmd": "cd EMIDEC"}
{"uuid": "4bf78cf1-c452-4887-8269-471156919dd4", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC", "cmd_prefix": "wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar", "cmd_postfix": "-O Normal.tar", "target_cmd": "wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar"}
{"uuid": "f03b5d11-e61a-4d5b-81d3-bb373f18417e", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar", "cmd_prefix": "tar", "cmd_postfix": "-xvf Normal.tar", "target_cmd": "tar -xvf Normal.tar"}
{"uuid": "3acdcc9a-17e5-4683-95ad-29dec8521c2e", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar", "cmd_prefix": "wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar", "cmd_postfix": "-O Pathological.tar", "target_cmd": "wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar"}
{"uuid": "0dfd1390-77d6-4899-819a-93dedc1a04f7", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar", "cmd_prefix": "cd", "cmd_postfix": "../..", "target_cmd": "cd ../.."}
{"uuid": "8663c87e-a5d5-4f6a-b012-96303cbf5a68", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..", "cmd_prefix": "cd", "cmd_postfix": "LeFusion", "target_cmd": "cd LeFusion"}
{"uuid": "a58cfa39-01f6-4c60-bca5-782af668e057", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion", "cmd_prefix": "mkdir", "cmd_postfix": "LeFusion_Model", "target_cmd": "mkdir LeFusion_Model"}
{"uuid": "a03ff3ce-4f68-45aa-81f2-5ee7f7e234d6", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model", "cmd_prefix": "cd", "cmd_postfix": "LeFusion_Model", "target_cmd": "cd LeFusion_Model"}
{"uuid": "6e06579a-5f95-412f-97f3-34d6cfb63ce9", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model", "cmd_prefix": "mkdir", "cmd_postfix": "LIDC", "target_cmd": "mkdir LIDC"}
{"uuid": "42e79693-0733-4013-bd78-c1c16bc931ce", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC", "cmd_prefix": "cd", "cmd_postfix": "LIDC", "target_cmd": "cd LIDC"}
{"uuid": "bb69bc43-a9b7-47e5-94fc-2ef698e46b11", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC", "cmd_prefix": "wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt", "cmd_postfix": "-O lidc.pt", "target_cmd": "wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt"}
{"uuid": "abe7e11e-0b31-4fed-9bc1-dfa2d3a9d2f9", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "de72399b-ca4e-4b3e-b714-a76e2833bbca", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt;cd ..", "cmd_prefix": "mkdir", "cmd_postfix": "EMIDEC", "target_cmd": "mkdir EMIDEC"}
{"uuid": "c64b2c58-19ff-4711-9632-03caedaeccdb", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt;cd ..;mkdir EMIDEC", "cmd_prefix": "cd", "cmd_postfix": "EMIDEC", "target_cmd": "cd EMIDEC"}
{"uuid": "1b7dae57-6646-4f2d-9487-76fdfd2fbd52", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt;cd ..;mkdir EMIDEC;cd EMIDEC", "cmd_prefix": "wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/emidec.pt", "cmd_postfix": "-O emidec.pt", "target_cmd": "wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/emidec.pt -O emidec.pt"}
{"uuid": "6cd33f40-c72d-4b98-8b36-0589829d2ac6", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/emidec.pt -O emidec.pt", "cmd_prefix": "chmod", "cmd_postfix": "+x lidc_train.sh", "target_cmd": "chmod +x lidc_train.sh"}
{"uuid": "d91ae9f0-2211-4fe8-a3ed-019776220e7e", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/emidec.pt -O emidec.pt;chmod +x lidc_train.sh;./lidc_train.sh", "cmd_prefix": "chmod", "cmd_postfix": "+x emidec_train.sh", "target_cmd": "chmod +x emidec_train.sh"}
{"uuid": "6aee6544-d019-46fe-bd32-5f4ae87a9b1a", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/emidec.pt -O emidec.pt;chmod +x lidc_train.sh;./lidc_train.sh;chmod +x emidec_train.sh;./emidec_train.sh", "cmd_prefix": "chmod", "cmd_postfix": "+x lidc_inference.sh", "target_cmd": "chmod +x lidc_inference.sh"}
{"uuid": "496b8bc0-32d0-4ce4-97e9-605c413f651f", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/emidec.pt -O emidec.pt;chmod +x lidc_train.sh;./lidc_train.sh;chmod +x emidec_train.sh;./emidec_train.sh;chmod +x lidc_inference.sh;./lidc_inference.sh;chmod +x emidec_inference.sh;./emidec_inference.sh", "cmd_prefix": "cd", "cmd_postfix": "DiffMask", "target_cmd": "cd DiffMask"}
{"uuid": "39a1cb44-160b-4b4d-9e9d-232e6c3e0bd4", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/emidec.pt -O emidec.pt;chmod +x lidc_train.sh;./lidc_train.sh;chmod +x emidec_train.sh;./emidec_train.sh;chmod +x lidc_inference.sh;./lidc_inference.sh;chmod +x emidec_inference.sh;./emidec_inference.sh;cd DiffMask", "cmd_prefix": "mkdir", "cmd_postfix": "DiffMask_Model", "target_cmd": "mkdir DiffMask_Model"}
{"uuid": "766ae87b-93da-46e6-9e6e-232142166e55", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/emidec.pt -O emidec.pt;chmod +x lidc_train.sh;./lidc_train.sh;chmod +x emidec_train.sh;./emidec_train.sh;chmod +x lidc_inference.sh;./lidc_inference.sh;chmod +x emidec_inference.sh;./emidec_inference.sh;cd DiffMask;mkdir DiffMask_Model", "cmd_prefix": "cd", "cmd_postfix": "DiffMask_Model", "target_cmd": "cd DiffMask_Model"}
{"uuid": "feb2a11e-0f5d-47a2-87d9-4b36f3576a5d", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/emidec.pt -O emidec.pt;chmod +x lidc_train.sh;./lidc_train.sh;chmod +x emidec_train.sh;./emidec_train.sh;chmod +x lidc_inference.sh;./lidc_inference.sh;chmod +x emidec_inference.sh;./emidec_inference.sh;cd DiffMask;mkdir DiffMask_Model;cd DiffMask_Model", "cmd_prefix": "wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/diffmask.pt", "cmd_postfix": "-O diffmask.pt", "target_cmd": "wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/diffmask.pt -O diffmask.pt"}
{"uuid": "a6f33e0f-c40e-485d-be13-adb5e3a7275a", "execution_plan": "step1: Set up the virtual environment and install necessary dependencies; step2: Download and prepare the LIDC-IDRI and EMIDEC datasets; step3: Download the pre-trained LeFusion Model weights; step4: Train the LeFusion Model on the LIDC or EMIDEC dataset (optional if using pre-trained weights); step5: Perform inference using the trained or pre-trained LeFusion Model; step6: Download and use the DiffMask Model for mask generation (optional); step7: Train and perform inference with the DiffMask Model (optional)", "executed_cmds": "conda create -n lefusion python=3.10;conda activate lefusion;git clone https://github.com/M3DV/LeFusion.git;pip install pip==22.3.1;cd LeFusion/LeFusion_LIDC;pip install -r requirements.txt;mkdir data;cd data;mkdir LIDC;cd LIDC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar;tar -xvf Demo.tar;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar;tar -xvf Normal.tar;wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar;tar -xvf Pathological.tar;cd ../..;cd LeFusion;mkdir LeFusion_Model;cd LeFusion_Model;mkdir LIDC;cd LIDC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt;cd ..;mkdir EMIDEC;cd EMIDEC;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/emidec.pt -O emidec.pt;chmod +x lidc_train.sh;./lidc_train.sh;chmod +x emidec_train.sh;./emidec_train.sh;chmod +x lidc_inference.sh;./lidc_inference.sh;chmod +x emidec_inference.sh;./emidec_inference.sh;cd DiffMask;mkdir DiffMask_Model;cd DiffMask_Model;wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/diffmask.pt -O diffmask.pt", "cmd_prefix": "cd", "cmd_postfix": "../..", "target_cmd": "cd ../.."}
{"uuid": "293251fb-85c4-4f07-a8fd-ad32217380d1", "execution_plan": "step1: Install the required libraries for the JAX environment, including Flax, Optax/JaxOpt, and Matplotlib; step2: Clone the repository to your local machine; step3: Navigate to the desired problem directory (Poisson 2D, Helmholtz 2D, or Allen-Cahn) to run specific experiments; step4: Run the Jupyter notebook tutorial (prediction_plots.ipynb) for the selected problem to execute the experiment and generate plots; step5: For experiments comparing against state-of-the-art, integrate with JaxPi by accessing the ActNet branch of the JaxPi repository", "executed_cmds": "pip install flax optax jaxopt matplotlib;git clone <repository_url>", "cmd_prefix": "cd", "cmd_postfix": "<problem_directory>", "target_cmd": "cd <problem_directory>"}
{"uuid": "20aa779c-ccc5-48df-91d3-44c26d4886da", "execution_plan": "step1: Request access to the MMIE dataset on HuggingFace and download all files, including unzipping the images.tar.gz file; step2: Clone the MMIE repository from GitHub; step3: Create a Conda environment for the project and install the required dependencies; step4: Request access to the MMIE-Score model on HuggingFace and prepare it for evaluation; step5: Prepare the input data in the specified JSON format and ensure the correct file structure; step6: Run the evaluation script using the main.py file with the appropriate arguments", "executed_cmds": "git clone https://github.com/Lillianwei-h/MMIE", "cmd_prefix": "cd", "cmd_postfix": "MMIE", "target_cmd": "cd MMIE"}
{"uuid": "e114cb99-9610-43ee-b5dd-45ee37a8659a", "execution_plan": "step1: Request access to the MMIE dataset on HuggingFace and download all files, including unzipping the images.tar.gz file; step2: Clone the MMIE repository from GitHub; step3: Create a Conda environment for the project and install the required dependencies; step4: Request access to the MMIE-Score model on HuggingFace and prepare it for evaluation; step5: Prepare the input data in the specified JSON format and ensure the correct file structure; step6: Run the evaluation script using the main.py file with the appropriate arguments", "executed_cmds": "git clone https://github.com/Lillianwei-h/MMIE;cd MMIE", "cmd_prefix": "conda create", "cmd_postfix": "-n MMIE python=3.11", "target_cmd": "conda create -n MMIE python=3.11"}
{"uuid": "94699158-7177-4892-a2bb-8e20bc4798fa", "execution_plan": "step1: Request access to the MMIE dataset on HuggingFace and download all files, including unzipping the images.tar.gz file; step2: Clone the MMIE repository from GitHub; step3: Create a Conda environment for the project and install the required dependencies; step4: Request access to the MMIE-Score model on HuggingFace and prepare it for evaluation; step5: Prepare the input data in the specified JSON format and ensure the correct file structure; step6: Run the evaluation script using the main.py file with the appropriate arguments", "executed_cmds": "git clone https://github.com/Lillianwei-h/MMIE;cd MMIE;conda create -n MMIE python=3.11", "cmd_prefix": "conda", "cmd_postfix": "activate MMIE", "target_cmd": "conda activate MMIE"}
{"uuid": "b05850d7-5f70-4c24-85ec-e176ebf5fc8d", "execution_plan": "step1: Request access to the MMIE dataset on HuggingFace and download all files, including unzipping the images.tar.gz file; step2: Clone the MMIE repository from GitHub; step3: Create a Conda environment for the project and install the required dependencies; step4: Request access to the MMIE-Score model on HuggingFace and prepare it for evaluation; step5: Prepare the input data in the specified JSON format and ensure the correct file structure; step6: Run the evaluation script using the main.py file with the appropriate arguments", "executed_cmds": "git clone https://github.com/Lillianwei-h/MMIE;cd MMIE;conda create -n MMIE python=3.11;conda activate MMIE", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "c2c97fca-b480-4468-9e13-61254b7a50db", "execution_plan": "step1: Request access to the MMIE dataset on HuggingFace and download all files, including unzipping the images.tar.gz file; step2: Clone the MMIE repository from GitHub; step3: Create a Conda environment for the project and install the required dependencies; step4: Request access to the MMIE-Score model on HuggingFace and prepare it for evaluation; step5: Prepare the input data in the specified JSON format and ensure the correct file structure; step6: Run the evaluation script using the main.py file with the appropriate arguments", "executed_cmds": "git clone https://github.com/Lillianwei-h/MMIE;cd MMIE;conda create -n MMIE python=3.11;conda activate MMIE;pip install -r requirements.txt", "cmd_prefix": "pip", "cmd_postfix": "install flash_attn", "target_cmd": "pip install flash_attn"}
{"uuid": "979bc3e0-b197-476d-baf5-82cee1d7f886", "execution_plan": "step1: Install Python 3.10 or higher and set up the project environment; step2: Clone the dictionary learning repository as a submodule; step3: Install project dependencies using pip; step4: Download or train sparse autoencoders for Pythia 70M; step5: Prepare the Subject-Verb Agreement dataset or Bias in Bios dataset as needed; step6: Discover a circuit for Subject-Verb Agreement or Bias in Bios using provided scripts; step7: Evaluate the faithfulness and completeness of the discovered circuit; step8: Generate a circuit for the Bias in Bios classifier using the provided notebook; step9: Explore and download clusters using the provided interface; step10: Generate a circuit for a downloaded cluster using the provided script", "executed_cmds": "git submodule update --init", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "27890376-552e-4a53-81dc-ab8b175d232b", "execution_plan": "step1: Create a conda environment and install necessary dependencies; step2: Clone and install LassoBench and NASLib repositories; step3: Install the current repository; step4: Modify requirements.txt for NASLib if installation fails; step5: Download required data and executables for benchmarks; step6: Install mujoco210 and gym for the Humanoid benchmark; step7: Run experiments using the provided run script; step8: Use the provided SBATCH file for running experiments on HPC with Slurm; step9: Conduct ablation study using the code in the ablation directory", "executed_cmds": "conda create -n gp_env python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate gp_env", "target_cmd": "conda activate gp_env"}
{"uuid": "49b6f8d9-d512-4518-9807-b23af870c5fb", "execution_plan": "step1: Create a conda environment and install necessary dependencies; step2: Clone and install LassoBench and NASLib repositories; step3: Install the current repository; step4: Modify requirements.txt for NASLib if installation fails; step5: Download required data and executables for benchmarks; step6: Install mujoco210 and gym for the Humanoid benchmark; step7: Run experiments using the provided run script; step8: Use the provided SBATCH file for running experiments on HPC with Slurm; step9: Conduct ablation study using the code in the ablation directory", "executed_cmds": "conda create -n gp_env python=3.8;conda activate gp_env", "cmd_prefix": "pip install --upgrade", "cmd_postfix": "pip setuptools wheel", "target_cmd": "pip install --upgrade pip setuptools wheel"}
{"uuid": "d459c431-a6cd-4e8f-ad6e-b53e85b05093", "execution_plan": "step1: Create a conda environment and install necessary dependencies; step2: Clone and install LassoBench and NASLib repositories; step3: Install the current repository; step4: Modify requirements.txt for NASLib if installation fails; step5: Download required data and executables for benchmarks; step6: Install mujoco210 and gym for the Humanoid benchmark; step7: Run experiments using the provided run script; step8: Use the provided SBATCH file for running experiments on HPC with Slurm; step9: Conduct ablation study using the code in the ablation directory", "executed_cmds": "conda create -n gp_env python=3.8;conda activate gp_env;pip install --upgrade pip setuptools wheel;git clone https://github.com/ksehic/LassoBench.git;git clone https://github.com/automl/NASLib.git", "cmd_prefix": "cd", "cmd_postfix": "LassoBench", "target_cmd": "cd LassoBench"}
{"uuid": "5297e30e-88e0-47a9-a6b2-86461375612c", "execution_plan": "step1: Create a conda environment and install necessary dependencies; step2: Clone and install LassoBench and NASLib repositories; step3: Install the current repository; step4: Modify requirements.txt for NASLib if installation fails; step5: Download required data and executables for benchmarks; step6: Install mujoco210 and gym for the Humanoid benchmark; step7: Run experiments using the provided run script; step8: Use the provided SBATCH file for running experiments on HPC with Slurm; step9: Conduct ablation study using the code in the ablation directory", "executed_cmds": "conda create -n gp_env python=3.8;conda activate gp_env;pip install --upgrade pip setuptools wheel;git clone https://github.com/ksehic/LassoBench.git;git clone https://github.com/automl/NASLib.git;cd LassoBench", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "659bd014-514e-4efe-bade-670771dff4ae", "execution_plan": "step1: Create a conda environment and install necessary dependencies; step2: Clone and install LassoBench and NASLib repositories; step3: Install the current repository; step4: Modify requirements.txt for NASLib if installation fails; step5: Download required data and executables for benchmarks; step6: Install mujoco210 and gym for the Humanoid benchmark; step7: Run experiments using the provided run script; step8: Use the provided SBATCH file for running experiments on HPC with Slurm; step9: Conduct ablation study using the code in the ablation directory", "executed_cmds": "conda create -n gp_env python=3.8;conda activate gp_env;pip install --upgrade pip setuptools wheel;git clone https://github.com/ksehic/LassoBench.git;git clone https://github.com/automl/NASLib.git;cd LassoBench;pip install -e .", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "debb0c16-df9f-491b-8946-b3a3ac57f02a", "execution_plan": "step1: Create a conda environment and install necessary dependencies; step2: Clone and install LassoBench and NASLib repositories; step3: Install the current repository; step4: Modify requirements.txt for NASLib if installation fails; step5: Download required data and executables for benchmarks; step6: Install mujoco210 and gym for the Humanoid benchmark; step7: Run experiments using the provided run script; step8: Use the provided SBATCH file for running experiments on HPC with Slurm; step9: Conduct ablation study using the code in the ablation directory", "executed_cmds": "conda create -n gp_env python=3.8;conda activate gp_env;pip install --upgrade pip setuptools wheel;git clone https://github.com/ksehic/LassoBench.git;git clone https://github.com/automl/NASLib.git;cd LassoBench;pip install -e .;cd ..", "cmd_prefix": "cd", "cmd_postfix": "NASLib", "target_cmd": "cd NASLib"}
{"uuid": "28b9d635-9172-4873-948b-40d32eacf722", "execution_plan": "step1: Create a conda environment and install necessary dependencies; step2: Clone and install LassoBench and NASLib repositories; step3: Install the current repository; step4: Modify requirements.txt for NASLib if installation fails; step5: Download required data and executables for benchmarks; step6: Install mujoco210 and gym for the Humanoid benchmark; step7: Run experiments using the provided run script; step8: Use the provided SBATCH file for running experiments on HPC with Slurm; step9: Conduct ablation study using the code in the ablation directory", "executed_cmds": "conda create -n gp_env python=3.8;conda activate gp_env;pip install --upgrade pip setuptools wheel;git clone https://github.com/ksehic/LassoBench.git;git clone https://github.com/automl/NASLib.git;cd LassoBench;pip install -e .;cd ..;cd NASLib", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "81dec7f5-484f-4fa6-a420-5c45d1a73483", "execution_plan": "step1: Create a conda environment and install necessary dependencies; step2: Clone and install LassoBench and NASLib repositories; step3: Install the current repository; step4: Modify requirements.txt for NASLib if installation fails; step5: Download required data and executables for benchmarks; step6: Install mujoco210 and gym for the Humanoid benchmark; step7: Run experiments using the provided run script; step8: Use the provided SBATCH file for running experiments on HPC with Slurm; step9: Conduct ablation study using the code in the ablation directory", "executed_cmds": "conda create -n gp_env python=3.8;conda activate gp_env;pip install --upgrade pip setuptools wheel;git clone https://github.com/ksehic/LassoBench.git;git clone https://github.com/automl/NASLib.git;cd LassoBench;pip install -e .;cd ..;cd NASLib;pip install -e .", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "98bd5e97-3207-4696-956a-bd74cb88da49", "execution_plan": "step1: Create a conda environment and install necessary dependencies; step2: Clone and install LassoBench and NASLib repositories; step3: Install the current repository; step4: Modify requirements.txt for NASLib if installation fails; step5: Download required data and executables for benchmarks; step6: Install mujoco210 and gym for the Humanoid benchmark; step7: Run experiments using the provided run script; step8: Use the provided SBATCH file for running experiments on HPC with Slurm; step9: Conduct ablation study using the code in the ablation directory", "executed_cmds": "conda create -n gp_env python=3.8;conda activate gp_env;pip install --upgrade pip setuptools wheel;git clone https://github.com/ksehic/LassoBench.git;git clone https://github.com/automl/NASLib.git;cd LassoBench;pip install -e .;cd ..;cd NASLib;pip install -e .;cd ..", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "50021093-e633-4dca-8ba3-cac4daa2cd21", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git", "cmd_prefix": "cd", "cmd_postfix": "Reti-Diff-demo", "target_cmd": "cd Reti-Diff-demo"}
{"uuid": "c33c6ecd-2d88-491f-b5b1-0ef495f7a628", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9", "cmd_prefix": "conda", "cmd_postfix": "activate Reti-Diff", "target_cmd": "conda activate Reti-Diff"}
{"uuid": "432862ec-4d37-490d-83b0-e50f6da68714", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "fed9238c-6046-44f0-a67d-715c1f2faa5b", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "setup.py develop", "target_cmd": "python setup.py develop"}
{"uuid": "e5013dbc-f16c-40d0-8c0e-818074472430", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git", "cmd_prefix": "cd", "cmd_postfix": "BasicSR", "target_cmd": "cd BasicSR"}
{"uuid": "b0a3deaf-b39e-4011-aff9-b5f167686f1b", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "76c3619f-f67b-4cf0-b45b-8e6414606cc6", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt", "cmd_prefix": "python", "cmd_postfix": "setup.py develop", "target_cmd": "python setup.py develop"}
{"uuid": "3dd805dc-afc2-4472-ad17-ecab45474153", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop", "cmd_prefix": "cd", "cmd_postfix": "..", "target_cmd": "cd .."}
{"uuid": "4fcf8ec7-d7c0-4e04-8aa2-99ae8c416cd3", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..", "cmd_prefix": "sh", "cmd_postfix": "trainS1_LLIE.sh", "target_cmd": "sh trainS1_LLIE.sh"}
{"uuid": "c10c81e0-0d27-45c0-b6f0-27eabfb3550c", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;sh trainS1_LLIE.sh", "cmd_prefix": "sh", "cmd_postfix": "trainS2_LLIE.sh", "target_cmd": "sh trainS2_LLIE.sh"}
{"uuid": "cb297702-9a80-4a49-b98d-90c1ae9255e0", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;sh trainS1_LLIE.sh;sh trainS2_LLIE.sh", "cmd_prefix": "sh", "cmd_postfix": "trainS1_UIE.sh", "target_cmd": "sh trainS1_UIE.sh"}
{"uuid": "03e4351a-05a0-4b0f-b9e2-3b3cc6f141f7", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;sh trainS1_LLIE.sh;sh trainS2_LLIE.sh;sh trainS1_UIE.sh", "cmd_prefix": "sh", "cmd_postfix": "trainS2_UIE.sh", "target_cmd": "sh trainS2_UIE.sh"}
{"uuid": "ca997f9f-92af-4715-accc-48c22a0af8a8", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;sh trainS1_LLIE.sh;sh trainS2_LLIE.sh;sh trainS1_UIE.sh;sh trainS2_UIE.sh", "cmd_prefix": "sh", "cmd_postfix": "trainS1_Backlit.sh", "target_cmd": "sh trainS1_Backlit.sh"}
{"uuid": "000fc9d8-6133-4269-9d1d-19cb83fc796a", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;sh trainS1_LLIE.sh;sh trainS2_LLIE.sh;sh trainS1_UIE.sh;sh trainS2_UIE.sh;sh trainS1_Backlit.sh", "cmd_prefix": "sh", "cmd_postfix": "trainS2_Backlit.sh", "target_cmd": "sh trainS2_Backlit.sh"}
{"uuid": "dbbb15e8-7e12-4148-82d7-3e049e1b11d3", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;sh trainS1_LLIE.sh;sh trainS2_LLIE.sh;sh trainS1_UIE.sh;sh trainS2_UIE.sh;sh trainS1_Backlit.sh;sh trainS2_Backlit.sh", "cmd_prefix": "sh", "cmd_postfix": "test_LLIE_syn.sh", "target_cmd": "sh test_LLIE_syn.sh"}
{"uuid": "1e4e4b4d-4f58-4d75-bc18-5f4c2edd19ec", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;sh trainS1_LLIE.sh;sh trainS2_LLIE.sh;sh trainS1_UIE.sh;sh trainS2_UIE.sh;sh trainS1_Backlit.sh;sh trainS2_Backlit.sh;sh test_LLIE_syn.sh", "cmd_prefix": "sh", "cmd_postfix": "test_LLIE_real.sh", "target_cmd": "sh test_LLIE_real.sh"}
{"uuid": "221ddf0e-9402-4e5f-8eef-284725902656", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;sh trainS1_LLIE.sh;sh trainS2_LLIE.sh;sh trainS1_UIE.sh;sh trainS2_UIE.sh;sh trainS1_Backlit.sh;sh trainS2_Backlit.sh;sh test_LLIE_syn.sh;sh test_LLIE_real.sh", "cmd_prefix": "sh", "cmd_postfix": "test_UIE_LSUI.sh", "target_cmd": "sh test_UIE_LSUI.sh"}
{"uuid": "ede7ff78-4a30-400b-806b-8995dababec9", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;sh trainS1_LLIE.sh;sh trainS2_LLIE.sh;sh trainS1_UIE.sh;sh trainS2_UIE.sh;sh trainS1_Backlit.sh;sh trainS2_Backlit.sh;sh test_LLIE_syn.sh;sh test_LLIE_real.sh;sh test_UIE_LSUI.sh", "cmd_prefix": "sh", "cmd_postfix": "test_UIE_UIEB.sh", "target_cmd": "sh test_UIE_UIEB.sh"}
{"uuid": "6dde94ca-7cca-45ca-b98a-ae54b733c86d", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a Conda environment with Python 3.9; step3: Install PyTorch and other dependencies; step4: Install BasicSR and its dependencies; step5: Download and place the pretrained models in the specified folder; step6: Prepare the training datasets and place them in the Datasets folder; step7: Modify the config files and shell scripts for training; step8: Run the training scripts for the desired tasks (LLIE, UIE, or Backlit); step9: Prepare the test datasets and place them in the Datasets folder; step10: Run the testing scripts for the desired tasks (LLIE, UIE, or Backlit)", "executed_cmds": "git clone https://github.com/cnyvfang/Reti-Diff-demo.git;cd Reti-Diff-demo;conda create -n Reti-Diff python=3.9;conda activate Reti-Diff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia;pip install numpy==1.25.2;pip install -r requirements.txt;python setup.py develop;git clone https://github.com/xinntao/BasicSR.git;cd BasicSR;pip install tb-nightly -i https://mirrors.aliyun.com/pypi/simple;pip install -r requirements.txt;python setup.py develop;cd ..;sh trainS1_LLIE.sh;sh trainS2_LLIE.sh;sh trainS1_UIE.sh;sh trainS2_UIE.sh;sh trainS1_Backlit.sh;sh trainS2_Backlit.sh;sh test_LLIE_syn.sh;sh test_LLIE_real.sh;sh test_UIE_LSUI.sh;sh test_UIE_UIEB.sh", "cmd_prefix": "sh", "cmd_postfix": "test_Backlit.sh", "target_cmd": "sh test_Backlit.sh"}
{"uuid": "abf83834-c990-423d-aa6d-1e1ea5051222", "execution_plan": "step1: Install the required dependencies and set up the conda environment; step2: Prepare the datasets for testing or training; step3: Download the pretrained checkpoints for testing; step4: Modify the configuration options (e.g., dataroot_GT, dataroot_LQ, pretrain_model_G) for testing or training; step5: Run the test script to generate results using the UniDB model; step6: Run the training script for single or multi-GPU training (optional); step7: Use the provided interface for deraining tasks (optional); step8: Compute FID scores for generated images (optional)", "executed_cmds": "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121;conda create --name UniDB python=3.9", "cmd_prefix": "conda", "cmd_postfix": "activate UniDB", "target_cmd": "conda activate UniDB"}
{"uuid": "574919c7-c87f-4c88-bda3-2f07ed577d02", "execution_plan": "step1: Install the required dependencies and set up the conda environment; step2: Prepare the datasets for testing or training; step3: Download the pretrained checkpoints for testing; step4: Modify the configuration options (e.g., dataroot_GT, dataroot_LQ, pretrain_model_G) for testing or training; step5: Run the test script to generate results using the UniDB model; step6: Run the training script for single or multi-GPU training (optional); step7: Use the provided interface for deraining tasks (optional); step8: Compute FID scores for generated images (optional)", "executed_cmds": "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121;conda create --name UniDB python=3.9;conda activate UniDB", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "cc96f381-da86-4c48-916a-f8f599966794", "execution_plan": "step1: Install the required dependencies and set up the conda environment; step2: Prepare the datasets for testing or training; step3: Download the pretrained checkpoints for testing; step4: Modify the configuration options (e.g., dataroot_GT, dataroot_LQ, pretrain_model_G) for testing or training; step5: Run the test script to generate results using the UniDB model; step6: Run the training script for single or multi-GPU training (optional); step7: Use the provided interface for deraining tasks (optional); step8: Compute FID scores for generated images (optional)", "executed_cmds": "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121;conda create --name UniDB python=3.9;conda activate UniDB;pip install -r requirements.txt;python test.py -opt=options/test.yml;python train.py -opt=options/train.yml;python -m torch.distributed.launch --nproc_per_node=2 --master_port=1111 train.py -opt=options/train.yml --launcher pytorch", "cmd_prefix": "python", "cmd_postfix": "interface.py", "target_cmd": "python interface.py"}
{"uuid": "d88377d7-089c-4468-9b31-7f3be051e970", "execution_plan": "step1: Install the required dependencies and set up the conda environment; step2: Prepare the datasets for testing or training; step3: Download the pretrained checkpoints for testing; step4: Modify the configuration options (e.g., dataroot_GT, dataroot_LQ, pretrain_model_G) for testing or training; step5: Run the test script to generate results using the UniDB model; step6: Run the training script for single or multi-GPU training (optional); step7: Use the provided interface for deraining tasks (optional); step8: Compute FID scores for generated images (optional)", "executed_cmds": "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121;conda create --name UniDB python=3.9;conda activate UniDB;pip install -r requirements.txt;python test.py -opt=options/test.yml;python train.py -opt=options/train.yml;python -m torch.distributed.launch --nproc_per_node=2 --master_port=1111 train.py -opt=options/train.yml --launcher pytorch;python interface.py", "cmd_prefix": "pip", "cmd_postfix": "install pytorch-fid", "target_cmd": "pip install pytorch-fid"}
{"uuid": "fe9d49ee-af02-4fda-9d80-536066db9a83", "execution_plan": "step1: Clone the repository to your local machine; step2: Create a Conda environment for the project; step3: Activate the Conda environment; step4: Install the required dependencies using pip; step5: Install the project in editable mode; step6: Download pretrained models and activations (optional); step7: Run the quick start Python script to load a pre-trained transformer and train a Hybrid Sparse Autoencoder; step8: Reproduce all experiments by running the provided script; step9: Alternatively, run experiments individually", "executed_cmds": "git clone https://github.com/yourusername/mechanistic-interpretability-scale;cd mechanistic-interpretability-scale;conda create -n mech-interp python=3.10 -y", "cmd_prefix": "conda", "cmd_postfix": "activate mech-interp", "target_cmd": "conda activate mech-interp"}
{"uuid": "dfc70bd4-8aaa-4f63-b97a-d60eb4d6cf4c", "execution_plan": "step1: Clone the repository to your local machine; step2: Create a Conda environment for the project; step3: Activate the Conda environment; step4: Install the required dependencies using pip; step5: Install the project in editable mode; step6: Download pretrained models and activations (optional); step7: Run the quick start Python script to load a pre-trained transformer and train a Hybrid Sparse Autoencoder; step8: Reproduce all experiments by running the provided script; step9: Alternatively, run experiments individually", "executed_cmds": "git clone https://github.com/yourusername/mechanistic-interpretability-scale;cd mechanistic-interpretability-scale;conda create -n mech-interp python=3.10 -y;conda activate mech-interp", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "522123b8-ea08-4d00-908a-9c2e3c4ceed0", "execution_plan": "step1: Clone the repository to your local machine; step2: Create a Conda environment for the project; step3: Activate the Conda environment; step4: Install the required dependencies using pip; step5: Install the project in editable mode; step6: Download pretrained models and activations (optional); step7: Run the quick start Python script to load a pre-trained transformer and train a Hybrid Sparse Autoencoder; step8: Reproduce all experiments by running the provided script; step9: Alternatively, run experiments individually", "executed_cmds": "git clone https://github.com/yourusername/mechanistic-interpretability-scale;cd mechanistic-interpretability-scale;conda create -n mech-interp python=3.10 -y;conda activate mech-interp;pip install -r requirements.txt", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "c002258e-a7fa-4d84-bc00-d1c03c7496a1", "execution_plan": "step1: Install the required Python and PyTorch versions along with necessary dependencies; step2: Install additional dependencies for FoSR and BORF baselines if needed; step3: Install optional wandb for logging if desired; step4: Navigate to the src folder to run the code; step5: Run GCN+JDR on the Cora dataset; step6: Run rewiring baselines (DIGL, FoSR, BORF) on the Cora dataset; step7: Reproduce the results from the paper by running provided scripts; step8: Add a custom GNN to the repository and test it with JDR; step9: Add a custom dataset to the repository and tune hyperparameters; step10: Download the Twitch-gamers dataset if needed; step11: Generate a new cSBM dataset if needed", "executed_cmds": "pip install numba pandas networkx GraphRicciCurvature scikit-learn", "cmd_prefix": "pip", "cmd_postfix": "install wandb", "target_cmd": "pip install wandb"}
{"uuid": "a250bc35-5b88-4510-b786-d025bb744e0f", "execution_plan": "step1: Install the required Python and PyTorch versions along with necessary dependencies; step2: Install additional dependencies for FoSR and BORF baselines if needed; step3: Install optional wandb for logging if desired; step4: Navigate to the src folder to run the code; step5: Run GCN+JDR on the Cora dataset; step6: Run rewiring baselines (DIGL, FoSR, BORF) on the Cora dataset; step7: Reproduce the results from the paper by running provided scripts; step8: Add a custom GNN to the repository and test it with JDR; step9: Add a custom dataset to the repository and tune hyperparameters; step10: Download the Twitch-gamers dataset if needed; step11: Generate a new cSBM dataset if needed", "executed_cmds": "pip install numba pandas networkx GraphRicciCurvature scikit-learn;pip install wandb", "cmd_prefix": "cd", "cmd_postfix": "src", "target_cmd": "cd src"}
{"uuid": "e9e3dc6b-5b19-4c60-8086-2c69b4f3c4ae", "execution_plan": "step1: Install the required Python and PyTorch versions along with necessary dependencies; step2: Install additional dependencies for FoSR and BORF baselines if needed; step3: Install optional wandb for logging if desired; step4: Navigate to the src folder to run the code; step5: Run GCN+JDR on the Cora dataset; step6: Run rewiring baselines (DIGL, FoSR, BORF) on the Cora dataset; step7: Reproduce the results from the paper by running provided scripts; step8: Add a custom GNN to the repository and test it with JDR; step9: Add a custom dataset to the repository and tune hyperparameters; step10: Download the Twitch-gamers dataset if needed; step11: Generate a new cSBM dataset if needed", "executed_cmds": "pip install numba pandas networkx GraphRicciCurvature scikit-learn;pip install wandb;cd src;python train_model.py --dataset Cora --net GCN --data_split sparse --denoise_default GCN;python train_model.py --dataset Cora --net GCN --data_split sparse --rewire_default ppr;python train_model.py --dataset Cora --net GCN --data_split sparse --rewire_default fosr;python train_model.py --dataset Cora --net GCN --data_split sparse --rewire_default borf", "cmd_prefix": "source", "cmd_postfix": "run_csbm_exp.sh", "target_cmd": "source run_csbm_exp.sh"}
{"uuid": "5e9a0bac-5bd6-4df7-b918-332f917c2c66", "execution_plan": "step1: Install the required Python and PyTorch versions along with necessary dependencies; step2: Install additional dependencies for FoSR and BORF baselines if needed; step3: Install optional wandb for logging if desired; step4: Navigate to the src folder to run the code; step5: Run GCN+JDR on the Cora dataset; step6: Run rewiring baselines (DIGL, FoSR, BORF) on the Cora dataset; step7: Reproduce the results from the paper by running provided scripts; step8: Add a custom GNN to the repository and test it with JDR; step9: Add a custom dataset to the repository and tune hyperparameters; step10: Download the Twitch-gamers dataset if needed; step11: Generate a new cSBM dataset if needed", "executed_cmds": "pip install numba pandas networkx GraphRicciCurvature scikit-learn;pip install wandb;cd src;python train_model.py --dataset Cora --net GCN --data_split sparse --denoise_default GCN;python train_model.py --dataset Cora --net GCN --data_split sparse --rewire_default ppr;python train_model.py --dataset Cora --net GCN --data_split sparse --rewire_default fosr;python train_model.py --dataset Cora --net GCN --data_split sparse --rewire_default borf;source run_csbm_exp.sh", "cmd_prefix": "source", "cmd_postfix": "run_exp_table_1.sh", "target_cmd": "source run_exp_table_1.sh"}
{"uuid": "49a1fcc7-865f-4aaf-aaf6-7a2fa30f76c0", "execution_plan": "step1: Install the required Python and PyTorch versions along with necessary dependencies; step2: Install additional dependencies for FoSR and BORF baselines if needed; step3: Install optional wandb for logging if desired; step4: Navigate to the src folder to run the code; step5: Run GCN+JDR on the Cora dataset; step6: Run rewiring baselines (DIGL, FoSR, BORF) on the Cora dataset; step7: Reproduce the results from the paper by running provided scripts; step8: Add a custom GNN to the repository and test it with JDR; step9: Add a custom dataset to the repository and tune hyperparameters; step10: Download the Twitch-gamers dataset if needed; step11: Generate a new cSBM dataset if needed", "executed_cmds": "pip install numba pandas networkx GraphRicciCurvature scikit-learn;pip install wandb;cd src;python train_model.py --dataset Cora --net GCN --data_split sparse --denoise_default GCN;python train_model.py --dataset Cora --net GCN --data_split sparse --rewire_default ppr;python train_model.py --dataset Cora --net GCN --data_split sparse --rewire_default fosr;python train_model.py --dataset Cora --net GCN --data_split sparse --rewire_default borf;source run_csbm_exp.sh;source run_exp_table_1.sh", "cmd_prefix": "source", "cmd_postfix": "run_exp_table_2.sh", "target_cmd": "source run_exp_table_2.sh"}
{"uuid": "ee741602-f444-4db1-95c7-8be05265fb9f", "execution_plan": "step1: Set up the Python environment and install necessary dependencies; step2: Download and prepare the CarlaSC dataset; step3: Obtain and set up the pretrained checkpoints; step4: Train the VAE model on the CarlaSC dataset; step5: Save hexplane rollouts after VAE training; step6: Train the DiT model using the trained VAE; step7: Generate novel city scenes using the trained DiT model", "executed_cmds": "conda create -n dyncity python=3.10 -y", "cmd_prefix": "conda", "cmd_postfix": "activate dyncity", "target_cmd": "conda activate dyncity"}
{"uuid": "4c0e82f0-b033-4643-87ef-dea6f6cf2661", "execution_plan": "step1: Set up the Python environment and install necessary dependencies; step2: Download and prepare the CarlaSC dataset; step3: Obtain and set up the pretrained checkpoints; step4: Train the VAE model on the CarlaSC dataset; step5: Save hexplane rollouts after VAE training; step6: Train the DiT model using the trained VAE; step7: Generate novel city scenes using the trained DiT model", "executed_cmds": "conda create -n dyncity python=3.10 -y;conda activate dyncity", "cmd_prefix": "conda install pytorch==2.4.0 pytorch-cuda=11.8", "cmd_postfix": "-c pytorch -c nvidia", "target_cmd": "conda install pytorch==2.4.0 pytorch-cuda=11.8 -c pytorch -c nvidia"}
{"uuid": "87e717ef-f944-43e9-8ff8-28b0b62c4762", "execution_plan": "step1: Install the transfusion-pytorch package; step2: Install additional dependencies for examples if needed; step3: Install safetensors and related packages if encountering errors; step4: Import the Transfusion model and necessary libraries in your Python script; step5: Initialize the Transfusion model with desired parameters; step6: Prepare input data (text and/or modalities) for training or inference; step7: Train the model by passing input data and computing loss; step8: Sample from the trained model to generate multimodal outputs; step9: Pretrain on language data if needed by passing text-only input; step10: Run example scripts from the repository if desired", "executed_cmds": "pip install transfusion-pytorch", "cmd_prefix": "pip", "cmd_postfix": "install .[examples]", "target_cmd": "pip install .[examples]"}
{"uuid": "7415934d-680a-4743-bd92-a061b147bb71", "execution_plan": "step1: Install the required dependencies using the environment.yaml file; step2: Navigate to the /src folder to run the code; step3: Reproduce the results by executing the code in the /src folder", "executed_cmds": "conda env create -f environment.yaml", "cmd_prefix": "cd", "cmd_postfix": "src", "target_cmd": "cd src"}
{"uuid": "597f2767-3a23-4907-b2a7-e7daa2811029", "execution_plan": "step1: Set up the MuJoCo environment by following the OpenAI guideline and creating the required directory structure with the provided file-tree; step2: Create a Conda environment named 'unicorn' with Python 3.10.4 and activate it; step3: Install required dependencies and libraries including setuptools, wheel, cython, patchelf, pyOpenGL, and others listed in requirements.txt; step4: Download the provided datasets for validation and extract them into the specified directories (batch_data and batch_data_copy); step5: Run the training script for the specified environment type (ant_dir or hopper_param); step6: Apply a quick fix if encountering numpy attribute errors by substituting np.bool to bool or np.int to int in the code", "executed_cmds": "conda create --name unicorn python=3.10.4", "cmd_prefix": "conda", "cmd_postfix": "activate unicorn", "target_cmd": "conda activate unicorn"}
{"uuid": "c7b121b6-3945-4e71-8a6d-75a2af3ef020", "execution_plan": "step1: Set up the MuJoCo environment by following the OpenAI guideline and creating the required directory structure with the provided file-tree; step2: Create a Conda environment named 'unicorn' with Python 3.10.4 and activate it; step3: Install required dependencies and libraries including setuptools, wheel, cython, patchelf, pyOpenGL, and others listed in requirements.txt; step4: Download the provided datasets for validation and extract them into the specified directories (batch_data and batch_data_copy); step5: Run the training script for the specified environment type (ant_dir or hopper_param); step6: Apply a quick fix if encountering numpy attribute errors by substituting np.bool to bool or np.int to int in the code", "executed_cmds": "conda create --name unicorn python=3.10.4;conda activate unicorn;pip install setuptools==59.5.0;pip install wheel==0.37.1;pip install cython==0.29.32", "cmd_prefix": "pip", "cmd_postfix": "install patchelf", "target_cmd": "pip install patchelf"}
{"uuid": "15009f76-98e2-4ed1-b795-70a5b06601dc", "execution_plan": "step1: Set up the MuJoCo environment by following the OpenAI guideline and creating the required directory structure with the provided file-tree; step2: Create a Conda environment named 'unicorn' with Python 3.10.4 and activate it; step3: Install required dependencies and libraries including setuptools, wheel, cython, patchelf, pyOpenGL, and others listed in requirements.txt; step4: Download the provided datasets for validation and extract them into the specified directories (batch_data and batch_data_copy); step5: Run the training script for the specified environment type (ant_dir or hopper_param); step6: Apply a quick fix if encountering numpy attribute errors by substituting np.bool to bool or np.int to int in the code", "executed_cmds": "conda create --name unicorn python=3.10.4;conda activate unicorn;pip install setuptools==59.5.0;pip install wheel==0.37.1;pip install cython==0.29.32;pip install patchelf;pip install pyOpenGL -i https://pypi.douban.com/simple", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "420e6c19-8b8d-4fa3-adb4-fb913f4fbcdb", "execution_plan": "step1: Set up the MuJoCo environment by following the OpenAI guideline and creating the required directory structure with the provided file-tree; step2: Create a Conda environment named 'unicorn' with Python 3.10.4 and activate it; step3: Install required dependencies and libraries including setuptools, wheel, cython, patchelf, pyOpenGL, and others listed in requirements.txt; step4: Download the provided datasets for validation and extract them into the specified directories (batch_data and batch_data_copy); step5: Run the training script for the specified environment type (ant_dir or hopper_param); step6: Apply a quick fix if encountering numpy attribute errors by substituting np.bool to bool or np.int to int in the code", "executed_cmds": "conda create --name unicorn python=3.10.4;conda activate unicorn;pip install setuptools==59.5.0;pip install wheel==0.37.1;pip install cython==0.29.32;pip install patchelf;pip install pyOpenGL -i https://pypi.douban.com/simple;pip install -r requirements.txt", "cmd_prefix": "pip install", "cmd_postfix": "-U 'mujoco-py<2.2", "target_cmd": "pip install -U 'mujoco-py<2.2"}
{"uuid": "5f5ee152-34a4-4819-889f-62a3a3752630", "execution_plan": "step1: Set up the MuJoCo environment by following the OpenAI guideline and creating the required directory structure with the provided file-tree; step2: Create a Conda environment named 'unicorn' with Python 3.10.4 and activate it; step3: Install required dependencies and libraries including setuptools, wheel, cython, patchelf, pyOpenGL, and others listed in requirements.txt; step4: Download the provided datasets for validation and extract them into the specified directories (batch_data and batch_data_copy); step5: Run the training script for the specified environment type (ant_dir or hopper_param); step6: Apply a quick fix if encountering numpy attribute errors by substituting np.bool to bool or np.int to int in the code", "executed_cmds": "conda create --name unicorn python=3.10.4;conda activate unicorn;pip install setuptools==59.5.0;pip install wheel==0.37.1;pip install cython==0.29.32;pip install patchelf;pip install pyOpenGL -i https://pypi.douban.com/simple;pip install -r requirements.txt;pip install -U 'mujoco-py<2.2;>=2.1'", "cmd_prefix": "pip", "cmd_postfix": "install gin-config", "target_cmd": "pip install gin-config"}
{"uuid": "2c114d4c-5275-4fe9-8523-e8a134e24139", "execution_plan": "step1: Set up the MuJoCo environment by following the OpenAI guideline and creating the required directory structure with the provided file-tree; step2: Create a Conda environment named 'unicorn' with Python 3.10.4 and activate it; step3: Install required dependencies and libraries including setuptools, wheel, cython, patchelf, pyOpenGL, and others listed in requirements.txt; step4: Download the provided datasets for validation and extract them into the specified directories (batch_data and batch_data_copy); step5: Run the training script for the specified environment type (ant_dir or hopper_param); step6: Apply a quick fix if encountering numpy attribute errors by substituting np.bool to bool or np.int to int in the code", "executed_cmds": "conda create --name unicorn python=3.10.4;conda activate unicorn;pip install setuptools==59.5.0;pip install wheel==0.37.1;pip install cython==0.29.32;pip install patchelf;pip install pyOpenGL -i https://pypi.douban.com/simple;pip install -r requirements.txt;pip install -U 'mujoco-py<2.2;>=2.1';pip install gin-config", "cmd_prefix": "pip", "cmd_postfix": "install scikit-learn", "target_cmd": "pip install scikit-learn"}
{"uuid": "17aa2f39-09ec-4d1d-9c99-7775a54dbf0a", "execution_plan": "step1: Install Python 3.10 or higher and set up the environment; step2: Install project dependencies using pip; step3: Run all experiments by executing the provided shell scripts; step4: Generate figures and tables from the experiment results; step5: Clean up cache and previous results if reproducing new results", "executed_cmds": "pip install -r requirements.txt;bash tmscm_sym_ablation.sh;bash tmscm_sym_exogenous.sh", "cmd_prefix": "bash", "cmd_postfix": "tmscm_er_ablation.sh", "target_cmd": "bash tmscm_er_ablation.sh"}
{"uuid": "f3b4c4a6-c4bd-4f88-895d-ee796a8a0d76", "execution_plan": "step1: Install Python 3.10 or higher and set up the environment; step2: Install project dependencies using pip; step3: Run all experiments by executing the provided shell scripts; step4: Generate figures and tables from the experiment results; step5: Clean up cache and previous results if reproducing new results", "executed_cmds": "pip install -r requirements.txt;bash tmscm_sym_ablation.sh;bash tmscm_sym_exogenous.sh;bash tmscm_er_ablation.sh", "cmd_prefix": "python", "cmd_postfix": "graphics.py", "target_cmd": "python graphics.py"}
{"uuid": "7171c41f-6fc3-4272-a388-ed0a7a2178d3", "execution_plan": "step1: Clone the GitHub repository and set up the environment; step2: Train the agent on the Atari100k benchmark; step3: Train the agent on the DeepMind Control Suite; step4: Visualize experiments using TensorBoard; step5: Override hyperparameters for custom training; step6: Evaluate the trained agent; step7: Use script options for additional configurations", "executed_cmds": "git clone https://github.com/burchim/TWISTER && cd TWISTER;./install.sh", "cmd_prefix": "env_name=atari100k-alien run_name=atari100k", "cmd_postfix": "python3 main.py", "target_cmd": "env_name=atari100k-alien run_name=atari100k python3 main.py"}
{"uuid": "9fc3a89c-dbe5-4997-9b88-fd576d18f24b", "execution_plan": "step1: Clone the GitHub repository and set up the environment; step2: Train the agent on the Atari100k benchmark; step3: Train the agent on the DeepMind Control Suite; step4: Visualize experiments using TensorBoard; step5: Override hyperparameters for custom training; step6: Evaluate the trained agent; step7: Use script options for additional configurations", "executed_cmds": "git clone https://github.com/burchim/TWISTER && cd TWISTER;./install.sh;env_name=atari100k-alien run_name=atari100k python3 main.py", "cmd_prefix": "env_name=dmc-Acrobot-swingup run_name=dmc", "cmd_postfix": "python3 main.py", "target_cmd": "env_name=dmc-Acrobot-swingup run_name=dmc python3 main.py"}
{"uuid": "886db478-18bd-43b9-bda8-19d52ec01a37", "execution_plan": "step1: Clone the GitHub repository and set up the environment; step2: Train the agent on the Atari100k benchmark; step3: Train the agent on the DeepMind Control Suite; step4: Visualize experiments using TensorBoard; step5: Override hyperparameters for custom training; step6: Evaluate the trained agent; step7: Use script options for additional configurations", "executed_cmds": "git clone https://github.com/burchim/TWISTER && cd TWISTER;./install.sh;env_name=atari100k-alien run_name=atari100k python3 main.py;env_name=dmc-Acrobot-swingup run_name=dmc python3 main.py", "cmd_prefix": "tensorboard", "cmd_postfix": "--logdir ./callbacks", "target_cmd": "tensorboard --logdir ./callbacks"}
{"uuid": "6108f5d4-1813-4528-a328-99b2d8a6275d", "execution_plan": "step1: Clone the GitHub repository and set up the environment; step2: Train the agent on the Atari100k benchmark; step3: Train the agent on the DeepMind Control Suite; step4: Visualize experiments using TensorBoard; step5: Override hyperparameters for custom training; step6: Evaluate the trained agent; step7: Use script options for additional configurations", "executed_cmds": "git clone https://github.com/burchim/TWISTER && cd TWISTER;./install.sh;env_name=atari100k-alien run_name=atari100k python3 main.py;env_name=dmc-Acrobot-swingup run_name=dmc python3 main.py;tensorboard --logdir ./callbacks;env_name=atari100k-alien run_name=atari100k override_config='{\"num_envs\": 4", "cmd_prefix": "\"epochs\":", "cmd_postfix": "100", "target_cmd": "\"epochs\": 100"}
{"uuid": "b34c17ff-45e5-4be0-a37c-463d3eb8cd27", "execution_plan": "step1: Clone the GitHub repository and set up the environment; step2: Train the agent on the Atari100k benchmark; step3: Train the agent on the DeepMind Control Suite; step4: Visualize experiments using TensorBoard; step5: Override hyperparameters for custom training; step6: Evaluate the trained agent; step7: Use script options for additional configurations", "executed_cmds": "git clone https://github.com/burchim/TWISTER && cd TWISTER;./install.sh;env_name=atari100k-alien run_name=atari100k python3 main.py;env_name=dmc-Acrobot-swingup run_name=dmc python3 main.py;tensorboard --logdir ./callbacks;env_name=atari100k-alien run_name=atari100k override_config='{\"num_envs\": 4;\"epochs\": 100", "cmd_prefix": "\"eval_episode_saving_path\": \"./videos\"}'", "cmd_postfix": "python3 main.py", "target_cmd": "\"eval_episode_saving_path\": \"./videos\"}' python3 main.py"}
{"uuid": "da116cf8-3791-4b65-8ad0-81b47fce9b45", "execution_plan": "step1: Create and activate a conda environment for the project; step2: Install PyTorch with CUDA support; step3: Install required packages from requirements.txt; step4: Install the ChartMoE package in editable mode; step5: (Optional) Install Flash-Attn for acceleration; step6: Download and organize the ChartMoE-Data; step7: Download the ChartMoE model from Hugging Face; step8: Customize the weight path of ChartMoE; step9: Run a code demo to test the setup; step10: (Optional) Evaluate the model on ChartQA with or without PoT; step11: (Optional) Run the WebUI demo", "executed_cmds": "conda create -n chartmoe_env python=3.9;conda activate chartmoe_env;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "7d2898fd-684a-46f4-ad50-ec4e428e248a", "execution_plan": "step1: Create and activate a conda environment for the project; step2: Install PyTorch with CUDA support; step3: Install required packages from requirements.txt; step4: Install the ChartMoE package in editable mode; step5: (Optional) Install Flash-Attn for acceleration; step6: Download and organize the ChartMoE-Data; step7: Download the ChartMoE model from Hugging Face; step8: Customize the weight path of ChartMoE; step9: Run a code demo to test the setup; step10: (Optional) Evaluate the model on ChartQA with or without PoT; step11: (Optional) Run the WebUI demo", "executed_cmds": "conda create -n chartmoe_env python=3.9;conda activate chartmoe_env;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121;pip install -r requirements.txt", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "fcb9c5ef-10ed-4c78-8884-e6547e2cf70a", "execution_plan": "step1: Create and activate a conda environment for the project; step2: Install PyTorch with CUDA support; step3: Install required packages from requirements.txt; step4: Install the ChartMoE package in editable mode; step5: (Optional) Install Flash-Attn for acceleration; step6: Download and organize the ChartMoE-Data; step7: Download the ChartMoE model from Hugging Face; step8: Customize the weight path of ChartMoE; step9: Run a code demo to test the setup; step10: (Optional) Evaluate the model on ChartQA with or without PoT; step11: (Optional) Run the WebUI demo", "executed_cmds": "conda create -n chartmoe_env python=3.9;conda activate chartmoe_env;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121;pip install -r requirements.txt;pip install -e .;pip install flash-attn==2.7.0.post2", "cmd_prefix": "cd", "cmd_postfix": "chartmoe/train", "target_cmd": "cd chartmoe/train"}
{"uuid": "a714276c-f3df-4420-b69d-90e3fb6bfb17", "execution_plan": "step1: Create and activate a conda environment for the project; step2: Install PyTorch with CUDA support; step3: Install required packages from requirements.txt; step4: Install the ChartMoE package in editable mode; step5: (Optional) Install Flash-Attn for acceleration; step6: Download and organize the ChartMoE-Data; step7: Download the ChartMoE model from Hugging Face; step8: Customize the weight path of ChartMoE; step9: Run a code demo to test the setup; step10: (Optional) Evaluate the model on ChartQA with or without PoT; step11: (Optional) Run the WebUI demo", "executed_cmds": "conda create -n chartmoe_env python=3.9;conda activate chartmoe_env;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121;pip install -r requirements.txt;pip install -e .;pip install flash-attn==2.7.0.post2;cd chartmoe/train;python scripts/chartmoe_data_download.py", "cmd_prefix": "unzip", "cmd_postfix": "ChartMoE-Align.zip", "target_cmd": "unzip ChartMoE-Align.zip"}
{"uuid": "471db131-aac0-4b8d-bb91-c5350a1072d2", "execution_plan": "step1: Create and activate a conda environment for the project; step2: Install PyTorch with CUDA support; step3: Install required packages from requirements.txt; step4: Install the ChartMoE package in editable mode; step5: (Optional) Install Flash-Attn for acceleration; step6: Download and organize the ChartMoE-Data; step7: Download the ChartMoE model from Hugging Face; step8: Customize the weight path of ChartMoE; step9: Run a code demo to test the setup; step10: (Optional) Evaluate the model on ChartQA with or without PoT; step11: (Optional) Run the WebUI demo", "executed_cmds": "conda create -n chartmoe_env python=3.9;conda activate chartmoe_env;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121;pip install -r requirements.txt;pip install -e .;pip install flash-attn==2.7.0.post2;cd chartmoe/train;python scripts/chartmoe_data_download.py;unzip ChartMoE-Align.zip", "cmd_prefix": "unzip", "cmd_postfix": "SFT.zip", "target_cmd": "unzip SFT.zip"}
{"uuid": "91373a3e-3b7d-4c9f-a0be-948cdafff9f7", "execution_plan": "step1: Create and activate a conda environment for the project; step2: Install PyTorch with CUDA support; step3: Install required packages from requirements.txt; step4: Install the ChartMoE package in editable mode; step5: (Optional) Install Flash-Attn for acceleration; step6: Download and organize the ChartMoE-Data; step7: Download the ChartMoE model from Hugging Face; step8: Customize the weight path of ChartMoE; step9: Run a code demo to test the setup; step10: (Optional) Evaluate the model on ChartQA with or without PoT; step11: (Optional) Run the WebUI demo", "executed_cmds": "conda create -n chartmoe_env python=3.9;conda activate chartmoe_env;pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121;pip install -r requirements.txt;pip install -e .;pip install flash-attn==2.7.0.post2;cd chartmoe/train;python scripts/chartmoe_data_download.py;unzip ChartMoE-Align.zip;unzip SFT.zip", "cmd_prefix": "cd", "cmd_postfix": "chartmoe/train", "target_cmd": "cd chartmoe/train"}
{"uuid": "24ba6030-cb91-4935-a9d5-e818d71a0033", "execution_plan": "step1: Set up the required environment using conda and install dependencies; step2: Run the evaluation for editing GPT-J with ROME-LTI on the EVOKE dataset; step3: Compute the covariance matrix estimation locally (time-consuming but reusable); step4: Run the evaluation for the Subject Specificity task; step5: Summarize the results using the provided script", "executed_cmds": "conda create -n evoke python=3.9.7", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "9bccc07c-9b16-4872-b653-d5c63f67d1b9", "execution_plan": "step1: Set up a conda environment with Python 3.9 and activate it; step2: Install the required libraries including PyTorch, Torchvision, Torchaudio, and PyGraphviz; step3: Install the custom library from the local `lib` directory; step4: Verify the dataset structure and ensure it follows the required JSON format; step5: Navigate to the task-graph-learning directory to run experiments or models; step6: Choose and run the desired model (DO, TGT, baselines) or experiment (online mistake detection, video understanding)", "executed_cmds": "conda create -n tgml python=3.9", "cmd_prefix": "conda", "cmd_postfix": "activate tgml", "target_cmd": "conda activate tgml"}
{"uuid": "308d9c5c-44b3-4e38-b576-9b8ab4e55c84", "execution_plan": "step1: Set up a conda environment with Python 3.9 and activate it; step2: Install the required libraries including PyTorch, Torchvision, Torchaudio, and PyGraphviz; step3: Install the custom library from the local `lib` directory; step4: Verify the dataset structure and ensure it follows the required JSON format; step5: Navigate to the task-graph-learning directory to run experiments or models; step6: Choose and run the desired model (DO, TGT, baselines) or experiment (online mistake detection, video understanding)", "executed_cmds": "conda create -n tgml python=3.9;conda activate tgml;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia;conda install -c conda-forge pygraphviz", "cmd_prefix": "python -m pip", "cmd_postfix": "install -e ./lib", "target_cmd": "python -m pip install -e ./lib"}
{"uuid": "04d76816-f81e-407b-93c1-2f2344652f5d", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create a conda environment and install required packages; step3: Download and organize the checkpoints into the specified directory structure; step4: Prepare the inference data by setting up the example directory structure; step5: Edit the arguments in inference.py to specify checkpoint, dataset, and output paths; step6: Run the inference script to generate edited images; step7: View the inference results in the specified output directory", "executed_cmds": "git clone https://github.com/weichaozeng/TextCtrl.git", "cmd_prefix": "cd", "cmd_postfix": "TextCtrl/", "target_cmd": "cd TextCtrl/"}
{"uuid": "95441826-1c43-4334-84c2-e715a1cdd1a8", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create a conda environment and install required packages; step3: Download and organize the checkpoints into the specified directory structure; step4: Prepare the inference data by setting up the example directory structure; step5: Edit the arguments in inference.py to specify checkpoint, dataset, and output paths; step6: Run the inference script to generate edited images; step7: View the inference results in the specified output directory", "executed_cmds": "git clone https://github.com/weichaozeng/TextCtrl.git;cd TextCtrl/;conda create --name textctrl python=3.8", "cmd_prefix": "conda", "cmd_postfix": "activate textctrl", "target_cmd": "conda activate textctrl"}
{"uuid": "72e42e9d-0725-4895-ae88-f61ee0a3c11e", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create a conda environment and install required packages; step3: Download and organize the checkpoints into the specified directory structure; step4: Prepare the inference data by setting up the example directory structure; step5: Edit the arguments in inference.py to specify checkpoint, dataset, and output paths; step6: Run the inference script to generate edited images; step7: View the inference results in the specified output directory", "executed_cmds": "git clone https://github.com/weichaozeng/TextCtrl.git;cd TextCtrl/;conda create --name textctrl python=3.8;conda activate textctrl;pip install torch==1.13.0+cu116 torchvision==0.14.0+cu116 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu116", "cmd_prefix": "pip install", "cmd_postfix": "-r requirement.txt", "target_cmd": "pip install -r requirement.txt"}
{"uuid": "77d70800-545b-48b0-9407-5d7f0985627a", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Create a conda environment and install required packages; step3: Download and organize the checkpoints into the specified directory structure; step4: Prepare the inference data by setting up the example directory structure; step5: Edit the arguments in inference.py to specify checkpoint, dataset, and output paths; step6: Run the inference script to generate edited images; step7: View the inference results in the specified output directory", "executed_cmds": "git clone https://github.com/weichaozeng/TextCtrl.git;cd TextCtrl/;conda create --name textctrl python=3.8;conda activate textctrl;pip install torch==1.13.0+cu116 torchvision==0.14.0+cu116 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu116;pip install -r requirement.txt", "cmd_prefix": "PYTHONPATH=.../TextCtrl/", "cmd_postfix": "python inference.py", "target_cmd": "PYTHONPATH=.../TextCtrl/ python inference.py"}
{"uuid": "98fe816e-bbb6-434a-b978-596c0c222b0d", "execution_plan": "step1: Understand the requirements and parameters for running Hierarchical Refinement (HiRef), including the input point clouds and rank scheduler parameters; step2: Compute the optimal rank schedule using the rank annealing module to minimize calls to the low-rank optimal transport subroutine; step3: Initialize the Hierarchical Refinement class with the point clouds and computed rank schedule; step4: Run Hierarchical Refinement to obtain the bijective Monge map between the datasets; step5: Compute and print the Optimal Transport (OT) cost to evaluate the solution; step6: (Optional) Accelerate the process by adjusting solver parameters or using lightweight low-rank OT solvers; step7: (Optional) Refer to example notebooks for additional guidance or acceleration examples; step8: (Optional) Reproduce experiments or preprocess datasets using provided scripts if needed", "executed_cmds": "import rank_annealing; rank_schedule = rank_annealing.optimal_rank_schedule(n=n;hierarchy_depth=hierarchy_depth;max_Q=max_Q;max_rank=max_rank);import HR_OT; hrot = HR_OT.HierarchicalRefinementOT.init_from_point_clouds(X;Y;rank_schedule;base_rank=1;device=device);Gamma_hrot = hrot.run(return_as_coupling=False);cost_hrot = hrot.compute_OT_cost(); print(f\"Refinement Cost: {cost_hrot.item()}\");import HR_OT; import LR_mini; solver_params = {'max_iter': 45", "cmd_prefix": "'min_iter':", "cmd_postfix": "30", "target_cmd": "'min_iter': 30"}
{"uuid": "9aa448a4-2c12-4fde-9881-6e49a912b3dd", "execution_plan": "step1: Create a Python 3.10 environment using conda; step2: Install poetry 1.8.4; step3: Install the poetry environment in the main directory; step4: Install and build trec_eval for evaluation; step5: Download and format the corpus data; step6: Download the queries; step7: Create a lexical index for negative sampling or sparse fields; step8: (Optional) Pre-compute BM25 scores for training efficiency; step9: Train the model with specified parameters; step10: Reload the model for evaluation or analysis; step11: (Optional) Bring your own dataset by ensuring correct TREC format and updating schema and formatting code", "executed_cmds": "conda create -n mfar python=3.10;pip install poetry==1.8.4", "cmd_prefix": "poetry", "cmd_postfix": "lock --no-update", "target_cmd": "poetry lock --no-update"}
{"uuid": "25b828a3-ea3d-4acc-9312-c021fad925ff", "execution_plan": "step1: Create a Python 3.10 environment using conda; step2: Install poetry 1.8.4; step3: Install the poetry environment in the main directory; step4: Install and build trec_eval for evaluation; step5: Download and format the corpus data; step6: Download the queries; step7: Create a lexical index for negative sampling or sparse fields; step8: (Optional) Pre-compute BM25 scores for training efficiency; step9: Train the model with specified parameters; step10: Reload the model for evaluation or analysis; step11: (Optional) Bring your own dataset by ensuring correct TREC format and updating schema and formatting code", "executed_cmds": "conda create -n mfar python=3.10;pip install poetry==1.8.4;poetry lock --no-update", "cmd_prefix": "poetry", "cmd_postfix": "install", "target_cmd": "poetry install"}
{"uuid": "9031b62b-c0af-4723-a99a-e050f45de788", "execution_plan": "step1: Create a Python 3.10 environment using conda; step2: Install poetry 1.8.4; step3: Install the poetry environment in the main directory; step4: Install and build trec_eval for evaluation; step5: Download and format the corpus data; step6: Download the queries; step7: Create a lexical index for negative sampling or sparse fields; step8: (Optional) Pre-compute BM25 scores for training efficiency; step9: Train the model with specified parameters; step10: Reload the model for evaluation or analysis; step11: (Optional) Bring your own dataset by ensuring correct TREC format and updating schema and formatting code", "executed_cmds": "conda create -n mfar python=3.10;pip install poetry==1.8.4;poetry lock --no-update;poetry install", "cmd_prefix": "cd", "cmd_postfix": "trec_eval", "target_cmd": "cd trec_eval"}
{"uuid": "9e01b690-598c-499d-aef8-d773d85fbc95", "execution_plan": "step1: Create a Python 3.10 environment using conda; step2: Install poetry 1.8.4; step3: Install the poetry environment in the main directory; step4: Install and build trec_eval for evaluation; step5: Download and format the corpus data; step6: Download the queries; step7: Create a lexical index for negative sampling or sparse fields; step8: (Optional) Pre-compute BM25 scores for training efficiency; step9: Train the model with specified parameters; step10: Reload the model for evaluation or analysis; step11: (Optional) Bring your own dataset by ensuring correct TREC format and updating schema and formatting code", "executed_cmds": "conda create -n mfar python=3.10;pip install poetry==1.8.4;poetry lock --no-update;poetry install;cd trec_eval;make", "cmd_prefix": "make", "cmd_postfix": "install", "target_cmd": "make install"}
{"uuid": "3b960c95-f95d-4e87-b8a9-4bd1e6dd5213", "execution_plan": "step1: Clone the repository and switch to the reproduce branch to ensure reproducibility of paper results; step2: Install the required conda environment and dependencies; step3: Download and install ProteinMPNN for structure-conditioned amino acid distributions; step4: Download ESM-2 model weights and place them in the models directory; step5: Download the reference file and assay data from ProteinGym; step6: Download predicted PDB structures for the assays; step7: Download precomputed zero-shot scores or compute them from scratch; step8: Preprocess data by extracting ESM-2 embeddings, structure-conditioned amino acid distributions, and 3D coordinates; step9: Run Kermut on the benchmark or specific assays using Hydra configuration; step10: Postprocess results to compute Spearman correlation and MSE", "executed_cmds": "git clone -b reproduce git@github.com:petergroth/kermut.git;conda env create --file environment.yaml", "cmd_prefix": "conda", "cmd_postfix": "activate kermut_envs", "target_cmd": "conda activate kermut_envs"}
{"uuid": "cd380147-1f59-4759-a732-d1674d322e04", "execution_plan": "step1: Clone the repository and switch to the reproduce branch to ensure reproducibility of paper results; step2: Install the required conda environment and dependencies; step3: Download and install ProteinMPNN for structure-conditioned amino acid distributions; step4: Download ESM-2 model weights and place them in the models directory; step5: Download the reference file and assay data from ProteinGym; step6: Download predicted PDB structures for the assays; step7: Download precomputed zero-shot scores or compute them from scratch; step8: Preprocess data by extracting ESM-2 embeddings, structure-conditioned amino acid distributions, and 3D coordinates; step9: Run Kermut on the benchmark or specific assays using Hydra configuration; step10: Postprocess results to compute Spearman correlation and MSE", "executed_cmds": "git clone -b reproduce git@github.com:petergroth/kermut.git;conda env create --file environment.yaml;conda activate kermut_envs", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "7acd4c81-0600-4115-a182-615be02b305c", "execution_plan": "step1: Clone the repository and switch to the reproduce branch to ensure reproducibility of paper results; step2: Install the required conda environment and dependencies; step3: Download and install ProteinMPNN for structure-conditioned amino acid distributions; step4: Download ESM-2 model weights and place them in the models directory; step5: Download the reference file and assay data from ProteinGym; step6: Download predicted PDB structures for the assays; step7: Download precomputed zero-shot scores or compute them from scratch; step8: Preprocess data by extracting ESM-2 embeddings, structure-conditioned amino acid distributions, and 3D coordinates; step9: Run Kermut on the benchmark or specific assays using Hydra configuration; step10: Postprocess results to compute Spearman correlation and MSE", "executed_cmds": "git clone -b reproduce git@github.com:petergroth/kermut.git;conda env create --file environment.yaml;conda activate kermut_envs;pip install -e .;export PROTEINMPNN_DIR=<path-to-ProteinMPNN-installation>;curl -o models/esm2_t33_650M_UR50D.pt https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt;curl -o models/esm2_t33_650M_UR50D-contact-regression.pt https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt;curl -o data/DMS_substitutions.csv https://raw.githubusercontent.com/OATML-Markslab/ProteinGym/main/reference_files/DMS_substitutions.csv;curl -o cv_folds_singles_substitutions.zip https://marks.hms.harvard.edu/proteingym/cv_folds_singles_substitutions.zip", "cmd_prefix": "unzip cv_folds_singles_substitutions.zip", "cmd_postfix": "-d data", "target_cmd": "unzip cv_folds_singles_substitutions.zip -d data"}
{"uuid": "06d2e15d-d62f-4f30-aca8-3f9db3d3947c", "execution_plan": "step1: Clone the repository and switch to the reproduce branch to ensure reproducibility of paper results; step2: Install the required conda environment and dependencies; step3: Download and install ProteinMPNN for structure-conditioned amino acid distributions; step4: Download ESM-2 model weights and place them in the models directory; step5: Download the reference file and assay data from ProteinGym; step6: Download predicted PDB structures for the assays; step7: Download precomputed zero-shot scores or compute them from scratch; step8: Preprocess data by extracting ESM-2 embeddings, structure-conditioned amino acid distributions, and 3D coordinates; step9: Run Kermut on the benchmark or specific assays using Hydra configuration; step10: Postprocess results to compute Spearman correlation and MSE", "executed_cmds": "git clone -b reproduce git@github.com:petergroth/kermut.git;conda env create --file environment.yaml;conda activate kermut_envs;pip install -e .;export PROTEINMPNN_DIR=<path-to-ProteinMPNN-installation>;curl -o models/esm2_t33_650M_UR50D.pt https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt;curl -o models/esm2_t33_650M_UR50D-contact-regression.pt https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt;curl -o data/DMS_substitutions.csv https://raw.githubusercontent.com/OATML-Markslab/ProteinGym/main/reference_files/DMS_substitutions.csv;curl -o cv_folds_singles_substitutions.zip https://marks.hms.harvard.edu/proteingym/cv_folds_singles_substitutions.zip;unzip cv_folds_singles_substitutions.zip -d data;rm cv_folds_singles_substitutions.zip;curl -o ProteinGym_AF2_structures.zip https://marks.hms.harvard.edu/proteingym/ProteinGym_AF2_structures.zip;unzip ProteinGym_AF2_structures.zip -d data/structures/pdbs;rm ProteinGym_AF2_structures.zip;curl -o zero_shot_substitutions_scores.zip https://marks.hms.harvard.edu/proteingym/zero_shot_substitutions_scores.zip;unzip zero_shot_substitutions_scores.zip -d data/zero_shot_fitness_predictions;rm zero_shot_substitutions_scores.zip] or [python -m kermut.cmdline.preprocess_data.extract_esm2_zero_shots dataset=all;python -m kermut.cmdline.preprocess_data.extract_esm2_embeddings dataset=all;bash example_scripts/conditional_probabilities.sh;python -m kermut.cmdline.preprocess_data.extract_ProteinMPNN_probs dataset=all;python -m kermut.cmdline.preprocess_data.extract_3d_coords dataset=all;python proteingym_benchmark.py --multirun dataset=benchmark cv_scheme=fold_random_5;fold_modulo_5", "cmd_prefix": "fold_contiguous_5", "cmd_postfix": "kernel=kermut", "target_cmd": "fold_contiguous_5 kernel=kermut"}
{"uuid": "b97588d7-bb57-48fc-9e63-7f02f5c1681a", "execution_plan": "step1: Clone the STC-GS repository and navigate into the project directory; step2: Create and activate a conda environment with Python 3.10, then install required dependencies including PyTorch and other packages listed in requirements.txt; step3: Compile the CUDA kernel for the project; step4: Download and extract the 3D-NEXRAD dataset from the provided split tar files; step5: Preprocess and split the dataset to generate a JSON file; step6: Convert raw data into Gaussian representation using the provided script; step7: Prepare Gaussian sequences for training by sorting and processing the dataset; step8: Train the model using the prepared Gaussian sequences; step9: Evaluate the trained model using the test script", "executed_cmds": "git clone https://github.com/Ziyeeee/STC-GS.git --recursive", "cmd_prefix": "cd", "cmd_postfix": "STC-GS", "target_cmd": "cd STC-GS"}
{"uuid": "cfd84d41-e663-48c3-8d04-8cc8967b8196", "execution_plan": "step1: Clone the STC-GS repository and navigate into the project directory; step2: Create and activate a conda environment with Python 3.10, then install required dependencies including PyTorch and other packages listed in requirements.txt; step3: Compile the CUDA kernel for the project; step4: Download and extract the 3D-NEXRAD dataset from the provided split tar files; step5: Preprocess and split the dataset to generate a JSON file; step6: Convert raw data into Gaussian representation using the provided script; step7: Prepare Gaussian sequences for training by sorting and processing the dataset; step8: Train the model using the prepared Gaussian sequences; step9: Evaluate the trained model using the test script", "executed_cmds": "git clone https://github.com/Ziyeeee/STC-GS.git --recursive;cd STC-GS", "cmd_prefix": "conda create", "cmd_postfix": "-n stcgs python=3.10", "target_cmd": "conda create -n stcgs python=3.10"}
{"uuid": "ab687021-c4cb-43c2-b402-a483e78a0530", "execution_plan": "step1: Clone the STC-GS repository and navigate into the project directory; step2: Create and activate a conda environment with Python 3.10, then install required dependencies including PyTorch and other packages listed in requirements.txt; step3: Compile the CUDA kernel for the project; step4: Download and extract the 3D-NEXRAD dataset from the provided split tar files; step5: Preprocess and split the dataset to generate a JSON file; step6: Convert raw data into Gaussian representation using the provided script; step7: Prepare Gaussian sequences for training by sorting and processing the dataset; step8: Train the model using the prepared Gaussian sequences; step9: Evaluate the trained model using the test script", "executed_cmds": "git clone https://github.com/Ziyeeee/STC-GS.git --recursive;cd STC-GS;conda create -n stcgs python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate stcgs", "target_cmd": "conda activate stcgs"}
{"uuid": "20c04241-2463-485e-ad1d-737028903c14", "execution_plan": "step1: Clone the STC-GS repository and navigate into the project directory; step2: Create and activate a conda environment with Python 3.10, then install required dependencies including PyTorch and other packages listed in requirements.txt; step3: Compile the CUDA kernel for the project; step4: Download and extract the 3D-NEXRAD dataset from the provided split tar files; step5: Preprocess and split the dataset to generate a JSON file; step6: Convert raw data into Gaussian representation using the provided script; step7: Prepare Gaussian sequences for training by sorting and processing the dataset; step8: Train the model using the prepared Gaussian sequences; step9: Evaluate the trained model using the test script", "executed_cmds": "git clone https://github.com/Ziyeeee/STC-GS.git --recursive;cd STC-GS;conda create -n stcgs python=3.10;conda activate stcgs;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "b1760eb1-24af-405c-98c2-5eef73fa395a", "execution_plan": "step1: Set up the environment by ensuring Python 3.11, PyTorch  2.0 with CUDA support, and a compatible NVIDIA GPU are available; step2: Navigate to the `bronet` directory to run the main experiments; step3: Execute the provided script to reproduce the main results from the paper; step4: Download pre-trained models from the provided HuggingFace links if testing is required; step5: Test the downloaded models by running the test script with the appropriate configuration and checkpoint paths", "executed_cmds": "cd bronet", "cmd_prefix": "bash", "cmd_postfix": "run.sh", "target_cmd": "bash run.sh"}
{"uuid": "966d9270-288c-44d5-83d8-da3dd3cd748b", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a conda virtual environment; step3: Install the required dependencies; step4: Prepare the datasets by organizing them in the specified directory structure; step5: Run the quick start scripts for concept extraction, embedding extraction, and interpretable prediction training; step6: Alternatively, run the process in stages: concept acquisition, concept vector extraction, and interpretable predictions based on concepts; step7: Compute the Inherent Interpretability Score (IIS) to evaluate the interpretability of pre-trained representations", "executed_cmds": "git clone https://github.com/ssfgunner/IIS.git", "cmd_prefix": "cd", "cmd_postfix": "IIS", "target_cmd": "cd IIS"}
{"uuid": "3f322e77-f5e2-4374-87d0-5fa94500734c", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a conda virtual environment; step3: Install the required dependencies; step4: Prepare the datasets by organizing them in the specified directory structure; step5: Run the quick start scripts for concept extraction, embedding extraction, and interpretable prediction training; step6: Alternatively, run the process in stages: concept acquisition, concept vector extraction, and interpretable predictions based on concepts; step7: Compute the Inherent Interpretability Score (IIS) to evaluate the interpretability of pre-trained representations", "executed_cmds": "git clone https://github.com/ssfgunner/IIS.git;cd IIS", "cmd_prefix": "conda create -n", "cmd_postfix": "iis python=3.8 -y", "target_cmd": "conda create -n iis python=3.8 -y"}
{"uuid": "e279664e-b12f-4cb7-a30b-63f96e9af467", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a conda virtual environment; step3: Install the required dependencies; step4: Prepare the datasets by organizing them in the specified directory structure; step5: Run the quick start scripts for concept extraction, embedding extraction, and interpretable prediction training; step6: Alternatively, run the process in stages: concept acquisition, concept vector extraction, and interpretable predictions based on concepts; step7: Compute the Inherent Interpretability Score (IIS) to evaluate the interpretability of pre-trained representations", "executed_cmds": "git clone https://github.com/ssfgunner/IIS.git;cd IIS;conda create -n iis python=3.8 -y", "cmd_prefix": "conda", "cmd_postfix": "activate iis", "target_cmd": "conda activate iis"}
{"uuid": "81533053-dc07-4654-bf04-a1acb0454eba", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a conda virtual environment; step3: Install the required dependencies; step4: Prepare the datasets by organizing them in the specified directory structure; step5: Run the quick start scripts for concept extraction, embedding extraction, and interpretable prediction training; step6: Alternatively, run the process in stages: concept acquisition, concept vector extraction, and interpretable predictions based on concepts; step7: Compute the Inherent Interpretability Score (IIS) to evaluate the interpretability of pre-trained representations", "executed_cmds": "git clone https://github.com/ssfgunner/IIS.git;cd IIS;conda create -n iis python=3.8 -y;conda activate iis", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "ef013332-617a-4e6b-83e7-d9bb9bedc645", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a conda virtual environment; step3: Install the required dependencies; step4: Prepare the datasets by organizing them in the specified directory structure; step5: Run the quick start scripts for concept extraction, embedding extraction, and interpretable prediction training; step6: Alternatively, run the process in stages: concept acquisition, concept vector extraction, and interpretable predictions based on concepts; step7: Compute the Inherent Interpretability Score (IIS) to evaluate the interpretability of pre-trained representations", "executed_cmds": "git clone https://github.com/ssfgunner/IIS.git;cd IIS;conda create -n iis python=3.8 -y;conda activate iis;pip install -r requirements.txt;bash ./scripts/concept_extract.sh;bash ./scripts/emb_extractor.sh;bash ./scripts/ip_train.sh", "cmd_prefix": "cd", "cmd_postfix": "./concept_extractor", "target_cmd": "cd ./concept_extractor"}
{"uuid": "800172fd-3423-4154-9240-a7985346552e", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a conda virtual environment; step3: Install the required dependencies; step4: Prepare the datasets by organizing them in the specified directory structure; step5: Run the quick start scripts for concept extraction, embedding extraction, and interpretable prediction training; step6: Alternatively, run the process in stages: concept acquisition, concept vector extraction, and interpretable predictions based on concepts; step7: Compute the Inherent Interpretability Score (IIS) to evaluate the interpretability of pre-trained representations", "executed_cmds": "git clone https://github.com/ssfgunner/IIS.git;cd IIS;conda create -n iis python=3.8 -y;conda activate iis;pip install -r requirements.txt;bash ./scripts/concept_extract.sh;bash ./scripts/emb_extractor.sh;bash ./scripts/ip_train.sh;cd ./concept_extractor;python segment_extractor.py --data_root ${path_of_concept_dataset} --save_root ../concept_library/prototype/segments;python patch_extractor.py --data_root ${path_of_concept_dataset} --save_root ../concept_library/prototype/patches;python visual_element_emb_extractor.py --config_yaml ${config_yaml} --model ${model} --target_layer_name ${target_layer_name} --weights ${weights} --data-path ${prototype_concept_path};python textual_emb_extractor.py --dataset ${target_dataset} --concept_set ${text_concept_path} --backbone ${model} --activation_dir ./embs/${model}/ --feature_layer ${target_layer_name}", "cmd_prefix": "cd", "cmd_postfix": "../IP_training", "target_cmd": "cd ../IP_training"}
{"uuid": "80a4a2a4-faff-4f2d-aa9c-f20748b3c8fb", "execution_plan": "step1: Clone the repository and navigate to the project directory; step2: Create and activate a conda virtual environment; step3: Install the required dependencies; step4: Prepare the datasets by organizing them in the specified directory structure; step5: Run the quick start scripts for concept extraction, embedding extraction, and interpretable prediction training; step6: Alternatively, run the process in stages: concept acquisition, concept vector extraction, and interpretable predictions based on concepts; step7: Compute the Inherent Interpretability Score (IIS) to evaluate the interpretability of pre-trained representations", "executed_cmds": "git clone https://github.com/ssfgunner/IIS.git;cd IIS;conda create -n iis python=3.8 -y;conda activate iis;pip install -r requirements.txt;bash ./scripts/concept_extract.sh;bash ./scripts/emb_extractor.sh;bash ./scripts/ip_train.sh;cd ./concept_extractor;python segment_extractor.py --data_root ${path_of_concept_dataset} --save_root ../concept_library/prototype/segments;python patch_extractor.py --data_root ${path_of_concept_dataset} --save_root ../concept_library/prototype/patches;python visual_element_emb_extractor.py --config_yaml ${config_yaml} --model ${model} --target_layer_name ${target_layer_name} --weights ${weights} --data-path ${prototype_concept_path};python textual_emb_extractor.py --dataset ${target_dataset} --concept_set ${text_concept_path} --backbone ${model} --activation_dir ./embs/${model}/ --feature_layer ${target_layer_name};cd ../IP_training;python IP_Prototype_training.py --model_name ${model_name} --in_channels 2048 --concept_path ../concept_extractor/embs/${model_name}/visual_element_emb.pkl --num_concepts 200 --num_classes 1000 --n_epoch 30 --train_pt ../concept_extractor/embs/${model_name}/${target_dataset}_train_${model_name}_${target_layer_name}.pt --val_pt ../concept_extractor/embs/${model_name}/${target_dataset}_val_${model_name}_${target_layer_name}.pt --save_dir ./IP_Prototype/${model_name};python IP_Text_training.py --dataset ${target_dataset} --backbone ${model_name} --concept_set '../concept_library/textual/${target_dataset}_filtered.txt' --feature_layer ${target_layer_name} --sparsity_ratio --save_dir ./output --activation_dir '../concept_extractor/embs/${model_name}'", "cmd_prefix": "python", "cmd_postfix": "iis_computing.py", "target_cmd": "python iis_computing.py"}
{"uuid": "2860c88c-4e6a-4966-a22e-8621490d2a7e", "execution_plan": "step1: Enable Developer Options and USB debugging on the Android device; step2: Install the ADB tool on the computer; step3: Install the pure-python-adb library using pip; step4: (Optional) Install scrcpy for device display and control; step5: Connect the Android device to the computer via USB and start the ADB server; step6: Verify the device connection using a Python script; step7: Understand the screen coordinate system and available ADB commands; step8: Create a Python script to simulate a selfie timer; step9: Create a Python script to search for a word definition and take a screenshot", "executed_cmds": "Install ADB via Android Studio or official docs;pip install pure-python-adb", "cmd_prefix": "Download scrcpy from GitHub", "cmd_postfix": "and add to PATH", "target_cmd": "Download scrcpy from GitHub and add to PATH"}
{"uuid": "29492d1c-a8c9-40e3-bb13-9d330eb4e1cd", "execution_plan": "step1: Enable Developer Options and USB debugging on the Android device; step2: Install the ADB tool on the computer; step3: Install the pure-python-adb library using pip; step4: (Optional) Install scrcpy for device display and control; step5: Connect the Android device to the computer via USB and start the ADB server; step6: Verify the device connection using a Python script; step7: Understand the screen coordinate system and available ADB commands; step8: Create a Python script to simulate a selfie timer; step9: Create a Python script to search for a word definition and take a screenshot", "executed_cmds": "Install ADB via Android Studio or official docs;pip install pure-python-adb;Download scrcpy from GitHub and add to PATH", "cmd_prefix": "adb", "cmd_postfix": "start-server", "target_cmd": "adb start-server"}
{"uuid": "64b6cbb0-be7b-4e33-9546-c2ac5acd29c3", "execution_plan": "step1: Enable Developer Options and USB debugging on the Android device; step2: Install the ADB tool on the computer; step3: Install the pure-python-adb library using pip; step4: (Optional) Install scrcpy for device display and control; step5: Connect the Android device to the computer via USB and start the ADB server; step6: Verify the device connection using a Python script; step7: Understand the screen coordinate system and available ADB commands; step8: Create a Python script to simulate a selfie timer; step9: Create a Python script to search for a word definition and take a screenshot", "executed_cmds": "Install ADB via Android Studio or official docs;pip install pure-python-adb;Download scrcpy from GitHub and add to PATH;adb start-server;Python script to list connected devices", "cmd_prefix": "Python script", "cmd_postfix": "to open camera", "target_cmd": "Python script to open camera"}
{"uuid": "0fcf7c5e-f1f2-4486-a0e6-de3413cba28d", "execution_plan": "step1: Enable Developer Options and USB debugging on the Android device; step2: Install the ADB tool on the computer; step3: Install the pure-python-adb library using pip; step4: (Optional) Install scrcpy for device display and control; step5: Connect the Android device to the computer via USB and start the ADB server; step6: Verify the device connection using a Python script; step7: Understand the screen coordinate system and available ADB commands; step8: Create a Python script to simulate a selfie timer; step9: Create a Python script to search for a word definition and take a screenshot", "executed_cmds": "Install ADB via Android Studio or official docs;pip install pure-python-adb;Download scrcpy from GitHub and add to PATH;adb start-server;Python script to list connected devices;Python script to open camera;wait", "cmd_prefix": "and take", "cmd_postfix": "a photo", "target_cmd": "and take a photo"}
{"uuid": "769cd96a-7364-4ab3-850b-29d099004a6c", "execution_plan": "step1: Enable Developer Options and USB debugging on the Android device; step2: Install the ADB tool on the computer; step3: Install the pure-python-adb library using pip; step4: (Optional) Install scrcpy for device display and control; step5: Connect the Android device to the computer via USB and start the ADB server; step6: Verify the device connection using a Python script; step7: Understand the screen coordinate system and available ADB commands; step8: Create a Python script to simulate a selfie timer; step9: Create a Python script to search for a word definition and take a screenshot", "executed_cmds": "Install ADB via Android Studio or official docs;pip install pure-python-adb;Download scrcpy from GitHub and add to PATH;adb start-server;Python script to list connected devices;Python script to open camera;wait;and take a photo", "cmd_prefix": "Python script", "cmd_postfix": "to open browser", "target_cmd": "Python script to open browser"}
{"uuid": "f33798f4-f8b6-46b4-af36-4552d79a5dac", "execution_plan": "step1: Enable Developer Options and USB debugging on the Android device; step2: Install the ADB tool on the computer; step3: Install the pure-python-adb library using pip; step4: (Optional) Install scrcpy for device display and control; step5: Connect the Android device to the computer via USB and start the ADB server; step6: Verify the device connection using a Python script; step7: Understand the screen coordinate system and available ADB commands; step8: Create a Python script to simulate a selfie timer; step9: Create a Python script to search for a word definition and take a screenshot", "executed_cmds": "Install ADB via Android Studio or official docs;pip install pure-python-adb;Download scrcpy from GitHub and add to PATH;adb start-server;Python script to list connected devices;Python script to open camera;wait;and take a photo;Python script to open browser;search", "cmd_prefix": "and", "cmd_postfix": "screenshot", "target_cmd": "and screenshot"}
{"uuid": "7d7720ef-d289-4b8c-9528-4a09f304ff6e", "execution_plan": "step1: Clone the repository to get the source code; step2: Set up the environment using the provided build script; step3: Prepare input files for docking predictions, including a CSV file and ESM embeddings; step4: Download the pre-trained models for docking, filtering, and relaxation; step5: Run the model for docking predictions using the provided script; step6: Preprocess datasets if needed, including extracting ESM embeddings and processing data; step7: Train the docking model using the provided config file; step8: Train the filtering and relaxation models using the provided config file", "executed_cmds": "git clone https://github.com/vsomnath/flexdock.git", "cmd_prefix": "bash", "cmd_postfix": "build_env.sh", "target_cmd": "bash build_env.sh"}
{"uuid": "2a66cc3f-306b-4ea8-9e6b-cc3ade03a2bb", "execution_plan": "step1: Download and prepare the datasets by placing them in the specified folder structure; step2: Download and prepare the model weights by placing them in the `model_zoo` folder; step3: Install the necessary requirements and dependencies; step4: Train the model on the HC-STVG dataset; step5: Train the model on the HC-STVG2 dataset; step6: Train the model on the VidSTG dataset; step7: Evaluate the model on the HC-STVG dataset; step8: Evaluate the model on the HC-STVG2 dataset; step9: Evaluate the model on the VidSTG dataset", "executed_cmds": "\"pip3 install -r requirements.txt\"", "cmd_prefix": "\"apt install", "cmd_postfix": "ffmpeg -y\"", "target_cmd": "\"apt install ffmpeg -y\""}
{"uuid": "73b71c85-c033-4433-8554-ae9a77f82884", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Install the required dependencies using pip; step3: Download the model weights (fap.bin) from HuggingFace and organize them into the pretrained_weight directory; step4: Prepare the weights for CLIP and Realistic_Vision_V4 and organize them into the pretrained_weight directory; step5: Run the inference script to perform model inference; step6: Run the training script to train the model", "executed_cmds": "git clone https://github.com/Thomas-wyh/FuseAnyPart.git", "cmd_prefix": "cd", "cmd_postfix": "FuseAnyPart", "target_cmd": "cd FuseAnyPart"}
{"uuid": "493e52e9-bdc5-4ed1-aac2-630bc6c20aa6", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Install the required dependencies using pip; step3: Download the model weights (fap.bin) from HuggingFace and organize them into the pretrained_weight directory; step4: Prepare the weights for CLIP and Realistic_Vision_V4 and organize them into the pretrained_weight directory; step5: Run the inference script to perform model inference; step6: Run the training script to train the model", "executed_cmds": "git clone https://github.com/Thomas-wyh/FuseAnyPart.git;cd FuseAnyPart", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "e5ee970d-5e9f-441a-befa-f078f604fb79", "execution_plan": "step1: Clone the repository and navigate into the project directory; step2: Install the required dependencies using pip; step3: Download the model weights (fap.bin) from HuggingFace and organize them into the pretrained_weight directory; step4: Prepare the weights for CLIP and Realistic_Vision_V4 and organize them into the pretrained_weight directory; step5: Run the inference script to perform model inference; step6: Run the training script to train the model", "executed_cmds": "git clone https://github.com/Thomas-wyh/FuseAnyPart.git;cd FuseAnyPart;pip install -r requirements.txt", "cmd_prefix": "bash", "cmd_postfix": "train.sh", "target_cmd": "bash train.sh"}
{"uuid": "ae2d2070-1d0f-4da6-a194-d0658d901c7e", "execution_plan": "step1: Download the pretrained model weights and tokenizers for either VAR or LlamaGen from their respective repositories; step2: Download the ImageNet dataset and prepare it for training; step3: For LlamaGen, generate image latents from the ImageNet dataset and store them in a local directory; step4: Train the VAR or LlamaGen model using the provided finetuning scripts; step5: Generate 50K image samples for evaluation and store them in an npz file; step6: Evaluate the generated samples using the provided evaluation scripts", "executed_cmds": "Refer to ./LlamaGen/GETTING_STARTED.md for details;torchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" VAR_finetune.py --depth=16 --bs=256 --ep=1 --tblr=2e-5 --fp16=1 --alng=1e-3 --wpe=0.1 --loss_type=\"CCA\" --beta=0.02 --lambda_=50.0 --ac=4 --exp_name=\"default\" --dpr_ratio=0.0 --uncond_ratio=0.1 --ref_ckpt=\"/path/to/var/var_d16.pth\" --data_path=\"/path/to/imagenet\";torchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" LlamaGen_finetune.py --global-batch-size 256 --gradient-accumulation-step 16 --epochs=1 --ckpt-every=5000 --lr=1e-5 --loss_type=\"CCA\" --expid \"default\" --lambda_=1000.0/300.0/1000.0 --beta=0.02 --uncond_ratio=0.1 --keep_dropout --ref_ckpt=\"/path/to/LlamaGen/c2i_B_384.pt/c2i_L_384.pt/c2i_XL_384.pt\" --code-path=\"/path/to/imagenet384_train_code_c2i_flip_ten_crop/\" --image-size=384 --gpt-model=\"GPT-B/GPT-L/GPT-XL\";python VAR_sample.py --cfg=0.0 --ckpt_path=\"/path/to/var/var_d20.pth\" --vae_ckpt=\"./vae_ch160v4096z32.pth\" --depth 20;torchrun --nnodes=1 --nproc_per_node=8 --node_rank=0 --master_port=12445 LLamaGen_sample_ddp.py --vq-ckpt=\"path/to/LlamaGen/vq_ds16_c2i.pt\" --ckpt_path=\"/path/to/LlamaGen/c2i_XL_384.pt\" --gpt-model=\"GPT-XL\" --image-size=384 --image-size-eval=256 --per-proc-batch-size=48 --cfg-scale=1.0 --num-fid-samples=50000", "cmd_prefix": "Refer to ./LlamaGen/evaluations/c2i", "cmd_postfix": "for evaluation code", "target_cmd": "Refer to ./LlamaGen/evaluations/c2i for evaluation code"}
{"uuid": "f6094b15-987e-4fdb-8caa-04aa85aaa968", "execution_plan": "step1: Clone the repository and navigate into it; step2: Install the required packages (SPLIT and RESPLIT) using pip; step3: Prepare the dataset by loading it into a pandas DataFrame; step4: Run SPLIT by importing the SPLIT class, configuring the model, and fitting it to the data; step5: Run LicketySPLIT by importing the LicketySPLIT class, configuring the model, and fitting it to the data; step6: Run RESPLIT by importing the RESPLIT class, configuring the model, and fitting it to the data; step7: (Optional) Run the RESPLIT example script to compare runtime between TreeFARMS and RESPLIT", "executed_cmds": "\"pip install resplit/ split/\";\"from split import SPLIT\\nimport pandas as pd\\nlookahead_depth = 2\\ndepth_buget = 5\\ndataset = pd.read_csv('path/to/compas.csv')\\nX", "cmd_prefix": "y", "cmd_postfix": "= dataset.iloc[:", "target_cmd": "y = dataset.iloc[:"}
{"uuid": "47143b26-43b2-456c-83b7-09945cd8d5f4", "execution_plan": "step1: Clone the repository and navigate into it; step2: Install the required packages (SPLIT and RESPLIT) using pip; step3: Prepare the dataset by loading it into a pandas DataFrame; step4: Run SPLIT by importing the SPLIT class, configuring the model, and fitting it to the data; step5: Run LicketySPLIT by importing the LicketySPLIT class, configuring the model, and fitting it to the data; step6: Run RESPLIT by importing the RESPLIT class, configuring the model, and fitting it to the data; step7: (Optional) Run the RESPLIT example script to compare runtime between TreeFARMS and RESPLIT", "executed_cmds": "\"pip install resplit/ split/\";\"from split import SPLIT\\nimport pandas as pd\\nlookahead_depth = 2\\ndepth_buget = 5\\ndataset = pd.read_csv('path/to/compas.csv')\\nX;y = dataset.iloc[:;:-1];dataset.iloc[:;-1]\\nregularization = 0.01\\nmodel = SPLIT(lookahead_depth_budget=lookahead_depth;reg=regularization;full_depth_budget=depth_buget;verbose=False;binarize=False;time_limit=100)\\nmodel.fit(X;y)\\ny_pred = model.predict(X)\\ntree = model.tree\\nprint(tree)\";\"from split import LicketySPLIT\\nmodel = LicketySPLIT(full_depth_budget=full_depth_budget", "cmd_prefix": "reg=regularization)\\n.... #", "cmd_postfix": "same as above\\n...\"", "target_cmd": "reg=regularization)\\n.... # same as above\\n...\""}
{"uuid": "244470e8-7adb-441e-acd4-5ed9660542c2", "execution_plan": "step1: Clone the repository and navigate into it; step2: Install the required packages (SPLIT and RESPLIT) using pip; step3: Prepare the dataset by loading it into a pandas DataFrame; step4: Run SPLIT by importing the SPLIT class, configuring the model, and fitting it to the data; step5: Run LicketySPLIT by importing the LicketySPLIT class, configuring the model, and fitting it to the data; step6: Run RESPLIT by importing the RESPLIT class, configuring the model, and fitting it to the data; step7: (Optional) Run the RESPLIT example script to compare runtime between TreeFARMS and RESPLIT", "executed_cmds": "\"pip install resplit/ split/\";\"from split import SPLIT\\nimport pandas as pd\\nlookahead_depth = 2\\ndepth_buget = 5\\ndataset = pd.read_csv('path/to/compas.csv')\\nX;y = dataset.iloc[:;:-1];dataset.iloc[:;-1]\\nregularization = 0.01\\nmodel = SPLIT(lookahead_depth_budget=lookahead_depth;reg=regularization;full_depth_budget=depth_buget;verbose=False;binarize=False;time_limit=100)\\nmodel.fit(X;y)\\ny_pred = model.predict(X)\\ntree = model.tree\\nprint(tree)\";\"from split import LicketySPLIT\\nmodel = LicketySPLIT(full_depth_budget=full_depth_budget;reg=regularization)\\n.... # same as above\\n...\";\"from resplit import RESPLIT\\nimport pandas as pd\\ndataset = pd.read_csv('path/to/compas.csv')\\nX", "cmd_prefix": "y", "cmd_postfix": "= dataset.iloc[:", "target_cmd": "y = dataset.iloc[:"}
{"uuid": "c1aa02f9-4d8c-4aa1-936e-6c63e51cd7be", "execution_plan": "step1: Clone the repository and navigate into it; step2: Install the required packages (SPLIT and RESPLIT) using pip; step3: Prepare the dataset by loading it into a pandas DataFrame; step4: Run SPLIT by importing the SPLIT class, configuring the model, and fitting it to the data; step5: Run LicketySPLIT by importing the LicketySPLIT class, configuring the model, and fitting it to the data; step6: Run RESPLIT by importing the RESPLIT class, configuring the model, and fitting it to the data; step7: (Optional) Run the RESPLIT example script to compare runtime between TreeFARMS and RESPLIT", "executed_cmds": "\"pip install resplit/ split/\";\"from split import SPLIT\\nimport pandas as pd\\nlookahead_depth = 2\\ndepth_buget = 5\\ndataset = pd.read_csv('path/to/compas.csv')\\nX;y = dataset.iloc[:;:-1];dataset.iloc[:;-1]\\nregularization = 0.01\\nmodel = SPLIT(lookahead_depth_budget=lookahead_depth;reg=regularization;full_depth_budget=depth_buget;verbose=False;binarize=False;time_limit=100)\\nmodel.fit(X;y)\\ny_pred = model.predict(X)\\ntree = model.tree\\nprint(tree)\";\"from split import LicketySPLIT\\nmodel = LicketySPLIT(full_depth_budget=full_depth_budget;reg=regularization)\\n.... # same as above\\n...\";\"from resplit import RESPLIT\\nimport pandas as pd\\ndataset = pd.read_csv('path/to/compas.csv')\\nX;y = dataset.iloc[:;:-1];dataset.iloc[:;-1]\\nconfig = {\\n    \\\"regularization\\\": 0.005;\\n    \\\"rashomon_bound_multiplier\\\": 0.01", "cmd_prefix": "\\n  ", "cmd_postfix": " \\\"depth_budget\\\": 5", "target_cmd": "\\n    \\\"depth_budget\\\": 5"}
{"uuid": "19902148-3fc1-4525-89ad-29da02560996", "execution_plan": "step1: Understand the project and its purpose, which is tuning-free and high-performance model merging for ViT models, language models (RoBERTa, GPT-2), and BEiT-3 models; step2: Navigate to the specific model merging directory (ViT, language models, or BEiT-3) based on the model you want to merge; step3: Follow the instructions or code provided in the respective model merging directory to perform the merging; step4: Cite the paper if the project is helpful for your work; step5: Acknowledge the referenced implementations if used in your work", "executed_cmds": "cd merge_vit", "cmd_prefix": "cd", "cmd_postfix": "merge_lm", "target_cmd": "cd merge_lm"}
{"uuid": "4280fbca-a2c1-457c-98e6-b25db57c1085", "execution_plan": "step1: Understand the project and its purpose, which is tuning-free and high-performance model merging for ViT models, language models (RoBERTa, GPT-2), and BEiT-3 models; step2: Navigate to the specific model merging directory (ViT, language models, or BEiT-3) based on the model you want to merge; step3: Follow the instructions or code provided in the respective model merging directory to perform the merging; step4: Cite the paper if the project is helpful for your work; step5: Acknowledge the referenced implementations if used in your work", "executed_cmds": "cd merge_vit;cd merge_lm", "cmd_prefix": "or", "cmd_postfix": "cd merge_beit3", "target_cmd": "or cd merge_beit3"}
{"uuid": "331cb637-ebca-4a14-9260-164ec9c30e09", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Download and prepare the dataset (ImageNet) and specify the data directory; step3: Train the model on ImageNet 256x256 resolution with specified parameters; step4: (Optional) Train the model on ImageNet 512x512 resolution with adjusted parameters; step5: (Optional) Train the model for text-to-image generation on MS-COCO dataset; step6: Evaluate the trained model by generating images and preparing for FID evaluation; step7: (Optional) Download additional pretrained visual encoder weights if needed", "executed_cmds": "conda create -n repa python=3.9 -y", "cmd_prefix": "conda", "cmd_postfix": "activate repa", "target_cmd": "conda activate repa"}
{"uuid": "fd8a700f-e546-4b7c-a9a1-646f07cb77ab", "execution_plan": "step1: Set up the Python environment using Conda and install required dependencies; step2: Download and prepare the dataset (ImageNet) and specify the data directory; step3: Train the model on ImageNet 256x256 resolution with specified parameters; step4: (Optional) Train the model on ImageNet 512x512 resolution with adjusted parameters; step5: (Optional) Train the model for text-to-image generation on MS-COCO dataset; step6: Evaluate the trained model by generating images and preparing for FID evaluation; step7: (Optional) Download additional pretrained visual encoder weights if needed", "executed_cmds": "conda create -n repa python=3.9 -y;conda activate repa", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "5b852017-f7f3-4f00-9c0b-1e8056e422e6", "execution_plan": "step1: Set up the Python environment using Conda to ensure all dependencies are installed; step2: Install PyTorch and PyTorch Geometric with CUDA support for GPU acceleration; step3: Install additional required libraries such as RDKit, pandas, and other dependencies; step4: Install optional libraries like pytorch-lightning, yacs, torchmetrics, and wandb for extended functionality; step5: Clean the Conda environment to remove unnecessary files; step6: Import the desired layer (OrthogonalGCNConvLayer or UnitaryGCNConvLayer) from the respective module; step7: Initialize the layer with the required parameters (input_dim, output_dim, etc.); step8: Prepare the data object using PyTorch Geometric's Data class; step9: Apply the layer to the data object to perform the graph message passing operation", "executed_cmds": "conda create -n graphgps python=3.10;conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia;pip install torch_geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu117.html;conda install openbabel fsspec rdkit -c conda-forge", "cmd_prefix": "conda", "cmd_postfix": "install pandas", "target_cmd": "conda install pandas"}
{"uuid": "36784623-eb9c-4a28-bced-e2efec472e9b", "execution_plan": "step1: Set up the Python environment using Conda to ensure all dependencies are installed; step2: Install PyTorch and PyTorch Geometric with CUDA support for GPU acceleration; step3: Install additional required libraries such as RDKit, pandas, and other dependencies; step4: Install optional libraries like pytorch-lightning, yacs, torchmetrics, and wandb for extended functionality; step5: Clean the Conda environment to remove unnecessary files; step6: Import the desired layer (OrthogonalGCNConvLayer or UnitaryGCNConvLayer) from the respective module; step7: Initialize the layer with the required parameters (input_dim, output_dim, etc.); step8: Prepare the data object using PyTorch Geometric's Data class; step9: Apply the layer to the data object to perform the graph message passing operation", "executed_cmds": "conda create -n graphgps python=3.10;conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia;pip install torch_geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu117.html;conda install openbabel fsspec rdkit -c conda-forge;conda install pandas;pip install pytorch-lightning yacs torchmetrics;pip install performer-pytorch", "cmd_prefix": "pip", "cmd_postfix": "install tensorboardX", "target_cmd": "pip install tensorboardX"}
{"uuid": "d1986e5e-cc62-4996-a2d4-13b5cfa37bbc", "execution_plan": "step1: Set up the Python environment using Conda to ensure all dependencies are installed; step2: Install PyTorch and PyTorch Geometric with CUDA support for GPU acceleration; step3: Install additional required libraries such as RDKit, pandas, and other dependencies; step4: Install optional libraries like pytorch-lightning, yacs, torchmetrics, and wandb for extended functionality; step5: Clean the Conda environment to remove unnecessary files; step6: Import the desired layer (OrthogonalGCNConvLayer or UnitaryGCNConvLayer) from the respective module; step7: Initialize the layer with the required parameters (input_dim, output_dim, etc.); step8: Prepare the data object using PyTorch Geometric's Data class; step9: Apply the layer to the data object to perform the graph message passing operation", "executed_cmds": "conda create -n graphgps python=3.10;conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia;pip install torch_geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu117.html;conda install openbabel fsspec rdkit -c conda-forge;conda install pandas;pip install pytorch-lightning yacs torchmetrics;pip install performer-pytorch;pip install tensorboardX", "cmd_prefix": "pip", "cmd_postfix": "install ogb", "target_cmd": "pip install ogb"}
{"uuid": "5745f5d3-ca12-4d74-a0b9-95333daed125", "execution_plan": "step1: Set up the Python environment using Conda to ensure all dependencies are installed; step2: Install PyTorch and PyTorch Geometric with CUDA support for GPU acceleration; step3: Install additional required libraries such as RDKit, pandas, and other dependencies; step4: Install optional libraries like pytorch-lightning, yacs, torchmetrics, and wandb for extended functionality; step5: Clean the Conda environment to remove unnecessary files; step6: Import the desired layer (OrthogonalGCNConvLayer or UnitaryGCNConvLayer) from the respective module; step7: Initialize the layer with the required parameters (input_dim, output_dim, etc.); step8: Prepare the data object using PyTorch Geometric's Data class; step9: Apply the layer to the data object to perform the graph message passing operation", "executed_cmds": "conda create -n graphgps python=3.10;conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia;pip install torch_geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu117.html;conda install openbabel fsspec rdkit -c conda-forge;conda install pandas;pip install pytorch-lightning yacs torchmetrics;pip install performer-pytorch;pip install tensorboardX;pip install ogb", "cmd_prefix": "pip", "cmd_postfix": "install wandb", "target_cmd": "pip install wandb"}
{"uuid": "85633ce2-059a-4e78-b5c1-7ea816518e73", "execution_plan": "step1: Set up the Python environment using Conda to ensure all dependencies are installed; step2: Install PyTorch and PyTorch Geometric with CUDA support for GPU acceleration; step3: Install additional required libraries such as RDKit, pandas, and other dependencies; step4: Install optional libraries like pytorch-lightning, yacs, torchmetrics, and wandb for extended functionality; step5: Clean the Conda environment to remove unnecessary files; step6: Import the desired layer (OrthogonalGCNConvLayer or UnitaryGCNConvLayer) from the respective module; step7: Initialize the layer with the required parameters (input_dim, output_dim, etc.); step8: Prepare the data object using PyTorch Geometric's Data class; step9: Apply the layer to the data object to perform the graph message passing operation", "executed_cmds": "conda create -n graphgps python=3.10;conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia;pip install torch_geometric==2.3.0;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu117.html;conda install openbabel fsspec rdkit -c conda-forge;conda install pandas;pip install pytorch-lightning yacs torchmetrics;pip install performer-pytorch;pip install tensorboardX;pip install ogb;pip install wandb", "cmd_prefix": "conda", "cmd_postfix": "clean --all", "target_cmd": "conda clean --all"}
{"uuid": "596c2b68-4617-482f-9c2f-b5d3fa0916ae", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the dataset (COCO2017, Objects365, or custom dataset) by downloading and organizing the files; step3: Modify the configuration files to point to the correct dataset paths; step4: Train the model on the selected dataset (COCO2017, Objects365, or custom dataset); step5: Test the trained model to evaluate performance; step6: Fine-tune the model on a different dataset if needed; step7: Export the model to ONNX format for deployment; step8: Convert the ONNX model to TensorRT for optimized inference; step9: Perform inference using the trained model (ONNX, TensorRT, or PyTorch); step10: Benchmark the model to measure FLOPs, MACs, and latency; step11: Visualize results using Fiftyone", "executed_cmds": "conda create -n dfine python=3.11.9", "cmd_prefix": "conda", "cmd_postfix": "activate dfine", "target_cmd": "conda activate dfine"}
{"uuid": "6080cbc5-2c91-4ba2-8fdd-c0537c469440", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the dataset (COCO2017, Objects365, or custom dataset) by downloading and organizing the files; step3: Modify the configuration files to point to the correct dataset paths; step4: Train the model on the selected dataset (COCO2017, Objects365, or custom dataset); step5: Test the trained model to evaluate performance; step6: Fine-tune the model on a different dataset if needed; step7: Export the model to ONNX format for deployment; step8: Convert the ONNX model to TensorRT for optimized inference; step9: Perform inference using the trained model (ONNX, TensorRT, or PyTorch); step10: Benchmark the model to measure FLOPs, MACs, and latency; step11: Visualize results using Fiftyone", "executed_cmds": "conda create -n dfine python=3.11.9;conda activate dfine", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "15d97194-5888-4e58-88e3-14480d65ba28", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages using UV; step2: Navigate to the DeFT directory and activate the environment; step3: Run a demo for quick start, such as Speculative Decoding, Multi-step Reasoning, or Few-shot Prompting; step4: Run large-scale experiments for tasks like few-shot prompting, multi-step reasoning, or speculative decoding; step5: Reproduce results by referring to provided reference data and notebooks for data processing", "executed_cmds": "conda create -n deft python=3.12", "cmd_prefix": "conda", "cmd_postfix": "activate deft", "target_cmd": "conda activate deft"}
{"uuid": "6270422a-1883-4a09-a6cc-3b47c92523ae", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages using UV; step2: Navigate to the DeFT directory and activate the environment; step3: Run a demo for quick start, such as Speculative Decoding, Multi-step Reasoning, or Few-shot Prompting; step4: Run large-scale experiments for tasks like few-shot prompting, multi-step reasoning, or speculative decoding; step5: Reproduce results by referring to provided reference data and notebooks for data processing", "executed_cmds": "conda create -n deft python=3.12;conda activate deft", "cmd_prefix": "pip", "cmd_postfix": "install uv", "target_cmd": "pip install uv"}
{"uuid": "0f7caa34-2980-456e-93e2-8df55dd607a4", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages using UV; step2: Navigate to the DeFT directory and activate the environment; step3: Run a demo for quick start, such as Speculative Decoding, Multi-step Reasoning, or Few-shot Prompting; step4: Run large-scale experiments for tasks like few-shot prompting, multi-step reasoning, or speculative decoding; step5: Reproduce results by referring to provided reference data and notebooks for data processing", "executed_cmds": "conda create -n deft python=3.12;conda activate deft;pip install uv", "cmd_prefix": "cd", "cmd_postfix": "DeFT", "target_cmd": "cd DeFT"}
{"uuid": "469b19ce-799d-495f-b08e-b81275245d90", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages using UV; step2: Navigate to the DeFT directory and activate the environment; step3: Run a demo for quick start, such as Speculative Decoding, Multi-step Reasoning, or Few-shot Prompting; step4: Run large-scale experiments for tasks like few-shot prompting, multi-step reasoning, or speculative decoding; step5: Reproduce results by referring to provided reference data and notebooks for data processing", "executed_cmds": "conda create -n deft python=3.12;conda activate deft;pip install uv;cd DeFT", "cmd_prefix": "uv", "cmd_postfix": "sync --dev", "target_cmd": "uv sync --dev"}
{"uuid": "db7ca962-e1e0-4015-84d2-63014b293392", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages using UV; step2: Navigate to the DeFT directory and activate the environment; step3: Run a demo for quick start, such as Speculative Decoding, Multi-step Reasoning, or Few-shot Prompting; step4: Run large-scale experiments for tasks like few-shot prompting, multi-step reasoning, or speculative decoding; step5: Reproduce results by referring to provided reference data and notebooks for data processing", "executed_cmds": "conda create -n deft python=3.12;conda activate deft;pip install uv;cd DeFT;uv sync --dev", "cmd_prefix": ".", "cmd_postfix": ".venv/bin/activate", "target_cmd": ". .venv/bin/activate"}
{"uuid": "313eda01-7e77-49e6-bcac-d302d3502904", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages using UV; step2: Navigate to the DeFT directory and activate the environment; step3: Run a demo for quick start, such as Speculative Decoding, Multi-step Reasoning, or Few-shot Prompting; step4: Run large-scale experiments for tasks like few-shot prompting, multi-step reasoning, or speculative decoding; step5: Reproduce results by referring to provided reference data and notebooks for data processing", "executed_cmds": "conda create -n deft python=3.12;conda activate deft;pip install uv;cd DeFT;uv sync --dev;. .venv/bin/activate;export CUDA_VISIBLE_DEVICES=2;export model=\"meta-llama/Meta-Llama-3.1-8B\"", "cmd_prefix": "export", "cmd_postfix": "mode=\"flatten\"", "target_cmd": "export mode=\"flatten\""}
{"uuid": "80e4ffe3-2db0-451b-a128-c44a91855662", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages using UV; step2: Navigate to the DeFT directory and activate the environment; step3: Run a demo for quick start, such as Speculative Decoding, Multi-step Reasoning, or Few-shot Prompting; step4: Run large-scale experiments for tasks like few-shot prompting, multi-step reasoning, or speculative decoding; step5: Reproduce results by referring to provided reference data and notebooks for data processing", "executed_cmds": "conda create -n deft python=3.12;conda activate deft;pip install uv;cd DeFT;uv sync --dev;. .venv/bin/activate;export CUDA_VISIBLE_DEVICES=2;export model=\"meta-llama/Meta-Llama-3.1-8B\";export mode=\"flatten\"", "cmd_prefix": "export", "cmd_postfix": "mem=\"paged\"", "target_cmd": "export mem=\"paged\""}
{"uuid": "9cb612a8-bebe-4356-a152-a9ef4d3fa1f8", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages using UV; step2: Navigate to the DeFT directory and activate the environment; step3: Run a demo for quick start, such as Speculative Decoding, Multi-step Reasoning, or Few-shot Prompting; step4: Run large-scale experiments for tasks like few-shot prompting, multi-step reasoning, or speculative decoding; step5: Reproduce results by referring to provided reference data and notebooks for data processing", "executed_cmds": "conda create -n deft python=3.12;conda activate deft;pip install uv;cd DeFT;uv sync --dev;. .venv/bin/activate;export CUDA_VISIBLE_DEVICES=2;export model=\"meta-llama/Meta-Llama-3.1-8B\";export mode=\"flatten\";export mem=\"paged\";export task=\"Speculative_Decoding\";export dataset=\"../dataset/generation/Speculative_Decoding/APPS_tree_size64.json\"", "cmd_prefix": "export", "cmd_postfix": "prompt_len=6000", "target_cmd": "export prompt_len=6000"}
{"uuid": "77dfd62b-037e-4c29-8140-d3ad51591ec8", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages using UV; step2: Navigate to the DeFT directory and activate the environment; step3: Run a demo for quick start, such as Speculative Decoding, Multi-step Reasoning, or Few-shot Prompting; step4: Run large-scale experiments for tasks like few-shot prompting, multi-step reasoning, or speculative decoding; step5: Reproduce results by referring to provided reference data and notebooks for data processing", "executed_cmds": "conda create -n deft python=3.12;conda activate deft;pip install uv;cd DeFT;uv sync --dev;. .venv/bin/activate;export CUDA_VISIBLE_DEVICES=2;export model=\"meta-llama/Meta-Llama-3.1-8B\";export mode=\"flatten\";export mem=\"paged\";export task=\"Speculative_Decoding\";export dataset=\"../dataset/generation/Speculative_Decoding/APPS_tree_size64.json\";export prompt_len=6000", "cmd_prefix": "export", "cmd_postfix": "maxseq=7000", "target_cmd": "export maxseq=7000"}
{"uuid": "72ceae16-d60c-4d04-b4d9-6d9eb5c60bb6", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages using UV; step2: Navigate to the DeFT directory and activate the environment; step3: Run a demo for quick start, such as Speculative Decoding, Multi-step Reasoning, or Few-shot Prompting; step4: Run large-scale experiments for tasks like few-shot prompting, multi-step reasoning, or speculative decoding; step5: Reproduce results by referring to provided reference data and notebooks for data processing", "executed_cmds": "conda create -n deft python=3.12;conda activate deft;pip install uv;cd DeFT;uv sync --dev;. .venv/bin/activate;export CUDA_VISIBLE_DEVICES=2;export model=\"meta-llama/Meta-Llama-3.1-8B\";export mode=\"flatten\";export mem=\"paged\";export task=\"Speculative_Decoding\";export dataset=\"../dataset/generation/Speculative_Decoding/APPS_tree_size64.json\";export prompt_len=6000;export maxseq=7000", "cmd_prefix": "export", "cmd_postfix": "tree_idx=0", "target_cmd": "export tree_idx=0"}
{"uuid": "76201a53-59fd-42a5-81f5-bfaa6c405d5d", "execution_plan": "step1: Set up the environment by creating a conda environment and installing necessary packages using UV; step2: Navigate to the DeFT directory and activate the environment; step3: Run a demo for quick start, such as Speculative Decoding, Multi-step Reasoning, or Few-shot Prompting; step4: Run large-scale experiments for tasks like few-shot prompting, multi-step reasoning, or speculative decoding; step5: Reproduce results by referring to provided reference data and notebooks for data processing", "executed_cmds": "conda create -n deft python=3.12;conda activate deft;pip install uv;cd DeFT;uv sync --dev;. .venv/bin/activate;export CUDA_VISIBLE_DEVICES=2;export model=\"meta-llama/Meta-Llama-3.1-8B\";export mode=\"flatten\";export mem=\"paged\";export task=\"Speculative_Decoding\";export dataset=\"../dataset/generation/Speculative_Decoding/APPS_tree_size64.json\";export prompt_len=6000;export maxseq=7000;export tree_idx=0;python examples/run_DeFT_llama_paged.py --model $model --max_seq_len $maxseq --mode $mode --Branch_controller $task --dataset $dataset --mem $mem --tree_idx $tree_idx --prompt_len $prompt_len;cd DeFT/experiments/few_shot_prompting;bash run_few_shot.sh [your_device_id]", "cmd_prefix": "cd", "cmd_postfix": "../reasoning", "target_cmd": "cd ../reasoning"}
{"uuid": "77483664-0708-43b6-a8c6-7c64b81bd808", "execution_plan": "step1: Create a virtual Python environment using Python 3.9 to isolate dependencies; step2: Activate the virtual environment to ensure all subsequent commands run within it; step3: Install all required packages listed in the requirements.txt file to set up the environment; step4: Run the Off-Policy Evaluation (OPE) experiments to reproduce the results from the paper; step5: Run the Off-Policy Selection (OPS) experiments to reproduce the results from the paper; step6: Run the Off-Policy Learning (OPL) experiments to reproduce the results from the paper", "executed_cmds": "virtualenv -p python3.9 pess_ls", "cmd_prefix": "source", "cmd_postfix": "pess_ls/bin/activate", "target_cmd": "source pess_ls/bin/activate"}
{"uuid": "20c56b92-27db-4852-85f2-380515ccbd0e", "execution_plan": "step1: Create a virtual Python environment using Python 3.9 to isolate dependencies; step2: Activate the virtual environment to ensure all subsequent commands run within it; step3: Install all required packages listed in the requirements.txt file to set up the environment; step4: Run the Off-Policy Evaluation (OPE) experiments to reproduce the results from the paper; step5: Run the Off-Policy Selection (OPS) experiments to reproduce the results from the paper; step6: Run the Off-Policy Learning (OPL) experiments to reproduce the results from the paper", "executed_cmds": "virtualenv -p python3.9 pess_ls;source pess_ls/bin/activate", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "f3604e78-19fa-4756-b1bc-66ee5e6357fe", "execution_plan": "step1: Install the AQLM inference library for GPU and/or CPU support; step2: Download and load a prequantized model from Hugging Face using the transformers library; step3: Quantize a model using AQLM with specified parameters and calibration data; step4: Fine-tune the quantized model using PV-Tuning for improved accuracy; step5: Evaluate the quantized model using zero-shot benchmarks via LM Evaluation Harness; step6: Convert the quantized model to a Hugging Face-compatible format for inference; step7: Prepare a fine-tuning dataset by tokenizing a subset of RedPajama data; step8: Log results to Weights and Biases (wandb) for tracking experiments", "executed_cmds": "pip install aqlm[gpu;cpu];from transformers import AutoModelForCausalLM; quantized_model = AutoModelForCausalLM.from_pretrained(\"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\";trust_remote_code=True;torch_dtype=\"auto\").cuda();python main.py $MODEL_PATH $DATASET_PATH --nsamples=1024 --val_size=128 --num_codebooks=1 --nbits_per_codebook=16 --in_group_size=8 --relative_mse_tolerance=0.01 --finetune_batch_size=32 --finetune_max_epochs=10 --finetune_early_stop=3 --finetune_keep_best --local_batch_size=1 --offload_activations --wandb --resume --save $SAVE_PATH;torchrun --nproc-per-node=$NUM_GPUS finetune.py --base_model $MODEL_PATH --quantized_model $QUANTIZED_WEIGHTS_PATH --model_seqlen=$SEQLEN --block_type LlamaDecoderLayer --load_dtype bfloat16 --amp_dtype bfloat16 --code_dtype uint16 --dataset_name=$TOKENIZED_DATASET_PATH --split none --seed 42 --preprocessing_chunk_length 100000 --cache_dir=$CACHE_DIR --trust_remote_code --update_codes --update_codebooks_and_scales --update_non_quantized_parameters --lamb --debias --lr 3e-4 --adam_beta1 0.90 --adam_beta2 0.95 --max_code_change_per_step 1e-2 --code_lr 1e-2 --code_beta1 0.0 --code_beta2 0.95 --beam_size 5 --delta_decay 0 --batch_size=128 --microbatch_size=1 --max_epochs 1 --gradient_checkpointing --print_every_steps=1 --verbose_optimizer --wandb --eval_every_steps=10 --keep_best_model --save $SAVE_PATH --save_every_steps 100 --attn_implementation flash_attention_2;python lmeval.py --model hf --model_args pretrained=$MODEL_PATH;dtype=float16", "cmd_prefix": "parallelize=True", "cmd_postfix": "--tasks winogrande", "target_cmd": "parallelize=True --tasks winogrande"}
{"uuid": "7f82aea7-f10e-42d8-b9be-dfb3fa9e3694", "execution_plan": "step1: Create and activate a Python virtual environment; step2: Install PyTorch based on the GPU type (NVIDIA or AMD); step3: Install PyTorch Geometric based on the GPU type (NVIDIA or AMD); step4: Install common Python packages from requirements.txt; step5: Modify the configuration files and evaluator settings as needed; step6: Configure wandb for logging; step7: Run the training script with the specified dataset and configuration file", "executed_cmds": "conda create --name grant python==3.8", "cmd_prefix": "conda", "cmd_postfix": "activate grant", "target_cmd": "conda activate grant"}
{"uuid": "9296ab35-8474-4349-b370-bee7362e16e6", "execution_plan": "step1: Create and activate a Python virtual environment; step2: Install PyTorch based on the GPU type (NVIDIA or AMD); step3: Install PyTorch Geometric based on the GPU type (NVIDIA or AMD); step4: Install common Python packages from requirements.txt; step5: Modify the configuration files and evaluator settings as needed; step6: Configure wandb for logging; step7: Run the training script with the specified dataset and configuration file", "executed_cmds": "conda create --name grant python==3.8;conda activate grant;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/${CUDA_VERSION} (for NVIDIA);docker pull rocm/pytorch:latest;docker run -it --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --device=/dev/kfd --device=/dev/dri --group-add video --ipc=host --shm-size 8G rocm/pytorch:latest (for AMD);pip install torch_geometric;pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-${PyTorch_VERSION}+${CUDA_VERSION}.html (for NVIDIA);pip install torch_geometric (for AMD)", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "0dc4f114-833f-4ee9-a220-eb97e2628b82", "execution_plan": "    step1: Set up the conda environment using the provided environment.yml file;     step2: Create necessary directories for pretrained models and temporary files;     step3: Train shadow models for CIFAR10 dataset;     step4: Train shadow models for CIFAR100 dataset;     step5: Convert pre-trained ImageNet models from TensorFlow to PyTorch;     step6: Compute scores for CIFAR100 dataset;     step7: Compute scores for CIFAR10 dataset;     step8: Compute scores for ImageNet dataset;     step9: Download and set up precomputed scores and subset indices for reproducing results;     step10: Run Jupyter notebooks to reproduce results from the paper", "executed_cmds": "conda env update -n py3.9_curv_clues --file environment.yml", "cmd_prefix": "mkdir", "cmd_postfix": "pretrained", "target_cmd": "mkdir pretrained"}
{"uuid": "8f298c84-5bd1-4264-a91b-32644e91877f", "execution_plan": "    step1: Set up the conda environment using the provided environment.yml file;     step2: Create necessary directories for pretrained models and temporary files;     step3: Train shadow models for CIFAR10 dataset;     step4: Train shadow models for CIFAR100 dataset;     step5: Convert pre-trained ImageNet models from TensorFlow to PyTorch;     step6: Compute scores for CIFAR100 dataset;     step7: Compute scores for CIFAR10 dataset;     step8: Compute scores for ImageNet dataset;     step9: Download and set up precomputed scores and subset indices for reproducing results;     step10: Run Jupyter notebooks to reproduce results from the paper", "executed_cmds": "conda env update -n py3.9_curv_clues --file environment.yml;mkdir pretrained", "cmd_prefix": "mkdir", "cmd_postfix": "pretrained/cifar100", "target_cmd": "mkdir pretrained/cifar100"}
{"uuid": "0f7e903f-13c0-447e-9172-6c3d82ba03d8", "execution_plan": "step1: Install the required Python version and dependencies; step2: Run covar_exp.py to perform experiments for figures 2 and 8; step3: Run inv_g_exp.py to perform experiments for figure 3; step4: Run cifar_acc_exp.py to perform experiments for figure 4; step5: Run normal_acc_exp.py to perform experiments for figure 10; step6: Run sbh_exp.py to perform experiments for figure 5; step7: Run orth_proj_exp.py to perform experiments for figure 6; step8: Run pruning_exp.py to perform experiments for tables 1 and 2; step9: Use compute_G function to compute  as described in the paper", "executed_cmds": "pip install torch==2.1.1 torchvision==0.16.1", "cmd_prefix": "python", "cmd_postfix": "covar_exp.py", "target_cmd": "python covar_exp.py"}
{"uuid": "109f8d2d-60d9-4e0f-917d-6dfbdd3eb370", "execution_plan": "step1: Install the required Python version and dependencies; step2: Run covar_exp.py to perform experiments for figures 2 and 8; step3: Run inv_g_exp.py to perform experiments for figure 3; step4: Run cifar_acc_exp.py to perform experiments for figure 4; step5: Run normal_acc_exp.py to perform experiments for figure 10; step6: Run sbh_exp.py to perform experiments for figure 5; step7: Run orth_proj_exp.py to perform experiments for figure 6; step8: Run pruning_exp.py to perform experiments for tables 1 and 2; step9: Use compute_G function to compute  as described in the paper", "executed_cmds": "pip install torch==2.1.1 torchvision==0.16.1;python covar_exp.py", "cmd_prefix": "python", "cmd_postfix": "inv_g_exp.py", "target_cmd": "python inv_g_exp.py"}
{"uuid": "1dd3ba0b-6720-467c-8d2c-8c5c36e93c68", "execution_plan": "step1: Install the required Python version and dependencies; step2: Run covar_exp.py to perform experiments for figures 2 and 8; step3: Run inv_g_exp.py to perform experiments for figure 3; step4: Run cifar_acc_exp.py to perform experiments for figure 4; step5: Run normal_acc_exp.py to perform experiments for figure 10; step6: Run sbh_exp.py to perform experiments for figure 5; step7: Run orth_proj_exp.py to perform experiments for figure 6; step8: Run pruning_exp.py to perform experiments for tables 1 and 2; step9: Use compute_G function to compute  as described in the paper", "executed_cmds": "pip install torch==2.1.1 torchvision==0.16.1;python covar_exp.py;python inv_g_exp.py", "cmd_prefix": "python", "cmd_postfix": "cifar_acc_exp.py", "target_cmd": "python cifar_acc_exp.py"}
{"uuid": "fb35f74d-9ba0-4c6b-a874-f2749dd6d1b4", "execution_plan": "step1: Install the required Python version and dependencies; step2: Run covar_exp.py to perform experiments for figures 2 and 8; step3: Run inv_g_exp.py to perform experiments for figure 3; step4: Run cifar_acc_exp.py to perform experiments for figure 4; step5: Run normal_acc_exp.py to perform experiments for figure 10; step6: Run sbh_exp.py to perform experiments for figure 5; step7: Run orth_proj_exp.py to perform experiments for figure 6; step8: Run pruning_exp.py to perform experiments for tables 1 and 2; step9: Use compute_G function to compute  as described in the paper", "executed_cmds": "pip install torch==2.1.1 torchvision==0.16.1;python covar_exp.py;python inv_g_exp.py;python cifar_acc_exp.py", "cmd_prefix": "python", "cmd_postfix": "normal_acc_exp.py", "target_cmd": "python normal_acc_exp.py"}
{"uuid": "3cc2ed95-501b-4814-9478-f39f19333af3", "execution_plan": "step1: Install the required Python version and dependencies; step2: Run covar_exp.py to perform experiments for figures 2 and 8; step3: Run inv_g_exp.py to perform experiments for figure 3; step4: Run cifar_acc_exp.py to perform experiments for figure 4; step5: Run normal_acc_exp.py to perform experiments for figure 10; step6: Run sbh_exp.py to perform experiments for figure 5; step7: Run orth_proj_exp.py to perform experiments for figure 6; step8: Run pruning_exp.py to perform experiments for tables 1 and 2; step9: Use compute_G function to compute  as described in the paper", "executed_cmds": "pip install torch==2.1.1 torchvision==0.16.1;python covar_exp.py;python inv_g_exp.py;python cifar_acc_exp.py;python normal_acc_exp.py", "cmd_prefix": "python", "cmd_postfix": "sbh_exp.py", "target_cmd": "python sbh_exp.py"}
{"uuid": "57230630-fff9-4b82-9220-af01832af9af", "execution_plan": "step1: Install the required Python version and dependencies; step2: Run covar_exp.py to perform experiments for figures 2 and 8; step3: Run inv_g_exp.py to perform experiments for figure 3; step4: Run cifar_acc_exp.py to perform experiments for figure 4; step5: Run normal_acc_exp.py to perform experiments for figure 10; step6: Run sbh_exp.py to perform experiments for figure 5; step7: Run orth_proj_exp.py to perform experiments for figure 6; step8: Run pruning_exp.py to perform experiments for tables 1 and 2; step9: Use compute_G function to compute  as described in the paper", "executed_cmds": "pip install torch==2.1.1 torchvision==0.16.1;python covar_exp.py;python inv_g_exp.py;python cifar_acc_exp.py;python normal_acc_exp.py;python sbh_exp.py", "cmd_prefix": "python", "cmd_postfix": "orth_proj_exp.py", "target_cmd": "python orth_proj_exp.py"}
{"uuid": "464dd9db-06c5-4081-9aa9-cee46bc1a52e", "execution_plan": "step1: Install the required Python version and dependencies; step2: Run covar_exp.py to perform experiments for figures 2 and 8; step3: Run inv_g_exp.py to perform experiments for figure 3; step4: Run cifar_acc_exp.py to perform experiments for figure 4; step5: Run normal_acc_exp.py to perform experiments for figure 10; step6: Run sbh_exp.py to perform experiments for figure 5; step7: Run orth_proj_exp.py to perform experiments for figure 6; step8: Run pruning_exp.py to perform experiments for tables 1 and 2; step9: Use compute_G function to compute  as described in the paper", "executed_cmds": "pip install torch==2.1.1 torchvision==0.16.1;python covar_exp.py;python inv_g_exp.py;python cifar_acc_exp.py;python normal_acc_exp.py;python sbh_exp.py;python orth_proj_exp.py", "cmd_prefix": "python", "cmd_postfix": "pruning_exp.py", "target_cmd": "python pruning_exp.py"}
{"uuid": "983f21de-49cd-4eaf-9438-0a2e78804eda", "execution_plan": "step1: Clone the repository from GitHub to your local machine; step2: Set up a conda environment using the provided conda-recipe.yaml file; step3: Activate the conda environment; step4: Export your Weights & Biases API key for training tracking; step5: Perform Supervised Fine-Tuning (SFT) using the provided script and dataset; step6: Register a new dataset if needed by following the instructions in the correction.py file; step7: Download and use the provided datasets and models from Hugging Face if needed", "executed_cmds": "git clone https://github.com/cby-pku/aligner.git", "cmd_prefix": "cd", "cmd_postfix": "aligner", "target_cmd": "cd aligner"}
{"uuid": "85087736-313f-462c-8443-6c530468dc23", "execution_plan": "step1: Clone the repository from GitHub to your local machine; step2: Set up a conda environment using the provided conda-recipe.yaml file; step3: Activate the conda environment; step4: Export your Weights & Biases API key for training tracking; step5: Perform Supervised Fine-Tuning (SFT) using the provided script and dataset; step6: Register a new dataset if needed by following the instructions in the correction.py file; step7: Download and use the provided datasets and models from Hugging Face if needed", "executed_cmds": "git clone https://github.com/cby-pku/aligner.git;cd aligner;conda env create --file conda-recipe.yaml", "cmd_prefix": "conda", "cmd_postfix": "activate aligner", "target_cmd": "conda activate aligner"}
{"uuid": "2e0f99b1-ba5d-4d84-a791-cf245d7846db", "execution_plan": "step1: Clone the repository from GitHub to your local machine; step2: Set up a conda environment using the provided conda-recipe.yaml file; step3: Activate the conda environment; step4: Export your Weights & Biases API key for training tracking; step5: Perform Supervised Fine-Tuning (SFT) using the provided script and dataset; step6: Register a new dataset if needed by following the instructions in the correction.py file; step7: Download and use the provided datasets and models from Hugging Face if needed", "executed_cmds": "git clone https://github.com/cby-pku/aligner.git;cd aligner;conda env create --file conda-recipe.yaml;conda activate aligner", "cmd_prefix": "export", "cmd_postfix": "WANDB_API_KEY=\"...\"", "target_cmd": "export WANDB_API_KEY=\"...\""}
{"uuid": "30bbaead-2770-4698-a377-ffec110aadfe", "execution_plan": "step1: Create a conda environment and install required dependencies; step2: Create necessary directories for storing models and logs; step3: Train the BD3-LM model on OpenWebText; step4: Generate arbitrary-length sequences using a pre-trained BD3-LM model; step5: Evaluate test perplexity on OpenWebText; step6: Reproduce experiments using provided scripts", "executed_cmds": "conda create --name bd3lm python=3.9", "cmd_prefix": "conda", "cmd_postfix": "activate bd3lm", "target_cmd": "conda activate bd3lm"}
{"uuid": "469ece85-c342-4479-8f30-ad7dbe1bb66c", "execution_plan": "step1: Create a conda environment and install required dependencies; step2: Create necessary directories for storing models and logs; step3: Train the BD3-LM model on OpenWebText; step4: Generate arbitrary-length sequences using a pre-trained BD3-LM model; step5: Evaluate test perplexity on OpenWebText; step6: Reproduce experiments using provided scripts", "executed_cmds": "conda create --name bd3lm python=3.9;conda activate bd3lm", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "8689141d-c4cb-472a-af12-3f8899b934f2", "execution_plan": "step1: Set up the conda environment and install necessary dependencies; step2: Prepare the dataset as described in the data README; step3: Perform SFT (Supervised Fine-Tuning) training using LoRA on the base models; step4: Merge the LoRA adapter weights with the model weights after SFT training; step5: Perform DPO (Direct Preference Optimization) training on the SFT model using LoRA; step6: Merge the LoRA adapter weights with the model weights after DPO training; step7: Perform WSPO (Weak-to-Strong Preference Optimization) training using LoRA; step8: Merge the LoRA adapter weights with the model weights after WSPO training; step9: Install additional dependencies for full fine-tuning; step10: Perform SFT training using full fine-tuning on the base models; step11: Perform DPO training using full fine-tuning on the SFT model; step12: Perform WSPO training using full fine-tuning", "executed_cmds": "conda create -n wspo python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate wspo", "target_cmd": "conda activate wspo"}
{"uuid": "6772b226-1953-457c-97d7-8fe495fec04b", "execution_plan": "step1: Set up the conda environment and install necessary dependencies; step2: Prepare the dataset as described in the data README; step3: Perform SFT (Supervised Fine-Tuning) training using LoRA on the base models; step4: Merge the LoRA adapter weights with the model weights after SFT training; step5: Perform DPO (Direct Preference Optimization) training on the SFT model using LoRA; step6: Merge the LoRA adapter weights with the model weights after DPO training; step7: Perform WSPO (Weak-to-Strong Preference Optimization) training using LoRA; step8: Merge the LoRA adapter weights with the model weights after WSPO training; step9: Install additional dependencies for full fine-tuning; step10: Perform SFT training using full fine-tuning on the base models; step11: Perform DPO training using full fine-tuning on the SFT model; step12: Perform WSPO training using full fine-tuning", "executed_cmds": "conda create -n wspo python=3.10;conda activate wspo;cd weak-to-strong-preference-optimization", "cmd_prefix": "pip install", "cmd_postfix": "-e \".[torch", "target_cmd": "pip install -e \".[torch"}
{"uuid": "3e7604de-d957-4b50-9468-2ccef21bd0f7", "execution_plan": "step1: Set up the Python environment and install dependencies; step2: Download the pretrained model and config files; step3: Place the model checkpoint and config files in the specified directory; step4: Generate audio from a text prompt using the simple generation method; step5: Generate audio from a text prompt using the coarse-to-fine generation method with GPT induction; step6: Generate audio from a text prompt using the coarse-to-fine generation method with manual settings", "executed_cmds": "cd models", "cmd_prefix": "conda create", "cmd_postfix": "-n \"bewo\" python=3.9", "target_cmd": "conda create -n \"bewo\" python=3.9"}
{"uuid": "44ddea2b-2021-48f4-93b6-572d3b19f344", "execution_plan": "step1: Set up the Python environment and install dependencies; step2: Download the pretrained model and config files; step3: Place the model checkpoint and config files in the specified directory; step4: Generate audio from a text prompt using the simple generation method; step5: Generate audio from a text prompt using the coarse-to-fine generation method with GPT induction; step6: Generate audio from a text prompt using the coarse-to-fine generation method with manual settings", "executed_cmds": "cd models;conda create -n \"bewo\" python=3.9", "cmd_prefix": "conda", "cmd_postfix": "activate bewo", "target_cmd": "conda activate bewo"}
{"uuid": "18724be4-484e-4f0c-8ad0-5c2c6e085b18", "execution_plan": "step1: Set up the Python environment and install dependencies; step2: Download the pretrained model and config files; step3: Place the model checkpoint and config files in the specified directory; step4: Generate audio from a text prompt using the simple generation method; step5: Generate audio from a text prompt using the coarse-to-fine generation method with GPT induction; step6: Generate audio from a text prompt using the coarse-to-fine generation method with manual settings", "executed_cmds": "cd models;conda create -n \"bewo\" python=3.9;conda activate bewo;pip install -r requirements.txt --no-dependencies", "cmd_prefix": "cd", "cmd_postfix": "models", "target_cmd": "cd models"}
{"uuid": "c1e23ab5-23e7-4be0-9e36-151212e290d0", "execution_plan": "step1: Set up the Python environment and install dependencies; step2: Download the pretrained model and config files; step3: Place the model checkpoint and config files in the specified directory; step4: Generate audio from a text prompt using the simple generation method; step5: Generate audio from a text prompt using the coarse-to-fine generation method with GPT induction; step6: Generate audio from a text prompt using the coarse-to-fine generation method with manual settings", "executed_cmds": "cd models;conda create -n \"bewo\" python=3.9;conda activate bewo;pip install -r requirements.txt --no-dependencies;cd models;python simple_generation.py --prompt \"A dog is barking on the left.\" --device cuda:0", "cmd_prefix": "cd", "cmd_postfix": "models", "target_cmd": "cd models"}
{"uuid": "d6b737d6-b197-4598-aa06-a6e887e5c762", "execution_plan": "step1: Set up the Python environment and install dependencies; step2: Download the pretrained model and config files; step3: Place the model checkpoint and config files in the specified directory; step4: Generate audio from a text prompt using the simple generation method; step5: Generate audio from a text prompt using the coarse-to-fine generation method with GPT induction; step6: Generate audio from a text prompt using the coarse-to-fine generation method with manual settings", "executed_cmds": "cd models;conda create -n \"bewo\" python=3.9;conda activate bewo;pip install -r requirements.txt --no-dependencies;cd models;python simple_generation.py --prompt \"A dog is barking on the left.\" --device cuda:0;cd models;python gpt_induction.py --prompt \"A dog is barking on the left.\" --device cuda:0", "cmd_prefix": "cd", "cmd_postfix": "models", "target_cmd": "cd models"}
{"uuid": "58574a3e-22b0-4960-8723-fc2d2f6373bc", "execution_plan": "step1: Set up a Python virtual environment and activate it; step2: Install the required dependencies and configure the environment; step3: Explore the demo notebooks to understand how to compute reliable conformal prediction sets (RPS); step4: Reproduce the paper's results by training models using SEML; step5: Compute reliable prediction sets and certificates after model training is complete; step6: Reproduce the full results by training models and computing RPS using provided configurations", "executed_cmds": "python -m venv venv", "cmd_prefix": "source", "cmd_postfix": "venv/bin/activate", "target_cmd": "source venv/bin/activate"}
{"uuid": "aa2c635e-f71a-4fb3-9d18-08ab5d001155", "execution_plan": "step1: Set up a Python virtual environment and activate it; step2: Install the required dependencies and configure the environment; step3: Explore the demo notebooks to understand how to compute reliable conformal prediction sets (RPS); step4: Reproduce the paper's results by training models using SEML; step5: Compute reliable prediction sets and certificates after model training is complete; step6: Reproduce the full results by training models and computing RPS using provided configurations", "executed_cmds": "python -m venv venv;source venv/bin/activate", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "62485d21-150f-4d43-bc08-933a18363bb5", "execution_plan": "step1: Set up a Python virtual environment and activate it; step2: Install the required dependencies and configure the environment; step3: Explore the demo notebooks to understand how to compute reliable conformal prediction sets (RPS); step4: Reproduce the paper's results by training models using SEML; step5: Compute reliable prediction sets and certificates after model training is complete; step6: Reproduce the full results by training models and computing RPS using provided configurations", "executed_cmds": "python -m venv venv;source venv/bin/activate;pip install -r requirements.txt", "cmd_prefix": "pip install", "cmd_postfix": "-e .", "target_cmd": "pip install -e ."}
{"uuid": "60acb407-43bd-4b04-9366-f1f6d4681bdb", "execution_plan": "step1: Set up a Python virtual environment and activate it; step2: Install the required dependencies and configure the environment; step3: Explore the demo notebooks to understand how to compute reliable conformal prediction sets (RPS); step4: Reproduce the paper's results by training models using SEML; step5: Compute reliable prediction sets and certificates after model training is complete; step6: Reproduce the full results by training models and computing RPS using provided configurations", "executed_cmds": "python -m venv venv;source venv/bin/activate;pip install -r requirements.txt;pip install -e .;export CUBLAS_WORKSPACE_CONFIG=:4096:8;seml rcp_training add configs/training/0-ResNet18-CIFAR10.yaml", "cmd_prefix": "seml", "cmd_postfix": "rcp_training start", "target_cmd": "seml rcp_training start"}
{"uuid": "dcf072d6-e711-4bae-bb25-ef283a6d73fc", "execution_plan": "step1: Set up a Python virtual environment and activate it; step2: Install the required dependencies and configure the environment; step3: Explore the demo notebooks to understand how to compute reliable conformal prediction sets (RPS); step4: Reproduce the paper's results by training models using SEML; step5: Compute reliable prediction sets and certificates after model training is complete; step6: Reproduce the full results by training models and computing RPS using provided configurations", "executed_cmds": "python -m venv venv;source venv/bin/activate;pip install -r requirements.txt;pip install -e .;export CUBLAS_WORKSPACE_CONFIG=:4096:8;seml rcp_training add configs/training/0-ResNet18-CIFAR10.yaml;seml rcp_training start;seml rcp add configs/rcp/0-cert-setting0.yaml", "cmd_prefix": "seml", "cmd_postfix": "rcp start", "target_cmd": "seml rcp start"}
{"uuid": "a0e3a07a-07c5-4b5a-9ea1-0f76a925f143", "execution_plan": "step1: Clone the repository to get the source code; step2: Create a virtual environment to isolate dependencies; step3: Install the required dependencies to set up the environment; step4: Run the experiments using the main.py script", "executed_cmds": "git clone https://github.com/log-postech/safe-torch.git", "cmd_prefix": "cd", "cmd_postfix": "safe-torch", "target_cmd": "cd safe-torch"}
{"uuid": "c8740b2f-979b-41b0-aab1-c7e1c6f5c280", "execution_plan": "step1: Clone the repository to get the source code; step2: Create a virtual environment to isolate dependencies; step3: Install the required dependencies to set up the environment; step4: Run the experiments using the main.py script", "executed_cmds": "git clone https://github.com/log-postech/safe-torch.git;cd safe-torch", "cmd_prefix": "conda create", "cmd_postfix": "-n safe python=3.10", "target_cmd": "conda create -n safe python=3.10"}
{"uuid": "e27eee28-ed47-49bc-ba53-703b2047bf33", "execution_plan": "step1: Clone the repository to get the source code; step2: Create a virtual environment to isolate dependencies; step3: Install the required dependencies to set up the environment; step4: Run the experiments using the main.py script", "executed_cmds": "git clone https://github.com/log-postech/safe-torch.git;cd safe-torch;conda create -n safe python=3.10", "cmd_prefix": "conda", "cmd_postfix": "activate safe", "target_cmd": "conda activate safe"}
{"uuid": "82261501-6889-40ba-8560-750c0a3586c7", "execution_plan": "step1: Clone the repository to get the source code; step2: Create a virtual environment to isolate dependencies; step3: Install the required dependencies to set up the environment; step4: Run the experiments using the main.py script", "executed_cmds": "git clone https://github.com/log-postech/safe-torch.git;cd safe-torch;conda create -n safe python=3.10;conda activate safe", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "541f8647-3eae-4c87-8023-c4f87b751e89", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the datasets by downloading and extracting them to the specified directories; step3: Download pretraining weights for stable diffusion and DINOv2; step4: Run the evaluation script with the provided configuration, checkpoint, and backbone paths", "executed_cmds": "conda create -n querydiff -y", "cmd_prefix": "conda", "cmd_postfix": "activate querydiff", "target_cmd": "conda activate querydiff"}
{"uuid": "f5a30a76-56f8-428c-a84f-8d3a2664f563", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the datasets by downloading and extracting them to the specified directories; step3: Download pretraining weights for stable diffusion and DINOv2; step4: Run the evaluation script with the provided configuration, checkpoint, and backbone paths", "executed_cmds": "conda create -n querydiff -y;conda activate querydiff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia -y", "cmd_prefix": "pip install", "cmd_postfix": "-U openmim", "target_cmd": "pip install -U openmim"}
{"uuid": "105b78f9-9926-476d-afbc-a9acd6efb4b0", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the datasets by downloading and extracting them to the specified directories; step3: Download pretraining weights for stable diffusion and DINOv2; step4: Run the evaluation script with the provided configuration, checkpoint, and backbone paths", "executed_cmds": "conda create -n querydiff -y;conda activate querydiff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia -y;pip install -U openmim", "cmd_prefix": "mim", "cmd_postfix": "install mmengine", "target_cmd": "mim install mmengine"}
{"uuid": "a49de83c-5e2f-4663-9639-a69aed062349", "execution_plan": "step1: Set up the conda environment and install required dependencies; step2: Prepare the datasets by downloading and extracting them to the specified directories; step3: Download pretraining weights for stable diffusion and DINOv2; step4: Run the evaluation script with the provided configuration, checkpoint, and backbone paths", "executed_cmds": "conda create -n querydiff -y;conda activate querydiff;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia -y;pip install -U openmim;mim install mmengine;mim install \"mmcv>=2.0.0\";pip install \"mmsegmentation>=1.0.0\";pip install \"mmdet>=3.0.0\";pip install xformers=='0.0.20'", "cmd_prefix": "pip install", "cmd_postfix": "-r requirements.txt", "target_cmd": "pip install -r requirements.txt"}
{"uuid": "be013c19-45df-4148-9c8b-55af55028d36", "execution_plan": "step1: Install the required libraries to run the SAM 2 implementation; step2: Clone the repository to get the project files; step3: Navigate into the cloned repository directory; step4: Open the Jupyter notebook to interact with the SAM 2 implementation; step5: Follow the notebook instructions to load images or videos and perform segmentation; step6: Use the automatic mask generator or point prompt feature as needed", "executed_cmds": "pip install opencv-python pandas numpy torch matplotlib;git clone https://github.com/mushaid01/segment-anything-2.git", "cmd_prefix": "cd", "cmd_postfix": "segment-anything-2", "target_cmd": "cd segment-anything-2"}
