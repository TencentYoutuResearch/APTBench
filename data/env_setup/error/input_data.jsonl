{"uuid": "a57b2ef3-8f3c-412a-961e-a6c44b85623c", "setup_instruct": "# Step-by-Step Execution Plan for DMD2 Project\n\n## 1. Environment Setup\n- **Description**: Create and activate a Conda environment, then install required dependencies.\n- **Commands**:\n  ```bash\n  conda create -n dmd2 python=3.8 -y\n  conda activate dmd2\n  pip install --upgrade anyio\n  pip install -r requirements.txt\n  python setup.py develop\n  ```\n\n## 2. Inference Examples\n### 2.1 ImageNet Inference\n- **Description**: Run inference on ImageNet using a pre-trained checkpoint.\n- **Commands**:\n  ```bash\n  python -m demo.imagenet_example --checkpoint_path IMAGENET_CKPT_PATH\n  ```\n\n### 2.2 Text-to-Image Inference (SDXL)\n- **Description**: Generate images from text prompts using SDXL checkpoints.\n- **Commands**:\n  ```bash\n  # 4-step generation (higher quality)\n  python -m demo.text_to_image_sdxl --checkpoint_path SDXL_CKPT_PATH --precision float16\n\n  # 1-step generation\n  python -m demo.text_to_image_sdxl --num_step 1 --checkpoint_path SDXL_CKPT_PATH --precision float16 --conditioning_timestep 399\n  ```\n\n### 2.3 Diffuser Pipeline Examples\n- **Description**: Use the `diffusers` library for UNet/LoRA-based generation.\n- **Commands**:  \n  (Refer to README for Python scripts for 4-step/1-step UNet/LoRA generation and 4-step T2I Adapter.)\n\n## 3. Training and Evaluation\n### 3.1 ImageNet-64x64\n- **Description**: Follow instructions in the ImageNet-specific README.\n- **Steps**: Navigate to `experiments/imagenet/README.md`.\n\n### 3.2 SDXL Training\n- **Description**: Follow instructions in the SDXL-specific README.\n- **Steps**: Navigate to `experiments/sdxl/README.md`.\n\n### 3.3 SDv1.5 Training\n- **Description**: Follow instructions in the SDv1.5-specific README.\n- **Steps**: Navigate to `experiments/sdv1.5/README.md`.\n\n## 4. Citation\n- **Description**: Cite the DMD2 papers if used in research.\n- **BibTeX**:  \n  ```bib\n  @inproceedings{yin2024improved,\n      title={Improved Distribution Matching Distillation for Fast Image Synthesis},\n      author={Yin, Tianwei and Gharbi, Micha{\\\"e}l and Park, Taesung and Zhang, Richard and Shechtman, Eli and Durand, Fredo and Freeman, William T},\n      booktitle={NeurIPS},\n      year={2024}\n  }\n  ```", "issue_title": "train a 4step SDXL got CUDA error: no kernel image is available for execution on the device", "issue_body": "I create conda env followed README and got this error when training SDXL.\r\n\r\nIt seems that the torch version installed is with **cu11** but my GPU has cuda **12.2** (I checked from nvidia-smi).\r\n\r\nHowever, the previous issue https://github.com/tianweiy/DMD2/issues/41 indicate the only working env is the one follows README?\r\n\r\nMassive thanks for your work", "choices": "(A) 1. Remove FSDP (Fully Sharded Data Parallel) from the training configuration.\n2. Set the mixed precision to 'bf16' by modifying the accelerator configuration to `mixed_precision=\"bf16\"` instead of `mixed_precision=\"no\"`.\n3. Ensure the models are properly prepared after making these changes.\n4. If training on multiple GPUs, note that batch size per GPU may need to be reduced (e.g., bs=1 per GPU) due to memory overhead from DDP (Distributed Data Parallel).\n5. Optionally, enable gradient checkpointing to save memory, and consider offloading parts of the model to CPU after computing certain losses to free up GPU memory. (B) 1. Remove FSDP (Fully Sharded Data Parallel) from the training configuration.  \n2. Set the mixed precision to 'bf16' by modifying the accelerator configuration to `mixed_precision=\"bf16\"` instead of `mixed_precision=\"no\"`.  \n3. Re-enable FSDP to improve training efficiency.  \n4. Ensure the models are properly prepared after making these changes.  \n5. If training on multiple GPUs, note that batch size per GPU may need to be reduced (e.g., bs=1 per GPU) due to memory overhead from DDP (Distributed Data Parallel).  \n6. Optionally, enable gradient checkpointing to save memory, and consider offloading parts of the model to CPU after computing certain losses to free up GPU memory. (C) 1. Remove FSDP (Fully Sharded Data Parallel) from the training configuration.  \n2. Set the mixed precision to 'bf16' by modifying the accelerator configuration to `mixed_precision=\"bf16\"` instead of `mixed_precision=\"no\"`.  \n3. Disable mixed precision to avoid potential numerical instability.  \n4. Ensure the models are properly prepared after making these changes.  \n5. If training on multiple GPUs, note that batch size per GPU may need to be reduced (e.g., bs=1 per GPU) due to memory overhead from DDP (Distributed Data Parallel).  \n6. Optionally, enable gradient checkpointing to save memory, and consider offloading parts of the model to CPU after computing certain losses to free up GPU memory. (D) 1. Remove FSDP (Fully Sharded Data Parallel) from the training configuration.  \n2. Set the mixed precision to 'bf16' by modifying the accelerator configuration to `mixed_precision=\"bf16\"` instead of `mixed_precision=\"no\"`.  \n3. Optionally, enable gradient checkpointing to save memory, and consider offloading parts of the model to CPU after computing certain losses to free up GPU memory.  \n4. Ensure the models are properly prepared after making these changes.  \n5. If training on multiple GPUs, note that batch size per GPU may need to be reduced (e.g., bs=1 per GPU) due to memory overhead from DDP (Distributed Data Parallel). (E) 1. Remove FSDP (Fully Sharded Data Parallel) from the training configuration.  \n2. Set the mixed precision to 'bf16' by modifying the accelerator configuration to `mixed_precision=\"bf16\"` instead of `mixed_precision=\"no\"`.  \n3. Ensure the models are properly prepared after making these changes.  \n4. Optionally, enable gradient checkpointing to save memory, and consider offloading parts of the model to CPU after computing certain losses to free up GPU memory. (F) 1. If training on multiple GPUs, note that batch size per GPU may need to be reduced (e.g., bs=1 per GPU) due to memory overhead from DDP (Distributed Data Parallel).  \n2. Remove FSDP (Fully Sharded Data Parallel) from the training configuration.  \n3. Set the mixed precision to 'bf16' by modifying the accelerator configuration to `mixed_precision=\"bf16\"` instead of `mixed_precision=\"no\"`.  \n4. Ensure the models are properly prepared after making these changes.  \n5. Optionally, enable gradient checkpointing to save memory, and consider offloading parts of the model to CPU after computing certain losses to free up GPU memory. (G) 1. Remove FSDP (Fully Sharded Data Parallel) from the training configuration.  \n2. Ensure the models are properly prepared after making these changes.  \n3. If training on multiple GPUs, note that batch size per GPU may need to be reduced (e.g., bs=1 per GPU) due to memory overhead from DDP (Distributed Data Parallel).  \n4. Optionally, enable gradient checkpointing to save memory, and consider offloading parts of the model to CPU after computing certain losses to free up GPU memory.", "answer": "A"}
{"uuid": "826ac55b-9b34-4569-ae51-88573eae81b9", "setup_instruct": "# Step-by-Step Execution Plan for OmniCorpus Dataset\n\n## 1. **Understand the Dataset Structure**\n   - **Description**: Review the three sections of OmniCorpus (CC, CW, YT) and their sources.\n   - **Resources**:\n     - [OmniCorpus-CC](https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC)\n     - [OmniCorpus-CW](https://openxlab.org.cn/datasets/Li-Qingyun/OmniCorpus-CW)\n     - [OmniCorpus-YT](https://huggingface.co/datasets/OpenGVLab/OmniCorpus-YT)\n\n## 2. **Access the Dataset**\n   - **Description**: Download the dataset from the provided links based on your needs.\n   - **Commands**:\n     ```bash\n     # For OmniCorpus-CC (Hugging Face)\n     git lfs install\n     git clone https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC\n\n     # For OmniCorpus-YT (Hugging Face)\n     git clone https://huggingface.co/datasets/OpenGVLab/OmniCorpus-YT\n\n     # For OmniCorpus-CW (OpenDataLab)\n     # Follow OpenDataLab's download instructions (may require registration)\n     ```\n\n## 3. **Explore the Data Pipeline**\n   - **Description**: Understand the five stages of the data pipeline (main body extraction, text filtering, deduplication, image filtering, detailed text filtering).\n   - **Resources**: Refer to the [paper](https://openreview.net/pdf?id=kwqhn2VuG4) for details.\n\n## 4. **Use the Dataset for Pre-training**\n   - **Description**: Utilize the dataset for multimodal large language model (MLLM) pre-training or other recommended usages.\n   - **Resources**:\n     - [Pre-trained Model](https://huggingface.co/Qingyun/OmniCorpus-InternVL)\n     - Example scripts may be available in the [GitHub repo](https://github.com/OpenGVLab/OmniCorpus).\n\n## 5. **Evaluate Performance**\n   - **Description**: Use the provided benchmarks to evaluate model performance.\n   - **Resources**: Refer to the experimental results section in the README or paper.\n\n## 6. **Comply with Licensing and Terms of Use**\n   - **Description**: Ensure compliance with CC BY 4.0 License and jurisdiction-specific terms (Common Crawl, Chinese regulations, YouTube ToUs).\n   - **Resources**:\n     - [CC BY 4.0 License](https://creativecommons.org/licenses/by/4.0/)\n     - [Common Crawl ToUs](https://commoncrawl.org/terms-of-use)\n     - [YouTube ToUs](https://www.youtube.com/terms)\n\n## 7. **Cite the Work**\n   - **Description**: Acknowledge the use of OmniCorpus in any derived work.\n   - **Citation**:\n     ```bibtex\n     @inproceedings{li2024omnicorpus,\n       title={OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text},\n       author={Li, Qingyun and Chen, Zhe and Wang, Weiyun and Wang, Wenhai and Ye, Shenglong and Jin, Zhenjiang and others},\n       booktitle={The Thirteenth International Conference on Learning Representations},\n       year={2025}\n     }\n     ```", "issue_title": "How can i obtain sampled_omnicorpus_cc.json file", "issue_body": "Hi, thanks for your great project and open-sourced data.\r\nI encounter a problem when using the training scripts you provide [here](https://github.com/OpenGVLab/OmniCorpus/blob/main/mllm_llava/README.md#pre-training-with-interleaved-image-text-corpus-with-natural-arrangement).\r\n\r\nI'm not clear on the data format for `sampled_omnicorpus_cc.json` and `sampled_omnicorpus_cc_with_similarities.json`.\r\nCould you provide a few examples from these files or guide me on how to generate them?", "choices": "(A) 1. Alternatively, sample documents from the [OmniCorpus-CC-210M](https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC-210M) dataset if you need a larger dataset.  \n2. Access the provided Baidu Yun link: https://pan.baidu.com/s/1_sl8GKtrOWwNUhHQsvlXHA?pwd=rixd to download the demo JSON files (`sampled_omnicorpus_cc.json` and `sampled_omnicorpus_cc_with_similarities.json`).  \n3. Note that the format of `sampled_omnicorpus_cc.json` follows the OBELICS-like format, which is different from the OmniCorpus-CC-210M dataset format. (B) 1. Access the provided Baidu Yun link: https://pan.baidu.com/s/1_sl8GKtrOWwNUhHQsvlXHA?pwd=rixd to download the demo JSON files (`sampled_omnicorpus_cc.json` and `sampled_omnicorpus_cc_with_similarities.json`).  \n2. Open the JSON files and manually edit their contents to add random data.  \n3. Alternatively, sample documents from the [OmniCorpus-CC-210M](https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC-210M) dataset if you need a larger dataset.  \n4. Note that the format of `sampled_omnicorpus_cc.json` follows the OBELICS-like format, which is different from the OmniCorpus-CC-210M dataset format. (C) 1. Access the provided Baidu Yun link: https://pan.baidu.com/s/1_sl8GKtrOWwNUhHQsvlXHA?pwd=rixd to download the demo JSON files (`sampled_omnicorpus_cc.json` and `sampled_omnicorpus_cc_with_similarities.json`).  \n2. Alternatively, sample documents from the [OmniCorpus-CC-210M](https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC-210M) dataset if you need a larger dataset. (D) 1. Note that the format of `sampled_omnicorpus_cc.json` follows the OBELICS-like format, which is different from the OmniCorpus-CC-210M dataset format.  \n2. Access the provided Baidu Yun link: https://pan.baidu.com/s/1_sl8GKtrOWwNUhHQsvlXHA?pwd=rixd to download the demo JSON files (`sampled_omnicorpus_cc.json` and `sampled_omnicorpus_cc_with_similarities.json`).  \n3. Alternatively, sample documents from the [OmniCorpus-CC-210M](https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC-210M) dataset if you need a larger dataset. (E) 1. Access the provided Baidu Yun link: https://pan.baidu.com/s/1_sl8GKtrOWwNUhHQsvlXHA?pwd=rixd to download the demo JSON files (`sampled_omnicorpus_cc.json` and `sampled_omnicorpus_cc_with_similarities.json`).  \n2. Delete the downloaded JSON files immediately after downloading them.  \n3. Alternatively, sample documents from the [OmniCorpus-CC-210M](https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC-210M) dataset if you need a larger dataset.  \n4. Note that the format of `sampled_omnicorpus_cc.json` follows the OBELICS-like format, which is different from the OmniCorpus-CC-210M dataset format. (F) 1. Alternatively, sample documents from the [OmniCorpus-CC-210M](https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC-210M) dataset if you need a larger dataset.  \n2. Note that the format of `sampled_omnicorpus_cc.json` follows the OBELICS-like format, which is different from the OmniCorpus-CC-210M dataset format. (G) 1. Access the provided Baidu Yun link: https://pan.baidu.com/s/1_sl8GKtrOWwNUhHQsvlXHA?pwd=rixd to download the demo JSON files (`sampled_omnicorpus_cc.json` and `sampled_omnicorpus_cc_with_similarities.json`).\n2. Alternatively, sample documents from the [OmniCorpus-CC-210M](https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC-210M) dataset if you need a larger dataset.\n3. Note that the format of `sampled_omnicorpus_cc.json` follows the OBELICS-like format, which is different from the OmniCorpus-CC-210M dataset format.", "answer": "G"}
{"uuid": "63b580be-dcc4-4693-a8d3-36af0e8d617f", "setup_instruct": "# Step-by-Step Execution Plan for \"1Prompt1Story\" Project\n\n## 1. Clone the Repository\n- **Description**: Download the project source code from GitHub.\n- **Command**:\n  ```bash\n  git clone https://github.com/byliutao/1Prompt1Story\n  ```\n\n## 2. Navigate to Project Directory\n- **Description**: Enter the cloned repository folder.\n- **Command**:\n  ```bash\n  cd 1Prompt1Story\n  ```\n\n## 3. Set Up Conda Environment\n- **Description**: Create and activate a Conda environment with Python 3.10.\n- **Commands**:\n  ```bash\n  conda create --name 1p1s python=3.10\n  conda activate 1p1s\n  ```\n\n## 4. Install PyTorch with CUDA\n- **Description**: Install PyTorch with CUDA 12.1 support (adjust version if needed).\n- **Command**:\n  ```bash\n  conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n  ```\n\n## 5. Install Additional Dependencies\n- **Description**: Install required libraries (`transformers`, `diffusers`, etc.).\n- **Commands**:\n  ```bash\n  pip install transformers==4.46.3  # or: conda install conda-forge::transformers\n  conda install -c conda-forge diffusers\n  pip install opencv-python scipy gradio==4.44.1 sympy==1.13.1\n  ```\n\n## 6. Run Inference Code\n- **Description**: Execute the main script for consistent image generation.\n- **Command**:\n  ```bash\n  python main.py\n  ```\n\n## 7. Launch Gradio Demo (Optional)\n- **Description**: Start the interactive web demo locally.\n- **Command**:\n  ```bash\n  python app.py\n  ```\n\n## 8. Generate Benchmark Data (Optional)\n- **Description**: Run the benchmark generation script with specified paths.\n- **Command**:\n  ```bash\n  python -m resource.gen_benchmark --save_dir ./result/benchmark --benchmark_path ./resource/consistory+.yaml\n  ```\n\n## 9. Verify Outputs\n- **Description**: Check generated images in `./result/benchmark` (if benchmark was run) or outputs from `main.py`.", "issue_title": "数值测评问题", "issue_body": "你好，这是个很有意思的工作，我们正在follow.\n我们用resources/gen_benchmark.py  (未修改任何参数）生成数据，然后自己写了数值测评代码(clip-i, clip-t和dream sim), dream sim 也使用了文章里提到的用噪声替代背景， 我们最后跑出来的结果是 clip-t 0.7591, clip-i 0.8994, dream sim 0.1943. 这个结果主要是clip-t 和文章的结果差别有点大，可能是我们自己写的数值测评代码和你用的不一样？ 请问你方便开源一下数值测评代码吗 ？ 如果不方便的话，可否多提供一些关于数值测评的信息 ？ 比如clip-i 是两两图片算score再求平均吗 ？\n\n谢谢。", "choices": "(A) 1. Update your DreamSim installation to version 0.1.3 if you are using a different version (e.g., 2.0.1).\n2. Ensure you are using the correct version of DreamSim (version 0.1.3) as specified by the maintainer.\n3. Re-run your evaluation code with the updated DreamSim version to obtain results consistent with the paper. (B) 1. Ensure you are using the correct version of DreamSim (version 0.1.3) as specified by the maintainer.\n2. Update your DreamSim installation to version 0.1.3 if you are using a different version (e.g., 2.0.1).\n3. Re-run your evaluation code with the updated DreamSim version to obtain results consistent with the paper. (C) 1. Ensure you are using the correct version of DreamSim (version 0.1.3) as specified by the maintainer.\n2. Re-run your evaluation code with the updated DreamSim version to obtain results consistent with the paper.\n3. Update your DreamSim installation to version 0.1.3 if you are using a different version (e.g., 2.0.1). (D) 1. Ensure you are using the correct version of DreamSim (version 0.1.3) as specified by the maintainer.\n2. Re-run your evaluation code with the updated DreamSim version to obtain results consistent with the paper. (E) 1. Ensure you are using the correct version of DreamSim (version 0.1.3) as specified by the maintainer.\n2. Update your DreamSim installation to version 0.1.3 if you are using a different version (e.g., 2.0.1).\n3. Downgrade DreamSim to version 0.0.9 for compatibility with older scripts.\n4. Re-run your evaluation code with the updated DreamSim version to obtain results consistent with the paper. (F) 1. Update your DreamSim installation to version 0.1.3 if you are using a different version (e.g., 2.0.1).\n2. Re-run your evaluation code with the updated DreamSim version to obtain results consistent with the paper. (G) 1. Skip version checking and proceed directly to running the evaluation code.\n2. Re-run your evaluation code with the updated DreamSim version to obtain results consistent with the paper.", "answer": "B"}
{"uuid": "16239c85-2bfd-48fa-aadd-2fcdd43b801f", "setup_instruct": "# DartControl Execution Plan\n\n## 1. Environment Setup\n- **Description**: Set up the conda environment using the provided YAML file.\n- **Command**:\n  ```bash\n  conda env create -f environment.yml\n  conda activate DART\n  ```\n\n## 2. Data and Model Checkpoints\n- **Description**: Download and organize required data and model checkpoints.\n- **Steps**:\n  1. Download model checkpoints and data from [Google Drive](https://drive.google.com/drive/folders/1vJg3GFVPT6kr6cA0HrQGmiAEBE2dkaps?usp=drive_link).\n  2. Extract and merge into the project folder.\n  3. Download additional datasets:\n     - [SMPL-X](https://download.is.tue.mpg.de/download.php?domain=smplx&sfile=smplx_lockedhead_20230207.zip)\n     - [SMPL-H](https://download.is.tue.mpg.de/download.php?domain=mano&resume=1&sfile=smplh.tar.xz)\n     - [AMASS](https://amass.is.tue.mpg.de/)\n     - [BABEL](https://download.is.tue.mpg.de/download.php?domain=teach&resume=1&sfile=babel-data/babel-teach.zip)\n     - [HumanML3D](https://github.com/EricGuo5513/HumanML3D)\n  4. Organize files as per the [folder structure](#data-and-model-checkpoints).\n\n## 3. Visualization Setup\n- **Description**: Configure visualization tools (Pyrender and Blender).\n- **Steps**:\n  1. Install Pyrender:\n     ```bash\n     pip install pyrender\n     ```\n  2. For Blender:\n     - Install [Blender](https://www.blender.org/).\n     - Install the [SMPL-X Blender Add-on](https://gitlab.tuebingen.mpg.de/jtesch/smplx_blender_addon#installation).\n\n## 4. Motion Generation Demos\n- **Description**: Run demos for text-conditioned motion generation.\n- **Commands**:\n  - Interactive Online Generation:\n    ```bash\n    source ./demos/run_demo.sh\n    ```\n  - Headless Text-Conditioned Composition:\n    ```bash\n    source ./demos/rollout.sh\n    ```\n  - Visualize Results:\n    ```bash\n    python -m visualize.vis_seq --add_floor 1 --translate_body 1 --seq_path './mld_denoiser/mld_fps_clip_repeat_euler/checkpoint_300000/rollout/walk_in_circles*20_guidance5.0_seed0/*.pkl'\n    ```\n\n## 5. Training\n- **Description**: Train models using BABEL or HML3D datasets.\n- **Steps**:\n  1. Preprocess Data:\n     ```bash\n     python -m data_scripts.extract_dataset\n     ```\n  2. Train Motion Primitive VAE:\n     ```bash\n     python -m mld.train_mvae --track 1 --exp_name 'mvae_babel_smplx' --data_args.dataset 'mp_seq_v2' --data_args.data_dir './data/seq_data_zero_male' --data_args.cfg_path './config_files/config_hydra/motion_primitive/mp_h2_f8_r8.yaml' --data_args.weight_scheme 'text_samp:0.' --train_args.batch_size 128  --train_args.weight_kl 1e-6  --train_args.stage1_steps 100000 --train_args.stage2_steps 50000 --train_args.stage3_steps 50000 --train_args.save_interval 50000  --train_args.weight_smpl_joints_rec 10.0 --train_args.weight_joints_consistency 10.0 --train_args.weight_transl_delta 100 --train_args.weight_joints_delta 100 --train_args.weight_orient_delta 100  --model_args.arch 'all_encoder' --train_args.ema_decay 0.999 --model_args.num_layers 7 --model_args.latent_dim 1 256\n     ```\n  3. Train Latent Diffusion Model:\n     ```bash\n     python -m mld.train_mld --track 1 --exp_name 'mld_babel_smplx' --train_args.batch_size 1024 --train_args.use_amp 1 --data_args.dataset 'mp_seq_v2' --data_args.data_dir './data/seq_data_zero_male' --data_args.cfg_path './config_files/config_hydra/motion_primitive/mp_h2_f8_r4.yaml' --denoiser_args.mvae_path './mvae/mvae_babel_smplx/checkpoint_200000.pt' --denoiser_args.train_rollout_type 'full' --denoiser_args.train_rollout_history 'rollout' --train_args.stage1_steps 100000 --train_args.stage2_steps 100000 --train_args.stage3_steps 100000 --train_args.save_interval 100000 --train_args.weight_latent_rec 1.0 --train_args.weight_feature_rec 1.0 --train_args.weight_smpl_joints_rec 0 --train_args.weight_joints_consistency 0 --train_args.weight_transl_delta 1e4 --train_args.weight_joints_delta 1e4 --train_args.weight_orient_delta 1e4 --data_args.weight_scheme 'text_samp:0.' denoiser-args.model-args:denoiser-transformer-args\n     ```\n\n## 6. Evaluation\n- **Description**: Evaluate model performance on motion composition, in-betweening, and goal reaching.\n- **Commands**:\n  - Motion Composition:\n    ```bash\n    source ./evaluation/eval_gen_composition.sh\n    ```\n  - In-betweening:\n    ```bash\n    source ./evaluation/eval_gen_inbetween.sh\n    ```\n  - Goal Reaching:\n    ```bash\n    source ./evaluation/eval_gen_goal_reach.sh\n    ```", "issue_title": "Fine tuning", "issue_body": "I tried to fine tune the model but i get lot of problems, can you guys provide a short  guide soo the model cannot only be trained from zero but also finetune? im using my custom dataset and is the same as babel and i prerpocessed eveyrhting into pkl", "choices": "(A) 1. **Preprocess the Dataset**: Ensure your custom dataset is preprocessed correctly. If some joints are not tracked (resulting in zero values), add very slight noise to those joints to avoid zero standard deviation issues.\n2. **Modify Training Script**: Load the provided checkpoint and your custom dataset in the training script. Adjust the number of iterations for each training stage as needed.\n3. **Fine-tune VAE if Necessary**: If your dataset introduces new action categories, fine-tune the VAE model to accommodate these changes.\n4. **Action Statistics**: For weighted sequence sampling, define action weights for your custom dataset or use uniform sampling if preferred.\n5. **Combine Datasets (Optional)**: You can choose to train on your custom dataset alone or combine it with other datasets like BABEL for better results. (B) 1. **Preprocess the Dataset**: Ensure your custom dataset is preprocessed correctly. If some joints are not tracked (resulting in zero values), add very slight noise to those joints to avoid zero standard deviation issues.\n2. **Use Provided Mean and Std Files**: If your dataset has joints with no movement (zero std), use the provided mean and std files from the repository instead of generating new ones.\n4. **Fine-tune VAE if Necessary**: If your dataset introduces new action categories, fine-tune the VAE model to accommodate these changes.\n3. **Modify Training Script**: Load the provided checkpoint and your custom dataset in the training script. Adjust the number of iterations for each training stage as needed.\n5. **Action Statistics**: For weighted sequence sampling, define action weights for your custom dataset or use uniform sampling if preferred.\n6. **Combine Datasets (Optional)**: You can choose to train on your custom dataset alone or combine it with other datasets like BABEL for better results. (C) 1. **Preprocess the Dataset**: Remove all joints with zero values from your custom dataset to avoid zero standard deviation issues.\n2. **Use Provided Mean and Std Files**: If your dataset has joints with no movement (zero std), use the provided mean and std files from the repository instead of generating new ones.\n3. **Modify Training Script**: Load the provided checkpoint and your custom dataset in the training script. Adjust the number of iterations for each training stage as needed.\n4. **Fine-tune VAE if Necessary**: If your dataset introduces new action categories, fine-tune the VAE model to accommodate these changes.\n5. **Action Statistics**: For weighted sequence sampling, define action weights for your custom dataset or use uniform sampling if preferred.\n6. **Combine Datasets (Optional)**: You can choose to train on your custom dataset alone or combine it with other datasets like BABEL for better results. (D) 1. **Use Provided Mean and Std Files**: If your dataset has joints with no movement (zero std), use the provided mean and std files from the repository instead of generating new ones.\n2. **Modify Training Script**: Load the provided checkpoint and your custom dataset in the training script. Adjust the number of iterations for each training stage as needed.\n3. **Fine-tune VAE if Necessary**: If your dataset introduces new action categories, fine-tune the VAE model to accommodate these changes.\n4. **Action Statistics**: For weighted sequence sampling, define action weights for your custom dataset or use uniform sampling if preferred.\n5. **Combine Datasets (Optional)**: You can choose to train on your custom dataset alone or combine it with other datasets like BABEL for better results. (E) 1. **Preprocess the Dataset**: Ensure your custom dataset is preprocessed correctly. If some joints are not tracked (resulting in zero values), add very slight noise to those joints to avoid zero standard deviation issues.\n2. **Use Provided Mean and Std Files**: If your dataset has joints with no movement (zero std), use the provided mean and std files from the repository instead of generating new ones.\n3. **Modify Training Script**: Load the provided checkpoint and your custom dataset in the training script. Adjust the number of iterations for each training stage as needed.\n4. **Fine-tune VAE if Necessary**: If your dataset introduces new action categories, fine-tune the VAE model to accommodate these changes.\n5. **Action Statistics**: For weighted sequence sampling, define action weights for your custom dataset or use uniform sampling if preferred.\n6. **Combine Datasets (Optional)**: You can choose to train on your custom dataset alone or combine it with other datasets like BABEL for better results. (F) 1. **Preprocess the Dataset**: Ensure your custom dataset is preprocessed correctly. If some joints are not tracked (resulting in zero values), add very slight noise to those joints to avoid zero standard deviation issues.\n2. **Generate New Mean and Std Files**: Always generate new mean and std files for your dataset, ignoring the provided ones.\n3. **Use Provided Mean and Std Files**: If your dataset has joints with no movement (zero std), use the provided mean and std files from the repository instead of generating new ones.\n4. **Modify Training Script**: Load the provided checkpoint and your custom dataset in the training script. Adjust the number of iterations for each training stage as needed.\n5. **Fine-tune VAE if Necessary**: If your dataset introduces new action categories, fine-tune the VAE model to accommodate these changes.\n6. **Action Statistics**: For weighted sequence sampling, define action weights for your custom dataset or use uniform sampling if preferred.\n7. **Combine Datasets (Optional)**: You can choose to train on your custom dataset alone or combine it with other datasets like BABEL for better results. (G) 6. **Combine Datasets (Optional)**: You can choose to train on your custom dataset alone or combine it with other datasets like BABEL for better results.\n1. **Preprocess the Dataset**: Ensure your custom dataset is preprocessed correctly. If some joints are not tracked (resulting in zero values), add very slight noise to those joints to avoid zero standard deviation issues.\n2. **Use Provided Mean and Std Files**: If your dataset has joints with no movement (zero std), use the provided mean and std files from the repository instead of generating new ones.\n3. **Modify Training Script**: Load the provided checkpoint and your custom dataset in the training script. Adjust the number of iterations for each training stage as needed.\n4. **Fine-tune VAE if Necessary**: If your dataset introduces new action categories, fine-tune the VAE model to accommodate these changes.\n5. **Action Statistics**: For weighted sequence sampling, define action weights for your custom dataset or use uniform sampling if preferred.", "answer": "E"}
{"uuid": "1f07f640-9975-4439-9247-c1980de93eea", "setup_instruct": "# GrootVL Project Execution Plan\n\n## Environment Setup\n\n### Vision Tasks\n1. **Create Conda Environment**\n   - Description: Create a new Conda environment named 'grootv' with Python 3.9\n   - Command: \n     ```bash\n     conda create -n grootv python=3.9\n     conda activate grootv\n     ```\n\n2. **Install PyTorch**\n   - Description: Install PyTorch with CUDA 11.7 support\n   - Command:\n     ```bash\n     pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n     ```\n\n3. **Install Requirements**\n   - Description: Install required packages for vision tasks\n   - Command:\n     ```bash\n     pip install -r GrootV/grootv_requirements.txt\n     ```\n\n4. **Install Tree Scanning**\n   - Description: Install the Tree Scanning package for vision tasks\n   - Command:\n     ```bash\n     cd GrootV/third-party/TreeScan\n     pip install -v -e .\n     ```\n\n### Language Tasks\n1. **Create Conda Environment**\n   - Description: Create a new Conda environment named 'grootl' with Python 3.9\n   - Command:\n     ```bash\n     conda create -n grootl python=3.9\n     conda activate grootl\n     ```\n\n2. **Install PyTorch**\n   - Description: Install PyTorch version 2.0.1\n   - Command:\n     ```bash\n     pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n     ```\n\n3. **Install Requirements**\n   - Description: Install required packages for language tasks\n   - Command:\n     ```bash\n     pip install -r GrootL/grootl_requirements.txt\n     ```\n\n4. **Install Tree Scanning**\n   - Description: Install the Tree Scanning package for language tasks\n   - Command:\n     ```bash\n     cd GrootL/third-party/TreeScanLan\n     pip install -v -e .\n     ```\n\n5. **Install Evaluation Tools**\n   - Description: Install language model evaluation tools\n   - Command:\n     ```bash\n     cd GrootL/3rdparty/lm-evaluation-harness\n     pip install -v -e .\n     ```\n\n## Model Training & Evaluation\n\n### Vision Tasks (Image Classification)\n1. **Train ImageNet-1k Model**\n   - Description: Execute training script for image classification\n   - Command:\n     ```bash\n     bash GrootV/scripts/bash_cls_train.sh\n     ```\n   - Note: Modify paths in the script as needed\n\n### Language Tasks\n1. **Evaluate Language Models**\n   - Description: Run evaluation script for language understanding tasks\n   - Command:\n     ```bash\n     cd GrootL\n     bash eval.sh\n     ```\n   - Note: Modify paths in the script as needed", "issue_title": "Classification weights for small and base model cannot be accessed, only tiny!", "issue_body": "Hi, I just realized today that I can only access tiny pretrained model.\r\n\r\nPlease, can you make small and base weights also shareable to everyone without restrictions?\r\n\r\nThanks.", "choices": "(A) 1. Open the Google Drive link provided in the readme for the small and base model weights.\n2. Right-click on the file and select 'Share'.\n3. Under 'Get Link', change the setting from 'Restricted' to 'Anyone with the link'.\n4. Set the permission level to 'Viewer' and click 'Copy Link' to update the permissions.\n5. For the GrootL language model, repeat the same steps to adjust permissions. (B) 1. Open the Google Drive link provided in the readme for the small and base model weights.\n2. Right-click on the file and select 'Share'.\n3. Click 'Copy Link' to update the permissions.\n4. Under 'Get Link', change the setting from 'Restricted' to 'Anyone with the link'.\n5. Set the permission level to 'Viewer'.\n6. For the GrootL language model, repeat the same steps to adjust permissions. (C) 1. Open the Google Drive link provided in the readme for the small and base model weights.\n2. Right-click on the file and select 'Share'.\n3. Under 'Get Link', change the setting from 'Restricted' to 'Anyone with the link'.\n4. Click 'Copy Link' to update the permissions.\n5. For the GrootL language model, repeat the same steps to adjust permissions. (D) 1. Open the Google Drive link provided in the readme for the small and base model weights.\n2. Under 'Get Link', change the setting from 'Restricted' to 'Anyone with the link'.\n3. Right-click on the file and select 'Share'.\n4. Set the permission level to 'Viewer' and click 'Copy Link' to update the permissions.\n5. For the GrootL language model, repeat the same steps to adjust permissions. (E) 1. Open the Google Drive link provided in the readme for the small and base model weights.\n2. Right-click on the file and select 'Share'.\n3. Under 'Get Link', change the setting from 'Restricted' to 'Anyone with the link'.\n4. Set the permission level to 'Editor' and click 'Copy Link' to update the permissions.\n5. For the GrootL language model, repeat the same steps to adjust permissions. (F) 1. Open the Google Drive link provided in the readme for the small and base model weights.\n2. Right-click on the file and select 'Share'.\n3. Set the permission level to 'Viewer' and click 'Copy Link' to update the permissions.\n4. For the GrootL language model, repeat the same steps to adjust permissions. (G) 1. Open the Google Drive link provided in the readme for the small and base model weights.\n2. Right-click on the file and select 'Share'.\n3. Under 'Get Link', change the setting from 'Restricted' to 'Anyone with the link'.\n4. Set the permission level to 'Viewer' and click 'Copy Link' to update the permissions.\n5. Delete the file from Google Drive.\n6. For the GrootL language model, repeat the same steps to adjust permissions.", "answer": "A"}
{"uuid": "9bf732b2-2daf-4f55-b383-7d85914eb4c1", "setup_instruct": "# Time-MoE Deployment and Execution Plan\n\n## 1. Prerequisites Installation\n- **Description**: Install Python 3.10+ and project dependencies.\n- **Command**:\n  ```shell\n  pip install -r requirements.txt\n  ```\n\n## 2. Optional Flash-Attn Installation\n- **Description**: Install flash-attn for improved performance (recommended).\n- **Command (Basic)**:\n  ```shell\n  pip install flash-attn==2.6.3\n  ```\n- **Command (Optimized)**:\n  ```shell\n  pip install packaging ninja\n  MAX_JOBS=64 pip install flash-attn==2.6.3 --no-build-isolation\n  ```\n\n## 3. Dataset Preparation\n- **Description**: Download and prepare the Time-300B dataset or custom datasets.\n- **Action**:\n  - Download Time-300B from [Hugging Face](https://huggingface.co/datasets/Maple728/Time-300B).\n  - For custom datasets, convert to `jsonl` format with `sequence` fields.\n\n## 4. Forecasting with Pre-trained Models\n- **Description**: Load and use pre-trained Time-MoE models for forecasting.\n- **Python Code**:\n  ```python\n  import torch\n  from transformers import AutoModelForCausalLM\n\n  model = AutoModelForCausalLM.from_pretrained(\n      'Maple728/TimeMoE-50M',\n      device_map=\"auto\",\n      trust_remote_code=True\n  )\n  # Example inference code (see README for full snippet)\n  ```\n\n## 5. Evaluation\n- **Description**: Evaluate models on benchmark datasets.\n- **Steps**:\n  1. Download datasets from [Google Drive](https://drive.google.com/drive/folders/1KjnAYr9X3D-jyJpo4yM7Giyq5V1Hga_7).\n  2. Place under `./dataset`.\n  3. Run evaluation:\n     ```shell\n     python run_eval.py -d dataset/ETT-small/ETTh1.csv -p 96\n     ```\n\n## 6. Fine-Tuning\n- **Description**: Fine-tune Time-MoE on custom datasets.\n- **Command (CPU)**:\n  ```shell\n  python main.py -d <data_path>\n  ```\n- **Command (Single Node Multi-GPU)**:\n  ```shell\n  python torch_dist_run.py main.py -d <data_path>\n  ```\n- **Command (Multi-Node)**:\n  ```shell\n  export MASTER_ADDR=<addr>; export MASTER_PORT=<port>\n  export WORLD_SIZE=<size>; export RANK=<rank>\n  python torch_dist_run.py main.py -d <data_path>\n  ```\n- **From Scratch Training**:\n  ```shell\n  python torch_dist_run.py main.py -d <data_path> --from_scratch\n  ```\n\n## 7. Citation\n- **Action**: Cite the paper if used in research:\n  ```bibtex\n  @misc{shi2024timemoe,\n    title={Time-MoE: Billion-Scale Time Series Foundation Models...},\n    author={Xiaoming Shi et al.},\n    year={2024},\n    eprint={2409.16040},\n    url={https://arxiv.org/abs/2409.16040}\n  }\n  ```", "issue_title": "NCCL Setup Failure and Hugging Face Rate Limit Error during Model Evaluation", "issue_body": "Dear Author,\nwhen i try to use time-moe 200M model, it shows the following errors.\nCould u give me some suggestion to solve these problem?\n```\n(time-moe) ibdeaa24829:/nfshdd/23125341r/time-moe/Time-MoE_Fine_tuning$ python run_eval.py -d dataset/wave/filtered_swh_50_-35.csv -m Maple728/TimeMoE-200M -p 96\nError:  Setup nccl fail, so set device to cpu: The server socket has failed to listen on any local network address. port: 9899, useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use\n/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nCould not locate the configuration_time_moe.py inside Maple728/TimeMoE-200M.\nTraceback (most recent call last):\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/Maple728/TimeMoE-200M/resolve/main/configuration_time_moe.py\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1376, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1296, in get_hf_file_metadata\n    r = _request_wrapper(\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 277, in _request_wrapper\n    response = _request_wrapper(\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 301, in _request_wrapper\n    hf_raise_for_status(response)\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 477, in hf_raise_for_status\n    raise _format(HfHubHTTPError, str(e), response) from e\nhuggingface_hub.errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/Maple728/TimeMoE-200M/resolve/main/configuration_time_moe.py\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1487, in _raise_on_head_call_error\n    raise LocalEntryNotFoundError(\nhuggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/nfshdd/time-moe/Time-MoE_Fine_tuning/run_eval.py\", line 247, in <module>\n    evaluate(args)\n  File \"/nfshdd/time-moe/Time-MoE_Fine_tuning/run_eval.py\", line 132, in evaluate\n    model = TimeMoE(\n  File \"/nfshdd/time-moe/Time-MoE_Fine_tuning/run_eval.py\", line 61, in __init__\n    model = AutoModelForCausalLM.from_pretrained(\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 523, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n  File \"/home/23125341r/.conda/envs/time-moe/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 937, in from_pretrained\n    config_class = get_class_from_dynamic_module(\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/transformers/dynamic_module_utils.py\", line 489, in get_class_from_dynamic_module\n    final_module = get_cached_module_file(\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/transformers/dynamic_module_utils.py\", line 294, in get_cached_module_file\n    resolved_module_file = cached_file(\n  File \"/home/.conda/envs/time-moe/lib/python3.10/site-packages/transformers/utils/hub.py\", line 441, in cached_file\n    raise EnvironmentError(\nOSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like Maple728/TimeMoE-200M is not the path to a directory containing a file named configuration_time_moe.py.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n```", "choices": "(A) 1. Ensure your internet connection allows access to huggingface.co.\n2. If the error persists, download the model from Hugging Face to your local disk first.\n3. Specify the local path in your evaluation command.\n4. Delete the downloaded model files to free up disk space.\n5. For NCCL setup failure, run the evaluation with the following command to set a different port:\n   ```bash\n   MASTER_PORT=10022 python run_eval.py -d dataset/scour/scour.csv -m Maple728/TimeMoE-50M -p 96\n   ```\n6. To accelerate evaluation across multiple GPUs, use:\n   ```bash\n   python torch_dist_run.py run_eval.py -d dataset/ETT-small/ETTh1.csv -p 96\n   ```\n7. If encountering out-of-memory errors, reduce the batch size. (B) 1. Ensure your internet connection allows access to huggingface.co.\n2. If the error persists, download the model from Hugging Face to your local disk first.\n3. Specify the local path in your evaluation command.\n4. For NCCL setup failure, run the evaluation with the following command to set a different port:\n   ```bash\n   MASTER_PORT=10022 python run_eval.py -d dataset/scour/scour.csv -m Maple728/TimeMoE-50M -p 96\n   ```\n5. To accelerate evaluation across multiple GPUs, use:\n   ```bash\n   python torch_dist_run.py run_eval.py -d dataset/ETT-small/ETTh1.csv -p 96\n   ```\n6. If encountering out-of-memory errors, reduce the batch size. (C) 1. Ensure your internet connection allows access to huggingface.co.\n2. For NCCL setup failure, run the evaluation with the following command to set a different port:\n   ```bash\n   MASTER_PORT=10022 python run_eval.py -d dataset/scour/scour.csv -m Maple728/TimeMoE-50M -p 96\n   ```\n3. If the error persists, download the model from Hugging Face to your local disk first.\n4. Specify the local path in your evaluation command.\n5. To accelerate evaluation across multiple GPUs, use:\n   ```bash\n   python torch_dist_run.py run_eval.py -d dataset/ETT-small/ETTh1.csv -p 96\n   ```\n6. If encountering out-of-memory errors, reduce the batch size. (D) 1. If the error persists, download the model from Hugging Face to your local disk first.\n2. For NCCL setup failure, run the evaluation with the following command to set a different port:\n   ```bash\n   MASTER_PORT=10022 python run_eval.py -d dataset/scour/scour.csv -m Maple728/TimeMoE-50M -p 96\n   ```\n3. To accelerate evaluation across multiple GPUs, use:\n   ```bash\n   python torch_dist_run.py run_eval.py -d dataset/ETT-small/ETTh1.csv -p 96\n   ```\n4. If encountering out-of-memory errors, reduce the batch size.", "answer": "B"}
{"uuid": "2c14167a-e2e8-4f97-813a-ff1bfa7404b7", "setup_instruct": "# Time-MoE Deployment and Usage Plan\n\n## 1. Prerequisites Installation\n- **Description**: Install Python 3.10+ and required dependencies.\n- **Command**:\n  ```shell\n  pip install -r requirements.txt\n  ```\n\n## 2. Optional Flash-Attention Installation\n- **Description**: Install flash-attn for improved performance (recommended).\n- **Command** (Basic):\n  ```shell\n  pip install flash-attn==2.6.3\n  ```\n- **Command** (Optimized for multi-core CPU):\n  ```shell\n  pip install packaging ninja\n  MAX_JOBS=64 pip install flash-attn==2.6.3 --no-build-isolation\n  ```\n\n## 3. Dataset Setup\n- **Description**: Download and prepare the Time-300B dataset or custom datasets.\n- **Action**: \n  - Download Time-300B from [Hugging Face](https://huggingface.co/datasets/Maple728/Time-300B).\n  - For custom datasets, convert to `jsonl` format with `{\"sequence\": [...]}` per line.\n\n## 4. Model Inference\n- **Description**: Load Time-MoE for forecasting.\n- **Python Code**:\n  ```python\n  import torch\n  from transformers import AutoModelForCausalLM\n\n  model = AutoModelForCausalLM.from_pretrained(\n      'Maple728/TimeMoE-50M',\n      device_map=\"auto\",\n      trust_remote_code=True\n  )\n  # With flash-attn (if installed):\n  # model = AutoModelForCausalLM.from_pretrained('Maple728/TimeMoE-50M', device_map=\"auto\", attn_implementation='flash_attention_2', trust_remote_code=True)\n  ```\n\n## 5. Evaluation\n- **Description**: Evaluate on benchmark datasets.\n- **Steps**:\n  1. Download datasets from [Google Drive](https://drive.google.com/drive/folders/1KjnAYr9X3D-jyJpo4yM7Giyq5V1Hga_7?usp=sharing).\n  2. Place under `./dataset`.\n  3. Run evaluation:\n     ```shell\n     python run_eval.py -d dataset/ETT-small/ETTh1.csv -p 96\n     ```\n\n## 6. Fine-Tuning\n- **Description**: Train Time-MoE on custom data.\n- **Command (CPU)**:\n  ```bash\n  python main.py -d <data_path>\n  ```\n- **Command (Single Node Multi-GPU)**:\n  ```bash\n  python torch_dist_run.py main.py -d <data_path>\n  ```\n- **Command (Multi-Node Multi-GPU)**:\n  ```bash\n  export MASTER_ADDR=<master_addr>\n  export MASTER_PORT=<master_port>\n  export WORLD_SIZE=<world_size>\n  export RANK=<rank>\n  python torch_dist_run.py main.py -d <data_path>\n  ```\n- **From-Scratch Training**:\n  ```bash\n  python torch_dist_run.py main.py -d <data_path> --from_scratch\n  ```\n\n## 7. Citation\n- **Action**: Cite the paper if used in research:\n  ```bibtex\n  @misc{shi2024timemoe,\n    title={Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts}, \n    author={Xiaoming Shi et al.},\n    year={2024},\n    eprint={2409.16040},\n    archivePrefix={arXiv},\n    url={https://arxiv.org/abs/2409.16040}, \n  }\n  ```", "issue_title": "Time-300B Dataset", "issue_body": "Dear Author,\nI'm trying to train the time-300b dataset directly without fine-tuning, so I'd like to ask you how to apply the data-cleaning pipeline. I downloaded the time-300b dataset and I'm trying it now. How did you apply the time-300b dataset to the model?\n\nThank you for your clarification, and I greatly appreciate your work on this project.", "choices": "(A) 1. Run the fine-tuning script again with fp16 precision enabled. This should resolve the dtype mismatch error (float vs. c10::Half) encountered during training. (B) 1. Pull the latest branch of the repository to ensure you have the most recent updates. 2. Delete the repository to free up space. 3. Run the fine-tuning script again with fp16 precision enabled. This should resolve the dtype mismatch error (float vs. c10::Half) encountered during training. (C) 1. Run the fine-tuning script again with fp16 precision enabled. This should resolve the dtype mismatch error (float vs. c10::Half) encountered during training. 2. Pull the latest branch of the repository to ensure you have the most recent updates. (D) 1. Pull the latest branch of the repository to ensure you have the most recent updates. 2. Run the fine-tuning script again with fp16 precision enabled. This should resolve the dtype mismatch error (float vs. c10::Half) encountered during training.", "answer": "D"}
{"uuid": "14c8d700-fbbc-4290-a0f5-95c37c78a5ed", "setup_instruct": "# Time-MoE Execution Plan\n\n## 1. Prerequisites Installation\n- **Step**: Install Python 3.10+ and project dependencies  \n  **Command**:  \n  ```shell\n  pip install -r requirements.txt\n  ```\n\n- **Step (Optional)**: Install Flash-Attention for optimized performance  \n  **Command (Basic)**:  \n  ```shell\n  pip install flash-attn==2.6.3\n  ```  \n  **Command (Advanced, with CPU core optimization)**:  \n  ```shell\n  pip install packaging ninja\n  MAX_JOBS=64 pip install flash-attn==2.6.3 --no-build-isolation\n  ```\n\n---\n\n## 2. Dataset Setup\n- **Step**: Download Time-300B dataset from Hugging Face  \n  **Action**: Manually download from [Time-300B Dataset](https://huggingface.co/datasets/Maple728/Time-300B) or use Python snippet:  \n  ```python\n  from time_moe.datasets.time_moe_dataset import TimeMoEDataset\n  ds = TimeMoEDataset('Time-300B')  # Ensure the dataset is saved locally\n  ```\n\n- **Step (Evaluation)**: Download benchmark datasets  \n  **Action**: Get pre-processed datasets from [Google Drive](https://drive.google.com/drive/folders/1KjnAYr9X3D-jyJpo4yM7Giyq5V1Hga_7?usp=sharing) and place under `./dataset`.\n\n---\n\n## 3. Forecasting with Pre-trained Models\n- **Step**: Load Time-MoE model and generate predictions  \n  **Python Code**:  \n  ```python\n  import torch\n  from transformers import AutoModelForCausalLM\n  model = AutoModelForCausalLM.from_pretrained('Maple728/TimeMoE-50M', device_map=\"auto\", trust_remote_code=True)\n  # For Flash-Attn: add `attn_implementation='flash_attention_2'`\n  normed_seqs = (torch.randn(2, 12) - mean) / std  # Example input\n  predictions = model.generate(normed_seqs, max_new_tokens=6)[:, -6:]\n  ```\n\n---\n\n## 4. Evaluation\n- **Step**: Run evaluation on ETTh1 dataset  \n  **Command**:  \n  ```shell\n  python run_eval.py -d dataset/ETT-small/ETTh1.csv -p 96\n  ```\n\n---\n\n## 5. Fine-Tuning\n### 5.1 Dataset Preparation\n- **Step**: Convert data to `jsonl` format (one sequence per line):  \n  ```jsonl\n  {\"sequence\": [1.0, 2.0, ...]}\n  ```\n\n### 5.2 Training Commands\n- **CPU Training**:  \n  ```shell\n  python main.py -d <data_path>\n  ```\n\n- **Single-Node Multi-GPU**:  \n  ```shell\n  python torch_dist_run.py main.py -d <data_path>\n  ```\n\n- **Multi-Node Multi-GPU**:  \n  ```shell\n  export MASTER_ADDR=<addr> MASTER_PORT=<port> WORLD_SIZE=<size> RANK=<rank>\n  python torch_dist_run.py main.py -d <data_path>\n  ```\n\n- **From Scratch Training**: Append `--from_scratch` to any command above.\n\n---\n\n## 6. Additional Notes\n- **Sequence Length**: Ensure `context_length + prediction_length ≤ 4096` (or fine-tune for longer sequences).\n- **Help**: View all CLI options via:  \n  ```shell\n  python main.py --help\n  ```", "issue_title": "train-dataset", "issue_body": "Your work is of great significance. I would like to reproduce your small - scale model. However, the time_moe_dataset class in git seems to be designed for evaluation. I wonder if you can share the training dataset code.", "choices": "(A) 1. Replace the standard normalization method with max-value normalization in your code.\n2. Modify the normalization step to use the maximum value of the sequence instead of mean and standard deviation.\n3. Normalize the sequence again using mean and standard deviation.\n4. Update the prediction denormalization step to multiply by the max value instead of adding the mean and multiplying by the standard deviation.\n5. The modified code should look like this:\n```python\nmax_val = seqs.max(dim=-1, keepdim=True)[0]\nnormed_seqs = seqs / max_val\n\n# ... (rest of the code remains the same until predictions)\n\npredictions = normed_predictions * max_val\n```\n6. This change should improve the forecasting performance for time-series data with obvious trends. (B) 1. Update the prediction denormalization step to multiply by the max value instead of adding the mean and multiplying by the standard deviation.\n2. Replace the standard normalization method with max-value normalization in your code.\n3. Modify the normalization step to use the maximum value of the sequence instead of mean and standard deviation.\n4. The modified code should look like this:\n```python\nmax_val = seqs.max(dim=-1, keepdim=True)[0]\nnormed_seqs = seqs / max_val\n\n# ... (rest of the code remains the same until predictions)\n\npredictions = normed_predictions * max_val\n```\n5. This change should improve the forecasting performance for time-series data with obvious trends. (C) 1. Replace the standard normalization method with max-value normalization in your code.\n2. Modify the normalization step to use the maximum value of the sequence instead of mean and standard deviation.\n3. The modified code should look like this:\n```python\nmax_val = seqs.max(dim=-1, keepdim=True)[0]\nnormed_seqs = seqs / max_val\n\n# ... (rest of the code remains the same until predictions)\n\npredictions = normed_predictions * max_val\n```\n4. This change should improve the forecasting performance for time-series data with obvious trends. (D) 1. Replace the standard normalization method with max-value normalization in your code.\n2. Update the prediction denormalization step to multiply by the max value instead of adding the mean and multiplying by the standard deviation.\n3. The modified code should look like this:\n```python\nmax_val = seqs.max(dim=-1, keepdim=True)[0]\nnormed_seqs = seqs / max_val\n\n# ... (rest of the code remains the same until predictions)\n\npredictions = normed_predictions * max_val\n```\n4. This change should improve the forecasting performance for time-series data with obvious trends. (E) 1. Replace the standard normalization method with max-value normalization in your code.\n2. Modify the normalization step to use the maximum value of the sequence instead of mean and standard deviation.\n3. Update the prediction denormalization step to multiply by the max value instead of adding the mean and multiplying by the standard deviation.\n4. The modified code should look like this:\n```python\nmax_val = seqs.max(dim=-1, keepdim=True)[0]\nnormed_seqs = seqs / max_val\n\n# ... (rest of the code remains the same until predictions)\n\npredictions = normed_predictions * max_val\n```\n5. This change should improve the forecasting performance for time-series data with obvious trends. (F) 1. The modified code should look like this:\n```python\nmax_val = seqs.max(dim=-1, keepdim=True)[0]\nnormed_seqs = seqs / max_val\n\n# ... (rest of the code remains the same until predictions)\n\npredictions = normed_predictions * max_val\n```\n2. Replace the standard normalization method with max-value normalization in your code.\n3. Modify the normalization step to use the maximum value of the sequence instead of mean and standard deviation.\n4. Update the prediction denormalization step to multiply by the max value instead of adding the mean and multiplying by the standard deviation.\n5. This change should improve the forecasting performance for time-series data with obvious trends. (G) 1. Replace the standard normalization method with max-value normalization in your code.\n2. Modify the normalization step to use the maximum value of the sequence instead of mean and standard deviation.\n3. Ensure the max value is zero before normalization.\n4. Update the prediction denormalization step to multiply by the max value instead of adding the mean and multiplying by the standard deviation.\n5. The modified code should look like this:\n```python\nmax_val = seqs.max(dim=-1, keepdim=True)[0]\nnormed_seqs = seqs / max_val\n\n# ... (rest of the code remains the same until predictions)\n\npredictions = normed_predictions * max_val\n```\n6. This change should improve the forecasting performance for time-series data with obvious trends.", "answer": "E"}
{"uuid": "da7546e6-5547-4659-8c76-079906fa989f", "setup_instruct": "# Step-by-Step Execution Plan for Time-MoE\n\n## 1. **Installation**\n- **Description**: Install Python 3.10+ and project dependencies. Optionally install Flash-Attention for optimized performance.\n- **Commands**:\n  ```shell\n  pip install -r requirements.txt\n  ```\n  **Optional (Flash-Attention)**:\n  ```shell\n  pip install flash-attn==2.6.3\n  ```\n  or (with compilation optimization):\n  ```shell\n  pip install packaging ninja\n  MAX_JOBS=64 pip install flash-attn==2.6.3 --no-build-isolation\n  ```\n\n---\n\n## 2. **Dataset Setup**\n- **Description**: Download and prepare datasets for evaluation or fine-tuning.\n  - **Option 1**: Use Time-300B from Hugging Face.\n    ```python\n    from time_moe.datasets.time_moe_dataset import TimeMoEDataset\n    ds = TimeMoEDataset('Time-300B')\n    ```\n  - **Option 2**: Download benchmark datasets from [Google Drive](https://drive.google.com/drive/folders/1KjnAYr9X3D-jyJpo4yM7Giyq5V1Hga_7?usp=sharing) and place under `./dataset`.\n\n---\n\n## 3. **Forecasting with Pre-trained Models**\n- **Description**: Load a pre-trained Time-MoE model and generate forecasts.\n- **Commands**:\n  ```python\n  import torch\n  from transformers import AutoModelForCausalLM\n\n  model = AutoModelForCausalLM.from_pretrained(\n      'Maple728/TimeMoE-50M',\n      device_map=\"auto\",  # or \"cpu\"/\"cuda\"\n      trust_remote_code=True,\n  )\n  # With Flash-Attention:\n  # model = AutoModelForCausalLM.from_pretrained('Maple728/TimeMoE-50M', device_map=\"auto\", attn_implementation='flash_attention_2', trust_remote_code=True)\n\n  # Normalize input and generate predictions\n  normed_seqs = (seqs - mean) / std\n  predictions = model.generate(normed_seqs, max_new_tokens=prediction_length)\n  ```\n\n---\n\n## 4. **Evaluation**\n- **Description**: Evaluate the model on benchmark datasets (e.g., ETTh1).\n- **Command**:\n  ```shell\n  python run_eval.py -d dataset/ETT-small/ETTh1.csv -p 96\n  ```\n\n---\n\n## 5. **Fine-tuning**\n- **Description**: Fine-tune Time-MoE on custom datasets in `jsonl` format.\n- **Steps**:\n  1. **Prepare Dataset**: Convert data to `jsonl` format (example entry):\n     ```json\n     {\"sequence\": [1.0, 2.0, 3.0, ...]}\n     ```\n  2. **Training**:\n     - **CPU**:\n       ```shell\n       python main.py -d <data_path>\n       ```\n     - **Single/Multi-GPU**:\n       ```shell\n       python torch_dist_run.py main.py -d <data_path>\n       ```\n     - **Multi-Node**:\n       ```shell\n       export MASTER_ADDR=<master_addr> MASTER_PORT=<master_port> WORLD_SIZE=<world_size> RANK=<rank>\n       python torch_dist_run.py main.py -d <data_path>\n       ```\n     - **From Scratch**:\n       ```shell\n       python torch_dist_run.py main.py -d <data_path> --from_scratch\n       ```\n\n---\n\n## 6. **Citation**\n- **Description**: Cite the paper if using Time-MoE in research.\n- **BibTeX**:\n  ```bibtex\n  @misc{shi2024timemoe,\n    title={Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts},\n    author={Xiaoming Shi et al.},\n    year={2024},\n    eprint={2409.16040},\n    archivePrefix={arXiv},\n    url={https://arxiv.org/abs/2409.16040},\n  }\n  ```", "issue_title": "Unexpected Poor Forecast for Air Passengers", "issue_body": "Thank you for releasing the pretrained model! I was testing the model on some toy datasets and found that it generates an unexpectedly poor forecast for the Air Passengers dataset. I am not sure if I am using the model incorrectly. \r\n\r\n**Result**:\r\n![image](https://github.com/user-attachments/assets/5232eeed-3679-45ed-90e1-434853931dce)\r\n\r\n\r\n**Code**:\r\n\r\n```py\r\nimport torch\r\nimport pandas as pd\r\nfrom transformers import AutoModelForCausalLM\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\ndf = pd.read_csv(\r\n    \"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\"\r\n)\r\nseqs = torch.tensor(df[\"#Passengers\"].values).float().unsqueeze(0)\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"Maple728/TimeMoE-50M\",\r\n    device_map=\"cpu\",\r\n    trust_remote_code=True,\r\n)\r\nmean, std = seqs.mean(dim=-1, keepdim=True), seqs.std(dim=-1, keepdim=True)\r\nnormed_seqs = (seqs - mean) / std\r\n\r\nprediction_length = 24\r\noutput = model.generate(normed_seqs, max_new_tokens=prediction_length)\r\nnormed_predictions = output[:, -prediction_length:]\r\n\r\npredictions = normed_predictions * std + mean\r\n\r\nplt.figure(figsize=(8, 4))\r\n\r\nforecast_index = np.arange(prediction_length) + len(df[\"#Passengers\"])\r\nplt.plot(df[\"#Passengers\"], color=\"royalblue\", label=\"historical data\")\r\nplt.plot(forecast_index, predictions.squeeze(), color=\"tomato\", label=\"forecast\")\r\nplt.legend()\r\nplt.grid()\r\nplt.show()\r\n```", "choices": "(A) 1. Ensure you are using a single GPU by setting `os.environ['CUDA_VISIBLE_DEVICES']='0'` before running the script.\n2. Alternatively, modify the `fpt2_*` scripts to handle multi-GPU training correctly by transposing the `batch size` dimension to `dim=0` before passing `corr_x` to the model and transposing it back inside the model.\n3. For the `FPT2InfoTrainer` class, add `corr_x = corr_x.transpose(0, 1)` before passing `corr_x` to the model. (B) 1. Ensure you are using a single GPU by setting `os.environ['CUDA_VISIBLE_DEVICES']='0'` before running the script.\n2. Alternatively, modify the `fpt2_*` scripts to handle multi-GPU training correctly by transposing the `batch size` dimension to `dim=0` before passing `corr_x` to the model and transposing it back inside the model.\n3. For the `FPT2InfoTrainer` class, add `corr_x = corr_x.transpose(0, 1)` before passing `corr_x` to the model.\n4. In the `FPT2LMHeadModel` class, add `corr_x = corr_x.transpose(0, 1)` inside the `forward` method before processing `corr_x`. (C) 1. In the `FPT2LMHeadModel` class, add `corr_x = corr_x.transpose(0, 1)` inside the `forward` method before processing `corr_x`.\n2. Ensure you are using a single GPU by setting `os.environ['CUDA_VISIBLE_DEVICES']='0'` before running the script.\n3. Alternatively, modify the `fpt2_*` scripts to handle multi-GPU training correctly by transposing the `batch size` dimension to `dim=0` before passing `corr_x` to the model and transposing it back inside the model.\n4. For the `FPT2InfoTrainer` class, add `corr_x = corr_x.transpose(0, 1)` before passing `corr_x` to the model. (D) 1. Alternatively, modify the `fpt2_*` scripts to handle multi-GPU training correctly by transposing the `batch size` dimension to `dim=0` before passing `corr_x` to the model and transposing it back inside the model.\n2. For the `FPT2InfoTrainer` class, add `corr_x = corr_x.transpose(0, 1)` before passing `corr_x` to the model.\n3. In the `FPT2LMHeadModel` class, add `corr_x = corr_x.transpose(0, 1)` inside the `forward` method before processing `corr_x`. (E) 1. Ensure you are using a single GPU by setting `os.environ['CUDA_VISIBLE_DEVICES']='0'` before running the script.\n2. Disable GPU usage by setting `os.environ['CUDA_VISIBLE_DEVICES']='-1'` after setting the GPU.\n3. Alternatively, modify the `fpt2_*` scripts to handle multi-GPU training correctly by transposing the `batch size` dimension to `dim=0` before passing `corr_x` to the model and transposing it back inside the model.\n4. For the `FPT2InfoTrainer` class, add `corr_x = corr_x.transpose(0, 1)` before passing `corr_x` to the model.\n5. In the `FPT2LMHeadModel` class, add `corr_x = corr_x.transpose(0, 1)` inside the `forward` method before processing `corr_x`. (F) 1. For the `FPT2InfoTrainer` class, add `corr_x = corr_x.transpose(0, 1)` before passing `corr_x` to the model.\n2. Alternatively, modify the `fpt2_*` scripts to handle multi-GPU training correctly by transposing the `batch size` dimension to `dim=0` before passing `corr_x` to the model and transposing it back inside the model.\n3. Ensure you are using a single GPU by setting `os.environ['CUDA_VISIBLE_DEVICES']='0'` before running the script.\n4. In the `FPT2LMHeadModel` class, add `corr_x = corr_x.transpose(0, 1)` inside the `forward` method before processing `corr_x`.", "answer": "B"}
{"uuid": "2ccf82c5-e55a-4ee7-9879-533fba2c7831", "setup_instruct": "# D-FINE Project Execution Plan\n\n## 1. Environment Setup\n- **Description**: Create and activate a conda environment with Python 3.11.9 and install required dependencies.\n- **Commands**:\n  ```shell\n  conda create -n dfine python=3.11.9\n  conda activate dfine\n  pip install -r requirements.txt\n  ```\n\n## 2. Data Preparation\n### For COCO2017:\n- **Description**: Download COCO2017 dataset and update paths in `coco_detection.yml`.\n- **Steps**:\n  1. Download from [OpenDataLab](https://opendatalab.com/OpenDataLab/COCO_2017).\n  2. Modify paths in `configs/dataset/coco_detection.yml`:\n     ```yaml\n     train_dataloader:\n         img_folder: /data/COCO2017/train2017/\n         ann_file: /data/COCO2017/annotations/instances_train2017.json\n     val_dataloader:\n         img_folder: /data/COCO2017/val2017/\n         ann_file: /data/COCO2017/annotations/instances_val2017.json\n     ```\n\n### For Objects365:\n- **Description**: Download and preprocess Objects365 dataset.\n- **Commands**:\n  ```shell\n  export BASE_DIR=/data/Objects365/data\n  mkdir -p ${BASE_DIR}/train/images_from_val\n  cp -r ${BASE_DIR}/val/images/v1 ${BASE_DIR}/train/images_from_val/\n  cp -r ${BASE_DIR}/val/images/v2 ${BASE_DIR}/train/images_from_val/\n  python tools/remap_obj365.py --base_dir ${BASE_DIR}\n  python tools/resize_obj365.py --base_dir ${BASE_DIR}\n  ```\n- **Config Update**: Modify `obj365_detection.yml` with new paths.\n\n## 3. Training\n### For COCO2017:\n- **Description**: Train model on COCO2017 dataset.\n- **Commands**:\n  ```shell\n  export model=l  # Options: n, s, m, l, x\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0\n  ```\n\n### For Objects365 to COCO2017:\n- **Description**: Pretrain on Objects365 then fine-tune on COCO2017.\n- **Commands**:\n  ```shell\n  # Pretrain\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj365.yml --use-amp --seed=0\n  # Fine-tune\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth\n  ```\n\n## 4. Testing\n- **Description**: Evaluate model performance.\n- **Command**:\n  ```shell\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth\n  ```\n\n## 5. Deployment\n- **Description**: Export model to ONNX and TensorRT formats.\n- **Commands**:\n  ```shell\n  # Export ONNX\n  python tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n  # Export TensorRT\n  trtexec --onnx=\"model.onnx\" --saveEngine=\"model.engine\" --fp16\n  ```\n\n## 6. Inference\n- **Description**: Run inference on images/videos.\n- **Commands**:\n  ```shell\n  # ONNX Runtime\n  python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg\n  # TensorRT\n  python tools/inference/trt_inf.py --trt model.engine --input image.jpg\n  # PyTorch\n  python tools/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0\n  ```\n\n## 7. Benchmarking\n- **Description**: Measure model performance metrics.\n- **Commands**:\n  ```shell\n  # FLOPs/MACs/Params\n  python tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml\n  # Latency\n  python tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine\n  ```", "issue_title": "Fix crash when evaluating background-only images", "issue_body": "Fix crash when evaluating background-only images\r\n\r\ncommand: `python train.py -c configs/dfine/dfine_hgnetv2_n_coco.yml --test-only -r dfine_n_coco.pth`\r\n\r\n# Before \r\n\r\n<img width=\"1091\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c27ff9b7-4bc6-498c-acb7-dfbdabe731c2\" />\r\n\r\n# After\r\n\r\n![image](https://github.com/user-attachments/assets/0a944090-d6f2-43f7-8dc5-2c5c5d706a93)\r\n\r\n\r\nRemoved unnecessary conversion:\r\n\r\n```python\r\ntorch.tensor(target[\"boxes\"].tolist()).to(device),\r\n```\r\nsince this conversion is already performed before the iteration.\r\n\r\n```python\r\nfor samples, targets in metric_logger.log_every(data_loader, 10, header):\r\n        samples = samples.to(device)\r\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\r\n```", "choices": "(A) 1. Check the available RAM on your machine before running the export_onnx.py script.\n2. Modify the batch size in the export_onnx.py script to a lower value (e.g., 16 or 1) based on your machine's RAM capacity.\n3. Run the script again with the adjusted batch size to avoid out-of-memory errors. (B) 1. Modify the batch size in the export_onnx.py script to a lower value (e.g., 16 or 1) based on your machine's RAM capacity.\n2. Check the available RAM on your machine before running the export_onnx.py script.\n3. Run the script again with the adjusted batch size to avoid out-of-memory errors. (C) 1. Check the available RAM on your machine before running the export_onnx.py script.\n2. Modify the batch size in the export_onnx.py script to a lower value (e.g., 16 or 1) based on your machine's RAM capacity.\n3. Run the script again with the adjusted batch size to avoid out-of-memory errors.\n4. Delete the export_onnx.py script. (D) 1. Run the script again with the adjusted batch size to avoid out-of-memory errors.\n2. Check the available RAM on your machine before running the export_onnx.py script.\n3. Modify the batch size in the export_onnx.py script to a lower value (e.g., 16 or 1) based on your machine's RAM capacity. (E) 1. Check the available RAM on your machine before running the export_onnx.py script.\n2. Run the script again with the adjusted batch size to avoid out-of-memory errors. (F) 1. Modify the batch size in the export_onnx.py script to a lower value (e.g., 16 or 1) based on your machine's RAM capacity.\n2. Run the script again with the adjusted batch size to avoid out-of-memory errors. (G) 1. Check the available RAM on your machine before running the export_onnx.py script.\n2. Modify the batch size in the export_onnx.py script to a higher value (e.g., 64 or 128) based on your machine's RAM capacity.\n3. Run the script again with the adjusted batch size to avoid out-of-memory errors.", "answer": "A"}
{"uuid": "fbe847e9-98e4-4093-acd7-b0b2badac3f7", "setup_instruct": "# D-FINE Project Execution Plan\n\n## 1. Environment Setup\n- **Description**: Create and activate a conda environment with Python 3.11.9 and install required dependencies\n- **Command**:\n  ```shell\n  conda create -n dfine python=3.11.9\n  conda activate dfine\n  pip install -r requirements.txt\n  ```\n\n## 2. Data Preparation\n### For COCO2017:\n1. **Download Dataset**: From OpenDataLab or COCO official site\n2. **Modify Paths** in `coco_detection.yml`:\n   ```yaml\n   train_dataloader:\n       img_folder: /data/COCO2017/train2017/\n       ann_file: /data/COCO2017/annotations/instances_train2017.json\n   val_dataloader:\n       img_folder: /data/COCO2017/val2017/\n       ann_file: /data/COCO2017/annotations/instances_val2017.json\n   ```\n\n### For Objects365:\n1. **Download Dataset**: From OpenDataLab\n2. **Set Base Directory**:\n   ```shell\n   export BASE_DIR=/data/Objects365/data\n   ```\n3. **Organize Directory Structure** (as specified in README)\n4. **Run Preprocessing Scripts**:\n   ```shell\n   python tools/remap_obj365.py --base_dir ${BASE_DIR}\n   python tools/resize_obj365.py --base_dir ${BASE_DIR}\n   ```\n5. **Modify Paths** in `obj365_detection.yml`\n\n## 3. Training\n### For COCO2017:\n- **Command** (example for D-FINE-L):\n  ```shell\n  export model=l\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0\n  ```\n\n### For Objects365 to COCO2017:\n1. **Pre-train on Objects365**:\n   ```shell\n   CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj365.yml --use-amp --seed=0\n   ```\n2. **Fine-tune on COCO**:\n   ```shell\n   CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth\n   ```\n\n## 4. Testing\n- **Command**:\n  ```shell\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth\n  ```\n\n## 5. Deployment\n1. **Export to ONNX**:\n   ```shell\n   python tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n   ```\n2. **Export to TensorRT**:\n   ```shell\n   trtexec --onnx=\"model.onnx\" --saveEngine=\"model.engine\" --fp16\n   ```\n\n## 6. Inference\n- **ONNX Runtime**:\n  ```shell\n  python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg\n  ```\n- **TensorRT**:\n  ```shell\n  python tools/inference/trt_inf.py --trt model.engine --input image.jpg\n  ```\n- **PyTorch**:\n  ```shell\n  python tools/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0\n  ```\n\n## 7. Benchmarking\n1. **Model Metrics**:\n   ```shell\n   python tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml\n   ```\n2. **Latency Test**:\n   ```shell\n   python tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine\n   ```\n\n## 8. Visualization\n- **Fiftyone Visualization**:\n  ```shell\n  python tools/visualization/fiftyone_vis.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n  ```", "issue_title": "Convert to onnx failed:", "issue_body": "Hello, I trained a custom s model on my local gpu and I want to convert the best model into tensorrt. I tried to use the export_onnx.py program to do so and I am getting the following error:\r\n\r\n```\r\npython tools/deployment/export_onnx.py --check -c configs/dfine/custom/dfine_hgnetv2_s_custom.yml -r output/dfine_hgnetv2_s_custom/best_stg1.pth\r\n/home/user/D-FINE/tools/deployment/export_onnx.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  checkpoint = torch.load(args.resume, map_location='cpu')\r\nKilled\r\n```\r\n \r\n The program crashes at this particular block to be precise:\r\n\r\n```\r\n    torch.onnx.export(\r\n        model,\r\n        (data, size),\r\n        output_file,\r\n        input_names=['images', 'orig_target_sizes'],\r\n        output_names=['labels', 'boxes', 'scores'],\r\n        dynamic_axes=dynamic_axes,\r\n        opset_version=16,\r\n        verbose=False,\r\n        do_constant_folding=True,\r\n    )\r\n```\r\n\r\nAny help on how to fix this problem would be greatly appreciated. Thank you!", "choices": "(A) 1. Open the file `trt_inf.py`.\n2. Locate line 138.\n3. Change the line to `orig_size = torch.tensor([w, h], dtype=torch.int32)[None].to(device)`.\n4. Save the file and rerun the inference script.\n5. Revert the changes made to line 138. (B) 1. Open the file `trt_inf.py`.\n2. Save the file and rerun the inference script.\n3. Locate line 138.\n4. Change the line to `orig_size = torch.tensor([w, h], dtype=torch.int32)[None].to(device)`. (C) 1. Open the file `trt_inf.py`.\n2. Locate line 138.\n3. Change the line to `orig_size = torch.tensor([w, h], dtype=torch.int32)[None].to(device)`.\n4. Save the file and rerun the inference script. (D) 1. Open the file `trt_inf.py`.\n2. Locate line 138.\n3. Change the line to `orig_size = torch.tensor([w, h], dtype=torch.int32)[None].to(device)`. (E) 1. Open the file `trt_inf.py`.\n2. Locate line 138.\n3. Change the line to `orig_size = torch.tensor([w, h], dtype=torch.int32)[None].to(device)`.\n4. Save the file and rerun the inference script.\n5. Delete the file `trt_inf.py`. (F) 1. Open the file `trt_inf.py`.\n2. Save the file and rerun the inference script.\n3. Change the line to `orig_size = torch.tensor([w, h], dtype=torch.int32)[None].to(device)`.\n4. Locate line 138. (G) 1. Open the file `trt_inf.py`.\n2. Change the line to `orig_size = torch.tensor([w, h], dtype=torch.int32)[None].to(device)`.\n3. Save the file and rerun the inference script.", "answer": "C"}
{"uuid": "b548f145-8a11-4dd4-a4d5-618ac9201b63", "setup_instruct": "# D-FINE Project Execution Plan\n\n## 1. Environment Setup\n- **Description**: Create and activate a conda environment with Python 3.11.9 and install required dependencies.\n- **Commands**:\n  ```shell\n  conda create -n dfine python=3.11.9\n  conda activate dfine\n  pip install -r requirements.txt\n  ```\n\n## 2. Data Preparation\n### Option A: COCO2017\n- **Description**: Download and configure COCO2017 dataset.\n- **Steps**:\n  1. Download from [OpenDataLab](https://opendatalab.com/OpenDataLab/COCO_2017) or [COCO](https://cocodataset.org/#download).\n  2. Modify paths in `configs/dataset/coco_detection.yml`:\n     ```yaml\n     train_dataloader:\n         img_folder: /data/COCO2017/train2017/\n         ann_file: /data/COCO2017/annotations/instances_train2017.json\n     val_dataloader:\n         img_folder: /data/COCO2017/val2017/\n         ann_file: /data/COCO2017/annotations/instances_val2017.json\n     ```\n\n### Option B: Objects365\n- **Description**: Download and preprocess Objects365 dataset.\n- **Commands**:\n  ```shell\n  export BASE_DIR=/data/Objects365/data\n  mkdir -p ${BASE_DIR}/train/images_from_val\n  cp -r ${BASE_DIR}/val/images/v1 ${BASE_DIR}/train/images_from_val/\n  cp -r ${BASE_DIR}/val/images/v2 ${BASE_DIR}/train/images_from_val/\n  python tools/remap_obj365.py --base_dir ${BASE_DIR}\n  python tools/resize_obj365.py --base_dir ${BASE_DIR}\n  ```\n- Update `configs/dataset/obj365_detection.yml` with paths.\n\n## 3. Training\n### COCO2017 Training\n- **Description**: Train a selected model variant on COCO2017.\n- **Commands**:\n  ```shell\n  export model=l  # Options: n, s, m, l, x\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0\n  ```\n\n### Objects365 to COCO Finetuning\n- **Description**: Pretrain on Objects365 and finetune on COCO.\n- **Commands**:\n  ```shell\n  # Pretrain\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj365.yml --use-amp --seed=0\n  # Finetune\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth\n  ```\n\n## 4. Testing\n- **Description**: Evaluate model performance.\n- **Command**:\n  ```shell\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth\n  ```\n\n## 5. Deployment\n### ONNX Export\n- **Description**: Convert model to ONNX format.\n- **Commands**:\n  ```shell\n  pip install onnx onnxsim\n  python tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n  ```\n\n### TensorRT Conversion\n- **Description**: Optimize model with TensorRT.\n- **Command**:\n  ```shell\n  trtexec --onnx=\"model.onnx\" --saveEngine=\"model.engine\" --fp16\n  ```\n\n## 6. Inference\n- **Description**: Run inference on images/videos.\n- **Commands**:\n  ```shell\n  # ONNX Runtime\n  python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg\n  # TensorRT\n  python tools/inference/trt_inf.py --trt model.engine --input image.jpg\n  # PyTorch\n  python tools/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0\n  ```\n\n## 7. Benchmarking\n- **Description**: Measure model performance metrics.\n- **Commands**:\n  ```shell\n  # FLOPs/MACs/Params\n  python tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml\n  # Latency\n  python tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine\n  ```\n\n## 8. Visualization\n- **Description**: Visualize results with FiftyOne.\n- **Command**:\n  ```shell\n  pip install fiftyone\n  python tools/visualization/fiftyone_vis.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n  ```", "issue_title": "trt_inf.py tensorrt推理加速出现orig_target_sizes不匹配的错误无法成功推理", "issue_body": "1. 我在训练的时候，将图像大小由640改为了1280，修改了图像大小相关的适配为1280,其他的没改,成功训练完毕\r\n2. 训练好之后保存为best_stg2.pth，然后用官方的代码和命令转成best_stg2.onnx\r\n3. 再将onnx用官方的命令转成engine文件，trtexec --onnx=\"best_stg2.onnx\" --saveEngine=\"best_stg2.engine\" --fp16\r\n4. 最后运行官方的trt推理文件python tools/inference/trt_inf.py --trt best_stg2.engine --input image.jpg \r\n5. 结果就报错了，如下错误，Traceback (most recent call last):\r\n  File \"/localdata/lj/code/D-FINE/tools/inference/trt_inf.py\", line 226, in <module>\r\n    process_image(m, file_path, args.device)\r\n  File \"/localdata/lj/code/D-FINE/tools/inference/trt_inf.py\", line 151, in process_image\r\n    output = m(blob)\r\n  File \"/localdata/lj/code/D-FINE/tools/inference/trt_inf.py\", line 109, in __call__\r\n    return self.run_torch(blob)\r\n  File \"/localdata/lj/code/D-FINE/tools/inference/trt_inf.py\", line 99, in run_torch\r\n    assert self.bindings[n].data.dtype == blob[n].dtype, '{} dtype mismatch'.format(n)\r\nAssertionError: orig_target_sizes dtype mismatch\r\n6. 我用tools/inference/torch_inf.py推理是成功的，而且效果相当不错（感谢作者们的工作）", "choices": "(A) 1. Ensure you are using the correct version of PyTorch (e.g., torch==2.2.2, torchvision==0.17.2, torchaudio==2.2.2).\n2. When converting ONNX to TensorRT engine, use `opset=17` for exporting the ONNX model.\n3. Set all layers to FP16 precision during TensorRT engine conversion.\n4. During the TensorRT engine conversion, set the precision for NORMALIZATION layers to FP32 while keeping other layers in FP16. This can be done by iterating through the network layers and setting the precision for NORMALIZATION layers to FP32.\n5. Compile the engine with the modified precision settings to ensure accuracy is maintained while using FP16 for other layers. (B) 1. Ensure you are using the correct version of PyTorch (e.g., torch==2.2.2, torchvision==0.17.2, torchaudio==2.2.2).\n2. During the TensorRT engine conversion, set the precision for NORMALIZATION layers to FP32 while keeping other layers in FP16. This can be done by iterating through the network layers and setting the precision for NORMALIZATION layers to FP32.\n3. When converting ONNX to TensorRT engine, use `opset=17` for exporting the ONNX model.\n4. Compile the engine with the modified precision settings to ensure accuracy is maintained while using FP16 for other layers. (C) 1. Ensure you are using the correct version of PyTorch (e.g., torch==2.2.2, torchvision==0.17.2, torchaudio==2.2.2).\n2. When converting ONNX to TensorRT engine, use `opset=10` for exporting the ONNX model.\n3. When converting ONNX to TensorRT engine, use `opset=17` for exporting the ONNX model.\n4. During the TensorRT engine conversion, set the precision for NORMALIZATION layers to FP32 while keeping other layers in FP16. This can be done by iterating through the network layers and setting the precision for NORMALIZATION layers to FP32.\n5. Compile the engine with the modified precision settings to ensure accuracy is maintained while using FP16 for other layers. (D) 1. Ensure you are using the correct version of PyTorch (e.g., torch==2.2.2, torchvision==0.17.2, torchaudio==2.2.2).\n2. When converting ONNX to TensorRT engine, use `opset=17` for exporting the ONNX model.\n3. During the TensorRT engine conversion, set the precision for NORMALIZATION layers to FP32 while keeping other layers in FP16. This can be done by iterating through the network layers and setting the precision for NORMALIZATION layers to FP32.\n4. Compile the engine with the modified precision settings to ensure accuracy is maintained while using FP16 for other layers. (E) 1. Ensure you are using the correct version of PyTorch (e.g., torch==2.2.2, torchvision==0.17.2, torchaudio==2.2.2).\n2. When converting ONNX to TensorRT engine, use `opset=17` for exporting the ONNX model.\n3. Compile the engine with the modified precision settings to ensure accuracy is maintained while using FP16 for other layers.\n4. During the TensorRT engine conversion, set the precision for NORMALIZATION layers to FP32 while keeping other layers in FP16. This can be done by iterating through the network layers and setting the precision for NORMALIZATION layers to FP32. (F) 1. Ensure you are using the correct version of PyTorch (e.g., torch==2.2.2, torchvision==0.17.2, torchaudio==2.2.2).\n2. During the TensorRT engine conversion, set the precision for NORMALIZATION layers to FP32 while keeping other layers in FP16. This can be done by iterating through the network layers and setting the precision for NORMALIZATION layers to FP32.\n3. Compile the engine with the modified precision settings to ensure accuracy is maintained while using FP16 for other layers. (G) 1. Ensure you are using the correct version of PyTorch (e.g., torch==2.2.2, torchvision==0.17.2, torchaudio==2.2.2).\n2. When converting ONNX to TensorRT engine, use `opset=17` for exporting the ONNX model.\n3. Compile the engine with the modified precision settings to ensure accuracy is maintained while using FP16 for other layers.", "answer": "D"}
{"uuid": "c2676333-247d-4dd2-8b16-fcb3ed6dd26c", "setup_instruct": "# D-FINE Project Execution Plan\n\n## 1. Environment Setup\n- **Description**: Create and activate a conda environment with Python 3.11.9 and install required packages.\n- **Command**:\n  ```shell\n  conda create -n dfine python=3.11.9\n  conda activate dfine\n  pip install -r requirements.txt\n  ```\n\n## 2. Data Preparation\n### COCO2017 Dataset\n- **Description**: Download and configure COCO2017 dataset.\n- **Steps**:\n  1. Download from [OpenDataLab](https://opendatalab.com/OpenDataLab/COCO_2017) or [COCO](https://cocodataset.org/#download).\n  2. Modify paths in `configs/dataset/coco_detection.yml`:\n     ```yaml\n     train_dataloader:\n         img_folder: /data/COCO2017/train2017/\n         ann_file: /data/COCO2017/annotations/instances_train2017.json\n     val_dataloader:\n         img_folder: /data/COCO2017/val2017/\n         ann_file: /data/COCO2017/annotations/instances_val2017.json\n     ```\n\n### Objects365 Dataset\n- **Description**: Download and configure Objects365 dataset.\n- **Steps**:\n  1. Download from [OpenDataLab](https://opendatalab.com/OpenDataLab/Objects365).\n  2. Set base directory:\n     ```shell\n     export BASE_DIR=/data/Objects365/data\n     ```\n  3. Organize files and run preprocessing scripts:\n     ```shell\n     mkdir -p ${BASE_DIR}/train/images_from_val\n     cp -r ${BASE_DIR}/val/images/v1 ${BASE_DIR}/train/images_from_val/\n     cp -r ${BASE_DIR}/val/images/v2 ${BASE_DIR}/train/images_from_val/\n     python tools/remap_obj365.py --base_dir ${BASE_DIR}\n     python tools/resize_obj365.py --base_dir ${BASE_DIR}\n     ```\n  4. Modify paths in `configs/dataset/obj365_detection.yml`.\n\n## 3. Training\n### COCO2017 Training\n- **Description**: Train model on COCO2017 dataset.\n- **Command**:\n  ```shell\n  export model=l  # n, s, m, l, x\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0\n  ```\n\n### Objects365 to COCO2017 Training\n- **Description**: Pretrain on Objects365 then fine-tune on COCO2017.\n- **Commands**:\n  ```shell\n  # Pretrain on Objects365\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj365.yml --use-amp --seed=0\n  \n  # Fine-tune on COCO2017\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth\n  ```\n\n## 4. Testing\n- **Description**: Evaluate model performance.\n- **Command**:\n  ```shell\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth\n  ```\n\n## 5. Deployment\n- **Description**: Export model to ONNX and TensorRT formats.\n- **Commands**:\n  ```shell\n  # Export to ONNX\n  python tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n  \n  # Convert to TensorRT\n  trtexec --onnx=\"model.onnx\" --saveEngine=\"model.engine\" --fp16\n  ```\n\n## 6. Inference\n- **Description**: Run inference on images/videos using different backends.\n- **Commands**:\n  ```shell\n  # ONNX Runtime\n  python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg\n  \n  # TensorRT\n  python tools/inference/trt_inf.py --trt model.engine --input image.jpg\n  \n  # PyTorch\n  python tools/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0\n  ```\n\n## 7. Benchmarking\n- **Description**: Measure model performance metrics.\n- **Commands**:\n  ```shell\n  # FLOPs/MACs/Params\n  python tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml\n  \n  # TensorRT Latency\n  python tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine\n  ```", "issue_title": "训练新的类别", "issue_body": "请问我要如何训练既不在obj365也不在coco数据集里面的类别呢", "choices": "(A) 1. Modify the `train_one_epoch` function to detect and skip batches where any rank has empty labels by adding the following code:\n   ```python\n   local_empty = torch.tensor(\n       int(all(len(t['boxes']) == 0 for t in targets)),\n       device=samples.device\n   )\n   dist.all_reduce(local_empty, op=dist.ReduceOp.SUM)\n   if local_empty.item() > 0:\n       if dist.get_rank() == 0:\n           print(f\"[Rank 0] Epoch {epoch}, iter {i}: Skipping batch with empty labels\")\n       continue\n   ```\n2. Set `find_unused_parameters=True` in the DDP configuration to handle cases where some parameters do not receive gradients. (B) 1. Set `find_unused_parameters=True` in the DDP configuration to handle cases where some parameters do not receive gradients.\n2. Ensure that the dataset does not contain batches where all samples have no labels (empty targets).\n3. Modify the `train_one_epoch` function to detect and skip batches where any rank has empty labels by adding the following code:\n   ```python\n   local_empty = torch.tensor(\n       int(all(len(t['boxes']) == 0 for t in targets)),\n       device=samples.device\n   )\n   dist.all_reduce(local_empty, op=dist.ReduceOp.SUM)\n   if local_empty.item() > 0:\n       if dist.get_rank() == 0:\n           print(f\"[Rank 0] Epoch {epoch}, iter {i}: Skipping batch with empty labels\")\n       continue\n   ```\n4. Verify that GPU memory is sufficient by checking `nvitop` during training to ensure no memory overflow occurs. (C) 1. Ensure that the dataset does not contain batches where all samples have no labels (empty targets).\n2. Modify the `train_one_epoch` function to detect and skip batches where any rank has empty labels by adding the following code:\n   ```python\n   local_empty = torch.tensor(\n       int(all(len(t['boxes']) == 0 for t in targets)),\n       device=samples.device\n   )\n   dist.all_reduce(local_empty, op=dist.ReduceOp.SUM)\n   if local_empty.item() > 0:\n       if dist.get_rank() == 0:\n           print(f\"[Rank 0] Epoch {epoch}, iter {i}: Skipping batch with empty labels\")\n       continue\n   ```\n3. Set `find_unused_parameters=True` in the DDP configuration to handle cases where some parameters do not receive gradients.\n4. Verify that GPU memory is sufficient by checking `nvitop` during training to ensure no memory overflow occurs. (D) 1. Ensure that the dataset does not contain batches where all samples have no labels (empty targets).\n2. Modify the `train_one_epoch` function to detect and skip batches where any rank has empty labels by adding the following code:\n   ```python\n   local_empty = torch.tensor(\n       int(all(len(t['boxes']) == 0 for t in targets)),\n       device=samples.device\n   )\n   dist.all_reduce(local_empty, op=dist.ReduceOp.SUM)\n   if local_empty.item() > 0:\n       if dist.get_rank() == 0:\n           print(f\"[Rank 0] Epoch {epoch}, iter {i}: Skipping batch with empty labels\")\n       continue\n   ```\n3. Disable gradient computation for all parameters by adding `torch.set_grad_enabled(False)`.\n4. Set `find_unused_parameters=True` in the DDP configuration to handle cases where some parameters do not receive gradients.\n5. Verify that GPU memory is sufficient by checking `nvitop` during training to ensure no memory overflow occurs.", "answer": "C"}
{"uuid": "2b602b98-75b8-4a44-b492-7538d0dd111e", "setup_instruct": "# D-FINE Project Execution Plan\n\n## 1. Environment Setup\n```shell\nconda create -n dfine python=3.11.9\nconda activate dfine\npip install -r requirements.txt\n```\n\n## 2. Data Preparation\n### For COCO2017:\n1. Download dataset from OpenDataLab/COCO\n2. Modify paths in `configs/dataset/coco_detection.yml`:\n```yaml\ntrain_dataloader:\n    img_folder: /data/COCO2017/train2017/\n    ann_file: /data/COCO2017/annotations/instances_train2017.json\nval_dataloader:\n    img_folder: /data/COCO2017/val2017/\n    ann_file: /data/COCO2017/annotations/instances_val2017.json\n```\n\n### For Objects365:\n1. Download dataset from OpenDataLab\n2. Run preprocessing scripts:\n```shell\nexport BASE_DIR=/data/Objects365/data\npython tools/remap_obj365.py --base_dir ${BASE_DIR}\npython tools/resize_obj365.py --base_dir ${BASE_DIR}\n```\n\n## 3. Training Execution\n### COCO2017 Training:\n```shell\nexport model=l  # n/s/m/l/x\nCUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0\n```\n\n### Objects365 to COCO Finetuning:\n```shell\nCUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth\n```\n\n## 4. Model Evaluation\n```shell\nCUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth\n```\n\n## 5. Deployment\n### ONNX Export:\n```shell\npython tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n```\n\n### TensorRT Conversion:\n```shell\ntrtexec --onnx=\"model.onnx\" --saveEngine=\"model.engine\" --fp16\n```\n\n## 6. Inference\n### Visual Inference:\n```shell\npython tools/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0\n```\n\n## 7. Benchmarking\n### Performance Metrics:\n```shell\npython tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml\n```\n\n### Latency Test:\n```shell\npython tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine\n```", "issue_title": "Faster coco eval support", "issue_body": "Hello, I suggest to include the fast validation library \"faster-coco-eval\" for validating coco & lvis datasets.\r\n\r\nThe guys from lightning.ai have been using it for a long time as an alternative backend for validation, instead of the dying pycocotools.\r\n\r\nhttps://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html\r\n\r\nThe guys from ultralytics are also preparing to accept my changes, I think they will be useful for you too!\r\n\r\nI wrote a simple integrator of this library for you.\r\nI have also prepared examples of this library in advance for you, they are available here:\r\n\r\nhttps://github.com/MiXaiLL76/faster_coco_eval/blob/main/examples/comparison/ultralytics/colab_example.ipynb\r\nor\r\nhttps://nbviewer.org/github/MiXaiLL76/faster_coco_eval/blob/main/examples/comparison/ultralytics/colab_example.ipynb\r\n\r\nFor quick comparison, here is a table from the end of this document:\r\n\r\n### ultralytics eval compare considering data loading\r\n\r\n| lib | model | time | profit | \r\n| ---- | ----- | ---- | ------ |\r\n| faster-coco-eval  | bbox | 7.34 | 4.5x |\r\n| pycocotools  | bbox | 34.35 | 1x |\r\n| ---- | ----- | ---- | ------ |\r\n| faster-coco-eval  | bbox+segm | 26.51 | 3.2x |\r\n| pycocotools  | bbox+segm | 86.79 | 1x |\r\n| ---- | ----- | ---- | ------ |\r\n| faster-coco-eval  | bbox+keypoints | 4.45 | 2,2x |\r\n| pycocotools  | bbox+keypoints | 10.17 | 1x |\r\n\r\n\r\nI understand that you have a legacy, fast-coco-eval. But it is not convenient to use and requires additional steps to install. My library, took the best of pycocotools & detectron2 fast-coco-eval", "choices": "(A) 1. Ensure the same configuration file is used for both training and inference.  \n2. Specifically, verify that the number of classes in the configuration file matches between training (16 classes) and inference (default was 777 classes).  \n3. Delete the configuration file if the class counts do not match.  \n4. If the configuration file was modified, revert it to the original version used during training to resolve the size mismatch error. (B) 1. Ensure the same configuration file is used for both training and inference.  \n2. If the configuration file was modified, revert it to the original version used during training to resolve the size mismatch error. (C) 1. Ensure the same configuration file is used for both training and inference.  \n2. If the configuration file was modified, revert it to the original version used during training to resolve the size mismatch error.  \n3. Specifically, verify that the number of classes in the configuration file matches between training (16 classes) and inference (default was 777 classes). (D) 1. Specifically, verify that the number of classes in the configuration file matches between training (16 classes) and inference (default was 777 classes).  \n2. Ensure the same configuration file is used for both training and inference.  \n3. If the configuration file was modified, revert it to the original version used during training to resolve the size mismatch error. (E) 1. Ensure the same configuration file is used for both training and inference.  \n2. Specifically, verify that the number of classes in the configuration file matches between training (16 classes) and inference (default was 777 classes). (F) 1. Ensure the same configuration file is used for both training and inference.  \n2. Specifically, verify that the number of classes in the configuration file matches between training (16 classes) and inference (default was 777 classes).  \n3. Manually set the number of classes to 777 in the configuration file during inference.  \n4. If the configuration file was modified, revert it to the original version used during training to resolve the size mismatch error. (G) 1. Ensure the same configuration file is used for both training and inference.\n2. Specifically, verify that the number of classes in the configuration file matches between training (16 classes) and inference (default was 777 classes).\n3. If the configuration file was modified, revert it to the original version used during training to resolve the size mismatch error.", "answer": "G"}
{"uuid": "b6e1299b-9599-46cf-bef8-473e2287c17a", "setup_instruct": "# D-FINE Project Execution Plan\n\n## 1. Environment Setup\n- **Description**: Create and activate a conda environment with Python 3.11.9\n- **Command**:\n  ```shell\n  conda create -n dfine python=3.11.9\n  conda activate dfine\n  pip install -r requirements.txt\n  ```\n\n## 2. Data Preparation\n### Option A: COCO2017\n- **Description**: Download and configure COCO2017 dataset\n- **Steps**:\n  1. Download from OpenDataLab or COCO website\n  2. Modify paths in `configs/dataset/coco_detection.yml`\n\n### Option B: Objects365\n- **Description**: Download and process Objects365 dataset\n- **Commands**:\n  ```shell\n  export BASE_DIR=/data/Objects365/data\n  mkdir -p ${BASE_DIR}/train/images_from_val\n  cp -r ${BASE_DIR}/val/images/v1 ${BASE_DIR}/train/images_from_val/\n  cp -r ${BASE_DIR}/val/images/v2 ${BASE_DIR}/train/images_from_val/\n  python tools/remap_obj365.py --base_dir ${BASE_DIR}\n  python tools/resize_obj365.py --base_dir ${BASE_DIR}\n  ```\n\n## 3. Training\n### COCO2017 Training\n- **Description**: Train model on COCO2017 dataset\n- **Command**:\n  ```shell\n  export model=l  # n/s/m/l/x\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0\n  ```\n\n### Objects365 to COCO Transfer\n- **Description**: Pretrain on Objects365 then fine-tune on COCO\n- **Commands**:\n  ```shell\n  # Pretrain\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj365.yml --use-amp --seed=0\n  \n  # Fine-tune\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth\n  ```\n\n## 4. Evaluation\n- **Description**: Test model performance\n- **Command**:\n  ```shell\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth\n  ```\n\n## 5. Deployment\n- **Description**: Export model to ONNX and TensorRT\n- **Commands**:\n  ```shell\n  pip install onnx onnxsim\n  python tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n  trtexec --onnx=\"model.onnx\" --saveEngine=\"model.engine\" --fp16\n  ```\n\n## 6. Inference\n- **Description**: Run inference on images/videos\n- **Commands**:\n  ```shell\n  # ONNX\n  python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg\n  \n  # TensorRT\n  python tools/inference/trt_inf.py --trt model.engine --input image.jpg\n  \n  # PyTorch\n  python tools/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0\n  ```\n\n## 7. Benchmarking\n- **Description**: Measure model performance metrics\n- **Commands**:\n  ```shell\n  # Model stats\n  python tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml\n  \n  # Latency\n  python tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine\n  ```", "issue_title": "CPU跑满 和GPU偷懒", "issue_body": "![image](https://github.com/user-attachments/assets/4ef3aee1-f7f1-4754-8ef0-8d54f6a32fb3)\r\n我跑的原始图片大小是1280x1024 .   resize什么的都是默认设置 应该都是640 640\r\n按照教程和Issues里的一些问题，设置了这些参数，其他都默认\r\n![image](https://github.com/user-attachments/assets/a0dbf4bc-c079-41c7-bd84-5ccb75e8b864)\r\nhttps://github.com/Peterande/D-FINE/issues/13#issuecomment-2446879522\r\nhttps://github.com/Peterande/D-FINE/issues/5#issuecomment-2428200244\r\n\r\n`total_batch_size:16`\r\n\r\n`num_classes: 1`\r\n\r\n还有个问题 跑出来这个曲线 看起来也太诡异了啊 起伏这么大的吗 是不是哪边参数设置错了\r\n\r\n![image](https://github.com/user-attachments/assets/6f927edd-94f2-4554-bbe9-8378c1abf70b)", "choices": "(A) 1. Modify the `train.py` file to manually set up the distributed environment for single-GPU training on Windows. Replace the distributed setup code with the following:\n```python\ndef main(args, ) -> None:\n    \"\"\"main\n    \"\"\"\n    torch.cuda.set_device(\"cuda:0\")\n    ...\n```\n2. Alternatively, if you need to keep the distributed training setup, modify the `dist_utils.py` file to explicitly disable libuv by changing the initialization method from `init_method=\"env://\"` to `init_method=\"env://?use_libuv=False\"`.\n3. Ensure you are using a compatible version of PyTorch (2.4.1 was confirmed to work with this solution). (B) 1. Modify the `train.py` file to manually set up the distributed environment for single-GPU training on Windows. Replace the distributed setup code with the following:\n```python\ndef main(args, ) -> None:\n    \"\"\"main\n    \"\"\"\n    torch.cuda.set_device(\"cuda:0\")\n    ...\n```\n2. Downgrade PyTorch to version 1.9.0 (known to be incompatible with this solution).\n3. Alternatively, if you need to keep the distributed training setup, modify the `dist_utils.py` file to explicitly disable libuv by changing the initialization method from `init_method=\"env://\"` to `init_method=\"env://?use_libuv=False\"`.\n4. Ensure you are using a compatible version of PyTorch (2.4.1 was confirmed to work with this solution). (C) 1. Modify the `train.py` file to manually set up the distributed environment for single-GPU training on Windows. Replace the distributed setup code with the following:\n```python\ndef main(args, ) -> None:\n    \"\"\"main\n    \"\"\"\n    torch.cuda.set_device(\"cuda:0\")\n    ...\n```\n2. Modify the `dist_utils.py` file to explicitly enable libuv by changing the initialization method from `init_method=\"env://\"` to `init_method=\"env://?use_libuv=True\"`.\n3. Ensure you are using a compatible version of PyTorch (2.4.1 was confirmed to work with this solution). (D) 1. Ensure you are using a compatible version of PyTorch (2.4.1 was confirmed to work with this solution).\n2. Modify the `train.py` file to manually set up the distributed environment for single-GPU training on Windows. Replace the distributed setup code with the following:\n```python\ndef main(args, ) -> None:\n    \"\"\"main\n    \"\"\"\n    torch.cuda.set_device(\"cuda:0\")\n    ...\n```\n3. Alternatively, if you need to keep the distributed training setup, modify the `dist_utils.py` file to explicitly disable libuv by changing the initialization method from `init_method=\"env://\"` to `init_method=\"env://?use_libuv=False\"`. (E) 1. Modify the `train.py` file to manually set up the distributed environment for single-GPU training on Windows. Replace the distributed setup code with the following:\n```python\ndef main(args, ) -> None:\n    \"\"\"main\n    \"\"\"\n    torch.cuda.set_device(\"cuda:0\")\n    ...\n```\n2. Ensure you are using a compatible version of PyTorch (2.4.1 was confirmed to work with this solution). (F) 1. Alternatively, if you need to keep the distributed training setup, modify the `dist_utils.py` file to explicitly disable libuv by changing the initialization method from `init_method=\"env://\"` to `init_method=\"env://?use_libuv=False\"`.\n2. Ensure you are using a compatible version of PyTorch (2.4.1 was confirmed to work with this solution).", "answer": "A"}
{"uuid": "a2fcf559-9744-42ea-9593-6be20300e3de", "setup_instruct": "# D-FINE Execution Plan\n\n## 1. Environment Setup\n- **Description**: Create and activate a conda environment with Python 3.11.9 and install required dependencies.\n- **Command**:\n  ```shell\n  conda create -n dfine python=3.11.9\n  conda activate dfine\n  pip install -r requirements.txt\n  ```\n\n## 2. Data Preparation\n### COCO2017\n- **Description**: Download and configure COCO2017 dataset paths.\n- **Steps**:\n  1. Download from [OpenDataLab](https://opendatalab.com/OpenDataLab/COCO_2017).\n  2. Update paths in `configs/dataset/coco_detection.yml`:\n     ```yaml\n     train_dataloader:\n         img_folder: /data/COCO2017/train2017/\n         ann_file: /data/COCO2017/annotations/instances_train2017.json\n     val_dataloader:\n         img_folder: /data/COCO2017/val2017/\n         ann_file: /data/COCO2017/annotations/instances_val2017.json\n     ```\n\n### Objects365\n- **Description**: Download and preprocess Objects365 dataset.\n- **Commands**:\n  ```shell\n  export BASE_DIR=/data/Objects365/data\n  mkdir -p ${BASE_DIR}/train/images_from_val\n  cp -r ${BASE_DIR}/val/images/v1 ${BASE_DIR}/train/images_from_val/\n  cp -r ${BASE_DIR}/val/images/v2 ${BASE_DIR}/train/images_from_val/\n  python tools/remap_obj365.py --base_dir ${BASE_DIR}\n  python tools/resize_obj365.py --base_dir ${BASE_DIR}\n  ```\n- **Config Update**: Modify `configs/dataset/obj365_detection.yml` with new paths.\n\n## 3. Training\n### COCO2017\n- **Description**: Train a selected model (e.g., D-FINE-L) on COCO2017.\n- **Command**:\n  ```shell\n  export model=l\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0\n  ```\n\n### Objects365 to COCO2017\n- **Description**: Pretrain on Objects365 and fine-tune on COCO2017.\n- **Commands**:\n  ```shell\n  # Pretrain\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj365.yml --use-amp --seed=0\n  # Fine-tune\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth\n  ```\n\n## 4. Testing\n- **Description**: Evaluate the trained model.\n- **Command**:\n  ```shell\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth\n  ```\n\n## 5. Deployment\n### ONNX Export\n- **Description**: Convert the model to ONNX format.\n- **Command**:\n  ```shell\n  pip install onnx onnxsim\n  python tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n  ```\n\n### TensorRT Export\n- **Description**: Optimize the ONNX model with TensorRT.\n- **Command**:\n  ```shell\n  trtexec --onnx=\"model.onnx\" --saveEngine=\"model.engine\" --fp16\n  ```\n\n## 6. Inference\n- **Description**: Run inference on images/videos using ONNX/TensorRT/PyTorch.\n- **Commands**:\n  ```shell\n  # ONNX\n  python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg\n  # TensorRT\n  python tools/inference/trt_inf.py --trt model.engine --input image.jpg\n  # PyTorch\n  python tools/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0\n  ```\n\n## 7. Benchmarking\n- **Description**: Measure model performance metrics.\n- **Commands**:\n  ```shell\n  # FLOPs/MACs/Params\n  python tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml\n  # Latency\n  python tools/benchmark/trt_benchmark.py --COCO_dir /path/to/COCO2017 --engine_dir model.engine\n  ```\n\n## 8. Visualization\n- **Description**: Visualize results using FiftyOne.\n- **Command**:\n  ```shell\n  pip install fiftyone\n  python tools/visualization/fiftyone_vis.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n  ```", "issue_title": "windows下训练报错", "issue_body": "请尽快推出windows下的训练教程。\r\n\r\n```\r\n(py310) D:\\pyworkspaces\\D-FINE>torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs\\dfine\\dfine_hgnetv2_l_coco.yml --use-amp --seed=0 -t model.pth\r\nW1101 18:25:56.674000 27360 torch\\distributed\\elastic\\multiprocessing\\redirects.py:28] NOTE: Redirects are currently not supported in Windows or MacOs.\r\nW1101 18:25:56.746000 27360 torch\\distributed\\run.py:779]\r\nW1101 18:25:56.746000 27360 torch\\distributed\\run.py:779] *****************************************\r\nW1101 18:25:56.746000 27360 torch\\distributed\\run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your systembeing overloaded, please further tune the variable for optimal performance in your application as needed.\r\nW1101 18:25:56.746000 27360 torch\\distributed\\run.py:779] *****************************************\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\D:\\ProgramData\\anaconda3\\envs\\py310\\Scripts\\torchrun-script.py\", line 33, in <module>\r\n    sys.exit(load_entry_point('torch==2.4.1', 'console_scripts', 'torchrun')())\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\errors\\__init__.py\", line 348, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\run.py\", line 901, in main\r\n    run(args)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\run.py\", line 892, in run\r\n    elastic_launch(\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\launcher\\api.py\", line 133, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\launcher\\api.py\", line 255, in launch_agent\r\n    result = agent.run()\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\metrics\\api.py\", line 124, in wrapper\r\n    result = f(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\agent\\server\\api.py\", line 680, in run\r\n    result = self._invoke_run(role)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\agent\\server\\api.py\", line 829, in _invoke_run\r\n    self._initialize_workers(self._worker_group)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\metrics\\api.py\", line 124, in wrapper\r\n    result = f(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\agent\\server\\api.py\", line 652, in _initialize_workers\r\n    self._rendezvous(worker_group)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\metrics\\api.py\", line 124, in wrapper\r\n    result = f(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\agent\\server\\api.py\", line 489, in _rendezvous\r\n    rdzv_info = spec.rdzv_handler.next_rendezvous()\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\rendezvous\\static_tcp_rendezvous.py\", line 66, in next_rendezvous\r\n    self._store = TCPStore(  # type: ignore[call-arg]\r\nRuntimeError: use_libuv was requested but PyTorch was build without libuv support\r\n\r\n```(py310) D:\\pyworkspaces\\D-FINE>torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs\\dfine\\dfine_hgnetv2_l_coco.yml --use-amp --seed=0 -t model.pth\r\nW1101 18:25:56.674000 27360 torch\\distributed\\elastic\\multiprocessing\\redirects.py:28] NOTE: Redirects are currently not supported in Windows or MacOs.\r\nW1101 18:25:56.746000 27360 torch\\distributed\\run.py:779]\r\nW1101 18:25:56.746000 27360 torch\\distributed\\run.py:779] *****************************************\r\nW1101 18:25:56.746000 27360 torch\\distributed\\run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your systembeing overloaded, please further tune the variable for optimal performance in your application as needed.\r\nW1101 18:25:56.746000 27360 torch\\distributed\\run.py:779] *****************************************\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\D:\\ProgramData\\anaconda3\\envs\\py310\\Scripts\\torchrun-script.py\", line 33, in <module>\r\n    sys.exit(load_entry_point('torch==2.4.1', 'console_scripts', 'torchrun')())\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\errors\\__init__.py\", line 348, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\run.py\", line 901, in main\r\n    run(args)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\run.py\", line 892, in run\r\n    elastic_launch(\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\launcher\\api.py\", line 133, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\launcher\\api.py\", line 255, in launch_agent\r\n    result = agent.run()\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\metrics\\api.py\", line 124, in wrapper\r\n    result = f(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\agent\\server\\api.py\", line 680, in run\r\n    result = self._invoke_run(role)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\agent\\server\\api.py\", line 829, in _invoke_run\r\n    self._initialize_workers(self._worker_group)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\metrics\\api.py\", line 124, in wrapper\r\n    result = f(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\agent\\server\\api.py\", line 652, in _initialize_workers\r\n    self._rendezvous(worker_group)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\metrics\\api.py\", line 124, in wrapper\r\n    result = f(*args, **kwargs)\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\agent\\server\\api.py\", line 489, in _rendezvous\r\n    rdzv_info = spec.rdzv_handler.next_rendezvous()\r\n  File \"D:\\ProgramData\\anaconda3\\envs\\py310\\Lib\\site-packages\\torch\\distributed\\elastic\\rendezvous\\static_tcp_rendezvous.py\", line 66, in next_rendezvous\r\n    self._store = TCPStore(  # type: ignore[call-arg]\r\nRuntimeError: use_libuv was requested but PyTorch was build without libuv support", "choices": "(A) 1. Upgrade PyTorch to version 2.2.2, torchvision to 0.17.2, and torchaudio to 2.2.2 using the command: `pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118`.\n2. Ensure CUDA version 11.8 is being used for compatibility with the upgraded PyTorch version. (B) 1. Downgrade CUDA to version 11.7 using the command: `conda install cudatoolkit=11.7`.\n2. Upgrade PyTorch to version 2.2.2, torchvision to 0.17.2, and torchaudio to 2.2.2 using the command: `pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118`.\n3. Ensure CUDA version 11.8 is being used for compatibility with the upgraded PyTorch version. (C) 1. Ensure CUDA version 11.8 is being used for compatibility with the upgraded PyTorch version.\n2. Upgrade PyTorch to version 2.2.2, torchvision to 0.17.2, and torchaudio to 2.2.2 using the command: `pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118`. (D) 1. Upgrade PyTorch to version 2.2.2, torchvision to 0.17.2, and torchaudio to 2.2.2 using the command: `pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118`.", "answer": "A"}
{"uuid": "6ceb3383-cadf-4c28-a1b1-67311fa5616a", "setup_instruct": "# D-FINE Deployment and Execution Plan\n\n## 1. Environment Setup\n- **Description**: Create a conda environment and install required dependencies\n- **Command**:\n  ```shell\n  conda create -n dfine python=3.11.9\n  conda activate dfine\n  pip install -r requirements.txt\n  ```\n\n## 2. Data Preparation\n### Option A: COCO2017 Dataset\n1. **Download**: Get COCO2017 from OpenDataLab or official COCO site\n2. **Configure Paths**: Edit `coco_detection.yml` with correct paths\n   ```yaml\n   train_dataloader:\n       img_folder: /data/COCO2017/train2017/\n       ann_file: /data/COCO2017/annotations/instances_train2017.json\n   val_dataloader:\n       img_folder: /data/COCO2017/val2017/\n       ann_file: /data/COCO2017/annotations/instances_val2017.json\n   ```\n\n### Option B: Objects365 Dataset\n1. **Download**: Get Objects365 from OpenDataLab\n2. **Organize Data**:\n   ```shell\n   export BASE_DIR=/data/Objects365/data\n   mkdir -p ${BASE_DIR}/train/images_from_val\n   cp -r ${BASE_DIR}/val/images/v1 ${BASE_DIR}/train/images_from_val/\n   cp -r ${BASE_DIR}/val/images/v2 ${BASE_DIR}/train/images_from_val/\n   python tools/remap_obj365.py --base_dir ${BASE_DIR}\n   python tools/resize_obj365.py --base_dir ${BASE_DIR}\n   ```\n\n## 3. Model Training\n### For COCO2017\n- **Command**:\n  ```shell\n  export model=l  # n/s/m/l/x\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0\n  ```\n\n### For Objects365 to COCO2017\n- **Command**:\n  ```shell\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth\n  ```\n\n## 4. Model Testing\n- **Command**:\n  ```shell\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth\n  ```\n\n## 5. Model Deployment\n### ONNX Export\n- **Command**:\n  ```shell\n  pip install onnx onnxsim\n  python tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n  ```\n\n### TensorRT Export\n- **Command**:\n  ```shell\n  trtexec --onnx=\"model.onnx\" --saveEngine=\"model.engine\" --fp16\n  ```\n\n## 6. Inference\n### ONNX Runtime\n- **Command**:\n  ```shell\n  python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg\n  ```\n\n### TensorRT\n- **Command**:\n  ```shell\n  python tools/inference/trt_inf.py --trt model.engine --input image.jpg\n  ```\n\n## 7. Benchmarking\n### Model Metrics\n- **Command**:\n  ```shell\n  python tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml\n  ```\n\n### Latency Test\n- **Command**:\n  ```shell\n  python tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine\n  ```\n\n## 8. Visualization\n### Fiftyone\n- **Command**:\n  ```shell\n  pip install fiftyone\n  python tools/visualization/fiftyone_vis.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n  ```", "issue_title": "训练速度慢、ONNX转换出错", "issue_body": "(D_FINE) ➜  D_FINE CUDA_VISIBLE_DEVICES=1,4,5,6 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/custom/dfine_hgnetv2_${model}_custom.yml --use-amp --seed=0\r\n[2024-10-30 19:35:14,844] torch.distributed.run: [WARNING] \r\n[2024-10-30 19:35:14,844] torch.distributed.run: [WARNING] *****************************************\r\n[2024-10-30 19:35:14,844] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n[2024-10-30 19:35:14,844] torch.distributed.run: [WARNING] *****************************************\r\nInitialized distributed mode...\r\ncfg:  {'task': 'detection', '_model': None, '_postprocessor': None, '_criterion': None, '_optimizer': None, '_lr_scheduler': None, '_lr_warmup_scheduler': None, '_train_dataloader': None, '_val_dataloader': None, '_ema': None, '_scaler': None, '_train_dataset': None, '_val_dataset': None, '_collate_fn': None, '_evaluator': None, '_writer': None, 'num_workers': 0, 'batch_size': None, '_train_batch_size': None, '_val_batch_size': None, '_train_shuffle': None, '_val_shuffle': None, 'resume': None, 'tuning': None, 'epoches': 132, 'last_epoch': -1, 'use_amp': True, 'use_ema': True, 'ema_decay': 0.9999, 'ema_warmups': 2000, 'sync_bn': True, 'clip_max_norm': 0.1, 'find_unused_parameters': False, 'seed': 0, 'print_freq': 100, 'checkpoint_freq': 12, 'output_dir': './output/dfine_hgnetv2_s_custom', 'summary_dir': None, 'device': '', 'yaml_cfg': {'task': 'detection', 'evaluator': {'type': 'CocoEvaluator', 'iou_types': ['bbox']}, 'num_classes': 4, 'remap_mscoco_category': False, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/home/ubuntu/projects/lcc/dk52/RT-DETR/rtdetrv2_pytorch/dataset/GT_dtv2D_gzt_not0txt_cls4_20241009/train', 'ann_file': '/home/ubuntu/projects/lcc/dk52/RT-DETR/rtdetrv2_pytorch/dataset/GT_dtv2D_gzt_not0txt_cls4_20241009/annotations/train.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.0}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}, {'type': 'ConvertBoxes', 'fmt': 'cxcywh', 'normalize': True}], 'policy': {'name': 'stop_epoch', 'epoch': 120, 'ops': ['RandomPhotometricDistort', 'RandomZoomOut', 'RandomIoUCrop']}}}, 'shuffle': True, 'num_workers': 4, 'drop_last': True, 'collate_fn': {'type': 'BatchImageCollateFuncion', 'base_size': 640, 'base_size_repeat': 20, 'stop_epoch': 120, 'ema_restart_decay': 0.9999}, 'total_batch_size': 64}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/home/ubuntu/projects/lcc/dk52/RT-DETR/rtdetrv2_pytorch/dataset/GT_dtv2D_gzt_not0txt_cls4_20241009/val', 'ann_file': '/home/ubuntu/projects/lcc/dk52/RT-DETR/rtdetrv2_pytorch/dataset/GT_dtv2D_gzt_not0txt_cls4_20241009/annotations/val.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}]}}, 'shuffle': False, 'num_workers': 4, 'drop_last': False, 'collate_fn': {'type': 'BatchImageCollateFuncion'}, 'total_batch_size': 64}, 'print_freq': 100, 'output_dir': './output/dfine_hgnetv2_s_custom', 'checkpoint_freq': 12, 'sync_bn': True, 'find_unused_parameters': False, 'use_amp': True, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 1000, 'start': 0}, 'epoches': 132, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': '^(?=.*backbone)(?!.*norm|bn).*$', 'lr': 0.0001}, {'params': '^(?=.*backbone)(?=.*norm|bn).*$', 'lr': 0.0001, 'weight_decay': 0.0}, {'params': '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn|bias)).*$', 'weight_decay': 0.0}], 'lr': 0.0002, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [500], 'gamma': 0.1}, 'lr_warmup_scheduler': {'type': 'LinearWarmup', 'warmup_duration': 500}, 'model': 'DFINE', 'criterion': 'DFINECriterion', 'postprocessor': 'DFINEPostProcessor', 'use_focal_loss': True, 'eval_spatial_size': [640, 640], 'DFINE': {'backbone': 'HGNetv2', 'encoder': 'HybridEncoder', 'decoder': 'DFINETransformer'}, 'HGNetv2': {'pretrained': True, 'local_model_dir': 'weight/hgnetv2/', 'name': 'B0', 'return_idx': [1, 2, 3], 'freeze_at': -1, 'freeze_norm': False, 'use_lab': True}, 'HybridEncoder': {'in_channels': [256, 512, 1024], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'expansion': 0.5, 'depth_mult': 0.34, 'act': 'silu'}, 'DFINETransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_layers': 3, 'eval_idx': -1, 'num_queries': 300, 'num_denoising': 100, 'label_noise_ratio': 0.5, 'box_noise_scale': 1.0, 'reg_max': 32, 'reg_scale': 4, 'layer_scale': 1, 'num_points': [3, 6, 3], 'cross_attn_method': 'default', 'query_select_method': 'default'}, 'DFINEPostProcessor': {'num_top_queries': 300}, 'DFINECriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2, 'loss_fgl': 0.15, 'loss_ddf': 1.5}, 'losses': ['vfl', 'boxes', 'local'], 'alpha': 0.75, 'gamma': 2.0, 'reg_max': 32, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../../dataset/custom_detection.yml', '../../runtime.yml', '../include/dataloader.yml', '../include/optimizer.yml', '../include/dfine_hgnetv2.yml'], 'config': 'configs/dfine/custom/dfine_hgnetv2_s_custom.yml', 'seed': 0, 'test_only': False, 'print_method': 'builtin', 'print_rank': 0}}\r\nLoaded stage1 B0 HGNetV2 from local file.\r\nInitial lr: [0.0001, 0.0001, 0.0002, 0.0002]\r\nbuilding train_dataloader with batch_size=16...\r\nloading annotations into memory...\r\nDone (t=0.36s)\r\ncreating index...\r\nindex created!\r\nbuilding val_dataloader with batch_size=16...\r\nloading annotations into memory...\r\nDone (t=0.02s)\r\ncreating index...\r\nindex created!\r\n\r\n------------------------------------- Calculate Flops Results -------------------------------------\r\nNotations:\r\nnumber of parameters (Params), number of multiply-accumulate operations(MACs),\r\nnumber of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\r\nfwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\r\ndefault model backpropagation takes 2.00 times as much computation as forward propagation.\r\n\r\nTotal Training Params:                                                  10.18 M \r\nfwd MACs:                                                               12.363 GMACs\r\nfwd FLOPs:                                                              24.8328 GFLOPS\r\nfwd+bwd MACs:                                                           37.0891 GMACs\r\nfwd+bwd FLOPs:                                                          74.4984 GFLOPS\r\n---------------------------------------------------------------------------------------------------\r\n{'Model FLOPs:24.8328 GFLOPS   MACs:12.363 GMACs   Params:10178971'}\r\n------------------------------------------Start training-------------------------------------------\r\nEpoch: [0]  [  0/176]  eta: 1:10:16  lr: 0.000000  loss: 51.1275 (51.1275)  loss_bbox: 4.3128 (4.3128)  loss_bbox_aux_0: 4.3128 (4.3128)  loss_bbox_aux_1: 4.3128 (4.3128)  loss_bbox_dn_0: 1.9389 (1.9389)  loss_bbox_dn_1: 1.9389 (1.9389)  loss_bbox_dn_2: 1.9389 (1.9389)  loss_bbox_dn_pre: 1.9389 (1.9389)  loss_bbox_enc_0: 4.3128 (4.3128)  loss_bbox_pre: 4.3128 (4.3128)  loss_fgl: 0.1732 (0.1732)  loss_fgl_aux_0: 0.1732 (0.1732)  loss_fgl_aux_1: 0.1732 (0.1732)  loss_fgl_dn_0: 0.8813 (0.8813)  loss_fgl_dn_1: 0.8813 (0.8813)  loss_fgl_dn_2: 0.8813 (0.8813)  loss_giou: 1.9120 (1.9120)  loss_giou_aux_0: 1.9120 (1.9120)  loss_giou_aux_1: 1.9120 (1.9120)  loss_giou_dn_0: 1.2848 (1.2848)  loss_giou_dn_1: 1.2848 (1.2848)  loss_giou_dn_2: 1.2848 (1.2848)  loss_giou_dn_pre: 1.2848 (1.2848)  loss_giou_enc_0: 1.9120 (1.9120)  loss_giou_pre: 1.9120 (1.9120)  loss_vfl: 0.1176 (0.1176)  loss_vfl_aux_0: 0.1197 (0.1197)  loss_vfl_aux_1: 0.1176 (0.1176)  loss_vfl_dn_0: 0.8148 (0.8148)  loss_vfl_dn_1: 0.8992 (0.8992)  loss_vfl_dn_2: 0.8214 (0.8214)  loss_vfl_dn_pre: 0.8148 (0.8148)  loss_vfl_enc_0: 0.1199 (0.1199)  loss_vfl_pre: 0.1197 (0.1197)  time: 23.9598  data: 10.0737  max mem: 6707\r\nEpoch: [0]  [100/176]  eta: 0:03:29  lr: 0.000020  loss: 38.3745 (41.4032)  loss_bbox: 1.8019 (2.6033)  loss_bbox_aux_0: 1.7953 (2.6397)  loss_bbox_aux_1: 1.7968 (2.6194)  loss_bbox_dn_0: 1.6007 (1.5815)  loss_bbox_dn_1: 1.6275 (1.5904)  loss_bbox_dn_2: 1.6347 (1.6061)  loss_bbox_dn_pre: 1.5850 (1.5764)  loss_bbox_enc_0: 2.0097 (2.7025)  loss_bbox_pre: 1.8164 (2.6547)  loss_fgl: 0.9912 (0.5893)  loss_fgl_aux_0: 0.9981 (0.5820)  loss_fgl_aux_1: 0.9980 (0.5868)  loss_fgl_dn_0: 0.8740 (0.8622)  loss_fgl_dn_1: 0.8718 (0.8617)  loss_fgl_dn_2: 0.8693 (0.8608)  loss_giou: 1.0497 (1.4870)  loss_giou_aux_0: 1.0463 (1.5001)  loss_giou_aux_1: 1.0468 (1.4924)  loss_giou_dn_0: 1.3033 (1.3164)  loss_giou_dn_1: 1.3053 (1.3168)  loss_giou_dn_2: 1.3086 (1.3178)  loss_giou_dn_pre: 1.3016 (1.3161)  loss_giou_enc_0: 1.1653 (1.5355)  loss_giou_pre: 1.0567 (1.5071)  loss_vfl: 0.9315 (0.5313)  loss_vfl_aux_0: 0.9689 (0.5103)  loss_vfl_aux_1: 0.9591 (0.5325)  loss_vfl_dn_0: 0.3807 (0.5390)  loss_vfl_dn_1: 0.3813 (0.5682)  loss_vfl_dn_2: 0.3716 (0.5022)  loss_vfl_dn_pre: 0.3814 (0.5394)  loss_vfl_enc_0: 0.8380 (0.4690)  loss_vfl_pre: 0.9890 (0.5028)  loss_ddf_aux_0: 0.0055 (0.0016)  loss_ddf_aux_1: 0.0009 (0.0004)  loss_ddf_dn_0: 0.0009 (0.0004)  loss_ddf_dn_1: 0.0002 (0.0001)  time: 2.4388  data: 0.5906  max mem: 9660\r\n作者您好，打扰您，我在使用您的代码的时候遇到了一些问题，希望您有空的时候能够帮忙看一下。\r\n以上是我的训练日志，我一直在反复的调节total_batch_size，调节下来的总结就是似乎我4个GPU（单个48G显存）只能设置total_batch_size为64，再往上加就会报错RuntimeError: received 0 items of ancdata，不确定是哪里的问题。目前一个epoch需要花费10分钟吧，是正常的吗？我的数据量也不大，训练加验证就1万3左右的图片数量。这让我很困惑。我试过把torch.multiprocessing.set_sharing_strategy('file_system')，但是这并不是一个好的方法。程序会卡住不动。", "choices": "(A) 1. Modify the dataset processing code to ensure labels start from 0. For example, in `coco_dataset.py`, change the label assignment to `labels = [obj[\"category_id\"] - 1 for obj in anno]` if using COCO format data.\n2. Check the number of classes in your dataset configuration file (e.g., `obj365_detection.yml`). Ensure it matches the actual number of classes in your custom dataset plus one (for background class).\n3. If the issue persists, add `os.environ['CUDA_LAUNCH_BLOCKING'] = '1'` to your `train.py` script to help identify the exact location of the error. (B) 1. Check the number of classes in your dataset configuration file (e.g., `obj365_detection.yml`). Ensure it matches the actual number of classes in your custom dataset plus one (for background class).\n2. If the issue persists, add `os.environ['CUDA_LAUNCH_BLOCKING'] = '1'` to your `train.py` script to help identify the exact location of the error. (C) 1. Check the number of classes in your dataset configuration file (e.g., `obj365_detection.yml`). Ensure it matches the actual number of classes in your custom dataset plus one (for background class).\n2. Modify the dataset processing code to ensure labels start from 0. For example, in `coco_dataset.py`, change the label assignment to `labels = [obj[\"category_id\"] - 1 for obj in anno]` if using COCO format data.\n3. If the issue persists, add `os.environ['CUDA_LAUNCH_BLOCKING'] = '1'` to your `train.py` script to help identify the exact location of the error. (D) 1. Check the number of classes in your dataset configuration file (e.g., `obj365_detection.yml`). Ensure it matches the actual number of classes in your custom dataset plus one (for background class).\n2. Manually set all labels to 0 in the dataset to simplify the training process.\n3. Modify the dataset processing code to ensure labels start from 0. For example, in `coco_dataset.py`, change the label assignment to `labels = [obj[\"category_id\"] - 1 for obj in anno]` if using COCO format data.\n4. If the issue persists, add `os.environ['CUDA_LAUNCH_BLOCKING'] = '1'` to your `train.py` script to help identify the exact location of the error.", "answer": "C"}
{"uuid": "ac6273a5-dead-49a6-a63f-82ced5a62f38", "setup_instruct": "# D-FINE Project Execution Plan\n\n## 1. Environment Setup\n- **Description**: Create and activate a conda environment with Python 3.11.9 and install required dependencies\n- **Commands**:\n```bash\nconda create -n dfine python=3.11.9\nconda activate dfine\npip install -r requirements.txt\n```\n\n## 2. Data Preparation\n### Option A: COCO2017\n- **Description**: Download and configure COCO2017 dataset\n- **Steps**:\n  1. Download from OpenDataLab or COCO website\n  2. Modify paths in `configs/dataset/coco_detection.yml`\n\n### Option B: Objects365\n- **Description**: Prepare Objects365 dataset with additional processing\n- **Commands**:\n```bash\nexport BASE_DIR=/data/Objects365/data\nmkdir -p ${BASE_DIR}/train/images_from_val\ncp -r ${BASE_DIR}/val/images/v1 ${BASE_DIR}/train/images_from_val/\ncp -r ${BASE_DIR}/val/images/v2 ${BASE_DIR}/train/images_from_val/\npython tools/remap_obj365.py --base_dir ${BASE_DIR}\npython tools/resize_obj365.py --base_dir ${BASE_DIR}\n```\n\n## 3. Model Training\n### For COCO2017\n- **Description**: Train model on COCO2017 dataset\n- **Commands**:\n```bash\nexport model=l  # n/s/m/l/x\nCUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0\n```\n\n### For Objects365 to COCO\n- **Description**: Pretrain on Objects365 then fine-tune on COCO\n- **Commands**:\n```bash\n# Pretraining\nCUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj365.yml --use-amp --seed=0\n\n# Fine-tuning\nCUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth\n```\n\n## 4. Model Evaluation\n- **Description**: Test trained model performance\n- **Command**:\n```bash\nCUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth\n```\n\n## 5. Deployment\n- **Description**: Export model to ONNX and TensorRT formats\n- **Commands**:\n```bash\n# ONNX Export\npython tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n\n# TensorRT Conversion\ntrtexec --onnx=\"model.onnx\" --saveEngine=\"model.engine\" --fp16\n```\n\n## 6. Inference\n- **Description**: Run inference on images/videos\n- **Commands**:\n```bash\n# ONNX Runtime\npython tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg\n\n# TensorRT\npython tools/inference/trt_inf.py --trt model.engine --input image.jpg\n\n# PyTorch\npython tools/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0\n```\n\n## 7. Benchmarking\n- **Description**: Measure model performance metrics\n- **Commands**:\n```bash\n# Model Metrics\npython tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml\n\n# Latency Test\npython tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine\n```", "issue_title": "How to solve this error when I finetune the  model use custom dataset?", "issue_body": "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [92,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [93,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [94,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [95,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [30,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [138,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [96,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [97,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [98,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [99,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [100,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [101,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [102,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [103,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [104,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [105,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [106,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [107,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [108,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [110,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [111,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [112,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [113,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [114,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [115,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [116,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [117,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [118,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [119,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [120,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [121,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [122,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [123,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [125,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [126,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [127,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [96,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [97,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [98,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [99,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [100,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [101,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [102,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [103,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [104,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [105,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [106,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [107,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [108,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [110,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [111,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [112,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [113,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [114,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [115,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [116,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [117,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [118,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [119,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [120,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [121,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [122,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [123,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [125,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [126,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [42,0,0], thread: [127,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [96,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [97,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [98,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [99,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [100,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [101,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [102,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [103,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [104,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [105,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [106,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [107,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [108,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [110,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [111,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [112,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [113,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [114,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [115,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [116,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [117,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [118,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [119,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [120,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [121,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [122,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [123,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [125,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [126,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [151,0,0], thread: [127,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [96,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [97,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [98,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [99,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [100,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [101,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [102,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [103,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [104,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [105,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [106,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [107,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [108,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [110,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [111,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [112,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [113,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [114,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [115,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [116,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [117,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [118,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [119,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [120,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [121,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [122,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [123,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [125,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [126,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [43,0,0], thread: [127,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n[rank1]:[E1030 19:36:07.513873021 ProcessGroupNCCL.cpp:1515] [PG 0 (default_pg) Rank 1] Process group watchdog thread terminated with exception: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f2f86a70f86 in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libc10.so)\r\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f2f86a1fd10 in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libc10.so)\r\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f2f86b4bf08 in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)\r\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f2f87d683e6 in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\r\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f2f87d6d600 in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\r\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f2f87d742ba in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\r\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f2f87d766fc in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\r\nframe #7: <unknown function> + 0xdbbf4 (0x7f2fd550fbf4 in /home/ahs/anaconda3/envs/py310/bin/../lib/libstdc++.so.6)\r\nframe #8: <unknown function> + 0x76db (0x7f2fd75486db in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #9: clone + 0x3f (0x7f2fd6acc61f in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\n[rank3]:[E1030 19:36:07.515488087 ProcessGroupNCCL.cpp:1515] [PG 0 (default_pg) Rank 3] Process group watchdog thread terminated with exception: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6bcb2adf86 in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libc10.so)\r\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f6bcb25cd10 in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libc10.so)\r\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f6bcb388f08 in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)\r\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f6bcc5a53e6 in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\r\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f6bcc5aa600 in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\r\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f6bcc5b12ba in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\r\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f6bcc5b36fc in /home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\r\nframe #7: <unknown function> + 0xdbbf4 (0x7f6c19d4cbf4 in /home/ahs/anaconda3/envs/py310/bin/../lib/libstdc++.so.6)\r\nframe #8: <unknown function> + 0x76db (0x7f6c1bd856db in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #9: clone + 0x3f (0x7f6c1b30961f in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\nW1030 19:36:08.253000 140521457722304 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 96054 closing signal SIGTERM\r\nE1030 19:36:08.468000 140521457722304 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: -6) local_rank: 0 (pid: 96052) of binary: /home/ahs/anaconda3/envs/py310/bin/python\r\nTraceback (most recent call last):\r\n  File \"/home/ahs/anaconda3/envs/py310/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\r\n    run(args)\r\n  File \"/home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\r\n    elastic_launch(\r\n  File \"/home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 133, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/home/ahs/anaconda3/envs/py310/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n======================================================\r\ntrain.py FAILED\r\n------------------------------------------------------\r\nFailures:\r\n[1]:\r\n  time      : 2024-10-30_19:36:08\r\n  host      : ahs-SYS-4029GP-TRTC-ZY001\r\n  rank      : 1 (local_rank: 1)\r\n  exitcode  : -6 (pid: 96053)\r\n  error_file: <N/A>\r\n  traceback : Signal 6 (SIGABRT) received by PID 96053\r\n[2]:\r\n  time      : 2024-10-30_19:36:08\r\n  host      : ahs-SYS-4029GP-TRTC-ZY001\r\n  rank      : 3 (local_rank: 3)\r\n  exitcode  : -6 (pid: 96055)\r\n  error_file: <N/A>\r\n  traceback : Signal 6 (SIGABRT) received by PID 96055\r\n------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2024-10-30_19:36:08\r\n  host      : ahs-SYS-4029GP-TRTC-ZY001\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : -6 (pid: 96052)\r\n  error_file: <N/A>\r\n  traceback : Signal 6 (SIGABRT) received by PID 96052\r\n======================================================", "choices": "(A) 1. Ensure your dataset's category IDs start from 0 and are sequential. If they don't, modify them to start from 0.\n2. Set `remap_mscoco_category: True` in your configuration file to ensure proper mapping of category IDs.\n3. Delete the `mscoco_category2name` mapping file in `src/data/dataset/coco_dataset.py`.\n4. If your dataset's categories are not part of the COCO categories, update the `mscoco_category2name` mapping in `src/data/dataset/coco_dataset.py` to match your dataset's labels.\n5. Verify that `num_classes` in your configuration matches the number of unique categories in your dataset. (B) 1. Verify that `num_classes` in your configuration matches the number of unique categories in your dataset.\n2. Ensure your dataset's category IDs start from 0 and are sequential. If they don't, modify them to start from 0.\n3. Set `remap_mscoco_category: True` in your configuration file to ensure proper mapping of category IDs.\n4. If your dataset's categories are not part of the COCO categories, update the `mscoco_category2name` mapping in `src/data/dataset/coco_dataset.py` to match your dataset's labels. (C) 1. Ensure your dataset's category IDs start from 0 and are sequential. If they don't, modify them to start from 0.\n2. Set `remap_mscoco_category: True` in your configuration file to ensure proper mapping of category IDs.\n3. If your dataset's categories are not part of the COCO categories, update the `mscoco_category2name` mapping in `src/data/dataset/coco_dataset.py` to match your dataset's labels.\n4. Verify that `num_classes` in your configuration matches the number of unique categories in your dataset. (D) 1. Set `remap_mscoco_category: True` in your configuration file to ensure proper mapping of category IDs.\n2. If your dataset's categories are not part of the COCO categories, update the `mscoco_category2name` mapping in `src/data/dataset/coco_dataset.py` to match your dataset's labels.\n3. Verify that `num_classes` in your configuration matches the number of unique categories in your dataset. (E) 1. Ensure your dataset's category IDs start from 0 and are sequential. If they don't, modify them to start from 0.\n2. Set `remap_mscoco_category: True` in your configuration file to ensure proper mapping of category IDs.\n3. Set `remap_mscoco_category: False` in your configuration file to disable proper mapping of category IDs.\n4. If your dataset's categories are not part of the COCO categories, update the `mscoco_category2name` mapping in `src/data/dataset/coco_dataset.py` to match your dataset's labels.\n5. Verify that `num_classes` in your configuration matches the number of unique categories in your dataset. (F) 1. Ensure your dataset's category IDs start from 0 and are sequential. If they don't, modify them to start from 0.\n2. If your dataset's categories are not part of the COCO categories, update the `mscoco_category2name` mapping in `src/data/dataset/coco_dataset.py` to match your dataset's labels.\n3. Set `remap_mscoco_category: True` in your configuration file to ensure proper mapping of category IDs.\n4. Verify that `num_classes` in your configuration matches the number of unique categories in your dataset. (G) 1. Ensure your dataset's category IDs start from 0 and are sequential. If they don't, modify them to start from 0.\n2. Set `remap_mscoco_category: True` in your configuration file to ensure proper mapping of category IDs.\n3. If your dataset's categories are not part of the COCO categories, update the `mscoco_category2name` mapping in `src/data/dataset/coco_dataset.py` to match your dataset's labels.", "answer": "C"}
{"uuid": "185e3d63-215f-4d07-a839-9a819a3bbc30", "setup_instruct": "# D-FINE Project Execution Plan\n\n## 1. Environment Setup\n- **Description**: Create a conda environment and install dependencies\n- **Commands**:\n  ```shell\n  conda create -n dfine python=3.11.9\n  conda activate dfine\n  pip install -r requirements.txt\n  ```\n\n## 2. Data Preparation\n### Option A: COCO2017\n- **Description**: Download and configure COCO2017 dataset\n- **Steps**:\n  1. Download from OpenDataLab or COCO website\n  2. Modify paths in `configs/dataset/coco_detection.yml`\n\n### Option B: Objects365\n- **Description**: Prepare Objects365 dataset\n- **Commands**:\n  ```shell\n  export BASE_DIR=/data/Objects365/data\n  mkdir -p ${BASE_DIR}/train/images_from_val\n  cp -r ${BASE_DIR}/val/images/v1 ${BASE_DIR}/train/images_from_val/\n  cp -r ${BASE_DIR}/val/images/v2 ${BASE_DIR}/train/images_from_val/\n  python tools/remap_obj365.py --base_dir ${BASE_DIR}\n  python tools/resize_obj365.py --base_dir ${BASE_DIR}\n  ```\n\n## 3. Training\n### Basic Training (COCO2017)\n- **Description**: Train model on COCO dataset\n- **Commands**:\n  ```shell\n  export model=l  # n/s/m/l/x\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0\n  ```\n\n### Transfer Learning (Objects365 → COCO)\n- **Description**: Pretrain on Objects365 then fine-tune on COCO\n- **Commands**:\n  ```shell\n  # Pretrain\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj365.yml --use-amp --seed=0\n  \n  # Fine-tune\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth\n  ```\n\n## 4. Evaluation\n- **Description**: Test model performance\n- **Commands**:\n  ```shell\n  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth\n  ```\n\n## 5. Deployment\n- **Description**: Export model to ONNX and TensorRT\n- **Commands**:\n  ```shell\n  # ONNX Export\n  python tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth\n  \n  # TensorRT Export\n  trtexec --onnx=\"model.onnx\" --saveEngine=\"model.engine\" --fp16\n  ```\n\n## 6. Inference\n- **Description**: Run inference on images/videos\n- **Commands**:\n  ```shell\n  # ONNX Runtime\n  python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg\n  \n  # TensorRT\n  python tools/inference/trt_inf.py --trt model.engine --input image.jpg\n  \n  # PyTorch\n  python tools/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0\n  ```\n\n## 7. Benchmarking\n- **Description**: Measure model performance metrics\n- **Commands**:\n  ```shell\n  # Model Metrics\n  python tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml\n  \n  # Latency Test\n  python tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine\n  ```", "issue_title": "bounding box assertion", "issue_body": "```\r\n[rank0]:   File \"/home/kenny/D-FINE/src/zoo/dfine/box_ops.py\", line 54, in generalized_box_iou          \r\n[rank0]:     assert (boxes1[:, 2:] >= boxes1[:, :2]).all()   \r\n\r\n```\r\nGreat work! After training on the CrowdHuman dataset for several epochs, I encountered an assertion error. I've attached the training logs for reference. Could you please help me understand what might have gone wrong?\r\n\r\n[training_log.txt](https://github.com/user-attachments/files/17456759/training_log.txt)\r\n[log.txt](https://github.com/user-attachments/files/17456761/log.txt)", "choices": "(A) 1. For the PIL decompression bomb error, either preprocess large images using the script at `tools/dataset/resize_obj365.py` before training or bypass the error by setting `MAX_IMAGE_PIXELS = None` in the function `_decompression_bomb_check` of PIL.Image. (B) 1. For the IndexError, customize the `self.obj365_ids` in the code to map to your own custom classes or use the updated code in `src/solver/_solver.py` where mismatched class heads are automatically skipped. (C) 1. For the IndexError, customize the `self.obj365_ids` in the code to map to your own custom classes or use the updated code in `src/solver/_solver.py` where mismatched class heads are automatically skipped.\n2. For the PIL decompression bomb error, either preprocess large images using the script at `tools/dataset/resize_obj365.py` before training or bypass the error by setting `MAX_IMAGE_PIXELS = None` in the function `_decompression_bomb_check` of PIL.Image. (D) 1. For the IndexError, customize the `self.obj365_ids` in the code to map to your own custom classes or use the updated code in `src/solver/_solver.py` where mismatched class heads are automatically skipped.\n2. For the PIL decompression bomb error, either preprocess large images using the script at `tools/dataset/resize_obj365.py` before training or bypass the error by setting `MAX_IMAGE_PIXELS = None` in the function `_decompression_bomb_check` of PIL.Image.\n3. Set `MAX_IMAGE_PIXELS = 1000` in the function `_decompression_bomb_check` of PIL.Image to prevent large images from being processed. (E) 1. For the PIL decompression bomb error, either preprocess large images using the script at `tools/dataset/resize_obj365.py` before training or bypass the error by setting `MAX_IMAGE_PIXELS = None` in the function `_decompression_bomb_check` of PIL.Image.\n2. For the IndexError, customize the `self.obj365_ids` in the code to map to your own custom classes or use the updated code in `src/solver/_solver.py` where mismatched class heads are automatically skipped. (F) 1. For the PIL decompression bomb error, bypass the error by setting `MAX_IMAGE_PIXELS = None` in the function `_decompression_bomb_check` of PIL.Image.\n2. For the IndexError, customize the `self.obj365_ids` in the code to map to your own custom classes or use the updated code in `src/solver/_solver.py` where mismatched class heads are automatically skipped.", "answer": "C"}
{"uuid": "6b057597-d062-4562-85a1-3023d3817147", "setup_instruct": "# NoPoSplat Execution Plan\n\n## 1. Installation\n1. **Clone the NoPoSplat repository**  \n   ```bash\n   git clone https://github.com/cvg/NoPoSplat\n   cd NoPoSplat\n   ```\n\n2. **Set up the conda environment**  \n   ```bash\n   conda create -y -n noposplat python=3.10\n   conda activate noposplat\n   pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n   pip install -r requirements.txt\n   ```\n\n3. **Optional: Compile CUDA kernels for RoPE**  \n   ```bash\n   cd src/model/encoder/backbone/croco/curope/\n   python setup.py build_ext --inplace\n   cd ../../../../../..\n   ```\n\n## 2. Download Pre-trained Checkpoints\n1. **Download models from Hugging Face**  \n   Place the downloaded checkpoints in the `pretrained_weights` directory. Example:  \n   ```bash\n   wget https://huggingface.co/botaoye/NoPoSplat/resolve/main/re10k.ckpt -P pretrained_weights/\n   ```\n\n## 3. Dataset Preparation\n1. **Refer to DATASETS.md**  \n   Follow instructions in [DATASETS.md](DATASETS.md) to prepare datasets.\n\n## 4. Training\n1. **Download MASt3R pretrained model**  \n   ```bash\n   wget https://download.europe.naverlabs.com/ComputerVision/MASt3R/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth -P pretrained_weights/\n   ```\n\n2. **Start training (8 GPUs)**  \n   ```bash\n   python -m src.main +experiment=re10k wandb.mode=online wandb.name=re10k\n   ```\n   - For 1 GPU (A6000), use config from `config/experiment/re10k_1x8.yaml`.\n\n## 5. Evaluation\n### Novel View Synthesis\n1. **Evaluate on RealEstate10K**  \n   ```bash\n   python -m src.main +experiment=re10k mode=test wandb.name=re10k dataset/view_sampler@dataset.re10k.view_sampler=evaluation dataset.re10k.view_sampler.index_path=assets/evaluation_index_re10k.json checkpointing.load=./pretrained_weights/re10k.ckpt test.save_image=true\n   ```\n\n2. **Evaluate on ACID**  \n   ```bash\n   python -m src.main +experiment=acid mode=test wandb.name=acid dataset/view_sampler@dataset.re10k.view_sampler=evaluation dataset.re10k.view_sampler.index_path=assets/evaluation_index_acid.json checkpointing.load=./pretrained_weights/acid.ckpt test.save_image=true\n   ```\n\n3. **Evaluate on DTU**  \n   ```bash\n   python -m src.main +experiment=re10k mode=test wandb.name=dtu dataset/view_sampler@dataset.re10k.view_sampler=evaluation dataset.re10k.view_sampler.index_path=assets/evaluation_index_dtu.json dataset.re10k.roots=[datasets/dtu] dataset.re10k.skip_bad_shape=false checkpointing.load=./pretrained_weights/re10k.ckpt test.save_image=true\n   ```\n\n### Pose Estimation\n1. **Evaluate on RealEstate10K**  \n   ```bash\n   python -m src.eval_pose +experiment=re10k +evaluation=eval_pose checkpointing.load=./pretrained_weights/mixRe10kDl3dv.ckpt dataset/view_sampler@dataset.re10k.view_sampler=evaluation dataset.re10k.view_sampler.index_path=assets/evaluation_index_re10k.json\n   ```\n\n2. **Evaluate on ACID**  \n   ```bash\n   python -m src.eval_pose +experiment=acid +evaluation=eval_pose checkpointing.load=./pretrained_weights/mixRe10kDl3dv.ckpt dataset/view_sampler@dataset.re10k.view_sampler=evaluation dataset.re10k.view_sampler.index_path=assets/evaluation_index_acid.json\n   ```", "issue_title": "About multi gpu training", "issue_body": "Thanks for the excenllent work!\n\nI encounter a problem to starting multi gpu training. I have 8 GPUs but only the first GPU can be utilized:\n\n![Image](https://github.com/user-attachments/assets/b1504845-f77a-4fb6-bae3-bf91ed95c517)\n\nI use the default command to train the model:\n\n```\npython -m src.main +experiment=re10k wandb.mode=online wandb.name=re10k\n```\n\nThis is my install dependencies:\n\n```\n(noposplat) root@eos0257:/workspace/code/NoPoSplat# pip list\nPackage                     Version\n--------------------------- ------------\naiohappyeyeballs            2.4.4\naiohttp                     3.11.11\naiosignal                   1.3.2\nannotated-types             0.7.0\nantlr4-python3-runtime      4.9.3\nasync-timeout               5.0.1\nattrs                       25.1.0\nbeartype                    0.19.0\nblack                       24.10.0\ncertifi                     2022.12.7\ncharset-normalizer          2.1.1\nclick                       8.1.8\ncolorama                    0.4.6\ncolorspacious               1.1.2\ncontourpy                   1.3.1\ncycler                      0.12.1\ndacite                      1.8.1\ndecorator                   4.4.2\ndiff_gaussian_rasterization 0.0.0\ndocker-pycreds              0.4.0\ne3nn                        0.5.4\neinops                      0.8.0\nfilelock                    3.13.1\nfonttools                   4.55.6\nfrozenlist                  1.5.0\nfsspec                      2024.2.0\ngitdb                       4.0.12\nGitPython                   3.1.44\nhuggingface-hub             0.27.1\nhydra-core                  1.3.2\nidna                        3.4\nimageio                     2.37.0\nimageio-ffmpeg              0.6.0\njaxtyping                   0.2.36\nJinja2                      3.1.3\nkiwisolver                  1.4.8\nlazy_loader                 0.4\nlightning                   2.5.0.post0\nlightning-utilities         0.11.9\nlpips                       0.1.4\nMarkupSafe                  2.1.5\nmatplotlib                  3.10.0\nmoviepy                     1.0.3\nmpmath                      1.3.0\nmultidict                   6.1.0\nmypy-extensions             1.0.0\nnetworkx                    3.2.1\nnumpy                       1.26.3\nnvidia-cublas-cu12          12.1.3.1\nnvidia-cuda-cupti-cu12      12.1.105\nnvidia-cuda-nvrtc-cu12      12.1.105\nnvidia-cuda-runtime-cu12    12.1.105\nnvidia-cudnn-cu12           8.9.2.26\nnvidia-cufft-cu12           11.0.2.54\nnvidia-curand-cu12          10.3.2.106\nnvidia-cusolver-cu12        11.4.5.107\nnvidia-cusparse-cu12        12.1.0.106\nnvidia-nccl-cu12            2.19.3\nnvidia-nvjitlink-cu12       12.1.105\nnvidia-nvtx-cu12            12.1.105\nomegaconf                   2.3.0\nopencv-python               4.11.0.86\nopt_einsum                  3.4.0\nopt-einsum-fx               0.1.4\npackaging                   24.2\npathspec                    0.12.1\npillow                      10.2.0\npip                         24.2\nplatformdirs                4.3.6\nplyfile                     1.1\nproglog                     0.1.10\npropcache                   0.2.1\nprotobuf                    5.29.3\npsutil                      6.1.1\npydantic                    2.10.6\npydantic_core               2.27.2\npyparsing                   3.2.1\npython-dateutil             2.9.0.post0\npython-dotenv               1.0.1\npytorch-lightning           2.5.0.post0\nPyYAML                      6.0.2\nrequests                    2.28.1\nruff                        0.9.3\nsafetensors                 0.5.2\nscikit-image                0.25.1\nscikit-video                1.1.11\nscipy                       1.15.1\nsentry-sdk                  2.20.0\nsetproctitle                1.3.4\nsetuptools                  75.1.0\nsix                         1.17.0\nsmmap                       5.0.2\nsvg.py                      1.5.0\nsympy                       1.13.1\ntabulate                    0.9.0\ntifffile                    2025.1.10\ntimm                        1.0.14\ntomli                       2.2.1\ntorch                       2.2.0+cu121\ntorchaudio                  2.2.0+cu121\ntorchmetrics                1.6.1\ntorchvision                 0.17.0+cu121\ntqdm                        4.67.1\ntriton                      2.2.0\ntyping_extensions           4.12.2\nurllib3                     1.26.13\nwandb                       0.19.4\nwheel                       0.44.0\nyarl                        1.18.3\n```", "choices": "(A) 1. Decrease the initial learning rate to 5x10^-5 as suggested by the OP's successful training with re10k_1x8 configuration. 2. If the issue persists, consider increasing the batch size if hardware allows. (B) 1. Increase the initial learning rate to 1x10^-3 to speed up training. 2. If the issue persists, consider increasing the batch size if hardware allows. 3. For further stability, apply gradient clipping and normalization as mentioned by another user, though the impact on final performance is uncertain. (C) 1. If the issue persists, consider increasing the batch size if hardware allows. 2. For further stability, apply gradient clipping and normalization as mentioned by another user, though the impact on final performance is uncertain. (D) 1. Decrease the initial learning rate to 5x10^-5 as suggested by the OP's successful training with re10k_1x8 configuration. 2. Disable gradient clipping to avoid unnecessary constraints. 3. If the issue persists, consider increasing the batch size if hardware allows. (E) 1. For further stability, apply gradient clipping and normalization as mentioned by another user, though the impact on final performance is uncertain. 2. Decrease the initial learning rate to 5x10^-5 as suggested by the OP's successful training with re10k_1x8 configuration. 3. If the issue persists, consider increasing the batch size if hardware allows. (F) 1. If the issue persists, consider increasing the batch size if hardware allows. 2. Decrease the initial learning rate to 5x10^-5 as suggested by the OP's successful training with re10k_1x8 configuration. 3. For further stability, apply gradient clipping and normalization as mentioned by another user, though the impact on final performance is uncertain. (G) 1. Decrease the initial learning rate to 5x10^-5 as suggested by the OP's successful training with re10k_1x8 configuration. 2. If the issue persists, consider increasing the batch size if hardware allows. 3. For further stability, apply gradient clipping and normalization as mentioned by another user, though the impact on final performance is uncertain.", "answer": "G"}
{"uuid": "cdb7eb77-0f8a-4f22-a140-a4582cb86a9e", "setup_instruct": "# NoPoSplat Execution Plan\n\n## 1. Installation\n### 1.1 Clone Repository\n```bash\ngit clone https://github.com/cvg/NoPoSplat\ncd NoPoSplat\n```\n\n### 1.2 Create Conda Environment\n```bash\nconda create -y -n noposplat python=3.10\nconda activate noposplat\n```\n\n### 1.3 Install Dependencies\n```bash\npip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\npip install -r requirements.txt\n```\n\n### 1.4 (Optional) Compile CUDA Kernels\n```bash\ncd src/model/encoder/backbone/croco/curope/\npython setup.py build_ext --inplace\ncd ../../../../../..\n```\n\n## 2. Download Pre-trained Models\n### 2.1 Download Models from Hugging Face\nDownload required checkpoints from [Hugging Face](https://huggingface.co/botaoye/noposplat) and place them in `pretrained_weights/` directory.\n\nExample for re10k model:\n```bash\nwget https://huggingface.co/botaoye/NoPoSplat/resolve/main/re10k.ckpt -P pretrained_weights/\n```\n\n## 3. Dataset Preparation\nRefer to [DATASETS.md](DATASETS.md) for dataset setup instructions.\n\n## 4. Training\n### 4.1 Download MASt3R Pretrained Model\n```bash\nwget https://download.europe.naverlabs.com/ComputerVision/MASt3R/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth -P pretrained_weights/\n```\n\n### 4.2 Start Training (8 GPUs)\n```bash\npython -m src.main +experiment=re10k wandb.mode=online wandb.name=re10k\n```\n\n### 4.3 Single GPU Training (A6000)\nUse configuration from [re10k_1x8.yaml](config/experiment/re10k_1x8.yaml).\n\n## 5. Evaluation\n### 5.1 Novel View Synthesis\n#### RealEstate10K\n```bash\npython -m src.main +experiment=re10k mode=test wandb.name=re10k dataset/view_sampler@dataset.re10k.view_sampler=evaluation dataset.re10k.view_sampler.index_path=assets/evaluation_index_re10k.json checkpointing.load=./pretrained_weights/re10k.ckpt test.save_image=true\n```\n\n#### ACID\n```bash\npython -m src.main +experiment=acid mode=test wandb.name=acid dataset/view_sampler@dataset.re10k.view_sampler=evaluation dataset.re10k.view_sampler.index_path=assets/evaluation_index_acid.json checkpointing.load=./pretrained_weights/acid.ckpt test.save_image=true\n```\n\n### 5.2 Pose Estimation\n#### RealEstate10K\n```bash\npython -m src.eval_pose +experiment=re10k +evaluation=eval_pose checkpointing.load=./pretrained_weights/mixRe10kDl3dv.ckpt dataset/view_sampler@dataset.re10k.view_sampler=evaluation dataset.re10k.view_sampler.index_path=assets/evaluation_index_re10k.json\n```\n\n#### ACID\n```bash\npython -m src.eval_pose +experiment=acid +evaluation=eval_pose checkpointing.load=./pretrained_weights/mixRe10kDl3dv.ckpt dataset/view_sampler@dataset.re10k.view_sampler=evaluation dataset.re10k.view_sampler.index_path=assets/evaluation_index_acid.json\n```", "issue_title": "Loss Increased", "issue_body": "Thanks for your awesome work. Do you have any idea why the loss increased after a certain step (one at 60k, and the other one at 70k) as follows? Thanks\r\n\r\n<img width=\"1064\" alt=\"Screen Shot 2024-11-08 at 11 07 35 AM\" src=\"https://github.com/user-attachments/assets/f4901a62-e4cb-4df8-9fe5-f6f180857ebf\">", "choices": "(A) 1. Remove the redundant code that references the 'epipolar' folder.  \n2. Confirm that the 'epipolar' folder is redundant.  \n3. No additional configuration changes are needed as the issue is resolved by code cleanup. (B) 1. Confirm that the 'epipolar' folder is redundant.  \n2. Create a new 'epipolar' folder to replace the missing one.  \n3. Remove the redundant code that references the 'epipolar' folder.  \n4. No additional configuration changes are needed as the issue is resolved by code cleanup. (C) 1. Remove the redundant code that references the 'epipolar' folder.  \n2. No additional configuration changes are needed as the issue is resolved by code cleanup. (D) The issue was caused by a missing 'epipolar' folder which was later confirmed to be redundant. The solution involved removing the redundant code that referenced the 'epipolar' folder. No additional configuration changes were needed as the issue was resolved by code cleanup.", "answer": "D"}
{"uuid": "7442a150-129c-4ed3-ad67-274464295045", "setup_instruct": "# MMIE Benchmark Setup and Evaluation Plan\n\n## 📚 Setup\n1. **Request Dataset Access**\n   - Visit [MMIE Dataset on HuggingFace](https://huggingface.co/datasets/MMIE/MMIE)\n   - Request access (auto-approved)\n\n2. **Download Dataset**\n   - Download all files from the repository\n   - Unzip `images.tar.gz` to extract images\n   - Keep `overview.json` as a format reference\n\n## 📦 Model Evaluation\n\n### Setup\n1. **Prepare Dataset Structure**\n   - Organize your input data in the required JSON format:\n     ```json\n     [\n       {\n         \"id\": \"\",\n         \"question\": [{\"text\": \"...\", \"image\": \"PATH_OR_NULL\"}],\n         \"answer\": [{\"text\": \"...\", \"image\": \"PATH_OR_NULL\"}],\n         \"model\": \"gt\",\n         \"gt_answer\": [{\"text\": \"...\", \"image\": \"PATH_OR_NULL\"}]\n       }\n     ]\n     ```\n   - Ensure file structure:\n     ```\n     INPUT_DIR/\n       ├── data.json\n       └── images/\n           ├── 0.png\n           ├── 1.png\n           └── ...\n     ```\n\n2. **Clone Repository**\n   ```bash\n   git clone https://github.com/Lillianwei-h/MMIE\n   cd MMIE\n   ```\n\n3. **Create Conda Environment**\n   ```bash\n   conda create -n MMIE python=3.11\n   conda activate MMIE\n   ```\n\n4. **Install Dependencies**\n   ```bash\n   pip install -r requirements.txt\n   pip install flash_attn\n   ```\n\n5. **Request Model Access**\n   - Visit [MMIE-Score on HuggingFace](https://huggingface.co/MMIE/MMIE-Score)\n   - Request access to the evaluation model\n\n### Execution\n1. **Run Evaluation**\n   ```bash\n   python main.py --input_dir INPUT_DIR --input_file data.json\n   ```\n\n2. **Output Results**\n   - Default output path: `./eval_outputs/eval_result.json`\n   - Optional custom output:\n     ```bash\n     python main.py --input_dir INPUT_DIR --input_file data.json \\\n                   --output_dir CUSTOM_DIR --output_file custom_name.json\n     ```\n\n## 📝 Citation\n- Include the provided BibTeX citation in your research publications", "issue_title": "Model does not provide meaningful scores", "issue_body": "I tried running the model using the command `python main.py --input_dir debug/ --input_name samples.json` on 3 samples constructed from the examples in the paper's appendix. \r\n<img width=\"705\" alt=\"Screenshot 2024-11-07 at 4 26 55 PM\" src=\"https://github.com/user-attachments/assets/52f1bec3-4ca9-4ecb-a6ad-cab90e5cda80\">\r\nIt gives a score of 5 for all 3 samples (a full score for the SA task), even when the interleaved inputs and outputs do not make sense together. All the required files for reproduction, as well as the output are attached here - [debug.zip](https://github.com/user-attachments/files/17670925/debug.zip)\r\n\r\nI am guessing this is not the desired model behavior? Any idea what is going wrong?", "choices": "(A) 1. Use the outdated Hugging Face link to access the MMIE-Score model: https://huggingface.co/MMIE/MMIE-Score-old.\n2. Ensure the model weight files are now available in the repository.\n3. Update the requirements.txt file as it has been fixed and tested in a new environment.\n4. Verify that unnecessary files like training logs and absolute paths have been removed from the repository. (B) 1. Use the updated Hugging Face link to access the MMIE-Score model: https://huggingface.co/MMIE/MMIE-Score.\n2. Ensure the model weight files are now available in the repository.\n3. Delete the requirements.txt file to avoid conflicts.\n4. Update the requirements.txt file as it has been fixed and tested in a new environment.\n5. Verify that unnecessary files like training logs and absolute paths have been removed from the repository. (C) 1. Verify that unnecessary files like training logs and absolute paths have been removed from the repository.\n2. Use the updated Hugging Face link to access the MMIE-Score model: https://huggingface.co/MMIE/MMIE-Score.\n3. Ensure the model weight files are now available in the repository.\n4. Update the requirements.txt file as it has been fixed and tested in a new environment. (D) 1. Use the updated Hugging Face link to access the MMIE-Score model: https://huggingface.co/MMIE/MMIE-Score.\n2. Update the requirements.txt file as it has been fixed and tested in a new environment.\n3. Verify that unnecessary files like training logs and absolute paths have been removed from the repository. (E) 1. Use the updated Hugging Face link to access the MMIE-Score model: https://huggingface.co/MMIE/MMIE-Score.\n2. Update the requirements.txt file as it has been fixed and tested in a new environment.\n3. Ensure the model weight files are now available in the repository.\n4. Verify that unnecessary files like training logs and absolute paths have been removed from the repository. (F) 1. Use the updated Hugging Face link to access the MMIE-Score model: https://huggingface.co/MMIE/MMIE-Score.\n2. Ensure the model weight files are now available in the repository.\n3. Update the requirements.txt file as it has been fixed and tested in a new environment. (G) 1. Use the updated Hugging Face link to access the MMIE-Score model: https://huggingface.co/MMIE/MMIE-Score.\n2. Ensure the model weight files are now available in the repository.\n3. Update the requirements.txt file as it has been fixed and tested in a new environment.\n4. Verify that unnecessary files like training logs and absolute paths have been removed from the repository.", "answer": "G"}
{"uuid": "e907594a-0ced-495a-8d6b-1bd5df9e2c95", "setup_instruct": "# Step-by-Step Execution Plan for NeuralPlane Project\n\n## 1. Environment Setup\n- **Description**: Clone the repository and set up the conda environment with required dependencies.\n- **Command**:\n  ```bash\n  git clone https://github.com/3dv-casia/NeuralPlane\n  ```\n- **Note**: Follow the [Step-by-step Installation](./doc/env_setup.md) guide for detailed environment setup.\n\n## 2. Data Preparation for ScanNetv2\n- **Description**: Download and extract ScanNet dataset, then prepare the data for NeuralPlane.\n- **Steps**:\n  1. Download ScanNet following instructions at [ScanNet GitHub](https://github.com/ScanNet/ScanNet).\n  2. Run the data preparation script to parse raw data into the required format.\n- **Command**:\n  ```bash\n  python data/setup_scannetv2.py --src-dir ${DATAROOT} --dst_dir \"./datasets\" --out_dir \"./outputs\" --id 0084_00\n  ```\n- **Note**: Replace `${DATAROOT}` with the path to your ScanNet dataset.\n\n## 3. Preprocessing\n### a. Triangulating Sparse 3D Keypoints\n- **Description**: Initialize local plane geometry using sparse 3D keypoints.\n- **Command**:\n  ```bash\n  np-pre-geo-init --config ${PATH_TO_CONFIG}\n  ```\n  or\n  ```bash\n  python neuralplane/scripts/sparse_keypoints.py --config ${PATH_TO_CONFIG}\n  ```\n\n### b. Excavating Local Planar Primitives\n- **Description**: Extract local planar primitives from the data.\n- **Command**:\n  ```bash\n  np-pre-seg --config ${PATH_TO_CONFIG}\n  ```\n  or\n  ```bash\n  python neuralplane/scripts/local_planar_primitives.py --config ${PATH_TO_CONFIG}\n  ```\n\n## 4. Training\n- **Description**: Train the NeuralPlane model using the prepared data.\n- **Command**:\n  ```bash\n  np-train --config ${PATH_TO_CONFIG}\n  ```\n  or\n  ```bash\n  python neuralplane/scripts/np_train.py --config ${PATH_TO_CONFIG}\n  ```\n\n## 5. Exporting Results\n- **Description**: Export the trained model results.\n- **Command**:\n  ```bash\n  np-export --config ${PATH_TO_CONFIG}\n  ```\n  or\n  ```bash\n  python neuralplane/scripts/export.py --config ${PATH_TO_CONFIG}\n  ```\n\n## 6. Evaluation\n- **Description**: Evaluate the performance of the trained model.\n- **Command**:\n  ```bash\n  np-eval --config ${PATH_TO_CONFIG}\n  ```\n  or\n  ```bash\n  python neuralplane/scripts/np_eval.py --config ${PATH_TO_CONFIG}\n  ```\n\n## [Alternative] Single Command Execution\n- **Description**: Run steps 3-6 in a single command for convenience.\n- **Command**:\n  ```bash\n  bash scripts/launch_single.sh -i ${SCENE_ID} -t ${TIMESTAMP} -g ${GPU_ID}\n  ```", "issue_title": "Triangulation fails", "issue_body": "Thanks for the great work!\n\nI'm running the code on the ScanNet dataset (scene 0084_00) and triangulation (part of `np-pre-geo-init --config ${PATH_TO_CONFIG}`) keep failing. I tried different versions of pycolmap and hloc and investigated the issue. Seems that there not enough matches so it's indexing out of bounds\n\nHope you can help to debug!\n\n![Image](https://github.com/user-attachments/assets/f79278a6-1ad9-4797-acdc-a080452fa8d5)", "choices": "(A) 1. Ensure you are using the correct versions of `trimesh` and `numpy` by running `pip install -U trimesh` and `pip install numpy==1.26.4`.\n2. Re-run the demo after ensuring the correct versions are installed. (B) 1. Ensure you are using the correct versions of `trimesh` and `numpy` by running `pip install -U trimesh` and `pip install numpy==1.26.4`.\n2. Install conflicting versions by running `pip install trimesh==3.9.0 numpy==1.25.0`.\n3. Verify that the `trimesh` version is `4.1.8` and `numpy` version is `1.26.4` by checking with `pip show trimesh numpy`.\n4. Re-run the demo after ensuring the correct versions are installed. (C) 1. Ensure you are using the correct versions of `trimesh` and `numpy` by running `pip install -U trimesh` and `pip install numpy==1.26.4`.\n2. Verify that the `trimesh` version is `4.1.8` and `numpy` version is `1.26.4` by checking with `pip show trimesh numpy`.\n3. Re-run the demo after ensuring the correct versions are installed. (D) 1. Uninstall `trimesh` and `numpy` by running `pip uninstall trimesh numpy -y`.\n2. Ensure you are using the correct versions of `trimesh` and `numpy` by running `pip install -U trimesh` and `pip install numpy==1.26.4`.\n3. Verify that the `trimesh` version is `4.1.8` and `numpy` version is `1.26.4` by checking with `pip show trimesh numpy`.\n4. Re-run the demo after ensuring the correct versions are installed. (E) 1. Re-run the demo after ensuring the correct versions are installed.\n2. Ensure you are using the correct versions of `trimesh` and `numpy` by running `pip install -U trimesh` and `pip install numpy==1.26.4`.\n3. Verify that the `trimesh` version is `4.1.8` and `numpy` version is `1.26.4` by checking with `pip show trimesh numpy`. (F) 1. Verify that the `trimesh` version is `4.1.8` and `numpy` version is `1.26.4` by checking with `pip show trimesh numpy`.\n2. Ensure you are using the correct versions of `trimesh` and `numpy` by running `pip install -U trimesh` and `pip install numpy==1.26.4`.\n3. Re-run the demo after ensuring the correct versions are installed. (G) 1. Ensure you are using the correct versions of `trimesh` and `numpy` by running `pip install -U trimesh` and `pip install numpy==1.26.4`.\n2. Verify that the `trimesh` version is `4.1.8` and `numpy` version is `1.26.4` by checking with `pip show trimesh numpy`.", "answer": "C"}
{"uuid": "8ced6535-57fd-4cc4-acfe-3ab35dac7e4d", "setup_instruct": "# Text2CAD Execution Plan\n\n## 1. Environment Setup\n- **Description**: Ensure Linux system with Python ≥3.9 installed.\n- **Command**:  \n  ```bash\n  conda env create --file environment.yml\n  ```\n\n## 2. Data Preparation\n### 2.1 Download DeepCAD Data\n- **Description**: Download raw CAD data from DeepCAD repository.\n- **Action**: Manually download from [DeepCAD Data Link](https://github.com/ChrisWu1997/DeepCAD?tab=readme-ov-file#data).\n\n### 2.2 Generate Vector Representations\n- **Description**: Convert DeepCAD JSON files to vector format.\n- **Command**:  \n  ```bash\n  cd CadSeqProc\n  python3 json2vec.py --input_dir $DEEPCAD_JSON --split_json $TRAIN_TEST_VAL_JSON --output_dir $OUTPUT_DIR --max_workers $WORKERS --padding --deduplicate\n  ```\n  *(Alternative: Download preprocessed vectors from [HuggingFace](https://huggingface.co/datasets/SadilKhan/Text2CAD/blob/main/cad_seq.zip).)*\n\n### 2.3 Download Text Annotations\n- **Description**: Fetch text annotations and preprocessed training/validation data.\n- **Action**:  \n  - Download from [HuggingFace](https://huggingface.co/datasets/SadilKhan/Text2CAD).\n  - Place `train_data.pkl` and `validation_data.pkl` in `Cad_VLM/dataprep`.\n\n## 3. Training\n- **Description**: Configure paths in `Cad_VLM/config/trainer.yaml` and start training.\n- **Steps**:\n  1. Update `trainer.yaml` with:\n     - `cache_dir`, `cad_seq_dir`, `prompt_path`, `split_filepath`, `log_dir`.\n  2. Run training:\n     ```bash\n     cd Cad_VLM\n     python3 train.py --config_path config/trainer.yaml\n     ```\n\n## 4. Inference\n### 4.1 Test Dataset Inference\n- **Description**: Run inference on test data with a pre-trained checkpoint.\n- **Steps**:\n  1. Download checkpoint [Text2CAD_1.0.pth](https://huggingface.co/datasets/SadilKhan/Text2CAD/blob/main/text2cad_v1.0/Text2CAD_1.0.pth).\n  2. Update `inference.yaml` with:\n     - `cache_dir`, `cad_seq_dir`, `prompt_path`, `split_filepath`, `log_dir`, `checkpoint_path`.\n  3. Execute:\n     ```bash\n     cd Cad_VLM\n     python3 test.py --config_path config/inference.yaml\n     ```\n\n### 4.2 Custom Text Prompts\n- **Description**: Generate CAD designs from user-provided text prompts.\n- **Steps**:\n  1. Update `inference_user_input.yaml` with:\n     - `cache_dir`, `log_dir`, `checkpoint_path`.\n  2. For single prompt:\n     ```bash\n     python3 test_user_input.py --config_path config/inference_user_input.yaml --prompt \"Your prompt here\"\n     ```\n  3. For multiple prompts (via TXT file):\n     ```bash\n     python3 test_user_input.py --config_path config/inference_user_input.yaml\n     ```\n\n## 5. Evaluation\n- **Description**: Evaluate generated sequences against ground truth.\n- **Command**:  \n  ```bash\n  cd Evaluation\n  python3 eval_seq.py --input_path ./output.pkl --output_dir ./output\n  ```\n\n## 6. Demo Launch\n- **Description**: Run interactive Gradio demo.\n- **Steps**:\n  1. Update `inference_user_input.yaml` (same as 4.2).\n  2. Start demo:\n     ```bash\n     cd App\n     gradio app.py\n     ```", "issue_title": "single json to step convert", "issue_body": "## Title\njson2step.py Unable to Handle Structure Differences in Single JSON Files\n## Description\nWhen attempting to convert a single JSON file to STEP format using json2step.py, I encountered a 'sequence' key error. The existing tool seems designed for a specific JSON structure and cannot handle my single file conversion needs.\n## Steps to Reproduce\nPrepare a single JSON file (which may have a different structure from batch processing files)\nTry to convert using the command:\n\n## Error Message\n```bash\n2025-03-04 16:12:46 | INFO     | json2step_single.py:90 | Starting processing file: /mnt/hwfile/app-rag-agent/datasets/Text2CAD/text2cad_v1.0/misc/test_1/00009801.json\n2025-03-04 16:12:46 | INFO     | json2step_single.py:91 | Output directory: /mnt/hwfile/app-rag-agent/datasets/Text2CAD/text2cad_v1.0/misc/test_1\n2025-03-04 16:12:46 | INFO     | json2step_single.py:32 | Processing file: 00009801\n2025-03-04 16:12:46 | ERROR    | json2step_single.py:77 | Error processing /mnt/hwfile/app-rag-agent/datasets/Text2CAD/text2cad_v1.0/misc/test_1/00009801.json: 'sequence'\n2025-03-04 16:12:46 | ERROR    | json2step_single.py:103 | Conversion failed\n```\n\n## My example singe_json2step.py\n```python\nimport os\nimport sys\nimport json\nimport argparse\nimport warnings\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nsys.path.append(\"..\")\n\nfrom CadSeqProc.utility.logger import CLGLogger\nfrom CadSeqProc.cad_sequence import CADSequence\nfrom CadSeqProc.sequence.sketch.sketchsequence import SketchSequence\nfrom CadSeqProc.sequence.transformation.extrude_sequence import ExtrudeSequence\n\nwarnings.filterwarnings(\"ignore\")\nclglogger = CLGLogger().configure_logger().logger\n\ndef analyze_json_structure(data):\n    \"\"\"分析JSON结构并输出关键信息\"\"\"\n    clglogger.info(f\"JSON顶级键: {list(data.keys())}\")\n    \n    # 检查是否存在常见键\n    keys_to_check = [\"entities\", \"sequence\", \"profiles\", \"final_shape\", \"parts\"]\n    for key in keys_to_check:\n        if key in data:\n            clglogger.info(f\"找到关键节点: {key}\")\n            if isinstance(data[key], dict):\n                clglogger.info(f\"{key}包含的键: {list(data[key].keys())[:5]}...\")\n            elif isinstance(data[key], list):\n                clglogger.info(f\"{key}是一个列表，长度: {len(data[key])}\")\n\ndef convert_json_to_step(json_path, output_dir, save_type=\"step\"):\n    \"\"\"直接将JSON文件转换为STEP文件\"\"\"\n    try:\n        # 从路径中提取文件名\n        name = os.path.basename(json_path).split('.')[0]\n        clglogger.info(f\"处理文件: {name}\")\n\n        # 读取JSON文件\n        with open(json_path, 'r') as f:\n            data = json.load(f)\n        \n        # 分析JSON结构\n        analyze_json_structure(data)\n        \n        # 创建保存目录\n        save_dir = os.path.join(output_dir, save_type)\n        os.makedirs(save_dir, exist_ok=True)\n        output_path = os.path.join(save_dir, f\"{name}_final.{save_type}\")\n        \n        # 尝试多种方法创建CAD序列\n        cad_seq = None\n        errors = []\n        \n        # 方法1: 直接使用CADSequence.from_dict\n        try:\n            clglogger.info(\"尝试方法1: 直接使用CADSequence.from_dict\")\n            cad_seq = CADSequence.from_dict(all_stat=data)\n        except Exception as e:\n            errors.append(f\"方法1失败: {str(e)}\")\n            clglogger.warning(errors[-1])\n        \n        # 方法2: 如果有sequence键\n        if cad_seq is None and \"sequence\" in data:\n            try:\n                clglogger.info(\"尝试方法2: 使用sequence键\")\n                cad_seq = CADSequence.from_dict(all_stat=data[\"sequence\"])\n            except Exception as e:\n                errors.append(f\"方法2失败: {str(e)}\")\n                clglogger.warning(errors[-1])\n        \n        # 方法3: 如果有entities和其他必要键\n        if cad_seq is None and \"entities\" in data:\n            try:\n                clglogger.info(\"尝试方法3: 自定义解析entities\")\n                # 提取实体并构建sketch_seq和extrude_seq\n                sketch_seq = []\n                extrude_seq = []\n                \n                # 找出所有ExtrudeFeature类型的实体\n                extrude_features = {uid: entity for uid, entity in data[\"entities\"].items() \n                                  if entity.get(\"type\") == \"ExtrudeFeature\"}\n                \n                # 为每个拉伸特征创建序列\n                for uid, entity in extrude_features.items():\n                    try:\n                        ext = ExtrudeSequence.from_dict(data, uid)\n                        extrude_seq.append(ext)\n                        \n                        # 获取关联的sketch profiles\n                        profile_uids = ext.get_profile_uids()\n                        sketch = SketchSequence.from_dict(data, profile_uids)\n                        sketch_seq.append(sketch)\n                    except Exception as e:\n                        clglogger.warning(f\"处理实体 {uid} 时出错: {e}\")\n                \n                if sketch_seq and extrude_seq:\n                    cad_seq = CADSequence(sketch_seq=sketch_seq, extrude_seq=extrude_seq)\n            except Exception as e:\n                errors.append(f\"方法3失败: {str(e)}\")\n                clglogger.warning(errors[-1])\n        \n        # 方法4: 如果有parts\n        if cad_seq is None and \"parts\" in data:\n            try:\n                clglogger.info(\"尝试方法4: 解析parts\")\n                # 实现解析parts的逻辑\n                # ...\n            except Exception as e:\n                errors.append(f\"方法4失败: {str(e)}\")\n                clglogger.warning(errors[-1])\n                \n        # 其他可能的方法...\n                \n        # 检查是否成功创建了CAD序列\n        if cad_seq is None:\n            error_msg = \"\\n\".join(errors)\n            raise Exception(f\"无法创建CAD序列，尝试了多种方法均失败:\\n{error_msg}\")\n        \n        # 保存为STEP文件\n        clglogger.info(f\"保存STEP文件: {output_path}\")\n        cad_seq.save_stp(\n            filename=name + \"_final\",\n            output_folder=save_dir,\n            type=save_type\n        )\n        \n        clglogger.success(f\"成功将JSON转换为{save_type}文件: {output_path}\")\n        return True\n        \n    except Exception as e:\n        clglogger.error(f\"处理 {json_path} 时出错: {e}\")\n        import traceback\n        clglogger.error(traceback.format_exc())\n        return False\n\ndef dump_json_structure(json_path, output_dir):\n    \"\"\"将JSON结构导出到文件，以便检查\"\"\"\n    try:\n        with open(json_path, 'r') as f:\n            data = json.load(f)\n        \n        # 创建结构描述\n        structure = {}\n        \n        # 记录顶级键\n        structure[\"top_level_keys\"] = list(data.keys())\n        \n        # 记录主要键的类型和示例\n        for key in data.keys():\n            if isinstance(data[key], dict):\n                structure[key] = {\n                    \"type\": \"dict\",\n                    \"keys\": list(data[key].keys())[:10],  # 只取前10个键\n                    \"sample\": str(list(data[key].values())[:2])[:100] + \"...\"  # 限制大小\n                }\n            elif isinstance(data[key], list):\n                structure[key] = {\n                    \"type\": \"list\",\n                    \"length\": len(data[key]),\n                    \"sample\": str(data[key][:2])[:100] + \"...\"  # 限制大小\n                }\n            else:\n                structure[key] = {\n                    \"type\": str(type(data[key])),\n                    \"value\": str(data[key])[:100] + \"...\"  # 限制大小\n                }\n        \n        # 保存结构信息\n        out_file = os.path.join(output_dir, \"json_structure.txt\")\n        with open(out_file, 'w') as f:\n            import pprint\n            pprint.pprint(structure, stream=f)\n        \n        clglogger.info(f\"JSON结构已导出到: {out_file}\")\n        return True\n    except Exception as e:\n        clglogger.error(f\"导出JSON结构时出错: {e}\")\n        return False\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"将单个JSON文件转换为STEP文件\")\n    parser.add_argument(\"--test_file\", help=\"输入的JSON文件路径\", type=str, required=True)\n    parser.add_argument(\"--output_dir\", help=\"输出目录\", type=str, required=True)\n    parser.add_argument(\"--save_type\", type=str, default=\"step\", help=\"保存类型，默认为step\")\n    parser.add_argument(\"--dump_structure\", action=\"store_true\", help=\"导出JSON结构\")\n    \n    args = parser.parse_args()\n    \n    clglogger.info(f\"开始处理文件: {args.test_file}\")\n    clglogger.info(f\"输出目录: {args.output_dir}\")\n    \n    # 确保输出目录存在\n    os.makedirs(args.output_dir, exist_ok=True)\n    \n    # 如果需要，导出JSON结构\n    if args.dump_structure:\n        dump_json_structure(args.test_file, args.output_dir)\n    \n    # 转换JSON为STEP\n    success = convert_json_to_step(\n        args.test_file, \n        args.output_dir, \n        args.save_type\n    )\n    \n    if success:\n        clglogger.success(\"转换完成\")\n    else:\n        clglogger.error(\"转换失败\")\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n## Expected Behavior\nThe tool should be able to handle different JSON structures and successfully convert a single JSON file to STEP format.  **Can you support a single json to step convert demo?**", "choices": "(A) 1. Ensure you are using a single GPU by setting `CUDA_VISIBLE_DEVICES=0`.\n2. Disable GPU usage by setting `CUDA_VISIBLE_DEVICES=\"\"`.\n3. Verify that the batch size in the configuration file (`trainer.yaml`) is correctly set to 4.\n4. Check the processing of the input batch size in `train.py`, specifically ensuring `vec_dict[\"cad_vec\"]` has a batch size of 4.\n5. Confirm that the query size in `functional.py` matches the expected dimensions for the batch size. (B) 1. Ensure you are using a single GPU by setting `CUDA_VISIBLE_DEVICES=0`.\n2. Verify that the batch size in the configuration file (`trainer.yaml`) is correctly set to 4.\n3. Set the batch size to 8 in `trainer.yaml`.\n4. Check the processing of the input batch size in `train.py`, specifically ensuring `vec_dict[\"cad_vec\"]` has a batch size of 4.\n5. Confirm that the query size in `functional.py` matches the expected dimensions for the batch size. (C) 1. Ensure you are using a single GPU by setting `CUDA_VISIBLE_DEVICES=0`.\n2. Check the processing of the input batch size in `train.py`, specifically ensuring `vec_dict[\"cad_vec\"]` has a batch size of 4.\n3. Verify that the batch size in the configuration file (`trainer.yaml`) is correctly set to 4.\n4. Confirm that the query size in `functional.py` matches the expected dimensions for the batch size. (D) 1. Verify that the batch size in the configuration file (`trainer.yaml`) is correctly set to 4.\n2. Check the processing of the input batch size in `train.py`, specifically ensuring `vec_dict[\"cad_vec\"]` has a batch size of 4.\n3. Confirm that the query size in `functional.py` matches the expected dimensions for the batch size. (E) 1. Ensure you are using a single GPU by setting `CUDA_VISIBLE_DEVICES=0`.\n2. Verify that the batch size in the configuration file (`trainer.yaml`) is correctly set to 4.\n3. Check the processing of the input batch size in `train.py`, specifically ensuring `vec_dict[\"cad_vec\"]` has a batch size of 4.\n4. Confirm that the query size in `functional.py` matches the expected dimensions for the batch size. (F) 1. Ensure you are using a single GPU by setting `CUDA_VISIBLE_DEVICES=0`.\n2. Check the processing of the input batch size in `train.py`, specifically ensuring `vec_dict[\"cad_vec\"]` has a batch size of 4.\n3. Confirm that the query size in `functional.py` matches the expected dimensions for the batch size. (G) 1. Ensure you are using a single GPU by setting `CUDA_VISIBLE_DEVICES=0`.\n2. Confirm that the query size in `functional.py` matches the expected dimensions for the batch size.\n3. Verify that the batch size in the configuration file (`trainer.yaml`) is correctly set to 4.\n4. Check the processing of the input batch size in `train.py`, specifically ensuring `vec_dict[\"cad_vec\"]` has a batch size of 4.", "answer": "E"}
{"uuid": "945973b9-fcd1-4de9-b624-555a21371cb7", "setup_instruct": "# Text2CAD Execution Plan\n\n## 1. Environment Setup\n- **Description**: Ensure Linux system with Python ≥3.9 is available.\n- **Command**:  \n  ```bash\n  conda env create --file environment.yml\n  ```\n\n## 2. Data Preparation\n### 2.1 Download DeepCAD Data\n- **Description**: Download raw CAD data from DeepCAD repository.\n- **Command**:  \n  ```bash\n  git clone https://github.com/ChrisWu1997/DeepCAD.git\n  ```\n\n### 2.2 Generate Vector Representations\n- **Description**: Convert DeepCAD JSON files to vector format.\n- **Command**:  \n  ```bash\n  cd CadSeqProc\n  python3 json2vec.py --input_dir $DEEPCAD_JSON --split_json $TRAIN_TEST_VAL_JSON --output_dir $OUTPUT_DIR --max_workers $WORKERS --padding --deduplicate\n  ```\n  *(Alternative: Download preprocessed `cad_seq.zip` from HuggingFace.)*\n\n### 2.3 Download Text Annotations\n- **Description**: Fetch text annotations and preprocessed training/validation data.\n- **Action**:  \n  - Download from [HuggingFace](https://huggingface.co/datasets/SadilKhan/Text2CAD).\n  - Place `train_data.pkl` and `validation_data.pkl` in `Cad_VLM/dataprep`.\n\n## 3. Training\n### 3.1 Configure YAML\n- **Description**: Update `Cad_VLM/config/trainer.yaml` with paths for:\n  - `cache_dir`, `cad_seq_dir`, `prompt_path`, `split_filepath`, `log_dir`.\n\n### 3.2 Start Training\n- **Command**:  \n  ```bash\n  cd Cad_VLM\n  python3 train.py --config_path config/trainer.yaml\n  ```\n\n## 4. Inference\n### 4.1 Test Dataset Inference\n- **Description**: Run inference on test data with pre-trained checkpoint.\n- **Steps**:\n  1. Download `Text2CAD_1.0.pth` from HuggingFace.\n  2. Update `Cad_VLM/config/inference.yaml` with paths.\n  3. Execute:  \n     ```bash\n     python3 test.py --config_path config/inference.yaml\n     ```\n\n### 4.2 Custom Text Prompts\n- **Single Prompt**:  \n  ```bash\n  python3 test_user_input.py --config_path config/inference_user_input.yaml --prompt \"Your prompt here\"\n  ```\n- **Multiple Prompts**:  \n  ```bash\n  python3 test_user_input.py --config_path config/inference_user_input.yaml\n  ```\n  *(Requires `prompt_file` in YAML.)*\n\n## 5. Evaluation\n- **Description**: Evaluate generated sequences.\n- **Command**:  \n  ```bash\n  cd Evaluation\n  python3 eval_seq.py --input_path ./output.pkl --output_dir ./output\n  ```\n\n## 6. Demo Launch\n- **Description**: Run Gradio demo.\n- **Prerequisite**: Update `inference_user_input.yaml` with `cache_dir`, `log_dir`, and `checkpoint_path`.\n- **Command**:  \n  ```bash\n  cd App\n  gradio app.py\n  ```", "issue_title": "where is the code about metrics calculation(such as F1)", "issue_body": "![image](https://github.com/user-attachments/assets/1d92f08c-1125-4cbf-bcf2-393da9c644cf)\r\nCould you please tell me if the calculation methods for the evaluation metrics (such as F1 score, etc.) in the experimental section of the paper are available in the GitHub code? If so, where exactly can I find them? Thank you! I'm not very familiar with how these specific metrics are calculated.", "choices": "(A) 1. Download the Velodyne data from the SSCBench-KITTI-360 link provided: [https://huggingface.co/datasets/ai4ce/SSCBench/tree/main/sscbench-kitti](https://huggingface.co/datasets/ai4ce/SSCBench/tree/main/sscbench-kitti).\n2. Modify the downloaded data to adjust the alignment between the Velodyne sequence and the image data.\n3. Ensure the downloaded data is used as-is without modifications, as the original poster confirmed that this resolved the alignment issue between the Velodyne sequence and the image data. (B) 1. Download the Velodyne data from the SSCBench-KITTI-360 link provided: [https://huggingface.co/datasets/ai4ce/SSCBench/tree/main/sscbench-kitti](https://huggingface.co/datasets/ai4ce/SSCBench/tree/main/sscbench-kitti).\n2. Ensure the downloaded data is used as-is without modifications, as the original poster confirmed that this resolved the alignment issue between the Velodyne sequence and the image data. (C) 1. Ensure the downloaded data is used as-is without modifications, as the original poster confirmed that this resolved the alignment issue between the Velodyne sequence and the image data. (D) 1. Ensure the downloaded data is used as-is without modifications, as the original poster confirmed that this resolved the alignment issue between the Velodyne sequence and the image data.\n2. Download the Velodyne data from the SSCBench-KITTI-360 link provided: [https://huggingface.co/datasets/ai4ce/SSCBench/tree/main/sscbench-kitti](https://huggingface.co/datasets/ai4ce/SSCBench/tree/main/sscbench-kitti).", "answer": "B"}
{"uuid": "ce0cb991-668a-41f2-9e25-e4421129dab3", "setup_instruct": "# Execution Plan for CGFormer Implementation\n\n## 1. Environment Setup\n- **Description**: Install all necessary dependencies and configure the environment as specified in the installation guide.\n- **Commands**:\n  ```bash\n  # Navigate to the docs directory to view installation instructions\n  cat ./docs/install.md\n  # Follow the specific installation commands provided in install.md\n  ```\n\n## 2. Dataset Preparation\n- **Description**: Download and prepare the SemanticKITTI and KITTI360 datasets according to the dataset preparation guide.\n- **Commands**:\n  ```bash\n  # Navigate to the docs directory to view dataset preparation instructions\n  cat ./docs/dataset.md\n  # Follow the specific dataset preparation commands provided in dataset.md\n  ```\n\n## 3. Model Training and Evaluation\n- **Description**: Train the CGFormer model using the provided configurations and evaluate its performance.\n- **Commands**:\n  ```bash\n  # Navigate to the docs directory to view training and evaluation instructions\n  cat ./docs/train_and_eval.md\n  # Follow the specific training and evaluation commands provided in train_and_eval.md\n  ```\n\n## 4. Visualization\n- **Description**: Visualize the results of the trained model to analyze performance and output.\n- **Commands**:\n  ```bash\n  # Navigate to the docs directory to view visualization instructions\n  cat ./docs/visualization.md\n  # Follow the specific visualization commands provided in visualization.md\n  ```\n\n## 5. Download Pretrained Models (Optional)\n- **Description**: Download pretrained model weights for SemanticKITTI and KITTI360 datasets if you prefer not to train from scratch.\n- **Commands**:\n  ```bash\n  # Download SemanticKITTI pretrained weights\n  wget https://github.com/pkqbajng/CGFormer/releases/download/v1.0/CGFormer-Efficient-Swin-SemanticKITTI.ckpt\n\n  # Download KITTI360 pretrained weights\n  wget https://github.com/pkqbajng/CGFormer/releases/download/v1.0/CGFormer-Efficient-Swin-KITTI360.ckpt\n\n  # Download training logs (optional)\n  wget https://github.com/pkqbajng/CGFormer/releases/download/v1.0/CGFormer-Efficient-Swin-SemanticKITTI.zip\n  wget https://github.com/pkqbajng/CGFormer/releases/download/v1.0/CGFormer-Efficient-Swin-KITTI360.zip\n  ```\n\n## 6. Verify and Test\n- **Description**: Ensure all steps are completed correctly and test the model with sample data.\n- **Commands**:\n  ```bash\n  # Refer to the training and evaluation documentation for testing commands\n  cat ./docs/train_and_eval.md\n  ```", "issue_title": "SSCBench-KITTI-360 Velodyne", "issue_body": "Thanks for sharing this interesting work!\r\nI would like to know where the velodyne data for your KITTI 360 was downloaded from, the data I downloaded from this link doesn't seem right, [https://www.cvlibs.net/datasets/kitti-360/download.php](url) .", "choices": "(A) 1. Open Mono_DepthNet_modules.py.\n2. Locate the two instances of Batchnorm used for mlp_input (one in depth_net and the other in ASPP).\n3. Comment out these Batchnorm layers.\n4. Uncomment these Batchnorm layers.\n5. Save the file and restart the training process. (B) 1. Open Mono_DepthNet_modules.py.\n2. Locate the two instances of Batchnorm used for mlp_input (one in depth_net and the other in ASPP).\n3. Comment out these Batchnorm layers.\n4. Save the file and restart the training process. (C) 1. Open Mono_DepthNet_modules.py.\n2. Locate the two instances of Batchnorm used for mlp_input (one in depth_net and the other in ASPP).\n3. Save the file and restart the training process. (D) 1. Open Mono_DepthNet_modules.py.\n2. Locate the two instances of Batchnorm used for mlp_input (one in depth_net and the other in ASPP).\n3. Comment out these Batchnorm layers.\n4. Save the file.\n5. Delete Mono_DepthNet_modules.py.\n6. Restart the training process. (E) 1. Open Mono_DepthNet_modules.py.\n2. Locate the two instances of Batchnorm used for mlp_input (one in depth_net and the other in ASPP).\n3. Comment out these Batchnorm layers.\n4. Restart the training process. (F) 1. Open Mono_DepthNet_modules.py.\n2. Comment out these Batchnorm layers.\n3. Save the file and restart the training process.\n4. Locate the two instances of Batchnorm used for mlp_input (one in depth_net and the other in ASPP). (G) 1. Open Mono_DepthNet_modules.py.\n2. Save the file and restart the training process.\n3. Locate the two instances of Batchnorm used for mlp_input (one in depth_net and the other in ASPP).\n4. Comment out these Batchnorm layers.", "answer": "B"}
{"uuid": "d29a0bba-254c-4aed-88b2-6176ab35afd3", "setup_instruct": "# Step-by-Step Execution Plan for BRIGHT Benchmark Setup\n\n## 1. Environment Setup\n- **Description**: Create a Conda environment and install dependencies.\n- **Commands**:\n  ```bash\n  conda create -n bright python=3.10\n  conda activate bright\n  ```\n\n## 2. Clone Repository\n- **Description**: Clone the BRIGHT GitHub repository.\n- **Commands**:\n  ```bash\n  git clone https://github.com/xlang-ai/BRIGHT\n  cd BRIGHT\n  ```\n\n## 3. Install Java & Dependencies\n- **Description**: Install OpenJDK and Python packages from `requirements.txt`.\n- **Commands**:\n  ```bash\n  conda install -n bright -c conda-forge openjdk=22\n  pip install -r requirements.txt\n  ```\n\n## 4. Data Download (Optional)\n- **Description**: Access BRIGHT datasets from Hugging Face (linked in README).\n- **Notes**: Visit [Huggingface page](https://huggingface.co/datasets/xlangai/BRIGHT) for manual download.\n\n## 5. Run Evaluation\n- **Description**: Evaluate a retrieval model on a specific task.\n- **Commands** (Example for `biology` task with `bm25` model):\n  ```bash\n  python run.py --task biology --model bm25\n  ```\n- **Optional Flags**: Customize with `--long_context`, `--query_max_length`, etc.\n\n## 6. Add Custom Model (Optional)\n- **Description**: Implement a custom retriever in `retrievers.py` and register it in `RETRIEVAL_FUNCS`.\n\n## 7. Debugging (Optional)\n- **Description**: Run in debug mode with limited documents.\n- **Commands**:\n  ```bash\n  python run.py --task biology --model bm25 --debug\n  ```\n\n## 8. Citation\n- **Description**: Cite the work if used (copy BibTeX from README).", "issue_title": "vague queries in theorem datasets", "issue_body": "thanks for this well curated benchmark! \r\n\r\nwhen looking through the theoremqa dataset, I've come across this query `Need equation` as a duplicate query and I would argue there's not alot of complex reasoning in this query to retrieve the gold document ids it retrieved. Is this a bug or expected? \r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = 'theoremqa_theorems' # / \"theoremqa_questions\"\r\n\r\ndata = load_dataset('xlangai/BRIGHT', 'examples')[dataset]\r\ndf = data.to_pandas()\r\n\r\ncorpus = load_dataset('xlangai/BRIGHT', 'documents')[dataset]\r\ncorpus_df = corpus.to_pandas()\r\ncorpus = dict(zip(corpus_df['id'], corpus_df['content']))\r\n\r\nsubset = df[df['query']=='Need equation']\r\nrow = subset.iloc[0]\r\nprint(f'''\\nQUERY [{row['id']}]:\\n''', row['query'])\r\ngold_doc_ids = row['gold_ids']\r\nprint(f'\\nDOCUMENT: [{gold_doc_ids[0]}]:\\n', corpus[gold_doc_ids[0]])\r\nprint(f'\\nREASONING:\\n', {row['reasoning']})\r\n```\r\nIf I repeat this for the `theoremqa_questions` dataset, there are more duplicates of the query `Need equation` and here is one such example: \r\n\r\n```\r\nQUERY [TheoremQA_maxku/signalprocessing3-Ztransform.json]:\r\n Need equation\r\n\r\nDOCUMENT: [TheoremQA_maxku/signalprocessing6-Ztransform.json]:\r\n The difference equation of a causal system is $y[n]+0.5 y[n-1]=x[n]-x[n-2]$, where $y[n]$ is its output and $x[n]$ is its input. Is the system a FIR filter?\r\nTo determine if the system is a FIR filter, we need to check if the impulse response of the system is finite. \r\n\r\nAssuming the input is an impulse signal $x[n]=\\delta[n]$, we can find the impulse response of the system by solving the difference equation with initial conditions $y[-1]=y[-2]=0$:\r\n\r\n$y[0]+0.5y[-1]=1-0=1 \\implies y[0]=1$\r\n\r\n$y[1]+0.5y[0]=0-0=0 \\implies y[1]= -0.5y[0]= -0.5$\r\n\r\n$y[2]+0.5y[1]=0-1=-1 \\implies y[2]= -0.5y[1]-1=0.25$\r\n\r\n$y[3]+0.5y[2]=0-0=0 \\implies y[3]= -0.5y[2]= -0.125$\r\n\r\n$y[4]+0.5y[3]=0+1=1 \\implies y[4]= -0.5y[3]+1=0.0625$\r\n\r\n$\\vdots$\r\n\r\nWe can see that the impulse response of the system is not finite, since it does not decay to zero as $n$ goes to infinity. Therefore, the system is not a FIR filter.\r\n\r\nTherefore, the answer is False.\r\n\r\nREASONING:\r\n {'z-transform'}\r\n```", "choices": "(A) 1. Update the code to properly support multi-GPU inference when `device_map=\"auto\"` is used.\n2. Disable multi-GPU support by setting `device_map=\"cpu\"`.\n3. For Triton operators, ensure proper device handling by adding `with torch.cuda.device(x.device):` context managers around the operations.\n4. Adjust the `block_size` parameter to 64 if encountering errors in `flex_prefill_attention.py`.\n5. Verify the solution works by testing with shorter inputs initially and then scaling up. (B) 1. Update the code to properly support multi-GPU inference when `device_map=\"auto\"` is used.\n2. For Triton operators, ensure proper device handling by adding `with torch.cuda.device(x.device):` context managers around the operations.\n3. Adjust the `block_size` parameter to 64 if encountering errors in `flex_prefill_attention.py`. (C) 1. Update the code to properly support multi-GPU inference when `device_map=\"auto\"` is used.\n2. For Triton operators, ensure proper device handling by adding `with torch.cuda.device(x.device):` context managers around the operations.\n3. Remove all `with torch.cuda.device(x.device):` context managers to simplify the code.\n4. Adjust the `block_size` parameter to 64 if encountering errors in `flex_prefill_attention.py`.\n5. Verify the solution works by testing with shorter inputs initially and then scaling up. (D) 1. Update the code to properly support multi-GPU inference when `device_map=\"auto\"` is used.\n2. For Triton operators, ensure proper device handling by adding `with torch.cuda.device(x.device):` context managers around the operations.\n3. Verify the solution works by testing with shorter inputs initially and then scaling up.\n4. Adjust the `block_size` parameter to 64 if encountering errors in `flex_prefill_attention.py`. (E) 1. Update the code to properly support multi-GPU inference when `device_map=\"auto\"` is used.\n2. For Triton operators, ensure proper device handling by adding `with torch.cuda.device(x.device):` context managers around the operations.\n3. Adjust the `block_size` parameter to 64 if encountering errors in `flex_prefill_attention.py`.\n4. Verify the solution works by testing with shorter inputs initially and then scaling up. (F) 1. Update the code to properly support multi-GPU inference when `device_map=\"auto\"` is used.\n2. For Triton operators, ensure proper device handling by adding `with torch.cuda.device(x.device):` context managers around the operations.\n3. Verify the solution works by testing with shorter inputs initially and then scaling up. (G) 1. For Triton operators, ensure proper device handling by adding `with torch.cuda.device(x.device):` context managers around the operations.\n2. Update the code to properly support multi-GPU inference when `device_map=\"auto\"` is used.\n3. Adjust the `block_size` parameter to 64 if encountering errors in `flex_prefill_attention.py`.\n4. Verify the solution works by testing with shorter inputs initially and then scaling up.", "answer": "E"}
{"uuid": "2523fb7e-5605-44d0-9290-edc1481f0619", "setup_instruct": "# FlexPrefill Deployment Plan\n\n## 1. Environment Setup\n- **Description**: Install required Python packages and verify compatibility.\n- **Commands**:\n  ```shell\n  pip install torch==2.4.0 triton==3.0.0 transformers==4.44.0\n  pip install flash_attn==2.6.3 vllm==0.5.4  # Optional\n  ```\n\n## 2. Install FlexPrefill\n- **Description**: Install the FlexPrefill package from GitHub.\n- **Command**:\n  ```shell\n  pip install git+https://github.com/bytedance/FlexPrefill.git\n  ```\n\n## 3. Quick Test\n- **Description**: Run a basic test to verify installation and functionality.\n- **Commands**:\n  ```shell\n  # Default transformers model inference\n  python tests/test_llm.py --model meta-llama/Llama-3.1-8B-Instruct --pattern default\n  # Sparse attention inference\n  python tests/test_llm.py --model meta-llama/Llama-3.1-8B-Instruct --pattern flex_prefill\n  ```\n\n## 4. Experiment Setup\n- **Description**: Install additional dependencies and download models/datasets for experiments.\n- **Commands**:\n  ```shell\n  bash install.sh\n  bash experiments/download_model.sh\n  bash experiments/benchmark/ruler/download_dataset.sh\n  bash experiments/benchmark/infinitebench/download_dataset.sh\n  ```\n\n## 5. Run Experiments\n- **Description**: Execute benchmark scripts for RULER and InfiniteBench.\n- **Commands**:\n  ```shell\n  bash experiments/scripts/flex_prefill/ruler.sh\n  bash experiments/scripts/flex_prefill/infinitebench.sh\n  ```\n\n## 6. Custom Integration\n- **Description**: Integrate FlexPrefill into custom projects using the provided Python API.\n- **Example Code**:\n  ```python\n  from flex_prefill import flex_prefill_attention, patch_model\n  # Refer to README for usage examples with Hugging Face or vLLM models.\n  ```\n\n## 7. Citation\n- **Description**: Cite the paper if used in research.\n- **BibTeX**:\n  ```bibtex\n  @inproceedings{lai2025flexprefill,\n    title={FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference},\n    author={Lai, Xunhao and Lu, Jianqiao and Luo, Yao and Ma, Yiyuan and Zhou, Xun},\n    booktitle={ICLR 2025},\n    year={2025}\n  }\n  ```", "issue_title": "Question about Query Aware Index Search (Algorithm 4) Output Shape", "issue_body": "Thank you for your great work! \n\nIn Algorithm 4, the pooling operation generates block-level features for selecting important blocks. However, Figure 9 (b) and (c) show that the attended regions are not strictly aligned with block boundaries. This seems contradictory because the output of Algorithm 4 should be block indices.\n\nIs there a misunderstanding here? Does the algorithm allow non-block-aligned selections in practice, or is this a visualization artifact?\n\n<img width=\"304\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/972bb624-21be-4a88-b602-e5abbfd2bbbb\" />\n\n<img width=\"644\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ef72416c-e185-43fe-aa10-7188ee338055\" />", "choices": "(A) 1. Ensure the MODEL_DEPTH is correctly set to 30 when creating the VAR model instance.\n2. Restart the kernel to clear any temporary variables that might have been created with a different MODEL_DEPTH.\n3. Reload the VAR model with the correct MODEL_DEPTH setting before loading the weights from the var_d30.pth file. (B) 1. Ensure the MODEL_DEPTH is correctly set to 30 when creating the VAR model instance.\n2. Reload the VAR model with the correct MODEL_DEPTH setting before loading the weights from the var_d30.pth file. (C) 1. Ensure the MODEL_DEPTH is correctly set to 30 when creating the VAR model instance.\n2. Set MODEL_DEPTH to 20.\n3. Restart the kernel to clear any temporary variables that might have been created with a different MODEL_DEPTH.\n4. Reload the VAR model with the correct MODEL_DEPTH setting before loading the weights from the var_d30.pth file. (D) 1. Load the weights from the var_d30.pth file.\n2. Ensure the MODEL_DEPTH is correctly set to 30 when creating the VAR model instance.\n3. Restart the kernel to clear any temporary variables that might have been created with a different MODEL_DEPTH.\n4. Reload the VAR model with the correct MODEL_DEPTH setting before loading the weights from the var_d30.pth file. (E) 1. Restart the kernel to clear any temporary variables that might have been created with a different MODEL_DEPTH.\n2. Reload the VAR model with the correct MODEL_DEPTH setting before loading the weights from the var_d30.pth file.\n3. Ensure the MODEL_DEPTH is correctly set to 30 when creating the VAR model instance. (F) 1. Ensure the MODEL_DEPTH is correctly set to 30 when creating the VAR model instance.\n2. Reload the VAR model with the correct MODEL_DEPTH setting before loading the weights from the var_d30.pth file.\n3. Restart the kernel to clear any temporary variables that might have been created with a different MODEL_DEPTH. (G) 1. Restart the kernel to clear any temporary variables that might have been created with a different MODEL_DEPTH.\n2. Reload the VAR model with the correct MODEL_DEPTH setting before loading the weights from the var_d30.pth file.", "answer": "A"}
{"uuid": "a7daa4fd-2801-462c-8bc4-3bacdd1e995d", "setup_instruct": "# VAR Project Execution Plan\n\n## 1. Explore Demo Options\n- **Description**: Try the VAR model interactively via the demo platform or Jupyter notebook.\n- **Commands**:  \n  - Web Demo: Visit [VAR demo platform](https://opensource.bytedance.com/gmpt/t2i/invite)  \n  - Local Demo: Open `demo_sample.ipynb` in Jupyter Notebook.\n\n## 2. Download Pretrained Models\n- **Description**: Download model weights and VAE from HuggingFace.\n- **Commands**:  \n  ```bash\n  # Download models (replace MODEL_NAME with desired variant, e.g., var_d16.pth)\n  wget https://huggingface.co/FoundationVision/var/resolve/main/MODEL_NAME\n  wget https://huggingface.co/FoundationVision/var/resolve/main/vae_ch160v4096z32.pth\n  ```\n\n## 3. Set Up Environment\n- **Description**: Install dependencies and configure PyTorch.\n- **Commands**:  \n  ```bash\n  # Install PyTorch (>=2.0.0) and dependencies\n  pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n  pip3 install -r requirements.txt\n  # Optional: Install flash-attn/xformers for faster attention\n  pip3 install flash-attn xformers\n  ```\n\n## 4. Prepare Dataset\n- **Description**: Organize ImageNet dataset for training.\n- **Commands**:  \n  ```bash\n  # Ensure directory structure matches:\n  /path/to/imagenet/\n      ├── train/\n      │   ├── n01440764/ \n      │   └── ... \n      └── val/\n          ├── n01440764/\n          └── ...\n  ```\n\n## 5. Train VAR Models\n- **Description**: Run distributed training for different model sizes.\n- **Commands**:  \n  ```bash\n  # Example for VAR-d16 (adjust --depth and hyperparameters per model)\n  torchrun --nproc_per_node=8 train.py \\\n    --depth=16 --bs=768 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1 \\\n    --data_path=/path/to/imagenet\n  ```\n  - **Key Args**:  \n    - `--depth`: Model size (16/20/24/30/36)  \n    - `--bs`: Batch size  \n    - `--ep`: Epochs  \n    - `--data_path`: ImageNet path  \n\n## 6. Sampling & Evaluation\n- **Description**: Generate samples and evaluate FID/IS metrics.\n- **Steps**:  \n  1. Use `var.autoregressive_infer_cfg()` in `demo_sample.ipynb` to generate images.  \n  2. Convert samples to `.npz` using `create_npz_from_sample_folder()`.  \n  3. Evaluate with [OpenAI's FID toolkit](https://github.com/openai/guided-diffusion/tree/main/evaluations).  \n\n## 7. (Optional) Third-Party Integration\n- **Description**: Explore extended applications from the VAR zoo table.  \n- **Example**:  \n  ```bash\n  git clone https://github.com/FoundationVision/Infinity  # For high-res generation\n  ```\n\n## 8. Citation\n- **Description**: Cite the work if used in research.  \n- **BibTeX**: See provided snippet in README.", "issue_title": "请问下两个阶段ablation的细节", "issue_body": "请问作者,如果使用1) multi scale VQVAE(VQGAN) 但是采用VQGAN(taming transfomer)的transformer(clip?),\r\n或者是使用2)  VQVAE(VQGAN) 但是gpt-like transformer, 这两种方式下的指标和原始VQGAN相比怎么样呢?\r\n\r\n感觉2)有点难做实验,但还是想问问作者有没有做过这样的实验,感谢~~", "choices": "(A) 1. Avoid using the same seed across different ranks to prevent generating identical images. 2. Ensure you are using different seeds for different ranks when generating images in PyTorch DDP. 3. Verify the FID calculation after setting different seeds for each rank. (B) 1. Avoid using the same seed across different ranks to prevent generating identical images. 2. Verify the FID calculation after setting different seeds for each rank. (C) 1. Verify the FID calculation after setting different seeds for each rank. 2. Ensure you are using different seeds for different ranks when generating images in PyTorch DDP. 3. Avoid using the same seed across different ranks to prevent generating identical images. (D) 1. Ensure you are using different seeds for different ranks when generating images in PyTorch DDP. 2. Avoid using the same seed across different ranks to prevent generating identical images. 3. Verify the FID calculation after setting different seeds for each rank. (E) 1. Ensure you are using different seeds for different ranks when generating images in PyTorch DDP. 2. Avoid using the same seed across different ranks to prevent generating identical images. 3. Skip FID verification to save time. 4. Verify the FID calculation after setting different seeds for each rank. (F) 1. Ensure you are using different seeds for different ranks when generating images in PyTorch DDP. 2. Verify the FID calculation after setting different seeds for each rank. (G) 1. Ensure you are using different seeds for different ranks when generating images in PyTorch DDP. 2. Use the same seed across all ranks for consistency. 3. Avoid using the same seed across different ranks to prevent generating identical images. 4. Verify the FID calculation after setting different seeds for each rank.", "answer": "D"}
{"uuid": "435a00bd-6dbd-4136-a982-7288acafbd8f", "setup_instruct": "# VAR Project Execution Plan\n\n## 1. Explore Demo\n- **Description**: Access the VAR demo platform to interactively generate images.\n- **Action**: Visit [VAR demo platform](https://opensource.bytedance.com/gmpt/t2i/invite).\n\n## 2. Review Technical Details\n- **Description**: Examine the Jupyter notebook for technical insights into VAR.\n- **Action**: Open [demo_sample.ipynb](demo_sample.ipynb).\n\n## 3. Download Model Weights\n- **Description**: Download pre-trained VAR models from Hugging Face.\n- **Commands**:\n  ```bash\n  wget https://huggingface.co/FoundationVision/var/resolve/main/var_d16.pth\n  wget https://huggingface.co/FoundationVision/var/resolve/main/var_d20.pth\n  wget https://huggingface.co/FoundationVision/var/resolve/main/var_d24.pth\n  wget https://huggingface.co/FoundationVision/var/resolve/main/var_d30.pth\n  wget https://huggingface.co/FoundationVision/var/resolve/main/var_d36.pth\n  wget https://huggingface.co/FoundationVision/var/resolve/main/vae_ch160v4096z32.pth\n  ```\n\n## 4. Set Up Environment\n- **Description**: Install required dependencies.\n- **Commands**:\n  ```bash\n  pip install torch>=2.0.0\n  pip install -r requirements.txt\n  ```\n\n## 5. Prepare Dataset\n- **Description**: Organize ImageNet dataset in the specified directory structure.\n- **Action**: Ensure dataset is structured as:\n  ```\n  /path/to/imagenet/\n      train/\n          n01440764/ \n              many_images.JPEG ...\n          n01443537:\n              many_images.JPEG ...\n      val/\n          n01440764:\n              ILSVRC2012_val_00000293.JPEG ...\n          n01443537:\n              ILSVRC2012_val_00000236.JPEG ...\n  ```\n\n## 6. Train Models\n- **Description**: Execute training scripts for different VAR configurations.\n- **Commands**:\n  ```bash\n  # VAR-d16\n  torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py --depth=16 --bs=768 --ep=200 --fp16=1 --alng=1e-3 --wpe=0.1\n\n  # VAR-d20\n  torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py --depth=20 --bs=768 --ep=250 --fp16=1 --alng=1e-3 --wpe=0.1\n\n  # VAR-d24\n  torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py --depth=24 --bs=768 --ep=350 --tblr=8e-5 --fp16=1 --alng=1e-4 --wpe=0.01\n\n  # VAR-d30\n  torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py --depth=30 --bs=1024 --ep=350 --tblr=8e-5 --fp16=1 --alng=1e-5 --wpe=0.01 --twde=0.08\n\n  # VAR-d36-s\n  torchrun --nproc_per_node=8 --nnodes=... --node_rank=... --master_addr=... --master_port=... train.py --depth=36 --saln=1 --pn=512 --bs=768 --ep=350 --tblr=8e-5 --fp16=1 --alng=5e-6 --wpe=0.01 --twde=0.08\n  ```\n\n## 7. Monitor Training\n- **Description**: Track training progress using logs and TensorBoard.\n- **Commands**:\n  ```bash\n  tensorboard --logdir=local_output/\n  ```\n\n## 8. Sampling & Evaluation\n- **Description**: Generate samples and evaluate FID metrics.\n- **Action**: Use `var.autoregressive_infer_cfg()` for sampling and follow FID evaluation steps as described in the README.\n\n## 9. Explore Third-party Research\n- **Description**: Review linked third-party repositories for extended applications of VAR.\n- **Action**: Visit provided GitHub links in the \"Third-party Usage and Research\" section.\n\n## 10. Cite the Work\n- **Description**: Acknowledge the VAR project in your research.\n- **Action**: Use the provided BibTeX entries for citation.", "issue_title": "About the input resolution scale", "issue_body": "From the method mentioned in the paper, if the output resolution are huge, such as 1024x2048, the actually generation time would be much more large than diffusion model.\r\n\r\nSo, in large image generation, what is the strength actually of this method?", "choices": "(A) 1. Set the random seed to 41 to improve AUROC results.\n2. Shuffle the dataset randomly before splitting.\n3. Consider increasing the number of validation samples from 100 to 200 for more stable performance, especially on smaller datasets like TruthfulQA.\n4. Ensure the dataset splits for unlabeled/eval/test sets are consistent with the intended configuration, as different random seeds can lead to varying splits and performance outcomes. (B) 1. Consider increasing the number of validation samples from 100 to 200 for more stable performance, especially on smaller datasets like TruthfulQA.\n2. Set the random seed to 41 to improve AUROC results.\n3. Ensure the dataset splits for unlabeled/eval/test sets are consistent with the intended configuration, as different random seeds can lead to varying splits and performance outcomes. (C) 1. Set the random seed to 41 to improve AUROC results.\n2. Consider increasing the number of validation samples from 100 to 200 for more stable performance, especially on smaller datasets like TruthfulQA.\n3. Ensure the dataset splits for unlabeled/eval/test sets are consistent with the intended configuration, as different random seeds can lead to varying splits and performance outcomes. (D) 1. Set the random seed to 41 to improve AUROC results.\n2. Consider increasing the number of validation samples from 100 to 200 for more stable performance, especially on smaller datasets like TruthfulQA. (E) 1. Set the random seed to 41 to improve AUROC results.\n2. Consider increasing the number of validation samples from 100 to 200 for more stable performance, especially on smaller datasets like TruthfulQA.\n3. Ensure the dataset splits for unlabeled/eval/test sets are consistent with the intended configuration, as different random seeds can lead to varying splits and performance outcomes.\n4. Set the random seed to 100 for a \"fresh\" split. (F) 1. Ensure the dataset splits for unlabeled/eval/test sets are consistent with the intended configuration, as different random seeds can lead to varying splits and performance outcomes.\n2. Set the random seed to 41 to improve AUROC results.\n3. Consider increasing the number of validation samples from 100 to 200 for more stable performance, especially on smaller datasets like TruthfulQA. (G) 1. Consider increasing the number of validation samples from 100 to 200 for more stable performance, especially on smaller datasets like TruthfulQA.\n2. Ensure the dataset splits for unlabeled/eval/test sets are consistent with the intended configuration, as different random seeds can lead to varying splits and performance outcomes.", "answer": "C"}
{"uuid": "ad7c3dcd-3fd3-48e0-b161-7551efc3611d", "setup_instruct": "# Step-by-Step Execution Plan for TTT-LM PyTorch Implementation\n\n## Step 1: Environment Setup\n- **Description**: Install required dependencies including PyTorch and Hugging Face Transformers.\n- **Command**:\n  ```bash\n  pip install \"transformers[torch]\"\n  ```\n\n## Step 2: Load Model Configuration\n- **Description**: Initialize the TTT model configuration (default or custom).\n- **Python Code**:\n  ```python\n  from transformers import AutoTokenizer\n  from ttt import TTTForCausalLM, TTTConfig, TTT_STANDARD_CONFIGS\n\n  # Option 1: Use default configuration\n  configuration = TTTConfig()\n\n  # Option 2: Use predefined '1b' configuration\n  # configuration = TTTConfig(**TTT_STANDARD_CONFIGS['1b'])\n  ```\n\n## Step 3: Initialize Model\n- **Description**: Create the TTT model instance from the configuration and set to evaluation mode.\n- **Python Code**:\n  ```python\n  model = TTTForCausalLM(configuration)\n  model.eval()\n  ```\n\n## Step 4: Load Tokenizer\n- **Description**: Load the tokenizer (Llama-2-7b tokenizer is recommended).\n- **Python Code**:\n  ```python\n  tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n  ```\n\n## Step 5: Prefill (Forward Pass)\n- **Description**: Tokenize input text and perform a forward pass to get logits.\n- **Python Code**:\n  ```python\n  input_ids = tokenizer(\"Greeting from TTT!\", return_tensors=\"pt\").input_ids\n  logits = model(input_ids=input_ids)\n  print(logits)\n  ```\n\n## Step 6: Text Generation\n- **Description**: Generate text using the model's `generate` method.\n- **Python Code**:\n  ```python\n  out_ids = model.generate(input_ids=input_ids, max_length=50)\n  out_str = tokenizer.batch_decode(out_ids, skip_special_tokens=True)\n  print(out_str)\n  ```\n\n## Step 7: (Optional) Advanced Usage\n- **Description**: For training or optimized inference, refer to:\n  - [JAX Training Codebase](https://github.com/test-time-training/ttt-lm-jax)\n  - [Inference Kernels](https://github.com/test-time-training/ttt-lm-kernels)", "issue_title": "The understanding of W", "issue_body": "![image](https://github.com/user-attachments/assets/29eb32ef-0005-4f44-ba5b-5764f329d608)\r\nHello author, thank you very much for your work. What does this W refer to? Is it W0 or Wb? I don't understand. Look forward to your reply, thank you!", "choices": "(A) 1. Since there is no display monitor, run the script with the `--headless` flag to avoid rendering issues.  \n2. Ensure the `assets` folder exists under the `metaurban` directory by unzipping the downloaded file into that location.  \n3. Install `mesa-utils` to check OpenGL configuration.  \n4. Verify the OpenGL renderer by running `glxinfo | grep \"OpenGL renderer\"`. If it shows `llvmpipe` instead of NVIDIA, the graphical environment is not properly configured. (B) 1. Verify the OpenGL renderer by running `glxinfo | grep \"OpenGL renderer\"`. If it shows `llvmpipe` instead of NVIDIA, the graphical environment is not properly configured.  \n2. Install `mesa-utils` to check OpenGL configuration.  \n3. Ensure the `assets` folder exists under the `metaurban` directory by unzipping the downloaded file into that location.  \n4. Since there is no display monitor, run the script with the `--headless` flag to avoid rendering issues. (C) 1. Ensure the `assets` folder exists under the `metaurban` directory by unzipping the downloaded file into that location.  \n2. Install `mesa-utils` to check OpenGL configuration.  \n3. Verify the OpenGL renderer by running `glxinfo | grep \"OpenGL renderer\"`. If it shows `llvmpipe` instead of NVIDIA, the graphical environment is not properly configured.  \n4. Run the script without the `--headless` flag to force graphical output. (D) 1. Delete the `assets` folder under the `metaurban` directory using `rm -rf`.  \n2. Ensure the `assets` folder exists under the `metaurban` directory by unzipping the downloaded file into that location.  \n3. Install `mesa-utils` to check OpenGL configuration.  \n4. Verify the OpenGL renderer by running `glxinfo | grep \"OpenGL renderer\"`. If it shows `llvmpipe` instead of NVIDIA, the graphical environment is not properly configured.  \n5. Since there is no display monitor, run the script with the `--headless` flag to avoid rendering issues. (E) 1. Ensure the `assets` folder exists under the `metaurban` directory by unzipping the downloaded file into that location.  \n2. Install `mesa-utils` to check OpenGL configuration.  \n3. Since there is no display monitor, run the script with the `--headless` flag to avoid rendering issues. (F) 1. Ensure the `assets` folder exists under the `metaurban` directory by unzipping the downloaded file into that location.  \n2. Verify the OpenGL renderer by running `glxinfo | grep \"OpenGL renderer\"`. If it shows `llvmpipe` instead of NVIDIA, the graphical environment is not properly configured.  \n3. Since there is no display monitor, run the script with the `--headless` flag to avoid rendering issues. (G) 1. Ensure the `assets` folder exists under the `metaurban` directory by unzipping the downloaded file into that location.\n2. Install `mesa-utils` to check OpenGL configuration.\n3. Verify the OpenGL renderer by running `glxinfo | grep \"OpenGL renderer\"`. If it shows `llvmpipe` instead of NVIDIA, the graphical environment is not properly configured.\n4. Since there is no display monitor, run the script with the `--headless` flag to avoid rendering issues.", "answer": "G"}
{"uuid": "d4e8c09f-4846-472a-b418-f9a3e76cd272", "setup_instruct": "### Step-by-Step Execution Plan\n\n1. **Environment Setup**\n   - Install the required dependencies using pip.\n   - Command:\n     ```bash\n     pip install \"transformers[torch]\"\n     ```\n\n2. **Load Model and Tokenizer**\n   - Initialize the TTT model configuration and load the model.\n   - Load the tokenizer from the Huggingface model hub.\n   - Commands (Python code):\n     ```python\n     from transformers import AutoTokenizer\n     from ttt import TTTForCausalLM, TTTConfig, TTT_STANDARD_CONFIGS\n\n     # Initialize configuration\n     configuration = TTTConfig()\n\n     # Initialize model\n     model = TTTForCausalLM(configuration)\n     model.eval()\n\n     # Load tokenizer\n     tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n     ```\n\n3. **Prefill and Generate Text**\n   - Tokenize input text and generate logits.\n   - Generate text using the model.\n   - Commands (Python code):\n     ```python\n     # Prefill\n     input_ids = tokenizer(\"Greeting from TTT!\", return_tensors=\"pt\").input_ids\n     logits = model(input_ids=input_ids)\n     print(logits)\n\n     # Decoding\n     out_ids = model.generate(input_ids=input_ids, max_length=50)\n     out_str = tokenizer.batch_decode(out_ids, skip_special_tokens=True)\n     print(out_str)\n     ```\n\n4. **Optional: Training (Not Recommended)**\n   - Note: Training is not recommended due to performance limitations.\n   - For training, refer to the [JAX codebase](https://github.com/test-time-training/ttt-lm-jax).\n\n5. **Optional: Faster Inference**\n   - For optimized inference, use the released kernel [here](https://github.com/test-time-training/ttt-lm-kernels).", "issue_title": "How to use TTTLinear/TTTMLP like RNNs", "issue_body": "How can I use TTTLinear/TTTMLP to perform time series modeling for data in the shape of [B, T, C], just like using RNN? Could you give me a sample?", "choices": "(A) To compute the SNS benchmark score appropriately in the SocialNav task, follow these configuration changes:\n\n1. **Update the agent spawning parameters** in the evaluation script to include the following:\n   ```\n   spawn_human_num = int(6 * den_scale)\n   spawn_wheelchairman_num = int(den_scale)\n   spawn_edog_num = int(den_scale)\n   spawn_erobot_num = int(den_scale)\n   spawn_drobot_num = int(den_scale)\n   ```\n   Here, `den_scale` should be sampled randomly in the range `[1.0, 2.0]`, making the number of agents per scenario vary between 10 and 20.\n\n2. **Use the full evaluation config** provided by the maintainer:\n   ```\n   eval_config = dict(\n       env=dict(\n           use_render=False,\n           object_density=0.6,\n           training=False,\n           map=\"X\",\n           drivable_area_extension=55,\n           spawn_human_num=int(6 * den_scale),\n           spawn_wheelchairman_num=int(den_scale),\n           spawn_edog_num=int(den_scale),\n           spawn_erobot_num=int(den_scale),\n           spawn_drobot_num=int(den_scale),\n           max_actor_num=1,\n           show_mid_block_map=False,\n           show_ego_navigation=False,\n           debug=False,\n           horizon=1000,\n           on_continuous_line_done=False,\n           random_lane_num=True,\n           out_of_route_done=True,\n           vehicle_config=dict(\n               show_lidar=False,\n               show_navi_mark=False,\n               show_line_to_navi_mark=False,\n               show_dest_mark=False,\n           ),\n           show_sidewalk=False,\n           show_crosswalk=False,\n           random_spawn_lane_index=False,\n           num_scenarios=200,\n           traffic_density=0.05,\n           accident_prob=0,\n           relax_out_of_road_done=True,\n           sequential_seeding=False,\n           start_seed=12800,\n           max_lateral_dist=5.,\n       ),\n   )\n   ```\n\n3. **Ensure `max_lateral_dist` is set to 5.0** for evaluation, as specified in the config.\n\n4. **Note the difference in metric computation**: In the released code, `not_comply_time` is computed based on mesh-level collision between the agent and the pedestrian. However, in the paper, `not_comply_time` is computed by measuring the distance between the agent and the pedestrian (if the distance is less than 0.75 meters, increment `not_comply_time` by 1).\n\nBy applying these configuration changes, you should be able to compute the SNS benchmark score as reported in the paper. (B) 1. **Update the agent spawning parameters** in the evaluation script to include the following:\n   ```\n   spawn_human_num = int(6 * den_scale)\n   spawn_wheelchairman_num = int(den_scale)\n   spawn_edog_num = int(den_scale)\n   spawn_erobot_num = int(den_scale)\n   spawn_drobot_num = int(den_scale)\n   ```\n   Here, `den_scale` should be sampled randomly in the range `[1.0, 2.0]`, making the number of agents per scenario vary between 10 and 20.\n2. **Set `den_scale` to a fixed value of 1.0** to simplify the evaluation.\n3. **Use the full evaluation config** provided by the maintainer:\n   ```\n   eval_config = dict(\n       env=dict(\n           use_render=False,\n           object_density=0.6,\n           training=False,\n           map=\"X\",\n           drivable_area_extension=55,\n           spawn_human_num=int(6 * den_scale),\n           spawn_wheelchairman_num=int(den_scale),\n           spawn_edog_num=int(den_scale),\n           spawn_erobot_num=int(den_scale),\n           spawn_drobot_num=int(den_scale),\n           max_actor_num=1,\n           show_mid_block_map=False,\n           show_ego_navigation=False,\n           debug=False,\n           horizon=1000,\n           on_continuous_line_done=False,\n           random_lane_num=True,\n           out_of_route_done=True,\n           vehicle_config=dict(\n               show_lidar=False,\n               show_navi_mark=False,\n               show_line_to_navi_mark=False,\n               show_dest_mark=False,\n           ),\n           show_sidewalk=False,\n           show_crosswalk=False,\n           random_spawn_lane_index=False,\n           num_scenarios=200,\n           traffic_density=0.05,\n           accident_prob=0,\n           relax_out_of_road_done=True,\n           sequential_seeding=False,\n           start_seed=12800,\n           max_lateral_dist=5.,\n       ),\n   )\n   ```\n4. **Ensure `max_lateral_dist` is set to 5.0** for evaluation, as specified in the config.\n5. **Note the difference in metric computation**: In the released code, `not_comply_time` is computed based on mesh-level collision between the agent and the pedestrian. However, in the paper, `not_comply_time` is computed by measuring the distance between the agent and the pedestrian (if the distance is less than 0.75 meters, increment `not_comply_time` by 1). (C) 1. **Use the full evaluation config** provided by the maintainer:\n   ```\n   eval_config = dict(\n       env=dict(\n           use_render=False,\n           object_density=0.6,\n           training=False,\n           map=\"X\",\n           drivable_area_extension=55,\n           spawn_human_num=int(6 * den_scale),\n           spawn_wheelchairman_num=int(den_scale),\n           spawn_edog_num=int(den_scale),\n           spawn_erobot_num=int(den_scale),\n           spawn_drobot_num=int(den_scale),\n           max_actor_num=1,\n           show_mid_block_map=False,\n           show_ego_navigation=False,\n           debug=False,\n           horizon=1000,\n           on_continuous_line_done=False,\n           random_lane_num=True,\n           out_of_route_done=True,\n           vehicle_config=dict(\n               show_lidar=False,\n               show_navi_mark=False,\n               show_line_to_navi_mark=False,\n               show_dest_mark=False,\n           ),\n           show_sidewalk=False,\n           show_crosswalk=False,\n           random_spawn_lane_index=False,\n           num_scenarios=200,\n           traffic_density=0.05,\n           accident_prob=0,\n           relax_out_of_road_done=True,\n           sequential_seeding=False,\n           start_seed=12800,\n           max_lateral_dist=5.,\n       ),\n   )\n   ```\n2. **Ensure `max_lateral_dist` is set to 5.0** for evaluation, as specified in the config.\n3. **Note the difference in metric computation**: In the released code, `not_comply_time` is computed based on mesh-level collision between the agent and the pedestrian. However, in the paper, `not_comply_time` is computed by measuring the distance between the agent and the pedestrian (if the distance is less than 0.75 meters, increment `not_comply_time` by 1). (D) 1. **Ensure `max_lateral_dist` is set to 5.0** for evaluation, as specified in the config.\n2. **Update the agent spawning parameters** in the evaluation script to include the following:\n   ```\n   spawn_human_num = int(6 * den_scale)\n   spawn_wheelchairman_num = int(den_scale)\n   spawn_edog_num = int(den_scale)\n   spawn_erobot_num = int(den_scale)\n   spawn_drobot_num = int(den_scale)\n   ```\n   Here, `den_scale` should be sampled randomly in the range `[1.0, 2.0]`, making the number of agents per scenario vary between 10 and 20.\n3. **Use the full evaluation config** provided by the maintainer:\n   ```\n   eval_config = dict(\n       env=dict(\n           use_render=False,\n           object_density=0.6,\n           training=False,\n           map=\"X\",\n           drivable_area_extension=55,\n           spawn_human_num=int(6 * den_scale),\n           spawn_wheelchairman_num=int(den_scale),\n           spawn_edog_num=int(den_scale),\n           spawn_erobot_num=int(den_scale),\n           spawn_drobot_num=int(den_scale),\n           max_actor_num=1,\n           show_mid_block_map=False,\n           show_ego_navigation=False,\n           debug=False,\n           horizon=1000,\n           on_continuous_line_done=False,\n           random_lane_num=True,\n           out_of_route_done=True,\n           vehicle_config=dict(\n               show_lidar=False,\n               show_navi_mark=False,\n               show_line_to_navi_mark=False,\n               show_dest_mark=False,\n           ),\n           show_sidewalk=False,\n           show_crosswalk=False,\n           random_spawn_lane_index=False,\n           num_scenarios=200,\n           traffic_density=0.05,\n           accident_prob=0,\n           relax_out_of_road_done=True,\n           sequential_seeding=False,\n           start_seed=12800,\n           max_lateral_dist=5.,\n       ),\n   )\n   ```\n4. **Note the difference in metric computation**: In the released code, `not_comply_time` is computed based on mesh-level collision between the agent and the pedestrian. However, in the paper, `not_comply_time` is computed by measuring the distance between the agent and the pedestrian (if the distance is less than 0.75 meters, increment `not_comply_time` by 1).", "answer": "A"}
{"uuid": "ba79c5ef-1307-44cf-a101-4bde81606d79", "setup_instruct": "# MetaUrban Execution Plan\n\n## 1. Hardware Preparation\n- **Description**: Verify system meets recommended hardware requirements (Linux/Windows WSL2/MacOS, NVIDIA GPU with ≥8GB RAM, 10GB storage).\n- **Commands**: N/A (Manual verification)\n\n## 2. One-Step Installation (Recommended)\n- **Description**: Clone repository and run automated installation script.\n- **Commands**:\n  ```bash\n  git clone -b main --depth 1 https://github.com/metadriverse/metaurban.git\n  cd metaurban\n  bash install.sh\n  conda activate metaurban\n  ```\n\n## 3. Step-by-Step Installation (Fallback)\n- **Description**: Manual environment setup if one-step installation fails.\n- **Commands**:\n  ```bash\n  conda create -n metaurban python=3.9\n  conda activate metaurban\n  pip install -e .\n  conda install pybind11 -c conda-forge\n  cd metaurban/orca_algo && rm -rf build\n  bash compile.sh && cd ../..\n  pip install stable_baselines3 imitation tensorboard wandb scikit-image pyyaml gdown\n  ```\n\n## 4. Quick Validation\n- **Description**: Test installation with minimal assets.\n- **Commands**:\n  ```bash\n  python metaurban/examples/tiny_example.py\n  ```\n\n## 5. Full Asset Setup\n- **Description**: Download complete assets after registration.\n- **Commands**:\n  ```bash\n  python metaurban/examples/drive_in_static_env.py\n  # OR\n  python metaurban/pull_asset.py --update\n  ```\n\n## 6. Docker Alternative\n- **Description**: Containerized setup (NVIDIA GPU required).\n- **Commands**:\n  ```bash\n  sudo docker -D build -t metaurban .\n  sudo docker run -it metaurban\n  cd metaurban/orca_algo && rm -rf build\n  bash compile.sh && cd ../..\n  ```\n\n## 7. Environment Exploration\n### Point Navigation\n- **Description**: Static environment demo with manual controls (WASD keys).\n- **Commands**:\n  ```bash\n  python -m metaurban.examples.drive_in_static_env --density_obj 0.4\n  ```\n\n### Social Navigation\n- **Description**: Dynamic environment with pedestrians/vehicles.\n- **Commands**:\n  ```bash\n  python -m metaurban.examples.drive_in_dynamic_env --density_obj 0.4 --density_ped 1.0\n  ```\n\n## 8. Pre-Trained Model Demo\n- **Description**: Run provided PPO navigation policy.\n- **Commands**:\n  ```bash\n  python -m metaurban.examples.drive_with_pretrained_policy\n  ```\n\n## 9. Training Pipelines\n### Reinforcement Learning\n- **Training**:\n  ```bash\n  python RL/PointNav/train_ppo.py  # Static env\n  python RL/SocialNav/train_ppo.py  # Dynamic env\n  ```\n- **Evaluation**:\n  ```bash\n  python RL/PointNav/eval_ppo.py --policy ./pretrained_policy_576k.zip\n  ```\n\n### Imitation Learning\n- **Data Collection**:\n  ```bash\n  python scripts/collect_data_in_custom_env.py\n  ```\n- **Training**:\n  ```bash\n  python IL/PointNav/train_BC.py    # Behavior Cloning\n  python IL/PointNav/train_GAIL.py  # GAIL\n  ```\n\n## 10. Troubleshooting\n- **Reference**: Consult [FAQs](documentation/FAQs.md) for known issues.", "issue_title": "Issue when using non-zero CUDA_VISIBLE_DEVICES — likely forced GPU-0 usage in rendering pipeline", "issue_body": "Dear authors,\n\nHi! I'm trying to leverage multiple GPUs on my machine by specifying GPU IDs using the `CUDA_VISIBLE_DEVICES=X` environment variable. However, I've encountered an issue: whenever `X` is not `0`, the program fails with a runtime error.\n\nHere's a minimal example to reproduce the issue (Prerequisite: multiple GPUs on a single machine):\n\n```bash\nCUDA_VISIBLE_DEVICES=1 python -m metaurban.tests.test_env.verify_headless_env --cuda\n```\n\nBelow is the detailed error traceback:\n\n<details><summary>Click to expand</summary>\n\n```\nTraceback (most recent call last):\n  File \"anaconda3/envs/metaurban/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"anaconda3/envs/metaurban/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"metaurban/metaurban/tests/test_env/verify_headless_env.py\", line 117, in <module>\n    verify_installation(args.cuda, args.camera)\n  File \"metaurban/metaurban/tests/test_env/verify_headless_env.py\", line 102, in verify_installation\n    capture_headless_image(cuda)\n  File \"metaurban/metaurban/tests/test_env/verify_headless_env.py\", line 57, in capture_headless_image\n    env.reset()\n  File \"metaurban/metaurban/envs/base_env.py\", line 527, in reset\n    self.lazy_init()  # it only works the first time when reset() is called to avoid the error when render\n  File \"metaurban/metaurban/envs/base_env.py\", line 419, in lazy_init\n    initialize_engine(self.config)\n  File \"metaurban/metaurban/engine/engine_utils.py\", line 12, in initialize_engine\n    cls.singleton = cls(env_global_config)\n  File \"metaurban/metaurban/engine/base_engine.py\", line 88, in __init__\n    self.warmup()\n  File \"metaurban/metaurban/engine/base_engine.py\", line 756, in warmup\n    self.taskMgr.step()\n  File \"anaconda3/envs/metaurban/lib/python3.9/site-packages/direct/task/Task.py\", line 504, in step\n    self.mgr.poll()\n  File \"anaconda3/envs/metaurban/lib/python3.9/site-packages/direct/showbase/ShowBase.py\", line 2158, in __igLoop\n    self.graphicsEngine.renderFrame()\n  File \"metaurban/metaurban/engine/core/main_camera.py\", line 148, in _callback_func\n    self.register()\n  File \"metaurban/metaurban/engine/core/main_camera.py\", line 581, in register\n    self.cuda_graphics_resource = check_cudart_err(\n  File \"metaurban/metaurban/utils/cuda.py\", line 32, in check_cudart_err\n    raise RuntimeError(format_cudart_err(err))\nRuntimeError: cudaErrorInvalidDevice(101): invalid device ordinal\n```\n\n</details>\n\nFrom my observations, even when I set `CUDA_VISIBLE_DEVICES=1`, a process still gets launched on GPU-0 in addition to GPU-1. This suggests that some component of the underlying rendering pipeline may be hardcoded to use GPU-0.\n\nCould you please help confirm whether this is expected behavior or a bug? Any guidance on how to fully isolate the process to a specified GPU would be greatly appreciated.\n\nThank you for your support!", "choices": "(A) 1. Download the updated zip file from the new Google Drive link: https://drive.google.com/uc?id=1coEVsCZq-Xvj9p2TnhBFoFTsY-UoYGmG.\n2. Extract the zip file to ensure all required SQLite databases (including `BowlingLeague.sqlite`, `electronic_sales.sqlite`, `school_scheduling.sqlite`, and `chinook.sqlite`) are present.\n3. Verify the database mappings in `spider2/resource/databases/local/local-map.jsonl` match the extracted files.\n4. Delete the extracted SQLite databases to free up space. (B) 1. Extract the zip file to ensure all required SQLite databases (including `BowlingLeague.sqlite`, `electronic_sales.sqlite`, `school_scheduling.sqlite`, and `chinook.sqlite`) are present.\n2. Verify the database mappings in `spider2/resource/databases/local/local-map.jsonl` match the extracted files. (C) 1. Download the updated zip file from the new Google Drive link: https://drive.google.com/uc?id=1coEVsCZq-Xvj9p2TnhBFoFTsY-UoYGmG.\n2. Verify the database mappings in `spider2/resource/databases/local/local-map.jsonl` match the extracted files.\n3. Extract the zip file to ensure all required SQLite databases (including `BowlingLeague.sqlite`, `electronic_sales.sqlite`, `school_scheduling.sqlite`, and `chinook.sqlite`) are present. (D) 1. Download the updated zip file from the new Google Drive link: https://drive.google.com/uc?id=1coEVsCZq-Xvj9p2TnhBFoFTsY-UoYGmG.\n2. Extract the zip file to ensure all required SQLite databases (including `BowlingLeague.sqlite`, `electronic_sales.sqlite`, `school_scheduling.sqlite`, and `chinook.sqlite`) are present.\n3. Verify the database mappings in `spider2/resource/databases/local/local-map.jsonl` match the extracted files.", "answer": "D"}
{"uuid": "eb7d1843-177a-41c2-bbeb-6d7e5b91bc67", "setup_instruct": "# MetaUrban Deployment Plan\n\n## 1. Hardware Preparation\n- **Verify System Compatibility**:  \n  - Linux (Ubuntu recommended), Windows (WSL2), or MacOS.  \n  - NVIDIA GPU (≥8GB RAM + 3GB VRAM).  \n  - 10GB free storage.  \n\n## 2. Installation\n### Option A: One-Step Installation\n```bash\ngit clone -b main --depth 1 https://github.com/metadriverse/metaurban.git\ncd metaurban\nbash install.sh\nconda activate metaurban\n```\n\n### Option B: Step-by-Step Installation\n```bash\nconda create -n metaurban python=3.9\nconda activate metaurban\npip install -e .\nconda install pybind11 -c conda-forge\ncd metaurban/orca_algo && rm -rf build\nbash compile.sh && cd ../..\npip install stable_baselines3 imitation tensorboard wandb scikit-image pyyaml gdown\n```\n\n## 3. Quick Validation\n```bash\npython metaurban/examples/tiny_example.py\n```\n\n## 4. Full Asset Setup\n- **Automatic Download** (Triggers registration form):  \n```bash\npython metaurban/examples/drive_in_static_env.py\n```\n- **Manual Download**:  \n  - Complete [registration form](https://forms.office.com/r/tFBRFk7u4E).  \n  - Organize assets as:  \n    ```\n    -metaurban\n      -metaurban\n        -assets\n        -assets_pedestrian\n        -base_class\n        -...\n    ```\n\n## 5. Docker Setup (Optional)\n```bash\nsudo docker -D build -t metaurban .\nsudo docker run -it metaurban\ncd metaurban/orca_algo && rm -rf build\nbash compile.sh && cd ../..\n```\n\n## 6. Simulation Examples\n### Point Navigation\n```bash\npython -m metaurban.examples.drive_in_static_env --density_obj 0.4\n```\n- Controls: `W/S/A/D` for movement, `R` to reload, `T` to enable control.\n\n### Social Navigation\n```bash\npython -m metaurban.examples.drive_in_dynamic_env --density_obj 0.4 --density_ped 1.0\n```\n\n## 7. Pre-Trained Model Demo\n```bash\npython -m metaurban.examples.drive_with_pretrained_policy\n```\n\n## 8. Training & Evaluation\n### Reinforcement Learning\n- **Training**:  \n  ```bash\n  python RL/PointNav/train_ppo.py  # PointNav\n  python RL/SocialNav/train_ppo.py  # SocialNav\n  ```\n- **Evaluation**:  \n  ```bash\n  python RL/PointNav/eval_ppo.py --policy ./pretrained_policy_576k.zip\n  ```\n\n### Imitation Learning\n- **Data Collection**:  \n  ```bash\n  python scripts/collect_data_in_custom_env.py\n  ```\n- **Training**:  \n  ```bash\n  python IL/PointNav/train_BC.py    # Behavior Cloning\n  python IL/PointNav/train_GAIL.py  # GAIL\n  ```\n\n## 9. Support\n- Consult [FAQs](documentation/FAQs.md) or [Discussions](https://github.com/metadriverse/metaurban/discussions) for issues.", "issue_title": "Can metaUrban simulator be accelerated with GPU for rendering?", "issue_body": "I have installed metaurban, but I find it running very slow. What can I do to solve it?", "choices": "(A) 1. Ensure the `ch_and_postgres_setup.zip` file is placed in the `./spider2` directory.\n2. Run `python setup.py` to complete the setup process.\n3. For the Docker issue with ClickHouse, use the updated Dockerfile provided at: https://github.com/xlang-ai/Spider2/blob/main/methods/spider-agent/spider_agent/images/spider_agent_clickhouse-image/Dockerfile.\n4. If you are in mainland China, use the provided Dockerfile with Aliyun mirrors for faster setup.\n5. After updating the Dockerfile, rebuild the Docker image and restart the container. (B) 1. Ensure the `ch_and_postgres_setup.zip` file is placed in the `./spider2` directory.\n2. Run `python setup.py` to complete the setup process.\n3. Delete the `ch_and_postgres_setup.zip` file to save space.\n4. For the Docker issue with ClickHouse, use the updated Dockerfile provided at: https://github.com/xlang-ai/Spider2/blob/main/methods/spider-agent/spider_agent/images/spider_agent_clickhouse-image/Dockerfile.\n5. If you are in mainland China, use the provided Dockerfile with Aliyun mirrors for faster setup.\n6. After updating the Dockerfile, rebuild the Docker image and restart the container. (C) 1. Ensure the `ch_and_postgres_setup.zip` file is placed in the `./spider2` directory.\n2. Run `python setup.py` to complete the setup process.\n3. After updating the Dockerfile, rebuild the Docker image and restart the container.\n4. For the Docker issue with ClickHouse, use the updated Dockerfile provided at: https://github.com/xlang-ai/Spider2/blob/main/methods/spider-agent/spider_agent/images/spider_agent_clickhouse-image/Dockerfile.\n5. If you are in mainland China, use the provided Dockerfile with Aliyun mirrors for faster setup. (D) 1. Ensure the `ch_and_postgres_setup.zip` file is placed in the `./spider2` directory.\n2. Run `python setup.py` to complete the setup process.\n3. For the Docker issue with ClickHouse, use the updated Dockerfile provided at: https://github.com/xlang-ai/Spider2/blob/main/methods/spider-agent/spider_agent/images/spider_agent_clickhouse-image/Dockerfile.\n4. If you are in mainland China, use the provided Dockerfile with Aliyun mirrors for faster setup. (E) 1. Run `python setup.py` to complete the setup process.\n2. Ensure the `ch_and_postgres_setup.zip` file is placed in the `./spider2` directory.\n3. For the Docker issue with ClickHouse, use the updated Dockerfile provided at: https://github.com/xlang-ai/Spider2/blob/main/methods/spider-agent/spider_agent/images/spider_agent_clickhouse-image/Dockerfile.\n4. If you are in mainland China, use the provided Dockerfile with Aliyun mirrors for faster setup.\n5. After updating the Dockerfile, rebuild the Docker image and restart the container. (F) 1. Ensure the `ch_and_postgres_setup.zip` file is placed in the `./spider2` directory.\n2. Run `python setup.py` to complete the setup process.\n3. For the Docker issue with ClickHouse, use the updated Dockerfile provided at: https://github.com/xlang-ai/Spider2/blob/main/methods/spider-agent/spider_agent/images/spider_agent_clickhouse-image/Dockerfile.\n4. If you are in mainland China, ignore the provided Dockerfile with Aliyun mirrors and use the original one.\n5. After updating the Dockerfile, rebuild the Docker image and restart the container. (G) 1. Run `python setup.py` to complete the setup process.\n2. For the Docker issue with ClickHouse, use the updated Dockerfile provided at: https://github.com/xlang-ai/Spider2/blob/main/methods/spider-agent/spider_agent/images/spider_agent_clickhouse-image/Dockerfile.\n3. If you are in mainland China, use the provided Dockerfile with Aliyun mirrors for faster setup.\n4. After updating the Dockerfile, rebuild the Docker image and restart the container.", "answer": "A"}
{"uuid": "85985e85-436d-4731-822d-95abb6170940", "setup_instruct": "# Spider 2.0 Execution Plan\n\n## 1. Sign Up for Database Accounts\n### BigQuery Account Setup\n- **Description**: Sign up for a BigQuery account to access Spider 2.0-Lite datasets.\n- **Command**: Follow the [BigQuery Guideline](https://github.com/xlang-ai/Spider2/blob/main/assets/Bigquery_Guideline.md).\n\n### Snowflake Account Setup\n- **Description**: Sign up for a Snowflake account to access Spider 2.0-Snow datasets.\n- **Command**: \n  1. Follow the [Snowflake Guideline](https://github.com/xlang-ai/Spider2/blob/main/assets/Snowflake_Guideline.md).\n  2. Fill out the [Spider2 Snowflake Access Form](https://docs.google.com/forms/d/e/1FAIpQLScbVIYcBkADVr-NcYm9fLMhlxR7zBAzg-jaew1VNRj6B8yD3Q/viewform?usp=sf_link).\n\n---\n\n## 2. Choose Evaluation Setting\n### Option A: Spider 2.0-Snow (Tool-Call Format)\n- **Description**: Use the Docker-free Spider-Agent for rapid benchmarking.\n- **Command**: Navigate to [spider-agent-tc](https://github.com/xlang-ai/Spider2/tree/main/methods/spider-agent-tc).\n\n### Option B: Spider 2.0-Snow/Lite (Docker-Based)\n- **Description**: Use Docker-based Spider-Agent for benchmarking.\n- **Commands**:\n  - For Lite: Navigate to [spider-agent-lite](https://github.com/xlang-ai/Spider2/tree/main/methods/spider-agent-lite).\n  - For Snow: Navigate to [spider-agent-snow](https://github.com/xlang-ai/Spider2/tree/main/methods/spider-agent-snow).\n\n### Option C: Spider 2.0-DBT\n- **Description**: Use the DBT setting for repository-level text-to-SQL tasks.\n- **Commands**:\n  - Data: Navigate to [spider2-dbt](https://github.com/xlang-ai/Spider2/tree/main/spider2-dbt).\n  - Method: Navigate to [spider-agent-dbt](https://github.com/xlang-ai/Spider2/tree/main/methods/spider-agent-dbt).\n\n---\n\n## 3. Download Data\n- **Description**: Download the required datasets for evaluation.\n- **Commands**:\n  - Spider 2.0-Lite: Download [spider2-lite.jsonl](https://github.com/xlang-ai/Spider2/blob/main/spider2-lite/spider2-lite.jsonl).\n  - Spider 2.0-Snow: Download [spider2-snow.jsonl](https://github.com/xlang-ai/Spider2/blob/main/spider2-snow/spider2-snow.jsonl).\n\n---\n\n## 4. Run Benchmarking\n- **Description**: Execute the chosen Spider-Agent to benchmark your model.\n- **Command**: Follow the instructions in the respective Spider-Agent repository.\n\n---\n\n## 5. Submit Results to Leaderboard\n- **Description**: Submit your results to the Spider 2.0 leaderboard.\n- **Command**: Follow the [Submission Guidance](https://docs.google.com/document/d/1sCobAqJZcko-Vl3biOycwvCIR7kTwBPrhsgVfvaX1Fg/edit?usp=sharing).", "issue_title": "Where is the full dataset?", "issue_body": "I see there are 178 examples in this file\r\nhttps://github.com/xlang-ai/Spider2/blob/main/spider2/examples/spider2.jsonl\r\n\r\nHowever, the paper says there are 600 examples? Where are the rest? Also, is it possible to have the correctly labeled sql as another field in the jsonl as well?\r\n\r\nLastly, is there a place where I can easily download all the referenced tables?", "choices": "(A) 1. Re-create the conda environment as specified in the setup.py and README.\n2. Ensure all dependencies are installed with the exact versions mentioned (e.g., botorch 0.14.0, gpytorch 1.14, torch 2.2.2, python 3.11.8).\n3. Run the code again to achieve performance similar to the reported results. (B) 1. Ensure all dependencies are installed with the exact versions mentioned (e.g., botorch 0.14.0, gpytorch 1.14, torch 2.2.2, python 3.11.8).\n2. Re-create the conda environment as specified in the setup.py and README.\n3. Run the code again to achieve performance similar to the reported results. (C) 1. Ensure all dependencies are installed with the exact versions mentioned (e.g., botorch 0.14.0, gpytorch 1.14, torch 2.2.2, python 3.11.8).\n2. Run the code again to achieve performance similar to the reported results. (D) 1. Re-create the conda environment as specified in the setup.py and README.\n2. Downgrade Python to version 3.7.0.\n3. Ensure all dependencies are installed with the exact versions mentioned (e.g., botorch 0.14.0, gpytorch 1.14, torch 2.2.2, python 3.11.8).\n4. Run the code again to achieve performance similar to the reported results. (E) 1. Run the code again to achieve performance similar to the reported results.\n2. Re-create the conda environment as specified in the setup.py and README.\n3. Ensure all dependencies are installed with the exact versions mentioned (e.g., botorch 0.14.0, gpytorch 1.14, torch 2.2.2, python 3.11.8). (F) 1. Re-create the conda environment as specified in the setup.py and README.\n2. Run the code again to achieve performance similar to the reported results. (G) 1. Re-create the conda environment as specified in the setup.py and README.\n2. Ensure all dependencies are installed with the exact versions mentioned (e.g., botorch 0.14.0, gpytorch 1.14, torch 2.2.2, python 3.11.8).\n3. Uninstall all dependencies to start fresh.\n4. Run the code again to achieve performance similar to the reported results.", "answer": "A"}
{"uuid": "ebb4af1d-d0d6-44cc-b0d1-64a53df5e85e", "setup_instruct": "# Flow Matching Project Execution Plan\n\n## 1. Installation\n- **Description**: Install the `flow_matching` package using pip.\n- **Command**: \n  ```bash\n  pip install flow_matching\n  ```\n\n## 2. Environment Setup (Optional for Development)\n- **Description**: Create and activate a conda environment with all required dependencies.\n- **Commands**: \n  ```bash\n  conda env create -f environment.yml\n  conda activate flow_matching\n  ```\n\n## 3. Pre-commit Hook Installation (Optional for Development)\n- **Description**: Install pre-commit hooks to ensure linting is done on each commit.\n- **Command**: \n  ```bash\n  pre-commit install\n  ```\n\n## 4. Editable Installation (Optional for Development)\n- **Description**: Install the `flow_matching` package in an editable mode for development.\n- **Command**: \n  ```bash\n  pip install -e .\n  ```\n\n## 5. Training Examples\n- **Description**: Explore and run training examples provided in the `examples` folder.\n- **Commands**: \n  - For continuous Flow Matching on synthetic data:\n    ```bash\n    jupyter notebook examples/2d_flow_matching.ipynb\n    ```\n  - For discrete Flow Matching on synthetic data:\n    ```bash\n    jupyter notebook examples/2d_discrete_flow_matching.ipynb\n    ```\n  - For Riemannian Flow Matching:\n    ```bash\n    jupyter notebook examples/2d_riemannian_flow_matching_flat_torus.ipynb\n    ```\n  - For image training (CIFAR10 or ImageNet):\n    ```bash\n    jupyter notebook examples/image/\n    ```\n  - For text modeling:\n    ```bash\n    jupyter notebook examples/text/\n    ```\n\n## 6. Contribution (Optional)\n- **Description**: Follow the contribution guide if you wish to contribute to the codebase.\n- **Command**: \n  - Refer to [CONTRIBUTING.md](CONTRIBUTING.md) for details.\n\n## 7. Citation\n- **Description**: Cite the repository if you find it useful in your work.\n- **Command**: \n  - Use the provided BibTeX entry in your publication.", "issue_title": "About conditional generation", "issue_body": "Is there any training example for conditional generation? like CFG-like way. \nI have implemented one, but it doesn't seem to be good, so I wonder if there is an example?", "choices": "(A) 1. Ensure the real-world dataset is placed in the correct directory: `test-set/realOOD`.\n2. Check that there are output images in the directory `outputs/objaverse_splatformer/test/real`.\n3. If the issue persists, comment out specific code in `GS.py` as a temporary workaround.\n4. For inference, set `CUDA_VISIBLE_DEVICES=0` to avoid data splitting issues. (B) 1. Check that there are output images in the directory `outputs/objaverse_splatformer/test/real`.\n2. Ensure the real-world dataset is placed in the correct directory: `test-set/realOOD`.\n3. For inference, set `CUDA_VISIBLE_DEVICES=0` to avoid data splitting issues.\n4. If the issue persists, comment out specific code in `GS.py` as a temporary workaround. (C) 1. Ensure the real-world dataset is placed in the correct directory: `test-set/realOOD`.\n2. Check that there are output images in the directory `outputs/objaverse_splatformer/test/real`.\n3. For inference, set `CUDA_VISIBLE_DEVICES=999` to avoid data splitting issues.\n4. If the issue persists, comment out specific code in `GS.py` as a temporary workaround. (D) 1. Ensure the real-world dataset is placed in the correct directory: `test-set/realOOD`.\n2. Check that there are output images in the directory `outputs/objaverse_splatformer/test/real`.\n3. If the issue persists, comment out specific code in `GS.py` as a temporary workaround. (E) 1. Check that there are output images in the directory `outputs/objaverse_splatformer/test/real`.\n2. For inference, set `CUDA_VISIBLE_DEVICES=0` to avoid data splitting issues.\n3. If the issue persists, comment out specific code in `GS.py` as a temporary workaround. (F) 1. Ensure the real-world dataset is placed in the correct directory: `test-set/realOOD`.\n2. Delete all files in `outputs/objaverse_splatformer/test/real`.\n3. Check that there are output images in the directory `outputs/objaverse_splatformer/test/real`.\n4. For inference, set `CUDA_VISIBLE_DEVICES=0` to avoid data splitting issues.\n5. If the issue persists, comment out specific code in `GS.py` as a temporary workaround. (G) 1. Ensure the real-world dataset is placed in the correct directory: `test-set/realOOD`.\n2. Check that there are output images in the directory `outputs/objaverse_splatformer/test/real`.\n3. For inference, set `CUDA_VISIBLE_DEVICES=0` to avoid data splitting issues.\n4. If the issue persists, comment out specific code in `GS.py` as a temporary workaround.", "answer": "G"}
{"uuid": "c1f98c25-63ab-4b7c-a426-9069adabe434", "setup_instruct": "# Flow Matching Project Execution Plan\n\n## 1. Installation\n- **Description**: Install the `flow_matching` package using pip.\n- **Command**:\n  ```bash\n  pip install flow_matching\n  ```\n\n## 2. Environment Setup (Optional for Development)\n- **Description**: Create a Conda environment and install dependencies for development.\n- **Commands**:\n  ```bash\n  conda env create -f environment.yml\n  conda activate flow_matching\n  ```\n\n## 3. Pre-commit Setup (Optional for Development)\n- **Description**: Install pre-commit hooks to ensure linting on each commit.\n- **Command**:\n  ```bash\n  pre-commit install\n  ```\n\n## 4. Editable Installation (Optional for Development)\n- **Description**: Install the `flow_matching` package in editable mode.\n- **Command**:\n  ```bash\n  pip install -e .\n  ```\n\n## 5. Training Examples\n- **Description**: Explore and run training examples provided in the `examples` folder.\n- **Commands** (examples):\n  - For continuous Flow Matching:\n    ```bash\n    jupyter notebook examples/2d_flow_matching.ipynb\n    ```\n  - For discrete Flow Matching:\n    ```bash\n    jupyter notebook examples/2d_discrete_flow_matching.ipynb\n    ```\n  - For image training (CIFAR10/ImageNet):\n    ```bash\n    python examples/image/train_image.py\n    ```\n  - For text modeling:\n    ```bash\n    python examples/text/train_text.py\n    ```\n\n## 6. Contribution (Optional)\n- **Description**: Follow the contribution guide if you wish to contribute to the project.\n- **Command**: Review the [CONTRIBUTING.md](CONTRIBUTING.md) file.\n\n## 7. Citation\n- **Description**: Cite the project if you use it in your work.\n- **Command**: Use the provided BibTeX entry in your publication.", "issue_title": "suggeste  to improve the math rigorousness for \"Flow Matching Guide and Code\"", "issue_body": "Thanks for your contribution in this tutorial(https://arxiv.org/pdf/2412.06264), but I noticed several math errors when reading the first several parts:\n\n1. please use the partial derivative not total derivative for the conservation.\n\n![Image](https://github.com/user-attachments/assets/a240d62d-b756-456d-9b62-fbf763ac8cb2)\n\n2. Error in this equation, should not include tri in Expectation. \n\n![Image](https://github.com/user-attachments/assets/3e49aa99-dbb2-49ff-86c0-d44a45f15f0a)\n\n.... I will keep on reviewing this book.", "choices": "(A) 1. Download the data.zip file from the ToolBench repository. 2. Delete the data.zip file. 3. Extract the contents of data.zip to access the required toolenv directory. (B) 1. Download the data.zip file from the ToolBench repository. 2. Edit the data.zip file in a text editor. 3. Extract the contents of data.zip to access the required toolenv directory. (C) 1. Extract the contents of data.zip to access the required toolenv directory. 2. Download the data.zip file from the ToolBench repository. (D) 1. Download the data.zip file from the ToolBench repository. 2. Extract the contents of data.zip to access the required toolenv directory. (E) 1. Extract the contents of data.zip to access the required toolenv directory. 2. Download the data.zip file from the ToolBench repository. (F) 1. Extract the contents of data.zip to access the required toolenv directory. (G) 1. Download the data.zip file from the ToolBench repository.", "answer": "D"}
{"uuid": "8461e9ec-dc03-4b8a-a0c4-e2b6bafea918", "setup_instruct": "# Step-by-Step Execution Plan for SplatFormer\n\n## 1. Environment Setup\n- **Description**: Clone the repository and set up a Conda environment with required dependencies.\n- **Commands**:\n  ```bash\n  git clone --recursive git@github.com:ChenYutongTHU/SplatFormer.git\n  cd SplatFormer\n  conda create -n splatformer python=3.8 -y\n  conda activate splatformer\n  pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n  pip install Pointcept/\n  pip install flash-attn --no-build-isolation\n  pip install -r requirements.txt\n  pip install git+https://github.com/nerfstudio-project/gsplat.git@v0.1.11\n  ```\n\n## 2. Download OOD-NVS Test Sets\n- **Description**: Download and prepare the Out-of-Distribution Novel View Synthesis test sets.\n- **Commands**:\n  - Download from [Google Drive](https://drive.google.com/file/d/1-mUCl-yxe1aE0rrQDHKlXk1J2n8d1-60/view?usp=sharing).\n  - Extract and place under `test-set/` as per the directory structure in the README.\n\n## 3. Training Set Generation\n- **Description**: Generate training datasets using provided scripts.\n- **Commands**:\n  - Follow instructions in `DataGenerator/` to render images and generate initial 3DGS.\n  - Organize files under `train-set/` as specified.\n\n## 4. Training SplatFormer\n- **Description**: Train the model on Objaverse and ShapeNet datasets.\n- **Commands**:\n  ```bash\n  sh scripts/train-on-objaverse_gpux8-accum4.sh\n  sh scripts/train-on-shapenet_gpux8-accum4.sh\n  ```\n\n## 5. Evaluation\n- **Description**: Evaluate the trained model on OOD-NVS test sets.\n- **Commands**:\n  ```bash\n  sh scripts/train-on-objaverse_inference.sh\n  sh scripts/train-on-shapenet_inference.sh\n  ```\n  - Metrics and outputs will be saved in `outputs/`.\n\n## 6. Real-Time 3DGS Viewer Setup\n- **Description**: Install and configure the SIBR viewer for real-time visualization.\n- **Commands**:\n  ```bash\n  # Follow SIBR viewer installation instructions from the provided link.\n  VIEW_DIR=outputs/objaverse_splatformer/test/objaverse/viewer/0a6e1a80d2e34d5981d6b2b440bbc8cd-10\n  cd SIBR_viewers/install/shaders/core\n  ../../bin/SIBR_gaussianViewer_app -m $VIEW_DIR --load_iteration iteration_1\n  ```\n\n## 7. Citation and Licensing\n- **Description**: Ensure proper citation and adherence to licensing terms for datasets used.\n- **Actions**:\n  - Cite the paper using the provided BibTeX entry.\n  - Review licenses for Objaverse-v1 and GSO datasets.", "issue_title": "Can't not eval on real dataset!", "issue_body": "When running `sh scripts/train-on-objaverse_inference.sh `\r\n\r\nGSO and objaverse work well, but when it comes to real, this happened:\r\n\r\n```bash\r\nFile \"/home/ryan/SplatFormer/utils/metrics.py\", line 65, in sum\r\nif max([x.dim() for x in self.results[metric]]) == 0:\r\nValueError: max() arg is an empty sequence\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/11c74043-b91a-4f03-ac8b-711cab5c024f)\r\n\r\nif I want to evaluate my own dataset, what i should do next, could u provide some tips?", "choices": "(A) 1. Run the visualization command with the correct configuration file and checkpoint path:\n```bash\npython main_vis.py --config cfgs/pretrain/base.yaml --exp_name final_vis \\\n--ckpts ./experiments/final_ckpt_collected/Pre-train/PCP-MAE-300.pth --test\n```\n2. Ensure the correct configuration file is used. The issue arises because `modlenet40.yaml` does not exist in the `./cfgs/pretrain` directory. Instead, use `base.yaml`.\n3. Check the `./vis` directory for the visualization results.\n4. If visualizing other datasets (e.g., ModelNet40), modify the configuration file and adjust `./runner.py` accordingly to support the new dataset. (B) 1. Ensure the correct configuration file is used. The issue arises because `modlenet40.yaml` does not exist in the `./cfgs/pretrain` directory. Instead, use `base.yaml`.\n2. Delete the `./vis` directory to clear old results.\n3. Run the visualization command with the correct configuration file and checkpoint path:\n```bash\npython main_vis.py --config cfgs/pretrain/base.yaml --exp_name final_vis \\\n--ckpts ./experiments/final_ckpt_collected/Pre-train/PCP-MAE-300.pth --test\n```\n4. Check the `./vis` directory for the visualization results.\n5. If visualizing other datasets (e.g., ModelNet40), modify the configuration file and adjust `./runner.py` accordingly to support the new dataset. (C) 1. Ensure the correct configuration file is used. The issue arises because `modlenet40.yaml` does not exist in the `./cfgs/pretrain` directory. Instead, use `base.yaml`.\n2. Run the visualization command with the correct configuration file and checkpoint path:\n```bash\npython main_vis.py --config cfgs/pretrain/base.yaml --exp_name final_vis \\\n--ckpts ./experiments/final_ckpt_collected/Pre-train/PCP-MAE-300.pth --test\n```\n3. If visualizing other datasets (e.g., ModelNet40), modify the configuration file and adjust `./runner.py` accordingly to support the new dataset. (D) 1. Ensure the correct configuration file is used. The issue arises because `modlenet40.yaml` does not exist in the `./cfgs/pretrain` directory. Instead, use `base.yaml`.\n2. Run the visualization command with the correct configuration file and checkpoint path:\n```bash\npython main_vis.py --config cfgs/pretrain/base.yaml --exp_name final_vis \\\n--ckpts ./experiments/final_ckpt_collected/Pre-train/PCP-MAE-300.pth --test\n```\n3. Check the `./vis` directory for the visualization results.\n4. If visualizing other datasets (e.g., ModelNet40), modify the configuration file and adjust `./runner.py` accordingly to support the new dataset.", "answer": "D"}
{"uuid": "c1b997b8-0214-426f-9ffb-aeec2fc2e13f", "setup_instruct": "# Step-by-Step Execution Plan for DRAFT Project\n\n## 1. Environment Setup\n- **Description**: Install required Python packages and verify versions.\n- **Commands**:\n  ```bash\n  pip install openai==0.28.0 numpy==1.26.4 pandas==2.2.2 torch==2.3.1\n  ```\n\n## 2. API Key Setup\n- **Description**: Obtain API keys for required services.\n- **Steps**:\n  1. Get OpenAI key: [OpenAI Playground](https://platform.openai.com/playground/chat)\n  2. Get RapidAPI key: [RapidAPI Hub](https://rapidapi.com/hub)\n  3. Get ToolBench key: [ToolBench Repo](https://github.com/OpenBMB/ToolBench)\n  4. Get TMDB key: [TMDB API](https://developer.themoviedb.org/reference/intro/getting-started)\n  5. Get Spotify key: [Spotify Web API](https://developer.spotify.com/documentation/web-api)\n\n## 3. Data Setup\n- **Description**: Download and extract datasets or use preprocessed documentation.\n- **Options**:\n  - **Option A**: Download ToolBench dataset:\n    - [Google Drive](https://drive.google.com/file/d/1M06p-OO1YM80MNhIbLYw2FtRB5Qmh39z/view)\n    - [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/c9e50625743b40bfbe10/)\n  - **Option B**: Download RestBench dataset:\n    - [RestBench Repo](https://github.com/Yifan-Song793/RestGPT)\n\n## 4. Run DRAFT\n- **Description**: Execute DRAFT to revise tool documentation.\n- **Command**:\n  ```bash\n  python DRAFT.py\n  ```\n\n## 5. Perform Inference\n- **Description**: Test the effectiveness of DRAFT-modified documentation.\n- **Command** (example for GPT-4o on G3 dataset):\n  ```bash\n  python Inference_DFSDT.py -model_name gpt-4o-2024-08-06 -data_type G3 -method DRAFT\n  ```\n\n## 6. Evaluation\n- **Description**: Calculate path rate and win rate for results.\n- **Steps**:\n  1. Run path rate calculation:\n     ```bash\n     python Cal_path_rate.py\n     ```\n  2. Refer to [ToolEval Repo](https://github.com/OpenBMB/ToolBench/blob/master/toolbench/tooleval/README.md) for win rate calculation.\n\n## 7. Citation\n- **Description**: Cite the work if used in research.\n- **BibTeX**:\n  ```bibtex\n  @inproceedings{quexploration,\n    title={From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions},\n    author={Qu, Changle and Dai, Sunhao and Wei, Xiaochi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Xu, Jun and Wen, Ji-Rong},\n    booktitle={The Thirteenth International Conference on Learning Representations},\n    year={2025},\n    url={https://openreview.net/forum?id=QKBu1BOAwd}\n  }\n  ```", "issue_title": "eval cp and win", "issue_body": "您好，请问运行ToolBench_DFSDT.py后结果的cp和win如何评估", "choices": "(A) 1. Identify the observation size mismatch issue in the code. 2. Modify the observation size in the `humanoid_amp_carryobject.py` file to match the required size (298) by adjusting the `obs_size` parameter to 75. 3. Ensure consistency with the two-agent scenario settings in `share_humanoid_amp_carryobject.py`. 4. Retrain the single agent policy checkpoint if necessary to align with the new observation size. (B) 1. Identify the observation size mismatch issue in the code. 4. Retrain the single agent policy checkpoint if necessary to align with the new observation size. 2. Modify the observation size in the `humanoid_amp_carryobject.py` file to match the required size (298) by adjusting the `obs_size` parameter to 75. 3. Ensure consistency with the two-agent scenario settings in `share_humanoid_amp_carryobject.py`. (C) 1. Identify the observation size mismatch issue in the code. 2. Modify the observation size in the `humanoid_amp_carryobject.py` file to match the required size (298) by adjusting the `obs_size` parameter to 75. 5. Delete the `humanoid_amp_carryobject.py` file. 3. Ensure consistency with the two-agent scenario settings in `share_humanoid_amp_carryobject.py`. 4. Retrain the single agent policy checkpoint if necessary to align with the new observation size. (D) 1. Identify the observation size mismatch issue in the code. 3. Ensure consistency with the two-agent scenario settings in `share_humanoid_amp_carryobject.py`. 2. Modify the observation size in the `humanoid_amp_carryobject.py` file to match the required size (298) by adjusting the `obs_size` parameter to 75. 4. Retrain the single agent policy checkpoint if necessary to align with the new observation size. (E) 1. Identify the observation size mismatch issue in the code. 2. Modify the observation size in the `humanoid_amp_carryobject.py` file to match the required size (298) by adjusting the `obs_size` parameter to 75. 4. Retrain the single agent policy checkpoint if necessary to align with the new observation size. (F) 1. Identify the observation size mismatch issue in the code. 3. Ensure consistency with the two-agent scenario settings in `share_humanoid_amp_carryobject.py`. 4. Retrain the single agent policy checkpoint if necessary to align with the new observation size. (G) 1. Identify the observation size mismatch issue in the code. 2. Modify the observation size in the `humanoid_amp_carryobject.py` file to match the required size (298) by adjusting the `obs_size` parameter to 75. 5. Set the `obs_size` parameter to 0. 3. Ensure consistency with the two-agent scenario settings in `share_humanoid_amp_carryobject.py`. 4. Retrain the single agent policy checkpoint if necessary to align with the new observation size.", "answer": "A"}
{"uuid": "427f8599-de09-4af0-bfa9-3f1c3c0e866c", "setup_instruct": "# CooHOI Project Execution Plan\n\n## 1. Prerequisites Installation\n- **Step 1.1**: Download Isaac Gym  \n  Download from [NVIDIA's website](https://developer.nvidia.com/isaac-gym) or via CLI:  \n  ```bash\n  wget https://developer.nvidia.com/isaac-gym-preview-4\n  tar -xvzf isaac-gym-preview-4\n  ```\n\n- **Step 1.2**: Set up Conda Environment  \n  Create and activate a Conda environment:  \n  ```bash\n  conda create -n coohoi python=3.8\n  conda activate coohoi\n  ```\n\n- **Step 1.3**: Install IsaacGym Python Wrappers  \n  Navigate to the IsaacGym directory and install:  \n  ```bash\n  pip install -e isaacgym/python\n  ```\n\n- **Step 1.4**: Install Project Dependencies  \n  Install required packages from `requirements.txt`:  \n  ```bash\n  pip install -r requirements.txt\n  ```\n\n- **Step 1.5**: Resolve Library Path Error (if encountered)  \n  Set `LD_LIBRARY_PATH` if `libpython3.8m.so.1.0` is missing:  \n  ```bash\n  export LD_LIBRARY_PATH=/path/to/conda/envs/coohoi/lib\n  ```\n\n---\n\n## 2. Reproduce Paper Results\n- **Step 2.1**: Single-Agent Object Carrying Task  \n  Execute the pre-trained single-agent model:  \n  ```bash\n  CUDA_VISIBLE_DEVICES=0 python coohoi/run.py --test \\\n  --task HumanoidAMPCarryObject \\\n  --num_envs 16 \\\n  --cfg_env coohoi/data/cfg/humanoid_carrybox.yaml \\\n  --cfg_train coohoi/data/cfg/train/amp_humanoid_task.yaml \\\n  --motion_file coohoi/data/motions/coohoi_data/coohoi_data.yaml \\\n  --checkpoint coohoi/data/models/SingleAgent.pth\n  ```\n\n- **Step 2.2**: Two-Agent Cooperative Task  \n  Run the pre-trained two-agent model:  \n  ```bash\n  CUDA_VISIBLE_DEVICES=0 python coohoi/run.py --test \\\n  --task ShareHumanoidCarryObject \\\n  --num_envs 16 \\\n  --cfg_env coohoi/data/cfg/share_humanoid_carrybox.yaml \\\n  --cfg_train coohoi/data/cfg/train/share_humanoid_task_coohoi.yaml \\\n  --motion_file coohoi/data/motions/coohoi_data/coohoi_data.yaml \\\n  --checkpoint coohoi/data/models/TwoAgent.pth\n  ```\n\n---\n\n## 3. Training Procedures\n### 3.1 Single Humanoid Skill Training\n- **Step 3.1.1**: Start Training  \n  Train a single-agent policy (headless mode with WandB logging):  \n  ```bash\n  CUDA_VISIBLE_DEVICES=0 python coohoi/run.py \\\n  --task HumanoidAMPCarryObject \\\n  --cfg_env coohoi/data/cfg/humanoid_carrybox.yaml \\\n  --cfg_train coohoi/data/cfg/train/amp_humanoid_task.yaml \\\n  --motion_file coohoi/data/motions/coohoi_data/coohoi_data.yaml \\\n  --headless \\\n  --wandb_name \"<experiment_name>\"\n  ```\n\n- **Step 3.1.2**: Evaluate Trained Model  \n  Test the saved checkpoint (replace `<checkpoint_path>`):  \n  ```bash\n  CUDA_VISIBLE_DEVICES=0 python coohoi/run.py --test \\\n  --task HumanoidAMPCarryObject \\\n  --num_envs 16 \\\n  --cfg_env coohoi/data/cfg/humanoid_carrybox.yaml \\\n  --cfg_train coohoi/data/cfg/train/amp_humanoid_task.yaml \\\n  --motion_file coohoi/data/motions/coohoi_data/coohoi_data.yaml \\\n  --checkpoint output/Humanoid_19-16-52-17/nn/Humanoid.pth\n  ```\n\n### 3.2 Two-Humanoid Cooperative Training\n- **Step 3.2.1**: Fine-Tune from Single-Agent Policy  \n  Train cooperative policy (requires single-agent checkpoint):  \n  ```bash\n  CUDA_VISIBLE_DEVICES=0 python coohoi/run.py \\\n  --task ShareHumanoidCarryObject \\\n  --cfg_env coohoi/data/cfg/share_humanoid_carrybox.yaml \\\n  --cfg_train coohoi/data/cfg/train/share_humanoid_task_coohoi.yaml \\\n  --motion_file coohoi/data/motions/coohoi_data/coohoi_data.yaml \\\n  --headless \\\n  --is_finetune \\\n  --pretrain_checkpoint coohoi/data/models/SingleAgent.pth \\\n  --wandb \\\n  --wandb_name \"CooHOI Training\"\n  ```\n\n- **Step 3.2.2**: Evaluate Cooperative Policy  \n  Test the trained model (replace `<ckpt_path>`):  \n  ```bash\n  CUDA_VISIBLE_DEVICES=0 python coohoi/run.py --test \\\n  --task ShareHumanoidCarryObject \\\n  --num_envs 16 \\\n  --cfg_env coohoi/data/cfg/share_humanoid_carrybox.yaml \\\n  --cfg_train coohoi/data/cfg/train/share_humanoid_task_coohoi.yaml \\\n  --motion_file coohoi/data/motions/coohoi_data/coohoi_data.yaml \\\n  --checkpoint <ckpt_path>\n  ```", "issue_title": "When will the source code be released?", "issue_body": "Hi jiawei, \r\n\r\nThank you very much for your work, very special\r\n\r\nI'm very interested in this project and would like to know if you have plans to open source the code. Do you have a timeline for making the repository public?\r\n\r\nThank you for your time.", "choices": "(A) 1. Locate the file `env/tasks/base_task.py` in your project directory.\n2. Find the line that sets `self.graphics_device_id = self.device_id + 1`.\n3. Change this line to `self.graphics_device_id = self.device_id`.\n4. Save the file and rerun your script to resolve the 'invalid device ordinal' error.\n5. Delete the file `env/tasks/base_task.py`. (B) 1. Find the line that sets `self.graphics_device_id = self.device_id + 1`.\n2. Change this line to `self.graphics_device_id = self.device_id`.\n3. Save the file and rerun your script to resolve the 'invalid device ordinal' error. (C) 1. Locate the file `env/tasks/base_task.py` in your project directory.\n2. Find the line that sets `self.graphics_device_id = self.device_id + 1`.\n3. Change this line to `self.graphics_device_id = self.device_id`.\n4. Save the file and rerun your script to resolve the 'invalid device ordinal' error.\n5. Change the line back to `self.graphics_device_id = self.device_id + 1`. (D) 1. Rerun your script to resolve the 'invalid device ordinal' error.\n2. Locate the file `env/tasks/base_task.py` in your project directory.\n3. Find the line that sets `self.graphics_device_id = self.device_id + 1`.\n4. Change this line to `self.graphics_device_id = self.device_id`.\n5. Save the file. (E) 1. Locate the file `env/tasks/base_task.py` in your project directory.\n2. Find the line that sets `self.graphics_device_id = self.device_id + 1`.\n3. Change this line to `self.graphics_device_id = self.device_id`. (F) 1. Locate the file `env/tasks/base_task.py` in your project directory.\n2. Save the file and rerun your script to resolve the 'invalid device ordinal' error.\n3. Find the line that sets `self.graphics_device_id = self.device_id + 1`.\n4. Change this line to `self.graphics_device_id = self.device_id`. (G) 1. Locate the file `env/tasks/base_task.py` in your project directory.\n2. Find the line that sets `self.graphics_device_id = self.device_id + 1`.\n3. Change this line to `self.graphics_device_id = self.device_id`.\n4. Save the file and rerun your script to resolve the 'invalid device ordinal' error.", "answer": "G"}
{"uuid": "3588b4ac-8555-4b81-99b6-4282f08c133f", "setup_instruct": "# CLoSD Deployment Plan\n\n## 1. Environment Setup\n- **Description**: Set up Python environment and install dependencies.\n- **Commands**:\n  ```bash\n  conda create -n closd python=3.8\n  conda activate closd\n  pip install -r requirement.txt\n  python -m spacy download en_core_web_sm\n  ```\n\n## 2. Install Isaac GYM\n- **Description**: Install Isaac GYM into the Conda environment.\n- **Commands**:\n  ```bash\n  conda activate closd\n  cd <ISSAC_GYM_DIR>/python\n  pip install -e .\n  ```\n\n## 3. Run CLoSD (Multi-task)\n- **Description**: Execute CLoSD for multi-task control.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big robot=smpl_humanoid\\\n    epoch=-1 test=True no_virtual_display=True\\\n    headless=False env.num_envs=9\\\n    env=closd_multitask exp_name=CLoSD_multitask_finetune\n  ```\n\n## 4. Run CLoSD (Sequence of Tasks)\n- **Description**: Execute CLoSD for sequence of tasks.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big robot=smpl_humanoid\\\n    epoch=-1 test=True no_virtual_display=True\\\n    headless=False env.num_envs=9\\\n    env=closd_sequence exp_name=CLoSD_multitask_finetune\n  ```\n\n## 5. Run CLoSD (Text-to-Motion)\n- **Description**: Execute CLoSD for text-to-motion tasks.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big robot=smpl_humanoid\\\n    epoch=-1 test=True no_virtual_display=True\\\n    headless=False env.num_envs=9\\\n    env=closd_t2m exp_name=CLoSD_t2m_finetune\n  ```\n\n## 6. Evaluate Multi-task Success Rate\n- **Description**: Evaluate multi-task success rate (reproduces Table 1).\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big env=closd_multitask robot=smpl_humanoid\\\n    exp_name=CLoSD_multitask_finetune\\\n    epoch=-1\\\n    env.episode_length=500\\\n    env.dip.cfg_param=7.5\\\n    env.num_envs=4096\\\n    test=True\\\n    no_virtual_display=True\\\n    headless=True\\\n    closd_eval=True\n  ```\n\n## 7. Evaluate Text-to-Motion\n- **Description**: Evaluate text-to-motion performance (reproduces Table 3).\n- **Commands**:\n  ```bash\n  python -m closd.diffusion_planner.eval.eval_humanml --external_results_file closd/diffusion_planner/saved_motions/closd/CloSD.pkl --do_unique\n  ```\n\n## 8. Blender Visualization Setup\n- **Description**: Set up Blender interpreter for visualization.\n- **Commands**:\n  ```bash\n  blender -b -P closd/blender/setup_blender.py\n  ```\n\n## 9. Visualize Recordings\n- **Description**: Visualize IsaacGym recordings using Blender.\n- **Commands**:\n  ```bash\n  blender -b -P closd/blender/record2anim.py -- --record_path output/states/YOUR_RECORD_NAME.pkl\n  ```\n\n## 10. Extract SMPL Parameters\n- **Description**: Extract SMPL parameters from recordings.\n- **Commands**:\n  ```bash\n  python closd/utils/extract_smpl.py --record_path output/states/YOUR_RECORD_NAME.pkl\n  ```\n\n## 11. Train Tracking Controller (PHC-based)\n- **Description**: Train the tracking controller.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big env=im_single_prim robot=smpl_humanoid\\\n    env.cycle_motion=True epoch=-1\\\n    exp_name=my_CLoSD_no_finetune\n  ```\n\n## 12. Fine-tune for Multi-task\n- **Description**: Fine-tune the model for multi-task control.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big env=closd_multitask robot=smpl_humanoid\\\n    learning.params.load_checkpoint=True\\\n    learning.params.load_path=output/CLoSD/my_CLoSD_no_finetune/Humanoid.pth\\\n    env.dip.cfg_param=2.5 env.num_envs=3072\\\n    has_eval=False epoch=-1\\\n    exp_name=my_CLoSD_multitask_finetune\n  ```\n\n## 13. Fine-tune for Text-to-Motion\n- **Description**: Fine-tune the model for text-to-motion tasks.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big env=closd_t2m robot=smpl_humanoid\\\n    learning.params.load_checkpoint=True\\\n    learning.params.load_path=output/CLoSD/my_CLoSD_no_finetune/Humanoid.pth\\\n    env.dip.cfg_param=2.5 env.num_envs=3072\\\n    has_eval=False epoch=-1\\\n    exp_name=my_CLoSD_t2m_finetune\n  ```\n\n## 14. Stand-alone DiP Motion Generation\n- **Description**: Generate motion using stand-alone DiP.\n- **Commands**:\n  ```bash\n  python -m closd.diffusion_planner.sample.generate\\\n    --model_path closd/diffusion_planner/save/DiP_no-target_10steps_context20_predict40/model000200000.pt\\\n    --num_repetitions 1 --autoregressive\n  ```\n\n## 15. Stand-alone DiP Evaluation\n- **Description**: Evaluate stand-alone DiP performance.\n- **Commands**:\n  ```bash\n  python -m closd.diffusion_planner.eval.eval_humanml\\\n    --guidance_param 7.5\\\n    --model_path closd/diffusion_planner/save/DiP_no-target_10steps_context20_predict40/model000600343.pt\\\n    --autoregressive\n  ```\n\n## 16. Train Stand-alone DiP\n- **Description**: Train your own DiP model.\n- **Commands**:\n  ```bash\n  python -m closd.diffusion_planner.train.train_mdm\\\n    --save_dir closd/diffusion_planner/save/my_DiP\\\n    --dataset humanml --arch trans_dec --text_encoder_type bert\\\n    --diffusion_steps 10 --context_len 20 --pred_len 40\\\n    --mask_frames --eval_during_training --gen_during_training --overwrite --use_ema --autoregressive --train_platform_type WandBPlatform\n  ```", "issue_title": "Questions about text-to-motion evaluations", "issue_body": "Dear authors of CLoSD,\r\n\r\nThank you for your excellent work!\r\n\r\nI am very interested in physically plausible motion generation and have a few questions related to the text-to-motion evaluation in the implementation you have released. I apologize in advance for the length of my questions.\r\n\r\n### 1. Regarding the creation of the file 'closd/diffusion_planner/saved_motions/closd/CloSD.pkl'\r\nIn the evaluation section of the code's README, I noted that for text-to-motion evaluation, the command\r\n```python -m closd.diffusion_planner.eval.eval_humanml --external_results_file closd/diffusion_planner/saved_motions/closd/CloSD.pkl --do_unique``` can be used. Upon running this command, I observed that the results were comparable to the physics-based metric of “CLoSD (Ours)” which is illustrated in the fourth row of Table 3. From this, I inferred that the file \"closd/diffusion_planner/saved_motions/closd/CloSD.pkl\" contains motions generated by combining DiP with PHC. However, I have explored the code you provided and was unable to locate the code responsible for creating the \".pkl\" file as shown above. Is this part still to be released? Or am I overlooking it? If it is coming and you are currently refactoring the code, would it be possible to share the code with me via email?\r\n\r\n### 2. Clarification of \"R2G\" and \"G2R\" Functions\r\nAre the \"R2G\" and \"G2R\" functions mentioned in the CLoSD paper equivalent to the hml_to_pose and pose_to_hml methods in the RepresentationHandler class, respectively? If this question is indeed correct, I am curious to know if the following scenario will work. If DiP generated x from text c and obtained the variable $x^{sim}$ via $x^{sim} = hml to pose(x)$ (where $hml to pose$ corresponds to \"hml_to_pose\" in RepresentationHandler), and then obtained the variable $\\hat{x}$ via $\\hat{x} = pose to hml(x^{sim})$ (where $pose to hml$ corresponds to \"pose_to_hml\" in RepresentationHandler), would it be accurate to conclude that $\\hat{x}$ and $x$ would have the same value? It seems to me that “R2G” is the inverse of “G2R,” which leaves this question.\r\n\r\n### 3. How do I evaluate metrics in HumanML3D from the generated motion?\r\nI understand that when evaluating a model that combines DiP and PHC with text-to-motion metrics for HumanML3D, a contrastive model trained on HumanML3D is employed. Since the input of the contrastive model is the pose representation used in HumanML3D, I also understand that the global position for next state, which was simulated by PHC, is converted to the pose representation used in HumanML3D to calculate metrics such as R-precision and FID. **If I understand correctly, is it correct to evaluate the global position of the next state simulated from DiP's generated $x^{pred}$ and PHC's action by converting it to pose representation used in HumanML3D using the \"pose_to_hml\" method in RepresentationHandler?**\r\n\r\nThank you for taking the time to read this long message!", "choices": "(A) 1. Set the `block_type` parameter to `LlamaDecoderLayer` when running the `finetune.py` script. Example: `--block_type=LlamaDecoderLayer`.\n2. Delete the data directory using `rm -rf data/`.\n3. Ensure the data path is correctly specified. Use the `ls` command to verify the contents of the data directory if unsure.\n4. Refer to the updated script or the specific issue (https://github.com/Vahe1994/AQLM/issues/118) for additional parameter settings if needed. (B) 1. Refer to the updated script or the specific issue (https://github.com/Vahe1994/AQLM/issues/118) for additional parameter settings if needed.\n2. Set the `block_type` parameter to `LlamaDecoderLayer` when running the `finetune.py` script. Example: `--block_type=LlamaDecoderLayer`.\n3. Ensure the data path is correctly specified. Use the `ls` command to verify the contents of the data directory if unsure. (C) 1. Set the `block_type` parameter to `LlamaDecoderLayer` when running the `finetune.py` script. Example: `--block_type=LlamaDecoderLayer`.\n2. Refer to the updated script or the specific issue (https://github.com/Vahe1994/AQLM/issues/118) for additional parameter settings if needed. (D) 1. Set the `block_type` parameter to `LlamaDecoderLayer` when running the `finetune.py` script. Example: `--block_type=LlamaDecoderLayer`.\n2. Ensure the data path is correctly specified. Use the `ls` command to verify the contents of the data directory if unsure.\n3. Refer to the updated script or the specific issue (https://github.com/Vahe1994/AQLM/issues/118) for additional parameter settings if needed.", "answer": "D"}
{"uuid": "2426ac20-bb59-4d9a-aa61-9b39da98fe61", "setup_instruct": "# CLoSD Execution Plan\n\n## 1. Environment Setup\n- **Description**: Set up the Python environment and install dependencies.\n- **Commands**:\n  ```bash\n  conda create -n closd python=3.8\n  conda activate closd\n  pip install -r requirement.txt\n  python -m spacy download en_core_web_sm\n  ```\n\n## 2. Install Isaac GYM\n- **Description**: Install Isaac GYM to the environment.\n- **Commands**:\n  ```bash\n  conda activate closd\n  cd <ISSAC_GYM_DIR>/python\n  pip install -e .\n  ```\n\n## 3. Run CLoSD (Multi-task)\n- **Description**: Execute CLoSD for multi-task control.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big robot=smpl_humanoid\\\n    epoch=-1 test=True no_virtual_display=True\\\n    headless=False env.num_envs=9\\\n    env=closd_multitask exp_name=CLoSD_multitask_finetune\n  ```\n\n## 4. Run CLoSD (Sequence of Tasks)\n- **Description**: Execute CLoSD for sequence of tasks.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big robot=smpl_humanoid\\\n    epoch=-1 test=True no_virtual_display=True\\\n    headless=False env.num_envs=9\\\n    env=closd_sequence exp_name=CLoSD_multitask_finetune\n  ```\n\n## 5. Run CLoSD (Text-to-Motion)\n- **Description**: Execute CLoSD for text-to-motion tasks.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big robot=smpl_humanoid\\\n    epoch=-1 test=True no_virtual_display=True\\\n    headless=False env.num_envs=9\\\n    env=closd_t2m exp_name=CLoSD_t2m_finetune\n  ```\n\n## 6. Evaluate Multi-task Success Rate\n- **Description**: Evaluate multi-task success rate.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big env=closd_multitask robot=smpl_humanoid\\\n    exp_name=CLoSD_multitask_finetune\\\n    epoch=-1\\\n    env.episode_length=500\\\n    env.dip.cfg_param=7.5\\\n    env.num_envs=4096\\\n    test=True\\\n    no_virtual_display=True\\\n    headless=True\\\n    closd_eval=True\n  ```\n\n## 7. Evaluate Text-to-Motion\n- **Description**: Evaluate text-to-motion performance.\n- **Commands**:\n  ```bash\n  python -m closd.diffusion_planner.eval.eval_humanml --external_results_file closd/diffusion_planner/saved_motions/closd/CloSD.pkl --do_unique\n  ```\n\n## 8. Blender Visualization Setup\n- **Description**: Set up Blender interpreter for visualization.\n- **Commands**:\n  ```bash\n  blender -b -P closd/blender/setup_blender.py\n  ```\n\n## 9. Visualize Recordings\n- **Description**: Visualize IsaacGym recordings using Blender.\n- **Commands**:\n  ```bash\n  blender -b -P closd/blender/record2anim.py -- --record_path output/states/YOUR_RECORD_NAME.pkl\n  ```\n\n## 10. Extract SMPL Parameters\n- **Description**: Extract SMPL parameters from recordings.\n- **Commands**:\n  ```bash\n  python closd/utils/extract_smpl.py --record_path output/states/YOUR_RECORD_NAME.pkl\n  ```\n\n## 11. Train Tracking Controller (PHC Based)\n- **Description**: Train the tracking controller.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big env=im_single_prim robot=smpl_humanoid\\\n    env.cycle_motion=True epoch=-1\\\n    exp_name=my_CLoSD_no_finetune\n  ```\n\n## 12. Fine-tune for Multi-task\n- **Description**: Fine-tune the model for multi-task control.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big env=closd_multitask robot=smpl_humanoid\\\n    learning.params.load_checkpoint=True\\\n    learning.params.load_path=output/CLoSD/my_CLoSD_no_finetune/Humanoid.pth\\\n    env.dip.cfg_param=2.5 env.num_envs=3072\\\n    has_eval=False epoch=-1\\\n    exp_name=my_CLoSD_multitask_finetune\n  ```\n\n## 13. Fine-tune for Text-to-Motion\n- **Description**: Fine-tune the model for text-to-motion tasks.\n- **Commands**:\n  ```bash\n  python closd/run.py\\\n    learning=im_big env=closd_t2m robot=smpl_humanoid\\\n    learning.params.load_checkpoint=True\\\n    learning.params.load_path=output/CLoSD/my_CLoSD_no_finetune/Humanoid.pth\\\n    env.dip.cfg_param=2.5 env.num_envs=3072\\\n    has_eval=False epoch=-1\\\n    exp_name=my_CLoSD_t2m_finetune\n  ```\n\n## 14. Generate Motion with Stand-alone DiP\n- **Description**: Generate motion using the stand-alone DiP model.\n- **Commands**:\n  ```bash\n  python -m closd.diffusion_planner.sample.generate\\\n    --model_path closd/diffusion_planner/save/DiP_no-target_10steps_context20_predict40/model000200000.pt\\\n    --num_repetitions 1 --autoregressive\n  ```\n\n## 15. Stand-alone Evaluation of DiP\n- **Description**: Evaluate the stand-alone DiP model.\n- **Commands**:\n  ```bash\n  python -m closd.diffusion_planner.eval.eval_humanml\\\n    --guidance_param 7.5\\\n    --model_path closd/diffusion_planner/save/DiP_no-target_10steps_context20_predict40/model000600343.pt\\\n    --autoregressive\n  ```\n\n## 16. Train Your Own DiP\n- **Description**: Train a custom DiP model.\n- **Commands**:\n  ```bash\n  python -m closd.diffusion_planner.train.train_mdm\\\n    --save_dir closd/diffusion_planner/save/my_DiP\\\n    --dataset humanml --arch trans_dec --text_encoder_type bert\\\n    --diffusion_steps 10 --context_len 20 --pred_len 40\\\n    --mask_frames --eval_during_training --gen_during_training --overwrite --use_ema --autoregressive --train_platform_type WandBPlatform\n  ```", "issue_title": "[Error] [carb.gym.plugin] Gym cuda error: invalid device ordinal: ../../../source/plugins/carb/gym/impl/Gym/GymPhysXCuda.cu: 991", "issue_body": "Thanks for your greast work.\r\nMay I ask the cuda version about your environment. I run the command  but can't start the environment. The GPU I used is 4070 with cuda 11-6.\r\n```\r\npython closd/run.py\\\r\n learning=im_big env=im_single_prim robot=smpl_humanoid\\\r\n env.cycle_motion=True epoch=-1\\\r\n exp_name=my_CLoSD_no_finetune\r\n```\r\n```\r\n=> loading checkpoint 'output/CLoSD/CLoSD_multitask_finetune/Humanoid.pth'\r\n=> loading checkpoint 'output/CLoSD/CLoSD_multitask_finetune/Humanoid.pth'\r\n[Error] [carb.gym.plugin] Gym cuda error: invalid device ordinal: ../../../source/plugins/carb/gym/impl/Gym/GymPhysXCuda.cu: 937\r\n[Error] [carb.gym.plugin] Failed to fill root state tensor\r\nUnhandled descriptor set 433\r\nUnhandled descriptor set -1692386480\r\nSegmentation fault (core dumped)\r\n```", "choices": "(A) 1. Run `convert_to_hf.py` with the necessary arguments, including the path to the directory containing the converted files and the `args.pt` file. 2. After running `finetune.py` and `convert_legacy_model_format.py`, ensure the `args.pt` file is present in the target directory. 3. Copy the `args.pt` file from the original quantized model directory to the directory where the converted files are stored. 4. Ensure the output directory (`HF_FINETUNED_QUANTIZED_MODEL_PATH`) is created before running `convert_to_hf.py`. (B) 1. After running `finetune.py` and `convert_legacy_model_format.py`, ensure the `args.pt` file is present in the target directory. 2. Copy the `args.pt` file from the original quantized model directory to the directory where the converted files are stored. 3. Run `convert_to_hf.py` with the necessary arguments, including the path to the directory containing the converted files and the `args.pt` file. (C) 1. After running `finetune.py` and `convert_legacy_model_format.py`, ensure the `args.pt` file is present in the target directory. 2. Ensure the output directory (`HF_FINETUNED_QUANTIZED_MODEL_PATH`) is created before running `convert_to_hf.py`. 3. Copy the `args.pt` file from the original quantized model directory to the directory where the converted files are stored. 4. Run `convert_to_hf.py` with the necessary arguments, including the path to the directory containing the converted files and the `args.pt` file. (D) 1. After running `finetune.py` and `convert_legacy_model_format.py`, ensure the `args.pt` file is present in the target directory. 2. Copy the `args.pt` file from the original quantized model directory to the directory where the converted files are stored. 3. Ensure the output directory (`HF_FINETUNED_QUANTIZED_MODEL_PATH`) is created before running `convert_to_hf.py`. 4. Overwrite the `args.pt` file with an empty file. 5. Run `convert_to_hf.py` with the necessary arguments, including the path to the directory containing the converted files and the `args.pt` file. (E) 1. After running `finetune.py` and `convert_legacy_model_format.py`, ensure the `args.pt` file is present in the target directory. 2. Copy the `args.pt` file from the original quantized model directory to the directory where the converted files are stored. 3. Ensure the output directory (`HF_FINETUNED_QUANTIZED_MODEL_PATH`) is created before running `convert_to_hf.py`. 4. Run `convert_to_hf.py` with the necessary arguments, including the path to the directory containing the converted files and the `args.pt` file. (F) 1. After running `finetune.py` and `convert_legacy_model_format.py`, ensure the `args.pt` file is present in the target directory. 2. Ensure the output directory (`HF_FINETUNED_QUANTIZED_MODEL_PATH`) is created before running `convert_to_hf.py`. 3. Run `convert_to_hf.py` with the necessary arguments, including the path to the directory containing the converted files and the `args.pt` file. (G) 1. After running `finetune.py` and `convert_legacy_model_format.py`, ensure the `args.pt` file is present in the target directory. 2. Copy the `args.pt` file from the original quantized model directory to the directory where the converted files are stored. 3. Ensure the output directory (`HF_FINETUNED_QUANTIZED_MODEL_PATH`) is created before running `convert_to_hf.py`. 4. Delete the `args.pt` file from the directory where the converted files are stored. 5. Run `convert_to_hf.py` with the necessary arguments, including the path to the directory containing the converted files and the `args.pt` file.", "answer": "E"}
{"uuid": "6640ab58-4df9-4dd4-8ac2-33c790dcf7e0", "setup_instruct": "# AQLM Project Execution Plan\n\n## 1. Installation\n- **Description**: Install the AQLM inference library with GPU and CPU support.\n- **Command**:\n  ```bash\n  pip install aqlm[gpu,cpu]>=1.1.6\n  ```\n\n## 2. Model Download\n- **Description**: Download and cache the desired model from Hugging Face.\n- **Command** (example for Llama-2-7b):\n  ```python\n  from transformers import AutoTokenizer, AutoModelForCausalLM\n  model_name = \"meta-llama/Llama-2-7b-hf\"\n  tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=\"auto\")\n  model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n  ```\n\n## 3. Inference Setup\n- **Description**: Load a prequantized model for inference.\n- **Command** (example for Llama-2-7b-AQLM-2Bit-1x16):\n  ```python\n  from transformers import AutoModelForCausalLM\n  quantized_model = AutoModelForCausalLM.from_pretrained(\n      \"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\",\n      trust_remote_code=True, torch_dtype=\"auto\"\n  ).cuda()\n  ```\n\n## 4. Quantization Preparation\n- **Description**: Install dependencies and set up environment for quantization.\n- **Commands**:\n  ```bash\n  pip install -r requirements.txt\n  export CUDA_VISIBLE_DEVICES=0  # or multiple GPUs, e.g., 0,1,2,3\n  export MODEL_PATH=<PATH_TO_MODEL_ON_HUB>\n  export DATASET_PATH=<DATASET_NAME_OR_PATH>\n  export SAVE_PATH=/path/to/save/quantized/model/\n  export WANDB_PROJECT=MY_AQ_EXPS\n  export WANDB_NAME=COOL_EXP_NAME\n  ```\n\n## 5. Run Quantization\n- **Description**: Execute the quantization script with specified parameters.\n- **Command**:\n  ```bash\n  python main.py $MODEL_PATH $DATASET_PATH \\\n   --nsamples=1024 \\\n   --val_size=128 \\\n   --num_codebooks=1 \\\n   --nbits_per_codebook=16 \\\n   --in_group_size=8 \\\n   --relative_mse_tolerance=0.01 \\\n   --finetune_batch_size=32 \\\n   --finetune_max_epochs=10 \\\n   --finetune_early_stop=3 \\\n   --finetune_keep_best \\\n   --local_batch_size=1 \\\n   --offload_activations \\\n   --wandb \\\n   --resume \\\n   --save $SAVE_PATH\n  ```\n\n## 6. Fine-Tuning (Optional)\n- **Description**: Improve quantized model accuracy via PV-Tuning.\n- **Command**:\n  ```bash\n  torchrun --nproc-per-node=$NUM_GPUS finetune.py \\\n      --base_model $MODEL_PATH \\\n      --quantized_model $QUANTIZED_WEIGHTS_PATH \\\n      --model_seqlen=$SEQLEN \\\n      --block_type LlamaDecoderLayer \\\n      --load_dtype bfloat16 \\\n      --amp_dtype bfloat16 \\\n      --code_dtype uint16 \\\n      --dataset_name=$TOKENIZED_DATASET_PATH \\\n      --split none \\\n      --seed 42 \\\n      --preprocessing_chunk_length 100000 \\\n      --cache_dir=$CACHE_DIR \\\n      --trust_remote_code \\\n      --update_codes \\\n      --update_codebooks_and_scales \\\n      --update_non_quantized_parameters \\\n      --lamb \\\n      --debias \\\n      --lr 3e-4 \\\n      --adam_beta1 0.90 \\\n      --adam_beta2 0.95 \\\n      --max_code_change_per_step 1e-2 \\\n      --code_lr 1e-2 \\\n      --code_beta1 0.0 \\\n      --code_beta2 0.95 \\\n      --beam_size 5 \\\n      --delta_decay 0 \\\n      --batch_size=128 \\\n      --microbatch_size=1 \\\n      --max_epochs 1 \\\n      --gradient_checkpointing \\\n      --print_every_steps=1 \\\n      --verbose_optimizer \\\n      --wandb \\\n      --eval_every_steps=10 \\\n      --keep_best_model \\\n      --save $SAVE_PATH \\\n      --save_every_steps 100 \\\n      --attn_implementation flash_attention_2\n  ```\n\n## 7. Evaluation\n- **Description**: Evaluate the model using LM Evaluation Harness.\n- **Commands**:\n  - For 0-shot evals:\n    ```bash\n    python lmeval.py \\\n        --model hf \\\n        --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n        --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \\\n        --batch_size <EVAL_BATCH_SIZE> \\\n        --aqlm_checkpoint_path $QUANTIZED_MODEL\n    ```\n  - For 5-shot MMLU:\n    ```bash\n    python lmeval.py \\\n        --model hf \\\n        --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n        --tasks mmlu \\\n        --batch_size <EVAL_BATCH_SIZE> \\\n        --num_fewshot 5 \\\n        --aqlm_checkpoint_path $QUANTIZED_MODEL\n    ```\n\n## 8. Convert to Hugging Face Format\n- **Description**: Convert the quantized model to Hugging Face format.\n- **Command**:\n  ```bash\n  python convert_to_hf.py meta-llama/Llama-2-7b-hf ./path/to/saved/quantization ./converted-llama2-7b-hf --save_safetensors\n  ```", "issue_title": "it run", "issue_body": "https://huggingface.co/ISTA-DASLab/Llama-3.2-1B-Instruct-AQLM-PV-2Bit-2x8\r\nhttps://github.com/yeyu2/Youtube_demos/blob/main/Mixtral_of_aqlm_transformers.ipynb\r\nhttps://huggingface.co/docs/transformers/main/en/quantization/aqlm\r\nhttps://pytorch.org/get-started/previous-versions/\r\n\r\n\r\n\r\n%%capture\r\n!pip install aqlm[gpu]>=1.0.1\r\n!pip install accelerate>=0.27.0\r\n!pip install transformers>=4.38.0\r\n\r\n\r\n!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu118\r\n\r\n!pip install aqlm[gpu]==1.0.1\r\n!pip install git+https://github.com/huggingface/accelerate.git@main\r\n!pip install git+https://github.com/BlackSamorez/transformers.git@aqlm\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\n\r\nquantized_model = AutoModelForCausalLM.from_pretrained(\r\n    \"ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf\",\r\n    torch_dtype=\"auto\", device_map=\"auto\", low_cpu_mem_usage=True,\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(\"ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf\")\r\n\r\n%%time\r\noutput = quantized_model.generate(tokenizer(\"The relationship between humans and AI  \", return_tensors=\"pt\")[\"input_ids\"].cuda(), min_new_tokens=128, max_new_tokens=128)\r\nprint(tokenizer.decode(output[0]))\r\n\r\nimport json\r\nimport textwrap\r\n\r\nsystem_prompt = \"A chat between a curious user and an blog writing assistant. \"\r\n\r\ndef get_prompt(human_prompt):\r\n    prompt_template=f\"{system_prompt}\\n\\nUSER: {human_prompt} \\nASSISTANT: \"\r\n    return prompt_template\r\n\r\n\r\ndef remove_human_text(text):\r\n    return text.split('USER:', 1)[0]\r\n\r\ndef parse_text(data):\r\n    for item in data:\r\n        text = item['generated_text']\r\n        assistant_text_index = text.find('ASSISTANT:')\r\n        if assistant_text_index != -1:\r\n            assistant_text = text[assistant_text_index+len('ASSISTANT:'):].strip()\r\n            assistant_text = remove_human_text(assistant_text)\r\n            wrapped_text = textwrap.fill(assistant_text, width=100)\r\n            print(\"#####\", wrapped_text)\r\n            # return assistant_text\r\n\r\n\r\nfrom transformers import GenerationConfig, pipeline\r\n\r\n\r\npipe = pipeline(\r\n    \"text-generation\",\r\n    model=quantized_model,\r\n    tokenizer=tokenizer,\r\n    max_length=1200,\r\n    temperature=0.7,\r\n    top_p=0.95,\r\n    do_sample=True,\r\n)\r\n\r\n%%time\r\nprompt = '''Write a short and engaging blog post of travelling in Bohol Island.\r\n          '''\r\nraw_output = pipe(get_prompt(prompt))\r\n\r\nparse_text(raw_output)", "choices": "(A) 1. For WSL2 users, ensure the system correctly recognizes the GPU and can allocate sufficient memory. Check WSL2 documentation for known limitations and configuration tips.\n2. Reduce the `nsamples` parameter to a smaller value (e.g., 512) to decrease memory usage when using the `--offload_activations` flag.\n3. Alternatively, avoid using the `--offload_activations` flag and distribute the workload across multiple GPU devices if available. (B) 1. Reduce the `nsamples` parameter to a smaller value (e.g., 512) to decrease memory usage when using the `--offload_activations` flag.\n2. Alternatively, avoid using the `--offload_activations` flag and distribute the workload across multiple GPU devices if available.\n3. Increase `nsamples` to 8192 to improve model accuracy while keeping `--offload_activations` enabled.\n4. For WSL2 users, ensure the system correctly recognizes the GPU and can allocate sufficient memory. Check WSL2 documentation for known limitations and configuration tips. (C) 1. Reduce the `nsamples` parameter to a smaller value (e.g., 512) to decrease memory usage when using the `--offload_activations` flag.\n2. For WSL2 users, ensure the system correctly recognizes the GPU and can allocate sufficient memory. Check WSL2 documentation for known limitations and configuration tips. (D) 1. Reduce the `nsamples` parameter to a smaller value (e.g., 512) to decrease memory usage when using the `--offload_activations` flag.\n2. Alternatively, avoid using the `--offload_activations` flag and distribute the workload across multiple GPU devices if available.\n3. For WSL2 users, ensure the system correctly recognizes the GPU and can allocate sufficient memory. Check WSL2 documentation for known limitations and configuration tips.", "answer": "D"}
{"uuid": "3ebc30f4-8fcd-4af8-a075-d0409fbdc8d4", "setup_instruct": "```markdown\n### Step 1: Installation\n1. Install the AQLM inference library with GPU and CPU support:\n   ```bash\n   pip install aqlm[gpu,cpu]>=1.1.6\n   ```\n\n### Step 2: Model Download\n1. Download and cache the desired model from Hugging Face:\n   ```python\n   from transformers import AutoTokenizer, AutoModelForCausalLM\n   model_name = \"meta-llama/Llama-2-7b-hf\"  # or other model\n   tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=\"auto\")\n   model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n   ```\n\n### Step 3: Inference\n1. Load a pre-quantized model for inference:\n   ```python\n   from transformers import AutoModelForCausalLM\n   quantized_model = AutoModelForCausalLM.from_pretrained(\n       \"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\",\n       trust_remote_code=True, torch_dtype=\"auto\"\n   ).cuda()\n   ```\n\n### Step 4: Quantization Preparation\n1. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n2. Prepare dataset (RedPajama example):\n   ```bash\n   TARGET_MODEL=meta-llama/Llama-2-7b-hf\n   SEQLEN=4096\n   DATASET=togethercomputer/RedPajama-Data-1T-Sample\n   OUTPUT_PATH=./redpajama_tokenized_llama2\n\n   CUDA_VISIBLE_DEVICES=0 HF_HOME=/mnt/LLM OMP_NUM_THREADS=16 torchrun --master-port 3456 --nproc-per-node=1 finetune.py --base_model $TARGET_MODEL --quantized_model ./doesnt_matter --dtype bfloat16 --block_type LlamaDecoderLayer --dataset_name=$DATASET --split train --dataset_config_name plain_text --cache_dir=./cache_dir --trust_remote_code --model_seqlen=$SEQLEN --preprocessing_num_workers=64 --preprocessing_chunk_length 100000 --save_dataset_and_exit $OUTPUT_PATH\n   ```\n\n### Step 5: Model Quantization\n1. Run quantization script:\n   ```bash\n   export CUDA_VISIBLE_DEVICES=0\n   export MODEL_PATH=meta-llama/Llama-2-7b-hf\n   export DATASET_PATH=./data/red_pajama_n=1024_4096_context_length.pth\n   export SAVE_PATH=/path/to/save/quantized/model/\n\n   python main.py $MODEL_PATH $DATASET_PATH \\\n    --nsamples=1024 \\\n    --val_size=128 \\\n    --num_codebooks=1 \\\n    --nbits_per_codebook=16 \\\n    --in_group_size=8 \\\n    --relative_mse_tolerance=0.01 \\\n    --finetune_batch_size=32 \\\n    --finetune_max_epochs=10 \\\n    --finetune_early_stop=3 \\\n    --finetune_keep_best \\\n    --local_batch_size=1 \\\n    --offload_activations \\\n    --save $SAVE_PATH\n   ```\n\n### Step 6: Finetuning (PV-Tuning)\n1. Run finetuning script:\n   ```bash\n   torchrun --nproc-per-node=$NUM_GPUS finetune.py \\\n       --base_model $MODEL_PATH \\\n       --quantized_model $QUANTIZED_WEIGHTS_PATH \\\n       --model_seqlen=$SEQLEN \\\n       --block_type LlamaDecoderLayer \\\n       --load_dtype bfloat16 \\\n       --amp_dtype bfloat16 \\\n       --code_dtype uint16 \\\n       --dataset_name=$TOKENIZED_DATASET_PATH \\\n       --split none \\\n       --seed 42 \\\n       --preprocessing_chunk_length 100000 \\\n       --cache_dir=$CACHE_DIR \\\n       --trust_remote_code \\\n       --update_codes \\\n       --update_codebooks_and_scales \\\n       --update_non_quantized_parameters \\\n       --lamb \\\n       --debias \\\n       --lr 3e-4 \\\n       --adam_beta1 0.90 \\\n       --adam_beta2 0.95 \\\n       --max_code_change_per_step 1e-2 \\\n       --code_lr 1e-2 \\\n       --code_beta1 0.0 \\\n       --code_beta2 0.95 \\\n       --beam_size 5 \\\n       --delta_decay 0 \\\n       --batch_size=128 \\\n       --microbatch_size=1 \\\n       --max_epochs 1 \\\n       --gradient_checkpointing \\\n       --print_every_steps=1 \\\n       --verbose_optimizer \\\n       --wandb \\\n       --eval_every_steps=10 \\\n       --keep_best_model \\\n       --save $SAVE_PATH \\\n       --save_every_steps 100 \\\n       --attn_implementation flash_attention_2\n   ```\n\n### Step 7: Model Conversion\n1. Convert quantized model to Hugging Face format:\n   ```bash\n   python convert_to_hf.py meta-llama/Llama-2-7b-hf ./path/to/saved/quantization ./converted-llama2-7b-hf --save_safetensors\n   ```\n\n### Step 8: Evaluation\n1. Run LM Evaluation Harness:\n   ```bash\n   export QUANTIZED_MODEL=./converted-llama2-7b-hf\n   export MODEL_PATH=meta-llama/Llama-2-7b-hf\n\n   # 0-shot evaluation\n   python lmeval.py \\\n       --model hf \\\n       --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n       --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \\\n       --batch_size 8 \\\n       --aqlm_checkpoint_path $QUANTIZED_MODEL\n\n   # 5-shot MMLU\n   python lmeval.py \\\n       --model hf \\\n       --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n       --tasks mmlu \\\n       --batch_size 8 \\\n       --num_fewshot 5 \\\n       --aqlm_checkpoint_path $QUANTIZED_MODEL\n   ```", "issue_title": "convert_to_hf.py does not work after finetune.py and convert_legacy_model_format.py", "issue_body": "Hello!\r\n\r\n`convert_to_hf.py` fails after running `finetune.py` and `convert_legacy_model_format.py` in sequence. \r\n\r\nNext we will talk about the `main (a441a3f) branch`.\r\n\r\nThe `main.py` file stores the result as [0.pth, ..., 32.pth](https://github.com/Vahe1994/AQLM/blob/main/main.py#L312), [args.pt](https://github.com/Vahe1994/AQLM/blob/main/main.py#L358), [not_quantized_weights.pt](https://github.com/Vahe1994/AQLM/blob/main/src/modelutils.py#L276). \r\n\r\nThen run `finetune.py`, `convert_legacy_model_format.py`, `convert_to_hf.py` in sequence. Executing `convert_legacy_model_format.py` results in files [0.pth, ..., 32.pth, not_quantized_weights.pt](https://github.com/Vahe1994/AQLM/blob/main/convert_legacy_model_format.py#L210) without the `args.pt` file, which [requires](https://github.com/Vahe1994/AQLM/blob/main/convert_to_hf.py#L81) the `convert_to_hf.py` file. Consequently, running `convert_to_hf.py` will terminate with an error.\r\n\r\nPrior to the last [MR](https://github.com/Vahe1994/AQLM/commit/a441a3f0ece4cbaa2a91a3421c95a8b7432e4d99) (`a441a3f`), the main branch state was at commit `559a366`, which had a special [copy of the args.pt](https://github.com/Vahe1994/AQLM/blob/559a36681398d7189297fccf3b1e59e8e030e942/finetune.py#L396) file in `finetune.py`  so that running `convert_to_hf.py` would work correctly.\r\n\r\nThe problem can be circumvented by copying the args.pt file to the correct directory, as shown in the example below:\r\n```\r\nMNT_PATH=...\r\n\r\nBASE_MODEL_NAME=meta-llama/Llama-2-7b-hf\r\nQUANTIZED_MODEL_PATH=${MNT_PATH}/quantized_llama_2_7b\r\nFINETUNED_QUANTIZED_MODEL_PATH=${MNT_PATH}/finetuned_quantized_llama_2_7b\r\nP_FINETUNED_STATE_DICT=${FINETUNED_QUANTIZED_MODEL_PATH}/quantized_model_state_dict_rank0.pt\r\n\r\nSAME_FORMAT_AS_QUANTIZED_MODEL_PATH=${MNT_PATH}/tmp\r\n\r\npython3 AQLM/convert_legacy_model_format.py \\\r\n    --base_model $BASE_MODEL_NAME \\\r\n    --quantized_model $QUANTIZED_MODEL_PATH \\\r\n    --p_finetuned_state_dict $P_FINETUNED_STATE_DICT \\\r\n    --save $SAME_FORMAT_AS_QUANTIZED_MODEL_PATH\r\n\r\n############################################################################\r\ncp $QUANTIZED_MODEL_PATH/args.pt $SAME_FORMAT_AS_QUANTIZED_MODEL_PATH\r\n############################################################################\r\n\r\nHF_FINETUNED_QUANTIZED_MODEL_PATH=${MNT_PATH}/finetuned_quantized_llama_2_7b_hf\r\nmkdir -p $HF_FINETUNED_QUANTIZED_MODEL_PATH\r\n\r\npython3 AQLM/convert_to_hf.py \\\r\n    $BASE_MODEL_NAME \\\r\n    $SAME_FORMAT_AS_QUANTIZED_MODEL_PATH \\\r\n    $HF_FINETUNED_QUANTIZED_MODEL_PATH \\\r\n    --save_safetensors \\\r\n    --save_tokenizer\r\n```\r\n\r\nHowever, this solution requires the user to understand how the python files of the AQLM repository are organized and what data they create.\r\n\r\nThe requirement for the user to create the `HF_FINETUNED_QUANTIZED_MODEL_PATH` (`out_path` in `AQLM/convert_to_hf.py`) directory also looks strange. If it is not created in advance, `convert_to_hf.py` will generate an error.", "choices": "(A) 1. If using quantization, ensure proper parameters are set (e.g., `--scale_nbits=4` for 4-bit quantization). 2. Upgrade to a GPU with at least 80GB of memory to resolve the CUDA out-of-memory error. 3. Use `--offload_activations` to reduce VRAM usage if needed. (B) 1. If using quantization, ensure proper parameters are set (e.g., `--scale_nbits=4` for 4-bit quantization). 2. Use `--offload_activations` to reduce VRAM usage if needed. (C) 1. Use `--offload_activations` to reduce VRAM usage if needed. 2. Upgrade to a GPU with at least 80GB of memory to resolve the CUDA out-of-memory error. 3. If using quantization, ensure proper parameters are set (e.g., `--scale_nbits=4` for 4-bit quantization). (D) 1. Upgrade to a GPU with at least 80GB of memory to resolve the CUDA out-of-memory error. 2. Use `--offload_activations` to reduce VRAM usage if needed. (E) 1. Upgrade to a GPU with at least 80GB of memory to resolve the CUDA out-of-memory error. 2. Disable quantization entirely (`--no_quantization`). 3. If using quantization, ensure proper parameters are set (e.g., `--scale_nbits=4` for 4-bit quantization). 4. Use `--offload_activations` to reduce VRAM usage if needed. (F) 1. Upgrade to a GPU with at least 80GB of memory to resolve the CUDA out-of-memory error. 2. If using quantization, ensure proper parameters are set (e.g., `--scale_nbits=4` for 4-bit quantization). 3. Reduce GPU memory allocation to 40GB (`--max_memory=40GB`). 4. Use `--offload_activations` to reduce VRAM usage if needed. (G) 1. Upgrade to a GPU with at least 80GB of memory to resolve the CUDA out-of-memory error. 2. If using quantization, ensure proper parameters are set (e.g., `--scale_nbits=4` for 4-bit quantization). 3. Use `--offload_activations` to reduce VRAM usage if needed.", "answer": "G"}
{"uuid": "69a71799-f6c4-4e2f-94bd-b16b39489bc6", "setup_instruct": "# AQLM Project Execution Plan\n\n## 1. Installation\n- **Description**: Install the AQLM inference library with GPU and CPU support.\n- **Command**:\n  ```bash\n  pip install aqlm[gpu,cpu]>=1.1.6\n  ```\n\n## 2. Model Download\n- **Description**: Download and cache the desired model from Hugging Face.\n- **Command** (example for Llama-2-7b):\n  ```python\n  from transformers import AutoTokenizer, AutoModelForCausalLM\n  model_name = \"meta-llama/Llama-2-7b-hf\"\n  tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=\"auto\")\n  model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n  ```\n\n## 3. Inference Setup\n- **Description**: Load a prequantized model for inference.\n- **Command** (example for Llama-2-7b-2Bit-1x16):\n  ```python\n  from transformers import AutoModelForCausalLM\n  quantized_model = AutoModelForCausalLM.from_pretrained(\n      \"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\",\n      trust_remote_code=True, torch_dtype=\"auto\"\n  ).cuda()\n  ```\n\n## 4. Quantization Preparation\n- **Description**: Install dependencies and set up environment for quantization.\n- **Commands**:\n  ```bash\n  pip install -r requirements.txt\n  export CUDA_VISIBLE_DEVICES=0  # Set GPU(s)\n  export MODEL_PATH=<PATH_TO_MODEL_ON_HUB>\n  export DATASET_PATH=<DATASET_NAME_OR_PATH>\n  export SAVE_PATH=/path/to/save/quantized/model/\n  ```\n\n## 5. Run Quantization\n- **Description**: Execute the main quantization script with desired parameters.\n- **Command**:\n  ```bash\n  python main.py $MODEL_PATH $DATASET_PATH \\\n   --nsamples=1024 \\\n   --val_size=128 \\\n   --num_codebooks=1 \\\n   --nbits_per_codebook=16 \\\n   --in_group_size=8 \\\n   --relative_mse_tolerance=0.01 \\\n   --finetune_batch_size=32 \\\n   --finetune_max_epochs=10 \\\n   --finetune_early_stop=3 \\\n   --finetune_keep_best \\\n   --local_batch_size=1 \\\n   --offload_activations \\\n   --wandb \\\n   --resume \\\n   --save $SAVE_PATH\n  ```\n\n## 6. Data Preparation (Optional)\n- **Description**: Pre-tokenize RedPajama data for fine-tuning.\n- **Command**:\n  ```bash\n  TARGET_MODEL=meta-llama/Llama-2-7b-hf\n  SEQLEN=4096\n  DATASET=togethercomputer/RedPajama-Data-1T-Sample\n  OUTPUT_PATH=./redpajama_tokenized_llama2\n\n  CUDA_VISIBLE_DEVICES=0 HF_HOME=/mnt/LLM OMP_NUM_THREADS=16 torchrun --master-port 3456 --nproc-per-node=1 finetune.py --base_model $TARGET_MODEL --quantized_model ./doesnt_matter --dtype bfloat16 --block_type LlamaDecoderLayer --dataset_name=$DATASET --split train --dataset_config_name plain_text --cache_dir=./cache_dir --trust_remote_code --model_seqlen=$SEQLEN --preprocessing_num_workers=64 --preprocessing_chunk_length 100000 --save_dataset_and_exit $OUTPUT_PATH\n  ```\n\n## 7. Fine-Tuning (Optional)\n- **Description**: Improve quantized model accuracy with PV-Tuning.\n- **Command**:\n  ```bash\n  torchrun --nproc-per-node=$NUM_GPUS finetune.py \\\n      --base_model $MODEL_PATH \\\n      --quantized_model $QUANTIZED_WEIGHTS_PATH \\\n      --model_seqlen=$SEQLEN \\\n      --block_type LlamaDecoderLayer \\\n      --load_dtype bfloat16 \\\n      --amp_dtype bfloat16 \\\n      --code_dtype uint16 \\\n      --dataset_name=$TOKENIZED_DATASET_PATH \\\n      --split none \\\n      --seed 42 \\\n      --preprocessing_chunk_length 100000 \\\n      --cache_dir=$CACHE_DIR \\\n      --trust_remote_code \\\n      --update_codes \\\n      --update_codebooks_and_scales \\\n      --update_non_quantized_parameters \\\n      --lamb \\\n      --debias \\\n      --lr 3e-4 \\\n      --adam_beta1 0.90 \\\n      --adam_beta2 0.95 \\\n      --max_code_change_per_step 1e-2 \\\n      --code_lr 1e-2 \\\n      --code_beta1 0.0 \\\n      --code_beta2 0.95 \\\n      --beam_size 5 \\\n      --delta_decay 0 \\\n      --batch_size=128 \\\n      --microbatch_size=1 \\\n      --max_epochs 1 \\\n      --gradient_checkpointing \\\n      --print_every_steps=1 \\\n      --verbose_optimizer \\\n      --wandb \\\n      --eval_every_steps=10 \\\n      --keep_best_model \\\n      --save $SAVE_PATH \\\n      --save_every_steps 100 \\\n      --attn_implementation flash_attention_2\n  ```\n\n## 8. Evaluation\n- **Description**: Run zero-shot benchmarks using LM Evaluation Harness.\n- **Commands**:\n  ```bash\n  # For 0-shot evaluation\n  python lmeval.py \\\n      --model hf \\\n      --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n      --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \\\n      --batch_size <EVAL_BATCH_SIZE> \\\n      --aqlm_checkpoint_path $QUANTIZED_MODEL\n\n  # For 5-shot MMLU evaluation\n  python lmeval.py \\\n      --model hf \\\n      --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n      --tasks mmlu \\\n      --batch_size <EVAL_BATCH_SIZE> \\\n      --num_fewshot 5 \\\n      --aqlm_checkpoint_path $QUANTIZED_MODEL\n  ```\n\n## 9. Model Conversion\n- **Description**: Convert quantized model to Hugging Face format.\n- **Command**:\n  ```bash\n  python convert_to_hf.py meta-llama/Llama-2-7b-hf ./path/to/saved/quantization ./converted-llama2-7b-hf --save_safetensors\n  ```", "issue_title": "FV tuning based on GPTQ", "issue_body": "FV tuning achieves very impressive result even on GPTQ. Is there any plan to relase the GPTQ-version of FV tuning? If not, shoudl we modify it based on finetune_fsdp.py?\r\n\r\nThe other question is your released model cover instruct-version model. May we know what's the dataset for non-pretrained model?", "choices": "(A) 1. Replace `LlamaTokenizer` with `AutoTokenizer` in the code to resolve the tokenizer type mismatch issue.\n2. Ensure sufficient GPU memory is available or reduce the microbatch size to address CUDA out-of-memory errors.\n3. Use `--finetune_dtype=float32` to maximize precision (conflicts with memory reduction).\n4. Optionally, use `--finetune_dtype=bfloat16` to further reduce memory usage during fine-tuning. (B) 1. Ensure sufficient GPU memory is available or reduce the microbatch size to address CUDA out-of-memory errors.\n2. Optionally, use `--finetune_dtype=bfloat16` to further reduce memory usage during fine-tuning. (C) 1. Replace `LlamaTokenizer` with `AutoTokenizer` in the code to resolve the tokenizer type mismatch issue.\n2. Optionally, use `--finetune_dtype=bfloat16` to further reduce memory usage during fine-tuning. (D) 1. Replace `LlamaTokenizer` with `AutoTokenizer` in the code to resolve the tokenizer type mismatch issue.\n2. Ensure sufficient GPU memory is available or reduce the microbatch size to address CUDA out-of-memory errors.\n3. Optionally, use `--finetune_dtype=bfloat16` to further reduce memory usage during fine-tuning. (E) 1. Optionally, use `--finetune_dtype=bfloat16` to further reduce memory usage during fine-tuning.\n2. Replace `LlamaTokenizer` with `AutoTokenizer` in the code to resolve the tokenizer type mismatch issue.\n3. Ensure sufficient GPU memory is available or reduce the microbatch size to address CUDA out-of-memory errors. (F) 1. Replace `LlamaTokenizer` with `AutoTokenizer` in the code to resolve the tokenizer type mismatch issue.\n2. Manually clear GPU memory using `torch.cuda.empty_cache()` (may cause instability).\n3. Ensure sufficient GPU memory is available or reduce the microbatch size to address CUDA out-of-memory errors.\n4. Optionally, use `--finetune_dtype=bfloat16` to further reduce memory usage during fine-tuning. (G) 1. Ensure sufficient GPU memory is available or reduce the microbatch size to address CUDA out-of-memory errors.\n2. Replace `LlamaTokenizer` with `AutoTokenizer` in the code to resolve the tokenizer type mismatch issue.\n3. Optionally, use `--finetune_dtype=bfloat16` to further reduce memory usage during fine-tuning.", "answer": "D"}
{"uuid": "436ca996-bd71-43f1-ba3b-f4e14f8c52f1", "setup_instruct": "# AQLM Deployment Plan\n\n## 1. Installation\n- **Description**: Install the AQLM inference library with GPU and CPU support.\n- **Command**:\n  ```bash\n  pip install aqlm[gpu,cpu]>=1.1.6\n  ```\n\n## 2. Model Download\n- **Description**: Download and cache the desired prequantized model from Hugging Face Hub.\n- **Command** (example for Llama-2-7b 2Bit 1x16):\n  ```python\n  from transformers import AutoModelForCausalLM\n  model = AutoModelForCausalLM.from_pretrained(\n      \"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\",\n      trust_remote_code=True,\n      torch_dtype=\"auto\"\n  ).cuda()\n  ```\n\n## 3. Inference Setup\n- **Description**: Configure the model for inference with appropriate attention implementation.\n- **Note**: Use `torch_dtype=\"auto\"` for GPU or `torch.float32` for CPU.\n\n## 4. Quantization (Optional)\n### 4.1 Install Dependencies\n- **Description**: Install required packages for quantization.\n- **Command**:\n  ```bash\n  pip install -r requirements.txt\n  ```\n\n### 4.2 Prepare Dataset\n- **Description**: Download and preprocess calibration data (e.g., RedPajama).\n- **Command** (example for RedPajama):\n  ```bash\n  TARGET_MODEL=meta-llama/Llama-2-7b-hf\n  SEQLEN=4096\n  DATASET=togethercomputer/RedPajama-Data-1T-Sample\n  OUTPUT_PATH=./redpajama_tokenized_llama2\n\n  torchrun --nproc-per-node=1 finetune.py \\\n      --base_model $TARGET_MODEL \\\n      --quantized_model ./dummy \\\n      --dataset_name=$DATASET \\\n      --save_dataset_and_exit $OUTPUT_PATH\n  ```\n\n### 4.3 Run Quantization\n- **Description**: Quantize the model using AQLM with specified parameters.\n- **Command**:\n  ```bash\n  export CUDA_VISIBLE_DEVICES=0\n  export MODEL_PATH=meta-llama/Llama-2-7b-hf\n  export DATASET_PATH=./redpajama_tokenized_llama2\n  export SAVE_PATH=./quantized_model\n\n  python main.py $MODEL_PATH $DATASET_PATH \\\n      --nsamples=1024 \\\n      --num_codebooks=1 \\\n      --nbits_per_codebook=16 \\\n      --save $SAVE_PATH\n  ```\n\n## 5. Finetuning (Optional)\n- **Description**: Improve quantized model accuracy using PV-Tuning.\n- **Command**:\n  ```bash\n  torchrun --nproc-per-node=4 finetune.py \\\n      --base_model $MODEL_PATH \\\n      --quantized_model $SAVE_PATH \\\n      --dataset_name=$DATASET_PATH \\\n      --save ./finetuned_model\n  ```\n\n## 6. Evaluation\n- **Description**: Evaluate model performance using LM Evaluation Harness.\n- **Command** (example for 5-shot MMLU):\n  ```bash\n  python lmeval.py \\\n      --model hf \\\n      --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n      --tasks mmlu \\\n      --num_fewshot 5 \\\n      --aqlm_checkpoint_path $SAVE_PATH\n  ```\n\n## 7. Convert to Hugging Face Format\n- **Description**: Convert quantized model to Hugging Face format for deployment.\n- **Command**:\n  ```bash\n  python convert_to_hf.py meta-llama/Llama-2-7b-hf ./quantized_model ./hf_model\n  ```\n\n## 8. Serve Model\n- **Description**: Deploy the model using `vLLM` or similar serving frameworks.\n- **Note**: Refer to provided Colab notebooks for serving examples.", "issue_title": "Request for Nvidia's RAG Implementation of Llama-3-70B \"ChatQA 1.5\"", "issue_body": "I'd love to see [this 'un](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B) as an AQLM 2-bit 1x16! I imagine it'd be a popular option for people -- at least it'd be the most logical solution for a RAG in my mind, for anyone with \"limited\" VRAM. \r\n\r\nI am TERRIBLY \"limited\" with 64GB VRAM and figure with a 2-bit RAG + 4-bit 70B-Instruct I'll actually be able to continue using my PC without issues/hiccups 😂...or on 2nd thought I'd swap out the 4-bit with a 2-bit WizardLM2-8x22B hint hint plz wink wink plz cough cough 🤪\r\n\r\nOf course, with how long it takes & a handful of new models to 2-bitify, introduce into the AQLM ecosystem, I'd understand this one being a low-priority addition to the mix\r\n\r\nhttps://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B", "choices": "(A) 1. Ensure you are using a GPU with Compute Capability greater than 8 (e.g., Tesla V100 is not supported for the latest dequantization kernels).  \n2. Downgrade `aqlm` to version `1.1.2` if you are using a GPU with Compute Capability 8 or less.  \n3. Alternatively, update `aqlm` to version `1.1.4` or later, which includes a fix for the compilation issue on GPUs with Compute Capability 8 or less. (B) 1. Update `aqlm` to version `1.1.4` or later, which includes a fix for the compilation issue on GPUs with Compute Capability 8 or less.  \n2. Ensure you are using a GPU with Compute Capability greater than 8 (e.g., Tesla V100 is not supported for the latest dequantization kernels).  \n3. Downgrade `aqlm` to version `1.1.2` if you are using a GPU with Compute Capability 8 or less.  \n4. If using `torch_dtype`, ensure it is set to `torch.float16` (default) unless `bfloat16` is explicitly required and supported by your GPU. (C) 1. If using `torch_dtype`, ensure it is set to `torch.float16` (default) unless `bfloat16` is explicitly required and supported by your GPU.  \n2. Ensure you are using a GPU with Compute Capability greater than 8 (e.g., Tesla V100 is not supported for the latest dequantization kernels).  \n3. Downgrade `aqlm` to version `1.1.2` if you are using a GPU with Compute Capability 8 or less.  \n4. Alternatively, update `aqlm` to version `1.1.4` or later, which includes a fix for the compilation issue on GPUs with Compute Capability 8 or less. (D) 1. Ensure you are using a GPU with Compute Capability greater than 8 (e.g., Tesla V100 is not supported for the latest dequantization kernels).\n2. Downgrade `aqlm` to version `1.1.2` if you are using a GPU with Compute Capability 8 or less.\n3. Alternatively, update `aqlm` to version `1.1.4` or later, which includes a fix for the compilation issue on GPUs with Compute Capability 8 or less.\n4. If using `torch_dtype`, ensure it is set to `torch.float16` (default) unless `bfloat16` is explicitly required and supported by your GPU. (E) 1. Downgrade `aqlm` to version `1.1.2` if you are using a GPU with Compute Capability 8 or less.  \n2. Alternatively, update `aqlm` to version `1.1.4` or later, which includes a fix for the compilation issue on GPUs with Compute Capability 8 or less.  \n3. If using `torch_dtype`, ensure it is set to `torch.float16` (default) unless `bfloat16` is explicitly required and supported by your GPU. (F) 1. Uninstall `aqlm` to avoid conflicts.  \n2. Ensure you are using a GPU with Compute Capability greater than 8 (e.g., Tesla V100 is not supported for the latest dequantization kernels).  \n3. Downgrade `aqlm` to version `1.1.2` if you are using a GPU with Compute Capability 8 or less.  \n4. Alternatively, update `aqlm` to version `1.1.4` or later, which includes a fix for the compilation issue on GPUs with Compute Capability 8 or less.  \n5. If using `torch_dtype`, ensure it is set to `torch.float16` (default) unless `bfloat16` is explicitly required and supported by your GPU. (G) 1. Ensure you are using a GPU with Compute Capability greater than 8 (e.g., Tesla V100 is not supported for the latest dequantization kernels).  \n2. Downgrade `aqlm` to version `1.1.2` if you are using a GPU with Compute Capability 8 or less.  \n3. Alternatively, update `aqlm` to version `1.1.4` or later, which includes a fix for the compilation issue on GPUs with Compute Capability 8 or less.  \n4. Set `torch_dtype` to `torch.float64` for higher precision.  \n5. If using `torch_dtype`, ensure it is set to `torch.float16` (default) unless `bfloat16` is explicitly required and supported by your GPU.", "answer": "D"}
{"uuid": "42dacaac-6bd3-4c4e-be63-99c954c069ad", "setup_instruct": "# AQLM Project Execution Plan\n\n## 1. Installation\n- **Description**: Install the AQLM inference library with GPU and CPU support\n- **Command**:\n  ```bash\n  pip install aqlm[gpu,cpu]>=1.1.6\n  ```\n\n## 2. Model Download\n- **Description**: Download and cache the base model from Hugging Face\n- **Command**:\n  ```python\n  from transformers import AutoTokenizer, AutoModelForCausalLM\n  model_name = \"meta-llama/Llama-2-7b-hf\"  # or other model\n  tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=\"auto\")\n  model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n  ```\n\n## 3. Dataset Preparation\n- **Description**: Download and preprocess RedPajama dataset for quantization\n- **Command**:\n  ```bash\n  TARGET_MODEL=meta-llama/Llama-2-7b-hf\n  SEQLEN=4096\n  DATASET=togethercomputer/RedPajama-Data-1T-Sample\n  OUTPUT_PATH=./redpajama_tokenized_llama2\n\n  CUDA_VISIBLE_DEVICES=0 HF_HOME=/mnt/LLM OMP_NUM_THREADS=16 torchrun --master-port 3456 --nproc-per-node=1 finetune.py --base_model $TARGET_MODEL --quantized_model ./doesnt_matter --dtype bfloat16 --block_type LlamaDecoderLayer --dataset_name=$DATASET --split train --dataset_config_name plain_text --cache_dir=./cache_dir --trust_remote_code --model_seqlen=$SEQLEN --preprocessing_num_workers=64 --preprocessing_chunk_length 100000 --save_dataset_and_exit $OUTPUT_PATH\n  ```\n\n## 4. Model Quantization\n- **Description**: Run AQLM quantization on the model\n- **Command**:\n  ```bash\n  export CUDA_VISIBLE_DEVICES=0\n  export MODEL_PATH=meta-llama/Llama-2-7b-hf\n  export DATASET_PATH=./data/red_pajama_n=1024_4096_context_length.pth\n  export SAVE_PATH=/path/to/save/quantized/model/\n\n  python main.py $MODEL_PATH $DATASET_PATH \\\n   --nsamples=1024 \\\n   --val_size=128 \\\n   --num_codebooks=1 \\\n   --nbits_per_codebook=16 \\\n   --in_group_size=8 \\\n   --relative_mse_tolerance=0.01 \\\n   --finetune_batch_size=32 \\\n   --finetune_max_epochs=10 \\\n   --finetune_early_stop=3 \\\n   --finetune_keep_best \\\n   --local_batch_size=1 \\\n   --offload_activations \\\n   --wandb \\\n   --resume \\\n   --save $SAVE_PATH\n  ```\n\n## 5. Model Finetuning (Optional)\n- **Description**: Improve quantized model accuracy with PV-Tuning\n- **Command**:\n  ```bash\n  torchrun --nproc-per-node=$NUM_GPUS finetune.py \\\n      --base_model $MODEL_PATH \\\n      --quantized_model $SAVE_PATH \\\n      --model_seqlen=4096 \\\n      --block_type LlamaDecoderLayer \\\n      --load_dtype bfloat16 \\\n      --amp_dtype bfloat16 \\\n      --code_dtype uint16 \\\n      --dataset_name=$DATASET_PATH \\\n      --split none \\\n      --seed 42 \\\n      --preprocessing_chunk_length 100000 \\\n      --cache_dir=./cache_dir \\\n      --trust_remote_code \\\n      --update_codes \\\n      --update_codebooks_and_scales \\\n      --update_non_quantized_parameters \\\n      --lamb \\\n      --debias \\\n      --lr 3e-4 \\\n      --adam_beta1 0.90 \\\n      --adam_beta2 0.95 \\\n      --max_code_change_per_step 1e-2 \\\n      --code_lr 1e-2 \\\n      --code_beta1 0.0 \\\n      --code_beta2 0.95 \\\n      --beam_size 5 \\\n      --delta_decay 0 \\\n      --batch_size=128 \\\n      --microbatch_size=1 \\\n      --max_epochs 1 \\\n      --gradient_checkpointing \\\n      --print_every_steps=1 \\\n      --verbose_optimizer \\\n      --wandb \\\n      --eval_every_steps=10 \\\n      --keep_best_model \\\n      --save $SAVE_PATH \\\n      --save_every_steps 100 \\\n      --attn_implementation flash_attention_2\n  ```\n\n## 6. Model Conversion\n- **Description**: Convert quantized model to Hugging Face format\n- **Command**:\n  ```bash\n  python convert_to_hf.py meta-llama/Llama-2-7b-hf $SAVE_PATH ./converted-llama2-7b-hf --save_safetensors\n  ```\n\n## 7. Inference\n- **Description**: Load and run the quantized model\n- **Command**:\n  ```python\n  from transformers import AutoModelForCausalLM\n  quantized_model = AutoModelForCausalLM.from_pretrained(\n      \"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\",\n      trust_remote_code=True, torch_dtype=\"auto\"\n  ).cuda()\n  ```\n\n## 8. Evaluation (Optional)\n- **Description**: Evaluate model performance using LM Evaluation Harness\n- **Command**:\n  ```bash\n  python lmeval.py \\\n      --model hf \\\n      --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n      --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \\\n      --batch_size 32 \\\n      --aqlm_checkpoint_path $SAVE_PATH\n  ```", "issue_title": "optimized 1x16 and 2x8 decompression/dequantization kernels now exist", "issue_body": "Hello, this isn't really a problem, I just wanted to inform you all that I wrote 1x16 and 2x8 decompression/dequantization kernels that run a factor of 6 and 9 (respectively) faster than the generic pytorch one you guys coded up, good for prefill performance.  I didn't roll the scale into them because scaling the output is very fast. \r\n\r\nAnyway, if you want to see them or bring them over, feel free, they are in:\r\n\r\nhttps://github.com/vllm-project/vllm/pull/3287\r\n\r\nCheers, and AQLM is really great, reaching Pareto optimal is a really nice achievement!\r\n\r\n-James", "choices": "(A) 1. Ensure you are using the latest version of AQLM (version 1.1.0 or later).\n2. The `aqlm.optimize_for_training()` wrapper is no longer needed as the optimization is now handled automatically.\n3. Disable automatic optimization by setting `aqlm.disable_auto_optimization = True` in your code.\n4. If you are using an older version of AQLM, you can manually optimize for training by wrapping the model loading code with `aqlm.optimize_for_training()` as shown:\n```python\nimport aqlm\n\nwith aqlm.optimize_for_training():\n    model = AutoModelForCausalLM.from_pretrained(...)\n```\n5. Note that using the `optimize_for_training` wrapper in older versions will make generation slower, so upgrading to the latest version is recommended for automatic optimization. (B) 1. The `aqlm.optimize_for_training()` wrapper is no longer needed as the optimization is now handled automatically.\n2. Note that using the `optimize_for_training` wrapper in older versions will make generation slower, so upgrading to the latest version is recommended for automatic optimization. (C) 1. If you are using an older version of AQLM, you can manually optimize for training by wrapping the model loading code with `aqlm.optimize_for_training()` as shown:\n```python\nimport aqlm\n\nwith aqlm.optimize_for_training():\n    model = AutoModelForCausalLM.from_pretrained(...)\n```\n2. Ensure you are using the latest version of AQLM (version 1.1.0 or later).\n3. The `aqlm.optimize_for_training()` wrapper is no longer needed as the optimization is now handled automatically.\n4. Note that using the `optimize_for_training` wrapper in older versions will make generation slower, so upgrading to the latest version is recommended for automatic optimization. (D) To improve the training speed of the AQLM model, follow these steps:\n1. Ensure you are using the latest version of AQLM (version 1.1.0 or later).\n2. The `aqlm.optimize_for_training()` wrapper is no longer needed as the optimization is now handled automatically.\n3. If you are using an older version of AQLM, you can manually optimize for training by wrapping the model loading code with `aqlm.optimize_for_training()` as shown:\n```python\nimport aqlm\n\nwith aqlm.optimize_for_training():\n    model = AutoModelForCausalLM.from_pretrained(...)\n```\n4. Note that using the `optimize_for_training` wrapper in older versions will make generation slower, so upgrading to the latest version is recommended for automatic optimization.", "answer": "D"}
{"uuid": "78d47973-35b6-4f9b-a5f5-37dd7801e702", "setup_instruct": "# AQLM Project Execution Plan\n\n## 1. Installation\n- **Description**: Install the AQLM inference library with GPU and CPU support.\n- **Command**:\n  ```bash\n  pip install aqlm[gpu,cpu]>=1.1.6\n  ```\n\n## 2. Model Download (Optional)\n- **Description**: Download the base model from Hugging Face if not already cached.\n- **Command**:\n  ```python\n  from transformers import AutoTokenizer, AutoModelForCausalLM\n  model_name = \"meta-llama/Llama-2-7b-hf\"  # or other model\n  tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=\"auto\")\n  model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n  ```\n\n## 3. Inference with Pre-Quantized Models\n- **Description**: Load and run a pre-quantized AQLM model for inference.\n- **Command**:\n  ```python\n  from transformers import AutoModelForCausalLM\n  quantized_model = AutoModelForCausalLM.from_pretrained(\n      \"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\",\n      trust_remote_code=True, torch_dtype=\"auto\"\n  ).cuda()\n  ```\n\n## 4. Quantization Setup\n- **Description**: Install dependencies for quantization.\n- **Command**:\n  ```bash\n  pip install -r requirements.txt\n  ```\n\n## 5. Data Preparation\n- **Description**: Download and preprocess calibration data (e.g., RedPajama).\n- **Command**:\n  ```bash\n  TARGET_MODEL=meta-llama/Llama-2-7b-hf\n  SEQLEN=4096\n  DATASET=togethercomputer/RedPajama-Data-1T-Sample\n  OUTPUT_PATH=./redpajama_tokenized_llama2\n\n  torchrun --master-port 3456 --nproc-per-node=1 finetune.py \\\n      --base_model $TARGET_MODEL \\\n      --quantized_model ./dummy \\\n      --dtype bfloat16 \\\n      --block_type LlamaDecoderLayer \\\n      --dataset_name=$DATASET \\\n      --split train \\\n      --dataset_config_name plain_text \\\n      --cache_dir=./cache_dir \\\n      --trust_remote_code \\\n      --model_seqlen=$SEQLEN \\\n      --preprocessing_num_workers=64 \\\n      --preprocessing_chunk_length 100000 \\\n      --save_dataset_and_exit $OUTPUT_PATH\n  ```\n\n## 6. Model Quantization\n- **Description**: Quantize the model using AQLM with specified parameters.\n- **Command**:\n  ```bash\n  export CUDA_VISIBLE_DEVICES=0\n  export MODEL_PATH=meta-llama/Llama-2-7b-hf\n  export DATASET_PATH=./data/red_pajama_n=1024_4096_context_length.pth\n  export SAVE_PATH=./quantized_model/\n  export WANDB_PROJECT=MY_AQ_EXPS\n  export WANDB_NAME=COOL_EXP_NAME\n\n  python main.py $MODEL_PATH $DATASET_PATH \\\n      --nsamples=1024 \\\n      --val_size=128 \\\n      --num_codebooks=1 \\\n      --nbits_per_codebook=16 \\\n      --in_group_size=8 \\\n      --relative_mse_tolerance=0.01 \\\n      --finetune_batch_size=32 \\\n      --finetune_max_epochs=10 \\\n      --finetune_early_stop=3 \\\n      --finetune_keep_best \\\n      --local_batch_size=1 \\\n      --offload_activations \\\n      --wandb \\\n      --resume \\\n      --save $SAVE_PATH\n  ```\n\n## 7. Fine-Tuning (PV-Tuning)\n- **Description**: Improve quantized model accuracy with PV-Tuning.\n- **Command**:\n  ```bash\n  torchrun --nproc-per-node=$NUM_GPUS finetune.py \\\n      --base_model $MODEL_PATH \\\n      --quantized_model $SAVE_PATH \\\n      --model_seqlen=4096 \\\n      --block_type LlamaDecoderLayer \\\n      --load_dtype bfloat16 \\\n      --amp_dtype bfloat16 \\\n      --code_dtype uint16 \\\n      --dataset_name=$OUTPUT_PATH \\\n      --split none \\\n      --seed 42 \\\n      --preprocessing_chunk_length 100000 \\\n      --cache_dir=./cache_dir \\\n      --trust_remote_code \\\n      --update_codes \\\n      --update_codebooks_and_scales \\\n      --update_non_quantized_parameters \\\n      --lamb \\\n      --debias \\\n      --lr 3e-4 \\\n      --adam_beta1 0.90 \\\n      --adam_beta2 0.95 \\\n      --max_code_change_per_step 1e-2 \\\n      --code_lr 1e-2 \\\n      --code_beta1 0.0 \\\n      --code_beta2 0.95 \\\n      --beam_size 5 \\\n      --delta_decay 0 \\\n      --batch_size=128 \\\n      --microbatch_size=1 \\\n      --max_epochs 1 \\\n      --gradient_checkpointing \\\n      --print_every_steps=1 \\\n      --verbose_optimizer \\\n      --wandb \\\n      --eval_every_steps=10 \\\n      --keep_best_model \\\n      --save $SAVE_PATH \\\n      --save_every_steps 100 \\\n      --attn_implementation flash_attention_2\n  ```\n\n## 8. Model Conversion for Hugging Face\n- **Description**: Convert the quantized model to Hugging Face format.\n- **Command**:\n  ```bash\n  python convert_to_hf.py meta-llama/Llama-2-7b-hf ./quantized_model ./converted_model --save_safetensors\n  ```\n\n## 9. Evaluation (Optional)\n- **Description**: Evaluate the model using LM Evaluation Harness.\n- **Command**:\n  ```bash\n  python lmeval.py \\\n      --model hf \\\n      --model_args pretrained=meta-llama/Llama-2-7b-hf,dtype=float16,parallelize=True \\\n      --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \\\n      --batch_size 8 \\\n      --aqlm_checkpoint_path ./converted_model\n  ```", "issue_title": "Case Study: Instruction Tuning on AQLM Models", "issue_body": "Hi, we have performed a small experiment on fine-tuning the Llama-2-70B-AQLM-2Bit model using the PEFT QLoRA method. We utilized the Alpaca and Glaive datasets for instruction tuning, and the fine-tuned version demonstrates preliminary conversation and tool-using abilities. We found that the training only requires 24GB of GRAM, while inference only needs 20GB. Thus fine-tuning a 70B model on consumer devices can be feasible. However, we found that AQLM significantly increases the overall training time. It would be better if the training speed could be improved. Thanks again for your excellent work!\r\n\r\nAdapter weights: https://huggingface.co/hiyouga/Llama-2-70b-AQLM-2Bit-QLoRA-function-calling\r\n\r\n![examples](https://github.com/Vahe1994/AQLM/assets/16256802/df7f2699-a88d-4b7e-9593-8c589c4c6da2)\r\n![train_loss](https://github.com/Vahe1994/AQLM/assets/16256802/52cb8895-a19c-443e-bfb1-f31a2cbd23b8)\r\n![gpu](https://github.com/Vahe1994/AQLM/assets/16256802/1f133611-d068-4c90-8fe1-333315a0a3be)", "choices": "(A) 1. The issue is related to the GPU's shared memory limitation on the NVIDIA 2080Ti (compute capability 7.5), which supports up to 64KB of shared memory per thread block.\n2. The AQLM 2x8 kernel attempts to allocate 70,144 bytes of shared memory, exceeding the GPU's limit.\n3. Manually increase the shared memory limit in the GPU's BIOS settings to 70,144 bytes.\n4. As a workaround, avoid using the 2x8 kernel on GPUs with compute capability 7.5 or lower.\n5. Consider using a different GPU with higher shared memory capacity or a different quantization configuration that requires less shared memory. (B) 1. The AQLM 2x8 kernel attempts to allocate 70,144 bytes of shared memory, exceeding the GPU's limit.\n2. As a workaround, avoid using the 2x8 kernel on GPUs with compute capability 7.5 or lower.\n3. Consider using a different GPU with higher shared memory capacity or a different quantization configuration that requires less shared memory. (C) 1. The issue is related to the GPU's shared memory limitation on the NVIDIA 2080Ti (compute capability 7.5), which supports up to 64KB of shared memory per thread block.\n2. The AQLM 2x8 kernel attempts to allocate 70,144 bytes of shared memory, exceeding the GPU's limit.\n3. As a workaround, avoid using the 2x8 kernel on GPUs with compute capability 7.5 or lower.\n4. Consider using a different GPU with higher shared memory capacity or a different quantization configuration that requires less shared memory. (D) 1. As a workaround, avoid using the 2x8 kernel on GPUs with compute capability 7.5 or lower.\n2. The issue is related to the GPU's shared memory limitation on the NVIDIA 2080Ti (compute capability 7.5), which supports up to 64KB of shared memory per thread block.\n3. The AQLM 2x8 kernel attempts to allocate 70,144 bytes of shared memory, exceeding the GPU's limit.\n4. Consider using a different GPU with higher shared memory capacity or a different quantization configuration that requires less shared memory.", "answer": "C"}
{"uuid": "b007b77f-0390-445e-a008-39a31efe7d7e", "setup_instruct": "# AQLM Project Execution Plan\n\n## 1. Installation\n- **Description**: Install the AQLM inference library with GPU and CPU support.\n- **Command**:\n  ```bash\n  pip install aqlm[gpu,cpu]>=1.1.6\n  ```\n\n## 2. Model Download (Optional)\n- **Description**: Download and cache the base model from Hugging Face if not already available.\n- **Command**:\n  ```python\n  from transformers import AutoTokenizer, AutoModelForCausalLM\n  model_name = \"meta-llama/Llama-2-7b-hf\"  # Example model\n  tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=\"auto\")\n  model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n  ```\n\n## 3. Inference with Pre-Quantized Models\n- **Description**: Load and run a pre-quantized AQLM model for inference.\n- **Command**:\n  ```python\n  from transformers import AutoModelForCausalLM\n  quantized_model = AutoModelForCausalLM.from_pretrained(\n      \"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\",\n      trust_remote_code=True, torch_dtype=\"auto\"\n  ).cuda()\n  ```\n\n## 4. Quantization Setup\n- **Description**: Install dependencies and prepare for quantization.\n- **Command**:\n  ```bash\n  pip install -r requirements.txt\n  ```\n\n## 5. Data Preparation\n- **Description**: Download and preprocess calibration data (e.g., RedPajama).\n- **Command**:\n  ```bash\n  TARGET_MODEL=meta-llama/Llama-2-7b-hf\n  SEQLEN=4096\n  DATASET=togethercomputer/RedPajama-Data-1T-Sample\n  OUTPUT_PATH=./redpajama_tokenized_llama2\n\n  CUDA_VISIBLE_DEVICES=0 HF_HOME=/mnt/LLM OMP_NUM_THREADS=16 torchrun --master-port 3456 --nproc-per-node=1 finetune.py --base_model $TARGET_MODEL --quantized_model ./doesnt_matter --dtype bfloat16 --block_type LlamaDecoderLayer --dataset_name=$DATASET --split train --dataset_config_name plain_text --cache_dir=./cache_dir --trust_remote_code --model_seqlen=$SEQLEN --preprocessing_num_workers=64 --preprocessing_chunk_length 100000 --save_dataset_and_exit $OUTPUT_PATH\n  ```\n\n## 6. Model Quantization\n- **Description**: Quantize the model using AQLM with specified parameters.\n- **Command**:\n  ```bash\n  export CUDA_VISIBLE_DEVICES=0\n  export MODEL_PATH=meta-llama/Llama-2-7b-hf\n  export DATASET_PATH=./data/red_pajama_n=1024_4096_context_length.pth\n  export SAVE_PATH=/path/to/save/quantized/model/\n  export WANDB_PROJECT=MY_AQ_EXPS\n  export WANDB_NAME=COOL_EXP_NAME\n\n  python main.py $MODEL_PATH $DATASET_PATH \\\n   --nsamples=1024 \\\n   --val_size=128 \\\n   --num_codebooks=1 \\\n   --nbits_per_codebook=16 \\\n   --in_group_size=8 \\\n   --relative_mse_tolerance=0.01 \\\n   --finetune_batch_size=32 \\\n   --finetune_max_epochs=10 \\\n   --finetune_early_stop=3 \\\n   --finetune_keep_best \\\n   --local_batch_size=1 \\\n   --offload_activations \\\n   --wandb \\\n   --resume \\\n   --save $SAVE_PATH\n  ```\n\n## 7. Fine-Tuning (PV-Tuning)\n- **Description**: Improve quantized model accuracy via PV-Tuning.\n- **Command**:\n  ```bash\n  torchrun --nproc-per-node=$NUM_GPUS finetune.py \\\n      --base_model $MODEL_PATH \\\n      --quantized_model $QUANTIZED_WEIGHTS_PATH \\\n      --model_seqlen=$SEQLEN \\\n      --block_type LlamaDecoderLayer \\\n      --load_dtype bfloat16 \\\n      --amp_dtype bfloat16 \\\n      --code_dtype uint16 \\\n      --dataset_name=$TOKENIZED_DATASET_PATH \\\n      --split none \\\n      --seed 42 \\\n      --preprocessing_chunk_length 100000 \\\n      --cache_dir=$CACHE_DIR \\\n      --trust_remote_code \\\n      --update_codes \\\n      --update_codebooks_and_scales \\\n      --update_non_quantized_parameters \\\n      --lamb \\\n      --debias \\\n      --lr 3e-4 \\\n      --adam_beta1 0.90 \\\n      --adam_beta2 0.95 \\\n      --max_code_change_per_step 1e-2 \\\n      --code_lr 1e-2 \\\n      --code_beta1 0.0 \\\n      --code_beta2 0.95 \\\n      --beam_size 5 \\\n      --delta_decay 0 \\\n      --batch_size=128 \\\n      --microbatch_size=1 \\\n      --max_epochs 1 \\\n      --gradient_checkpointing \\\n      --print_every_steps=1 \\\n      --verbose_optimizer \\\n      --wandb \\\n      --eval_every_steps=10 \\\n      --keep_best_model \\\n      --save $SAVE_PATH \\\n      --save_every_steps 100 \\\n      --attn_implementation flash_attention_2\n  ```\n\n## 8. Evaluation\n- **Description**: Evaluate the model using LM Evaluation Harness.\n- **Command**:\n  ```bash\n  export QUANTIZED_MODEL=/path/to/saved/quantized/model/\n  export MODEL_PATH=meta-llama/Llama-2-7b-hf\n  export WANDB_PROJECT=MY_AQLM_EVAL\n  export WANDB_NAME=COOL_EVAL_NAME\n\n  # 0-shot evaluation\n  python lmeval.py \\\n      --model hf \\\n      --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n      --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \\\n      --batch_size 8 \\\n      --aqlm_checkpoint_path $QUANTIZED_MODEL\n\n  # 5-shot MMLU evaluation\n  python lmeval.py \\\n      --model hf \\\n      --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n      --tasks mmlu \\\n      --batch_size 8 \\\n      --num_fewshot 5 \\\n      --aqlm_checkpoint_path $QUANTIZED_MODEL\n  ```\n\n## 9. Convert to Hugging Face Format\n- **Description**: Convert the quantized model to Hugging Face format for deployment.\n- **Command**:\n  ```bash\n  python convert_to_hf.py meta-llama/Llama-2-7b-hf ./path/to/saved/quantization ./converted-llama2-7b-hf --save_safetensors\n  ```", "issue_title": "How long for the quantizing a 70b model? I had ran for 2days", "issue_body": "is it toooo long to quantized a model ?", "choices": "(A) 1. Downgrade PyTorch to version 2.1.0 by running `pip install torch==2.1.0`.\n2. Ensure the downgrade is successful by verifying the PyTorch version with `pip show torch`. (B) 1. Downgrade PyTorch to version 2.1.0 by running `pip install torch==2.1.0`.\n2. Ensure the downgrade is successful by verifying the PyTorch version with `pip show torch`.\n3. Retry running the notebook or script that previously caused the 'Unknown layout' error. (C) 1. Retry running the notebook or script that previously caused the 'Unknown layout' error.\n2. Downgrade PyTorch to version 2.1.0 by running `pip install torch==2.1.0`.\n3. Ensure the downgrade is successful by verifying the PyTorch version with `pip show torch`. (D) 1. Downgrade PyTorch to version 2.1.0 by running `pip install torch==2.1.0`.\n2. Upgrade PyTorch to the latest version by running `pip install --upgrade torch`.\n3. Ensure the downgrade is successful by verifying the PyTorch version with `pip show torch`.\n4. Retry running the notebook or script that previously caused the 'Unknown layout' error. (E) 1. Uninstall PyTorch by running `pip uninstall torch`.\n2. Downgrade PyTorch to version 2.1.0 by running `pip install torch==2.1.0`.\n3. Ensure the downgrade is successful by verifying the PyTorch version with `pip show torch`.\n4. Retry running the notebook or script that previously caused the 'Unknown layout' error. (F) 1. Ensure the downgrade is successful by verifying the PyTorch version with `pip show torch`.\n2. Downgrade PyTorch to version 2.1.0 by running `pip install torch==2.1.0`.\n3. Retry running the notebook or script that previously caused the 'Unknown layout' error. (G) 1. Downgrade PyTorch to version 2.1.0 by running `pip install torch==2.1.0`.\n2. Retry running the notebook or script that previously caused the 'Unknown layout' error.", "answer": "B"}
{"uuid": "56f3f41a-7f80-4823-aead-ff283501bd74", "setup_instruct": "# AQLM Project Execution Plan\n\n## 1. Installation\n- **Description**: Install the AQLM inference library with GPU and CPU support\n- **Command**:\n  ```bash\n  pip install aqlm[gpu,cpu]>=1.1.6\n  ```\n\n## 2. Model Download\n- **Description**: Download and cache the base model from Hugging Face\n- **Command**:\n  ```python\n  from transformers import AutoTokenizer, AutoModelForCausalLM\n  model_name = \"meta-llama/Llama-2-7b-hf\"  # or other model\n  tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=\"auto\")\n  model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n  ```\n\n## 3. Quantization Preparation\n- **Description**: Install required dependencies and prepare environment\n- **Commands**:\n  ```bash\n  pip install -r requirements.txt\n  export CUDA_VISIBLE_DEVICES=0  # Set GPU(s) to use\n  export MODEL_PATH=<PATH_TO_MODEL_ON_HUB>\n  export DATASET_PATH=<DATASET_NAME_OR_PATH>\n  export SAVE_PATH=/path/to/save/quantized/model/\n  ```\n\n## 4. Run Quantization\n- **Description**: Execute main quantization script with recommended parameters\n- **Command**:\n  ```bash\n  python main.py $MODEL_PATH $DATASET_PATH \\\n   --nsamples=1024 \\\n   --val_size=128 \\\n   --num_codebooks=1 \\\n   --nbits_per_codebook=16 \\\n   --in_group_size=8 \\\n   --relative_mse_tolerance=0.01 \\\n   --finetune_batch_size=32 \\\n   --finetune_max_epochs=10 \\\n   --finetune_early_stop=3 \\\n   --finetune_keep_best \\\n   --local_batch_size=1 \\\n   --offload_activations \\\n   --wandb \\\n   --resume \\\n   --save $SAVE_PATH\n  ```\n\n## 5. Finetuning (Optional)\n- **Description**: Improve quantized model accuracy with PV-Tuning\n- **Command**:\n  ```bash\n  torchrun --nproc-per-node=$NUM_GPUS finetune.py \\\n      --base_model $MODEL_PATH \\\n      --quantized_model $SAVE_PATH \\\n      --model_seqlen=4096 \\\n      --block_type LlamaDecoderLayer \\\n      --load_dtype bfloat16 \\\n      --amp_dtype bfloat16 \\\n      --code_dtype uint16 \\\n      --dataset_name=$TOKENIZED_DATASET_PATH \\\n      --update_codes \\\n      --update_codebooks_and_scales \\\n      --update_non_quantized_parameters \\\n      --lamb \\\n      --debias \\\n      --lr 3e-4 \\\n      --batch_size=128 \\\n      --microbatch_size=1 \\\n      --max_epochs 1 \\\n      --gradient_checkpointing \\\n      --wandb \\\n      --save $SAVE_PATH/finetuned\n  ```\n\n## 6. Convert to Hugging Face Format\n- **Description**: Prepare quantized model for inference\n- **Command**:\n  ```bash\n  python convert_to_hf.py $MODEL_PATH $SAVE_PATH ./converted-model --save_safetensors\n  ```\n\n## 7. Inference\n- **Description**: Load and use the quantized model\n- **Command**:\n  ```python\n  from transformers import AutoModelForCausalLM\n  quantized_model = AutoModelForCausalLM.from_pretrained(\n      \"./converted-model\",\n      trust_remote_code=True, \n      torch_dtype=\"auto\"\n  ).cuda()\n  ```\n\n## 8. Evaluation (Optional)\n- **Description**: Evaluate model performance using LM Evaluation Harness\n- **Command**:\n  ```bash\n  python lmeval.py \\\n      --model hf \\\n      --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n      --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \\\n      --batch_size 8 \\\n      --aqlm_checkpoint_path ./converted-model\n  ```", "issue_title": "Quantization Time", "issue_body": "How long is the expected time to quantize a 7b mistral model ?", "choices": "(A) 1. Ensure you have Visual Studio 2022 installed.\n2. Make sure cl.exe is in your Windows PATH.\n3. Use the command `pip install --no-deps aqlm[gpu]` from the oobabooga cmd_windows prompt.\n4. Locate the pip installation of aqlm using `pip show aqlm`.\n5. Navigate to the aqlm/inference_kernels folder.\n6. Modify cuda_kernel.cpp by adding the __restrict keyword to all of the _cuda functions' pointer arguments. (B) 1. Ensure you have Visual Studio 2022 installed.\n2. Make sure cl.exe is in your Windows PATH.\n3. Use the command `pip install --no-deps aqlm[gpu]` from the oobabooga cmd_windows prompt.\n4. Locate the pip installation of aqlm using `pip show aqlm`.\n5. Delete the aqlm folder.\n6. Navigate to the aqlm/inference_kernels folder.\n7. Modify cuda_kernel.cpp by adding the __restrict keyword to all of the _cuda functions' pointer arguments. (C) 1. Ensure you have Visual Studio 2022 installed.\n2. Make sure cl.exe is in your Windows PATH.\n3. Use the command `pip install --no-deps aqlm[gpu]` from the oobabooga cmd_windows prompt.\n4. Uninstall Visual Studio 2022.\n5. Locate the pip installation of aqlm using `pip show aqlm`.\n6. Navigate to the aqlm/inference_kernels folder.\n7. Modify cuda_kernel.cpp by adding the __restrict keyword to all of the _cuda functions' pointer arguments. (D) 1. Use the command `pip install --no-deps aqlm[gpu]` from the oobabooga cmd_windows prompt.\n2. Ensure you have Visual Studio 2022 installed.\n3. Make sure cl.exe is in your Windows PATH.\n4. Locate the pip installation of aqlm using `pip show aqlm`.\n5. Navigate to the aqlm/inference_kernels folder.\n6. Modify cuda_kernel.cpp by adding the __restrict keyword to all of the _cuda functions' pointer arguments. (E) 1. Ensure you have Visual Studio 2022 installed.\n2. Make sure cl.exe is in your Windows PATH.\n3. Use the command `pip install --no-deps aqlm[gpu]` from the oobabooga cmd_windows prompt.\n4. Navigate to the aqlm/inference_kernels folder.\n5. Modify cuda_kernel.cpp by adding the __restrict keyword to all of the _cuda functions' pointer arguments.\n6. Locate the pip installation of aqlm using `pip show aqlm`. (F) 1. Ensure you have Visual Studio 2022 installed.\n2. Use the command `pip install --no-deps aqlm[gpu]` from the oobabooga cmd_windows prompt.\n3. Locate the pip installation of aqlm using `pip show aqlm`.\n4. Navigate to the aqlm/inference_kernels folder.\n5. Modify cuda_kernel.cpp by adding the __restrict keyword to all of the _cuda functions' pointer arguments. (G) 1. Ensure you have Visual Studio 2022 installed.\n2. Make sure cl.exe is in your Windows PATH.\n3. Use the command `pip install --no-deps aqlm[gpu]` from the oobabooga cmd_windows prompt.\n4. Locate the pip installation of aqlm using `pip show aqlm`.\n5. Modify cuda_kernel.cpp by adding the __restrict keyword to all of the _cuda functions' pointer arguments.", "answer": "A"}
{"uuid": "34d1c5ac-9ff8-42c3-aa8f-1df607ed9641", "setup_instruct": "# AQLM Deployment Plan\n\n## 1. Installation\n- **Description**: Install the AQLM inference library with GPU and CPU support\n- **Command**:\n  ```bash\n  pip install aqlm[gpu,cpu]>=1.1.6\n  ```\n\n## 2. Model Download\n- **Description**: Download a prequantized model from Hugging Face\n- **Command** (Python):\n  ```python\n  from transformers import AutoModelForCausalLM\n  model = AutoModelForCausalLM.from_pretrained(\n      \"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\",\n      trust_remote_code=True, torch_dtype=\"auto\"\n  ).cuda()\n  ```\n\n## 3. Quantization Preparation\n- **Description**: Install dependencies and prepare environment\n- **Commands**:\n  ```bash\n  pip install -r requirements.txt\n  ```\n\n## 4. Data Preparation\n- **Description**: Download and preprocess calibration data\n- **Options**:\n  - For RedPajama data:\n    ```bash\n    python finetune.py --base_model meta-llama/Llama-2-7b-hf --dataset_name togethercomputer/RedPajama-Data-1T-Sample --save_dataset_and_exit ./redpajama_tokenized_llama2\n    ```\n  - Or download pre-tokenized data:\n    ```python\n    from huggingface_hub import hf_hub_download\n    hf_hub_download(repo_id=\"Vahe1994/AQLM\", filename=\"data/name.pth\", repo_type=\"dataset\")\n    ```\n\n## 5. Model Quantization\n- **Description**: Run quantization with specified parameters\n- **Command**:\n  ```bash\n  python main.py meta-llama/Llama-2-7b-hf ./data/red_pajama_n=1024_4096_context_length.pth \\\n   --nsamples=1024 \\\n   --num_codebooks=1 \\\n   --nbits_per_codebook=16 \\\n   --save ./quantized_model\n  ```\n\n## 6. Fine-tuning (Optional)\n- **Description**: Improve quantized model accuracy with PV-Tuning\n- **Command**:\n  ```bash\n  torchrun --nproc-per-node=4 finetune.py \\\n    --base_model meta-llama/Llama-2-7b-hf \\\n    --quantized_model ./quantized_model \\\n    --dataset_name=./redpajama_tokenized_llama2 \\\n    --save ./finetuned_model\n  ```\n\n## 7. Model Conversion\n- **Description**: Convert to Hugging Face format\n- **Command**:\n  ```bash\n  python convert_to_hf.py meta-llama/Llama-2-7b-hf ./quantized_model ./converted_model --save_safetensors\n  ```\n\n## 8. Evaluation\n- **Description**: Evaluate model performance\n- **Command**:\n  ```bash\n  python lmeval.py \\\n    --model hf \\\n    --model_args pretrained=meta-llama/Llama-2-7b-hf,dtype=float16,parallelize=True \\\n    --tasks winogrande,piqa,hellaswag \\\n    --aqlm_checkpoint_path ./converted_model\n  ```\n\n## 9. Inference\n- **Description**: Use the quantized model for inference\n- **Command** (Python):\n  ```python\n  from transformers import AutoModelForCausalLM\n  quantized_model = AutoModelForCausalLM.from_pretrained(\n      \"./converted_model\",\n      trust_remote_code=True, torch_dtype=\"auto\"\n  ).cuda()\n  ```", "issue_title": "RuntimeError: Unknown layout", "issue_body": "Hi, I've been encountering this problem when running this [notebook](https://colab.research.google.com/drive/1-xZmBRXT5Fm3Ghn4Mwa2KRypORXb855X?usp=sharing#scrollTo=ZDOtpnJGizsx) on local machine with an L4. I've followed the instructions from the notebook on `Python3.10`. \r\n\r\n```\r\nName: torch\r\nVersion: 2.2.0\r\nSummary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\r\nHome-page: https://pytorch.org/\r\nAuthor: PyTorch Team\r\nAuthor-email: packages@pytorch.org\r\nLicense: BSD-3\r\nLocation: /home/ubuntu/anaconda3/envs/aqlm/lib/python3.10/site-packages\r\nRequires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\r\nRequired-by: accelerate, aqlm, torchaudio, torchvision\r\n\r\n```\r\n```\r\n  +---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA L4                       On | 00000000:00:04.0 Off |                    0 |\r\n| N/A   69C    P0               33W /  75W|  13858MiB / 23034MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n                                                                                         \r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n|    0   N/A  N/A     16482      C   ...untu/anaconda3/envs/aqlm/bin/python    13856MiB |\r\n+---------------------------------------------------------------------------------------+\r\n\r\n```\r\n> The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\n>   Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\n>   ---------------------------------------------------------------------------\r\n>   RuntimeError                              Traceback (most recent call last)\r\n>   Cell In[3], line 1\r\n>   ----> 1 output = quantized_model.generate(tokenizer(\"\", return_tensors=\"pt\")[\"input_ids\"].cuda(), max_new_tokens=10)\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/torch/utils/_contextlib.py:115, in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n>       112 @functools.wraps(func)\r\n>       113 def decorate_context(*args, **kwargs):\r\n>       114     with ctx_factory():\r\n>   --> 115         return func(*args, **kwargs)\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/transformers/generation/utils.py:1513, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\r\n>      1496     return self.assisted_decoding(\r\n>      1497         input_ids,\r\n>      1498         candidate_generator=candidate_generator,\r\n>      (...)\r\n>      1509         **model_kwargs,\r\n>      1510     )\r\n>      1511 if generation_mode == GenerationMode.GREEDY_SEARCH:\r\n>      1512     # 11. run greedy search\r\n>   -> 1513     return self.greedy_search(\r\n>      1514         input_ids,\r\n>      1515         logits_processor=prepared_logits_processor,\r\n>      1516         stopping_criteria=prepared_stopping_criteria,\r\n>      1517         pad_token_id=generation_config.pad_token_id,\r\n>      1518         eos_token_id=generation_config.eos_token_id,\r\n>      1519         output_scores=generation_config.output_scores,\r\n>      1520         return_dict_in_generate=generation_config.return_dict_in_generate,\r\n>      1521         synced_gpus=synced_gpus,\r\n>      1522         streamer=streamer,\r\n>      1523         **model_kwargs,\r\n>      1524     )\r\n>      1526 elif generation_mode == GenerationMode.CONTRASTIVE_SEARCH:\r\n>      1527     if not model_kwargs[\"use_cache\"]:\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/transformers/generation/utils.py:2350, in GenerationMixin.greedy_search(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\r\n>      2347 model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\r\n>      2349 # forward pass to get next token\r\n>   -> 2350 outputs = self(\r\n>      2351     **model_inputs,\r\n>      2352     return_dict=True,\r\n>      2353     output_attentions=output_attentions,\r\n>      2354     output_hidden_states=output_hidden_states,\r\n>      2355 )\r\n>      2357 if synced_gpus and this_peer_finished:\r\n>      2358     continue  # don't waste resources running the code we don't need\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n>      1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n>      1510 else:\r\n>   -> 1511     return self._call_impl(*args, **kwargs)\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\r\n>      1515 # If we don't have any hooks, we want to skip the rest of the logic in\r\n>      1516 # this function, and just call forward.\r\n>      1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n>      1518         or _global_backward_pre_hooks or _global_backward_hooks\r\n>      1519         or _global_forward_hooks or _global_forward_pre_hooks):\r\n>   -> 1520     return forward_call(*args, **kwargs)\r\n>      1522 try:\r\n>      1523     result = None\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:1360, in MixtralForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict)\r\n>      1357 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n>      1359 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\r\n>   -> 1360 outputs = self.model(\r\n>      1361     input_ids=input_ids,\r\n>      1362     attention_mask=attention_mask,\r\n>      1363     position_ids=position_ids,\r\n>      1364     past_key_values=past_key_values,\r\n>      1365     inputs_embeds=inputs_embeds,\r\n>      1366     use_cache=use_cache,\r\n>      1367     output_attentions=output_attentions,\r\n>      1368     output_hidden_states=output_hidden_states,\r\n>      1369     output_router_logits=output_router_logits,\r\n>      1370     return_dict=return_dict,\r\n>      1371 )\r\n>      1373 hidden_states = outputs[0]\r\n>      1374 logits = self.lm_head(hidden_states)\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n>      1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n>      1510 else:\r\n>   -> 1511     return self._call_impl(*args, **kwargs)\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\r\n>      1515 # If we don't have any hooks, we want to skip the rest of the logic in\r\n>      1516 # this function, and just call forward.\r\n>      1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n>      1518         or _global_backward_pre_hooks or _global_backward_hooks\r\n>      1519         or _global_forward_hooks or _global_forward_pre_hooks):\r\n>   -> 1520     return forward_call(*args, **kwargs)\r\n>      1522 try:\r\n>      1523     result = None\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:1228, in MixtralModel.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict)\r\n>      1217     layer_outputs = self._gradient_checkpointing_func(\r\n>      1218         decoder_layer.__call__,\r\n>      1219         hidden_states,\r\n>      (...)\r\n>      1225         use_cache,\r\n>      1226     )\r\n>      1227 else:\r\n>   -> 1228     layer_outputs = decoder_layer(\r\n>      1229         hidden_states,\r\n>      1230         attention_mask=attention_mask,\r\n>      1231         position_ids=position_ids,\r\n>      1232         past_key_value=past_key_values,\r\n>      1233         output_attentions=output_attentions,\r\n>      1234         output_router_logits=output_router_logits,\r\n>      1235         use_cache=use_cache,\r\n>      1236     )\r\n>      1238 hidden_states = layer_outputs[0]\r\n>      1240 if use_cache:\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n>      1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n>      1510 else:\r\n>   -> 1511     return self._call_impl(*args, **kwargs)\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\r\n>      1515 # If we don't have any hooks, we want to skip the rest of the logic in\r\n>      1516 # this function, and just call forward.\r\n>      1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n>      1518         or _global_backward_pre_hooks or _global_backward_hooks\r\n>      1519         or _global_forward_hooks or _global_forward_pre_hooks):\r\n>   -> 1520     return forward_call(*args, **kwargs)\r\n>      1522 try:\r\n>      1523     result = None\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:934, in MixtralDecoderLayer.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, output_router_logits, use_cache, **kwargs)\r\n>       931 hidden_states = self.input_layernorm(hidden_states)\r\n>       933 # Self Attention\r\n>   --> 934 hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n>       935     hidden_states=hidden_states,\r\n>       936     attention_mask=attention_mask,\r\n>       937     position_ids=position_ids,\r\n>       938     past_key_value=past_key_value,\r\n>       939     output_attentions=output_attentions,\r\n>       940     use_cache=use_cache,\r\n>       941 )\r\n>       942 hidden_states = residual + hidden_states\r\n>       944 # Fully Connected\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n>      1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n>      1510 else:\r\n>   -> 1511     return self._call_impl(*args, **kwargs)\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\r\n>      1515 # If we don't have any hooks, we want to skip the rest of the logic in\r\n>      1516 # this function, and just call forward.\r\n>      1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n>      1518         or _global_backward_pre_hooks or _global_backward_hooks\r\n>      1519         or _global_forward_hooks or _global_forward_pre_hooks):\r\n>   -> 1520     return forward_call(*args, **kwargs)\r\n>      1522 try:\r\n>      1523     result = None\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:730, in MixtralSdpaAttention.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\r\n>       719     return super().forward(\r\n>       720         hidden_states=hidden_states,\r\n>       721         attention_mask=attention_mask,\r\n>      (...)\r\n>       725         use_cache=use_cache,\r\n>       726     )\r\n>       728 bsz, q_len, _ = hidden_states.size()\r\n>   --> 730 query_states = self.q_proj(hidden_states)\r\n>       731 key_states = self.k_proj(hidden_states)\r\n>       732 value_states = self.v_proj(hidden_states)\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n>      1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\r\n>      1510 else:\r\n>   -> 1511     return self._call_impl(*args, **kwargs)\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\r\n>      1515 # If we don't have any hooks, we want to skip the rest of the logic in\r\n>      1516 # this function, and just call forward.\r\n>      1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n>      1518         or _global_backward_pre_hooks or _global_backward_hooks\r\n>      1519         or _global_forward_hooks or _global_forward_pre_hooks):\r\n>   -> 1520     return forward_call(*args, **kwargs)\r\n>      1522 try:\r\n>      1523     result = None\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/aqlm/inference.py:65, in QuantizedLinear.forward(self, input)\r\n>        59 if (\r\n>        60     not input.is_cuda\r\n>        61     and self.codebook_size == 256\r\n>        62     and self.codes.shape[0] == self.out_features // self.out_group_size\r\n>        63 ):\r\n>        64     self.codes.data = torch.permute(self.codes.data, (1, 0, 2)).contiguous()  #  TODO: fix this thing\r\n>   ---> 65 return forward_pass_quantized_linear(input, self.codes, self.codebooks, self.scales, self.bias)\r\n>   \r\n>   File ~/anaconda3/envs/aqlm/lib/python3.10/site-packages/aqlm/inference_kernels/kernel_selector.py:24, in forward_pass_quantized_linear(input, codes, codebooks, scales, bias)\r\n>        19     from .cuda_kernel import CUDA_KERNEL\r\n>        21     assert (\r\n>        22         input.dtype == torch.float16\r\n>        23     ), f\"please load the model with `torch_dtype=torch.float16`, as {input.dtype} is not supported on GPU yet\"\r\n>   ---> 24     return CUDA_KERNEL.code1x16_matmat(input, codes, codebooks, scales) + (bias if bias is not None else 0)\r\n>        25 case (True, 2, 256, 1, 8):\r\n>        26     from .cuda_kernel import CUDA_KERNEL\r\n>   \r\n>   RuntimeError: Unknown layout\r\n>", "choices": "(A) 1. Set the finetune learning rate to 1e-4 (`finetune_lr=1e-4`).\n2. Use 1 codebook with a size of 2^15 (`num_codebooks=1`, `nbits_per_codebook=15`).\n3. Set the group size to 8 (`in_group_size=8`).\n4. Use the calibration dataset `data/red_pajama_n=4096_4096_context_length.pth` with `nsamples=2048`.\n5. Configure the relative MSE tolerance to 0.01 (`relative_mse_tolerance=0.01`).\n6. Use Adam optimizer with beta1=0.90 and beta2=0.95 (`finetune_adam_beta1=0.90`, `finetune_adam_beta2=0.95`).\n7. Enable keeping the best model during finetuning (`finetune_keep_best`).\n8. Set the finetune relative MSE tolerance to 0.001 (`finetune_relative_mse_tolerance=0.001`).\n9. Use a finetune batch size of 32 and a local batch size of 4 (`finetune_batch_size=32`, `local_batch_size=4`).\n10. Offload activations to save memory (`offload_activations`). (B) 1. Use 1 codebook with a size of 2^15 (`num_codebooks=1`, `nbits_per_codebook=15`).\n2. Set the group size to 8 (`in_group_size=8`).\n3. Use the calibration dataset `data/red_pajama_n=4096_4096_context_length.pth` with `nsamples=2048`.\n4. Configure the relative MSE tolerance to 0.01 (`relative_mse_tolerance=0.01`).\n5. Set the finetune learning rate to 1e-4 (`finetune_lr=1e-4`).\n6. Use Adam optimizer with beta1=0.90 and beta2=0.95 (`finetune_adam_beta1=0.90`, `finetune_adam_beta2=0.95`).\n7. Enable keeping the best model during finetuning (`finetune_keep_best`).\n8. Set the finetune relative MSE tolerance to 0.001 (`finetune_relative_mse_tolerance=0.001`).\n9. Use a finetune batch size of 32 and a local batch size of 4 (`finetune_batch_size=32`, `local_batch_size=4`).\n10. Disable offloading activations to maximize performance (`no_offload_activations`). (C) 1. Use 1 codebook with a size of 2^15 (`num_codebooks=1`, `nbits_per_codebook=15`).\n2. Set the group size to 8 (`in_group_size=8`).\n3. Configure the relative MSE tolerance to 0.01 (`relative_mse_tolerance=0.01`).\n4. Set the finetune learning rate to 1e-4 (`finetune_lr=1e-4`).\n5. Use Adam optimizer with beta1=0.90 and beta2=0.95 (`finetune_adam_beta1=0.90`, `finetune_adam_beta2=0.95`).\n6. Enable keeping the best model during finetuning (`finetune_keep_best`).\n7. Set the finetune relative MSE tolerance to 0.001 (`finetune_relative_mse_tolerance=0.001`).\n8. Use a finetune batch size of 32 and a local batch size of 4 (`finetune_batch_size=32`, `local_batch_size=4`).\n9. Offload activations to save memory (`offload_activations`). (D) To achieve an average of 2.02 bits per layer for LLaMA2-7b, use the following configuration:\n1. Use 1 codebook with a size of 2^15 (`num_codebooks=1`, `nbits_per_codebook=15`).\n2. Set the group size to 8 (`in_group_size=8`).\n3. Use the calibration dataset `data/red_pajama_n=4096_4096_context_length.pth` with `nsamples=2048`.\n4. Configure the relative MSE tolerance to 0.01 (`relative_mse_tolerance=0.01`).\n5. Set the finetune learning rate to 1e-4 (`finetune_lr=1e-4`).\n6. Use Adam optimizer with beta1=0.90 and beta2=0.95 (`finetune_adam_beta1=0.90`, `finetune_adam_beta2=0.95`).\n7. Enable keeping the best model during finetuning (`finetune_keep_best`).\n8. Set the finetune relative MSE tolerance to 0.001 (`finetune_relative_mse_tolerance=0.001`).\n9. Use a finetune batch size of 32 and a local batch size of 4 (`finetune_batch_size=32`, `local_batch_size=4`).\n10. Offload activations to save memory (`offload_activations`).\n\nFor more details, refer to the README.md and ensure you are using the correct calibration dataset from Hugging Face.", "answer": "D"}
{"uuid": "2f3090b9-a727-4ce3-afcc-168127d32c72", "setup_instruct": "# AQLM Project Execution Plan\n\n## 1. Installation\n- **Description**: Install the AQLM inference library with GPU and CPU support.\n- **Command**:\n  ```bash\n  pip install aqlm[gpu,cpu]>=1.1.6\n  ```\n\n## 2. Model Download (Optional)\n- **Description**: Download and cache the base model from Hugging Face if not already available.\n- **Command**:\n  ```python\n  from transformers import AutoTokenizer, AutoModelForCausalLM\n  model_name = \"meta-llama/Llama-2-7b-hf\"  # Replace with desired model\n  tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=\"auto\")\n  model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n  ```\n\n## 3. Inference with Pre-Quantized Models\n- **Description**: Load and run a pre-quantized AQLM model for inference.\n- **Command**:\n  ```python\n  from transformers import AutoModelForCausalLM\n  quantized_model = AutoModelForCausalLM.from_pretrained(\n      \"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\",\n      trust_remote_code=True, torch_dtype=\"auto\"\n  ).cuda()\n  ```\n\n## 4. Quantization Setup\n- **Description**: Install dependencies and prepare for quantization.\n- **Command**:\n  ```bash\n  pip install -r requirements.txt\n  ```\n\n## 5. Data Preparation\n- **Description**: Download and preprocess calibration data (e.g., RedPajama subset).\n- **Command**:\n  ```bash\n  TARGET_MODEL=meta-llama/Llama-2-7b-hf\n  SEQLEN=4096\n  DATASET=togethercomputer/RedPajama-Data-1T-Sample\n  OUTPUT_PATH=./redpajama_tokenized_llama2\n\n  CUDA_VISIBLE_DEVICES=0 HF_HOME=/mnt/LLM OMP_NUM_THREADS=16 torchrun --master-port 3456 --nproc-per-node=1 finetune.py --base_model $TARGET_MODEL --quantized_model ./doesnt_matter --dtype bfloat16 --block_type LlamaDecoderLayer --dataset_name=$DATASET --split train --dataset_config_name plain_text --cache_dir=./cache_dir --trust_remote_code --model_seqlen=$SEQLEN --preprocessing_num_workers=64 --preprocessing_chunk_length 100000 --save_dataset_and_exit $OUTPUT_PATH\n  ```\n\n## 6. Model Quantization\n- **Description**: Run AQLM quantization on the target model.\n- **Command**:\n  ```bash\n  export CUDA_VISIBLE_DEVICES=0\n  export MODEL_PATH=meta-llama/Llama-2-7b-hf\n  export DATASET_PATH=./data/red_pajama_n=1024_4096_context_length.pth\n  export SAVE_PATH=/path/to/save/quantized/model/\n\n  python main.py $MODEL_PATH $DATASET_PATH \\\n   --nsamples=1024 \\\n   --val_size=128 \\\n   --num_codebooks=1 \\\n   --nbits_per_codebook=16 \\\n   --in_group_size=8 \\\n   --relative_mse_tolerance=0.01 \\\n   --finetune_batch_size=32 \\\n   --finetune_max_epochs=10 \\\n   --finetune_early_stop=3 \\\n   --finetune_keep_best \\\n   --local_batch_size=1 \\\n   --offload_activations \\\n   --save $SAVE_PATH\n  ```\n\n## 7. PV-Tuning (Optional)\n- **Description**: Improve quantized model accuracy with PV-Tuning.\n- **Command**:\n  ```bash\n  torchrun --nproc-per-node=$NUM_GPUS finetune.py \\\n      --base_model $MODEL_PATH \\\n      --quantized_model $QUANTIZED_WEIGHTS_PATH \\\n      --model_seqlen=$SEQLEN \\\n      --block_type LlamaDecoderLayer \\\n      --load_dtype bfloat16 \\\n      --amp_dtype bfloat16 \\\n      --code_dtype uint16 \\\n      --dataset_name=$TOKENIZED_DATASET_PATH \\\n      --split none \\\n      --seed 42 \\\n      --preprocessing_chunk_length 100000 \\\n      --cache_dir=$CACHE_DIR \\\n      --trust_remote_code \\\n      --update_codes \\\n      --update_codebooks_and_scales \\\n      --update_non_quantized_parameters \\\n      --lamb \\\n      --debias \\\n      --lr 3e-4 \\\n      --adam_beta1 0.90 \\\n      --adam_beta2 0.95 \\\n      --max_code_change_per_step 1e-2 \\\n      --code_lr 1e-2 \\\n      --code_beta1 0.0 \\\n      --code_beta2 0.95 \\\n      --beam_size 5 \\\n      --delta_decay 0 \\\n      --batch_size=128 \\\n      --microbatch_size=1 \\\n      --max_epochs 1 \\\n      --gradient_checkpointing \\\n      --print_every_steps=1 \\\n      --verbose_optimizer \\\n      --eval_every_steps=10 \\\n      --keep_best_model \\\n      --save $SAVE_PATH \\\n      --save_every_steps 100 \\\n      --attn_implementation flash_attention_2\n  ```\n\n## 8. Model Conversion for Inference\n- **Description**: Convert quantized model to Hugging Face format.\n- **Command**:\n  ```bash\n  python convert_to_hf.py meta-llama/Llama-2-7b-hf ./path/to/saved/quantization ./converted-llama2-7b-hf --save_safetensors\n  ```\n\n## 9. Evaluation (Optional)\n- **Description**: Evaluate model performance using LM Evaluation Harness.\n- **Command**:\n  ```bash\n  python lmeval.py \\\n      --model hf \\\n      --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n      --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \\\n      --batch_size 8 \\\n      --aqlm_checkpoint_path $QUANTIZED_MODEL\n  ```", "issue_title": "unable to install", "issue_body": "py -m pip install aqlm[gpu,cpu]\r\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\nCollecting aqlm[cpu,gpu]\r\n  Downloading aqlm-1.0.0-py3-none-any.whl (10 kB)\r\nCollecting torch>=2.1.1\r\n  Downloading torch-2.2.0-cp311-cp311-win_amd64.whl (198.6 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 198.6/198.6 MB 12.6 MB/s eta 0:00:00\r\nCollecting transformers==4.37.0\r\n  Downloading transformers-4.37.0-py3-none-any.whl (8.4 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.4/8.4 MB 8.5 MB/s eta 0:00:00\r\nCollecting numba>=0.56.4\r\n  Downloading numba-0.59.0-cp311-cp311-win_amd64.whl (2.6 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 18.7 MB/s eta 0:00:00\r\nCollecting scipy>=1.11.3\r\n  Downloading scipy-1.12.0-cp311-cp311-win_amd64.whl (46.2 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.2/46.2 MB 13.1 MB/s eta 0:00:00\r\nERROR: Ignored the following versions that require a different python version: 0.52.0 Requires-Python >=3.6,<3.9; 0.52.0rc3 Requires-Python >=3.6,<3.9; 0.53.0 Requires-Python >=3.6,<3.10; 0.53.0rc1.post1 Requires-Python >=3.6,<3.10; 0.53.0rc2 Requires-Python >=3.6,<3.10; 0.53.0rc3 Requires-Python >=3.6,<3.10; 0.53.1 Requires-Python >=3.6,<3.10; 0.54.0 Requires-Python >=3.7,<3.10; 0.54.0rc2 Requires-Python >=3.7,<3.10; 0.54.0rc3 Requires-Python >=3.7,<3.10; 0.54.1 Requires-Python >=3.7,<3.10; 0.55.0 Requires-Python >=3.7,<3.11; 0.55.0rc1 Requires-Python >=3.7,<3.11; 0.55.1 Requires-Python >=3.7,<3.11; 0.55.2 Requires-Python >=3.7,<3.11; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11\r\nERROR: Could not find a version that satisfies the requirement triton>=2.1; extra == \"gpu\" (from aqlm[cpu,gpu]) (from versions: none)\r\nERROR: No matching distribution found for triton>=2.1; extra == \"gpu\"", "choices": "(A) 1. Ensure `drop_hex=False` is set in the inference configuration to align the injected hex condition with the inference results.\n2. Set `drop_hex=True` to disable hex condition alignment.\n3. Check the dataloader logic in `dynamic_city/dataset/hexplane_dataset.py` at line 35. Replace `self.hexplanes` with `self.conditions` to ensure the correct data is loaded for `hex_cond`.\n4. Verify that the same hexplane is used for both condition and generation during training and inference.\n5. Adjust inference parameters such as `cfg_scale`, `num_sampling_steps`, and `seed` if further issues arise with the prediction results. (B) 1. Ensure `drop_hex=False` is set in the inference configuration to align the injected hex condition with the inference results.\n2. Verify that the same hexplane is used for both condition and generation during training and inference.\n3. Adjust inference parameters such as `cfg_scale`, `num_sampling_steps`, and `seed` if further issues arise with the prediction results. (C) 1. Ensure `drop_hex=False` is set in the inference configuration to align the injected hex condition with the inference results.\n2. Verify that the same hexplane is used for both condition and generation during training and inference.\n3. Check the dataloader logic in `dynamic_city/dataset/hexplane_dataset.py` at line 35. Replace `self.hexplanes` with `self.conditions` to ensure the correct data is loaded for `hex_cond`.\n4. Adjust inference parameters such as `cfg_scale`, `num_sampling_steps`, and `seed` if further issues arise with the prediction results. (D) 1. Ensure `drop_hex=False` is set in the inference configuration to align the injected hex condition with the inference results.\n2. Check the dataloader logic in `dynamic_city/dataset/hexplane_dataset.py` at line 35. Replace `self.hexplanes` with `self.conditions` to ensure the correct data is loaded for `hex_cond`.\n3. Verify that the same hexplane is used for both condition and generation during training and inference.\n4. Adjust inference parameters such as `cfg_scale`, `num_sampling_steps`, and `seed` if further issues arise with the prediction results.", "answer": "D"}
{"uuid": "9f1354ba-2611-455a-80e1-831faf3e0849", "setup_instruct": "# AQLM Project Execution Plan\n\n## 1. Installation\n- **Description**: Install the AQLM inference library with GPU and CPU support.\n- **Command**:\n  ```bash\n  pip install aqlm[gpu,cpu]>=1.1.6\n  ```\n\n## 2. Model Download\n- **Description**: Download and cache the base model in Hugging Face format.\n- **Command**:\n  ```python\n  from transformers import AutoTokenizer, AutoModelForCausalLM\n  model_name = \"meta-llama/Llama-2-7b-hf\"  # or other supported model\n  tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=\"auto\")\n  model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n  ```\n\n## 3. Data Preparation\n- **Description**: Pre-tokenize calibration data (RedPajama recommended for Llama-2).\n- **Command**:\n  ```bash\n  TARGET_MODEL=meta-llama/Llama-2-7b-hf\n  SEQLEN=4096\n  DATASET=togethercomputer/RedPajama-Data-1T-Sample\n  OUTPUT_PATH=./redpajama_tokenized_llama2\n\n  CUDA_VISIBLE_DEVICES=0 HF_HOME=/mnt/LLM OMP_NUM_THREADS=16 torchrun --master-port 3456 --nproc-per-node=1 finetune.py --base_model $TARGET_MODEL --quantized_model ./doesnt_matter --dtype bfloat16 --block_type LlamaDecoderLayer --dataset_name=$DATASET --split train --dataset_config_name plain_text --cache_dir=./cache_dir --trust_remote_code --model_seqlen=$SEQLEN --preprocessing_num_workers=64 --preprocessing_chunk_length 100000 --save_dataset_and_exit $OUTPUT_PATH\n  ```\n\n## 4. Quantization\n- **Description**: Run AQLM quantization with specified parameters.\n- **Command**:\n  ```bash\n  export CUDA_VISIBLE_DEVICES=0\n  export MODEL_PATH=meta-llama/Llama-2-7b-hf\n  export DATASET_PATH=./data/red_pajama_n=1024_4096_context_length.pth\n  export SAVE_PATH=/path/to/save/quantized/model/\n  export WANDB_PROJECT=MY_AQ_EXPS\n  export WANDB_NAME=COOL_EXP_NAME\n\n  python main.py $MODEL_PATH $DATASET_PATH \\\n   --nsamples=1024 \\\n   --val_size=128 \\\n   --num_codebooks=1 \\\n   --nbits_per_codebook=16 \\\n   --in_group_size=8 \\\n   --relative_mse_tolerance=0.01 \\\n   --finetune_batch_size=32 \\\n   --finetune_max_epochs=10 \\\n   --finetune_early_stop=3 \\\n   --finetune_keep_best \\\n   --local_batch_size=1 \\\n   --offload_activations \\\n   --wandb \\\n   --resume \\\n   --save $SAVE_PATH\n  ```\n\n## 5. Finetuning (Optional)\n- **Description**: Improve quantized model accuracy with PV-Tuning.\n- **Command**:\n  ```bash\n  torchrun --nproc-per-node=$NUM_GPUS finetune.py \\\n      --base_model $MODEL_PATH \\\n      --quantized_model $QUANTIZED_WEIGHTS_PATH \\\n      --model_seqlen=$SEQLEN \\\n      --block_type LlamaDecoderLayer \\\n      --load_dtype bfloat16 \\\n      --amp_dtype bfloat16 \\\n      --code_dtype uint16 \\\n      --dataset_name=$TOKENIZED_DATASET_PATH \\\n      --split none \\\n      --seed 42 \\\n      --preprocessing_chunk_length 100000 \\\n      --cache_dir=$CACHE_DIR \\\n      --trust_remote_code \\\n      --update_codes \\\n      --update_codebooks_and_scales \\\n      --update_non_quantized_parameters \\\n      --lamb \\\n      --debias \\\n      --lr 3e-4 \\\n      --adam_beta1 0.90 \\\n      --adam_beta2 0.95 \\\n      --max_code_change_per_step 1e-2 \\\n      --code_lr 1e-2 \\\n      --code_beta1 0.0 \\\n      --code_beta2 0.95 \\\n      --beam_size 5 \\\n      --delta_decay 0 \\\n      --batch_size=128 \\\n      --microbatch_size=1 \\\n      --max_epochs 1 \\\n      --gradient_checkpointing \\\n      --print_every_steps=1 \\\n      --verbose_optimizer \\\n      --wandb \\\n      --eval_every_steps=10 \\\n      --keep_best_model \\\n      --save $SAVE_PATH \\\n      --save_every_steps 100 \\\n      --attn_implementation flash_attention_2\n  ```\n\n## 6. Model Conversion\n- **Description**: Convert quantized model to Hugging Face format.\n- **Command**:\n  ```bash\n  python convert_to_hf.py meta-llama/Llama-2-7b-hf ./path/to/saved/quantization ./converted-llama2-7b-hf --save_safetensors\n  ```\n\n## 7. Inference\n- **Description**: Load and run the quantized model.\n- **Command**:\n  ```python\n  from transformers import AutoModelForCausalLM\n  quantized_model = AutoModelForCausalLM.from_pretrained(\n      \"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\",\n      trust_remote_code=True, torch_dtype=\"auto\"\n  ).cuda()\n  ```\n\n## 8. Evaluation (Optional)\n- **Description**: Evaluate model performance using LM Evaluation Harness.\n- **Command**:\n  ```bash\n  export QUANTIZED_MODEL=./converted-llama2-7b-hf\n  export MODEL_PATH=meta-llama/Llama-2-7b-hf\n\n  # 0-shot evaluation\n  python lmeval.py \\\n      --model hf \\\n      --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n      --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \\\n      --batch_size 32 \\\n      --aqlm_checkpoint_path $QUANTIZED_MODEL\n\n  # 5-shot MMLU evaluation\n  python lmeval.py \\\n      --model hf \\\n      --model_args pretrained=$MODEL_PATH,dtype=float16,parallelize=True \\\n      --tasks mmlu \\\n      --batch_size 32 \\\n      --num_fewshot 5 \\\n      --aqlm_checkpoint_path $QUANTIZED_MODEL\n  ```", "issue_title": "I wanted to know what is the beauty of technology", "issue_body": "I welcome the developers of the new method. I launched your goolge colab and got very dubious inference. Can you explain a little about the essence of the breakthrough of this compression? I may not have understood something in terms of use, but judging by the inference, it is completely lost.\r\n\r\n```\r\nquantized_model = AutoModelForCausalLM.from_pretrained(\r\n    \"BlackSamorez/Llama-2-7b-AQLM-2Bit-1x16-hf\", trust_remote_code=True, torch_dtype=torch.float16,\r\n).cuda()\r\ntokenizer = AutoTokenizer.from_pretrained(\"BlackSamorez/Llama-2-7b-AQLM-2Bit-1x16-hf\")\r\n\r\ninputs = tokenizer([\"Write a poem about python\"], return_tensors=\"pt\")[\"input_ids\"].cuda()\r\n\r\nstreamer = TextStreamer(tokenizer)\r\n_ = quantized_model.generate(inputs, streamer=streamer, max_new_tokens=120)\r\n\r\n<s> Write a poem about python.\r\nWrite a poem about python.\r\nWrite a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem about python. Write a poem\r\n```", "choices": "(A) 1. Ensure the checkpoint file `ckpts/DIT_CARLA/last.ckpt` is correctly placed in the specified directory.\n2. Verify the file permissions and ensure the script has the necessary access rights to load the checkpoint.\n3. Run the inference command again without the `--best_vae` flag to ensure proper loading of the checkpoint: `torchrun --nproc-per-node 8 infer_dit.py -d DIT_CARLA`.\n4. Check the console output to confirm that the checkpoint is loaded successfully. (B) 1. Run the inference command again without the `--best_vae` flag to ensure proper loading of the checkpoint: `torchrun --nproc-per-node 8 infer_dit.py -d DIT_CARLA`.\n2. Ensure the checkpoint file `ckpts/DIT_CARLA/last.ckpt` is correctly placed in the specified directory.\n3. Verify the file permissions and ensure the script has the necessary access rights to load the checkpoint.\n4. Check the console output to confirm that the checkpoint is loaded successfully. (C) 1. Ensure the checkpoint file `ckpts/DIT_CARLA/last.ckpt` is correctly placed in the specified directory.\n2. Run the inference command again without the `--best_vae` flag to ensure proper loading of the checkpoint: `torchrun --nproc-per-node 8 infer_dit.py -d DIT_CARLA`.\n3. Check the console output to confirm that the checkpoint is loaded successfully. (D) 1. Ensure the checkpoint file `ckpts/DIT_CARLA/last.ckpt` is correctly placed in the specified directory.\n2. Verify the file permissions and ensure the script has the necessary access rights to load the checkpoint.\n3. Run the inference command again without the `--best_vae` flag to ensure proper loading of the checkpoint: `torchrun --nproc-per-node 8 infer_dit.py -d DIT_CARLA`.\n4. Delete the checkpoint file `ckpts/DIT_CARLA/last.ckpt` to free up space.\n5. Check the console output to confirm that the checkpoint is loaded successfully.", "answer": "A"}
{"uuid": "026d21a9-fb8f-41e4-92ff-df2c26bbcf4c", "setup_instruct": "# DynamicCity Deployment Plan\n\n## 1. Environment Setup\n- **Description**: Create a Conda environment and install required dependencies\n- **Commands**:\n  ```shell\n  conda create -n dyncity python=3.10 -y\n  conda activate dyncity\n  conda install pytorch==2.4.0 pytorch-cuda=11.8 -c pytorch -c nvidia\n  conda install einops hydra-core matplotlib numpy omegaconf timm tqdm wandb -c conda-forge -y\n  pip install flash-attn --no-build-isolation\n  ```\n\n## 2. Data Preparation\n- **Description**: Download and organize the CarlaSC dataset\n- **Steps**:\n  1. Download dataset from [CarlaSC website](https://umich-curly.github.io/CarlaSC.github.io/download/)\n  2. Extract to `./carlasc` directory with structure:\n     ```\n     DynamicCity\n     ├── carlasc/\n     │   ├── Cartesian/\n     │   │   ├── Train/\n     │   │   │   ├── Town01_Heavy\n     │   │   │   ├── ...\n     │   │   ├── Test/\n     ```\n\n## 3. Model Checkpoints\n- **Description**: Download pretrained models\n- **Command**:\n  ```shell\n  # Download from OneDrive link and unzip to ./ckpts\n  unzip ckpts.zip -d ./ckpts\n  ```\n\n## 4. Training Pipeline\n### 4.1 VAE Training\n- **Description**: Train the VAE model on CarlaSC data\n- **Command**:\n  ```shell\n  torchrun --nproc-per-node 8 train.py VAE carlasc name=DYNAMIC_CITY_VAE\n  ```\n\n### 4.2 HexPlane Rollout Generation\n- **Description**: Generate hexplane rollouts from trained VAE\n- **Command**:\n  ```shell\n  torchrun --nproc-per-node 8 infer_vae.py -n DYNAMIC_CITY_VAE --save_rollout --best\n  ```\n\n### 4.3 DiT Training\n- **Description**: Train the Diffusion Transformer model\n- **Command**:\n  ```shell\n  torchrun --nproc-per-node 8 train.py DiT carlasc name=DYNAMIC_CITY_DIT vae_name=DYNAMIC_CITY_VAE\n  ```\n\n## 5. Inference\n- **Description**: Generate novel city scenes using trained models\n- **Command**:\n  ```shell\n  torchrun --nproc-per-node 8 infer_dit.py -d DYNAMIC_CITY_DIT --best_vae\n  ```\n\n## 6. Generation Modes (Optional)\n- **Description**: Explore different generation modes by modifying inference parameters:\n  - Unconditional generation (default)\n  - HexPlane conditional generation\n  - Command & Trajectory-driven\n  - Layout-conditioned\n  - Dynamic scene inpainting", "issue_title": "There is a problem with the hex conditional train.", "issue_body": "I trained hex conditional on carlasc dataset from scratch. The loss converged to 0.01 and about 800 epochs. Then I used this ckpt for inference, but the inference scenario is very different from the injected hex condition.\n我在carlasc数据集从头开始训练hex条件，loss收敛到0.01，大概800个epoch。然后我用这个ckpt进行推理，但是推理的4D Occ和注入的hex条件差距比较大\n\nThe loss curve is as follows，\n![Image](https://github.com/user-attachments/assets/2419398b-45a9-4651-84a4-b0d21c3c4045)", "choices": "(A) 1. Download the required pre-trained weights for all branches as specified in the config file.\n2. Ensure the paths to the pre-trained weights in the config file are correctly set to the downloaded files.\n3. Do not modify the 'pretrained' parameter in the config file unless you have downloaded the corresponding weights.\n4. If using a single GPU (e.g., 4090), ensure any additional settings like deepspeed are turned off during evaluation. (B) 1. Download the required pre-trained weights for all branches as specified in the config file.\n2. Do not modify the 'pretrained' parameter in the config file unless you have downloaded the corresponding weights.\n3. If using a single GPU (e.g., 4090), ensure any additional settings like deepspeed are turned off during evaluation. (C) 1. Download the required pre-trained weights for all branches as specified in the config file.\n2. Ensure the paths to the pre-trained weights in the config file are correctly set to the downloaded files.\n3. Do not modify the 'pretrained' parameter in the config file unless you have downloaded the corresponding weights. (D) 1. Download the required pre-trained weights for all branches as specified in the config file.\n2. Ensure the paths to the pre-trained weights in the config file are correctly set to the downloaded files.\n3. Modify the 'pretrained' parameter in the config file to a custom value.\n4. Do not modify the 'pretrained' parameter in the config file unless you have downloaded the corresponding weights.\n5. If using a single GPU (e.g., 4090), ensure any additional settings like deepspeed are turned off during evaluation. (E) 1. Ensure the paths to the pre-trained weights in the config file are correctly set to the downloaded files.\n2. Download the required pre-trained weights for all branches as specified in the config file.\n3. Do not modify the 'pretrained' parameter in the config file unless you have downloaded the corresponding weights.\n4. If using a single GPU (e.g., 4090), ensure any additional settings like deepspeed are turned off during evaluation. (F) 1. Download the required pre-trained weights for all branches as specified in the config file.\n2. Ensure the paths to the pre-trained weights in the config file are correctly set to the downloaded files.\n3. Do not modify the 'pretrained' parameter in the config file unless you have downloaded the corresponding weights.\n4. Enable deepspeed settings for single GPU (e.g., 4090) during evaluation. (G) 1. Download the required pre-trained weights for all branches as specified in the config file.\n2. Ensure the paths to the pre-trained weights in the config file are correctly set to the downloaded files.\n3. If using a single GPU (e.g., 4090), ensure any additional settings like deepspeed are turned off during evaluation.\n4. Do not modify the 'pretrained' parameter in the config file unless you have downloaded the corresponding weights.", "answer": "A"}
{"uuid": "f6c7a579-4582-4ebf-a28f-4209f1daa143", "setup_instruct": "# PIIP Project Execution Plan\n\n## 1. Project Setup\n- **Description**: Clone the repository and navigate to the project directory.\n- **Command**:\n  ```bash\n  git clone https://github.com/OpenGVLab/PIIP.git\n  cd PIIP\n  ```\n\n## 2. Installation\n- **Description**: Install required dependencies based on your task (detection/segmentation/classification/multimodal).\n- **Commands**:\n  ```bash\n  # For object detection/instance segmentation (mmdetection)\n  pip install -v -e mmdetection\n\n  # For semantic segmentation (mmsegmentation)\n  pip install -v -e mmsegmentation\n\n  # For image classification\n  pip install timm\n\n  # For multimodal understanding (LLaVA)\n  pip install -e llava\n  ```\n\n## 3. Download Pretrained Models\n- **Description**: Download the appropriate pretrained models from Hugging Face based on your task.\n- **Example Commands**:\n  ```bash\n  # Example for DeiT-based detection model\n  wget https://huggingface.co/OpenGVLab/PIIP/resolve/main/detection/mask_rcnn_deit_vit_b_fpn_1x_coco_bs16.pth -P checkpoints/\n\n  # Example for PIIP-LLaVA model\n  wget https://huggingface.co/OpenGVLab/PIIP-LLaVA_CLIP-BL_512-256_7B/resolve/main/piip-llava_clip-bl_512-256_7b.pth -P checkpoints/\n  ```\n\n## 4. Training\n- **Description**: Run training with the appropriate config file for your task.\n- **Example Commands**:\n  ```bash\n  # For detection (single GPU)\n  python mmdetection/tools/train.py mmdetection/configs/piip/baseline/mask_rcnn_deit_vit_b_fpn_1x_coco_bs16.py\n\n  # For segmentation (distributed training)\n  bash mmsegmentation/tools/dist_train.sh mmsegmentation/configs/piip/baseline/upernet_internvit_6b_512_80k_ade20k_bs16_lr4e-5.py 8\n\n  # For multimodal training\n  torchrun --nproc_per_node=8 llava/train/train_mem.py llava/configs/piip-llava_clip-bl_512-256_7B.py\n  ```\n\n## 5. Evaluation\n- **Description**: Evaluate trained models on validation sets.\n- **Example Commands**:\n  ```bash\n  # For detection\n  python mmdetection/tools/test.py mmdetection/configs/piip/baseline/mask_rcnn_deit_vit_b_fpn_1x_coco_bs16.py checkpoints/mask_rcnn_deit_vit_b_fpn_1x_coco_bs16.pth --eval bbox segm\n\n  # For segmentation\n  python mmsegmentation/tools/test.py mmsegmentation/configs/piip/baseline/upernet_internvit_6b_512_80k_ade20k_bs16_lr4e-5.py checkpoints/upernet_internvit_6b_512_80k_ade20k_bs16_lr4e-5.pth --eval mIoU\n  ```\n\n## 6. Inference/Demo\n- **Description**: Run inference on custom images or use provided demo scripts.\n- **Example Commands**:\n  ```bash\n  # For multimodal demo\n  python llava/serve/cli.py --model-path checkpoints/piip-llava_clip-bl_512-256_7b.pth --config llava/configs/piip-llava_clip-bl_512-256_7B.py --image-file demo.jpg\n  ```\n\n## 7. Model Conversion (Optional)\n- **Description**: Convert models to different formats if needed.\n- **Example Command**:\n  ```bash\n  python tools/convert_models.py --input checkpoints/original.pth --output checkpoints/converted.pth\n  ```", "issue_title": "A question about model loading", "issue_body": "The congfig file needs to fill in the pre-trained weight paths of multiple branches, but the downloaded ckpt file is only one. How can I load it?", "choices": "(A) 1. Adjust the 'accum_iter' parameter to 2 instead of the default value of 3 to match the effective batch size of 768 as per the paper's settings. 2. Ensure the batch size is correctly set to 32 when using 8*A100 GPUs. 3. Remove the random seed if the data loader is working slowly to improve performance. (B) 1. Ensure the batch size is correctly set to 32 when using 8*A100 GPUs. 2. Remove the random seed if the data loader is working slowly to improve performance. (C) 1. Adjust the 'accum_iter' parameter to 2 instead of the default value of 3 to match the effective batch size of 768 as per the paper's settings. 2. Remove the random seed if the data loader is working slowly to improve performance. (D) 1. Ensure the batch size is correctly set to 32 when using 8*A100 GPUs. 2. Adjust the 'accum_iter' parameter to 2 instead of the default value of 3 to match the effective batch size of 768 as per the paper's settings. 3. Set the random seed to 42 for reproducibility. 4. Remove the random seed if the data loader is working slowly to improve performance. (E) 1. Remove the random seed if the data loader is working slowly to improve performance. 2. Ensure the batch size is correctly set to 32 when using 8*A100 GPUs. 3. Adjust the 'accum_iter' parameter to 2 instead of the default value of 3 to match the effective batch size of 768 as per the paper's settings. (F) 1. Ensure the batch size is correctly set to 64 when using 8*A100 GPUs. 2. Adjust the 'accum_iter' parameter to 2 instead of the default value of 3 to match the effective batch size of 768 as per the paper's settings. 3. Remove the random seed if the data loader is working slowly to improve performance. (G) 1. Ensure the batch size is correctly set to 32 when using 8*A100 GPUs. 2. Adjust the 'accum_iter' parameter to 2 instead of the default value of 3 to match the effective batch size of 768 as per the paper's settings. 3. Remove the random seed if the data loader is working slowly to improve performance.", "answer": "G"}
{"uuid": "608eee57-5306-428a-a3d7-8c08b31bf84b", "setup_instruct": "# Execution Plan for Both Ears Wide Open Project\n\n## 1. Environment Setup\n- **Description**: Set up a Python environment with the required dependencies.\n- **Commands**:\n  ```bash\n  cd models\n  conda create -n \"bewo\" python=3.9\n  conda activate bewo\n  pip install -r requirements.txt --no-dependencies\n  ```\n\n## 2. Download Pretrained Models\n- **Description**: Download the pretrained model checkpoints and config files.\n- **Steps**:\n  1. Download the following models from Hugging Face:\n     - [BEWO_nl.ckpt](https://huggingface.co/spw2000/bewo-1m/resolve/main/BEWO_nl.ckpt)\n     - [BEWO_attri.ckpt](https://huggingface.co/spw2000/bewo-1m/resolve/main/BEWO_attri.ckpt)\n     - [BEWO_mix.ckpt](https://huggingface.co/spw2000/bewo-1m/resolve/main/BEWO_mix.ckpt)\n  2. Place the downloaded files in `./bewo_config/`:\n     - `BEWO_nl.ckpt`\n     - `BEWO_attri.ckpt`\n     - `BEWO_mix.ckpt`\n     - `model_config_sim.json`\n     - `model_config_sim_mix.json`\n\n## 3. Simple Audio Generation\n- **Description**: Generate audio from text prompts using the pretrained model.\n- **Commands**:\n  ```bash\n  cd models\n  python simple_generation.py --prompt \"A dog is barking on the left.\" --device cuda:0\n  python simple_generation.py --prompt \"a car is moving from left to right.\" --device cuda:0\n  ```\n\n## 4. Coarse-to-Fine Audio Generation (GPT Induction)\n- **Description**: Generate audio with spatial attributes using GPT induction.\n- **Commands**:\n  ```bash\n  cd models\n  python gpt_induction.py --prompt \"A dog is barking on the left.\" --device cuda:0\n  python gpt_induction.py --prompt \"a dog is barking and running from left to right.\" --device cuda:0\n  ```\n\n## 5. Coarse-to-Fine Audio Generation (Manual Setting)\n- **Description**: Generate audio with manually specified spatial attributes.\n- **Commands**:\n  ```bash\n  cd models\n  python gpt_induction.py --prompt \"a dog is barking.\" --device cuda:0 --manual True --init_direction 1 --final_direction 1 --moving 0\n  python gpt_induction.py --prompt \"a dog is barking.\" --device cuda:0 --manual True --init_direction 1 --final_direction 5 --moving 1\n  ```\n\n## 6. Evaluation\n- **Description**: Evaluate the generated audio using the provided evaluation scripts.\n- **Steps**:\n  1. For ITD evaluation, refer to `./evaluations`.\n  2. For 1-channel T2A evaluation, refer to [MAA](https://github.com/Text-to-Audio/Make-An-Audio).\n\n## 7. Citation\n- **Description**: Cite the relevant papers if you use this work.\n- **Steps**:\n  - Add the provided BibTeX entries to your references.", "issue_title": "Worse metric than paper mentioned", "issue_body": "Recently I try to test **BEWO_nl** model on **Audiocaps test set**, with code as below. I use code on **'./models/simple_generation.py'**. \nBut I get worse metric than paper mentioned.\n\nThis code I not change the generation code:\n`\ndef generate_audio(prompt, model, device, output_dir, sample_idx, resampler):\n    conditioning = [{\"prompt\": prompt, \"seconds_start\": 0, \"seconds_total\": 10}]\n\n    audio = generate_diffusion_cond(\n        model,\n        conditioning=conditioning,\n        negative_conditioning=None,\n        steps=250,\n        cfg_scale=7.5,\n        batch_size=1,\n        sample_size=int(44100 * 10),\n        sample_rate=44100,\n        seed=-1,\n        device=device,\n        sampler_type=\"dpmpp-3m-sde\",\n        sigma_min=0.03,\n        sigma_max=1000,\n        init_audio=None,\n        init_noise_level=1.0,\n        mask_args=None,\n        callback=None,\n        scale_phi=0\n    )\n\n\n    audio_16k = resampler(audio.squeeze(0).cpu())\n    audio_16k_filename = f\"test_sample_{sample_idx}.wav\"\n    torchaudio.save(output_dir / audio_16k_filename, audio_16k, sample_rate=16000)\n\n    return {\n        \"sample_index\": sample_idx,\n        \"prompt\": prompt,\n        \"audio_filename\": audio_16k_filename,\n    }\n\n\nif __name__ == \"__main__\":\n    output_dir = Path(\"/workspace/BothEars/audiocaps_test\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    resampler = torchaudio.transforms.Resample(orig_freq=44100, new_freq=16000)\n\n    model_config_path = \"/workspace/BothEars/models/bewo_config/model_config_sim.json\"\n    model_ckpt_path = \"/workspace/BothEars/BEWO_nl.ckpt\"\n\n    with open(model_config_path) as f:\n        model_config = json.load(f)\n\n    model, model_config = load_model(\n        model_config=model_config,\n        model_ckpt_path=model_ckpt_path,\n        device='cuda:1'\n    )\n\n    # 准备数据集\n    test_ds = AudioCaps(data_type=\"test\")\n    test_dataloader = torch.utils.data.DataLoader(\n        test_ds,\n        batch_size=1,\n        shuffle=False,\n        num_workers=4\n    )\n\n    # 初始化结果数据结构\n    generated_data = {\n        \"epoch\": \"test\",\n        \"samples\": []\n    }\n\n    # 生成音频\n    for sample_idx, batch in enumerate(tqdm(test_dataloader, desc=\"Generating samples\")):\n        prompt = batch['prompt'][0]\n\n        sample_info = generate_audio(\n            prompt=prompt,\n            model=model,\n            device='cuda:1',\n            output_dir=output_dir,\n            sample_idx=sample_idx,\n            resampler=resampler\n        )\n\n        generated_data[\"samples\"].append(sample_info)\n\n    metadata_path = output_dir / \"test_metadata.json\"\n    with open(metadata_path, 'w', encoding='utf-8') as f:\n        json.dump(generated_data, f, ensure_ascii=False, indent=4)\n\n    print(f\"Generated {len(generated_data['samples'])} samples\")\n    print(f\"Metadata saved to {metadata_path}\")`\n\nAnd result as blow:\n`{'frechet_distance': 89.97694638372688, 'frechet_audio_distance': 15.051326558644718, 'kullback_leibler_divergence_sigmoid': 11.803584098815918, 'kullback_leibler_divergence_softmax': 4.642857074737549, 'lsd': nan, 'psnr': nan, 'ssim': nan, 'inception_score_mean': 3.2329116491147487, 'inception_score_std': 0.47343594734223854, 'kernel_inception_distance_mean': nan, 'kernel_inception_distance_std': nan}`", "choices": "(A) 1. Modify the data loading pipeline to handle MONAI's `MetaTensor` by extracting the numpy array from the loaded data.\n2. Input the extracted numpy array into the subsequent processing pipeline as described in the original code. (B) 1. Modify the data loading pipeline to handle MONAI's `MetaTensor` by extracting the numpy array from the loaded data.\n2. Ensure the `demo_case` configuration in `config_demo.json` has the correct absolute paths to the `.nii.gz` files for both `ct_path` and `gt_path`.\n3. Input the extracted numpy array into the subsequent processing pipeline as described in the original code. (C) 1. Ensure the `demo_case` configuration in `config_demo.json` has the correct absolute paths to the `.nii.gz` files for both `ct_path` and `gt_path`.\n2. Overwrite `config_demo.json` with empty paths for `ct_path` and `gt_path`.\n3. Modify the data loading pipeline to handle MONAI's `MetaTensor` by extracting the numpy array from the loaded data.\n4. Input the extracted numpy array into the subsequent processing pipeline as described in the original code. (D) 1. Ensure the `demo_case` configuration in `config_demo.json` has the correct absolute paths to the `.nii.gz` files for both `ct_path` and `gt_path`.\n2. Modify the data loading pipeline to handle MONAI's `MetaTensor` by extracting the numpy array from the loaded data.\n3. Input the extracted numpy array into the subsequent processing pipeline as described in the original code. (E) 1. Ensure the `demo_case` configuration in `config_demo.json` has the correct absolute paths to the `.nii.gz` files for both `ct_path` and `gt_path`.\n2. Input the extracted numpy array into the subsequent processing pipeline as described in the original code.\n3. Modify the data loading pipeline to handle MONAI's `MetaTensor` by extracting the numpy array from the loaded data. (F) 1. Ensure the `demo_case` configuration in `config_demo.json` has the correct absolute paths to the `.nii.gz` files for both `ct_path` and `gt_path`.\n2. Input the extracted numpy array into the subsequent processing pipeline as described in the original code. (G) 1. Ensure the `demo_case` configuration in `config_demo.json` has the correct absolute paths to the `.nii.gz` files for both `ct_path` and `gt_path`.\n2. Modify the data loading pipeline to handle MONAI's `MetaTensor` by extracting the numpy array from the loaded data.\n3. Delete the extracted numpy array to free up memory.\n4. Input the extracted numpy array into the subsequent processing pipeline as described in the original code.", "answer": "D"}
{"uuid": "b8b86b48-95c9-4f41-8e9a-129bf3663585", "setup_instruct": "# QKFormer Deployment and Execution Plan\n\n## 1. Environment Setup\n- **Description**: Install required Python packages and verify compatibility.\n- **Commands**:\n  ```bash\n  pip install timm==0.6.12 cupy==11.4.0 torch==1.12.1 spikingjelly==0.0.0.0.12 pyyaml tensorboard\n  ```\n\n## 2. Dataset Preparation\n- **Description**: Download and structure the ImageNet dataset.\n- **Steps**:\n  1. Download ImageNet dataset.\n  2. Use the provided [script](https://gist.github.com/BIGBALLON/8a71d225eff18d88e469e6ea9b39cef4) to extract into the required folder structure:\n     ```\n     imagenet/\n     ├── train/\n     │   ├── n01440764/\n     │   │   ├── n01440764_10026.JPEG\n     │   │   └── ...\n     └── val/\n         ├── n01440764/\n         │   ├── ILSVRC2012_val_00000293.JPEG\n         │   └── ...\n     ```\n\n## 3. Model Download\n- **Description**: Download pre-trained QKFormer models from Baidu Drive or Google Drive.\n- **Links**:\n  - Baidu Drive (Password: `abcd`):\n    - [HST-10-384](https://pan.baidu.com/s/1mX0jQyKZ5p6ZDzvMVeY20A)\n    - [HST-10-512](https://pan.baidu.com/s/1luWM1L8gV3BI7REh4MgbkA)\n    - [HST-10-768](https://pan.baidu.com/s/1WJW1wC0Vs-lvGjYr5pGV_w)\n  - [Google Drive](https://drive.google.com/drive/folders/1vhq9jmhmuyZ5_RGHuWD4wniza856qF8U?usp=drive_link)\n\n## 4. Training on ImageNet\n- **Description**: Train QKFormer on ImageNet using distributed training.\n- **Command**:\n  ```bash\n  cd imagenet\n  python -m torch.distributed.launch --nproc_per_node=8 train.py\n  ```\n\n## 5. Testing on ImageNet\n- **Description**: Evaluate the trained model on ImageNet validation data.\n- **Command**:\n  ```bash\n  cd imagenet\n  python test.py\n  ```\n\n## 6. Training on Other Datasets\n- **CIFAR10**:\n  - **Description**: Configure `cifar10.yml` and train.\n  - **Command**:\n    ```bash\n    cd cifar10\n    python train.py\n    ```\n- **CIFAR100**:\n  - **Description**: Configure `cifar100.yml` and train.\n  - **Command**:\n    ```bash\n    cd cifar10  # Note: README mentions \"cd cifar10\" for CIFAR100\n    python train.py\n    ```\n- **DVS128 Gesture**:\n  - **Command**:\n    ```bash\n    cd dvs128-gesture\n    python train.py\n    ```\n- **CIFAR10-DVS**:\n  - **Command**:\n    ```bash\n    cd cifar10-dvs\n    python train.py\n    ```\n\n## 7. Citation and Contact\n- **Description**: Cite the work if used in research. Contact authors for issues via GitHub or email (`zhouchl@pcl.ac.cn` or `zhouchenlin19@mails.ucas.ac.cn`).", "issue_title": "关于QKFormer在CIFAR10_DVS上T=10时的精度", "issue_body": "你好！根据你们的工作在T=16时是可以实现与论文相当的精度的，但是当T=10时训练精度时只有78%左右。请问是配置问题吗？那你们工作的配置是怎么样的呢?如果不是配置问题，请问是什么原因导致的呢，怎么解决？", "choices": "(A) 1. Update to version 0.3.5 of the repository to resolve the issue with erroneous masking in adaptive modules and text labels.\n2. For image generation, ensure the training process includes both text and images, and avoid manually inserting end text tokens before the image in the list.\n3. For better results, train longer with more data and consider using a bigger transformer model.\n4. For multimodal training (text and images), refer to the example in `train_latent_with_text.py` and allow sufficient training time for convergence. (B) 1. For image generation, ensure the training process includes both text and images, and avoid manually inserting end text tokens before the image in the list.\n2. For using pretrained VAE, follow the example provided in `train_mnist_vae.py` and ensure the VAE is frozen during training.\n3. For better results, train longer with more data and consider using a bigger transformer model.\n4. For multimodal training (text and images), refer to the example in `train_latent_with_text.py` and allow sufficient training time for convergence. (C) 1. Update to version 0.3.5 of the repository to resolve the issue with erroneous masking in adaptive modules and text labels.\n2. For image generation, ensure the training process includes both text and images, and avoid manually inserting end text tokens before the image in the list.\n3. For multimodal training (text and images), refer to the example in `train_latent_with_text.py` and allow sufficient training time for convergence.\n4. For using pretrained VAE, follow the example provided in `train_mnist_vae.py` and ensure the VAE is frozen during training.\n5. For better results, train longer with more data and consider using a bigger transformer model. (D) 1. Update to version 0.3.5 of the repository to resolve the issue with erroneous masking in adaptive modules and text labels.\n2. For image generation, ensure the training process includes both text and images, and avoid manually inserting end text tokens before the image in the list.\n3. For using pretrained VAE, follow the example provided in `train_mnist_vae.py` and unfreeze the VAE during training.\n4. For better results, train longer with more data and consider using a bigger transformer model.\n5. For multimodal training (text and images), refer to the example in `train_latent_with_text.py` and allow sufficient training time for convergence. (E) 1. Update to version 0.3.5 of the repository to resolve the issue with erroneous masking in adaptive modules and text labels.\n2. For image generation, ensure the training process includes both text and images, and avoid manually inserting end text tokens before the image in the list.\n3. For using pretrained VAE, follow the example provided in `train_mnist_vae.py` and ensure the VAE is frozen during training.\n4. For better results, train longer with more data and consider using a bigger transformer model.\n5. For multimodal training (text and images), refer to the example in `train_latent_with_text.py` and allow sufficient training time for convergence. (F) 1. For better results, train longer with more data and consider using a bigger transformer model.\n2. Update to version 0.3.5 of the repository to resolve the issue with erroneous masking in adaptive modules and text labels.\n3. For image generation, ensure the training process includes both text and images, and avoid manually inserting end text tokens before the image in the list.\n4. For using pretrained VAE, follow the example provided in `train_mnist_vae.py` and ensure the VAE is frozen during training.\n5. For multimodal training (text and images), refer to the example in `train_latent_with_text.py` and allow sufficient training time for convergence. (G) 1. Update to version 0.3.5 of the repository to resolve the issue with erroneous masking in adaptive modules and text labels.\n2. For image generation, ensure the training process includes both text and images, and manually insert end text tokens before the image in the list.\n3. For using pretrained VAE, follow the example provided in `train_mnist_vae.py` and ensure the VAE is frozen during training.\n4. For better results, train longer with more data and consider using a bigger transformer model.\n5. For multimodal training (text and images), refer to the example in `train_latent_with_text.py` and allow sufficient training time for convergence.", "answer": "E"}
{"uuid": "d92b700b-0634-47e0-b2ee-b0174cd1305a", "setup_instruct": "# SegVol Execution Plan\n\n## 1. Environment Setup\n- **Description**: Install PyTorch and required dependencies.\n- **Commands**:\n  ```bash\n  # Install PyTorch (v1.11.0 or higher)\n  pip install torch==1.11.0\n  # Install key requirements\n  pip install 'monai[all]==0.9.0' einops==0.6.1 transformers==4.18.0 matplotlib\n  ```\n\n## 2. Download Model & Datasets\n- **Description**: Download SegVol model weights and datasets from ModelScope or HuggingFace.\n- **Options**:\n  - **ModelScope**: [SegVol Model](https://www.modelscope.cn/models/yuxindu/SegVol/summary)\n  - **HuggingFace**: [SegVol Model](https://huggingface.co/BAAI/SegVol)\n  - **Datasets**: [ModelScope](https://www.modelscope.cn/datasets/GoodBaiBai88/M3D-Seg/summary) or [HuggingFace](https://huggingface.co/datasets/GoodBaiBai88/M3D-Seg)\n\n## 3. Inference Demo\n- **Description**: Run the inference demo as per the [inference guide](https://github.com/BAAI-DCAI/SegVol/blob/main/documents/inference_demo.md).\n- **Steps**:\n  1. Clone the repository:\n     ```bash\n     git clone https://github.com/BAAI-DCAI/SegVol.git\n     cd SegVol\n     ```\n  2. Follow the inference demo instructions.\n\n## 4. Training (Optional)\n- **Description**: Train SegVol using the [training guide](https://github.com/BAAI-DCAI/SegVol/blob/main/documents/training.md).\n- **Steps**:\n  1. Prepare datasets (refer to the dataset links in the README).\n  2. Follow the training instructions.\n\n## 5. Pre-trained ViT Usage (Optional)\n- **Description**: Use the pre-trained ViT encoder as per the [guide](https://github.com/BAAI-DCAI/SegVol/blob/main/documents/pretrained_vit.md).\n- **Steps**:\n  1. Download ViT weights from [HuggingFace](https://huggingface.co/BAAI/SegVol/tree/main) or [Google Drive](https://drive.google.com/drive/folders/1TEJtgctH534Ko5r4i79usJvqmXVuLf54?usp=drive_link).\n  2. Follow the pre-trained ViT usage instructions.\n\n## 6. Web Tool (Optional)\n- **Description**: Use the interactive [SegVol Web Tool](https://www.modelscope.cn/studios/YuxinDu/SegVol/summary) for segmentation without local setup.", "issue_title": "DICOM FILE", "issue_body": "I encountered a problem when running the demo.... zzzz\r\n \"File is missing DICOM File Meta Information header or the 'DICM' \"\r\nTraceback (most recent call last):\r\n  File \"inference_demo.py\", line 221, in <module>\r\n    main(args)\r\n  File \"inference_demo.py\", line 208, in main\r\n    data_item = process_ct_gt(ct_path, gt_path, categories, args.spatial_size)   # keys: image, label\r\n\r\n    \"File is missing DICOM File Meta Information header or the 'DICM' \"\r\npydicom.errors.InvalidDicomError: File is missing DICOM File Meta Information header or the 'DICM' pre                      \r\n fix is missing from the header. Use force=True to force reading.", "choices": "(A) 1. Check if you are using the latest master branch of the repository.\n2. Follow the updated README example provided by the maintainer. (B) 1. Follow the updated README example provided by the maintainer.\n2. Ensure that the input tensors are correctly formatted: `torch.int` or `torch.long` for text, and `torch.float` for modalities. (C) 1. Check if you are using the latest master branch of the repository.\n2. Follow the updated README example provided by the maintainer.\n3. Mix `torch.int` and `torch.float` tensors for text and modalities.\n4. Ensure that the input tensors are correctly formatted: `torch.int` or `torch.long` for text, and `torch.float` for modalities. (D) 1. Check if you are using the latest master branch of the repository.\n2. Downgrade the repository to an older version for compatibility.\n3. Follow the updated README example provided by the maintainer.\n4. Ensure that the input tensors are correctly formatted: `torch.int` or `torch.long` for text, and `torch.float` for modalities. (E) 1. Follow the updated README example provided by the maintainer.\n2. Check if you are using the latest master branch of the repository.\n3. Ensure that the input tensors are correctly formatted: `torch.int` or `torch.long` for text, and `torch.float` for modalities. (F) 1. Check if you are using the latest master branch of the repository.\n2. Follow the updated README example provided by the maintainer.\n3. Ensure that the input tensors are correctly formatted: `torch.int` or `torch.long` for text, and `torch.float` for modalities. (G) 1. Ensure that the input tensors are correctly formatted: `torch.int` or `torch.long` for text, and `torch.float` for modalities.\n2. Check if you are using the latest master branch of the repository.\n3. Follow the updated README example provided by the maintainer.", "answer": "F"}
{"uuid": "8e04749b-1e78-49a2-928e-a73e06a3adfa", "setup_instruct": "# Transfusion-Pytorch Implementation Plan\n\n## 1. Installation\n- **Description**: Install the transfusion-pytorch package and its dependencies\n- **Command**:\n  ```bash\n  pip install transfusion-pytorch\n  ```\n\n## 2. Basic Setup (Single Modality)\n- **Description**: Set up a basic Transfusion model for single modality (images)\n- **Python Code**:\n  ```python\n  from torch import randint, randn\n  from transfusion_pytorch import Transfusion\n\n  model = Transfusion(\n      num_text_tokens = 256,\n      dim_latent = 384,\n      modality_default_shape = (4,),\n      transformer = dict(\n          dim = 512,\n          depth = 8\n      )\n  )\n  ```\n\n## 3. Training (Single Modality)\n- **Description**: Train the model with mixed text and image data\n- **Python Code**:\n  ```python\n  text_and_images = [\n      [randint(0, 256, (16,)), randn(4, 384), randint(0, 256, (8,)), randn(6, 384)],\n      [randint(0, 256, (16,)), randn(7, 384), randint(0, 256, (5,)), randn(2, 384), randint(0, 256, (9,))]\n  ]\n\n  loss = model(text_and_images)\n  loss.backward()\n  ```\n\n## 4. Multiple Modalities Setup\n- **Description**: Configure the model for multiple different modalities\n- **Python Code**:\n  ```python\n  model = Transfusion(\n      num_text_tokens = 256,\n      dim_latent = (384, 192),\n      modality_default_shape = ((4,), (2,)),\n      transformer = dict(\n          dim = 512,\n          depth = 8\n      )\n  )\n  ```\n\n## 5. Training (Multiple Modalities)\n- **Description**: Train with text, images, and audio data\n- **Python Code**:\n  ```python\n  text_images_and_audio = [\n      [randint(0, 256, (16,)), (0, randn(4, 384)), randint(0, 256, (8,)), (1, randn(6, 192))],\n      [randint(0, 256, (16,)), randn(7, 384), randint(0, 256, (5,)), (1, randn(2, 192)), randint(0, 256, (9,))]\n  ]\n\n  loss = model(text_images_and_audio)\n  loss.backward()\n  ```\n\n## 6. Image Encoding/Decoding Setup\n- **Description**: Configure model with custom image encoders/decoders\n- **Python Code**:\n  ```python\n  import torch\n  from torch import nn\n  from transfusion_pytorch import Transfusion, print_modality_sample\n\n  mock_encoder = nn.Conv2d(3, 384, 3, padding = 1)\n  mock_decoder = nn.Conv2d(384, 3, 3, padding = 1)\n\n  model = Transfusion(\n      num_text_tokens = 12,\n      dim_latent = 384,\n      channel_first_latent = True,\n      modality_default_shape = (4, 4),\n      modality_encoder = mock_encoder,\n      modality_decoder = mock_decoder,\n      transformer = dict(\n          dim = 512,\n          depth = 8\n      )\n  )\n  ```\n\n## 7. Text-Only Pretraining\n- **Description**: Pretrain the model on text-only data\n- **Python Code**:\n  ```python\n  model = Transfusion(\n      num_text_tokens = 256,\n      dim_latent = 384,\n      transformer = dict(\n          dim = 512,\n          depth = 8,\n      )\n  ).cuda()\n\n  text = torch.randint(0, 256, (2, 1024)).cuda()\n  loss = model(text)\n  loss.backward()\n  ```\n\n## 8. Running Examples\n- **Description**: Install additional dependencies and run example scripts\n- **Commands**:\n  ```bash\n  pip install .[examples]\n  pip install -U diffusers transformers accelerate scipy ftfy safetensors\n  ```\n\n## 9. Sampling\n- **Description**: Generate samples after training\n- **Python Code**:\n  ```python\n  # For single modality\n  one_multimodal_sample = model.sample()\n\n  # For image modality with decoder\n  print_modality_sample(one_multimodal_sample)\n\n  # For text-only generation\n  sampled = model.generate_text_only(text[:, :1], 1024)\n  ```", "issue_title": "Image Generation", "issue_body": "Hi\r\n\r\nUsing this code, I could sample text successfully ( model.sample()) but the image part is empty. Do you know how to sample (or generate) an image for the inference? It would be helpful if you could give me any pointers for this. Thank you!", "choices": "(A) 1. Ensure `coverage` is added back to the dev dependencies.\n2. Remove `coverage` from the dev dependencies.\n3. Use the `latest` image tag and point to the `main` branch in the configuration.\n4. Verify that the CI passes after making these changes. (B) 1. Ensure `coverage` is added back to the dev dependencies.\n2. Verify that the CI passes after making these changes.\n3. Use the `latest` image tag and point to the `main` branch in the configuration. (C) 1. Ensure `coverage` is added back to the dev dependencies.\n2. Use the `latest` image tag and point to the `main` branch in the configuration. (D) 1. Use the `latest` image tag and point to the `main` branch in the configuration.\n2. Ensure `coverage` is added back to the dev dependencies.\n3. Verify that the CI passes after making these changes. (E) 1. Use the `latest` image tag and point to the `main` branch in the configuration.\n2. Verify that the CI passes after making these changes. (F) 1. Ensure `coverage` is added back to the dev dependencies.\n2. Use the `latest` image tag and point to the `main` branch in the configuration.\n3. Verify that the CI passes after making these changes. (G) 1. Ensure `coverage` is added back to the dev dependencies.\n2. Use the `outdated` image tag and point to the `main` branch in the configuration.\n3. Verify that the CI passes after making these changes.", "answer": "F"}
{"uuid": "27b76ca0-bee1-4853-ba09-63b3be37cc38", "setup_instruct": "# Transfusion-PyTorch Execution Plan\n\n## 1. Installation\n- **Description**: Install the `transfusion-pytorch` package and optional dependencies for examples.\n- **Command**:\n  ```bash\n  pip install transfusion-pytorch\n  ```\n\n## 2. Setup for Examples (Optional)\n- **Description**: Install additional dependencies required to run example scripts.\n- **Commands**:\n  ```bash\n  pip install .[examples]\n  pip install -U diffusers transformers accelerate scipy ftfy safetensors\n  ```\n\n## 3. Basic Usage (Single Modality)\n- **Description**: Initialize the model for single-modality (e.g., images) and train with mixed text/image data.\n- **Python Code**:\n  ```python\n  from torch import randint, randn\n  from transfusion_pytorch import Transfusion\n\n  model = Transfusion(\n      num_text_tokens=256,\n      dim_latent=384,\n      modality_default_shape=(4,),\n      transformer=dict(dim=512, depth=8)\n  )\n\n  text_and_images = [\n      [randint(0, 256, (16,)), randn(4, 384), randint(0, 256, (8,)), randn(6, 384)],\n      [randint(0, 256, (16,)), randn(7, 384), randint(0, 256, (5,)), randn(2, 384), randint(0, 256, (9,))]\n  ]\n\n  loss = model(text_and_images)\n  loss.backward()\n  ```\n\n## 4. Multiple Modalities\n- **Description**: Extend to multiple modalities (e.g., images + audio) by specifying latent dimensions and shapes.\n- **Python Code**:\n  ```python\n  model = Transfusion(\n      num_text_tokens=256,\n      dim_latent=(384, 192),\n      modality_default_shape=((4,), (2,)),\n      transformer=dict(dim=512, depth=8)\n  )\n\n  text_images_and_audio = [\n      [randint(0, 256, (16,)), (0, randn(4, 384)), randint(0, 256, (8,)), (1, randn(6, 192))],\n      [randint(0, 256, (16,)), randn(7, 384), randint(0, 256, (5,)), (1, randn(2, 192)), randint(0, 256, (9,))]\n  ]\n\n  loss = model(text_images_and_audio)\n  loss.backward()\n  ```\n\n## 5. Image Encoding/Decoding\n- **Description**: Use custom encoders/decoders for image data with channel-first latent representations.\n- **Python Code**:\n  ```python\n  mock_encoder = nn.Conv2d(3, 384, 3, padding=1)\n  mock_decoder = nn.Conv2d(384, 3, 3, padding=1)\n\n  model = Transfusion(\n      num_text_tokens=12,\n      dim_latent=384,\n      channel_first_latent=True,\n      modality_default_shape=(4, 4),\n      modality_encoder=mock_encoder,\n      modality_decoder=mock_decoder,\n      transformer=dict(dim=512, depth=8)\n  )\n\n  text_and_images = [\n      [randint(0, 12, (16,)), randn(3, 8, 8), randint(0, 12, (8,)), randn(3, 7, 7)],\n      [randint(0, 12, (16,)), randn(3, 8, 5), randint(0, 12, (5,)), randn(3, 2, 16), randint(0, 12, (9,))]\n  ]\n\n  loss = model(text_and_images)\n  loss.backward()\n  ```\n\n## 6. Text-Only Pretraining\n- **Description**: Pretrain the model on text-only data before multimodal training.\n- **Python Code**:\n  ```python\n  model = Transfusion(\n      num_text_tokens=256,\n      dim_latent=384,\n      transformer=dict(dim=512, depth=8)\n  ).cuda()\n\n  text = torch.randint(0, 256, (2, 1024)).cuda()\n  loss = model(text)\n  loss.backward()\n  ```\n\n## 7. Sampling\n- **Description**: Generate multimodal or text-only samples after training.\n- **Python Code**:\n  ```python\n  # Multimodal sampling\n  one_multimodal_sample = model.sample()\n\n  # Text-only generation\n  sampled_text = model.generate_text_only(text[:, :1], 1024)\n  ```\n\n## 8. Running Examples\n- **Description**: Execute example training scripts (ensure dependencies are installed).\n- **Command** (replace `{example_name}`):\n  ```bash\n  python train_{example_name}.py\n  ```", "issue_title": "Got error when running example", "issue_body": "![Screenshot 2024-09-13 at 10 01 32](https://github.com/user-attachments/assets/ec664986-5f34-43f0-b584-1e789ee8b553)", "choices": "(A) 1. Ensure the model is trained with DDP enabled to avoid missing keys during simulation.  \n2. Enable DDP (Distributed Data Parallel) during model training.  \n3. After enabling DDP, retrain the model and use the new model files (args.json and latest.pth) for simulation. (B) 1. After enabling DDP, retrain the model and use the new model files (args.json and latest.pth) for simulation.  \n2. Enable DDP (Distributed Data Parallel) during model training.  \n3. Ensure the model is trained with DDP enabled to avoid missing keys during simulation. (C) 1. Enable DDP (Distributed Data Parallel) during model training.  \n2. Use the old model files (args.json and latest.pth) for simulation.  \n3. Ensure the model is trained with DDP enabled to avoid missing keys during simulation.  \n4. After enabling DDP, retrain the model and use the new model files (args.json and latest.pth) for simulation. (D) 1. Ensure the model is trained with DDP enabled to avoid missing keys during simulation.  \n2. After enabling DDP, retrain the model and use the new model files (args.json and latest.pth) for simulation. (E) 1. Enable DDP (Distributed Data Parallel) during model training.  \n2. Disable DDP to reduce computational overhead.  \n3. Ensure the model is trained with DDP enabled to avoid missing keys during simulation.  \n4. After enabling DDP, retrain the model and use the new model files (args.json and latest.pth) for simulation. (F) 1. Enable DDP (Distributed Data Parallel) during model training. 2. Ensure the model is trained with DDP enabled to avoid missing keys during simulation. 3. After enabling DDP, retrain the model and use the new model files (args.json and latest.pth) for simulation. (G) 1. Enable DDP (Distributed Data Parallel) during model training.  \n2. After enabling DDP, retrain the model and use the new model files (args.json and latest.pth) for simulation.", "answer": "F"}
{"uuid": "452ad4d1-ee07-44fb-81ad-b56ea54dde58", "setup_instruct": "# UM2N Project Execution Plan\n\n## 1. Installation\n### Option A: All-in-one installation\n```shell\n# Navigate to project root and run install script (CPU version)\n./install.sh\n\n# For GPU support (specify CUDA version, e.g., 11.8)\n./install_gpu.sh 118\n```\n\n### Option B: Manual step-by-step installation\n1. **Install Firedrake**  \n   Follow official instructions at [firedrakeproject.org](https://www.firedrakeproject.org/download.html)\n\n2. **Activate Firedrake virtual environment**  \n   ```shell\n   source /path/to/firedrake/bin/activate\n   ```\n\n3. **Install Movement package**  \n   Follow instructions at [mesh-adaptation-docs](https://github.com/mesh-adaptation/mesh-adaptation-docs/wiki/Installation-Instructions)\n\n4. **Install PyTorch**  \n   Follow platform-specific instructions from [pytorch.org](https://pytorch.org/get-started/locally)\n\n5. **Install PyTorch3D**  \n   ```shell\n   python3 -m pip install \"git+https://github.com/facebookresearch/pytorch3d\"\n   ```\n\n6. **Install UM2N package**  \n   ```shell\n   pip install .\n   ```\n\n## 2. Dataset Setup\n### Option A: Use pre-generated dataset\n1. Download from [Google Drive](https://drive.google.com/drive/folders/1sQ-9zWbTryCXwihqaqazrQ4Vp1MRdBPK)\n2. Place downloaded `helmholtz` folder in `data/dataset/`\n\n### Option B: Generate dataset manually\n```shell\n# Run dataset generation script (modify parameters in script as needed)\n. script/make_dataset.sh\n```\n\n## 3. Model Training\n1. Access training notebook: `script/train_um2n.ipynb`\n2. Alternative: Download pre-trained models from [Google Drive](https://drive.google.com/drive/folders/1P_JMpU1qmLdmbGTz8fL5VO-lEBoP3_2n)\n\n## 4. Model Evaluation\n1. Use visualization scripts in `script/` folder\n2. Note: Update dataset/model paths in scripts before execution\n\n## 5. Documentation\n```shell\n# Navigate to docs directory and build documentation\ncd docs/\nmake html\n```\n\n## 6. Project Structure Overview\nKey directories:\n- `UM2N/`: Core implementation\n- `data/`: Dataset storage\n- `docs/`: Documentation\n- `script/`: Utility scripts\n- `test/`: Test cases", "issue_title": "Add coverage to deps", "issue_body": "The UM2N regular CI is failing\r\nhttps://github.com/mesh-adaptation/UM2N/actions/runs/13743708007/job/38436017628\r\n\r\nLooks like we're missing some dependencies.", "choices": "(A) 1. Change the model type from \"score\" to \"x_start\" to resolve the issue of training loss remaining high (8-9).\n2. Verify the dataset size and preprocessing steps to ensure sufficient data is being used for training.\n3. Ensure the dimensions in the normalization.json file match the expected tensor sizes to avoid RuntimeError (e.g., modify the \"mean\" and \"std\" shapes if necessary). (B) 1. Change the model type from \"score\" to \"x_start\" to resolve the issue of training loss remaining high (8-9).\n2. Ensure the dimensions in the normalization.json file match the expected tensor sizes to avoid RuntimeError (e.g., modify the \"mean\" and \"std\" shapes if necessary).\n3. Verify the dataset size and preprocessing steps to ensure sufficient data is being used for training. (C) 1. Change the model type from \"score\" to \"x_start\" to resolve the issue of training loss remaining high (8-9).\n2. Verify the dataset size and preprocessing steps to ensure sufficient data is being used for training. (D) 1. Ensure the dimensions in the normalization.json file match the expected tensor sizes to avoid RuntimeError (e.g., modify the \"mean\" and \"std\" shapes if necessary).\n2. Change the model type from \"score\" to \"x_start\" to resolve the issue of training loss remaining high (8-9).\n3. Verify the dataset size and preprocessing steps to ensure sufficient data is being used for training. (E) 1. Change the model type from \"score\" to \"x_start\" to resolve the issue of training loss remaining high (8-9).\n2. Manually set the training loss to a low value (e.g., 0.1) in the training script to bypass the issue.\n3. Ensure the dimensions in the normalization.json file match the expected tensor sizes to avoid RuntimeError (e.g., modify the \"mean\" and \"std\" shapes if necessary).\n4. Verify the dataset size and preprocessing steps to ensure sufficient data is being used for training. (F) 1. Ensure the dimensions in the normalization.json file match the expected tensor sizes to avoid RuntimeError (e.g., modify the \"mean\" and \"std\" shapes if necessary).\n2. Verify the dataset size and preprocessing steps to ensure sufficient data is being used for training. (G) 1. Change the model type from \"score\" to \"x_start\" to resolve the issue of training loss remaining high (8-9).\n2. Disable normalization in the preprocessing steps to simplify the pipeline.\n3. Ensure the dimensions in the normalization.json file match the expected tensor sizes to avoid RuntimeError (e.g., modify the \"mean\" and \"std\" shapes if necessary).\n4. Verify the dataset size and preprocessing steps to ensure sufficient data is being used for training.", "answer": "B"}
{"uuid": "4234601b-e455-4938-9ca6-31eeb85f16fb", "setup_instruct": "# UM2N Project Execution Plan\n\n## 1. Installation\n### Option A: All-in-one Installation (CPU)\n- **Description**: Installs Firedrake, Movement, and UM2N with CPU-only PyTorch.\n- **Command**:\n  ```shell\n  ./install.sh\n  ```\n\n### Option B: GPU Installation\n- **Description**: Installs with CUDA support. Specify your CUDA version (e.g., `11.8`).\n- **Command**:\n  ```shell\n  ./install_gpu.sh 118\n  ```\n\n### Option C: Manual Installation\n1. **Install Firedrake**  \n   - **Description**: Follow official instructions to install Firedrake.\n   - **Command**:  \n     Visit [firedrakeproject.org](https://www.firedrakeproject.org/download.html).\n\n2. **Activate Firedrake Virtual Environment**  \n   - **Description**: Activate the Firedrake environment.\n   - **Command**:\n     ```shell\n     source /path/to/firedrake/bin/activate\n     ```\n\n3. **Install Movement**  \n   - **Description**: Install the mesh movement library.\n   - **Command**:  \n     Follow [Movement installation guide](https://github.com/mesh-adaptation/mesh-adaptation-docs/wiki/Installation-Instructions).\n\n4. **Install PyTorch**  \n   - **Description**: Install PyTorch within the Firedrake environment.\n   - **Command**:  \n     Visit [pytorch.org](https://pytorch.org/get-started/locally) for OS-specific instructions.\n\n5. **Install PyTorch3D**  \n   - **Description**: Install PyTorch3D from source.\n   - **Command**:\n     ```shell\n     python3 -m pip install \"git+https://github.com/facebookresearch/pytorch3d\"\n     ```\n\n6. **Install UM2N**  \n   - **Description**: Install the project and dependencies.\n   - **Command**:\n     ```shell\n     pip install .\n     ```\n\n---\n\n## 2. Dataset Setup\n### Option A: Use Pre-Generated Dataset\n- **Description**: Download datasets from Google Drive.\n- **Steps**:\n  1. Download from [this link](https://drive.google.com/drive/folders/1sQ-9zWbTryCXwihqaqazrQ4Vp1MRdBPK).\n  2. Place the `helmholtz` folder under `data/dataset/`.\n\n### Option B: Generate Dataset Locally\n- **Description**: Generate datasets for Burgers, Helmholtz, and Poisson equations.\n- **Command**:\n  ```shell\n  . script/make_dataset.sh\n  ```\n- **Customization**: Modify `n_dist_start`, `n_dist_end`, `n_grid_start`, `n_grid_end` in `make_dataset.sh` for dataset size.\n\n---\n\n## 3. Training (Outdated)\n- **Description**: Use the provided notebook or pre-trained models.\n- **Steps**:\n  1. Open `script/train_um2n.ipynb` for training.\n  2. Download pre-trained models from [Google Drive](https://drive.google.com/drive/folders/1P_JMpU1qmLdmbGTz8fL5VO-lEBoP3_2n).\n\n---\n\n## 4. Evaluation (Outdated)\n- **Description**: Use scripts in `script/` for visualization and evaluation.\n- **Note**: Update paths in scripts to point to your datasets/models.\n\n---\n\n## 5. Documentation\n- **Description**: Build project documentation via Sphinx.\n- **Command** (from `docs/` folder):\n  ```shell\n  make html\n  ```", "issue_title": "Transfer ownership of repo to mesh-adaptation organisation?", "issue_body": "Offer to take UM2N under the `mesh-adaptation` organisation for development and maintenance alongside the other codes.\r\n\r\nIf you think this sounds like a good idea I can process the transfer for you @erizmr. If not then just say so and we can close this issue.", "choices": "(A) 1. Remove the unused `ego_agent_past` data and extra dimensions in `ego_current` as they are not utilized in practice. 2. Ensure the dimensions in the `normalization.json` file match the data being processed. (B) 1. Update the `normalization.json` file to align with the current dataprocess script. 2. Ensure the dimensions in the `normalization.json` file match the data being processed. (C) 1. Remove the unused `ego_agent_past` data and extra dimensions in `ego_current` as they are not utilized in practice. 2. Update the `normalization.json` file to align with the current dataprocess script. 3. Ensure the dimensions in the `normalization.json` file match the data being processed. (D) 1. Delete the `normalization.json` file. 2. Update the `normalization.json` file to align with the current dataprocess script. 3. Remove the unused `ego_agent_past` data and extra dimensions in `ego_current` as they are not utilized in practice. 4. Ensure the dimensions in the `normalization.json` file match the data being processed. (E) 1. Update the `normalization.json` file to align with the current dataprocess script. 2. Remove the unused `ego_agent_past` data and extra dimensions in `ego_current` as they are not utilized in practice. 3. Ensure the dimensions in the `normalization.json` file match the data being processed. (F) 1. Ensure the dimensions in the `normalization.json` file match the data being processed. 2. Update the `normalization.json` file to align with the current dataprocess script. 3. Remove the unused `ego_agent_past` data and extra dimensions in `ego_current` as they are not utilized in practice. (G) 1. Update the `normalization.json` file to align with the current dataprocess script. 2. Add irrelevant dimensions to `ego_current`. 3. Remove the unused `ego_agent_past` data and extra dimensions in `ego_current` as they are not utilized in practice. 4. Ensure the dimensions in the `normalization.json` file match the data being processed.", "answer": "E"}
{"uuid": "ec885540-313b-4773-a3cc-7931b7cf9089", "setup_instruct": "# Step-by-Step Execution Plan for Diffusion Planner\n\n## 1. Environment Setup\n- **Description**: Set up the conda environment and install dependencies.\n- **Commands**:\n  ```bash\n  conda create -n diffusion_planner python=3.9\n  conda activate diffusion_planner\n  git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit\n  pip install -e .\n  pip install -r requirements.txt\n  cd ..\n  git clone https://github.com/ZhengYinan-AIR/Diffusion-Planner.git && cd Diffusion-Planner\n  pip install -e .\n  pip install -r requirements_torch.txt\n  ```\n\n## 2. Download Model Checkpoint\n- **Description**: Download the pre-trained model checkpoint from Huggingface.\n- **Commands**:\n  ```bash\n  mkdir -p checkpoints\n  wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/args.json\n  wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/model.pth\n  ```\n\n## 3. Closed-Loop Evaluation\n- **Description**: Run the simulation for closed-loop evaluation.\n- **Steps**:\n  1. Configure `sim_diffusion_planner_runner.sh`.\n  2. Execute the simulation.\n- **Commands**:\n  ```bash\n  bash sim_diffusion_planner_runner.sh\n  ```\n\n## 4. Visualization\n- **Description**: Visualize the simulation results.\n- **Steps**:\n  1. Configure `run_nuboard.ipynb`.\n  2. Launch Jupyter Notebook/Lab to execute it.\n- **Commands**:\n  ```bash\n  jupyter notebook run_nuboard.ipynb\n  ```\n\n## 5. Classifier Guidance Demo\n- **Description**: Run the classifier guidance demo.\n- **Steps**:\n  1. Configure `sim_diffusion_planner_runner.sh`.\n  2. Execute the demo.\n- **Commands**:\n  ```bash\n  bash sim_guidance_demo.sh\n  ```\n\n## 6. Training Data Preprocessing\n- **Description**: Preprocess the training data.\n- **Commands**:\n  ```bash\n  chmod +x data_process.sh\n  ./data_process.sh\n  ```\n\n## 7. Training\n- **Description**: Train the Diffusion Planner model.\n- **Commands**:\n  ```bash\n  chmod +x torch_run.sh\n  ./torch_run.sh\n  ```", "issue_title": "dimension error from normalization.json", "issue_body": "In the \"normalization.json\", the dimensions do not match the data used. \n\nThe \"mean\" dimension of \"ego_agent_past\" is 14. However, the data[\"ego_agent_past\"] is [2, 21, 8]. \n\nHope you can check it.\n\nThanks\n\n![Image](https://github.com/user-attachments/assets/ab8301ab-1af3-4171-b9b7-5f0dc9338754)\n\n![Image](https://github.com/user-attachments/assets/41440b51-4a2c-441f-8362-f0f064eea233)", "choices": "(A) 1. Open the config file `ESAM-E_online_stream.py`.\n2. Locate the `inst_score_thr` parameter.\n3. Adjust the value of `inst_score_thr` to a lower threshold to prevent masks from being filtered out. (B) 1. Open the config file `ESAM-E_online_stream.py`.\n2. Locate the `inst_score_thr` parameter.\n3. Adjust the value of `inst_score_thr` to a lower threshold to prevent masks from being filtered out.\n4. Save the changes and rerun the script.\n5. Delete the config file `ESAM-E_online_stream.py`. (C) 1. Open the config file `ESAM-E_online_stream.py`.\n2. Adjust the value of `inst_score_thr` to a lower threshold to prevent masks from being filtered out.\n3. Locate the `inst_score_thr` parameter.\n4. Save the changes and rerun the script. (D) 1. Open the config file `ESAM-E_online_stream.py`.\n2. Locate the `inst_score_thr` parameter.\n3. Save the changes and rerun the script. (E) 1. Open the config file `ESAM-E_online_stream.py`.\n2. Locate the `inst_score_thr` parameter.\n3. Adjust the value of `inst_score_thr` to a lower threshold to prevent masks from being filtered out.\n4. Save the changes and rerun the script. (F) 1. Open the config file `ESAM-E_online_stream.py`.\n2. Locate the `inst_score_thr` parameter.\n3. Save the changes and rerun the script.\n4. Adjust the value of `inst_score_thr` to a lower threshold to prevent masks from being filtered out. (G) 1. Open the config file `ESAM-E_online_stream.py`.\n2. Locate the `inst_score_thr` parameter.\n3. Adjust the value of `inst_score_thr` to a higher threshold to filter out more masks.\n4. Save the changes and rerun the script.", "answer": "E"}
{"uuid": "84bebb1e-4b82-4cce-9947-3a5999be4eff", "setup_instruct": "# Step-by-Step Execution Plan for Diffusion Planner\n\n## 1. Environment Setup\n- **Description**: Set up the conda environment and install dependencies.\n- **Commands**:\n  ```bash\n  conda create -n diffusion_planner python=3.9\n  conda activate diffusion_planner\n  git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit\n  pip install -e .\n  pip install -r requirements.txt\n  cd ..\n  git clone https://github.com/ZhengYinan-AIR/Diffusion-Planner.git && cd Diffusion-Planner\n  pip install -e .\n  pip install -r requirements_torch.txt\n  ```\n\n## 2. Dataset Setup\n- **Description**: Follow the nuPlan dataset setup instructions.\n- **Commands**: Refer to the [official documentation](https://nuplan-devkit.readthedocs.io/en/latest/dataset_setup.html).\n\n## 3. Download Model Checkpoint\n- **Description**: Download the pre-trained model checkpoint from HuggingFace.\n- **Commands**:\n  ```bash\n  mkdir -p checkpoints\n  wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/args.json\n  wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/model.pth\n  ```\n\n## 4. Closed-Loop Evaluation\n- **Description**: Run the simulation for closed-loop evaluation.\n- **Commands**:\n  ```bash\n  # Edit sim_diffusion_planner_runner.sh with your configuration\n  bash sim_diffusion_planner_runner.sh\n  ```\n\n## 5. Visualization\n- **Description**: Visualize the simulation results using Jupyter Notebook.\n- **Commands**:\n  ```bash\n  # Edit run_nuboard.ipynb with your configuration\n  jupyter notebook run_nuboard.ipynb\n  ```\n\n## 6. Classifier Guidance Demo\n- **Description**: Run the classifier guidance demo.\n- **Commands**:\n  ```bash\n  # Edit sim_guidance_demo.sh with your configuration\n  bash sim_guidance_demo.sh\n  ```\n\n## 7. Data Preprocessing\n- **Description**: Preprocess the training data.\n- **Commands**:\n  ```bash\n  chmod +x data_process.sh\n  ./data_process.sh\n  ```\n\n## 8. Training\n- **Description**: Train the Diffusion Planner model.\n- **Commands**:\n  ```bash\n  chmod +x torch_run.sh\n  ./torch_run.sh\n  ```\n\n## 9. Citation\n- **Description**: Cite the paper if you use this work.\n- **Bibtex**:\n  ```bibtex\n  @inproceedings{\n    zheng2025diffusionbased,\n    title={Diffusion-Based Planning for Autonomous Driving with Flexible Guidance},\n    author={Yinan Zheng and Ruiming Liang and Kexin ZHENG and Jinliang Zheng and Liyuan Mao and Jianxiong Li and Weihao Gu and Rui Ai and Shengbo Eben Li and Xianyuan Zhan and Jingjing Liu},\n    booktitle={The Thirteenth International Conference on Learning Representations},\n    year={2025},\n    url={https://openreview.net/forum?id=wM2sfVgMDH}\n  }\n  ```", "issue_title": "ABOUT Training scripts", "issue_body": "Thanks for your great work！\n\nin the training scripts torch_run.sh\n\nIf I use the nuplan data set, how could I\nSet the following path?\nTRAIN_SET_PATH=?\nTRAIN_SET_LIST_PATH=?\n\nI try to use the original *.db file path : **/dataset/nuplan -v1.1/splits/trainval as TRAIN_SET_PATH please check the picture attached below \n\nAnd I set the TRAIN_SET_LIST_PATH=”**/normalization.json\"\nwhen I use nuplan dataset. I wonder if it's right?\n\n![Image](https://github.com/user-attachments/assets/2c071718-b144-4d38-9608-9391b402149f)\n\nBig Thanks ahead for your help!", "choices": "(A) 1. Ensure you are using the correct demo script (`stream_demo.py`) as referenced in the documentation.\n2. In the file `ESAM/vis_demo/utils/vis_utils.py`, locate the visualization code that clears geometries during online visualization.\n4. Run the demo again to see incremental updates to the segmentation results, building up the full scene reconstruction over time.\n3. Comment out the following lines to prevent clearing geometries between frames:\n   ```python\n   # if self.online_vis:\n   #     self.vis.clear_geometries()\n   ``` (B) 1. Ensure you are using the correct demo script (`stream_demo.py`) as referenced in the documentation.\n2. In the file `ESAM/vis_demo/utils/vis_utils.py`, locate the visualization code that clears geometries during online visualization.\n3. Run the demo again to see incremental updates to the segmentation results, building up the full scene reconstruction over time. (C) To implement temporal fusion in the visualization demo and achieve complete room reconstruction with consistent instance IDs across frames, follow these steps:\n1. Ensure you are using the correct demo script (`stream_demo.py`) as referenced in the documentation.\n2. In the file `ESAM/vis_demo/utils/vis_utils.py`, locate the visualization code that clears geometries during online visualization.\n3. Comment out the following lines to prevent clearing geometries between frames:\n   ```python\n   # if self.online_vis:\n   #     self.vis.clear_geometries()\n   ```\n4. Run the demo again to see incremental updates to the segmentation results, building up the full scene reconstruction over time. (D) 2. In the file `ESAM/vis_demo/utils/vis_utils.py`, locate the visualization code that clears geometries during online visualization.\n1. Ensure you are using the correct demo script (`stream_demo.py`) as referenced in the documentation.\n3. Comment out the following lines to prevent clearing geometries between frames:\n   ```python\n   # if self.online_vis:\n   #     self.vis.clear_geometries()\n   ```\n4. Run the demo again to see incremental updates to the segmentation results, building up the full scene reconstruction over time. (E) 2. In the file `ESAM/vis_demo/utils/vis_utils.py`, locate the visualization code that clears geometries during online visualization.\n3. Comment out the following lines to prevent clearing geometries between frames:\n   ```python\n   # if self.online_vis:\n   #     self.vis.clear_geometries()\n   ```\n4. Run the demo again to see incremental updates to the segmentation results, building up the full scene reconstruction over time. (F) 1. Ensure you are using the correct demo script (`stream_demo.py`) as referenced in the documentation.\n2. In the file `ESAM/vis_demo/utils/vis_utils.py`, locate the visualization code that clears geometries during online visualization.\n3. Uncomment the following lines to ensure geometries are cleared between frames:\n   ```python\n   if self.online_vis:\n       self.vis.clear_geometries()\n   ```\n4. Run the demo again to see incremental updates to the segmentation results, building up the full scene reconstruction over time.", "answer": "C"}
{"uuid": "1539e0f3-1925-4ee0-a26a-c2d6d2220665", "setup_instruct": "# Step-by-Step Execution Plan for Diffusion Planner\n\n## 1. Environment Setup\n- **Description**: Set up the conda environment and install dependencies.\n- **Commands**:\n  ```bash\n  conda create -n diffusion_planner python=3.9\n  conda activate diffusion_planner\n  git clone https://github.com/motional/nuplan-devkit.git && cd nuplan-devkit\n  pip install -e .\n  pip install -r requirements.txt\n  cd ..\n  git clone https://github.com/ZhengYinan-AIR/Diffusion-Planner.git && cd Diffusion-Planner\n  pip install -e .\n  pip install -r requirements_torch.txt\n  ```\n\n## 2. Download Model Checkpoint\n- **Description**: Download the pre-trained model checkpoint from Huggingface.\n- **Commands**:\n  ```bash\n  mkdir -p checkpoints\n  wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/args.json\n  wget -P ./checkpoints https://huggingface.co/ZhengYinan2001/Diffusion-Planner/resolve/main/model.pth\n  ```\n\n## 3. Closed-Loop Evaluation\n- **Description**: Run the simulation for closed-loop evaluation.\n- **Steps**:\n  1. Configure `sim_diffusion_planner_runner.sh`.\n  2. Execute the simulation:\n     ```bash\n     bash sim_diffusion_planner_runner.sh\n     ```\n  3. Visualize results:\n     - Configure `run_nuboard.ipynb`.\n     - Launch Jupyter Notebook/Lab to execute `run_nuboard.ipynb`.\n\n## 4. Classifier Guidance Demo\n- **Description**: Run the classifier guidance demo.\n- **Steps**:\n  1. Configure `sim_diffusion_planner_runner.sh`.\n  2. Execute the demo:\n     ```bash\n     bash sim_guidance_demo.sh\n     ```\n  3. Refer to [Classifier Guidance Doc](diffusion_planner/model/guidance/documentation_guidance.md) for details.\n\n## 5. Training Preparation\n- **Description**: Preprocess training data.\n- **Commands**:\n  ```bash\n  chmod +x data_process.sh\n  ./data_process.sh\n  ```\n\n## 6. Training\n- **Description**: Train the model.\n- **Commands**:\n  ```bash\n  chmod +x torch_run.sh\n  ./torch_run.sh\n  ```\n\n## 7. Citation (Optional)\n- **Description**: Cite the paper if using the code.\n- **Bibtex**:\n  ```bibtex\n  @inproceedings{\n    zheng2025diffusionbased,\n    title={Diffusion-Based Planning for Autonomous Driving with Flexible Guidance},\n    author={Yinan Zheng and Ruiming Liang and Kexin ZHENG and Jinliang Zheng and Liyuan Mao and Jianxiong Li and Weihao Gu and Rui Ai and Shengbo Eben Li and Xianyuan Zhan and Jingjing Liu},\n    booktitle={The Thirteenth International Conference on Learning Representations},\n    year={2025},\n    url={https://openreview.net/forum?id=wM2sfVgMDH}\n  }\n  ```", "issue_title": "Energy Function Guidance", "issue_body": "Hello! Thanks for your great work. Here is my question. Energy function guidance by posterior sampling is mentioned in your work on 4.3 section. But 'uncond' is set when apply dpm-sampler with out any classifier setting in decoder.py. Do you have any plan to share the part of code in this repo. in future? Thanks~", "choices": "(A) 1. Download only the necessary 3D data files (e.g., scene0000_01.aggregation.json, scene0000_01.txt, scene0000_01_vh_clean_2.0.010000.segs.json, scene0000_01_vh_clean_2.labels.ply, and scene0000_01_vh_clean_2.ply) using the official ScanNet Group's data downloading script.\n2. Set the PYTHONPATH environment variable to ensure proper module imports.\n3. Delete the __pycache__ files under the Oneformer3d folder to resolve the ImportError related to custom modules.\n4. Install Ultralytics version 8.0.100 by running `pip install Ultralytics==8.0.100` to resolve the ImportError related to 'is_git_dir'. (B) 1. Download only the necessary 3D data files (e.g., scene0000_01.aggregation.json, scene0000_01.txt, scene0000_01_vh_clean_2.0.010000.segs.json, scene0000_01_vh_clean_2.labels.ply, and scene0000_01_vh_clean_2.ply) using the official ScanNet Group's data downloading script.\n2. Set the PYTHONPATH environment variable to ensure proper module imports.\n3. Install Ultralytics version 8.0.100 by running `pip install Ultralytics==8.0.100` to resolve the ImportError related to 'is_git_dir'. (C) 1. Download only the necessary 3D data files (e.g., scene0000_01.aggregation.json, scene0000_01.txt, scene0000_01_vh_clean_2.0.010000.segs.json, scene0000_01_vh_clean_2.labels.ply, and scene0000_01_vh_clean_2.ply) using the official ScanNet Group's data downloading script.\n2. Delete the __pycache__ files under the Oneformer3d folder to resolve the ImportError related to custom modules.\n3. Install Ultralytics version 8.0.100 by running `pip install Ultralytics==8.0.100` to resolve the ImportError related to 'is_git_dir'. (D) 1. Download only the necessary 3D data files (e.g., scene0000_01.aggregation.json, scene0000_01.txt, scene0000_01_vh_clean_2.0.010000.segs.json, scene0000_01_vh_clean_2.labels.ply, and scene0000_01_vh_clean_2.ply) using the official ScanNet Group's data downloading script.\n2. Delete all .json files in the working directory.\n3. Delete the __pycache__ files under the Oneformer3d folder to resolve the ImportError related to custom modules.\n4. Set the PYTHONPATH environment variable to ensure proper module imports.\n5. Install Ultralytics version 8.0.100 by running `pip install Ultralytics==8.0.100` to resolve the ImportError related to 'is_git_dir'. (E) 1. Download only the necessary 3D data files (e.g., scene0000_01.aggregation.json, scene0000_01.txt, scene0000_01_vh_clean_2.0.010000.segs.json, scene0000_01_vh_clean_2.labels.ply, and scene0000_01_vh_clean_2.ply) using the official ScanNet Group's data downloading script.\n2. Delete the __pycache__ files under the Oneformer3d folder to resolve the ImportError related to custom modules.\n3. Set the PYTHONPATH environment variable to ensure proper module imports.\n4. Uninstall Ultralytics by running `pip uninstall Ultralytics`.\n5. Install Ultralytics version 8.0.100 by running `pip install Ultralytics==8.0.100` to resolve the ImportError related to 'is_git_dir'. (F) 1. Install Ultralytics version 8.0.100 by running `pip install Ultralytics==8.0.100` to resolve the ImportError related to 'is_git_dir'.\n2. Download only the necessary 3D data files (e.g., scene0000_01.aggregation.json, scene0000_01.txt, scene0000_01_vh_clean_2.0.010000.segs.json, scene0000_01_vh_clean_2.labels.ply, and scene0000_01_vh_clean_2.ply) using the official ScanNet Group's data downloading script.\n3. Delete the __pycache__ files under the Oneformer3d folder to resolve the ImportError related to custom modules.\n4. Set the PYTHONPATH environment variable to ensure proper module imports. (G) 1. Download only the necessary 3D data files (e.g., scene0000_01.aggregation.json, scene0000_01.txt, scene0000_01_vh_clean_2.0.010000.segs.json, scene0000_01_vh_clean_2.labels.ply, and scene0000_01_vh_clean_2.ply) using the official ScanNet Group's data downloading script.\n2. Delete the __pycache__ files under the Oneformer3d folder to resolve the ImportError related to custom modules.\n3. Set the PYTHONPATH environment variable to ensure proper module imports.\n4. Install Ultralytics version 8.0.100 by running `pip install Ultralytics==8.0.100` to resolve the ImportError related to 'is_git_dir'.", "answer": "G"}
{"uuid": "643df4e7-3915-4411-bf63-621b8c338e97", "setup_instruct": "# EmbodiedSAM Execution Plan\n\n## 1. Environment Setup\n- **Objective**: Install dependencies and configure the environment.\n- **Steps**:\n  1. Follow the instructions in [Installation](./docs/installation.md).\n  2. Verify CUDA/cuDNN compatibility if using GPU acceleration.\n  3. Create a Python virtual environment (recommended).\n  4. Install required packages via `pip` or `conda` as specified.\n\n## 2. Dataset Preparation\n- **Objective**: Download and preprocess datasets for training/evaluation.\n- **Steps**:\n  1. Follow the instructions in [Dataset Preparation](./docs/dataset_preparation.md).\n  2. Download ScanNet, SceneNN, or 3RScan datasets if needed.\n  3. Preprocess data using provided scripts (if any).\n  4. Organize data into the required directory structure.\n\n## 3. Download Pretrained Models\n- **Objective**: Obtain checkpoints for quick reproduction of results.\n- **Steps**:\n  1. Download models from:\n     - [ESAM Model](https://cloud.tsinghua.edu.cn/f/426d6eb693ff4b1fa04b/?dl=1)\n     - [ESAM-E Model](https://cloud.tsinghua.edu.cn/f/7578d7e3d6764f6a93ee/?dl=1)\n     - [ESAM-E+FF Model](https://cloud.tsinghua.edu.cn/f/4c2dd1559e854f48be76/?dl=1)\n  2. Place checkpoints in the specified directory (e.g., `./checkpoints/`).\n\n## 4. Training and Evaluation\n- **Objective**: Train or evaluate the model on supported datasets.\n- **Steps**:\n  1. Follow the instructions in [Train and Evaluation](./docs/run.md).\n  2. For training:\n     - Configure hyperparameters in the provided config files.\n     - Run training script (e.g., `python train.py --config configs/esam.yaml`).\n  3. For evaluation:\n     - Run evaluation script with the appropriate checkpoint (e.g., `python eval.py --checkpoint ./checkpoints/esam.pth`).\n\n## 5. Run Demo on Custom Data\n- **Objective**: Visualize results on custom RGB-D videos or datasets.\n- **Steps**:\n  1. Follow the instructions in [Visualization Demo](./docs/demo.md).\n  2. For custom data:\n     - Ensure data is in the required format (e.g., `.png` for RGB, `.depth` for depth).\n     - Run the demo script (e.g., `python demo.py --input ./custom_data/ --output ./results/`).\n\n## 6. (Optional) Benchmarking\n- **Objective**: Reproduce paper results or benchmark on new datasets.\n- **Steps**:\n  1. Use the provided evaluation scripts with the correct dataset splits.\n  2. Compare metrics (AP, AP@50, AP@25) against the paper's tables.", "issue_title": "Question about Temporal Fusion Implementation in Visualization Demo", "issue_body": "First, thanks for your impressive work on this project!  \n\nWhile trying to reproduce the full-scene 3D segmentation results shown in your paper/demo videos, I noticed that the provided visualization demo processes each frame independently without temporal fusion. \n\nCould you please advise on how to implement the temporal fusion mechanism described in the paper to achieve complete room reconstruction with consistent instance IDs across frames?", "choices": "(A) 1. Ensure the version of mmengine is 0.10.3.\n2. Modify `ESAM/oneformer3d/__init__.py` to manually register `ScanNet200MixFormer3D_Online`.\n3. Check `ESAM/oneformer3d/mixformer3d.py` and `ESAM/oneformer3d/__init__.py` to confirm that `ScanNet200MixFormer3D_Online` is correctly registered and imported.\n4. Install mmdet3d in a new environment to avoid conflicts with previous installations.\n5. Verify the environment variables and ensure the correct PYTHONPATH is set before running the command. (B) 1. Ensure the version of mmengine is 0.10.3.\n2. Check `ESAM/oneformer3d/mixformer3d.py` and `ESAM/oneformer3d/__init__.py` to confirm that `ScanNet200MixFormer3D_Online` is correctly registered and imported.\n3. Install mmdet3d in a new environment to avoid conflicts with previous installations.\n4. Verify the environment variables and ensure the correct PYTHONPATH is set before running the command. (C) 1. Install mmdet3d in a new environment to avoid conflicts with previous installations.\n2. Ensure the version of mmengine is 0.10.3.\n3. Check `ESAM/oneformer3d/mixformer3d.py` and `ESAM/oneformer3d/__init__.py` to confirm that `ScanNet200MixFormer3D_Online` is correctly registered and imported.\n4. Verify the environment variables and ensure the correct PYTHONPATH is set before running the command. (D) 1. Ensure the version of mmengine is 0.10.3.\n2. Check `ESAM/oneformer3d/mixformer3d.py` and `ESAM/oneformer3d/__init__.py` to confirm that `ScanNet200MixFormer3D_Online` is correctly registered and imported.\n3. Uninstall mmengine to avoid conflicts.\n4. Install mmdet3d in a new environment to avoid conflicts with previous installations.\n5. Verify the environment variables and ensure the correct PYTHONPATH is set before running the command. (E) 1. Ensure the version of mmengine is 0.10.3.\n2. Check `ESAM/oneformer3d/mixformer3d.py` and `ESAM/oneformer3d/__init__.py` to confirm that `ScanNet200MixFormer3D_Online` is correctly registered and imported.\n3. Verify the environment variables and ensure the correct PYTHONPATH is set before running the command.\n4. Install mmdet3d in a new environment to avoid conflicts with previous installations. (F) 1. Check `ESAM/oneformer3d/mixformer3d.py` and `ESAM/oneformer3d/__init__.py` to confirm that `ScanNet200MixFormer3D_Online` is correctly registered and imported.\n2. Install mmdet3d in a new environment to avoid conflicts with previous installations.\n3. Verify the environment variables and ensure the correct PYTHONPATH is set before running the command. (G) 1. Ensure the version of mmengine is 0.10.3.\n2. Check `ESAM/oneformer3d/mixformer3d.py` and `ESAM/oneformer3d/__init__.py` to confirm that `ScanNet200MixFormer3D_Online` is correctly registered and imported.\n3. Install mmdet3d in a new environment to avoid conflicts with previous installations.", "answer": "B"}
{"uuid": "12ca8589-9d3f-4694-9ca3-9848ef1d03b4", "setup_instruct": "# EmbodiedSAM Execution Plan\n\n## 1. Environment Setup\n- **Objective**: Install dependencies and configure the environment.\n- **Steps**:\n  1. Follow the instructions in [Installation](./docs/installation.md).\n  2. Verify GPU compatibility and CUDA version if applicable.\n  3. Install Python dependencies via `pip install -r requirements.txt`.\n\n## 2. Dataset Preparation\n- **Objective**: Download and preprocess datasets for training/evaluation.\n- **Steps**:\n  1. Follow the instructions in [Dataset Preparation](./docs/dataset_preparation.md).\n  2. Download ScanNet200, SceneNN, or 3RScan datasets as needed.\n  3. Preprocess data using provided scripts (if any).\n\n## 3. Download Pretrained Models\n- **Objective**: Obtain checkpoints for quick reproduction.\n- **Steps**:\n  1. Download ESAM or ESAM-E models from:\n     - [ESAM Model](https://cloud.tsinghua.edu.cn/f/426d6eb693ff4b1fa04b/?dl=1)\n     - [ESAM-E Model](https://cloud.tsinghua.edu.cn/f/7578d7e3d6764f6a93ee/?dl=1)\n  2. Place checkpoints in the specified directory (e.g., `./checkpoints/`).\n\n## 4. Training and Evaluation\n- **Objective**: Train or evaluate the model on target datasets.\n- **Steps**:\n  1. Follow instructions in [Train and Evaluation](./docs/run.md).\n  2. For training, run:\n     ```bash\n     python train.py --config [CONFIG_FILE] --dataset [DATASET_NAME]\n     ```\n  3. For evaluation, run:\n     ```bash\n     python evaluate.py --config [CONFIG_FILE] --dataset [DATASET_NAME] --checkpoint [CHECKPOINT_PATH]\n     ```\n\n## 5. Visualization Demo\n- **Objective**: Run demos on provided or custom datasets.\n- **Steps**:\n  1. Follow instructions in [Visualization Demo](./docs/demo.md).\n  2. For custom data, prepare RGB-D streams and run:\n     ```bash\n     python demo.py --input [INPUT_PATH] --output [OUTPUT_PATH] --checkpoint [CHECKPOINT_PATH]\n     ```\n\n## 6. Custom Dataset Support\n- **Objective**: Apply ESAM to user-provided data.\n- **Steps**:\n  1. Format data according to guidelines in [Demo Documentation](./docs/demo.md).\n  2. Use the `demo.py` script with custom data paths.\n\n## 7. (Optional) Open-Vocabulary Testing\n- **Objective**: Evaluate open-vocabulary performance.\n- **Steps**:\n  1. Modify the evaluation script to include open-vocabulary settings.\n  2. Run evaluation on ScanNet200 with open-vocabulary labels.\n\n## 8. Citation\n- **Objective**: Acknowledge the authors' work if used in research.\n- **Steps**:\n  - Include the provided BibTeX entry in publications.", "issue_title": "About the processed SceneNN data from the repo of Online3D", "issue_body": "Thank you very much for your excellent open-source work! I have a question I would like to consult you about. In your outstanding work, you provided the SceneNN dataset for direct download via https://cloud.tsinghua.edu.cn/d/dab5f9ea7f0c42f38364/, which can be further used to generate SceneNN-MV. \r\nIn the downloaded dataset, each scene contains `.npy` files corresponding to the number of images, including \"ins,\" \"label,\" \"point,\" and \"pose.\" May I ask whether these `.npy` files were generated following the process described in the official SceneNN repository (https://github.com/hkust-vgd/scenenn)? Since the steps in the official SceneNN repository are unclear, I will try to understand and replicate the process if that is the case. If not, could you provide links or papers on the data processing method for the dataset provided at https://cloud.tsinghua.edu.cn/d/dab5f9ea7f0c42f38364/?\r\nThank you!", "choices": "(A) 1. The issue was related to an incomplete installation of `torch_scatter`. \n2. Follow the recommended solution from the linked issue: https://github.com/rusty1s/pytorch_scatter/issues/370 to properly install or reinstall `torch_scatter`.\n3. After applying the fix, the issue was resolved as confirmed by the OP's comment: 'I fixed. Waiting untill dataset will download'. (B) 1. Follow the recommended solution from the linked issue: https://github.com/rusty1s/pytorch_scatter/issues/370 to properly install or reinstall `torch_scatter`.\n2. The issue was related to an incomplete installation of `torch_scatter`.\n3. After applying the fix, the issue was resolved as confirmed by the OP's comment: 'I fixed. Waiting untill dataset will download'. (C) 1. The issue was related to an incomplete installation of `torch_scatter`.\n2. Uninstall `torch_scatter` using `pip uninstall torch_scatter`.\n3. Follow the recommended solution from the linked issue: https://github.com/rusty1s/pytorch_scatter/issues/370 to properly install or reinstall `torch_scatter`.\n4. After applying the fix, the issue was resolved as confirmed by the OP's comment: 'I fixed. Waiting untill dataset will download'. (D) 1. The issue was related to an incomplete installation of `torch_scatter`.\n2. After applying the fix, the issue was resolved as confirmed by the OP's comment: 'I fixed. Waiting untill dataset will download'.", "answer": "A"}
{"uuid": "3944fa55-1bf5-44b0-9f75-a8c2721c6e81", "setup_instruct": "# EmbodiedSAM Execution Plan\n\n## 1. Environment Setup\n- **Objective**: Prepare the system environment for running EmbodiedSAM.\n- **Steps**:\n  1. Follow the installation guide in [Installation](./docs/installation.md).\n  2. Install required dependencies (Python, CUDA, etc.).\n  3. Verify GPU compatibility and drivers.\n\n## 2. Dataset Preparation\n- **Objective**: Download and preprocess datasets for training/evaluation.\n- **Steps**:\n  1. Refer to [Dataset Preparation](./docs/dataset_preparation.md).\n  2. Download datasets (ScanNet, SceneNN, 3RScan) from official sources.\n  3. Preprocess data using provided scripts (if any).\n\n## 3. Download Checkpoints (Optional)\n- **Objective**: Obtain pre-trained models for quick results.\n- **Steps**:\n  1. Download checkpoints from:\n     - [ESAM Model](https://cloud.tsinghua.edu.cn/f/426d6eb693ff4b1fa04b/?dl=1)\n     - [ESAM-E Model](https://cloud.tsinghua.edu.cn/f/7578d7e3d6764f6a93ee/?dl=1)\n     - [ESAM-E+FF Model](https://cloud.tsinghua.edu.cn/f/4c2dd1559e854f48be76/?dl=1)\n  2. Place checkpoints in the specified directory (check docs).\n\n## 4. Training and Evaluation\n- **Objective**: Train or evaluate models on prepared datasets.\n- **Steps**:\n  1. Follow instructions in [Train and Evaluation](./docs/run.md).\n  2. Run training scripts (e.g., `python train.py --config configs/esam.yaml`).\n  3. For evaluation, use provided evaluation scripts (e.g., `python eval.py --checkpoint /path/to/model`).\n\n## 5. Run Demo on Custom Data\n- **Objective**: Visualize results on custom RGB-D videos or datasets.\n- **Steps**:\n  1. Refer to [Visualization Demo](./docs/demo.md).\n  2. Prepare custom data in the required format (e.g., `.ply` or `.obj`).\n  3. Execute demo script (e.g., `python demo.py --input /path/to/data --output /path/to/results`).\n\n## 6. Benchmarking (Optional)\n- **Objective**: Reproduce paper results or compare models.\n- **Steps**:\n  1. Use provided benchmark scripts (if any) to test AP/AP@50 metrics.\n  2. Compare results with tables in the README.\n\n## 7. Troubleshooting\n- **Common Issues**:\n  - Dependency conflicts: Use `conda` or virtual environments.\n  - Data format errors: Verify preprocessing steps.\n  - GPU memory issues: Reduce batch size or use `--fp16`.", "issue_title": "for scannet200-mv,the reproducible results are inconsistent", "issue_body": "Hello author, after training and getting the corresponding weight of scannet200-mv, I tested and found that the AP obtained was about ten percent lower than that mentioned in the paper. But I didn't modify the configuration file, and I got the same result as you after using the weights you provided, which may mean that the dataset and the processing of it is fine. Is there any other possible reason?\r\n![scannet200-mv](https://github.com/user-attachments/assets/d12e0c74-26bb-491e-8bab-8138a555496c)", "choices": "(A) 1. Verify that the file paths in the code correctly point to the `.bin` files in `data/scannet/points/scenexxxx_xx.bin`. 2. Ensure you have processed the reconstructed data by following the instructions in the README under `data/scannet`. 3. Run the dataset generation file in the `scannet` folder before attempting to train the model. (B) 1. Run the dataset generation file in the `scannet` folder before attempting to train the model. 2. Ensure you have processed the reconstructed data by following the instructions in the README under `data/scannet`. 3. Verify that the file paths in the code correctly point to the `.bin` files in `data/scannet/points/scenexxxx_xx.bin`. (C) 1. Ensure you have processed the reconstructed data by following the instructions in the README under `data/scannet`. 2. Run the dataset generation file in the `scannet` folder before attempting to train the model. (D) 1. Ensure you have processed the reconstructed data by following the instructions in the README under `data/scannet`. 2. Verify that the file paths in the code correctly point to the `.bin` files in `data/scannet/points/scenexxxx_xx.bin`. 3. Manually edit the `.bin` files to \"fix\" any anomalies. 4. Run the dataset generation file in the `scannet` folder before attempting to train the model. (E) 1. Ensure you have processed the reconstructed data by following the instructions in the README under `data/scannet`. 2. Verify that the file paths in the code correctly point to the `.bin` files in `data/scannet/points/scenexxxx_xx.bin`. 3. Delete all `.bin` files in `data/scannet/points/` to free up space. 4. Run the dataset generation file in the `scannet` folder before attempting to train the model. (F) 1. Verify that the file paths in the code correctly point to the `.bin` files in `data/scannet/points/scenexxxx_xx.bin`. 2. Run the dataset generation file in the `scannet` folder before attempting to train the model. (G) 1. Ensure you have processed the reconstructed data by following the instructions in the README under `data/scannet`. 2. Verify that the file paths in the code correctly point to the `.bin` files in `data/scannet/points/scenexxxx_xx.bin`. 3. Run the dataset generation file in the `scannet` folder before attempting to train the model.", "answer": "G"}
{"uuid": "55da8223-4354-4be4-978d-40536f103082", "setup_instruct": "# EmbodiedSAM Deployment Plan\n\n## 1. Environment Setup\n- **Objective**: Install required dependencies and configure the environment.\n- **Steps**:\n  1. Follow the instructions in [Installation.md](./docs/installation.md) to set up Python environment, CUDA, and other dependencies.\n  2. Verify GPU compatibility and install PyTorch with CUDA support.\n\n## 2. Dataset Preparation\n- **Objective**: Download and preprocess datasets for training/evaluation.\n- **Steps**:\n  1. Refer to [Dataset_Preparation.md](./docs/dataset_preparation.md) for dataset download links (ScanNet, SceneNN, 3RScan).\n  2. Preprocess datasets using provided scripts (if any).\n  3. Organize data in the specified directory structure.\n\n## 3. Download Pretrained Models\n- **Objective**: Obtain checkpoints for quick reproduction of results.\n- **Steps**:\n  1. Download models from:\n     - [ESAM Model](https://cloud.tsinghua.edu.cn/f/426d6eb693ff4b1fa04b/?dl=1)\n     - [ESAM-E Model](https://cloud.tsinghua.edu.cn/f/7578d7e3d6764f6a93ee/?dl=1)\n     - [ESAM-E+FF Model](https://cloud.tsinghua.edu.cn/f/4c2dd1559e854f48be76/?dl=1)\n  2. Place checkpoints in the `./checkpoints` directory.\n\n## 4. Training and Evaluation\n- **Objective**: Train or evaluate models on prepared datasets.\n- **Steps**:\n  1. Follow instructions in [Run.md](./docs/run.md) for:\n     - Training: Execute training scripts with dataset paths and hyperparameters.\n     - Evaluation: Run evaluation scripts with checkpoint paths and dataset splits.\n\n## 5. Visualization Demo\n- **Objective**: Visualize results on provided or custom datasets.\n- **Steps**:\n  1. For custom data, format RGB-D streams as per [Demo.md](./docs/demo.md).\n  2. Run the demo script:\n     ```bash\n     python demo.py --input_path /path/to/data --checkpoint /path/to/model\n     ```\n  3. Adjust parameters for real-time streaming (if applicable).\n\n## 6. Custom Dataset Support\n- **Objective**: Apply ESAM to user-provided data.\n- **Steps**:\n  1. Ensure data follows the structure specified in [Demo.md](./docs/demo.md).\n  2. Modify config files (if needed) to match custom data attributes.\n  3. Run inference using the demo script with `--custom_data` flag.\n\n## 7. Performance Benchmarking\n- **Objective**: Compare results with reported metrics.\n- **Steps**:\n  1. Use evaluation scripts to generate AP/AP@50/AP@25 scores.\n  2. Compare with tables in the README for ScanNet200, SceneNN, and 3RScan.\n\n## 8. Optional: HuggingFace Integration\n- **Objective**: Use alternative model/data sources.\n- **Steps**:\n  1. Download checkpoints/processed data from [HuggingFace](https://huggingface.co/XXXCARREY/EmbodiedSAM/tree/main).\n  2. Replace paths in config files accordingly.", "issue_title": "How to obtain the 3D instance segmentation results on ScanNet dataset?", "issue_body": "Dear authors, when I tried to use the checkpoint download from ESAM-E under 3D instance segmentation results on ScanNet dataset section in README. I test the result on scannet-mv dataset, but the AP performance is much less than the reported result.\r\nSo may I ask the detail about how does the 3D instance segmentation be evaluated on ScanNet.\r\nThank you.", "choices": "(A) 1. Ensure the tokenizer_path is correctly set and does not include an extra <pad> token, as this can alter the model structure.\n2. Verify the fine-tuning process by setting the number of B modules (lora_b_nums) to 1 to test performance under MMLU, which helps identify if the issue lies in the fine-tuning process.\n3. Confirm that the evaluation method in OpenCompass is correctly configured and matches the version used in the paper (0.4.2).\n4. Adjust hyperparameters such as lora_rank, lora_alpha, and lora_dropout to find an optimal combination for better performance. (B) 1. Ensure the tokenizer_path is correctly set and does not include an extra <pad> token, as this can alter the model structure.\n2. Verify the fine-tuning process by setting the number of B modules (lora_b_nums) to 1 to test performance under MMLU, which helps identify if the issue lies in the fine-tuning process.\n3. Adjust hyperparameters such as lora_rank, lora_alpha, and lora_dropout to find an optimal combination for better performance.\n4. Confirm that the evaluation method in OpenCompass is correctly configured and matches the version used in the paper (0.4.2). (C) 1. Ensure the tokenizer_path is correctly set and does not include an extra <pad> token, as this can alter the model structure.\n2. Add an extra <pad> token to the tokenizer_path to increase model flexibility.\n3. Verify the fine-tuning process by setting the number of B modules (lora_b_nums) to 1 to test performance under MMLU, which helps identify if the issue lies in the fine-tuning process.\n4. Adjust hyperparameters such as lora_rank, lora_alpha, and lora_dropout to find an optimal combination for better performance.\n5. Confirm that the evaluation method in OpenCompass is correctly configured and matches the version used in the paper (0.4.2). (D) 1. Ensure the tokenizer_path is correctly set and does not include an extra <pad> token, as this can alter the model structure.\n2. Verify the fine-tuning process by setting the number of B modules (lora_b_nums) to 1 to test performance under MMLU, which helps identify if the issue lies in the fine-tuning process.\n3. Adjust hyperparameters such as lora_rank, lora_alpha, and lora_dropout to find an optimal combination for better performance. (E) 1. Ensure the tokenizer_path is correctly set and does not include an extra <pad> token, as this can alter the model structure.\n2. Adjust hyperparameters such as lora_rank, lora_alpha, and lora_dropout to find an optimal combination for better performance.\n3. Confirm that the evaluation method in OpenCompass is correctly configured and matches the version used in the paper (0.4.2). (F) 1. Ensure the tokenizer_path is correctly set and does not include an extra <pad> token, as this can alter the model structure.\n2. Adjust hyperparameters such as lora_rank, lora_alpha, and lora_dropout to find an optimal combination for better performance.\n3. Verify the fine-tuning process by setting the number of B modules (lora_b_nums) to 1 to test performance under MMLU, which helps identify if the issue lies in the fine-tuning process.\n4. Confirm that the evaluation method in OpenCompass is correctly configured and matches the version used in the paper (0.4.2).", "answer": "B"}
{"uuid": "1bd752e8-ae22-494d-966e-3008c066f39e", "setup_instruct": "# EmbodiedSAM Deployment Plan\n\n## 1. Environment Setup\n- **Description**: Install required dependencies and configure the environment.\n- **Commands**: Follow instructions in [Installation](./docs/installation.md).\n\n## 2. Dataset Preparation\n- **Description**: Download and preprocess datasets (ScanNet, SceneNN, 3RScan).\n- **Commands**: Follow instructions in [Dataset Preparation](./docs/dataset_preparation.md).\n\n## 3. Download Checkpoints\n- **Description**: Download pre-trained models for quick reproduction.\n- **Commands**:\n  ```bash\n  # ESAM model\n  wget https://cloud.tsinghua.edu.cn/f/426d6eb693ff4b1fa04b/?dl=1 -O esam_model.pth\n\n  # ESAM-E model\n  wget https://cloud.tsinghua.edu.cn/f/7578d7e3d6764f6a93ee/?dl=1 -O esam_e_model.pth\n  ```\n\n## 4. Training and Evaluation\n- **Description**: Train and evaluate the model on the prepared datasets.\n- **Commands**: Follow instructions in [Train and Evaluation](./docs/run.md).\n\n## 5. Visualization Demo\n- **Description**: Run demos on provided datasets or custom data.\n- **Commands**: Follow instructions in [Visualization Demo](./docs/demo.md).\n\n## 6. Custom Dataset Support\n- **Description**: Run EmbodiedSAM on custom datasets.\n- **Commands**: Follow instructions in [Demo](./docs/demo.md) for custom data.\n\n## 7. Open-Vocabulary Segmentation\n- **Description**: Test open-vocabulary 3D instance segmentation.\n- **Commands**: Use the provided checkpoints and follow evaluation steps in [Train and Evaluation](./docs/run.md).", "issue_title": "When I run train.py on the scannet-mv_fast dataset, a bug occurs", "issue_body": "Hello, authors! I found that after I modified the following code, the following bug appeared. I found that the length of each instance_mask file is 20000, while the length of each semantic_mask is 40000, so IndexError appeared. What should I do in the face of this bug?Thank you very much!\r\n![image](https://github.com/user-attachments/assets/d9a80df6-b17c-4132-9d60-dfd2d5e5fd2c)\r\n![image](https://github.com/user-attachments/assets/41374844-20ac-4bd1-af9f-d6d629391387)", "choices": "(A) 1. Enable GitHub Actions in your forked repo.\n2. Replace the content of `script/run.sh` with the provided script that includes a loop to copy raw data to clean, run flake8, parse the data, and execute pytest for each name in the NAMES array.\n3. Ensure all finalized samples are added to the `data/raw` directory.\n4. For logging issues, specify a save path for the log file in the test cases and ensure the directory is set up and torn down appropriately.\n5. Verify that all samples meet the required criteria (e.g., use at least 2 libraries, include requirements, returns, and examples where necessary).\n6. Remove any external data accidentally pushed to the repo and update to the latest version. (B) 1. Enable GitHub Actions in your forked repo.\n2. Replace the content of `script/run.sh` with the provided script that includes a loop to copy raw data to clean, run flake8, parse the data, and execute pytest for each name in the NAMES array.\n3. Verify that all samples meet the required criteria (e.g., use at least 2 libraries, include requirements, returns, and examples where necessary).\n4. Ensure all finalized samples are added to the `data/raw` directory.\n5. For logging issues, specify a save path for the log file in the test cases and ensure the directory is set up and torn down appropriately.\n6. Remove any external data accidentally pushed to the repo and update to the latest version. (C) 1. Enable GitHub Actions in your forked repo.\n2. Replace the content of `script/run.sh` with the provided script that includes a loop to copy raw data to clean, run flake8, parse the data, and execute pytest for each name in the NAMES array.\n3. Ensure all finalized samples are added to the `data/raw` directory.\n4. Verify that all samples meet the required criteria (e.g., use at least 2 libraries, include requirements, returns, and examples where necessary).\n5. Remove any external data accidentally pushed to the repo and update to the latest version. (D) 1. Enable GitHub Actions in your forked repo.\n2. Disable GitHub Actions in your forked repo.\n3. Replace the content of `script/run.sh` with the provided script that includes a loop to copy raw data to clean, run flake8, parse the data, and execute pytest for each name in the NAMES array.\n4. Ensure all finalized samples are added to the `data/raw` directory.\n5. For logging issues, specify a save path for the log file in the test cases and ensure the directory is set up and torn down appropriately.\n6. Verify that all samples meet the required criteria (e.g., use at least 2 libraries, include requirements, returns, and examples where necessary).\n7. Remove any external data accidentally pushed to the repo and update to the latest version. (E) 1. Enable GitHub Actions in your forked repo.\n2. Replace the content of `script/run.sh` with the provided script that includes a loop to copy raw data to clean, run flake8, parse the data, and execute pytest for each name in the NAMES array.\n3. Ensure all finalized samples are added to the `data/raw` directory.\n4. Delete the `data/raw` directory.\n5. For logging issues, specify a save path for the log file in the test cases and ensure the directory is set up and torn down appropriately.\n6. Verify that all samples meet the required criteria (e.g., use at least 2 libraries, include requirements, returns, and examples where necessary).\n7. Remove any external data accidentally pushed to the repo and update to the latest version. (F) 1. Enable GitHub Actions in your forked repo.\n2. Ensure all finalized samples are added to the `data/raw` directory.\n3. For logging issues, specify a save path for the log file in the test cases and ensure the directory is set up and torn down appropriately.\n4. Verify that all samples meet the required criteria (e.g., use at least 2 libraries, include requirements, returns, and examples where necessary).\n5. Remove any external data accidentally pushed to the repo and update to the latest version. (G) 1. Enable GitHub Actions in your forked repo.\n2. Remove any external data accidentally pushed to the repo and update to the latest version.\n3. Replace the content of `script/run.sh` with the provided script that includes a loop to copy raw data to clean, run flake8, parse the data, and execute pytest for each name in the NAMES array.\n4. Ensure all finalized samples are added to the `data/raw` directory.\n5. For logging issues, specify a save path for the log file in the test cases and ensure the directory is set up and torn down appropriately.\n6. Verify that all samples meet the required criteria (e.g., use at least 2 libraries, include requirements, returns, and examples where necessary).", "answer": "A"}
{"uuid": "20985304-2ae8-43fa-82cb-af6a1876e69f", "setup_instruct": "# Step-by-Step Execution Plan\n\n## 1. Install Dependencies\n- **Description**: Install all required Python packages as specified in `requirements.txt`.\n- **Command**:\n  ```bash\n  pip install -r requirements.txt\n  ```\n\n## 2. Download Pre-trained Models (Optional)\n- **Description**: If you want to use the pre-compressed models, download them from Hugging Face:\n  - [Llama-2-4.7B](https://huggingface.co/XiaodongChen/Llama-2-4.7B)\n  - [Llama-3.1-5.4B](https://huggingface.co/XiaodongChen/Llama-3.1-5.4B)\n\n## 3. Layer Pruning with MSE Loss (Single GPU)\n- **Description**: Train the lightweight network using MSE loss on a single GPU. This is recommended for limited GPU resources.\n- **Command**:\n  ```bash\n  python mseloss_entry.py\n  ```\n- **Notes**: \n  - The script will automatically download the pre-trained model and dataset.\n  - Ensure sufficient memory, as hidden states are stored in memory by default.\n\n## 4. Layer Pruning with LLM Loss (Multi-GPU)\n- **Description**: Train the lightweight network using LLM loss on 4 GPUs for better results. Requires more GPU memory.\n- **Command**:\n  ```bash\n  CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch --config_file 4gpu.yaml llmloss_entry.py\n  ```\n\n## 5. Customize Training Parameters (Optional)\n- **Description**: Modify training parameters in `LLM_Streamline/args.py` if needed. Key parameters include:\n  - `model_name`: Path to the pre-trained model.\n  - `layer_intervals`: Number of layers to prune.\n  - `batch_size`, `epoches`, `lr`, etc.\n\n## 6. Calculate Stability (Optional)\n- **Description**: Compare model predictions before and after pruning to calculate stability.\n- **Command**:\n  ```bash\n  python calculate_stability.py arg1 arg2\n  ```\n- **Example**:\n  ```bash\n  python calculate_stability.py \"./opencompass/outputs/default/20241121_220629/predictions/llama-3-70b-hf\" \"./opencompass/outputs/default/20241123_220629/predictions/llama-3-70b-hf\"\n  ```", "issue_title": "Issue About Memory Usage", "issue_body": "Hi\nThanks for your code release and that's definitely a great work\nWhle when I reproduce your work, I notice some strange about my memory usage, during the training process my memory usage are continuously increase and even exceed the memory cap.\nI noticed a lot of memory release strategies in the code but I can't seem to work it out.\nI want to know if there are any strategies to mitigate this, because I can't wait until the training is completed and the memory will explode.\nBy the way my memory cap is 1TB,\n\nHoping for your reply!", "choices": "(A) 1. Disable the flow_loss by setting `flow_loss_weight=0` in the demo.py file at line 357.\n2. Alternatively, implement a window-wise optimization for memory-efficient global alignment as suggested in PR #59.\n3. For better results with long videos, consider using the window-wise optimization implementation from PR #72. (B) 1. Disable the flow_loss by setting `flow_loss_weight=0` in the demo.py file at line 357.\n2. For better results with long videos, consider using the window-wise optimization implementation from PR #72.\n3. Alternatively, implement a window-wise optimization for memory-efficient global alignment as suggested in PR #59. (C) 1. Alternatively, implement a window-wise optimization for memory-efficient global alignment as suggested in PR #59.\n2. For better results with long videos, consider using the window-wise optimization implementation from PR #72. (D) 1. Disable the flow_loss by setting `flow_loss_weight=0` in the demo.py file at line 357.\n2. Enable the flow_loss by setting `flow_loss_weight=1` in the demo.py file at line 357.\n3. Alternatively, implement a window-wise optimization for memory-efficient global alignment as suggested in PR #59.\n4. For better results with long videos, consider using the window-wise optimization implementation from PR #72.", "answer": "A"}
{"uuid": "74c760d9-5170-431f-b521-333ff3f7502f", "setup_instruct": "# MonST3R Execution Plan\n\n## 1. Installation\n1. **Clone the repository**  \n   ```bash\n   git clone --recursive https://github.com/junyi42/monst3r\n   cd monst3r\n   ```\n   - If already cloned, update submodules:  \n     ```bash\n     git submodule update --init --recursive\n     ```\n\n2. **Set up Conda environment**  \n   ```bash\n   conda create -n monst3r python=3.11 cmake=3.14.0\n   conda activate monst3r\n   conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia\n   pip install -r requirements.txt\n   ```\n   - Optional (for training/evaluation/dataset prep):  \n     ```bash\n     pip install -r requirements_optional.txt\n     ```\n\n3. **Install 4D visualization tool (optional)**  \n   ```bash\n   pip install -e viser\n   ```\n\n4. **Compile CUDA kernels for RoPE (optional)**  \n   ```bash\n   cd croco/models/curope/\n   python setup.py build_ext --inplace\n   cd ../../../\n   ```\n\n## 2. Download Checkpoints\n1. **Download model weights**  \n   ```bash\n   cd data\n   bash download_ckpt.sh\n   cd ..\n   ```\n\n## 3. Inference\n1. **Run interactive GUI mode**  \n   ```bash\n   python demo.py\n   ```\n   - For memory-efficient mode:  \n     ```bash\n     python demo.py --not_batchify\n     ```\n\n2. **Run non-interactive mode**  \n   ```bash\n   python demo.py --input demo_data/lady-running --output_dir demo_tmp --seq_name lady-running\n   ```\n   - Additional flags:  \n     - Video input: `--input demo_data/lady-running.mp4 --num_frames 65`  \n     - Real-time mode: `--real_time`  \n     - Window-wise mode: `--window_wise --window_size 100 --window_overlap_ratio 0.5`  \n     - Optimize with previous segment: `--prev_output_dir ./demo_tmp/lady-running`  \n\n## 4. Visualization\n1. **View 4D results**  \n   ```bash\n   python viser/visualizer_monst3r.py --data demo_tmp/lady-running\n   ```\n   - For real-time mode results:  \n     ```bash\n     python viser/visualizer_monst3r_realtime.py --data_path demo_tmp/lady-running\n     ```\n\n## 5. Evaluation\n1. **Download DAVIS dataset**  \n   ```bash\n   cd data; python download_davis.py; cd ..\n   ```\n\n2. **Run evaluation script**  \n   ```bash\n   CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 --master_port=29604 launch.py --mode=eval_pose \\\n       --pretrained=\"checkpoints/MonST3R_PO-TA-S-W_ViTLarge_BaseDecoder_512_dpt.pth\" \\\n       --eval_dataset=davis --output_dir=\"results/davis_joint\"\n   ```\n   - Use ground truth mask: `--use_gt_mask`  \n\n3. **Visualize evaluation results**  \n   ```bash\n   python viser/visualizer_monst3r.py --data results/davis_joint/bear\n   ```\n\n## 6. Training\n1. **Prepare datasets**  \n   Follow instructions in [prepare_training.md](data/prepare_training.md).\n\n2. **Train the model**  \n   ```bash\n   CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=29604 launch.py --mode=train \\\n       --train_dataset=\"[dataset_config]\" \\\n       --test_dataset=\"[dataset_config]\" \\\n       --pretrained=\"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\" \\\n       --output_dir=\"results/MonST3R_PO-TA-S-W_ViTLarge_BaseDecoder_512_dpt\"\n   ```\n   - Replace `[dataset_config]` with parameters from the README.\n\n## 7. Citation\n- Use the provided BibTeX entry to cite the work if used in research.", "issue_title": "Output accurate or normalized depth values", "issue_body": "This is an excellent work! I want to know the depth output (`frame_.npy` files) is accurate depth or the normalized depth? After trying an example, I print the depth values:\r\n\r\n```\r\n>>> a.shape\r\n(384, 512)\r\n>>> a.min()\r\n0.052985173\r\n>>> a.max()\r\n0.6308663\r\n```\r\n\r\nNot an expert on this, could you give some advice if I can get the accurate depth values?", "choices": "(A) 1. Ensure you have `gdown` installed by running `pip install gdown`.\n2. Re-run the 'precomputing flow' step in your Pycharm environment.\n3. Download the required checkpoints by running the script `download_ckpt.sh` from the repository: https://github.com/Junyi42/monst3r/blob/main/data/download_ckpt.sh.\n4. Verify that the checkpoints are properly downloaded and placed in the correct directory as specified in the script. (B) 1. Ensure you have `gdown` installed by running `pip install gdown`.\n2. Download the required checkpoints by running the script `download_ckpt.sh` from the repository: https://github.com/Junyi42/monst3r/blob/main/data/download_ckpt.sh.\n3. Verify that the checkpoints are properly downloaded and placed in the correct directory as specified in the script.\n4. Re-run the 'precomputing flow' step in your Pycharm environment. (C) 1. Ensure you have `gdown` installed by running `pip install gdown`.\n2. Verify that the checkpoints are properly downloaded and placed in the correct directory as specified in the script.\n3. Download the required checkpoints by running the script `download_ckpt.sh` from the repository: https://github.com/Junyi42/monst3r/blob/main/data/download_ckpt.sh.\n4. Re-run the 'precomputing flow' step in your Pycharm environment. (D) 1. Ensure you have `gdown` installed by running `pip install gdown`.\n2. Download the required checkpoints by running the script `download_ckpt.sh` from the repository: https://github.com/Junyi42/monst3r/blob/main/data/download_ckpt.sh.\n3. Re-run the 'precomputing flow' step in your Pycharm environment. (E) 1. Ensure you have `gdown` installed by running `pip install gdown`.\n2. Download the required checkpoints by running the script `download_ckpt.sh` from the repository: https://github.com/Junyi42/monst3r/blob/main/data/download_ckpt.sh.\n3. Delete the downloaded checkpoints using `rm -rf` command.\n4. Verify that the checkpoints are properly downloaded and placed in the correct directory as specified in the script.\n5. Re-run the 'precomputing flow' step in your Pycharm environment. (F) 1. Ensure you have `gdown` installed by running `pip install gdown`.\n2. Download the required checkpoints by running the script `download_ckpt.sh` from the repository: https://github.com/Junyi42/monst3r/blob/main/data/download_ckpt.sh.\n3. Open and edit the checkpoint files manually using a text editor.\n4. Verify that the checkpoints are properly downloaded and placed in the correct directory as specified in the script.\n5. Re-run the 'precomputing flow' step in your Pycharm environment. (G) 1. Download the required checkpoints by running the script `download_ckpt.sh` from the repository: https://github.com/Junyi42/monst3r/blob/main/data/download_ckpt.sh.\n2. Verify that the checkpoints are properly downloaded and placed in the correct directory as specified in the script.\n3. Re-run the 'precomputing flow' step in your Pycharm environment.", "answer": "B"}
{"uuid": "2fa11006-863a-484d-a8fb-222629ce712c", "setup_instruct": "# MonST3R Execution Plan\n\n## 1. Installation\n### 1.1 Clone Repository\n```bash\ngit clone --recursive https://github.com/junyi42/monst3r\ncd monst3r\n# If already cloned, update submodules:\n# git submodule update --init --recursive\n```\n\n### 1.2 Create Conda Environment\n```bash\nconda create -n monst3r python=3.11 cmake=3.14.0\nconda activate monst3r\nconda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia\npip install -r requirements.txt\n# Optional packages for training/evaluation:\n# pip install -r requirements_optional.txt\n```\n\n### 1.3 Install Visualization Tool (Optional)\n```bash\npip install -e viser\n```\n\n### 1.4 Compile CUDA Kernels (Optional)\n```bash\ncd croco/models/curope/\npython setup.py build_ext --inplace\ncd ../../../\n```\n\n## 2. Download Checkpoints\n```bash\ncd data\nbash download_ckpt.sh\ncd ..\n```\n\n## 3. Inference\n### 3.1 Interactive Mode (GUI)\n```bash\npython demo.py\n```\n\n### 3.2 Non-Interactive Mode\n```bash\npython demo.py --input demo_data/lady-running --output_dir demo_tmp --seq_name lady-running\n# Additional flags:\n# --not_batchify (memory-efficient)\n# --real_time (real-time mode)\n# --window_wise --window_size 100 --window_overlap_ratio 0.5 (window-wise optimization)\n# --prev_output_dir ./demo_tmp/lady-running (sequential optimization)\n```\n\n## 4. Visualization\n### 4.1 Standard Mode\n```bash\npython viser/visualizer_monst3r.py --data demo_tmp/lady-running\n# Optional flags:\n# --init_conf --fg_conf_thre 1.0 (remove floaters)\n```\n\n### 4.2 Real-Time Mode (if used)\n```bash\npython viser/visualizer_monst3r_realtime.py --data_path demo_tmp/lady-running\n```\n\n## 5. Evaluation (DAVIS Example)\n### 5.1 Download Dataset\n```bash\ncd data; python download_davis.py; cd ..\n```\n\n### 5.2 Run Evaluation\n```bash\nCUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 --master_port=29604 launch.py --mode=eval_pose \\\n    --pretrained=\"checkpoints/MonST3R_PO-TA-S-W_ViTLarge_BaseDecoder_512_dpt.pth\" \\\n    --eval_dataset=davis --output_dir=\"results/davis_joint\"\n    # Add --use_gt_mask for GT dynamic masks\n```\n\n### 5.3 Visualize Results\n```bash\npython viser/visualizer_monst3r.py --data results/davis_joint/bear\n# --no_mask for per-frame pointcloud\n```\n\n## 6. Training\n### 6.1 Prepare Data\nFollow [prepare_training.md](data/prepare_training.md).\n\n### 6.2 Start Training\n```bash\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=29604 launch.py --mode=train \\\n    --train_dataset=\"[dataset config]\" \\\n    --test_dataset=\"[test config]\" \\\n    --pretrained=\"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\" \\\n    --output_dir=\"results/MonST3R_PO-TA-S-W_ViTLarge_BaseDecoder_512_dpt\"\n```", "issue_title": "Tartan-C-T-TSKH-spring540x960-M.pth", "issue_body": "Dear Authors,\r\nThank you for sharing this amazing work!\r\n I have a question when running 'precomputing flow...', Pycharm prompts' FileNotFoundError: [Errno 2] No such file or directory: ='third_party/RAFT/models/Tartan-C-T-TSKH-spring540x960-Mpth' .But I cannot find the file from your contents. May I ask how I can solve this problem. \r\nThank you in advance!\r\n![屏幕截图 2024-10-25 213929](https://github.com/user-attachments/assets/4340fef4-69f3-4cfb-bf12-736e90ff0afe)", "choices": "(A) 1. Check the version of the `transformers` package installed in your environment.  \n2. Compare it with the version specified in the `requirements.txt` file.  \n3. If the versions do not match, revert the `transformers` package to the version specified in `requirements.txt`. (B) 1. Revert the `transformers` package to the version specified in `requirements.txt`.  \n2. Check the version of the `transformers` package installed in your environment.  \n3. Compare it with the version specified in the `requirements.txt` file.  \n4. Re-run the script to verify the issue is resolved. (C) 1. Check the version of the `transformers` package installed in your environment.  \n2. Compare it with the version specified in the `requirements.txt` file.  \n3. Re-run the script to verify the issue is resolved.  \n4. If the versions do not match, revert the `transformers` package to the version specified in `requirements.txt`. (D) 1. Check the version of the `transformers` package installed in your environment.  \n2. If the versions do not match, revert the `transformers` package to the version specified in `requirements.txt`.  \n3. Re-run the script to verify the issue is resolved. (E) 1. Check the version of the `transformers` package installed in your environment. 2. Compare it with the version specified in the `requirements.txt` file. 3. If the versions do not match, revert the `transformers` package to the version specified in `requirements.txt`. 4. Re-run the script to verify the issue is resolved. (F) 1. Check the version of the `transformers` package installed in your environment.  \n2. Compare it with the version specified in the `requirements.txt` file.  \n3. Uninstall the `transformers` package.  \n4. If the versions do not match, revert the `transformers` package to the version specified in `requirements.txt`.  \n5. Re-run the script to verify the issue is resolved. (G) 1. Check the version of the `transformers` package installed in your environment.  \n2. Compare it with the version specified in the `requirements.txt` file.  \n3. Upgrade all packages to their latest versions.  \n4. If the versions do not match, revert the `transformers` package to the version specified in `requirements.txt`.  \n5. Re-run the script to verify the issue is resolved.", "answer": "E"}
{"uuid": "f73378a3-b293-4d0f-9af1-a8b49cdeb80a", "setup_instruct": "# OLMoE Deployment & Execution Plan\n\n## 1. Environment Setup\n- **Objective**: Prepare the environment for OLMoE model deployment.\n- **Steps**:\n  1. Install Python 3.8+ and pip.\n  2. Set up a virtual environment (recommended):\n     ```bash\n     python -m venv olmoe_env\n     source olmoe_env/bin/activate  # Linux/Mac\n     .\\olmoe_env\\Scripts\\activate  # Windows\n     ```\n\n## 2. Model Inference\n### Option A: vLLM (Recommended)\n- **Objective**: Run inference using vLLM for optimal performance.\n- **Steps**:\n  1. Install vLLM:\n     ```bash\n     pip install vllm\n     ```\n  2. Run inference:\n     ```python\n     from vllm import LLM, SamplingParams\n     model = LLM(\"allenai/OLMoE-1B-7B-0924\")\n     out = model.generate(\"Bitcoin is\", SamplingParams(temperature=0.0))\n     print(\"Bitcoin is\" + out[0].outputs[0].text)\n     ```\n\n### Option B: llama.cpp\n- **Objective**: Run quantized model on CPU/edge devices.\n- **Steps**:\n  1. Install llama.cpp:\n     ```bash\n     git clone https://github.com/ggerganov/llama.cpp && cd llama.cpp && make\n     ```\n  2. Download quantized GGUF model:\n     ```bash\n     wget https://huggingface.co/allenai/OLMoE-1B-7B-0924-GGUF/resolve/main/olmoe-1b-7b-0924-q4_0.gguf\n     ```\n  3. Run inference:\n     ```bash\n     ./main -m olmoe-1b-7b-0924-q4_0.gguf -p \"Bitcoin is\" -n 128\n     ```\n\n## 3. Pretraining (Advanced)\n- **Objective**: Pretrain OLMoE from scratch.\n- **Steps**:\n  1. Clone OLMo repository:\n     ```bash\n     git clone -b Muennighoff/MoE https://github.com/allenai/OLMo\n     cd OLMo\n     pip install -e .\n     ```\n  2. Install MegaBlocks:\n     ```bash\n     pip install git+https://github.com/Muennighoff/megablocks.git@olmoe\n     ```\n  3. Download and tokenize data:\n     ```bash\n     dolma tokens \\\n       --documents ${PATH_TO_DOWNLOADED_DATA} \\\n       --destination ${PATH_WHERE_TO_SAVE_TOKENIZED_DATA} \\\n       --tokenizer.name_or_path 'allenai/gpt-neox-olmo-dolma-v1_5' \\\n       --max_size '2_147_483_648' \\\n       --seed 0 \\\n       --tokenizer.eos_token_id 50279 \\\n       --tokenizer.pad_token_id 1 \\\n       --processes ${NUMBER_OF_CPU_CORES_TO_USE}\n     ```\n  4. Start training (modify config as needed):\n     ```bash\n     bash scripts/olmoe-gantry.sh\n     ```\n\n## 4. Model Adaptation\n### Supervised Fine-Tuning (SFT)\n- **Objective**: Fine-tune OLMoE on custom data.\n- **Steps**:\n  1. Clone Open Instruct:\n     ```bash\n     git clone https://github.com/allenai/open-instruct\n     cd open-instruct\n     pip install -e .\n     ```\n  2. Run SFT:\n     ```bash\n     accelerate launch \\\n       --mixed_precision bf16 \\\n       --num_machines 1 \\\n       --num_processes 8 \\\n       --use_deepspeed \\\n       --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \\\n       open_instruct/finetune.py \\\n       --model_name_or_path allenai/OLMoE-1B-7B-0924 \\\n       --tokenizer_name allenai/OLMoE-1B-7B-0924 \\\n       --use_flash_attn \\\n       --max_seq_length 4096 \\\n       --preprocessing_num_workers 128 \\\n       --per_device_train_batch_size 2 \\\n       --gradient_accumulation_steps 8 \\\n       --learning_rate 2e-05 \\\n       --lr_scheduler_type linear \\\n       --warmup_ratio 0.03 \\\n       --weight_decay 0.0 \\\n       --num_train_epochs 2 \\\n       --output_dir output/ \\\n       --with_tracking \\\n       --report_to wandb \\\n       --logging_steps 1\n     ```\n\n## 5. Evaluation\n- **Objective**: Evaluate model performance.\n- **Steps**:\n  1. For pretraining evaluation: Configure in training config (automatic).\n  2. For post-training evaluation:\n     ```bash\n     git clone https://github.com/allenai/OLMo-Eval\n     cd OLMo-Eval\n     pip install -e .\n     ```\n  3. Follow specific task instructions in `olmo_eval/tasks/olmes_v0_1/README.md`\n\n## 6. Visualization\n- **Objective**: Generate paper figures.\n- **Steps**:\n  1. Run Jupyter notebook:\n     ```bash\n     jupyter notebook scripts/olmoe_visuals.ipynb\n     ```\n  2. Execute specific cells for each figure (see notebook comments).\n\n## 7. Citation\n- **Objective**: Properly cite OLMoE in publications.\n- **BibTeX**:\n  ```bibtex\n  @misc{muennighoff2024olmoeopenmixtureofexpertslanguage,\n    title={OLMoE: Open Mixture-of-Experts Language Models}, \n    author={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},\n    year={2024},\n    eprint={2409.02060},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2409.02060}, \n  }\n  ```", "issue_title": "Early loss divergence for upcycling", "issue_body": "Hi, thanks for the great work and sharing your wandb training logs! After analysing the plots, I have some questions regarding the upcycling experiment done for OLMoE and would greatly appreciate if you could answer them in any capacity:\r\n\r\nI observed that the training loss for the upcycled OLMoE increased for the first 5k steps (~20B tokens) and that the training loss does not recover (to the early loss value of 2.25 at step 300) until around 120k steps. May I ask what was the peak learning rate used for training the upcycled OLMoE? And if any other experiments were to try to mitigate this early loss divergence?\r\n\r\n![image](https://github.com/user-attachments/assets/98fbcf42-7125-4189-9ec0-b85797dcb2cb)\r\n\r\nThanks!", "choices": "(A) 1. Train the model for 150K iterations instead of 400K iterations.\n2. Use the ODE sampler with the --heun option for evaluation.\n3. Ensure REPA is applied appropriately by checking the proj_loss values (should be around -0.7).\n4. Use VAE's EMA weights during preprocessing and decoding for better performance with REPA's scaling factor of 0.18215.\n5. During preprocessing, resize and save the images to 256x256 using the provided dataloader (https://github.com/baofff/U-ViT/blob/main/datasets.py). (B) 1. Train the model for 150K iterations instead of 400K iterations.\n2. Use the ODE sampler with the --heun option for evaluation.\n3. Ensure REPA is applied appropriately by checking the proj_loss values (should be around -0.7).\n4. During preprocessing, resize and save the images to 256x256 using the provided dataloader (https://github.com/baofff/U-ViT/blob/main/datasets.py).\n5. Disable VAE's EMA weights during preprocessing to avoid scaling issues.\n6. Use VAE's EMA weights during preprocessing and decoding for better performance with REPA's scaling factor of 0.18215. (C) 1. Train the model for 150K iterations instead of 400K iterations.\n2. Use the ODE sampler with the --heun option for evaluation.\n3. During preprocessing, resize and save the images to 256x256 using the provided dataloader (https://github.com/baofff/U-ViT/blob/main/datasets.py).\n4. Use VAE's EMA weights during preprocessing and decoding for better performance with REPA's scaling factor of 0.18215. (D) 1. During preprocessing, resize and save the images to 256x256 using the provided dataloader (https://github.com/baofff/U-ViT/blob/main/datasets.py).\n2. Train the model for 150K iterations instead of 400K iterations.\n3. Ensure REPA is applied appropriately by checking the proj_loss values (should be around -0.7).\n4. Use the ODE sampler with the --heun option for evaluation.\n5. Use VAE's EMA weights during preprocessing and decoding for better performance with REPA's scaling factor of 0.18215. (E) 1. Train the model for 150K iterations instead of 400K iterations.\n2. Use the ODE sampler with the --heun option for evaluation.\n3. Ensure REPA is applied appropriately by checking the proj_loss values (should be around -0.7).\n4. During preprocessing, resize and save the images to 256x256 using the provided dataloader (https://github.com/baofff/U-ViT/blob/main/datasets.py). (F) 1. Train the model for 400K iterations to ensure convergence.\n2. Train the model for 150K iterations instead of 400K iterations.\n3. Use the ODE sampler with the --heun option for evaluation.\n4. Ensure REPA is applied appropriately by checking the proj_loss values (should be around -0.7).\n5. During preprocessing, resize and save the images to 256x256 using the provided dataloader (https://github.com/baofff/U-ViT/blob/main/datasets.py).\n6. Use VAE's EMA weights during preprocessing and decoding for better performance with REPA's scaling factor of 0.18215. (G) 1. Train the model for 150K iterations instead of 400K iterations.\n2. Use the ODE sampler with the --heun option for evaluation.\n3. Ensure REPA is applied appropriately by checking the proj_loss values (should be around -0.7).\n4. During preprocessing, resize and save the images to 256x256 using the provided dataloader (https://github.com/baofff/U-ViT/blob/main/datasets.py).\n5. Use VAE's EMA weights during preprocessing and decoding for better performance with REPA's scaling factor of 0.18215.", "answer": "G"}
{"uuid": "fd6ec59e-36e3-4cb8-b979-44d37ff10ce4", "setup_instruct": "# AlphaEdit Execution Plan\n\n## 1. Environment Setup\n- **Description**: Install all required dependencies and verify GPU availability (A40 48G GPU minimum).\n- **Commands**:\n  ```bash\n  pip install torch==2.6.0 einops==0.8.1 higher==0.2.1 hydra-core==1.3.2 transformers==4.51.3\n  pip install datasets==2.21.0 matplotlib==3.10.3 spacy==3.4.1 scipy==1.15.2 scikit-learn==1.6.1 nltk==3.9.1\n  ```\n\n## 2. Download Precomputed Data\n- **Description**: Download and decompress the \"cov\" matrix for Llama3-8B-instruct into `./data/stats`.\n- **Steps**:\n  1. Download from [Google Drive](https://drive.google.com/file/d/1rAeGBJccEaZYFpPMlD5tb5TNjkaUqwq6/view?usp=drive_link).\n  2. Decompress and place in `./data/stats`.\n\n## 3. Run AlphaEdit on Llama3-8B\n- **Description**: Execute the evaluation script to edit Llama3-8B on the counterfact dataset.\n- **Command**:\n  ```bash\n  python3 -m experiments.evaluate --alg_name=AlphaEdit --model_name=meta-llama/Meta-Llama-3-8B-Instruct --hparams_fname=Llama3-8B.json --ds_name=mcf --dataset_size_limit=2000 --num_edits=100 --downstream_eval_steps=5\n  ```\n- **Output Location**: Results stored in `results/AlphaEdit/run_<run_id>/`.\n\n## 4. Summarize Results\n- **Description**: Aggregate results from multiple runs into a summary.\n- **Command**:\n  ```bash\n  python summarize.py --dir_name=AlphaEdit --runs=run_<run1>,run_<run2>\n  ```", "issue_title": "MQuAKE", "issue_body": "打扰您，非常感谢您的工作！\n请问您在附录中的Table5在MQuAKE上进行了实验，请问可以提供相关评测代码吗？\n非常感谢！\n\n祝好！", "choices": "(A) 1. Locate the `vrwkv6.py` file in the project directory (typically under `classification/mmcls_custom/models/backbones/`).\n2. Open the file and find the `T_MAX` parameter (around line 69).\n3. Rerun the test.\n4. Increase the value of `T_MAX` to a higher number (the exact value may vary based on your setup; the OP did not specify the exact value but confirmed it worked).\n5. Save the file. (B) 1. Locate the `vrwkv6.py` file in the project directory (typically under `classification/mmcls_custom/models/backbones/`).\n2. Open the file and find the `T_MAX` parameter (around line 69).\n3. Decrease the value of `T_MAX` to a lower number (the exact value may vary based on your setup; the OP did not specify the exact value but confirmed it worked).\n4. Save the file and rerun the test. (C) 1. Locate the `vrwkv6.py` file in the project directory (typically under `classification/mmcls_custom/models/backbones/`).\n2. Open the file and find the `T_MAX` parameter (around line 69).\n3. Increase the value of `T_MAX` to a higher number (the exact value may vary based on your setup; the OP did not specify the exact value but confirmed it worked).\n4. Rerun the test. (D) 1. Locate the `vrwkv6.py` file in the project directory (typically under `classification/mmcls_custom/models/backbones/`).\n2. Open the file and find the `T_MAX` parameter (around line 69).\n3. Save the file and rerun the test. (E) 1. Locate the `vrwkv6.py` file in the project directory (typically under `classification/mmcls_custom/models/backbones/`).\n2. Open the file and find the `T_MAX` parameter (around line 69).\n3. Increase the value of `T_MAX` to a higher number (the exact value may vary based on your setup; the OP did not specify the exact value but confirmed it worked).\n4. Save the file.\n5. Delete the `vrwkv6.py` file.\n6. Rerun the test. (F) 1. Locate the `vrwkv6.py` file in the project directory (typically under `classification/mmcls_custom/models/backbones/`).\n2. Open the file and find the `T_MAX` parameter (around line 69).\n3. Save the file and rerun the test.\n4. Increase the value of `T_MAX` to a higher number (the exact value may vary based on your setup; the OP did not specify the exact value but confirmed it worked). (G) 1. Locate the `vrwkv6.py` file in the project directory (typically under `classification/mmcls_custom/models/backbones/`).\n2. Open the file and find the `T_MAX` parameter (around line 69).\n3. Increase the value of `T_MAX` to a higher number (the exact value may vary based on your setup; the OP did not specify the exact value but confirmed it worked).\n4. Save the file and rerun the test. This should resolve the CUDA illegal memory access error.", "answer": "G"}
{"uuid": "2df5872e-43fc-44b1-b5eb-be993f75c55e", "setup_instruct": "# Vision-RWKV Deployment Plan\n\n## 1. Environment Setup\n- **Description**: Prepare the environment by installing necessary dependencies and cloning the repository.\n- **Commands**:\n  ```bash\n  git clone https://github.com/OpenGVLab/Vision-RWKV.git\n  cd Vision-RWKV\n  pip install -r requirements.txt  # Assuming requirements.txt exists\n  ```\n\n## 2. Model Download\n- **Description**: Download the desired pre-trained model(s) from HuggingFace based on the task (classification, detection, or segmentation).\n- **Commands** (Example for VRWKV-T classification):\n  ```bash\n  wget https://huggingface.co/OpenGVLab/Vision-RWKV/resolve/main/vrwkv_t_in1k_224.pth -P ./models/\n  ```\n\n## 3. Configuration Setup\n- **Description**: Ensure the corresponding configuration file is available for the selected model.\n- **Commands** (Example for VRWKV-T classification):\n  ```bash\n  # Verify config file exists at: classification/configs/vrwkv/vrwkv_tiny_8xb128_in1k.py\n  ```\n\n## 4. Inference Execution\n- **Description**: Run inference using the downloaded model and configuration.\n- **Commands** (Example for VRWKV-T classification):\n  ```bash\n  python tools/test.py classification/configs/vrwkv/vrwkv_tiny_8xb128_in1k.py ./models/vrwkv_t_in1k_224.pth\n  ```\n\n## 5. Validation\n- **Description**: Validate the output against expected metrics (e.g., Top-1 Accuracy for classification).\n- **Actions**: Compare the output logs with the metrics in the Model Zoo table.\n\n## 6. Advanced Tasks (Optional)\n- **Description**: For object detection or segmentation, follow similar steps with respective configs and models.\n- **Commands** (Example for VRWKV-T detection):\n  ```bash\n  wget https://huggingface.co/OpenGVLab/Vision-RWKV/resolve/main/mask_rcnn_vrwkv_adapter_tiny_fpn_1x_coco.pth -P ./models/\n  python tools/test.py detection/configs/mask_rcnn/mask_rcnn_vrwkv_adapter_tiny_fpn_1x_coco.py ./models/mask_rcnn_vrwkv_adapter_tiny_fpn_1x_coco.pth\n  ```\n\n## 7. CUDA Optimization (Optional)\n- **Description**: Use the updated CUDA code for improved flexibility (eliminates `T_MAX` hardcoding).\n- **Actions**: Navigate to the `cuda_new` folder and follow any additional instructions provided there.\n\n## 8. Citation and Compliance\n- **Description**: Ensure proper citation and license compliance if used in research or production.\n- **Actions**: Include the provided BibTeX entry in any publications and adhere to the Apache 2.0 license.", "issue_title": "CUDA error for the RWKV6 testing", "issue_body": "Hi, I did the following simple test for RWKV6, but it shows the following error. \r\n\r\nHave you met this before?\r\ncould you share me your detailed python,cuda,torch version?\r\n\r\n```python \r\nmodel = RWKV6_Model(\r\n      )\r\n\r\nx = torch.rand(10, in_channels, img_dim, img_dim).to(\"cuda\")\r\no = model(x)\r\no.backward(torch.randn_like(x))\r\n```\r\n\r\n\r\n\r\nVariable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.", "choices": "(A) 1. Verify the diffusers version is >= 0.35.0.dev0, installed from GitHub source if necessary.\n2. Update the nunchaku package to the latest version.\n3. Ensure the correct models are downloaded and placed in the correct paths:\n   - black-forest-labs/FLUX.1-Kontext-dev\n   - mit-han-lab/nunchaku-flux.1-kontext-dev/svdq-int4_r32-flux.1-kontext-dev.safetensors\n   - Optionally, mit-han-lab/nunchaku-t5/awq-int4-flux.1-t5xxl.safetensors if using quantized T.E.2. (B) 1. Ensure the correct models are downloaded and placed in the correct paths:\n   - black-forest-labs/FLUX.1-Kontext-dev\n   - mit-han-lab/nunchaku-flux.1-kontext-dev/svdq-int4_r32-flux.1-kontext-dev.safetensors\n   - Optionally, mit-han-lab/nunchaku-t5/awq-int4-flux.1-t5xxl.safetensors if using quantized T.E.2.\n2. Update the nunchaku package to the latest version.\n3. Verify the diffusers version is >= 0.35.0.dev0, installed from GitHub source if necessary. (C) 1. Ensure the correct models are downloaded and placed in the correct paths:\n   - black-forest-labs/FLUX.1-Kontext-dev\n   - mit-han-lab/nunchaku-flux.1-kontext-dev/svdq-int4_r32-flux.1-kontext-dev.safetensors\n   - Optionally, mit-han-lab/nunchaku-t5/awq-int4-flux.1-t5xxl.safetensors if using quantized T.E.2.\n2. Verify the diffusers version is >= 0.35.0.dev0, installed from GitHub source if necessary. (D) 1. Update the nunchaku package to the latest version.\n2. Ensure the correct models are downloaded and placed in the correct paths:\n   - black-forest-labs/FLUX.1-Kontext-dev\n   - mit-han-lab/nunchaku-flux.1-kontext-dev/svdq-int4_r32-flux.1-kontext-dev.safetensors\n   - Optionally, mit-han-lab/nunchaku-t5/awq-int4-flux.1-t5xxl.safetensors if using quantized T.E.2.\n3. Delete all downloaded models.\n4. Verify the diffusers version is >= 0.35.0.dev0, installed from GitHub source if necessary. (E) 1. Update the nunchaku package to the latest version.\n2. Ensure the correct models are downloaded and placed in the correct paths:\n   - black-forest-labs/FLUX.1-Kontext-dev\n   - mit-han-lab/nunchaku-flux.1-kontext-dev/svdq-int4_r32-flux.1-kontext-dev.safetensors\n   - Optionally, mit-han-lab/nunchaku-t5/awq-int4-flux.1-t5xxl.safetensors if using quantized T.E.2.\n3. Downgrade the diffusers version to 0.34.0.\n4. Verify the diffusers version is >= 0.35.0.dev0, installed from GitHub source if necessary. (F) 1. Update the nunchaku package to the latest version.\n2. Ensure the correct models are downloaded and placed in the correct paths:\n   - black-forest-labs/FLUX.1-Kontext-dev\n   - mit-han-lab/nunchaku-flux.1-kontext-dev/svdq-int4_r32-flux.1-kontext-dev.safetensors\n   - Optionally, mit-han-lab/nunchaku-t5/awq-int4-flux.1-t5xxl.safetensors if using quantized T.E.2.\n3. Verify the diffusers version is >= 0.35.0.dev0, installed from GitHub source if necessary. (G) 1. Update the nunchaku package to the latest version.\n2. Ensure the correct models are downloaded and placed in the correct paths:\n   - black-forest-labs/FLUX.1-Kontext-dev\n   - mit-han-lab/nunchaku-flux.1-kontext-dev/svdq-int4_r32-flux.1-kontext-dev.safetensors\n   - Optionally, mit-han-lab/nunchaku-t5/awq-int4-flux.1-t5xxl.safetensors if using quantized T.E.2.", "answer": "F"}
{"uuid": "7d0a0765-bfa2-496c-8442-564fcd62fe0b", "setup_instruct": "# Vision-RWKV Deployment Plan\n\n## 1. Environment Setup\n- **Description**: Prepare the environment by installing necessary dependencies and cloning the repository.\n- **Commands**:\n  ```bash\n  git clone https://github.com/OpenGVLab/Vision-RWKV.git\n  cd Vision-RWKV\n  pip install -r requirements.txt\n  ```\n\n## 2. Model Download\n- **Description**: Download the desired pre-trained model from the Model Zoo based on your task (classification, detection, or segmentation).\n- **Commands** (Example for VRWKV-T classification):\n  ```bash\n  wget https://huggingface.co/OpenGVLab/Vision-RWKV/resolve/main/vrwkv_t_in1k_224.pth -P ./checkpoints/\n  ```\n\n## 3. Configuration Setup\n- **Description**: Ensure the configuration file matches the downloaded model. Adjust paths if necessary.\n- **Commands** (Example for VRWKV-T classification):\n  ```bash\n  cp classification/configs/vrwkv/vrwkv_tiny_8xb128_in1k.py ./configs/\n  ```\n\n## 4. Inference Execution\n- **Description**: Run inference using the downloaded model and configuration.\n- **Commands** (Example for VRWKV-T classification):\n  ```bash\n  python tools/test.py \\\n    configs/vrwkv_tiny_8xb128_in1k.py \\\n    checkpoints/vrwkv_t_in1k_224.pth \\\n    --metrics accuracy\n  ```\n\n## 5. Validation\n- **Description**: Verify the output metrics (e.g., Top-1 Accuracy for classification) match the expected results from the Model Zoo.\n\n## 6. Advanced Usage (Optional)\n- **Description**: For tasks like detection or segmentation, use the corresponding configs and checkpoints.\n- **Commands** (Example for VRWKV-T detection):\n  ```bash\n  python tools/test.py \\\n    detection/configs/mask_rcnn/mask_rcnn_vrwkv_adapter_tiny_fpn_1x_coco.py \\\n    checkpoints/mask_rcnn_vrwkv_adapter_tiny_fpn_1x_coco.pth \\\n    --eval bbox segm\n  ```\n\n## 7. CUDA Optimization (Optional)\n- **Description**: Use the updated CUDA code for better performance (if applicable).\n- **Commands**:\n  ```bash\n  cd cuda_new\n  make\n  cd ..\n  ```\n\n## 8. Citation & Compliance\n- **Description**: Ensure proper citation and license compliance if used in research or production.", "issue_title": "What parameters are important to tune when using VRWKV block as drop-in replacement for attention?", "issue_body": "Currently, my model is performing worse when using VRWKV block as drop-in replacement for attention. Do you have any suggestion on what are the important parameters to tune?", "choices": "(A) 1. Install the `nunchaku` package correctly by following the installation instructions or using the provided workflow.\n2. If you do not want to use PuLID, replace the file `custom_nodes/ComfyUI-nunchaku/wrappers/flux.py` with the version provided in the dev branch.\n3. For PuLID usage, ensure all dependencies are correctly installed as per issue #392.\n4. Verify the installation by checking the logs and ensuring no import errors remain. (B) 1. Install the `nunchaku` package correctly by following the installation instructions or using the provided workflow.\n2. If you encounter an `insightface` dependency error, install `insightface` using the following commands in the ComfyUI environment:\n   - `pip install insightface`\n   - `pip install onnxruntime-gpu`\n   - `pip install insightface==0.7.3`\n3. If you do not want to use PuLID, replace the file `custom_nodes/ComfyUI-nunchaku/wrappers/flux.py` with the version provided in the dev branch.\n4. For PuLID usage, ensure all dependencies are correctly installed as per issue #392. (C) 1. Install the `nunchaku` package correctly by following the installation instructions or using the provided workflow.\n2. If you encounter an `insightface` dependency error, install `insightface` using the following commands in the ComfyUI environment:\n   - `pip install insightface`\n   - `pip install onnxruntime-gpu`\n   - `pip install insightface==0.7.3`\n3. If you do not want to use PuLID, replace the file `custom_nodes/ComfyUI-nunchaku/wrappers/flux.py` with the version provided in the dev branch.\n4. For PuLID usage, ensure all dependencies are correctly installed as per issue #392.\n5. Verify the installation by checking the logs and ensuring no import errors remain. (D) 1. Install the `nunchaku` package correctly by following the installation instructions or using the provided workflow.\n2. If you encounter an `insightface` dependency error, install `insightface` using the following commands in the ComfyUI environment:\n   - `pip install insightface`\n   - `pip install onnxruntime-gpu`\n   - `pip install insightface==0.7.3`\n3. Uninstall `onnxruntime-gpu` using `pip uninstall onnxruntime-gpu -y`.\n4. If you do not want to use PuLID, replace the file `custom_nodes/ComfyUI-nunchaku/wrappers/flux.py` with the version provided in the dev branch.\n5. For PuLID usage, ensure all dependencies are correctly installed as per issue #392.\n6. Verify the installation by checking the logs and ensuring no import errors remain. (E) 1. Install the `nunchaku` package correctly by following the installation instructions or using the provided workflow.\n2. If you do not want to use PuLID, replace the file `custom_nodes/ComfyUI-nunchaku/wrappers/flux.py` with the version provided in the dev branch.\n3. If you encounter an `insightface` dependency error, install `insightface` using the following commands in the ComfyUI environment:\n   - `pip install insightface`\n   - `pip install onnxruntime-gpu`\n   - `pip install insightface==0.7.3`\n4. For PuLID usage, ensure all dependencies are correctly installed as per issue #392.\n5. Verify the installation by checking the logs and ensuring no import errors remain. (F) 1. Install the `nunchaku` package correctly by following the installation instructions or using the provided workflow.\n2. If you encounter an `insightface` dependency error, install `insightface` using the following commands in the ComfyUI environment:\n   - `pip install insightface`\n   - `pip install onnxruntime-gpu`\n   - `pip install insightface==0.7.3`\n3. If you do not want to use PuLID, replace the file `custom_nodes/ComfyUI-nunchaku/wrappers/flux.py` with the version provided in the dev branch.\n4. Verify the installation by checking the logs and ensuring no import errors remain.\n5. For PuLID usage, ensure all dependencies are correctly installed as per issue #392. (G) 1. Install the `nunchaku` package correctly by following the installation instructions or using the provided workflow.\n2. If you encounter an `insightface` dependency error, install `insightface` using the following commands in the ComfyUI environment:\n   - `pip install insightface`\n   - `pip install onnxruntime-gpu`\n   - `pip install insightface==0.7.3`\n3. Delete the `custom_nodes/ComfyUI-nunchaku` directory using `rm -rf custom_nodes/ComfyUI-nunchaku`.\n4. If you do not want to use PuLID, replace the file `custom_nodes/ComfyUI-nunchaku/wrappers/flux.py` with the version provided in the dev branch.\n5. For PuLID usage, ensure all dependencies are correctly installed as per issue #392.\n6. Verify the installation by checking the logs and ensuring no import errors remain.", "answer": "C"}
{"uuid": "74aefac4-ead1-4a2d-a76b-058b1a452100", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- Ensure NVIDIA GPU with CUDA support (RTX 20-series or newer recommended)\n- Python 3.8+ environment\n- [Optional] Join community channels (Slack/Discord/WeChat) for support\n\n## 2. Installation\n```bash\n# Clone the repository\ngit clone https://github.com/nunchaku-tech/nunchaku.git\ncd nunchaku\n\n# Create and activate virtual environment\npython -m venv nunchaku-env\nsource nunchaku-env/bin/activate  # Linux/Mac\n.\\nunchaku-env\\Scripts\\activate  # Windows\n\n# Install core dependencies\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\npip install -r requirements.txt\n```\n\n## 3. Model Setup\n```bash\n# Download pre-quantized models (example for FLUX.1)\npython -m nunchaku.download --model flux.1-dev --precision int4\n```\n\n## 4. Basic Inference Test\n```bash\n# Run example script\npython examples/flux.1-dev-basic.py \\\n    --prompt \"A scenic landscape\" \\\n    --output result.png\n```\n\n## 5. Advanced Features\n```bash\n# Multi-LoRA example\npython examples/flux.1-dev-multiple-lora.py \\\n    --prompt \"Portrait with artistic style\" \\\n    --lora weights/style1.safetensors+weights/style2.safetensors \\\n    --output styled_portrait.png\n\n# ControlNet example\npython examples/flux.1-dev-controlnet-union-pro.py \\\n    --input sketch.jpg \\\n    --prompt \"Colorized sketch of a castle\" \\\n    --output colorized.png\n```\n\n## 6. ComfyUI Integration\n```bash\n# Install ComfyUI plugin\ngit clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\ncp -r ComfyUI-nunchaku /path/to/ComfyUI/custom_nodes/\n```\n\n## 7. Performance Optimization\n```bash\n# Enable FP16 attention (RTX 30/40-series)\npython your_script.py --use-fp16-attention\n\n# Enable First-Block Cache for memory optimization\npython your_script.py --fbcache\n```\n\n## 8. Verification\n- Check output images in specified directory\n- Monitor GPU utilization with `nvidia-smi`\n- Compare speed with baseline models\n\n## 9. Next Steps\n- Explore [custom quantization](https://github.com/nunchaku-tech/deepcompressor)\n- Try [Gradio demos](https://github.com/nunchaku-tech/nunchaku/tree/main/app)\n- Refer to [API docs](https://nunchaku.tech/docs/nunchaku/python_api/nunchaku.html) for advanced usage", "issue_title": "Flux 1 dev kontext nunchaku model issue", "issue_body": "I've tried a simple style change for an image with the nunchaku model, and the results were an entirely different image, which does not resemble the input image.\nThe full flux 1 dev kontext model did, however, convert the input image correctly.\nDon't get me wrong, I'm super grateful for Nunchaku and having the Flux Kontext implemented so quickly. Nunchaku is my favorite tool and all I use.\nCould this be something lacking in the Nunchaku model, or does the prompt need to be altered?\nFor reference, the prompt used was turned into a realistic photo\nWorked on the Flux context full model, but not nunchaku.", "choices": "(A) 1. Try using the alternative solution provided by @nitinmukesh: https://github.com/newgenai79/sd-diffuser-webui. 2. Refer to the SVDQuant / Nunchaku tab in the provided solution. 3. Ensure all required packages are installed from the requirements.txt file. 4. Use Python 3.12 without modifying the whl URL. (B) 1. Try using the alternative solution provided by @nitinmukesh: https://github.com/newgenai79/sd-diffuser-webui. 2. Refer to the SVDQuant / Nunchaku tab in the provided solution. 3. If using Python 3.12, modify the whl URL accordingly for pip installation. 4. Ensure all required packages are installed from the requirements.txt file. (C) 1. Ensure all required packages are installed from the requirements.txt file. 2. Try using the alternative solution provided by @nitinmukesh: https://github.com/newgenai79/sd-diffuser-webui. 3. Refer to the SVDQuant / Nunchaku tab in the provided solution. 4. If using Python 3.12, modify the whl URL accordingly for pip installation. (D) 1. Try using the alternative solution provided by @nitinmukesh: https://github.com/newgenai79/sd-diffuser-webui. 2. Ensure all required packages are installed from the requirements.txt file. 3. If using Python 3.12, modify the whl URL accordingly for pip installation. (E) 1. Try using the alternative solution provided by @nitinmukesh: https://github.com/newgenai79/sd-diffuser-webui. 2. Refer to the SVDQuant / Nunchaku tab in the provided solution. 3. If using Python 3.12, modify the whl URL accordingly for pip installation. (F) 1. Try using the alternative solution provided by @nitinmukesh: https://github.com/newgenai79/sd-diffuser-webui. 2. Refer to the SVDQuant / Nunchaku tab in the provided solution. 3. Ensure all required packages are installed from the requirements.txt file. 4. If using Python 3.12, modify the whl URL accordingly for pip installation. (G) 1. Try using the alternative solution provided by @nitinmukesh: https://github.com/newgenai79/sd-diffuser-webui. 2. Refer to the SVDQuant / Nunchaku tab in the provided solution. 3. Delete the requirements.txt file. 4. Ensure all required packages are installed from the requirements.txt file. 5. If using Python 3.12, modify the whl URL accordingly for pip installation.", "answer": "F"}
{"uuid": "b49ba3a9-d47e-4029-9c32-755e6f7437dc", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n```bash\n# Clone the repository\ngit clone https://github.com/nunchaku-tech/nunchaku.git\ncd nunchaku\n\n# Install dependencies (use conda or venv)\nconda create -n nunchaku python=3.10\nconda activate nunchaku\npip install -r requirements.txt\n\n# Install Nunchaku core\npip install --pre --extra-index-url https://pypi.nunchaku.tech/simple/ nunchaku\n```\n\n## 3. Model Setup\n```bash\n# Download pre-quantized models (e.g., FLUX.1)\nwget https://huggingface.co/nunchaku-tech/flux.1-4bit/resolve/main/flux.1-4bit.nk\n```\n\n## 4. Basic Inference\n```bash\n# Run text-to-image generation\npython examples/flux.1-basic.py \\\n  --model flux.1-4bit.nk \\\n  --prompt \"A cat wearing sunglasses\"\n```\n\n## 5. Advanced Features\n### Multi-LoRA Support\n```bash\npython examples/flux.1-dev-multiple-lora.py \\\n  --model flux.1-4bit.nk \\\n  --lora lora1.safetensors lora2.safetensors\n```\n\n### ControlNet Integration\n```bash\npython examples/flux.1-dev-controlnet-union-pro.py \\\n  --model flux.1-4bit.nk \\\n  --controlnet canny-controlnet.nk\n```\n\n## 6. ComfyUI Integration\n```bash\n# Install ComfyUI plugin\ngit clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git \\\n  /path/to/ComfyUI/custom_nodes/\n```\n\n## 7. Performance Tuning\n```bash\n# Enable FP16 attention and cache\nexport NUNCHAKU_USE_FP16_ATTN=1\nexport NUNCHAKU_USE_FB_CACHE=1\n```\n\n## 8. Verification\n```bash\n# Check installation\npython -c \"import nunchaku; print(nunchaku.__version__)\"\n# Expected output: v0.3.0 or higher\n```\n\n## 9. Troubleshooting\n- For GPU compatibility issues, see `examples/flux.1-dev-turing.py`\n- Memory optimization: Use `--cpu-offload` flag for <8GB GPUs", "issue_title": "[Bug] CUDA out of memory (flux demo app) - 12GB GPU", "issue_body": "### Checklist\n\n- [x] 1. I have searched for related issues and FAQs (https://github.com/mit-han-lab/nunchaku/blob/main/docs/faq.md) but was unable to find a solution.\n- [x] 2. The issue persists in the latest version.\n- [x] 3. Please note that without environment information and a minimal reproducible example, it will be difficult for us to reproduce and address the issue, which may delay our response.\n- [x] 4. If your report is a question rather than a bug, please submit it as a discussion at https://github.com/mit-han-lab/nunchaku/discussions/new/choose. Otherwise, this issue will be closed.\n- [x] 5. If this is related to ComfyUI, please report it at https://github.com/mit-han-lab/ComfyUI-nunchaku/issues.\n- [x] 6. I will do my best to describe the issue in English.\n\n### Describe the Bug\n\nnunchaku  0.3.1+torch2.7\n\nI run the demo application like this:\n~~~\ncd app/flux.1/t2i\npython run_gradio.py --use-qencoder --no-safety-checker --model dev\nor\npython run_gradio.py --use-qencoder --no-safety-checker --model schnell\n~~~\n\nI have a 12GB GPU.\n\nI get an error:\n~~~\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 11.59 GiB of which 521.62 MiB is free. Including non-PyTorch memory, this process has 10.28 GiB memory in use.\n~~~\n\nBut I remember that on the previous version 2.x I could use the demo without this problem. What has changed in the demo? Can you add optimization for 12GB GPU?\n\n### Environment\n\nUbuntu 24.04, torch==2.7.0, CUDA 12.0 or CUDA 12.8\n\n### Reproduction Steps\n\n~~~\ncd app/flux.1/t2i\npython run_gradio.py --use-qencoder --no-safety-checker --model dev\nor\npython run_gradio.py --use-qencoder --no-safety-checker --model schnell\n~~~", "choices": "(A) 1. Test the workflow with the new T5 model to confirm it resolves the 'This kernel is not supported on your GPU' error.\n2. Identify that the issue is related to the T5 model's compatibility with Turing architecture GPUs (e.g., RTX 2060).\n3. Replace the current T5 model with a 'regular' FP16/FP8/GGUF version of T5 that is compatible with Turing architecture. (B) 1. Identify that the issue is related to the T5 model's compatibility with Turing architecture GPUs (e.g., RTX 2060).\n2. Downgrade the GPU drivers to an older version.\n3. Replace the current T5 model with a 'regular' FP16/FP8/GGUF version of T5 that is compatible with Turing architecture.\n4. Test the workflow with the new T5 model to confirm it resolves the 'This kernel is not supported on your GPU' error. (C) 1. Replace the current T5 model with a 'regular' FP16/FP8/GGUF version of T5 that is compatible with Turing architecture.\n2. Identify that the issue is related to the T5 model's compatibility with Turing architecture GPUs (e.g., RTX 2060).\n3. Test the workflow with the new T5 model to confirm it resolves the 'This kernel is not supported on your GPU' error. (D) 1. Replace the current T5 model with a 'regular' FP16/FP8/GGUF version of T5 that is compatible with Turing architecture.\n2. Test the workflow with the new T5 model to confirm it resolves the 'This kernel is not supported on your GPU' error. (E) 1. Identify that the issue is related to the T5 model's compatibility with Turing architecture GPUs (e.g., RTX 2060).\n2. Replace the current T5 model with a 'regular' FP16/FP8/GGUF version of T5 that is compatible with Turing architecture.\n3. Test the workflow with the new T5 model to confirm it resolves the 'This kernel is not supported on your GPU' error.\n4. Revert to the original T5 model to \"compare results.\" (F) 1. Identify that the issue is related to the T5 model's compatibility with Turing architecture GPUs (e.g., RTX 2060).\n2. Replace the current T5 model with a 'regular' FP16/FP8/GGUF version of T5 that is compatible with Turing architecture. (G) 1. Identify that the issue is related to the T5 model's compatibility with Turing architecture GPUs (e.g., RTX 2060).\n2. Replace the current T5 model with a 'regular' FP16/FP8/GGUF version of T5 that is compatible with Turing architecture.\n3. Test the workflow with the new T5 model to confirm it resolves the 'This kernel is not supported on your GPU' error.", "answer": "G"}
{"uuid": "a33f59f7-95ac-4c0c-8b7d-6b3848b7860c", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- Ensure you have a compatible NVIDIA GPU (RTX 20-series or newer recommended)\n- Install Python 3.8 or later\n- Install CUDA Toolkit (version 11.7 or later)\n\n## 2. Installation\n```bash\n# Clone the repository\ngit clone https://github.com/nunchaku-tech/nunchaku.git\ncd nunchaku\n\n# Create and activate a virtual environment (optional but recommended)\npython -m venv venv\nsource venv/bin/activate  # Linux/MacOS\nvenv\\Scripts\\activate     # Windows\n\n# Install dependencies\npip install -r requirements.txt\n\n# Install Nunchaku package\npip install .\n```\n\n## 3. Verify Installation\n```bash\n# Check if Nunchaku is properly installed\npython -c \"import nunchaku; print(nunchaku.__version__)\"\n```\n\n## 4. Download Pre-quantized Models\n```bash\n# Example for FLUX.1 model\nwget https://huggingface.co/nunchaku-tech/nunchaku-flux1/resolve/main/flux1-4bit.safetensors\n```\n\n## 5. Run Inference\n```bash\n# Basic text-to-image generation example\npython examples/flux.1-basic.py \\\n    --model-path ./flux1-4bit.safetensors \\\n    --prompt \"A beautiful landscape\" \\\n    --output ./output.png\n```\n\n## 6. Advanced Features\n```bash\n# Run with ControlNet support\npython examples/flux.1-dev-controlnet-union-pro.py \\\n    --model-path ./flux1-4bit.safetensors \\\n    --controlnet-path ./controlnet-union-pro.safetensors \\\n    --prompt \"A futuristic city\" \\\n    --control-image ./sketch.png \\\n    --output ./controlled_output.png\n\n# Run with multiple LoRAs\npython examples/flux.1-dev-multiple-lora.py \\\n    --model-path ./flux1-4bit.safetensors \\\n    --loras ./style1.safetensors ./style2.safetensors \\\n    --prompt \"Portrait in style1 and style2\" \\\n    --output ./multi_lora_output.png\n```\n\n## 7. ComfyUI Integration (Optional)\n```bash\n# Install ComfyUI plugin\ngit clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\ncd ComfyUI-nunchaku\npip install -r requirements.txt\n```\n\n## 8. Quantize Custom Models (Advanced)\nRefer to [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization instructions.\n\n## 9. Troubleshooting\n- For GPU memory issues, try reducing batch size or enabling CPU offloading\n- Check the [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html) for common solutions\n- Join the [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for community support", "issue_title": "fix: txt2_img pipe and controlnet pipe share components", "issue_body": "Closes #437\r\n\r\nIssue\r\n-Previously `FluxPipeline` (txt2img) and `FluxControlNetPipeline` shared the same `FluxTransformer2DModel` instance, so calling `apply_cache_on_pipe(pipe_cn, …)` also mutated the shared transformer. This caused two kinds of failures:\r\n\r\n-When you only patched `pipe_cn`, then later ran `pipe(...)`, its transformer was already in “cached” mode but no `cache_context` was set.\r\n\r\n- Both pipelines were given the same `transformer` object at construction time.\r\n- `apply_cache_on_pipe` modifies the transformer in‐place, installing a forward‐wrapper and setting `_is_cached = True` without preserving the original blocks list in an instance attribute.\r\n\r\n- In `apply_cache_on_transformer` (flux.py) and its Sana counterpart (sana.py), the `new_forward` wrapper now first checks cache context.", "choices": "(A) 1. Install a pre-built wheel for Python 3.12 of `insightface` from https://github.com/cobanov/insightface_windows.\n2. Ensure that `triton-windows` is not installed, as it conflicts with Nunchaku 0.3.0. If installed, uninstall it using `python_embeded\\python.exe -m pip uninstall triton-windows`.\n3. Use the main branch of ComfyUI-nunchaku instead of the dev branch.\n4. If using the ComfyUI-nunchaku dev branch, pull the latest changes to your `custom_nodes` directory to make PuLID-related nodes optional.\n5. Use a separate ComfyUI version for running Nunchaku 0.3.0 if conflicts persist. (B) 1. Install a pre-built wheel for Python 3.12 of `insightface` from https://github.com/cobanov/insightface_windows.\n2. If using the ComfyUI-nunchaku dev branch, pull the latest changes to your `custom_nodes` directory to make PuLID-related nodes optional.\n3. Use a separate ComfyUI version for running Nunchaku 0.3.0 if conflicts persist. (C) 1. Install a pre-built wheel for Python 3.12 of `insightface` from https://github.com/cobanov/insightface_windows.\n2. Ensure that `triton-windows` is not installed, as it conflicts with Nunchaku 0.3.0. If installed, uninstall it using `python_embeded\\python.exe -m pip uninstall triton-windows`.\n3. If using the ComfyUI-nunchaku dev branch, pull the latest changes to your `custom_nodes` directory to make PuLID-related nodes optional.\n4. Use a separate ComfyUI version for running Nunchaku 0.3.0 if conflicts persist. (D) 1. Ensure that `triton-windows` is not installed, as it conflicts with Nunchaku 0.3.0. If installed, uninstall it using `python_embeded\\python.exe -m pip uninstall triton-windows`.\n2. If using the ComfyUI-nunchaku dev branch, pull the latest changes to your `custom_nodes` directory to make PuLID-related nodes optional.\n3. Use a separate ComfyUI version for running Nunchaku 0.3.0 if conflicts persist. (E) 1. Install a pre-built wheel for Python 3.12 of `insightface` from https://github.com/cobanov/insightface_windows.\n2. Install `triton-windows` using `python_embeded\\python.exe -m pip install triton-windows`.\n3. Ensure that `triton-windows` is not installed, as it conflicts with Nunchaku 0.3.0. If installed, uninstall it using `python_embeded\\python.exe -m pip uninstall triton-windows`.\n4. If using the ComfyUI-nunchaku dev branch, pull the latest changes to your `custom_nodes` directory to make PuLID-related nodes optional.\n5. Use a separate ComfyUI version for running Nunchaku 0.3.0 if conflicts persist. (F) 1. If using the ComfyUI-nunchaku dev branch, pull the latest changes to your `custom_nodes` directory to make PuLID-related nodes optional.\n2. Install a pre-built wheel for Python 3.12 of `insightface` from https://github.com/cobanov/insightface_windows.\n3. Ensure that `triton-windows` is not installed, as it conflicts with Nunchaku 0.3.0. If installed, uninstall it using `python_embeded\\python.exe -m pip uninstall triton-windows`.\n4. Use a separate ComfyUI version for running Nunchaku 0.3.0 if conflicts persist. (G) 1. Install a pre-built wheel for Python 3.12 of `insightface` from https://github.com/cobanov/insightface_windows.\n2. Use a separate ComfyUI version for running Nunchaku 0.3.0 if conflicts persist.\n3. Ensure that `triton-windows` is not installed, as it conflicts with Nunchaku 0.3.0. If installed, uninstall it using `python_embeded\\python.exe -m pip uninstall triton-windows`.\n4. If using the ComfyUI-nunchaku dev branch, pull the latest changes to your `custom_nodes` directory to make PuLID-related nodes optional.", "answer": "C"}
{"uuid": "78e61334-d0f7-4297-bf97-70ae8dabeab8", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face](https://huggingface.co/nunchaku-tech) or [ModelScope](https://modelscope.cn/organization/nunchaku-tech).  \n   Example:  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux1-dev\n   ```\n\n## 4. Basic Usage\n1. **Run Inference Script**  \n   Example for FLUX.1-Kontext:  \n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path ./nunchaku-flux1-dev\n   ```\n\n2. **Enable Advanced Features**  \n   - For multi-LoRA:  \n     ```bash\n     python examples/flux.1-dev-multiple-lora.py\n     ```\n   - For ControlNet:  \n     ```bash\n     python examples/flux.1-dev-controlnet-union-pro.py\n     ```\n\n## 5. ComfyUI Integration\n1. **Install ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku\n   pip install -r requirements.txt\n   ```\n\n2. **Launch ComfyUI**  \n   ```bash\n   python main.py\n   ```\n\n## 6. Custom Model Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n\n2. **Quantize Custom Model**  \n   Follow instructions in [DeepCompressor README](https://github.com/nunchaku-tech/deepcompressor).\n\n## 7. Verification\n1. **Check GPU Utilization**  \n   ```bash\n   nvidia-smi\n   ```\n\n2. **Validate Output Quality**  \n   Compare results with [demo outputs](https://svdquant.mit.edu).\n\n## 8. Troubleshooting\n- **Memory Issues**: Enable CPU offloading:  \n  ```python\n  from nunchaku import enable_cpu_offload\n  enable_cpu_offload()\n  ```\n- **Performance Tuning**: Refer to [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html).\n\n## 9. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for assistance.", "issue_title": "[Feature] On-the-fly LoRA Conversion is Slow", "issue_body": "我使用的腾讯云上的L20服务器，nunchaku版本是0.3.0，cuda12.8 pytorch2.7.0 python 3.11.12, ComfyUI版本是0.3.40，[ComfyUI-nunchaku]插件版本也是0.3.0，现在的问题是，我正常使用nunchaku跑FLUX文生图，fill抹除重绘，readux这些都正常，就是无法正常使用lora，加载lora运行时，后台会自动开始进行转换，运行要等好久，虽然转换后可以起效果，但是如果我把Lora加载节点忽略重开，或者替换别的lora再换回来，也需要重新转换，", "choices": "(A) 1. If encountering import errors with the wheel, compile the package from local source code.\n2. Update to the latest released version of nunchaku (v0.3.0).\n3. Verify that the memory usage has dropped and the system no longer gets stuck after multiple inference runs. (B) 1. Update to the latest released version of nunchaku (v0.3.0).\n2. Verify that the memory usage has dropped and the system no longer gets stuck after multiple inference runs. (C) 1. Update to the latest released version of nunchaku (v0.3.0).\n2. If encountering import errors with the wheel, compile the package from local source code. (D) 1. Verify that the memory usage has dropped and the system no longer gets stuck after multiple inference runs.\n2. Update to the latest released version of nunchaku (v0.3.0).\n3. If encountering import errors with the wheel, compile the package from local source code. (E) 1. Update to the latest released version of nunchaku (v0.3.0).\n2. If encountering import errors with the wheel, compile the package from local source code.\n3. Skip verification to save time. (F) 1. Update to the latest released version of nunchaku (v0.3.0).\n2. Downgrade nunchaku to v0.2.0 to avoid compatibility issues.\n3. If encountering import errors with the wheel, compile the package from local source code.\n4. Verify that the memory usage has dropped and the system no longer gets stuck after multiple inference runs. (G) 1. Update to the latest released version of nunchaku (v0.3.0).\n2. If encountering import errors with the wheel, compile the package from local source code.\n3. Verify that the memory usage has dropped and the system no longer gets stuck after multiple inference runs.", "answer": "G"}
{"uuid": "8ba253d8-464e-47c5-93a8-6165bae59da1", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- Ensure you have a compatible NVIDIA GPU (RTX 20-series or newer recommended)\n- Install Python 3.8 or later\n- Install CUDA toolkit (version 11.8 or later recommended)\n\n## 2. Installation\n1. **Clone the Nunchaku repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Create and activate a Python virtual environment**  \n   ```bash\n   python -m venv nunchaku-env\n   source nunchaku-env/bin/activate  # Linux/MacOS\n   .\\nunchaku-env\\Scripts\\activate  # Windows\n   ```\n\n3. **Install dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **Install Nunchaku package**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download pre-quantized models**  \n   Visit [Hugging Face models](https://huggingface.co/nunchaku-tech) or [ModelScope](https://modelscope.cn/organization/nunchaku-tech) to download desired models.\n\n2. **Place models in correct directory**  \n   ```bash\n   mkdir -p models/flux.1\n   # Place downloaded model files in models/flux.1/\n   ```\n\n## 4. Basic Usage\n1. **Run inference with FLUX.1 model**  \n   ```bash\n   python examples/flux.1-dev.py --model-path models/flux.1/ --prompt \"A scenic landscape\"\n   ```\n\n2. **Enable ControlNet support**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --model-path models/flux.1/ --prompt \"A scenic landscape\" --control-image input.png\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model-path models/flux.1/ --prompt \"A scenic landscape\" --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n2. **Low-memory inference**  \n   ```bash\n   python examples/flux.1-dev-low-memory.py --model-path models/flux.1/ --prompt \"A scenic landscape\" --cpu-offload\n   ```\n\n## 6. ComfyUI Integration\n1. **Install ComfyUI plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku\n   pip install -r requirements.txt\n   ```\n\n2. **Follow ComfyUI setup instructions**  \n   Refer to [ComfyUI-nunchaku](https://github.com/nunchaku-tech/ComfyUI-nunchaku) for detailed setup.\n\n## 7. Verification\n1. **Check installation**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.__version__)\"\n   ```\n\n2. **Run test script**  \n   ```bash\n   python tests/test_basic.py\n   ```\n\n## 8. Troubleshooting\n- For GPU compatibility issues, refer to [examples/flux.1-dev-turing.py](examples/flux.1-dev-turing.py) for 20-series GPU support\n- For memory issues, enable CPU offloading or reduce batch size\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for community support", "issue_title": "Missing nodes.", "issue_body": "I installed nunchaku-0.3.0+torch2.8-cp312-cp312-win_amd64.whl then used ComfyUI Manager to install / update nunchaku nodes. But loading a workflow still gave missing node error. Looking at the node in ComfyUI Manager, it said imported failed with following errors:\n\n```\nError message occurred while importing the 'ComfyUI-nunchaku' module.\n\nTraceback (most recent call last):\n  File \"Q:\\ComfyUI_windows_portable-nightly\\ComfyUI\\nodes.py\", line 2124, in load_custom_node\n    module_spec.loader.exec_module(module)\n  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"Q:\\ComfyUI_windows_portable-nightly\\ComfyUI\\custom_nodes\\ComfyUI-nunchaku\\__init__.py\", line 2, in <module>\n    from .nodes.lora import NunchakuFluxLoraLoader\n  File \"Q:\\ComfyUI_windows_portable-nightly\\ComfyUI\\custom_nodes\\ComfyUI-nunchaku\\nodes\\lora\\__init__.py\", line 1, in <module>\n    from .flux import NunchakuFluxLoraLoader\n  File \"Q:\\ComfyUI_windows_portable-nightly\\ComfyUI\\custom_nodes\\ComfyUI-nunchaku\\nodes\\lora\\flux.py\", line 6, in <module>\n    from nunchaku.lora.flux import to_diffusers\n  File \"Q:\\ComfyUI_windows_portable-nightly\\python_embeded\\Lib\\site-packages\\nunchaku\\__init__.py\", line 1, in <module>\n    from .models import NunchakuFluxTransformer2dModel, NunchakuSanaTransformer2DModel, NunchakuT5EncoderModel\n  File \"Q:\\ComfyUI_windows_portable-nightly\\python_embeded\\Lib\\site-packages\\nunchaku\\models\\__init__.py\", line 1, in <module>\n    from .text_encoders.t5_encoder import NunchakuT5EncoderModel\n  File \"Q:\\ComfyUI_windows_portable-nightly\\python_embeded\\Lib\\site-packages\\nunchaku\\models\\text_encoders\\t5_encoder.py\", line 12, in <module>\n    from .linear import W4Linear\n  File \"Q:\\ComfyUI_windows_portable-nightly\\python_embeded\\Lib\\site-packages\\nunchaku\\models\\text_encoders\\linear.py\", line 7, in <module>\n    from ..._C.ops import gemm_awq, gemv_awq\nImportError: DLL load failed while importing _C: The specified procedure could not be found.\n```\nI'm using Windows 10, Cuda Toolkit 12.8, ComfyUI portable with python 3.12 and pytorch 2.8 dev. Nunchaku 0.2.0 works without any issue.", "choices": "(A) 1. Install GCC and G++ version 11 in your conda environment using the command: `conda install -c conda-forge gxx=11 gcc=11`.\n2. Rebuild the nunchaku wheels after installing the specified GCC and G++ versions.\n3. Uninstall GCC and G++ using the command: `conda remove gxx gcc`.\n4. Verify that the import error is resolved by importing the library again. (B) 1. Install GCC and G++ version 11 in your conda environment using the command: `conda install -c conda-forge gxx=11 gcc=11`.\n2. Verify that the import error is resolved by importing the library again.\n3. Rebuild the nunchaku wheels after installing the specified GCC and G++ versions. (C) 1. Install GCC and G++ version 11 in your conda environment using the command: `conda install -c conda-forge gxx=11 gcc=11`.\n2. Verify that the import error is resolved by importing the library again. (D) 1. Rebuild the nunchaku wheels after installing the specified GCC and G++ versions.\n2. Verify that the import error is resolved by importing the library again. (E) 1. Install GCC and G++ version 11 in your conda environment using the command: `conda install -c conda-forge gxx=11 gcc=11`.\n2. Downgrade GCC and G++ to version 9 using the command: `conda install -c conda-forge gxx=9 gcc=9`.\n3. Rebuild the nunchaku wheels after installing the specified GCC and G++ versions.\n4. Verify that the import error is resolved by importing the library again. (F) 1. Install GCC and G++ version 11 in your conda environment using the command: `conda install -c conda-forge gxx=11 gcc=11`.\n2. Rebuild the nunchaku wheels after installing the specified GCC and G++ versions.\n3. Verify that the import error is resolved by importing the library again. (G) 1. Rebuild the nunchaku wheels after installing the specified GCC and G++ versions.\n2. Install GCC and G++ version 11 in your conda environment using the command: `conda install -c conda-forge gxx=11 gcc=11`.\n3. Verify that the import error is resolved by importing the library again.", "answer": "F"}
{"uuid": "724df8f0-1318-4c8c-b298-4f432c594c52", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_compatibility())\"\n   ```\n\n2. **Run Basic Test**  \n   ```bash\n   python examples/flux.1-dev-basic.py\n   ```\n\n## 4. Model Deployment\n1. **Download Pre-Quantized Models**  \n   ```bash\n   huggingface-cli download nunchaku-tech/nunchaku-flux --local-dir ./models\n   ```\n\n2. **Run Inference**  \n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path ./models/flux.1-4bit.safetensors\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --controlnet canny\n   ```\n\n## 6. Custom Model Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor && pip install .\n   ```\n\n2. **Quantize Custom Model**  \n   ```bash\n   python quantize.py --model your_model.ckpt --output quantized_model.safetensors\n   ```\n\n## 7. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI**  \n   ```bash\n   cd /path/to/ComfyUI && python main.py\n   ```\n\n## 8. Monitoring & Optimization\n1. **Enable First-Block Cache**  \n   Add `--use-fb-cache` flag to inference scripts.\n\n2. **Profile Performance**  \n   ```bash\n   nvprof python examples/flux.1-dev-benchmark.py\n   ```\n\n## 9. Troubleshooting\n- **Memory Issues**: Use `--cpu-offload` flag for low-memory GPUs.\n- **Performance Tuning**: Adjust `--batch-size` and `--rank` parameters.", "issue_title": "feat: enable IP-Adapter (XLabs-AI/flux-ip-adapter-v2) support", "issue_body": "This change allows FluxModel to apply the IP-Adapter for end-to-end inference.\r\n\r\n* Hooked up `XLabs-AI/flux-ip-adapter-v2` in `FluxModel::forward_ip_adapter` so that inference runs through the adapter logic.\r\n* Verified that behavior matches the existing FluxModel + adapter reference. (I think PuLID performs better.)\r\n\r\nIssues\r\n\r\n1. We currently bring the Query back to Python and call `F.scaled_dot_product_attention` because there’s no C++ kernel with masking support. Those lines are commented out for now.\r\n If you’re okay modifying the kernel, we just need to implement `scaled_dot_product_attention` there.\r\n Otherwise, I can remove the commented code entirely.\r\n \r\n2. Added `test/flux.1-dev-IP-adapter.py`, modeled after `test/flux.1-dev-pulid.py`.\r\n Please let me know if you’d like additional evaluation metrics or alternative test cases.", "choices": "(A) 1. Uninstall the facexlib package by running the command: `pip uninstall facexlib`.\n2. Reinstall the facexlib package by running the command: `pip install facexlib`.\n3. Ensure that the network connection is stable during the download process to prevent any interruptions. (B) 1. Uninstall the facexlib package by running the command: `pip uninstall facexlib`.\n2. Delete the facexlib installation directory manually using `rm -rf /path/to/facexlib`.\n3. Reinstall the facexlib package by running the command: `pip install facexlib`.\n4. Ensure that the network connection is stable during the download process to prevent any interruptions. (C) 1. Uninstall the facexlib package by running the command: `pip uninstall facexlib`.\n2. Reinstall the facexlib package by running the command: `pip install facexlib`. (D) 1. Reinstall the facexlib package by running the command: `pip install facexlib`.\n2. Uninstall the facexlib package by running the command: `pip uninstall facexlib`.\n3. Ensure that the network connection is stable during the download process to prevent any interruptions.", "answer": "A"}
{"uuid": "57eef76e-94b6-40dc-8ecd-93e11bc232a8", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 4090/5090 recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.3+\n\n## 2. Installation\n```bash\n# Clone the repository\ngit clone https://github.com/nunchaku-tech/nunchaku.git\ncd nunchaku\n\n# Install dependencies (use conda/virtualenv recommended)\npip install -r requirements.txt\n\n# Install Nunchaku core package\npip install .\n```\n\n## 3. Model Setup\n```bash\n# Download pre-quantized models from Hugging Face\ngit lfs install\ngit clone https://huggingface.co/nunchaku-tech/nunchaku-t5\ngit clone https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\n```\n\n## 4. Basic Inference Test\n```bash\n# Run FLUX.1 example with default settings\npython examples/flux.1-dev-basic.py \\\n    --prompt \"A cat wearing sunglasses\" \\\n    --output_dir ./results\n```\n\n## 5. Advanced Features\n```bash\n# Multi-LoRA inference\npython examples/flux.1-dev-multiple-lora.py \\\n    --prompt \"Portrait of a wizard\" \\\n    --lora_weights ./path/to/lora1.safetensors ./path/to/lora2.safetensors \\\n    --output_dir ./multi_lora_results\n\n# ControlNet integration\npython examples/flux.1-dev-controlnet-union-pro.py \\\n    --input_image ./sketch.png \\\n    --prompt \"Colorful digital painting\" \\\n    --output_dir ./controlnet_results\n```\n\n## 6. Performance Optimization\n```bash\n# Enable FP16 attention and First-Block Cache\npython examples/flux.1-dev-optimized.py \\\n    --use_fp16_attention \\\n    --enable_fb_cache \\\n    --batch_size 4\n```\n\n## 7. ComfyUI Integration\n```bash\n# Install ComfyUI plugin\ngit clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git \\\n    /path/to/ComfyUI/custom_nodes/\n```\n\n## 8. Verification\n1. Check output images in specified `--output_dir`\n2. Monitor GPU utilization with `nvidia-smi -l 1`\n3. Validate throughput with different batch sizes (2/4/8)\n\n## 9. Troubleshooting\n- For 20-series GPUs: Add `--turing_support` flag\n- Low memory systems: Use `--cpu_offload` and `--quant_level 4`\n- NVFP4 issues: Set `--precision nvfp4` on RTX 5090", "issue_title": "[Bug] ubuntu system get stuck after several times of inference running", "issue_body": "### Checklist\n\n- [x] 1. I have searched for related issues and FAQs (https://github.com/mit-han-lab/nunchaku/blob/main/docs/faq.md) but was unable to find a solution.\n- [x] 2. The issue persists in the latest version.\n- [x] 3. Please note that without environment information and a minimal reproducible example, it will be difficult for us to reproduce and address the issue, which may delay our response.\n- [ ] 4. If your report is a question rather than a bug, please submit it as a discussion at https://github.com/mit-han-lab/nunchaku/discussions/new/choose. Otherwise, this issue will be closed.\n- [ ] 5. If this is related to ComfyUI, please report it at https://github.com/mit-han-lab/ComfyUI-nunchaku/issues.\n- [x] 6. I will do my best to describe the issue in English.\n\n### Describe the Bug\n\nI just install the 0.3.0.dev20250529+torch2.6 version locally from source code.\nfirst time, the inference(either offical wieght or my custom fluxgym lora weight) works fine.\n\nAfter several inference times, just during the 3rd or 4th inference running, the system got stuck and i have to restart.\n\nWhats more, the latest generated image will lost after restart.\n\n\n### Environment\n\nos:ubuntu 24.4 x86/64\npython 3.10\ncuda 12.6\npytorch 2.6\nnunchaku==0.3.0.dev20250529+torch2.6, build locally from github source code\n\n### Reproduction Steps\n\nhttps://github.com/mit-han-lab/nunchaku/blob/main/examples/flux.1-dev-lora.py\nhttps://huggingface.co/black-forest-labs/FLUX.1-dev\n\neither above  script will reproduce this problem.", "choices": "(A) 1. Ensure the driver is set to GRID mode. If the command `nvidia-smi -dm 0` fails, refer to cloud service provider documentation (e.g., Tencent Cloud or AWS) for specific instructions on setting the driver mode.\n2. Verify the driver installation and configuration by following the provider's guidelines to ensure compatibility with Nunchaku. (B) 1. Check if the A100 GPU supports INT4 Matrix Multiplication Accumulation (MMA) as it is required for Nunchaku to run properly.\n2. Ensure the driver is set to GRID mode. If the command `nvidia-smi -dm 0` fails, refer to cloud service provider documentation (e.g., Tencent Cloud or AWS) for specific instructions on setting the driver mode.\n3. Verify the driver installation and configuration by following the provider's guidelines to ensure compatibility with Nunchaku. (C) 1. Ensure the driver is set to GRID mode. If the command `nvidia-smi -dm 0` fails, refer to cloud service provider documentation (e.g., Tencent Cloud or AWS) for specific instructions on setting the driver mode.\n2. Check if the A100 GPU supports INT4 Matrix Multiplication Accumulation (MMA) as it is required for Nunchaku to run properly.\n3. Verify the driver installation and configuration by following the provider's guidelines to ensure compatibility with Nunchaku. (D) 1. Disable the GPU drivers using `sudo systemctl disable nvidia-driver`.\n2. Check if the A100 GPU supports INT4 Matrix Multiplication Accumulation (MMA) as it is required for Nunchaku to run properly.\n3. Ensure the driver is set to GRID mode. If the command `nvidia-smi -dm 0` fails, refer to cloud service provider documentation (e.g., Tencent Cloud or AWS) for specific instructions on setting the driver mode.\n4. Verify the driver installation and configuration by following the provider's guidelines to ensure compatibility with Nunchaku.", "answer": "B"}
{"uuid": "e99408e9-d1b7-4fbf-8cd2-bc0a8e1b67ab", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face Hub](https://huggingface.co/nunchaku-tech)  \n   Example:  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux1-dev\n   ```\n\n2. **Custom Quantization (Optional)**  \n   Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 4. Basic Inference\n1. **Run Example Script**  \n   ```bash\n   python examples/flux.1-dev-basic.py --model-path ./nunchaku-flux1-dev\n   ```\n\n2. **Multi-LoRA Example**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model-path ./nunchaku-flux1-dev\n   ```\n\n## 5. Advanced Features\n1. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n2. **Low-Memory Inference**  \n   ```bash\n   python examples/flux.1-dev-low-memory.py --cpu-offload\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku && pip install -e .\n   ```\n\n2. **Launch ComfyUI**  \n   ```bash\n   python main.py\n   ```\n\n## 7. Monitoring & Debugging\n- Check logs in `./logs/nunchaku.log`\n- Enable debug mode:  \n  ```bash\n  export NUNCHAKU_LOG_LEVEL=DEBUG\n  ```\n\n## 8. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for assistance.", "issue_title": "[Bug] released wheel for linux-torch2.6-python310 import error.", "issue_body": "### Checklist\n\n- [x] 1. I have searched for related issues and FAQs (https://github.com/mit-han-lab/nunchaku/blob/main/docs/faq.md) but was unable to find a solution.\n- [x] 2. The issue persists in the latest version.\n- [x] 3. Please note that without environment information and a minimal reproducible example, it will be difficult for us to reproduce and address the issue, which may delay our response.\n- [ ] 4. If your report is a question rather than a bug, please submit it as a discussion at https://github.com/mit-han-lab/nunchaku/discussions/new/choose. Otherwise, this issue will be closed.\n- [ ] 5. If this is related to ComfyUI, please report it at https://github.com/mit-han-lab/ComfyUI-nunchaku/issues.\n- [x] 6. I will do my best to describe the issue in English.\n\n### Describe the Bug\n\nI did have tried several alternative versions(0.2/0.3nightly versions) from release page, but all of them gives same error while import.\nshall i config the compiling environment and build from source?\n\nos:ubuntu 24.4\npython 3.10\ncuda 12.6\npytorch 2.6\n\ninstall result log:\nSuccessfully installed nunchaku-0.3.0.dev20250522+torch2.6\n\n\nerror log:\nImportError: ****python3.10/site-packages/nunchaku/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs\n\n\n### Environment\n\nos:ubuntu 24.4\npython 3.10\ncuda 12.6\npytorch 2.6\n\n### Reproduction Steps\n\nhttps://github.com/mit-han-lab/nunchaku/blob/main/examples/flux.1-dev-lora.py", "choices": "(A) 1. For running the example on a GPU with limited VRAM (e.g., 16GB), use the provided optimized code snippet that includes offloading and quantization to reduce VRAM usage.\n2. Copy the `model_configs` folder along with the `EVA02-CLIP-L-14-336.json` file into the specified directory. (B) 1. Check if the `model_configs` folder exists under `nunchaku/venv/lib/python3.11/site-packages/nunchaku/models/pulid/eva_clip`.\n2. Delete the `model_configs` folder if it exists.\n3. Copy the `model_configs` folder along with the `EVA02-CLIP-L-14-336.json` file into the specified directory.\n4. For running the example on a GPU with limited VRAM (e.g., 16GB), use the provided optimized code snippet that includes offloading and quantization to reduce VRAM usage. (C) 1. Check if the `model_configs` folder exists under `nunchaku/venv/lib/python3.11/site-packages/nunchaku/models/pulid/eva_clip`.\n2. If the folder is missing, copy the `model_configs` folder along with the `EVA02-CLIP-L-14-336.json` file into the specified directory.\n3. For running the example on a GPU with limited VRAM (e.g., 16GB), use the provided optimized code snippet that includes offloading and quantization to reduce VRAM usage. (D) 1. For running the example on a GPU with limited VRAM (e.g., 16GB), use the provided optimized code snippet that includes offloading and quantization to reduce VRAM usage.\n2. Check if the `model_configs` folder exists under `nunchaku/venv/lib/python3.11/site-packages/nunchaku/models/pulid/eva_clip`.\n3. If the folder is missing, copy the `model_configs` folder along with the `EVA02-CLIP-L-14-336.json` file into the specified directory.", "answer": "C"}
{"uuid": "7cba480f-1c3a-461b-aef6-9c8e68e08433", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- Ensure you have a compatible NVIDIA GPU (RTX 20-series or newer recommended)\n- Install Python 3.8 or later\n- Verify CUDA toolkit is installed (v12.1+ recommended)\n\n## 2. Installation\n```bash\n# Clone the repository\ngit clone https://github.com/nunchaku-tech/nunchaku.git\ncd nunchaku\n\n# Create and activate a virtual environment (recommended)\npython -m venv nunchaku-env\nsource nunchaku-env/bin/activate  # Linux/MacOS\n.\\nunchaku-env\\Scripts\\activate   # Windows\n\n# Install core dependencies\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\npip install -r requirements.txt\n\n# Install Nunchaku package\npip install -e .\n```\n\n## 3. Model Setup\n```bash\n# Download pre-quantized models (example: FLUX.1)\npython -m nunchaku.download --model flux.1-dev --precision int4\n\n# Or quantize your own model using DeepCompressor\ngit clone https://github.com/nunchaku-tech/deepcompressor\ncd deepcompressor\npip install -e .\npython quantize.py --model your_model --output quantized_model\n```\n\n## 4. Basic Inference\n```bash\n# Run text-to-image generation with default parameters\npython examples/flux.1-dev.py --prompt \"A beautiful landscape\" --output output.png\n\n# For multi-batch inference\npython examples/flux.1-dev-multi.py --prompts \"landscape,cityscape\" --batch_size 2\n```\n\n## 5. Advanced Features\n```bash\n# ControlNet integration\npython examples/flux.1-dev-controlnet-union-pro.py --input_sketch sketch.png\n\n# LoRA integration\npython examples/flux.1-dev-multiple-lora.py --lora_weights lora1.safetensors lora2.safetensors\n\n# Low-memory mode (4GB GPUs)\npython examples/flux.1-dev-lowmem.py --cpu_offload\n```\n\n## 6. ComfyUI Integration\n```bash\n# Install ComfyUI plugin\ngit clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\ncp -r ComfyUI-nunchaku /path/to/ComfyUI/custom_nodes/\n\n# Launch ComfyUI\ncd /path/to/ComfyUI\npython main.py\n```\n\n## 7. Verification\n```bash\n# Run verification tests\npytest tests/\n\n# Check GPU utilization during inference\nnvidia-smi -l 1\n```\n\n## 8. Troubleshooting\n- For installation issues: Check [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html)\n- For runtime errors: Enable debug mode with `--log_level DEBUG`\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for community support", "issue_title": "[Bug] The A100 cannot use nunchucks", "issue_body": "### Checklist\n\n- [x] 1. I have searched for related issues and FAQs (https://github.com/mit-han-lab/nunchaku/blob/main/docs/faq.md) but was unable to find a solution.\n- [x] 2. The issue persists in the latest version.\n- [ ] 3. Please note that without environment information and a minimal reproducible example, it will be difficult for us to reproduce and address the issue, which may delay our response.\n- [ ] 4. If your report is a question rather than a bug, please submit it as a discussion at https://github.com/mit-han-lab/nunchaku/discussions/new/choose. Otherwise, this issue will be closed.\n- [ ] 5. If this is related to ComfyUI, please report it at https://github.com/mit-han-lab/ComfyUI-nunchaku/issues.\n- [ ] 6. I will do my best to describe the issue in English.\n\n### Describe the Bug\n\nAn error occurred during the usage process\n\n\nRuntimeError: CUDA error: operation not supported (at C:\\Users\\muyang\\Desktop\\nunchaku-dev\\nunchaku\\csrc\\utils.h:20)\n\nIn the previous answer, it was found that the driver needed to be modified. \nTo check if your current driver mode is Tesla (TCC):\nnvidia-smi -dm 1\nTo switch to GRID mode, use the following command:\nnvidia-smi -dm 0\n\nHowever, after attempting, it failed. Prompt:\nUnable to set driver model for GPU 00000000:00:0C.0: Not Supported\nTreating as warning and moving on.\nAll done.\n\n\n\n\n### Environment\n\n@ @Total VRAM 40654 MB, total RAM 131071 MB\npytorch version: 2.5.1+cu124\nxformers version: 0.0.28.post3\nSet vram state to: NORMAL_VRAM\nDevice: cuda:0 NVIDIA A100-PCIE-40GB : native\nUsing xformers attention\nPython version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\nComfyUI version: 0.3.33\nComfyUI frontend version: 1.18.9\n\n### Reproduction Steps\n\nAfter searching for relevant materials, it was found that\n<img width=\"989\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/acac9b8d-b364-4d90-8c67-080c66d59474\" />\n\nMay I ask if there are any other ways to make the A100 run nunchaku", "choices": "(A) 1. Set the correct CUDA version environment variables:\n   ```\n   CUDAVER=cuda-12.8\n   export PATH=/usr/local/$CUDAVER/bin:$PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib:$LD_LIBRARY_PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib64:$LD_LIBRARY_PATH\n   export CUDA_PATH=/usr/local/$CUDAVER\n   export CUDA_ROOT=/usr/local/$CUDAVER\n   export CUDA_HOME=/usr/local/$CUDAVER\n   export CUDA_HOST_COMPILER=/usr/bin/gcc-11\n   ```\n2. Install the correct versions of torch, torchaudio, torchvision, and xformers:\n   ```\n   pip install torch==2.7 torchaudio==2.7 torchvision==0.22\n   pip install xformers==0.0.30\n   ```\n3. If encountering ninja build errors, modify the torch cpp_extension.py file to use 'ninja --version' instead of 'ninja -v'.\n4. Alternatively, use pre-built wheels from the releases page if available for your platform. (B) 1. Set the correct CUDA version environment variables:\n   ```\n   CUDAVER=cuda-12.8\n   export PATH=/usr/local/$CUDAVER/bin:$PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib:$LD_LIBRARY_PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib64:$LD_LIBRARY_PATH\n   export CUDA_PATH=/usr/local/$CUDAVER\n   export CUDA_ROOT=/usr/local/$CUDAVER\n   export CUDA_HOME=/usr/local/$CUDAVER\n   export CUDA_HOST_COMPILER=/usr/bin/gcc-11\n   ```\n2. Install the correct versions of torch, torchaudio, and torchvision:\n   ```\n   pip install torch==2.7 torchaudio==2.7 torchvision==0.22\n   ```\n3. If encountering ninja build errors, modify the torch cpp_extension.py file to use 'ninja --version' instead of 'ninja -v'.\n4. Alternatively, use pre-built wheels from the releases page if available for your platform. (C) 1. Uninstall existing CUDA drivers:\n   ```\n   sudo apt-get purge nvidia-cuda*\n   ```\n2. Set the correct CUDA version environment variables:\n   ```\n   CUDAVER=cuda-12.8\n   export PATH=/usr/local/$CUDAVER/bin:$PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib:$LD_LIBRARY_PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib64:$LD_LIBRARY_PATH\n   export CUDA_PATH=/usr/local/$CUDAVER\n   export CUDA_ROOT=/usr/local/$CUDAVER\n   export CUDA_HOME=/usr/local/$CUDAVER\n   export CUDA_HOST_COMPILER=/usr/bin/gcc-11\n   ```\n3. Install the correct versions of torch, torchaudio, torchvision, and xformers:\n   ```\n   pip install torch==2.7 torchaudio==2.7 torchvision==0.22\n   pip install xformers==0.0.30\n   ```\n4. If encountering ninja build errors, modify the torch cpp_extension.py file to use 'ninja --version' instead of 'ninja -v'.\n5. Alternatively, use pre-built wheels from the releases page if available for your platform. (D) 1. Set the correct CUDA version environment variables:\n   ```\n   CUDAVER=cuda-12.8\n   export PATH=/usr/local/$CUDAVER/bin:$PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib:$LD_LIBRARY_PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib64:$LD_LIBRARY_PATH\n   export CUDA_PATH=/usr/local/$CUDAVER\n   export CUDA_ROOT=/usr/local/$CUDAVER\n   export CUDA_HOME=/usr/local/$CUDAVER\n   export CUDA_HOST_COMPILER=/usr/bin/gcc-11\n   ```\n2. Install incorrect versions of torch, torchaudio, torchvision, and xformers:\n   ```\n   pip install torch==1.9 torchaudio==0.9 torchvision==0.10\n   pip install xformers==0.1.0\n   ```\n3. If encountering ninja build errors, modify the torch cpp_extension.py file to use 'ninja --version' instead of 'ninja -v'.\n4. Alternatively, use pre-built wheels from the releases page if available for your platform. (E) 1. Install the correct versions of torch, torchaudio, torchvision, and xformers:\n   ```\n   pip install torch==2.7 torchaudio==2.7 torchvision==0.22\n   pip install xformers==0.0.30\n   ```\n2. Set the correct CUDA version environment variables:\n   ```\n   CUDAVER=cuda-12.8\n   export PATH=/usr/local/$CUDAVER/bin:$PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib:$LD_LIBRARY_PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib64:$LD_LIBRARY_PATH\n   export CUDA_PATH=/usr/local/$CUDAVER\n   export CUDA_ROOT=/usr/local/$CUDAVER\n   export CUDA_HOME=/usr/local/$CUDAVER\n   export CUDA_HOST_COMPILER=/usr/bin/gcc-11\n   ```\n3. If encountering ninja build errors, modify the torch cpp_extension.py file to use 'ninja --version' instead of 'ninja -v'.\n4. Alternatively, use pre-built wheels from the releases page if available for your platform. (F) 1. If encountering ninja build errors, modify the torch cpp_extension.py file to use 'ninja --version' instead of 'ninja -v'.\n2. Set the correct CUDA version environment variables:\n   ```\n   CUDAVER=cuda-12.8\n   export PATH=/usr/local/$CUDAVER/bin:$PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib:$LD_LIBRARY_PATH\n   export LD_LIBRARY_PATH=/usr/local/$CUDAVER/lib64:$LD_LIBRARY_PATH\n   export CUDA_PATH=/usr/local/$CUDAVER\n   export CUDA_ROOT=/usr/local/$CUDAVER\n   export CUDA_HOME=/usr/local/$CUDAVER\n   export CUDA_HOST_COMPILER=/usr/bin/gcc-11\n   ```\n3. Install the correct versions of torch, torchaudio, torchvision, and xformers:\n   ```\n   pip install torch==2.7 torchaudio==2.7 torchvision==0.22\n   pip install xformers==0.0.30\n   ```\n4. Alternatively, use pre-built wheels from the releases page if available for your platform. (G) 1. Install the correct versions of torch, torchaudio, torchvision, and xformers:\n   ```\n   pip install torch==2.7 torchaudio==2.7 torchvision==0.22\n   pip install xformers==0.0.30\n   ```\n2. If encountering ninja build errors, modify the torch cpp_extension.py file to use 'ninja --version' instead of 'ninja -v'.\n3. Alternatively, use pre-built wheels from the releases page if available for your platform.", "answer": "A"}
{"uuid": "80a64022-8122-44e1-af59-c9a73c927108", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- Ensure you have a compatible NVIDIA GPU (RTX 20-series or newer recommended)\n- Install Python 3.8 or later\n- Install CUDA toolkit (version 11.8 or later recommended)\n\n## 2. Installation\n```bash\n# Clone the Nunchaku repository\ngit clone https://github.com/nunchaku-tech/nunchaku.git\ncd nunchaku\n\n# Create and activate a Python virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/MacOS\n# OR\n.\\venv\\Scripts\\activate   # Windows\n\n# Install required dependencies\npip install -r requirements.txt\n\n# Install Nunchaku package\npip install .\n```\n\n## 3. Verify Installation\n```bash\n# Check if Nunchaku is properly installed\npython -c \"import nunchaku; print(nunchaku.__version__)\"\n```\n\n## 4. Download Pre-quantized Models\n```bash\n# Example for FLUX.1 model\nwget https://huggingface.co/nunchaku-tech/nunchaku-flux1/resolve/main/flux1-4bit.safetensors\n```\n\n## 5. Run Basic Inference\n```bash\n# Example inference script\npython examples/flux.1-basic.py --model-path flux1-4bit.safetensors --prompt \"A beautiful landscape\"\n```\n\n## 6. Advanced Features\n### 6.1 Multi-LoRA Support\n```bash\npython examples/flux.1-dev-multiple-lora.py --model-path flux1-4bit.safetensors --lora-weights lora1.safetensors lora2.safetensors\n```\n\n### 6.2 ControlNet Integration\n```bash\npython examples/flux.1-dev-controlnet-union-pro.py --model-path flux1-4bit.safetensors --controlnet controlnet-union-pro.safetensors\n```\n\n### 6.3 Low-Memory Inference\n```bash\npython examples/flux.1-low-memory.py --model-path flux1-4bit.safetensors --cpu-offload\n```\n\n## 7. ComfyUI Integration\n```bash\n# Clone ComfyUI-nunchaku plugin\ngit clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n# Follow installation instructions in the plugin's README\n```\n\n## 8. Custom Model Quantization\n```bash\n# Clone DeepCompressor for custom quantization\ngit clone https://github.com/nunchaku-tech/deepcompressor.git\ncd deepcompressor\n# Follow quantization instructions in DeepCompressor's documentation\n```\n\n## 9. Monitoring and Optimization\n```bash\n# Use NVIDIA-smi to monitor GPU utilization\nnvidia-smi -l 1\n```\n\n## 10. Troubleshooting\n- Check logs for error messages\n- Verify CUDA and cuDNN versions\n- Ensure sufficient GPU memory is available\n- Consult the [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html) for common issues", "issue_title": "[Bug] Failed python.exe -c \"import nunchaku\"", "issue_body": "### Checklist\n\n- [x] 1. I have searched for related issues and FAQs (https://github.com/mit-han-lab/nunchaku/blob/main/docs/faq.md) but was unable to find a solution.\n- [x] 2. The issue persists in the latest version.\n- [x] 3. Please note that without environment information and a minimal reproducible example, it will be difficult for us to reproduce and address the issue, which may delay our response.\n- [x] 4. If your report is a question rather than a bug, please submit it as a discussion at https://github.com/mit-han-lab/nunchaku/discussions/new/choose. Otherwise, this issue will be closed.\n- [x] 5. If this is related to ComfyUI, please report it at https://github.com/mit-han-lab/ComfyUI-nunchaku/issues.\n- [x] 6. I will do my best to describe the issue in English.\n\n### Describe the Bug\n\nHello, i'm following the windows tutorial for nunchaku installation. i'm at the step where it says to try python.exe -c \"import nunchaku\".\nUnfortunately, i get this error. I have no clue if there is a workaround.\n![Image](https://github.com/user-attachments/assets/1d839467-dd8d-4a9c-9bf7-00a3f650ee7d)\n\n### Environment\n\nhttps://huggingface.co/mit-han-lab/nunchaku/resolve/main/nunchaku-0.2.0+torch2.6-cp312-cp312-win_amd64.whl\nCuda 12.9\nPython 3.10 + PyTorch 2.6\n\n### Reproduction Steps\n\nfollow the tutorial till the step 3\nNotice the error on verifying the installation command", "choices": "(A) 1. Open the file `cpp_extension.py` located in your Python environment's `Lib\\site-packages\\torch\\utils\\` directory.\n2. Create a backup of the file by copying it to `cpp_extension.py.bak`.\n3. Save the changes and try building the wheel again. (B) 1. Open the file `cpp_extension.py` located in your Python environment's `Lib\\site-packages\\torch\\utils\\` directory.\n2. Use a text editor or the `sed` command to modify the file, changing the line `['ninja', '-v']` to `['ninja', '--version']`.\n3. Save the changes and try building the wheel again. (C) 1. Open the file `cpp_extension.py` located in your Python environment's `Lib\\site-packages\\torch\\utils\\` directory.\n2. Use a text editor or the `sed` command to modify the file, changing the line `['ninja', '-v']` to `['ninja', '--version']`.\n3. Create a backup of the file by copying it to `cpp_extension.py.bak`.\n4. Save the changes and try building the wheel again. (D) 1. Create a backup of the file by copying it to `cpp_extension.py.bak`.\n2. Open the file `cpp_extension.py` located in your Python environment's `Lib\\site-packages\\torch\\utils\\` directory.\n3. Use a text editor or the `sed` command to modify the file, changing the line `['ninja', '-v']` to `['ninja', '--version']`.\n4. Save the changes and try building the wheel again. (E) 1. Open the file `cpp_extension.py` located in your Python environment's `Lib\\site-packages\\torch\\utils\\` directory.\n2. Create a backup of the file by copying it to `cpp_extension.py.bak`.\n3. Use a text editor or the `sed` command to modify the file, changing the line `['ninja', '-v']` to `['ninja', '--version']`.\n4. Save the changes and try building the wheel again. (F) 1. Open the file `cpp_extension.py` located in your Python environment's `Lib\\site-packages\\torch\\utils\\` directory.\n2. Create a backup of the file by copying it to `cpp_extension.py.bak`.\n3. Use a text editor or the `sed` command to modify the file `cpp_extension.py.bak`, changing the line `['ninja', '-v']` to `['ninja', '--version']`.\n4. Save the changes and try building the wheel again. (G) 1. Open the file `cpp_extension.py` located in your Python environment's `Lib\\site-packages\\torch\\utils\\` directory.\n2. Create a backup of the file by copying it to `cpp_extension.py.bak`.\n3. Use a text editor or the `sed` command to modify the file, changing the line `['ninja', '-v']` to `['ninja', '--version']`.\n4. Delete the original file `cpp_extension.py`.\n5. Save the changes and try building the wheel again.", "answer": "E"}
{"uuid": "07b284b0-2bc9-4259-a352-6e1d6bffb1ca", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.3+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_support())\"\n   ```\n\n2. **Run Basic Test**  \n   ```bash\n   python examples/flux.1-dev-basic.py\n   ```\n\n## 4. Model Deployment\n1. **Download Pre-Quantized Models**  \n   ```bash\n   huggingface-cli download nunchaku-tech/nunchaku-flux1 --local-dir ./models\n   ```\n\n2. **Run Inference**  \n   ```bash\n   python examples/flux.1-dev-inference.py --model-path ./models/flux1-4bit.safetensors\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --lora-weights ./lora/*.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --controlnet-config ./controlnet/config.yaml\n   ```\n\n## 6. Performance Optimization\n1. **Enable FP16 Attention**  \n   Add `--use-fp16-attention` flag to inference scripts.\n\n2. **Activate First-Block Cache**  \n   ```bash\n   python examples/flux.1-dev-double_cache.py --enable-fb-cache\n   ```\n\n## 7. Monitoring\n1. **Check GPU Utilization**  \n   ```bash\n   nvidia-smi --query-gpu=utilization.gpu --format=csv -l 1\n   ```\n\n2. **Profile Latency**  \n   ```bash\n   nsys profile --stats=true python examples/flux.1-dev-benchmark.py\n   ```\n\n## 8. Troubleshooting\n- **Memory Issues**: Use `--low-vram` flag or enable CPU offloading with `--cpu-offload`\n- **Kernel Errors**: Update GPU drivers and verify CUDA version matches PyTorch installation", "issue_title": "[Bug] 0.3.0dev轮子构建失败torch2.6 cuda12.6.20 python3.12.8 同求轮子", "issue_body": "### Checklist\n\n- [x] 1. I have searched for related issues and FAQs (https://github.com/mit-han-lab/nunchaku/blob/main/docs/faq.md) but was unable to find a solution.\n- [ ] 2. The issue persists in the latest version.\n- [ ] 3. Please note that without environment information and a minimal reproducible example, it will be difficult for us to reproduce and address the issue, which may delay our response.\n- [ ] 4. If your report is a question rather than a bug, please submit it as a discussion at https://github.com/mit-han-lab/nunchaku/discussions/new/choose. Otherwise, this issue will be closed.\n- [ ] 5. If this is related to ComfyUI, please report it at https://github.com/mit-han-lab/ComfyUI-nunchaku/issues.\n- [ ] 6. I will do my best to describe the issue in English.\n\n### Describe the Bug\n\nFound nvcc version: 12.6.20\nDetected SM targets: ['75']\nH:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\dist.py:334: InformationOnly: Normalizing '0.3.0dev1+torch2.6' to '0.3.0.dev1+torch2.6'\n  self.metadata.version = self._normalize_version(self.metadata.version)\nrunning develop\nH:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py:90: DevelopDeprecationWarning: develop command is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` and ``develop``.\n        Instead, use standards-based tools like pip or uv.\n\n        By 2025-Oct-31, you need to update your project and remove deprecated calls\n        or your builds will no longer be supported.\n\n        See https://github.com/pypa/setuptools/issues/917 for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\nLooking in indexes: https://mirrors.aliyun.com/pypi/simple/\nObtaining file:///H:/ComfyUI_windows_portable/nck\n  Installing build dependencies ... done\n  Checking if build backend supports build_editable ... done\n  Getting requirements to build editable ... done\n  Preparing editable metadata (pyproject.toml) ... done\nRequirement already satisfied: diffusers>=0.32.2 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from nunchaku==0.3.0.dev1+torch2.6) (0.33.1)\nRequirement already satisfied: transformers in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from nunchaku==0.3.0.dev1+torch2.6) (4.51.3)\nRequirement already satisfied: accelerate in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from nunchaku==0.3.0.dev1+torch2.6) (1.4.0)\nRequirement already satisfied: sentencepiece in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from nunchaku==0.3.0.dev1+torch2.6) (0.2.0)\nRequirement already satisfied: protobuf in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from nunchaku==0.3.0.dev1+torch2.6) (5.29.3)\nRequirement already satisfied: huggingface_hub in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from nunchaku==0.3.0.dev1+torch2.6) (0.30.2)\nRequirement already satisfied: importlib-metadata in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (8.6.1)\nRequirement already satisfied: filelock in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (3.17.0)\nRequirement already satisfied: numpy in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (2024.11.6)\nRequirement already satisfied: requests in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (0.5.2)\nRequirement already satisfied: Pillow in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (11.1.0)\nRequirement already satisfied: fsspec>=2023.5.0 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from huggingface_hub->nunchaku==0.3.0.dev1+torch2.6) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from huggingface_hub->nunchaku==0.3.0.dev1+torch2.6) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from huggingface_hub->nunchaku==0.3.0.dev1+torch2.6) (6.0.2)\nRequirement already satisfied: tqdm>=4.42.1 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from huggingface_hub->nunchaku==0.3.0.dev1+torch2.6) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from huggingface_hub->nunchaku==0.3.0.dev1+torch2.6) (4.12.2)\nRequirement already satisfied: colorama in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->nunchaku==0.3.0.dev1+torch2.6) (0.4.6)\nRequirement already satisfied: psutil in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from accelerate->nunchaku==0.3.0.dev1+torch2.6) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from accelerate->nunchaku==0.3.0.dev1+torch2.6) (2.6.0+cu126)\nRequirement already satisfied: networkx in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from torch>=2.0.0->accelerate->nunchaku==0.3.0.dev1+torch2.6) (3.4.2)\nRequirement already satisfied: jinja2 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from torch>=2.0.0->accelerate->nunchaku==0.3.0.dev1+torch2.6) (3.1.5)\nRequirement already satisfied: setuptools in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from torch>=2.0.0->accelerate->nunchaku==0.3.0.dev1+torch2.6) (80.3.0)\nRequirement already satisfied: sympy==1.13.1 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from torch>=2.0.0->accelerate->nunchaku==0.3.0.dev1+torch2.6) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate->nunchaku==0.3.0.dev1+torch2.6) (1.3.0)\nRequirement already satisfied: zipp>=3.20 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from importlib-metadata->diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate->nunchaku==0.3.0.dev1+torch2.6) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from requests->diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from requests->diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from requests->diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (1.26.20)\nRequirement already satisfied: certifi>=2017.4.17 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from requests->diffusers>=0.32.2->nunchaku==0.3.0.dev1+torch2.6) (2024.12.14)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in h:\\comfyui_windows_portable\\python_embeded\\lib\\site-packages (from transformers->nunchaku==0.3.0.dev1+torch2.6) (0.21.0)\nBuilding wheels for collected packages: nunchaku\n  Building editable for nunchaku (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n  × Building editable for nunchaku (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [226 lines of output]\n      Detected SM targets: ['75']\n      H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\dist.py:334: InformationOnly: Normalizing '0.3.0dev1+torch2.6' to '0.3.0.dev1+torch2.6'\n        self.metadata.version = self._normalize_version(self.metadata.version)\n      Found nvcc version: 12.6.20\n      running editable_wheel\n      creating E:\\TEMP\\pip-wheel-ql7nno28\\.tmp-ukhom3ni\\nunchaku.egg-info\n      writing E:\\TEMP\\pip-wheel-ql7nno28\\.tmp-ukhom3ni\\nunchaku.egg-info\\PKG-INFO\n      writing dependency_links to E:\\TEMP\\pip-wheel-ql7nno28\\.tmp-ukhom3ni\\nunchaku.egg-info\\dependency_links.txt\n      writing requirements to E:\\TEMP\\pip-wheel-ql7nno28\\.tmp-ukhom3ni\\nunchaku.egg-info\\requires.txt\n      writing top-level names to E:\\TEMP\\pip-wheel-ql7nno28\\.tmp-ukhom3ni\\nunchaku.egg-info\\top_level.txt\n      writing manifest file 'E:\\TEMP\\pip-wheel-ql7nno28\\.tmp-ukhom3ni\\nunchaku.egg-info\\SOURCES.txt'\n      reading manifest file 'E:\\TEMP\\pip-wheel-ql7nno28\\.tmp-ukhom3ni\\nunchaku.egg-info\\SOURCES.txt'\n      reading manifest template 'MANIFEST.in'\n      warning: no files found matching '*.hpp' under directory 'src'\n      warning: no files found matching '*.ipp' under directory 'src'\n      warning: no files found matching '*.hpp' under directory 'nunchaku\\csrc'\n      warning: no files found matching '*.ipp' under directory 'nunchaku\\csrc'\n      warning: no files found matching '*.cu' under directory 'nunchaku\\csrc'\n      warning: no files found matching '*.cuh' under directory 'nunchaku\\csrc'\n      warning: no files found matching '*.cpp' under directory 'third_party\\Block-Sparse-Attention\\csrc\\block_sparse_attn'\n      warning: no files found matching '*.h' under directory 'third_party\\Block-Sparse-Attention\\csrc\\block_sparse_attn'\n      warning: no files found matching '*.hpp' under directory 'third_party\\Block-Sparse-Attention\\csrc\\block_sparse_attn'\n      warning: no files found matching '*.ipp' under directory 'third_party\\Block-Sparse-Attention\\csrc\\block_sparse_attn'\n      warning: no files found matching '*.cu' under directory 'third_party\\Block-Sparse-Attention\\csrc\\block_sparse_attn'\n      warning: no files found matching '*.cuh' under directory 'third_party\\Block-Sparse-Attention\\csrc\\block_sparse_attn'\n      warning: no files found matching '*.cpp' under directory 'third_party\\cutlass\\include'\n      warning: no files found matching '*.h' under directory 'third_party\\cutlass\\include'\n      warning: no files found matching '*.hpp' under directory 'third_party\\cutlass\\include'\n      warning: no files found matching '*.ipp' under directory 'third_party\\cutlass\\include'\n      warning: no files found matching '*.cu' under directory 'third_party\\cutlass\\include'\n      warning: no files found matching '*.cuh' under directory 'third_party\\cutlass\\include'\n      warning: no files found matching '*.cpp' under directory 'third_party\\json\\include'\n      warning: no files found matching '*.h' under directory 'third_party\\json\\include'\n      warning: no files found matching '*.hpp' under directory 'third_party\\json\\include'\n      warning: no files found matching '*.ipp' under directory 'third_party\\json\\include'\n      warning: no files found matching '*.cpp' under directory 'third_party\\mio\\include'\n      warning: no files found matching '*.h' under directory 'third_party\\mio\\include'\n      warning: no files found matching '*.hpp' under directory 'third_party\\mio\\include'\n      warning: no files found matching '*.ipp' under directory 'third_party\\mio\\include'\n      warning: no files found matching '*.cpp' under directory 'third_party\\spdlog\\include'\n      warning: no files found matching '*.h' under directory 'third_party\\spdlog\\include'\n      warning: no files found matching '*.hpp' under directory 'third_party\\spdlog\\include'\n      warning: no files found matching '*.ipp' under directory 'third_party\\spdlog\\include'\n      warning: no files found matching 'third_party\\Block-Sparse-Attention\\LICENSE'\n      warning: no files found matching 'third_party\\cutlass\\LICENSE.txt'\n      warning: no files found matching 'third_party\\json\\LICENSE.MIT'\n      warning: no files found matching 'third_party\\mio\\LICENSE'\n      warning: no files found matching 'third_party\\spdlog\\LICENSE'\n      adding license file 'LICENCE.txt'\n      writing manifest file 'E:\\TEMP\\pip-wheel-ql7nno28\\.tmp-ukhom3ni\\nunchaku.egg-info\\SOURCES.txt'\n      creating 'E:\\TEMP\\pip-wheel-ql7nno28\\.tmp-ukhom3ni\\nunchaku-0.3.0.dev1+torch2.6.dist-info'\n      H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\bdist_wheel.py:103: RuntimeWarning: Config variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect\n        if get_flag(\"Py_DEBUG\", hasattr(sys, \"gettotalrefcount\"), warn=(impl == \"cp\")):\n      creating E:\\TEMP\\pip-wheel-ql7nno28\\.tmp-ukhom3ni\\nunchaku-0.3.0.dev1+torch2.6.dist-info\\WHEEL\n      running build_py\n      running build_ext\n      building 'nunchaku._C' extension\n      creating E:\\TEMP\\tmpdyaag9l3.build-temp\\Release\\nunchaku\\csrc\n      creating E:\\TEMP\\tmpdyaag9l3.build-temp\\Release\\src\n      creating E:\\TEMP\\tmpdyaag9l3.build-temp\\Release\\src\\interop\n      creating E:\\TEMP\\tmpdyaag9l3.build-temp\\Release\\src\\kernels\n      creating E:\\TEMP\\tmpdyaag9l3.build-temp\\Release\\src\\kernels\\awq\n      creating E:\\TEMP\\tmpdyaag9l3.build-temp\\Release\\src\\kernels\\zgemm\n      creating E:\\TEMP\\tmpdyaag9l3.build-temp\\Release\\third_party\\Block-Sparse-Attention\\csrc\\block_sparse_attn\n      creating E:\\TEMP\\tmpdyaag9l3.build-temp\\Release\\third_party\\Block-Sparse-Attention\\csrc\\block_sparse_attn\\src\n      Emitting ninja build file E:\\TEMP\\tmpdyaag9l3.build-temp\\Release\\build.ninja...\n      Compiling objects...\n      Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n      ninja: error: 'H:/ComfyUI_windows_portable/nck/third_party/Block-Sparse-Attention/csrc/block_sparse_attn/flash_api.cpp', needed by 'E:/TEMP/tmpdyaag9l3.build-temp/Release/third_party/Block-Sparse-Attention/csrc/block_sparse_attn/flash_api.obj', missing and no known rule to make it\n      Traceback (most recent call last):\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 2209, in _run_ninja_build\n          subprocess.run(\n        File \"subprocess.py\", line 571, in run\n      subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\n      The above exception was the direct cause of the following exception:\n\n      Traceback (most recent call last):\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\editable_wheel.py\", line 139, in run\n          self._create_wheel_file(bdist_wheel)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\editable_wheel.py\", line 345, in _create_wheel_file\n          files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\editable_wheel.py\", line 268, in _run_build_commands\n          self._run_build_subcommands()\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\editable_wheel.py\", line 295, in _run_build_subcommands\n          self.run_command(name)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 357, in run_command\n          self.distribution.run_command(command)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\dist.py\", line 1106, in run_command\n          super().run_command(command)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1021, in run_command\n          cmd_obj.run()\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\build_ext.py\", line 99, in run\n          _build_ext.run(self)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 368, in run\n          self.build_extensions()\n        File \"<string>\", line 24, in build_extensions\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 900, in build_extensions\n          build_ext.build_extensions(self)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 484, in build_extensions\n          self._build_extensions_serial()\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 510, in _build_extensions_serial\n          self.build_extension(ext)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\build_ext.py\", line 264, in build_extension\n          _build_ext.build_extension(self, ext)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\Cython\\Distutils\\build_ext.py\", line 135, in build_extension\n          super(build_ext, self).build_extension(ext)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 565, in build_extension\n          objects = self.compiler.compile(\n                    ^^^^^^^^^^^^^^^^^^^^^^\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 872, in win_wrap_ninja_compile\n          _write_ninja_file_and_compile_objects(\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1869, in _write_ninja_file_and_compile_objects\n          _run_ninja_build(\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 2225, in _run_ninja_build\n          raise RuntimeError(message) from e\n      RuntimeError: Error compiling objects for extension\n      H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:1021: _DebuggingTips: Problem in editable installation.\n      !!\n\n              ********************************************************************************\n              An error happened while installing `nunchaku` in editable mode.\n\n              The following steps are recommended to help debug this problem:\n\n              - Try to install the project normally, without using the editable mode.\n                Does the error still persist?\n                (If it does, try fixing the problem before attempting the editable mode).\n              - If you are using binary extensions, make sure you have all OS-level\n                dependencies installed (e.g. compilers, toolchains, binary libraries, ...).\n              - Try the latest version of setuptools (maybe the error was already fixed).\n              - If you (or your project dependencies) are using any setuptools extension\n                or customization, make sure they support the editable mode.\n\n              After following the steps above, if the problem still persists and\n              you think this is related to how setuptools handles editable installations,\n              please submit a reproducible example\n              (see https://stackoverflow.com/help/minimal-reproducible-example) to:\n\n                  https://github.com/pypa/setuptools/issues\n\n              See https://setuptools.pypa.io/en/latest/userguide/development_mode.html for details.\n              ********************************************************************************\n\n      !!\n        cmd_obj.run()\n      Traceback (most recent call last):\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 2209, in _run_ninja_build\n          subprocess.run(\n        File \"subprocess.py\", line 571, in run\n      subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\n      The above exception was the direct cause of the following exception:\n\n      Traceback (most recent call last):\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n          main()\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 303, in build_editable\n          return hook(wheel_directory, config_settings, metadata_directory)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\build_meta.py\", line 468, in build_editable\n          return self._build_with_temp_dir(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\build_meta.py\", line 404, in _build_with_temp_dir\n          self.run_setup()\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n          exec(code, locals())\n        File \"<string>\", line 182, in <module>\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\__init__.py\", line 117, in setup\n          return distutils.core.setup(**attrs)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 186, in setup\n          return run_commands(dist)\n                 ^^^^^^^^^^^^^^^^^^\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 202, in run_commands\n          dist.run_commands()\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1002, in run_commands\n          self.run_command(cmd)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\dist.py\", line 1106, in run_command\n          super().run_command(command)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1021, in run_command\n          cmd_obj.run()\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\editable_wheel.py\", line 139, in run\n          self._create_wheel_file(bdist_wheel)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\editable_wheel.py\", line 345, in _create_wheel_file\n          files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\editable_wheel.py\", line 268, in _run_build_commands\n          self._run_build_subcommands()\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\editable_wheel.py\", line 295, in _run_build_subcommands\n          self.run_command(name)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 357, in run_command\n          self.distribution.run_command(command)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\dist.py\", line 1106, in run_command\n          super().run_command(command)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1021, in run_command\n          cmd_obj.run()\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\build_ext.py\", line 99, in run\n          _build_ext.run(self)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 368, in run\n          self.build_extensions()\n        File \"<string>\", line 24, in build_extensions\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 900, in build_extensions\n          build_ext.build_extensions(self)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 484, in build_extensions\n          self._build_extensions_serial()\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 510, in _build_extensions_serial\n          self.build_extension(ext)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\build_ext.py\", line 264, in build_extension\n          _build_ext.build_extension(self, ext)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\Cython\\Distutils\\build_ext.py\", line 135, in build_extension\n          super(build_ext, self).build_extension(ext)\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 565, in build_extension\n          objects = self.compiler.compile(\n                    ^^^^^^^^^^^^^^^^^^^^^^\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 872, in win_wrap_ninja_compile\n          _write_ninja_file_and_compile_objects(\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1869, in _write_ninja_file_and_compile_objects\n          _run_ninja_build(\n        File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 2225, in _run_ninja_build\n          raise RuntimeError(message) from e\n      RuntimeError: Error compiling objects for extension\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building editable for nunchaku\nFailed to build nunchaku\nERROR: Failed to build installable wheels for some pyproject.toml based projects (nunchaku)\nTraceback (most recent call last):\n  File \"H:\\ComfyUI_windows_portable\\nck\\setup.py\", line 182, in <module>\n    setuptools.setup(\n  File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\__init__.py\", line 117, in setup\n    return distutils.core.setup(**attrs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 186, in setup\n    return run_commands(dist)\n           ^^^^^^^^^^^^^^^^^^\n  File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 202, in run_commands\n    dist.run_commands()\n  File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1002, in run_commands\n    self.run_command(cmd)\n  File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\dist.py\", line 1106, in run_command\n    super().run_command(command)\n  File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1021, in run_command\n    cmd_obj.run()\n  File \"H:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\setuptools\\command\\develop.py\", line 39, in run\n    subprocess.check_call(cmd)\n  File \"subprocess.py\", line 413, in check_call\nsubprocess.CalledProcessError: Command '['H:\\\\ComfyUI_windows_portable\\\\python_embeded\\\\python.exe', '-m', 'pip', 'install', '-e', '.', '--use-pep517']' returned non-zero exit status 1.\n\n### Environment\n\ntorch2.6 cuda12.6.20 python3.12.8 \n\n### Reproduction Steps\n\n操作步骤\nD:\\soft\\Microsoft Visual Studio\\2022\\Community\\Common7\\Tools\\VsDevCmd.bat -startdir=none -arch=x64 -host_arch=x64\nset DISTUTILS_USE_SDK=1\nset NUNCHAKU_INSTALL_MODE=ALL\n\"H:\\ComfyUI_windows_portable\\python_embeded\\python.exe\" setup.py develop", "choices": "(A) 1. Place the downloaded folder directly into `models/diffusion_models`. 2. Download the entire model folder. (B) 1. Download the entire model folder. 2. Place the downloaded folder directly into `models/diffusion_models`. 3. Delete the folder from `models/diffusion_models`. (C) 1. Download the entire model folder. 2. Place the downloaded folder directly into `models/diffusion_models`. (D) 1. Place the downloaded folder directly into `models/diffusion_models`.", "answer": "C"}
{"uuid": "9e30f584-79f6-400d-ad10-c9dd10bc528a", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.7+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face](https://huggingface.co/nunchaku-tech) or [ModelScope](https://modelscope.cn/organization/nunchaku-tech).  \n   Example:  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux1-dev\n   ```\n\n2. **Custom Model Quantization (Optional)**  \n   Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install -e .\n   ```\n\n## 4. Basic Usage\n1. **Run Inference Script**  \n   Example for FLUX.1-Kontext:  \n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path ./nunchaku-flux1-dev\n   ```\n\n2. **Enable FP16 Attention (Optional)**  \n   Add `--use-fp16-attention` flag for faster performance.  \n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --controlnet-path ./controlnet-union-pro\n   ```\n\n## 6. ComfyUI Integration\n1. **Install ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cp -r ComfyUI-nunchaku /path/to/ComfyUI/custom_nodes/\n   ```\n\n## 7. Verification\n1. **Check Installation**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.__version__)\"\n   ```\n\n2. **Test Demo**  \n   Visit [Live Demo](https://svdquant.mit.edu) or run local Gradio apps:  \n   ```bash\n   python app/flux.1/t2i/app.py\n   ```\n\n## 8. Troubleshooting\n- **CUDA Errors**: Verify CUDA version matches PyTorch installation.  \n- **Memory Issues**: Enable `--cpu-offload` for low-memory GPUs.  \n- **Community Support**: Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm).", "issue_title": "[Bug] The node and environment are installed, but the model file cannot be read, and there is no drop-down box model to choose from.", "issue_body": "### Checklist\n\n- [x] 1. I have searched for related issues and FAQs (https://github.com/mit-han-lab/nunchaku/blob/main/docs/faq.md) but was unable to find a solution.\n- [x] 2. The issue persists in the latest version.\n- [x] 3. Please note that without environment information and a minimal reproducible example, it will be difficult for us to reproduce and address the issue, which may delay our response.\n- [ ] 4. If your report is a question rather than a bug, please submit it as a discussion at https://github.com/mit-han-lab/nunchaku/discussions/new/choose. Otherwise, this issue will be closed.\n- [x] 5. If this is related to ComfyUI, please report it at https://github.com/mit-han-lab/ComfyUI-nunchaku/issues.\n- [x] 6. I will do my best to describe the issue in English.\n\n### Describe the Bug\n\n![Image](https:github.com/user-attachments/assets/a685e312-cd8d-46c4-b69f-de4f1f7fd103)\nThe model file cannot be read, but my models have been downloaded and placed in the corresponding folder. My computer system is win11, python3.12.7, Pytorch2.7, CUDA 12.8. The node is installed locally, and there is no error when opening COMFYUI, but it is not installed in the manager. It is really strange. Please help to find out what is the reason and how to solve it.\n\n### Environment\n\n![Image](https://github.com/user-attachments/assets/418ac10d-c577-4465-959c-822418fb40d9)\n\n### Reproduction Steps\n\nNVIDIA 3060 graphics card, pytorch 2.7.0+cu12.8.", "choices": "(A) 1. Visit the PyTorch nightly builds page (https://download.pytorch.org/whl/nightly/) to find a compatible version.\n2. Install a previous version of PyTorch and torchvision that is known to work with Nunchaku. For example:\n   ```\n   python -m pip install \"https://download.pytorch.org/whl/nightly/cu128/torch-2.8.0.dev20250405%2Bcu128-cp312-cp312-win_amd64.whl\" \"https://download.pytorch.org/whl/nightly/cu128/torchvision-0.22.0.dev20250406%2Bcu128-cp312-cp312-win_amd64.whl\"\n   ```\n3. Verify the installation by running your application again. If the issue persists, consider compiling Nunchaku from source by following the provided guides (https://github.com/mit-han-lab/nunchaku/blob/main/docs/setup_windows.md). (B) 1. Identify the version of PyTorch (torch) and torchvision currently installed in your environment.\n2. Uninstall PyTorch and torchvision using `pip uninstall torch torchvision`.\n3. Visit the PyTorch nightly builds page (https://download.pytorch.org/whl/nightly/) to find a compatible version.\n4. Install a previous version of PyTorch and torchvision that is known to work with Nunchaku. For example:\n   ```\n   python -m pip install \"https://download.pytorch.org/whl/nightly/cu128/torch-2.8.0.dev20250405%2Bcu128-cp312-cp312-win_amd64.whl\" \"https://download.pytorch.org/whl/nightly/cu128/torchvision-0.22.0.dev20250406%2Bcu128-cp312-cp312-win_amd64.whl\"\n   ```\n5. Verify the installation by running your application again. If the issue persists, consider compiling Nunchaku from source by following the provided guides (https://github.com/mit-han-lab/nunchaku/blob/main/docs/setup_windows.md). (C) 1. Identify the version of PyTorch (torch) and torchvision currently installed in your environment.\n2. Visit the PyTorch nightly builds page (https://download.pytorch.org/whl/nightly/) to find a compatible version.\n3. Install a previous version of PyTorch and torchvision that is known to work with Nunchaku. For example:\n   ```\n   python -m pip install \"https://download.pytorch.org/whl/nightly/cu128/torch-2.8.0.dev20250405%2Bcu128-cp312-cp312-win_amd64.whl\" \"https://download.pytorch.org/whl/nightly/cu128/torchvision-0.22.0.dev20250406%2Bcu128-cp312-cp312-win_amd64.whl\"\n   ``` (D) 1. Visit the PyTorch nightly builds page (https://download.pytorch.org/whl/nightly/) to find a compatible version.\n2. Identify the version of PyTorch (torch) and torchvision currently installed in your environment.\n3. Install a previous version of PyTorch and torchvision that is known to work with Nunchaku. For example:\n   ```\n   python -m pip install \"https://download.pytorch.org/whl/nightly/cu128/torch-2.8.0.dev20250405%2Bcu128-cp312-cp312-win_amd64.whl\" \"https://download.pytorch.org/whl/nightly/cu128/torchvision-0.22.0.dev20250406%2Bcu128-cp312-cp312-win_amd64.whl\"\n   ```\n4. Verify the installation by running your application again. If the issue persists, consider compiling Nunchaku from source by following the provided guides (https://github.com/mit-han-lab/nunchaku/blob/main/docs/setup_windows.md). (E) 1. Identify the version of PyTorch (torch) and torchvision currently installed in your environment.\n2. Visit the PyTorch nightly builds page (https://download.pytorch.org/whl/nightly/) to find a compatible version.\n3. Install the latest versions of PyTorch and torchvision using `pip install torch torchvision --upgrade`.\n4. Verify the installation by running your application again. If the issue persists, consider compiling Nunchaku from source by following the provided guides (https://github.com/mit-han-lab/nunchaku/blob/main/docs/setup_windows.md). (F) 1. Identify the version of PyTorch (torch) and torchvision currently installed in your environment.\n2. Visit the PyTorch nightly builds page (https://download.pytorch.org/whl/nightly/) to find a compatible version.\n3. Install a previous version of PyTorch and torchvision that is known to work with Nunchaku. For example:\n   ```\n   python -m pip install \"https://download.pytorch.org/whl/nightly/cu128/torch-2.8.0.dev20250405%2Bcu128-cp312-cp312-win_amd64.whl\" \"https://download.pytorch.org/whl/nightly/cu128/torchvision-0.22.0.dev20250406%2Bcu128-cp312-cp312-win_amd64.whl\"\n   ```\n4. Verify the installation by running your application again. If the issue persists, consider compiling Nunchaku from source by following the provided guides (https://github.com/mit-han-lab/nunchaku/blob/main/docs/setup_windows.md). (G) 1. Identify the version of PyTorch (torch) and torchvision currently installed in your environment.\n2. Visit the PyTorch nightly builds page (https://download.pytorch.org/whl/nightly/) to find a compatible version.\n3. Verify the installation by running your application again. If the issue persists, consider compiling Nunchaku from source by following the provided guides (https://github.com/mit-han-lab/nunchaku/blob/main/docs/setup_windows.md).\n4. Install a previous version of PyTorch and torchvision that is known to work with Nunchaku. For example:\n   ```\n   python -m pip install \"https://download.pytorch.org/whl/nightly/cu128/torch-2.8.0.dev20250405%2Bcu128-cp312-cp312-win_amd64.whl\" \"https://download.pytorch.org/whl/nightly/cu128/torchvision-0.22.0.dev20250406%2Bcu128-cp312-cp312-win_amd64.whl\"\n   ```", "answer": "F"}
{"uuid": "6cdfe506-8c4f-471b-8a35-e03aa5332c98", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Visit [Hugging Face Hub](https://huggingface.co/nunchaku-tech) or [ModelScope](https://modelscope.cn/organization/nunchaku-tech)  \n   - Example for FLUX.1:  \n     ```bash\n     git lfs install\n     git clone https://huggingface.co/nunchaku-tech/flux.1-4bit\n     ```\n\n## 4. Basic Inference\n1. **Run Text-to-Image Example**  \n   ```bash\n   python examples/flux.1-t2i.py --model-path ./flux.1-4bit --prompt \"A scenic landscape\"\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model-path ./flux.1-4bit --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --model-path ./flux.1-4bit --controlnet canny\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n2. **Restart ComfyUI** and load workflows from `comfyui/` directory.\n\n## 7. Custom Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n2. **Quantize Custom Model**  \n   Follow [quantization guide](https://github.com/nunchaku-tech/deepcompressor#customized-model-quantization).\n\n## 8. Verification\n1. **Check GPU Utilization**  \n   ```bash\n   nvidia-smi\n   ```\n2. **Validate Outputs** in `./outputs/` directory.\n\n## 9. Troubleshooting\n- For 20-series GPUs: Use `--use-turing` flag  \n- Low VRAM: Enable `--cpu-offload`  \n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for support.", "issue_title": "[Bug]  ImportError: DLL load failed while importing _C, what might cause this error", "issue_body": "### Checklist\n\n- [x] 1. I have searched for related issues and FAQs (https://github.com/mit-han-lab/nunchaku/discussions/262) but was unable to find a solution.\n- [x] 2. The issue persists in the latest version.\n- [x] 3. Please note that without environment information and a minimal reproducible example, it will be difficult for us to reproduce and address the issue, which may delay our response.\n- [x] 4. If your report is a question rather than a bug, please submit it as a discussion at https://github.com/mit-han-lab/nunchaku/discussions/new/choose. Otherwise, this issue will be closed.\n- [x] 5. If this is related to ComfyUI, please report it at https://github.com/mit-han-lab/ComfyUI-nunchaku/issues.\n- [x] 6. I will do my best to describe the issue in English.\n\n### Describe the Bug\n\nTraceback (most recent call last):\n  File \"K:\\ComfyUI_windows_portable\\ComfyUI\\nodes.py\", line 2153, in load_custom_node\n    module_spec.loader.exec_module(module)\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"K:\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-nunchaku\\__init__.py\", line 2, in <module>\n    from .nodes.lora import NunchakuFluxLoraLoader\n  File \"K:\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-nunchaku\\nodes\\lora\\__init__.py\", line 1, in <module>\n    from .flux import NunchakuFluxLoraLoader\n  File \"K:\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-nunchaku\\nodes\\lora\\flux.py\", line 5, in <module>\n    from nunchaku.lora.flux import to_diffusers\n  File \"K:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\nunchaku\\__init__.py\", line 1, in <module>\n    from .models import NunchakuFluxTransformer2dModel, NunchakuSanaTransformer2DModel, NunchakuT5EncoderModel\n  File \"K:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\nunchaku\\models\\__init__.py\", line 1, in <module>\n    from .text_encoders.t5_encoder import NunchakuT5EncoderModel\n  File \"K:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\nunchaku\\models\\text_encoders\\t5_encoder.py\", line 9, in <module>\n    from .linear import W4Linear\n  File \"K:\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\nunchaku\\models\\text_encoders\\linear.py\", line 8, in <module>\n    from ..._C.ops import gemm_awq, gemv_awq\nImportError: DLL load failed while importing _C: 找不到指定的程序。\n\n### Environment\n\nwin10, python 3.11, 2.8.0.dev20250414+cu126, cuda 12.6 \n\n### Reproduction Steps\n\nrun run_nvidia_gpu.bat for comfyui", "choices": "(A) 1. Modify the training parameters for the LoRA model by changing `save_precision` from `bf16` to `fp16`.\n2. Ensure that `flux_block_dim` is disabled or closed during the training process.\n3. Adjust the `timestep_sampling` parameter from `shift` to `sigmoid`.\n4. Retrain the LoRA model with these updated settings to resolve the incompatible keys issue. (B) 1. Modify the training parameters for the LoRA model by changing `save_precision` from `bf16` to `fp16`.\n2. Adjust the `timestep_sampling` parameter from `shift` to `sigmoid`.\n3. Ensure that `flux_block_dim` is disabled or closed during the training process.\n4. Reset `save_precision` to `bf16` to avoid compatibility issues.\n5. Retrain the LoRA model with these updated settings to resolve the incompatible keys issue. (C) 1. Modify the training parameters for the LoRA model by changing `save_precision` from `bf16` to `fp16`.\n2. Adjust the `timestep_sampling` parameter from `shift` to `sigmoid`.\n3. Retrain the LoRA model with these updated settings to resolve the incompatible keys issue. (D) 1. Modify the training parameters for the LoRA model by changing `save_precision` from `bf16` to `fp16`.\n2. Adjust the `timestep_sampling` parameter from `shift` to `sigmoid`.\n3. Ensure that `flux_block_dim` is disabled or closed during the training process.\n4. Enable `flux_block_dim` to improve training stability.\n5. Retrain the LoRA model with these updated settings to resolve the incompatible keys issue. (E) 1. Adjust the `timestep_sampling` parameter from `shift` to `sigmoid`.\n2. Ensure that `flux_block_dim` is disabled or closed during the training process.\n3. Retrain the LoRA model with these updated settings to resolve the incompatible keys issue. (F) 1. Modify the training parameters for the LoRA model by changing `save_precision` from `bf16` to `fp16`.\n2. Adjust the `timestep_sampling` parameter from `shift` to `sigmoid`.\n3. Ensure that `flux_block_dim` is disabled or closed during the training process.\n4. Retrain the LoRA model with these updated settings to resolve the incompatible keys issue. (G) 1. Retrain the LoRA model with these updated settings to resolve the incompatible keys issue.\n2. Modify the training parameters for the LoRA model by changing `save_precision` from `bf16` to `fp16`.\n3. Adjust the `timestep_sampling` parameter from `shift` to `sigmoid`.\n4. Ensure that `flux_block_dim` is disabled or closed during the training process.", "answer": "F"}
{"uuid": "ebaddfb0-ebc5-4c23-b738-c231e4dcfc88", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone the repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Create a Python virtual environment**  \n   ```bash\n   python -m venv nunchaku_env\n   source nunchaku_env/bin/activate  # Linux/Mac\n   .\\nunchaku_env\\Scripts\\activate  # Windows\n   ```\n\n3. **Install dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n## 3. Model Setup\n1. **Download pre-quantized models**  \n   - Available on [Hugging Face](https://huggingface.co/nunchaku-tech)  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-t5  # Example for T5 encoder\n   ```\n\n## 4. Basic Inference\n1. **Run a test script**  \n   Example for FLUX.1-Kontext:  \n   ```bash\n   python examples/flux.1-kontext-dev.py\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   ```\n\n2. **ControlNet integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n## 6. ComfyUI Integration\n1. **Install ComfyUI plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   # Follow plugin-specific setup instructions\n   ```\n\n## 7. Custom Quantization\n1. **Use DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install -e .\n   ```\n\n## 8. Verification\n1. **Check GPU utilization**  \n   ```bash\n   nvidia-smi\n   ```\n\n2. **Validate outputs**  \n   Compare results with [demo outputs](https://svdquant.mit.edu/kontext/)\n\n## 9. Troubleshooting\n- Refer to [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html)\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for support", "issue_title": "Nunchaku April Development Roadmap", "issue_body": "Hello everyone,\n\nAs promised, last month we brought multiple-LoRA and ControlNet-Union-Pro support with faster generation speed. Additionally, we expanded support for 20-series GPUs. We understand some of you may still have faced issues, but rest assured. We're actively working on refining the codebase for better stability, compatibility and user experience.\n\nThis roadmap outlines our key development goals for April 2025. The next release is scheduled for mid-May. As always, we welcome your contributions and feedback!\n\n## April Focus Areas\n\n- Simplify the [deepcompressor](https://github.com/mit-han-lab/deepcompressor/) backend to reduce quantization costs.\n- More comprehensive control support.\n- Address memory-related issues to improve stability.\n\n### Quantization\n\n- [ ] Simplify [deepcompressor](https://github.com/mit-han-lab/deepcompressor/) backend to ease the use and reduce the quantization cost (@synxlin, https://github.com/mit-han-lab/ComfyUI-nunchaku/issues/31)\n- [ ] Add customized model quantization support in [ComfyUI-nunchaku](https://github.com/mit-han-lab/deepcompressor/) (@lmxyy)\n- [ ] Improve fidelity of the 4-bit T5 text encoder (@Aprilhuu)\n\n### LoRA  \n\n- [ ] Add FLUX-turbo support with FLUX-fill base model (@lmxyy, mit-han-lab/ComfyUI-nunchaku#46)\n- [ ] Support additional LoRA formats (@lmxyy, mit-han-lab/ComfyUI-nunchaku#64, https://github.com/mit-han-lab/nunchaku/issues/265)\n- [ ] Fix LoRA combination bugs (@lmxyy, https://github.com/mit-han-lab/ComfyUI-nunchaku/issues/71)\n\n### Controls\n\n- [x] FP8 ControlNet-Union-Pro support (@ita9naiwa,mit-han-lab/nunchaku#241, mit-han-lab/ComfyUI-nunchaku#37)\n- [ ] Expand support for other ControlNet models (@ita9naiwa, mit-han-lab/ComfyUI-nunchaku#37)\n- [ ] Add EasyControl support\n- [x] Add PuLID support (@bowen, mit-han-lab/ComfyUI-nunchaku#50, mit-han-lab/nunchaku#258)\n- [ ] INT4/FP4 ControlNets (mit-han-lab/ComfyUI-nunchaku#37, mit-han-lab/nunchaku#256)\n\n### Speed\n\n- [ ]  Implement fine-grained First-Block Cache (@Bluear7878)\n\n### Memory & Stability\n\n- [ ] Optimize memory usage when loading T5 (@Aprilhuu )\n- [x] Clean memory cache when deleting models (@lmxyy @sxtyzhangzk, mit-han-lab/ComfyUI-nunchaku#65, mit-han-lab/ComfyUI-nunchaku#57)\n- [x] Serialization errors (@sxtyzhangzk , mit-han-lab/ComfyUI-nunchaku#60)\n- [ ] Improve CPU offloading speed in ComfyUI (@lmxyy)\n\n### Quality\n\n- [ ] Investigate FLUX.1-fill quality performance (@lmxyy)\n- [ ] Resolve quality issues when combining [ACE-plus](https://github.com/ali-vilab/ACE_plus) with FLUX.1-fill (@lmxyy)\n\n### Installation  \n\n- [x] Create an installation and usage guide video (@lmxyy)\n- [ ] Add PyPI installation support (@lmxyy)\n\n### Other Fixes & Improvements\n\n- [ ] Enable multiple-batch inference (@sxtyzhangzk @Bluear7878, mit-han-lab/nunchaku#148)\n- [ ] Improve HuggingFace and Modelscope model documentation (@lmxyy)\n- [x] Fix device ID setting (@sxtyzhangzk , mit-han-lab/ComfyUI-nunchaku#45)\n- [x] downloading `cache_dir` handling (@lmxyy, mit-han-lab/nunchaku#255)\n- [x] Reset `residual_diff_threshold` in First-Block Cache (@ita9naiwa, mit-han-lab/nunchaku#242)\n- [ ] Autotests and deployment CI.\n\n## Some future features in plan\n\n- [ ] [Wan2.1](https://github.com/Wan-Video/Wan2.1) Support.\n- [ ] 8-bit model support.\n- [ ] Operator modularization.", "choices": "(A) 1. Set the environment variable `NUNCHAKU_LOAD_METHOD` to either `READ` or `READNOPIN` to address the safetensors loading issue.\n2. Verify the changes by running the application again and checking if the warnings persist. (B) 1. Upgrade your CUDA driver to the latest version compatible with your system.\n2. Set the environment variable `NUNCHAKU_LOAD_METHOD` to either `READ` or `READNOPIN` to address the safetensors loading issue.\n3. Verify the changes by running the application again and checking if the warnings persist. (C) 1. Upgrade your CUDA driver to the latest version compatible with your system.\n2. Verify the changes by running the application again and checking if the warnings persist. (D) 1. Upgrade your CUDA driver to the latest version compatible with your system.\n2. Set the environment variable `NUNCHAKU_LOAD_METHOD` to either `READ` or `READNOPIN` to address the safetensors loading issue.\n3. Unset the environment variable `NUNCHAKU_LOAD_METHOD`.\n4. Verify the changes by running the application again and checking if the warnings persist. (E) 1. Set the environment variable `NUNCHAKU_LOAD_METHOD` to either `READ` or `READNOPIN` to address the safetensors loading issue.\n2. Upgrade your CUDA driver to the latest version compatible with your system.\n3. Verify the changes by running the application again and checking if the warnings persist. (F) 1. Upgrade your CUDA driver to the latest version compatible with your system.\n2. Set the environment variable `NUNCHAKU_LOAD_METHOD` to either `READ` or `READNOPIN` to address the safetensors loading issue.\n3. Downgrade your CUDA driver to an older version.\n4. Verify the changes by running the application again and checking if the warnings persist. (G) 1. Upgrade your CUDA driver to the latest version compatible with your system.\n2. Verify the changes by running the application again and checking if the warnings persist.\n3. Set the environment variable `NUNCHAKU_LOAD_METHOD` to either `READ` or `READNOPIN` to address the safetensors loading issue.", "answer": "B"}
{"uuid": "f478aee9-1ebd-4042-813e-b91629162520", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face](https://huggingface.co/nunchaku-tech) or [ModelScope](https://modelscope.cn/organization/nunchaku-tech).  \n   Example for FLUX.1-Kontext:  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/FLUX.1-Kontext\n   ```\n\n2. **Verify Model Compatibility**  \n   Ensure the model matches the supported versions listed in the [docs](https://nunchaku.tech/docs/nunchaku/).\n\n## 4. Basic Inference\n1. **Run Example Script**  \n   Example for FLUX.1-Kontext:  \n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path ./FLUX.1-Kontext\n   ```\n\n2. **Customize Inference**  \n   Modify parameters in the script (e.g., batch size, resolution) as needed.\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   Use the [multi-LoRA example script](./examples/flux.1-dev-multiple-lora.py).\n\n2. **ControlNet Integration**  \n   Example:  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n3. **Low-Memory Inference**  \n   Enable per-layer CPU offloading:  \n   ```bash\n   python examples/flux.1-dev-low-memory.py --cpu-offload\n   ```\n\n## 6. ComfyUI Integration\n1. **Install ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku\n   pip install -r requirements.txt\n   ```\n\n2. **Launch ComfyUI**  \n   Follow instructions in the [plugin README](https://github.com/nunchaku-tech/ComfyUI-nunchaku).\n\n## 7. Custom Model Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n\n2. **Quantize Your Model**  \n   Refer to the [quantization guide](https://github.com/nunchaku-tech/deepcompressor#customized-model-quantization).\n\n## 8. Troubleshooting\n- **Common Issues**: Check the [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html).\n- **Community Support**: Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm).", "issue_title": "special_lora_error", "issue_body": "I successfully installed it but still encountered the same issue as before. I can guarantee that this model works in the normal dev mode and performs better than the original model. I used a special reverse distillation dev model for training, but it seems that the quantization is not compatible with it.\r\nNunchakuFluxLoraLoader\r\nIncompatible keys detected:\r\nlora_unet_img_in.alpha, lora_unet_img_in.lora_down.weight, lora_unet_img_in.lora_up.weight, lora_unet_txt_in.alpha, lora_unet_txt_in.lora_down.weight, lora_unet_txt_in.lora_up.weight\r\n[11:39](https://nunchaku.slack.com/archives/C08GF6E8684/p1744083564619579)\r\n\r\n lora_unet_img_in.alpha, lora_unet_img_in.lora_down.weight, lora_unet_img_in.lora_up.weight, lora_unet_txt_in.alpha, lora_unet_txt_in.lora_down.weight, lora_unet_txt_in.lora_up.weight\r\n2025-04-08T11:38:43.325874 - Traceback (most recent call last):\r\n  File \"/root/autodl-tmp/ComfyUI/execution.py\", line 327, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/autodl-tmp/ComfyUI/execution.py\", line 202, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/autodl-tmp/ComfyUI/execution.py\", line 174, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"/root/autodl-tmp/ComfyUI/execution.py\", line 163, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/autodl-tmp/ComfyUI/custom_nodes/ComfyUI-nunchaku/nodes/lora/flux.py\", line 61, in load_lora\r\n    sd = to_diffusers(lora_path)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/autodl-tmp/ComfyUI/ENV_ComFyUI/lib/python3.11/site-packages/nunchaku/lora/flux/diffusers_converter.py\", line 17, in to_diffusers\r\n    new_tensors, alphas = FluxLoraLoaderMixin.lora_state_dict(tensors, return_alphas=True)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/autodl-tmp/ComfyUI/ENV_ComFyUI/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/autodl-tmp/ComfyUI/ENV_ComFyUI/lib/python3.11/site-packages/diffusers/loaders/lora_pipeline.py\", line 1449, in lora_state_dict\r\n    state_dict = _convert_kohya_flux_lora_to_diffusers(state_dict)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/autodl-tmp/ComfyUI/ENV_ComFyUI/lib/python3.11/site-packages/diffusers/loaders/lora_conversion_utils.py\", line 561, in _convert_kohya_flux_lora_to_diffusers\r\n    return _convert_sd_scripts_to_ai_toolkit(state_dict)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/autodl-tmp/ComfyUI/ENV_ComFyUI/lib/python3.11/site-packages/diffusers/loaders/lora_conversion_utils.py\", line 523, in _convert_sd_scripts_to_ai_toolkit\r\n    raise ValueError(f\"Incompatible keys detected: \\n\\n {', '.join(remaining_keys)}\")\r\nValueError: Incompatible keys detected: \r\n\r\n lora_unet_img_in.alpha, lora_unet_img_in.lora_down.weight, lora_unet_img_in.lora_up.weight, lora_unet_txt_in.alpha, lora_unet_txt_in.lora_down.weight, lora_unet_txt_in.lora_up.weight\r\n\r\n_Originally posted by @taiczhi in https://github.com/mit-han-lab/nunchaku/discussions/262#discussioncomment-12759158_\r\n\r\n\r\nOkay, this is the model I trained. Thank you very much\r\nhttps://modelscope.cn/models/ewq32213/test_model_nunchaku/file/view/master?fileName=models%252Fxin-000001.safetensors&status=2", "choices": "(A) 1. Ensure the CUDA version installed matches the required version (12.6).\n2. Install the correct CUDA version from the official website if there is a mismatch.\n3. Follow the build instructions from the Readme, ignoring any deprecated setup.py warnings. (B) 1. Build the package from scratch to avoid pre-built wheel issues.\n2. Ensure the CUDA version installed matches the required version (12.6).\n3. Install the correct CUDA version from the official website if there is a mismatch.\n4. Follow the build instructions from the Readme, ignoring any deprecated setup.py warnings. (C) 1. Ensure the CUDA version installed matches the required version (12.6).\n2. Follow the build instructions from the Readme, ignoring any deprecated setup.py warnings.\n3. Install the correct CUDA version from the official website if there is a mismatch.\n4. Build the package from scratch to avoid pre-built wheel issues. (D) 1. Ensure the CUDA version installed matches the required version (12.6).\n2. Install CUDA version 11.0 if there is a mismatch.\n3. Install the correct CUDA version from the official website if there is a mismatch.\n4. Follow the build instructions from the Readme, ignoring any deprecated setup.py warnings.\n5. Build the package from scratch to avoid pre-built wheel issues. (E) 1. Install the correct CUDA version from the official website if there is a mismatch.\n2. Follow the build instructions from the Readme, ignoring any deprecated setup.py warnings.\n3. Build the package from scratch to avoid pre-built wheel issues. (F) 1. Skip checking the CUDA version and proceed directly to installation.\n2. Install the correct CUDA version from the official website if there is a mismatch.\n3. Follow the build instructions from the Readme, ignoring any deprecated setup.py warnings.\n4. Build the package from scratch to avoid pre-built wheel issues. (G) 1. Ensure the CUDA version installed matches the required version (12.6).\n2. Install the correct CUDA version from the official website if there is a mismatch.\n3. Follow the build instructions from the Readme, ignoring any deprecated setup.py warnings.\n4. Build the package from scratch to avoid pre-built wheel issues.", "answer": "G"}
{"uuid": "9c2322d6-d150-4703-86a7-26570561ee7b", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: \n  - Python 3.8+\n  - CUDA 12.1+\n  - PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   ```bash\n   huggingface-cli download nunchaku-tech/nunchaku-flux1 --local-dir ./models\n   ```\n\n2. **Verify Model Integrity**  \n   ```bash\n   python scripts/verify_model.py --model-path ./models/flux1-4bit.nk\n   ```\n\n## 4. Basic Inference\n1. **Run Text-to-Image Generation**  \n   ```bash\n   python examples/flux1-t2i.py --prompt \"A cat wearing sunglasses\" --output output.png\n   ```\n\n2. **Enable FP16 Attention (Optional)**  \n   Add `--use-fp16-attention` flag for faster inference.\n\n## 5. Advanced Features\n1. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --control-image sketch.png\n   ```\n\n## 6. Performance Optimization\n1. **Enable First-Block Cache**  \n   ```bash\n   python examples/flux.1-dev-double_cache.py --use-fb-cache\n   ```\n\n2. **NVFP4 Precision (RTX 5090 Only)**  \n   ```bash\n   export NUNCHAKU_PRECISION=nvfp4\n   ```\n\n## 7. Monitoring\n1. **Check GPU Utilization**  \n   ```bash\n   nvidia-smi --query-gpu=utilization.gpu --format=csv -l 1\n   ```\n\n## 8. Troubleshooting\n- **Memory Issues**: Use `--cpu-offload` flag for 4GiB+ GPUs\n- **Installation Errors**: Verify CUDA toolkit version matches GPU driver", "issue_title": "2080ti cuda error", "issue_body": "[2025-04-05 22:47:06.756] [info] Loading weights from /home/hucd/ComfyUI/models/diffusion_models/svdq-int4-flux.1-dev/transformer_blocks.safetensors\n[2025-04-05 22:47:12.273] [warning] Failed to load safetensors using method PRIVATE: CUDA error: invalid argument (at /nunchaku/src/Serialization.cpp:124)\n[2025-04-05 22:47:12.810] [warning] Failed to load safetensors using method MIO: CUDA error: invalid argument (at /nunchaku/src/Serialization.cpp:130)\n[2025-04-05 22:47:18.489] [warning] Failed to load safetensors using method READ: CUDA error: invalid argument (at /nunchaku/src/Tensor.h:72)\n[2025-04-05 22:47:21.866] [warning] Memory not pinned\n[2025-04-05 22:47:22.699] [info] Done.\n\nare these errors normal? i can still draw picture.", "choices": "(A) 1. Use the Nunchaku LoRA Loader to apply LoRA in the quantized model.\n2. Upgrade both nunchaku and ComfyUI-nunchaku to v0.2.0.\n3. For Fluxgym LoRAs, ensure they are converted to the SVDQuant format using the updated conversion function in v0.2.0. (B) 1. Upgrade both nunchaku and ComfyUI-nunchaku to v0.2.0.\n2. Downgrade both nunchaku and ComfyUI-nunchaku to v0.1.0.\n3. Use the Nunchaku LoRA Loader to apply LoRA in the quantized model.\n4. For Fluxgym LoRAs, ensure they are converted to the SVDQuant format using the updated conversion function in v0.2.0. (C) 1. Use the Nunchaku LoRA Loader to apply LoRA in the quantized model.\n2. For Fluxgym LoRAs, ensure they are converted to the SVDQuant format using the updated conversion function in v0.2.0. (D) 1. Upgrade both nunchaku and ComfyUI-nunchaku to v0.2.0.\n2. Use the Nunchaku LoRA Loader to apply LoRA in the quantized model.\n3. Skip the conversion of Fluxgym LoRAs to SVDQuant format. (E) 1. Upgrade both nunchaku and ComfyUI-nunchaku to v0.2.0.\n2. Use the Nunchaku LoRA Loader to apply LoRA in the quantized model. (F) 1. Upgrade both nunchaku and ComfyUI-nunchaku to v0.2.0.\n2. For Fluxgym LoRAs, ensure they are converted to the SVDQuant format using the updated conversion function in v0.2.0.\n3. Use the Nunchaku LoRA Loader to apply LoRA in the quantized model. (G) 1. Upgrade both nunchaku and ComfyUI-nunchaku to v0.2.0.\n2. Use the Nunchaku LoRA Loader to apply LoRA in the quantized model.\n3. For Fluxgym LoRAs, ensure they are converted to the SVDQuant format using the updated conversion function in v0.2.0.", "answer": "G"}
{"uuid": "fdf881bb-e7c7-4f7f-af49-0222f606fdbe", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Create Virtual Environment**  \n   ```bash\n   python -m venv nunchaku-env\n   source nunchaku-env/bin/activate  # Linux/Mac\n   .\\nunchaku-env\\Scripts\\activate   # Windows\n   ```\n\n3. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   Access models via Hugging Face:  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-t5\n   ```\n\n2. **Verify GPU Compatibility**  \n   Run diagnostic script:  \n   ```bash\n   python examples/gpu_check.py\n   ```\n\n## 4. Basic Inference\n1. **Run Text-to-Image Example**  \n   ```bash\n   python examples/flux.1-kontext-dev.py --prompt \"A futuristic cityscape\"\n   ```\n\n2. **Enable Multi-LoRA**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --lora_weights lora1.safetensors lora2.safetensors\n   ```\n\n## 5. Advanced Features\n1. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --control_image input.png\n   ```\n\n2. **Low-Memory Mode (4GB GPUs)**  \n   ```bash\n   python examples/low_memory_inference.py --cpu_offload\n   ```\n\n## 6. Deployment Options\n1. **Gradio Web UI**  \n   ```bash\n   python app/flux.1/t2i/app.py\n   ```\n\n2. **ComfyUI Integration**  \n   Follow instructions at:  \n   https://github.com/nunchaku-tech/ComfyUI-nunchaku\n\n## 7. Customization\n1. **Quantize Custom Models**  \n   Use DeepCompressor:  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor\n   cd deepcompressor && pip install -e .\n   ```\n\n2. **LoRA Conversion**  \n   ```bash\n   python tools/convert_lora.py --input lora.ckpt --output lora.safetensors\n   ```\n\n## 8. Monitoring\n- Check GPU utilization:  \n  ```bash\n  nvidia-smi -l 1\n  ```\n- Profile inference speed:  \n  ```bash\n  python benchmarks/latency_test.py --iterations 100\n  ```", "issue_title": "Pre-built wheels do not work", "issue_body": "When trying to use the prebuilt wheel nunchaku-0.2.0+torch2.6-cp312-cp312-linux_x86_64.whl, the following error occurs with Python 3.12.9. \n\nWhen building from source, it works as intended.\n\n```\nTraceback (most recent call last):\n  File \"/workspace/ComfyUI/nodes.py\", line 2141, in load_custom_node\n    module_spec.loader.exec_module(module)\n  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \"/workspace/ComfyUI/custom_nodes/ComfyUI-nunchaku/__init__.py\", line 2, in <module>\n    from .nodes.lora import NunchakuFluxLoraLoader\n  File \"/workspace/ComfyUI/custom_nodes/ComfyUI-nunchaku/nodes/lora/__init__.py\", line 1, in <module>\n    from .flux import NunchakuFluxLoraLoader\n  File \"/workspace/ComfyUI/custom_nodes/ComfyUI-nunchaku/nodes/lora/flux.py\", line 5, in <module>\n    from nunchaku.lora.flux import to_diffusers\n  File \"/venv/main/lib/python3.12/site-packages/nunchaku/__init__.py\", line 1, in <module>\n    from .models import NunchakuFluxTransformer2dModel, NunchakuSanaTransformer2DModel, NunchakuT5EncoderModel\n  File \"/venv/main/lib/python3.12/site-packages/nunchaku/models/__init__.py\", line 1, in <module>\n    from .text_encoders.t5_encoder import NunchakuT5EncoderModel\n  File \"/venv/main/lib/python3.12/site-packages/nunchaku/models/text_encoders/t5_encoder.py\", line 9, in <module>\n    from .linear import W4Linear\n  File \"/venv/main/lib/python3.12/site-packages/nunchaku/models/text_encoders/linear.py\", line 8, in <module>\n    from ..._C.ops import gemm_awq, gemv_awq\nImportError: /venv/main/lib/python3.12/site-packages/nunchaku/_C.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs\n\nCannot import /workspace/ComfyUI/custom_nodes/ComfyUI-nunchaku module for custom nodes: /venv/main/lib/python3.12/site-packages/nunchaku/_C.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs\n```\n\n```\nName: torch\nVersion: 2.6.0+cu126\nSummary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\nHome-page: https://pytorch.org/\nAuthor: PyTorch Team\nAuthor-email: packages@pytorch.org\nLicense: BSD-3-Clause\nLocation: /venv/main/lib/python3.12/site-packages\n```", "choices": "(A) 1. Check if your resolution width and height are multiples of 16, as required by diffusers limits. 2. Upgrade both nunchaku and ComfyUI-nunchaku to version 0.2.0 to resolve the high-resolution issue. (B) 1. Upgrade both nunchaku and ComfyUI-nunchaku to version 0.2.0 to resolve the high-resolution issue. 2. Check if your resolution width and height are multiples of 16, as required by diffusers limits. (C) 1. Upgrade both nunchaku and ComfyUI-nunchaku to version 0.2.0 to resolve the high-resolution issue. (D) 1. Check if your resolution width and height are multiples of 16, as required by diffusers limits. 2. Downgrade both nunchaku and ComfyUI-nunchaku to version 0.1.0. 3. Upgrade both nunchaku and ComfyUI-nunchaku to version 0.2.0 to resolve the high-resolution issue.", "answer": "A"}
{"uuid": "8ba315c8-70cd-4747-a47d-9b2f9b0ff75e", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone the repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download pre-quantized models**  \n   - From Hugging Face:  \n     ```bash\n     git lfs install\n     git clone https://huggingface.co/nunchaku-tech/nunchaku-t5\n     ```\n   - From ModelScope (for Chinese users):  \n     ```bash\n     git clone https://www.modelscope.cn/nunchaku-tech/nunchaku-t5.git\n     ```\n\n2. **Custom model quantization (optional)**  \n   Follow [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) instructions.\n\n## 4. Basic Usage\n1. **Run inference with FLUX.1**  \n   ```bash\n   python examples/flux.1-dev.py --model-path ./nunchaku-t5 --prompt \"A photo of a cat\"\n   ```\n\n2. **Multi-LoRA inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model-path ./nunchaku-t5 --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n## 5. Advanced Features\n1. **ControlNet integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --controlnet canny --input-image input.jpg\n   ```\n\n2. **Low-memory mode (4GB VRAM)**  \n   ```bash\n   python examples/flux.1-dev-lowmem.py --cpu-offload\n   ```\n\n## 6. Deployment Options\n1. **Gradio demo**  \n   ```bash\n   python app/flux.1/t2i/app.py\n   ```\n\n2. **ComfyUI integration**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n## 7. Verification\n1. **Benchmark performance**  \n   ```bash\n   python benchmarks/speed_test.py --precision int4 --batch-size 4\n   ```\n\n2. **Quality comparison**  \n   ```bash\n   python benchmarks/quality_test.py --compare-with fp16\n   ```\n\n## 8. Maintenance\n1. **Update Nunchaku**  \n   ```bash\n   git pull origin main\n   pip install --upgrade .\n   ```\n\n2. **Join community**  \n   - [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q)  \n   - [Discord](https://discord.gg/Wk6PnwX9Sm)  \n   - Check [issues](https://github.com/nunchaku-tech/nunchaku/issues) for known problems", "issue_title": "feat: expose norm1 layer to support TeaCache", "issue_body": "Fixes https://github.com/mit-han-lab/nunchaku/issues/233\r\n\r\nThis PR exposes the `norm1` layer from the transformer blocks, which is used by TeaCache. In this way, SVDQuant and TeaCache can be combined to get an even larger speedup on Flux", "choices": "(A) 1. Upgrade to the latest Nunchaku v0.2.0 release. 2. Download and use the prebuilt wheels compatible with your system (Windows or Linux, Python 3.10–3.13, PyTorch 2.5–2.8) from Hugging Face, ModelScope, or GitHub Releases. (B) 1. Upgrade to the latest Nunchaku v0.2.0 release. 2. Download and use the prebuilt wheels compatible with your system (Windows or Linux, Python 3.10–3.13, PyTorch 2.5–2.8) from Hugging Face, ModelScope, or GitHub Releases. 3. Ensure ComfyUI-nunchaku is also upgraded to v0.2.0 for full compatibility. (C) 1. Download and use the prebuilt wheels compatible with your system (Windows or Linux, Python 3.10–3.13, PyTorch 2.5–2.8) from Hugging Face, ModelScope, or GitHub Releases. 2. Upgrade to the latest Nunchaku v0.2.0 release. 3. Ensure ComfyUI-nunchaku is also upgraded to v0.2.0 for full compatibility. (D) 1. Delete all existing Nunchaku files and folders. 2. Upgrade to the latest Nunchaku v0.2.0 release. 3. Download and use the prebuilt wheels compatible with your system (Windows or Linux, Python 3.10–3.13, PyTorch 2.5–2.8) from Hugging Face, ModelScope, or GitHub Releases. 4. Ensure ComfyUI-nunchaku is also upgraded to v0.2.0 for full compatibility. (E) 1. Download and use the prebuilt wheels compatible with your system (Windows or Linux, Python 3.10–3.13, PyTorch 2.5–2.8) from Hugging Face, ModelScope, or GitHub Releases. 2. Ensure ComfyUI-nunchaku is also upgraded to v0.2.0 for full compatibility. (F) 1. Download and use the prebuilt wheels compatible with your system (Windows or Linux, Python 3.10–3.13, PyTorch 2.5–2.8) from Hugging Face, ModelScope, or GitHub Releases. 2. Ensure ComfyUI-nunchaku is also upgraded to v0.2.0 for full compatibility. 3. Upgrade to the latest Nunchaku v0.2.0 release. (G) 1. Upgrade to the latest Nunchaku v0.2.0 release. 2. Downgrade PyTorch to version 1.12. 3. Download and use the prebuilt wheels compatible with your system (Windows or Linux, Python 3.10–3.13, PyTorch 2.5–2.8) from Hugging Face, ModelScope, or GitHub Releases. 4. Ensure ComfyUI-nunchaku is also upgraded to v0.2.0 for full compatibility.", "answer": "B"}
{"uuid": "70208898-700a-4ae8-81f7-da42f98850f5", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - From Hugging Face:  \n     ```bash\n     git lfs install\n     git clone https://huggingface.co/nunchaku-tech/nunchaku-t5\n     ```\n   - From ModelScope (China):  \n     ```bash\n     git clone https://www.modelscope.cn/nunchaku-tech/nunchaku-t5.git\n     ```\n\n## 4. Basic Usage\n1. **Run Inference Example**  \n   ```bash\n   python examples/flux.1-kontext-dev.py\n   ```\n\n2. **Enable FP16 Attention (Optional)**  \n   Add `--use_fp16_attention` flag to example scripts.\n\n## 5. Advanced Features\n1. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n## 6. Performance Optimization\n1. **Enable First-Block Cache**  \n   Add `--use_fb_cache` flag to reduce memory usage.\n\n2. **NVFP4 Precision (RTX 5090)**  \n   ```bash\n   python examples/flux.1-dev-nvfp4.py\n   ```\n\n## 7. Deployment Options\n1. **ComfyUI Integration**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku && pip install -r requirements.txt\n   ```\n\n2. **Gradio Demo**  \n   ```bash\n   python app/flux.1/t2i/app.py\n   ```\n\n## 8. Custom Model Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor && pip install .\n   ```\n\n2. **Quantize Custom Model**  \n   Follow [DeepCompressor documentation](https://github.com/nunchaku-tech/deepcompressor) for SVDQuant workflow.\n\n## 9. Troubleshooting\n- For 20-series GPUs: Use `--turing_support` flag\n- Low-memory systems: Enable `--per_layer_cpu_offload`\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for support", "issue_title": "Is it not compatible with most models in comfyui?", "issue_body": "Running most workflows will report an error.", "choices": "(A) 1. Update to version 0.3.0 of the software as mentioned in the comments. 2. For optimizing RAM usage, choose the FP8 text encoder file and set the T5 precision to BF16 as suggested. 3. Use the v2 version node of the nunchaku text encoder for compatibility. (B) 1. Update to version 0.3.0 of the software as mentioned in the comments. 2. Use the v2 version node of the nunchaku text encoder for compatibility. 3. For optimizing RAM usage, choose the FP16 text encoder file and set the T5 precision to BF16 as suggested. (C) 1. Use the v2 version node of the nunchaku text encoder for compatibility. 2. For optimizing RAM usage, choose the FP8 text encoder file and set the T5 precision to BF16 as suggested. (D) 1. Use the v2 version node of the nunchaku text encoder for compatibility. 2. Update to version 0.3.0 of the software as mentioned in the comments. 3. For optimizing RAM usage, choose the FP8 text encoder file and set the T5 precision to BF16 as suggested. (E) 1. Downgrade to version 0.2.0 of the software. 2. Update to version 0.3.0 of the software as mentioned in the comments. 3. Use the v2 version node of the nunchaku text encoder for compatibility. 4. For optimizing RAM usage, choose the FP8 text encoder file and set the T5 precision to BF16 as suggested. (F) 1. Update to version 0.3.0 of the software as mentioned in the comments. 2. Use the v2 version node of the nunchaku text encoder for compatibility. 3. For optimizing RAM usage, choose the FP8 text encoder file and set the T5 precision to BF16 as suggested. (G) 1. Update to version 0.3.0 of the software as mentioned in the comments. 2. For optimizing RAM usage, choose the FP8 text encoder file and set the T5 precision to BF16 as suggested.", "answer": "F"}
{"uuid": "e58eddd1-35c2-4262-812f-289982100cb1", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face](https://huggingface.co/nunchaku-tech) or [ModelScope](https://modelscope.cn/organization/nunchaku-tech).  \n   Example:\n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux1-dev\n   ```\n\n2. **Custom Quantization (Optional)**  \n   Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 4. Basic Usage\n1. **Run Inference Script**  \n   Example for FLUX.1-Kontext:\n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path ./nunchaku-flux1-dev\n   ```\n\n2. **Multi-LoRA/ControlNet (Optional)**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n## 5. Advanced Features\n1. **FP16 Attention & First-Block Cache**  \n   Enable via `--use-fp16-attention` and `--use-fb-cache` flags in scripts.\n\n2. **Low-Memory Inference**  \n   ```bash\n   python examples/flux.1-dev-low-memory.py --cpu-offload\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku && pip install -e .\n   ```\n\n## 7. Verification\n1. **Check GPU Utilization**  \n   ```bash\n   nvidia-smi\n   ```\n2. **Validate Outputs**  \n   Compare results with [demo](https://svdquant.mit.edu/kontext/).\n\n## 8. Troubleshooting\n- **Common Issues**: Refer to [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html).\n- **Community Support**: Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm).", "issue_title": "Can't convert fluxgym lora to SVDQuant format", "issue_body": "When I try to convert lora created by fluxgym, it says \"Already in SVDQuant format, no conversion needed.\". But when using the lora it has no effect, same lora when using with normal Flux workflow works without any problem.", "choices": "(A) 1. Use the original LoRA file directly in the LoRA loader, as the LoRA conversion feature has been deprecated.\n2. Upgrade both nunchaku and ComfyUI-nunchaku to version v0.2.0. (B) 1. Upgrade both nunchaku and ComfyUI-nunchaku to version v0.2.0.\n2. Use the original LoRA file directly in the LoRA loader, as the LoRA conversion feature has been deprecated. (C) 1. Upgrade nunchaku to version v0.2.0.\n2. Use the original LoRA file directly in the LoRA loader, as the LoRA conversion feature has been deprecated. (D) 1. Upgrade both nunchaku and ComfyUI-nunchaku to version v0.2.0.\n2. Convert the LoRA file using the deprecated conversion feature.\n3. Use the original LoRA file directly in the LoRA loader, as the LoRA conversion feature has been deprecated.", "answer": "B"}
{"uuid": "525e613b-7a69-48f8-af0d-8cfa11eb6bd3", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install --pre --upgrade nunchaku\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   Example for FLUX.1-Kontext:\n   ```bash\n   wget https://huggingface.co/nunchaku-tech/flux.1-kontext/resolve/main/flux.1-kontext-4bit.nk\n   ```\n\n2. **Verify Model Compatibility**  \n   Check supported architectures in [documentation](https://nunchaku.tech/docs/nunchaku/).\n\n## 4. Basic Inference\n1. **Run Text-to-Image Example**  \n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path ./flux.1-kontext-4bit.nk --prompt \"A futuristic cityscape\"\n   ```\n\n2. **Enable Advanced Features**  \n   For ControlNet or LoRA:\n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --model-path ./model.nk --controlnet-path ./controlnet.nk\n   ```\n\n## 5. Performance Optimization\n1. **Enable FP16 Attention**  \n   Add `--use-fp16-attn` flag to inference commands.\n\n2. **Activate First-Block Cache**  \n   Use `--enable-fb-cache` for recurrent generation tasks.\n\n## 6. Deployment Options\n1. **ComfyUI Integration**  \n   Follow [ComfyUI-nunchaku](https://github.com/nunchaku-tech/ComfyUI-nunchaku) setup instructions.\n\n2. **Gradio Demo Deployment**  \n   ```bash\n   python app/flux.1/t2i/app.py --share\n   ```\n\n## 7. Custom Model Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor\n   cd deepcompressor && pip install -e .\n   ```\n\n2. **Quantize Custom Model**  \n   ```bash\n   python quantize.py --model ./original_model --output ./quantized.nk --wbits 4 --abits 4\n   ```\n\n## 8. Monitoring & Debugging\n1. **Memory Usage Tracking**  \n   Add `--log-memory` flag to inference commands.\n\n2. **Generate Performance Report**  \n   ```bash\n   nunchaku-benchmark --model ./model.nk --batch-size 4\n   ```\n\n## 9. Community Resources\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for support\n- Check [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html) for common issues", "issue_title": "RuntimeError: CUDA error: operation not supported", "issue_body": "[2025-03-10 12:51:47.723] [info] Initializing QuantizedFluxModel\n[2025-03-10 12:51:47.761] [info] Loading weights from E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\models\\diffusion_models\\mit-han-lab/svdq-int4-flux.1-dev\\transformer_blocks.safetensors\n[2025-03-10 12:51:47.764] [warning] Failed to load safetensors using method MIO: CUDA error: operation not supported (at E:\\comfyui\\nunchaku0.1.4\\nunchaku\\src\\Serialization.cpp:130)\n[2025-03-10 12:51:54.705] [info] Done.\n!!! Exception during processing !!! CUDA error: operation not supported\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nTraceback (most recent call last):\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\svdquant\\nodes\\models\\flux.py\", line 158, in load_model\n    transformer = transformer.to(device)\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 1077, in to\n    return super().to(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1343, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 903, in _apply\n    module._apply(fn)\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 903, in _apply\n    module._apply(fn)\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 903, in _apply\n    module._apply(fn)\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 930, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"E:\\Comfyui\\ComfyUI_windows_portable_nvidia\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1329, in convert\n    return t.to(\n           ^^^^^\nRuntimeError: CUDA error: operation not supported\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.", "choices": "(A) 1. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub release page for version 0.2.0.\n2. For the build error, reduce the supported LoRA ranks by modifying the file `src/kernels/zgemm/gemm_w4a4_launch.cuh`.\n3. Change the line to `using LoraRanks = std::integer_sequence<int, 0, 32, 48, 64, 80, 96, 112, 128>;`.\n4. Recompile the project. (B) 1. For the build error, reduce the supported LoRA ranks by modifying the file `src/kernels/zgemm/gemm_w4a4_launch.cuh`.\n2. Change the line to `using LoraRanks = std::integer_sequence<int, 0, 32, 48, 64, 80, 96, 112, 128>;`.\n3. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub release page for version 0.2.0. (C) 1. For the build error, reduce the supported LoRA ranks by modifying the file `src/kernels/zgemm/gemm_w4a4_launch.cuh`.\n2. Recompile the project.\n3. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub release page for version 0.2.0. (D) 1. Recompile the project.\n2. For the build error, reduce the supported LoRA ranks by modifying the file `src/kernels/zgemm/gemm_w4a4_launch.cuh`.\n3. Change the line to `using LoraRanks = std::integer_sequence<int, 0, 32, 48, 64, 80, 96, 112, 128>;`.\n4. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub release page for version 0.2.0. (E) 1. For the build error, reduce the supported LoRA ranks by modifying the file `src/kernels/zgemm/gemm_w4a4_launch.cuh`.\n2. Change the line to `using LoraRanks = std::integer_sequence<int, 0, 32, 48, 64, 80, 96, 112, 128>;`.\n3. Recompile the project.\n4. Revert the changes made to `src/kernels/zgemm/gemm_w4a4_launch.cuh`.\n5. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub release page for version 0.2.0. (F) 1. For the build error, reduce the supported LoRA ranks by modifying the file `src/kernels/zgemm/gemm_w4a4_launch.cuh`.\n2. Change the line to `using LoraRanks = std::integer_sequence<int, 0, 32, 48, 64, 80, 96, 112, 128>;`.\n3. Recompile the project.\n4. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub release page for version 0.2.0. (G) 1. Delete the file `src/kernels/zgemm/gemm_w4a4_launch.cuh`.\n2. For the build error, reduce the supported LoRA ranks by modifying the file `src/kernels/zgemm/gemm_w4a4_launch.cuh`.\n3. Change the line to `using LoraRanks = std::integer_sequence<int, 0, 32, 48, 64, 80, 96, 112, 128>;`.\n4. Recompile the project.\n5. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub release page for version 0.2.0.", "answer": "F"}
{"uuid": "5353a42b-f246-43d1-9648-9801307dd456", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face](https://huggingface.co/nunchaku-tech) or [ModelScope](https://modelscope.cn/organization/nunchaku-tech).  \n   Example:  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux1\n   ```\n\n## 4. Basic Inference\n1. **Run Example Script**  \n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path ./nunchaku-flux1\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --controlnet-config controlnet.yaml\n   ```\n\n## 6. ComfyUI Integration\n1. **Install ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n## 7. Custom Quantization (Optional)\n1. **Use DeepCompressor**  \n   Follow instructions at [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor).\n\n## 8. Verification\n1. **Check GPU Utilization**  \n   ```bash\n   nvidia-smi\n   ```\n   Ensure GPU memory usage aligns with model requirements (≥4GiB for FLUX.1).\n\n## 9. Troubleshooting\n- **Common Issues**:  \n  - CUDA version mismatch: Reinstall PyTorch with correct CUDA version.  \n  - Out-of-memory: Enable CPU offloading via `--cpu-offload` flag.  \n- **Support**: Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm).", "issue_title": "[Bug] t5 encoder problem", "issue_body": "i use it in flux comfyui workflow, the textencoder node seems needing three files,clip-L t5 and svdq-t5. The memory usage is huge,if we can only use clip-L.safetensor and svdq-t5.safetensors(3G file only),it will be great.", "choices": "(A) 1. Upgrade both `nunchaku` and `ComfyUI-nunchaku` to version v0.2.0 using the prebuilt wheels available on Hugging Face, ModelScope, or the GitHub release page.\n2. Set the environment variable `NUNCHAKU_LOAD_METHOD` to either `READ` or `READNOPIN` by running `set NUNCHAKU_LOAD_METHOD=READ` or `set NUNCHAKU_LOAD_METHOD=READNOPIN` in the command line. (B) 1. Set the environment variable `NUNCHAKU_LOAD_METHOD` to either `READ` or `READNOPIN` by running `set NUNCHAKU_LOAD_METHOD=READ` or `set NUNCHAKU_LOAD_METHOD=READNOPIN` in the command line.\n2. Uninstall both `nunchaku` and `ComfyUI-nunchaku` using `pip uninstall nunchaku ComfyUI-nunchaku`.\n3. Upgrade both `nunchaku` and `ComfyUI-nunchaku` to version v0.2.0 using the prebuilt wheels available on Hugging Face, ModelScope, or the GitHub release page. (C) 1. Set the environment variable `NUNCHAKU_LOAD_METHOD` to either `READ` or `READNOPIN` by running `set NUNCHAKU_LOAD_METHOD=READ` or `set NUNCHAKU_LOAD_METHOD=READNOPIN` in the command line.\n2. Alternatively, upgrade both `nunchaku` and `ComfyUI-nunchaku` to version v0.2.0 using the prebuilt wheels available on Hugging Face, ModelScope, or the GitHub release page. (D) 1. Upgrade both `nunchaku` and `ComfyUI-nunchaku` to version v0.2.0 using the prebuilt wheels available on Hugging Face, ModelScope, or the GitHub release page.", "answer": "C"}
{"uuid": "200bffde-8056-4128-b8f7-5d343927e626", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone the repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Create a virtual environment** (Optional but recommended)  \n   ```bash\n   python -m venv nunchaku-env\n   source nunchaku-env/bin/activate  # Linux/Mac\n   .\\nunchaku-env\\Scripts\\activate   # Windows\n   ```\n\n3. **Install dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **Install Nunchaku**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU compatibility**  \n   ```bash\n   python -c \"import torch; print(torch.cuda.is_available())\"\n   ```\n\n2. **Run a basic test**  \n   ```bash\n   python examples/flux.1-dev-double_cache.py\n   ```\n\n## 4. Model Quantization (Optional)\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n\n2. **Quantize a custom model**  \n   Refer to [DeepCompressor docs](https://github.com/nunchaku-tech/deepcompressor) for specific commands.\n\n## 5. Run Inference\n1. **Download pre-quantized models**  \n   ```bash\n   huggingface-cli download nunchaku-tech/nunchaku-t5 --local-dir ./models\n   ```\n\n2. **Execute inference script**  \n   ```bash\n   python examples/flux.1-kontext-dev.py\n   ```\n\n## 6. ComfyUI Integration (Optional)\n1. **Install ComfyUI plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI** and load Nunchaku nodes.\n\n## 7. Troubleshooting\n- **CUDA errors**: Verify CUDA version matches PyTorch build.\n- **Memory issues**: Enable `--low-mem` flag or use CPU offloading.\n- **Community support**: Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm).\n\n## 8. Uninstallation\n```bash\npip uninstall nunchaku\ndeactivate  # If using virtual environment\nrm -rf nunchaku-env  # Optional\n```", "issue_title": "lora加载转换节点的使用方法", "issue_body": "V0.1.4LORA加载转换节点，加载FLUX使用auto进行转换报下列错误\n!!! Exception during processing !!! could not open file <C:\\Users\\MINGZH~1\\AppData\\Local\\Temp\\tmpdodo69cg.safetensors> in read-only mode; error code: <32>\nTraceback (most recent call last):\n  File \"D:\\Program Files\\ComfyUI\\execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Program Files\\ComfyUI\\execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Program Files\\ComfyUI\\execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"D:\\Program Files\\ComfyUI\\execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Program Files\\ComfyUI\\custom_nodes\\svdquant\\nodes\\lora\\flux.py\", line 115, in load_lora\n    model.model.diffusion_model.model.update_lora_params(tmp_file.name)\n  File \"C:\\Users\\Mingzhenwang\\AppData\\Roaming\\Python\\Python311\\site-packages\\nunchaku\\models\\transformers\\transformer_flux.py\", line 180, in update_lora_params\n    state_dict = load_state_dict_in_safetensors(path)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Mingzhenwang\\AppData\\Roaming\\Python\\Python311\\site-packages\\nunchaku\\utils.py\", line 50, in load_state_dict_in_safetensors\n    with safetensors.safe_open(fetch_or_download(path), framework=\"pt\", device=device) as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: could not open file <C:\\Users\\MINGZH~1\\AppData\\Local\\Temp\\tmpdodo69cg.safetensors> in read-only mode; error code: <32>", "choices": "(A) 1. Ensure your RTX 2060 Super GPU supports CUDA 12.6 drivers and torch 2.5.1+cu124. 2. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or higher) to enable support for 20-series GPUs. 3. Disable INT4 tensor cores in BIOS. 4. Verify that your GPU has INT4 tensor cores, as this is a hardware requirement for nunchaku. (B) 1. Ensure your RTX 2060 Super GPU supports CUDA 12.6 drivers and torch 2.5.1+cu124. 2. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or higher) to enable support for 20-series GPUs. (C) 1. Verify that your GPU has INT4 tensor cores, as this is a hardware requirement for nunchaku. 2. Ensure your RTX 2060 Super GPU supports CUDA 12.6 drivers and torch 2.5.1+cu124. 3. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or higher) to enable support for 20-series GPUs. (D) 1. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or higher) to enable support for 20-series GPUs. 2. Ensure your RTX 2060 Super GPU supports CUDA 12.6 drivers and torch 2.5.1+cu124. 3. Verify that your GPU has INT4 tensor cores, as this is a hardware requirement for nunchaku. (E) 1. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or higher) to enable support for 20-series GPUs. 2. Verify that your GPU has INT4 tensor cores, as this is a hardware requirement for nunchaku. (F) 1. Ensure your RTX 2060 Super GPU supports CUDA 12.6 drivers and torch 2.5.1+cu124. 2. Downgrade CUDA drivers to version 10.0. 3. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or higher) to enable support for 20-series GPUs. 4. Verify that your GPU has INT4 tensor cores, as this is a hardware requirement for nunchaku. (G) 1. Ensure your RTX 2060 Super GPU supports CUDA 12.6 drivers and torch 2.5.1+cu124. 2. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or higher) to enable support for 20-series GPUs. 3. Verify that your GPU has INT4 tensor cores, as this is a hardware requirement for nunchaku.", "answer": "G"}
{"uuid": "630f36d3-fe03-462d-b85d-3778e5cc32da", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 4090/5090 recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   ```bash\n   huggingface-cli download nunchaku-tech/nunchaku-flux1 --local-dir ./models\n   ```\n\n2. **Verify Model Integrity**  \n   ```bash\n   python -c \"from nunchaku import check_model; check_model('./models/flux1.4bit')\"\n   ```\n\n## 4. Basic Inference\n1. **Run Text-to-Image Demo**  \n   ```bash\n   python examples/flux.1-t2i.py --model ./models/flux1.4bit --prompt \"A cat wearing sunglasses\"\n   ```\n\n2. **Enable FP16 Attention (Optional)**  \n   Add `--use-fp16-attn` flag for faster inference.\n\n## 5. Advanced Features\n1. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model ./models/flux1.4bit --loras lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --model ./models/flux1.4bit --controlnet canny\n   ```\n\n## 6. Performance Optimization\n1. **Enable Double FB Cache**  \n   ```bash\n   export NUNCHAKU_USE_DOUBLE_CACHE=1\n   ```\n\n2. **Benchmark Mode**  \n   ```bash\n   python -m nunchaku.benchmark --model ./models/flux1.4bit --batch-size 4\n   ```\n\n## 7. Deployment Options\n1. **ComfyUI Integration**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Gradio Web Demo**  \n   ```bash\n   python app/flux1/t2i/app.py --share\n   ```\n\n## 8. Verification\n1. **Check GPU Utilization**  \n   ```bash\n   nvidia-smi --query-gpu=utilization.gpu --format=csv\n   ```\n\n2. **Validate Output Quality**  \n   Inspect generated images in `./outputs/` directory.", "issue_title": "Preliminary Windows Wheels Released!", "issue_body": "Hi, we have released Windows wheel [here](https://huggingface.co/mit-han-lab/nunchaku/blob/main/nunchaku-0.1.4%2Btorch2.6-cp312-cp312-win_amd64.whl) with Python 3.11 and 3.12 thanks to @sxtyzhangzk . After installing PyTorch 2.6 and ComfyUI, you can simply run \n```shell\npip install https://huggingface.co/mit-han-lab/nunchaku/resolve/main/nunchaku-0.1.4+torch2.6-cp312-cp312-win_amd64.whl\n```\nYou can also get the wheels from our [modelscope mirror](https://modelscope.cn/models/Lmxyy1999/nunchaku/files). More Windows wheels and support are on the way!", "choices": "(A) 1. Ensure you are using the correct versions of the required packages. For nunchaku-0.1.3, use svdquant node 0.1.4.\n2. Update all components to version 0.1.4, including the convert lora tool and the comfyui node.\n3. If you encounter an 'out of memory' error, check your GPU's VRAM and ensure it meets the requirements (e.g., 3060 12G).\n5. After updating, test the turbo lora to confirm it works as expected.\n4. Verify the compatibility of the lora file with the updated versions. (B) 1. Ensure you are using the correct versions of the required packages. For nunchaku-0.1.3, use svdquant node 0.1.4.\n2. Update all components to version 0.1.4, including the convert lora tool and the comfyui node.\n3. If you encounter an 'out of memory' error, check your GPU's VRAM and ensure it meets the requirements (e.g., 3060 12G).\n4. Verify the compatibility of the lora file with the updated versions.\n5. After updating, test the turbo lora to confirm it works as expected. (C) 1. Ensure you are using the correct versions of the required packages. For nunchaku-0.1.3, use svdquant node 0.1.4.\n2. Update all components to version 0.1.4, including the convert lora tool and the comfyui node.\n6. If you encounter an 'out of memory' error, ignore it and continue execution.\n3. If you encounter an 'out of memory' error, check your GPU's VRAM and ensure it meets the requirements (e.g., 3060 12G).\n4. Verify the compatibility of the lora file with the updated versions.\n5. After updating, test the turbo lora to confirm it works as expected. (D) 1. Ensure you are using the correct versions of the required packages. For nunchaku-0.1.3, use svdquant node 0.1.4.\n3. If you encounter an 'out of memory' error, check your GPU's VRAM and ensure it meets the requirements (e.g., 3060 12G).\n4. Verify the compatibility of the lora file with the updated versions.\n5. After updating, test the turbo lora to confirm it works as expected. (E) 1. Ensure you are using the correct versions of the required packages. For nunchaku-0.1.3, use svdquant node 0.1.4.\n3. If you encounter an 'out of memory' error, check your GPU's VRAM and ensure it meets the requirements (e.g., 3060 12G).\n2. Update all components to version 0.1.4, including the convert lora tool and the comfyui node.\n4. Verify the compatibility of the lora file with the updated versions.\n5. After updating, test the turbo lora to confirm it works as expected. (F) 1. Ensure you are using the correct versions of the required packages. For nunchaku-0.1.3, use svdquant node 0.1.4.\n2. Update all components to version 0.1.4, including the convert lora tool and the comfyui node.\n6. Downgrade the convert lora tool to version 0.1.3.\n3. If you encounter an 'out of memory' error, check your GPU's VRAM and ensure it meets the requirements (e.g., 3060 12G).\n4. Verify the compatibility of the lora file with the updated versions.\n5. After updating, test the turbo lora to confirm it works as expected. (G) 1. Ensure you are using the correct versions of the required packages. For nunchaku-0.1.3, use svdquant node 0.1.4.\n2. Update all components to version 0.1.4, including the convert lora tool and the comfyui node.\n3. If you encounter an 'out of memory' error, check your GPU's VRAM and ensure it meets the requirements (e.g., 3060 12G).\n4. After updating, test the turbo lora to confirm it works as expected.", "answer": "B"}
{"uuid": "84296b02-55fe-4f54-b605-f75b236bf385", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: \n  - CUDA Toolkit (version >= 12.0)\n  - Python (version >= 3.8)\n  - PyTorch (version >= 2.0)\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_compatibility())\"\n   ```\n\n2. **Run a Test Script**  \n   ```bash\n   python examples/flux.1-kontext-dev.py\n   ```\n\n## 4. Basic Usage\n1. **Load a Pre-Quantized Model**  \n   ```python\n   from nunchaku import load_model\n   model = load_model(\"nunchaku-tech/flux.1-4bit\")\n   ```\n\n2. **Run Inference**  \n   ```python\n   output = model.generate(prompt=\"A cat sitting on a couch\")\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```python\n   model.load_lora([\"lora1.safetensors\", \"lora2.safetensors\"])\n   ```\n\n2. **ControlNet Integration**  \n   ```python\n   model.enable_controlnet(\"controlnet-union-pro-2.0\")\n   ```\n\n3. **Low-Memory Mode**  \n   ```python\n   model.enable_cpu_offload()\n   ```\n\n## 6. Custom Model Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n\n2. **Quantize a Model**  \n   ```python\n   from deepcompressor import quantize\n   quantize(\"input_model.pth\", \"output_model_4bit.nk\", bits=4)\n   ```\n\n## 7. ComfyUI Integration\n1. **Install ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cp -r ComfyUI-nunchaku /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI**  \n   ```bash\n   cd /path/to/ComfyUI\n   python main.py\n   ```\n\n## 8. Troubleshooting\n- **Common Issues**:  \n  - CUDA errors: Reinstall CUDA Toolkit and PyTorch with CUDA support.\n  - Memory issues: Enable `cpu_offload` or reduce batch size.\n- **Community Support**:  \n  - [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q)\n  - [Discord](https://discord.gg/Wk6PnwX9Sm)\n\n## 9. Documentation\n- Explore full documentation at [nunchaku.tech/docs](https://nunchaku.tech/docs/nunchaku/)", "issue_title": "Build Error for 0.1.4 in windows10", "issue_body": "FAILED: E:/ComfyUI_lester/python_embeded/tmp/nunchaku/build/temp.win-amd64-cpython-312/Release/src/kernels/zgemm/gemm_w4a4_launch_fp16.obj\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\bin\\nvcc --generate-dependencies-with-compile --dependency-output E:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\build\\temp.win-amd64-cpython-312\\Release\\src\\kernels\\zgemm\\gemm_w4a4_launch_fp16.obj.d -std=c++17 --use-local-env -Xcompiler /MD -Xcompiler /wd4819 -Xcompiler /wd4251 -Xcompiler /wd4244 -Xcompiler /wd4267 -Xcompiler /wd4275 -Xcompiler /wd4018 -Xcompiler /wd4190 -Xcompiler /wd4624 -Xcompiler /wd4067 -Xcompiler /wd4068 -Xcompiler /EHsc -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -IE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\src -IE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party/cutlass/include -IE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party/json/include -IE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party/mio/include -IE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party/spdlog/include -IE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party/Block-Sparse-Attention/csrc/block_sparse_attn -IE:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\torch\\include -IE:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include -IE:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\torch\\include\\TH -IE:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\torch\\include\\THC \"-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\include\" -IE:\\ComfyUI_lester\\python_embeded\\include -IE:\\ComfyUI_lester\\python_embeded\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.20348.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.20348.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.20348.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.20348.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.20348.0\\\\cppwinrt\" -c E:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\src\\kernels\\zgemm\\gemm_w4a4_launch_fp16.cu -o E:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\build\\temp.win-amd64-cpython-312\\Release\\src\\kernels\\zgemm\\gemm_w4a4_launch_fp16.obj -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=1 --expt-relaxed-constexpr --expt-extended-lambda --ptxas-options=--allow-expensive-optimizations=true --generate-line-info -gencode arch=compute_89,code=sm_89 -Xcompiler /Zc:__cplusplus -Xcompiler /FS -Xcompiler /bigobj -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\ncl: 命令行 warning D9025 :正在重写“/D__CUDA_NO_HALF_OPERATORS__”(用“/U__CUDA_NO_HALF_OPERATORS__”)\ncl: 命令行 warning D9025 :正在重写“/D__CUDA_NO_HALF_CONVERSIONS__”(用“/U__CUDA_NO_HALF_CONVERSIONS__”)\ncl: 命令行 warning D9025 :正在重写“/D__CUDA_NO_HALF2_OPERATORS__”(用“/U__CUDA_NO_HALF2_OPERATORS__”)\ncl: 命令行 warning D9025 :正在重写“/D__CUDA_NO_BFLOAT16_CONVERSIONS__”(用“/U__CUDA_NO_BFLOAT16_CONVERSIONS__”)\ngemm_w4a4_launch_fp16.cu\ncl: 命令行 warning D9025 :正在重写“/D__CUDA_NO_HALF_OPERATORS__”(用“/U__CUDA_NO_HALF_OPERATORS__”)\ncl: 命令行 warning D9025 :正在重写“/D__CUDA_NO_HALF_CONVERSIONS__”(用“/U__CUDA_NO_HALF_CONVERSIONS__”)\ncl: 命令行 warning D9025 :正在重写“/D__CUDA_NO_HALF2_OPERATORS__”(用“/U__CUDA_NO_HALF2_OPERATORS__”)\ncl: 命令行 warning D9025 :正在重写“/D__CUDA_NO_BFLOAT16_CONVERSIONS__”(用“/U__CUDA_NO_BFLOAT16_CONVERSIONS__”)\ngemm_w4a4_launch_fp16.cu\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n             ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n                       ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n                                 ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n                                           ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n                                                     ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n                                                               ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3274): warning #27-D: character value is out of range\n           U\"\\x800001ae\\x8000002b\"[index];\n             ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3274): warning #27-D: character value is out of range\n           U\"\\x800001ae\\x8000002b\"[index];\n                       ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n             ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n                       ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n                                 ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n                                           ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n                                                     ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3273): warning #27-D: character value is out of range\n    return U\"\\x9999999a\\x828f5c29\\x80418938\\x80068db9\\x8000a7c6\\x800010c7\"\n                                                               ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3274): warning #27-D: character value is out of range\n           U\"\\x800001ae\\x8000002b\"[index];\n             ^\n\nE:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\third_party\\spdlog\\include\\spdlog\\fmt\\bundled\\format.h(3274): warning #27-D: character value is out of range\n           U\"\\x800001ae\\x8000002b\"[index];\n                       ^\n\nnvcc warning : incompatible redefinition for option 'std', the last value of this option was used\ngemm_w4a4_launch_fp16.cu\ntmpxft_00002018_00000000-7_gemm_w4a4_launch_fp16.cudafe1.cpp\nC:\\Users\\computer\\AppData\\Local\\Temp\\tmpxft_00002018_00000000-4_gemm_w4a4_launch_fp16.fatbin.c(4308203): fatal error C1060: 编译器的堆空间不足\nninja: build stopped: subcommand failed.\nTraceback (most recent call last):\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 2209, in _run_ninja_build\n    subprocess.run(\n  File \"subprocess.py\", line 571, in run\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"E:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\setup.py\", line 176, in <module>\n    setuptools.setup(\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\__init__.py\", line 117, in setup\n    return distutils.core.setup(**attrs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 186, in setup\n    return run_commands(dist)\n           ^^^^^^^^^^^^^^^^^^\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 202, in run_commands\n    dist.run_commands()\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 983, in run_commands\n    self.run_command(cmd)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\dist.py\", line 999, in run_command\n    super().run_command(command)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1002, in run_command\n    cmd_obj.run()\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\command\\bdist_wheel.py\", line 369, in run\n    self.run_command(\"build\")\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 339, in run_command\n    self.distribution.run_command(command)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\dist.py\", line 999, in run_command\n    super().run_command(command)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1002, in run_command\n    cmd_obj.run()\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 136, in run\n    self.run_command(cmd_name)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 339, in run_command\n    self.distribution.run_command(command)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\dist.py\", line 999, in run_command\n    super().run_command(command)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1002, in run_command\n    cmd_obj.run()\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\command\\build_ext.py\", line 99, in run\n    _build_ext.run(self)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 365, in run\n    self.build_extensions()\n  File \"E:\\ComfyUI_lester\\python_embeded\\tmp\\nunchaku\\setup.py\", line 24, in build_extensions\n    super().build_extensions()\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 900, in build_extensions\n    build_ext.build_extensions(self)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 481, in build_extensions\n    self._build_extensions_serial()\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 507, in _build_extensions_serial\n    self.build_extension(ext)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\command\\build_ext.py\", line 264, in build_extension\n    _build_ext.build_extension(self, ext)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\Cython\\Distutils\\build_ext.py\", line 135, in build_extension\n    super(build_ext, self).build_extension(ext)\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\setuptools\\_distutils\\command\\build_ext.py\", line 562, in build_extension\n    objects = self.compiler.compile(\n              ^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 872, in win_wrap_ninja_compile\n    _write_ninja_file_and_compile_objects(\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1869, in _write_ninja_file_and_compile_objects\n    _run_ninja_build(\n  File \"E:\\ComfyUI_lester\\python_embeded\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 2225, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error compiling objects for extension", "choices": "(A) 1. Open the file `comfyui/nodes/models/flux.py` in your ComfyUI installation.\n2. Locate line 50 and replace the existing code with `img_ids = img_ids.reshape(-1, 3)`.\n3. Locate line 52 and replace the existing code with `txt_ids = torch.zeros(context.shape[1], 3).to(device=x.device, dtype=x.dtype)`.\n4. Revert the changes made to line 50 by restoring the original code.\n5. Alternatively, upgrade to the latest version of the Nunchaku wheel (v0.3.2 or higher) to automatically apply these fixes. (B) 1. Locate line 50 and replace the existing code with `img_ids = img_ids.reshape(-1, 3)`.\n2. Locate line 52 and replace the existing code with `txt_ids = torch.zeros(context.shape[1], 3).to(device=x.device, dtype=x.dtype)`.\n3. Alternatively, upgrade to the latest version of the Nunchaku wheel (v0.3.2 or higher) to automatically apply these fixes. (C) 1. Open the file `comfyui/nodes/models/flux.py` in your ComfyUI installation.\n2. Locate line 50 and replace the existing code with `img_ids = img_ids.reshape(-1, 3)`.\n3. Locate line 52 and replace the existing code with `txt_ids = torch.zeros(context.shape[1], 3).to(device=x.device, dtype=x.dtype)`.\n4. Delete the file `comfyui/nodes/models/flux.py`.\n5. Alternatively, upgrade to the latest version of the Nunchaku wheel (v0.3.2 or higher) to automatically apply these fixes. (D) 1. Open the file `comfyui/nodes/models/flux.py` in your ComfyUI installation.\n2. Locate line 52 and replace the existing code with `txt_ids = torch.zeros(context.shape[1], 3).to(device=x.device, dtype=x.dtype)`.\n3. Locate line 50 and replace the existing code with `img_ids = img_ids.reshape(-1, 3)`.\n4. Alternatively, upgrade to the latest version of the Nunchaku wheel (v0.3.2 or higher) to automatically apply these fixes. (E) 1. Open the file `comfyui/nodes/models/flux.py` in your ComfyUI installation.\n2. Locate line 50 and replace the existing code with `img_ids = img_ids.reshape(-1, 3)`.\n3. Locate line 52 and replace the existing code with `txt_ids = torch.zeros(context.shape[1], 3).to(device=x.device, dtype=x.dtype)`.\n4. Alternatively, upgrade to the latest version of the Nunchaku wheel (v0.3.2 or higher) to automatically apply these fixes. (F) 1. Open the file `comfyui/nodes/models/flux.py` in your ComfyUI installation.\n2. Locate line 52 and replace the existing code with `txt_ids = torch.zeros(context.shape[1], 3).to(device=x.device, dtype=x.dtype)`.\n3. Alternatively, upgrade to the latest version of the Nunchaku wheel (v0.3.2 or higher) to automatically apply these fixes. (G) 1. Alternatively, upgrade to the latest version of the Nunchaku wheel (v0.3.2 or higher) to automatically apply these fixes.\n2. Open the file `comfyui/nodes/models/flux.py` in your ComfyUI installation.\n3. Locate line 50 and replace the existing code with `img_ids = img_ids.reshape(-1, 3)`.\n4. Locate line 52 and replace the existing code with `txt_ids = torch.zeros(context.shape[1], 3).to(device=x.device, dtype=x.dtype)`.", "answer": "E"}
{"uuid": "80ea8248-8ea2-4b21-9cf6-077fad2ea5d2", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_compatibility())\"\n   ```\n\n## 4. Basic Usage\n1. **Run Example Script**  \n   ```bash\n   python examples/flux.1-kontext-dev.py\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n## 6. Custom Model Quantization\n1. **Quantize Custom Model**  \n   ```bash\n   python -m deepcompressor.quantize --model_path your_model.ckpt --output quantized_model.nk\n   ```\n\n## 7. ComfyUI Integration\n1. **Install ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku && pip install -e .\n   ```\n\n## 8. Troubleshooting\n- **Memory Issues**: Use per-layer CPU offloading  \n  ```python\n  nunchaku.set_config(memory_mode='low')\n  ```\n- **Performance Tuning**: Enable FP16 attention  \n  ```python\n  nunchaku.set_config(use_fp16_attn=True)\n  ```\n\n## 9. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for assistance.", "issue_title": "Support for RTX 2060 Super 8GB VRAM?", "issue_body": "Does this support the second generation of Nvidia RTX cards, like the RTX 2060 8GB Super, which has 8GB VRAM? Will it work with less than 8GB VRAM?", "choices": "(A) 1. Update to the latest version of the software (v0.3.0dev1) as mentioned in the comments. 2. Skip checking batch size and image resolution compatibility. 3. Test the inpainting workflow again with the provided example to confirm the issue is resolved. (B) 1. Ensure that the batch size and image resolution are compatible with the updated version. 2. Test the inpainting workflow again with the provided example to confirm the issue is resolved. (C) 1. Test the inpainting workflow again with the provided example to confirm the issue is resolved. 2. Update to the latest version of the software (v0.3.0dev1) as mentioned in the comments. 3. Ensure that the batch size and image resolution are compatible with the updated version. (D) 1. Update to the latest version of the software (v0.3.0dev1) as mentioned in the comments. 2. Downgrade to the previous version of the software (v0.2.0). 3. Ensure that the batch size and image resolution are compatible with the updated version. 4. Test the inpainting workflow again with the provided example to confirm the issue is resolved. (E) 1. Update to the latest version of the software (v0.3.0dev1) as mentioned in the comments. 2. Ensure that the batch size and image resolution are compatible with the updated version. (F) 1. Ensure that the batch size and image resolution are compatible with the updated version. 2. Update to the latest version of the software (v0.3.0dev1) as mentioned in the comments. 3. Test the inpainting workflow again with the provided example to confirm the issue is resolved. (G) 1. Update to the latest version of the software (v0.3.0dev1) as mentioned in the comments. 2. Ensure that the batch size and image resolution are compatible with the updated version. 3. Test the inpainting workflow again with the provided example to confirm the issue is resolved.", "answer": "G"}
{"uuid": "adb867f0-290b-445c-9f26-6fe4d23d994f", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: \n  - Python 3.8+\n  - CUDA 12.1+\n  - PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   Example for FLUX.1-Kontext:  \n   ```bash\n   wget https://huggingface.co/nunchaku-tech/flux.1-kontext/resolve/main/model.nk\n   ```\n\n2. **Verify Model Compatibility**  \n   Check supported architectures in [documentation](https://nunchaku.tech/docs/nunchaku/).\n\n## 4. Basic Inference\n1. **Run Example Script**  \n   ```bash\n   python examples/flux.1-kontext-dev.py --model_path ./model.nk\n   ```\n\n2. **Customize Inference**  \n   Modify batch size/resolution:  \n   ```python\n   from nunchaku import Nunchaku\n   model = Nunchaku.from_pretrained(\"model.nk\")\n   outputs = model.generate(prompts=[\"your_prompt\"], height=512, width=512)\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model model.nk --lora lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --model model.nk --controlnet controlnet.nk\n   ```\n\n## 6. Performance Optimization\n1. **Enable FP16 Attention**  \n   Add to your script:  \n   ```python\n   model.enable_fp16_attention()\n   ```\n\n2. **Activate First-Block Cache**  \n   ```python\n   model.enable_fb_cache()\n   ```\n\n## 7. Deployment Options\n1. **ComfyUI Integration**  \n   Follow [ComfyUI-nunchaku](https://github.com/nunchaku-tech/ComfyUI-nunchaku) setup.\n\n2. **Gradio Demo**  \n   ```bash\n   python app/flux.1/t2i/app.py\n   ```\n\n## 8. Monitoring\n- Check GPU utilization:  \n  ```bash\n  nvidia-smi -l 1\n  ```\n- Monitor memory usage in Python:  \n  ```python\n  print(model.get_memory_stats())\n  ```\n\n## 9. Troubleshooting\n- Common issues: See [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html)\n- Community support: [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm)", "issue_title": "new version (IMPORT FAILED): F:\\ComfyUI\\ComfyUI\\custom_nodes\\svdquant", "issue_body": "Traceback (most recent call last):\n  File \"F:\\ComfyUI\\ComfyUI\\nodes.py\", line 2147, in load_custom_node\n    module_spec.loader.exec_module(module)\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"F:\\ComfyUI\\ComfyUI\\custom_nodes\\svdquant\\__init__.py\", line 3, in <module>\n    from .nodes.lora import SVDQuantFluxLoraLoader\n  File \"F:\\ComfyUI\\ComfyUI\\custom_nodes\\svdquant\\nodes\\lora\\__init__.py\", line 1, in <module>\n    from .flux import SVDQuantFluxLoraLoader\n  File \"F:\\ComfyUI\\ComfyUI\\custom_nodes\\svdquant\\nodes\\lora\\flux.py\", line 7, in <module>\n    from nunchaku.lora.flux import comfyui2diffusers, convert_to_nunchaku_flux_lowrank_dict, detect_format, xlab2diffusers\nImportError: cannot import name 'comfyui2diffusers' from 'nunchaku.lora.flux' (F:\\ComfyUI\\python_embeded\\Lib\\site-packages\\nunchaku\\lora\\flux\\__init__.py)\n\nCannot import F:\\ComfyUI\\ComfyUI\\custom_nodes\\svdquant module for custom nodes: cannot import name 'comfyui2diffusers' from 'nunchaku.lora.flux' (F:\\ComfyUI\\python_embeded\\Lib\\site-packages\\nunchaku\\lora\\flux\\__init__.py)\n\n(IMPORT FAILED): F:\\ComfyUI\\ComfyUI\\custom_nodes\\svdquant", "choices": "(A) 1. Ensure you are using nunchaku v0.1.4 or later and svdquant node v0.1.5 or later.\n2. Verify that your `extra_model_paths.yaml` file is correctly configured to point to the desired `diffusion_models` directory.\n3. Delete the `extra_model_paths.yaml` file to clear any potential conflicts.\n4. Restart ComfyUI to apply the changes and ensure the plugin recognizes the directory. (B) 1. Verify that your `extra_model_paths.yaml` file is correctly configured to point to the desired `diffusion_models` directory.\n2. Restart ComfyUI to apply the changes and ensure the plugin recognizes the directory. (C) 1. Ensure you are using nunchaku v0.1.4 or later and svdquant node v0.1.5 or later.\n2. Verify that your `extra_model_paths.yaml` file is correctly configured to point to the desired `diffusion_models` directory.\n3. Restart ComfyUI to apply the changes and ensure the plugin recognizes the directory. (D) 1. Ensure you are using nunchaku v0.1.4 or later and svdquant node v0.1.5 or later.\n2. Restart ComfyUI to apply the changes and ensure the plugin recognizes the directory.\n3. Verify that your `extra_model_paths.yaml` file is correctly configured to point to the desired `diffusion_models` directory.", "answer": "C"}
{"uuid": "c46a0390-1785-45d2-9315-4097e16056b5", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.7+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_compatibility())\"\n   ```\n\n2. **Run a Basic Test**  \n   ```bash\n   python examples/flux.1-dev-basic.py\n   ```\n\n## 4. Model Deployment\n1. **Download Pre-Quantized Models**  \n   ```bash\n   huggingface-cli download nunchaku-tech/nunchaku-flux.1 --local-dir ./models/flux.1\n   ```\n\n2. **Run Inference**  \n   ```bash\n   python examples/flux.1-dev-inference.py --model-path ./models/flux.1\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --lora-weights ./lora/example1.safetensors ./lora/example2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --controlnet-model ./controlnet/canny\n   ```\n\n## 6. Performance Optimization\n1. **Enable FP16 Attention**  \n   Add `--use-fp16-attention` to your inference script.\n\n2. **Activate First-Block Cache**  \n   Add `--enable-fb-cache` to reduce memory usage.\n\n## 7. ComfyUI Integration (Optional)\n1. **Install ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI**  \n   Ensure the plugin appears in your ComfyUI workflow editor.\n\n## 8. Troubleshooting\n- **Memory Issues**: Use `--low-memory` flag or enable CPU offloading with `--cpu-offload`.\n- **Performance Bugs**: Check [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html) or open a GitHub issue.", "issue_title": "ComfyUI how to remove the warning \"3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\"", "issue_body": "![Image](https://github.com/user-attachments/assets/69bb0cf7-6384-4741-805e-6ddc171399f5)\n\nHello! Is there a way to remove these annoying notification? How to edit my comfyUI setup to eliminate this warning during generation?\n**Passing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor**\n\nMy setup:\nWindows 10\nPython version: 3.12.8\nTotal VRAM 24563 MB, total RAM 65288 MB\npytorch version: 2.6.0+cu126\nxformers version: 0.0.29.post2\nSet vram state to: NORMAL_VRAM\nDevice: cuda:0 NVIDIA GeForce RTX 4090 : cudaMallocAsync\nUsing xformers attention\nComfyUI version: 0.3.19\n\n```\ngot prompt\nUsing xformers attention in VAE\nUsing xformers attention in VAE\nVAE load device: cuda:0, offload device: cpu, dtype: torch.bfloat16\n[2025-03-06 19:31:56.331] [info] Initializing QuantizedFluxModel\n[2025-03-06 19:31:56.404] [info] Loading weights from E:\\ComfyUI_windows_portable\\ComfyUI\\models\\diffusion_models\\svdq-int4-flux.1-dev\\transformer_blocks.safetensors\n[2025-03-06 19:31:56.405] [warning] Unable to pin memory: operation not supported\n[2025-03-06 19:32:01.205] [info] Done.\nmodel_type FLUX\nCLIP/text encoder model load device: cuda:0, offload device: cpu, current: cpu, dtype: torch.float16\nclip missing: ['text_projection.weight']\nRequested to load FluxClipModel_\nloaded completely 15121.75376586914 4777.53759765625 True\nRequested to load Flux\nloaded completely 21564.187284179687 122.3087158203125 True\n  0%|                                                                                                                                                   | 0/20 [00:00<?, ?it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n  5%|██████▉                                                                                                                                    | 1/20 [00:00<00:05,  3.38it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 10%|█████████████▉                                                                                                                             | 2/20 [00:00<00:04,  3.61it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 15%|████████████████████▊                                                                                                                      | 3/20 [00:00<00:04,  3.54it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 20%|███████████████████████████▊                                                                                                               | 4/20 [00:01<00:04,  3.51it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 25%|██████████████████████████████████▊                                                                                                        | 5/20 [00:01<00:04,  3.50it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 30%|█████████████████████████████████████████▋                                                                                                 | 6/20 [00:01<00:03,  3.50it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 35%|████████████████████████████████████████████████▋                                                                                          | 7/20 [00:01<00:03,  3.49it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 40%|███████████████████████████████████████████████████████▌                                                                                   | 8/20 [00:02<00:03,  3.51it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 45%|██████████████████████████████████████████████████████████████▌                                                                            | 9/20 [00:02<00:03,  3.50it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 50%|█████████████████████████████████████████████████████████████████████                                                                     | 10/20 [00:02<00:02,  3.51it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 55%|███████████████████████████████████████████████████████████████████████████▉                                                              | 11/20 [00:03<00:02,  3.50it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 60%|██████████████████████████████████████████████████████████████████████████████████▊                                                       | 12/20 [00:03<00:02,  3.51it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 65%|█████████████████████████████████████████████████████████████████████████████████████████▋                                                | 13/20 [00:03<00:01,  3.51it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 70%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                         | 14/20 [00:03<00:01,  3.50it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 15/20 [00:04<00:01,  3.50it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 16/20 [00:04<00:01,  3.48it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 17/20 [00:04<00:00,  3.47it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 18/20 [00:05<00:00,  3.46it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 19/20 [00:05<00:00,  3.46it/s]Passing `txt_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\nPassing `img_ids` 3d torch.Tensor is deprecated.Please remove the batch dimension and pass it as a 2d torch Tensor\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:05<00:00,  3.49it/s]\nRequested to load AutoencodingEngine\nloaded completely 6807.416160583496 159.87335777282715 True\nPrompt executed in 14.55 seconds\n```", "choices": "(A) 1. Install `deepcompressor` to enable a 4-bit text encoder for reducing VRAM usage:\n```shell\npip install poetry\ngit clone https://github.com/mit-han-lab/deepcompressor\ncd deepcompressor\n```\n2. Alternatively, use CPU offloading for the CLIP model to reduce GPU memory usage by loading the CLIP model on the CPU instead of the GPU. (B) 1. Install `deepcompressor` to enable a 4-bit text encoder for reducing VRAM usage:\n```shell\npip install poetry\ngit clone https://github.com/mit-han-lab/deepcompressor\ncd deepcompressor\npoetry install\nrm -rf deepcompressor\n```\n2. Alternatively, use CPU offloading for the CLIP model to reduce GPU memory usage by loading the CLIP model on the CPU instead of the GPU. (C) 1. Install `deepcompressor` to enable a 4-bit text encoder for reducing VRAM usage:\n```shell\npip install poetry\npoetry install\ngit clone https://github.com/mit-han-lab/deepcompressor\ncd deepcompressor\n```\n2. Alternatively, use CPU offloading for the CLIP model to reduce GPU memory usage by loading the CLIP model on the CPU instead of the GPU. (D) 1. Install `deepcompressor` to enable a 4-bit text encoder for reducing VRAM usage:\n```shell\npip install poetry\ngit clone https://github.com/mit-han-lab/deepcompressor\ncd deepcompressor\npoetry install\n```\n2. Alternatively, use CPU offloading for the CLIP model to reduce GPU memory usage by loading the CLIP model on the CPU instead of the GPU.", "answer": "D"}
{"uuid": "786c9389-9bfb-471d-b62e-eb9531090bd4", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_compatibility())\"\n   ```\n\n2. **Run Basic Test**  \n   ```bash\n   python examples/flux.1-dev-basic.py\n   ```\n\n## 4. Model Deployment\n1. **Download Pre-Quantized Models**  \n   ```bash\n   huggingface-cli download nunchaku-tech/nunchaku-flux1 --local-dir ./models\n   ```\n\n2. **Run Inference**  \n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path ./models/flux1-4bit.safetensors\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --controlnet canny\n   ```\n\n## 6. Performance Optimization\n1. **Enable FP16 Attention**  \n   Add `--use-fp16-attention` to inference scripts.\n\n2. **Activate First-Block Cache**  \n   Add `--enable-fb-cache` to reduce memory usage.\n\n## 7. Monitoring and Debugging\n1. **Memory Profiling**  \n   ```bash\n   nvidia-smi -l 1\n   ```\n\n2. **Latency Benchmarking**  \n   ```bash\n   python -m nunchaku.benchmark --model flux1-4bit\n   ```\n\n## 8. Community Resources\n- Join support channels: [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) | [Discord](https://discord.gg/Wk6PnwX9Sm)\n- Report issues: [GitHub Issues](https://github.com/nunchaku-tech/nunchaku/issues)", "issue_title": "User Slack, Discord and Wechat Groups for Discussion", "issue_body": "Hi everyone! I've created user groups on both Slack and WeChat to facilitate discussions and support each other. If you have any questions or issues while using SVDQuant or Nunchaku, or if you're interested in contributing to the codebase, feel free to join and share your thoughts. We're always happy to help!\n\nSlack link: https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q\nDiscord link: https://discord.gg/Wk6PnwX9Sm\nWechat:\n\n<img src=\"https://github.com/user-attachments/assets/7bb2206c-2ff9-4fbf-beb2-4431b25eafb8\" width=\"400\"/>", "choices": "(A) 1. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or later).\n2. Download the prebuilt wheels from Hugging Face, ModelScope, or the GitHub release page for installation.\n3. Delete the downloaded prebuilt wheels after installation.\n4. Ensure all dependencies are correctly installed and compatible with the new versions. (B) 1. Download the prebuilt wheels from Hugging Face, ModelScope, or the GitHub release page for installation.\n2. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or later).\n3. Ensure all dependencies are correctly installed and compatible with the new versions. (C) 1. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or later).\n2. Ensure all dependencies are correctly installed and compatible with the new versions.\n3. Download the prebuilt wheels from Hugging Face, ModelScope, or the GitHub release page for installation. (D) 1. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or later).\n2. Ensure all dependencies are correctly installed and compatible with the new versions. (E) 1. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or later).\n2. Download the prebuilt wheels from Hugging Face, ModelScope, or the GitHub release page for installation.\n3. Downgrade all dependencies to versions older than v0.1.0.\n4. Ensure all dependencies are correctly installed and compatible with the new versions. (F) 1. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or later).\n2. Download the prebuilt wheels from Hugging Face, ModelScope, or the GitHub release page for installation. (G) 1. Upgrade both nunchaku and ComfyUI-nunchaku to the latest version (v0.2.0 or later).\n2. Download the prebuilt wheels from Hugging Face, ModelScope, or the GitHub release page for installation.\n3. Ensure all dependencies are correctly installed and compatible with the new versions.", "answer": "G"}
{"uuid": "93dd8c87-f81a-4616-9270-fd8d5e31b6b6", "setup_instruct": "```markdown\n### Step 1: Review Project Resources\n- Read the [SVDQuant paper](http://arxiv.org/abs/2411.05007) to understand the theoretical foundation\n- Explore the [official documentation](https://nunchaku.tech/docs/nunchaku/) for comprehensive guides\n- Check [demo website](https://svdquant.mit.edu) to see capabilities\n\n### Step 2: Join Community Channels\n- Join Slack: `https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q`\n- Join Discord: `https://discord.gg/Wk6PnwX9Sm`\n- Scan WeChat QR code: `https://huggingface.co/datasets/nunchaku-tech/cdn/resolve/main/nunchaku/assets/wechat.jpg`\n\n### Step 3: Installation\n1. Follow installation guide:\n   ```bash\n   # Recommended to create new conda environment\n   conda create -n nunchaku python=3.10\n   conda activate nunchaku\n   ```\n2. Install via pip (check docs for latest version):\n   ```bash\n   pip install nunchaku\n   ```\n\n### Step 4: Verify Installation\n- Run basic test:\n  ```python\n  import nunchaku\n  print(nunchaku.__version__)\n  ```\n\n### Step 5: Explore Examples\n1. Clone repository:\n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku/examples\n   ```\n2. Try example scripts:\n   ```bash\n   python flux.1-kontext-dev.py\n   python flux.1-dev-double_cache.py\n   ```\n\n### Step 6: ComfyUI Integration (Optional)\n1. Install ComfyUI plugin:\n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   ```\n2. Follow setup instructions in the plugin's README\n\n### Step 7: Model Quantization (Advanced)\n1. Install DeepCompressor:\n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install -e .\n   ```\n2. Follow quantization guide in DeepCompressor docs\n\n### Step 8: Run Gradio Demos\n1. Navigate to app directory:\n   ```bash\n   cd ../app\n   ```\n2. Launch demo (e.g. for FLUX.1):\n   ```bash\n   python -m flux.1.t2i\n   ```\n\n### Step 9: Troubleshooting\n- Check [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html)\n- Review [issue tracker](https://github.com/nunchaku-tech/nunchaku/issues)\n- Ask questions in community channels from Step 2", "issue_title": "AssertionError: assert image_rotary_emb.shape[2] == batch_size * (txt_tokens + img_tokens)", "issue_body": "I get the following error when execute inpainting with **batch size=1**, using flux-fill workflow,\n\n```\nTraceback (most recent call last):\n  File \"/root/ComfyUI/execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"/root/ComfyUI/execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/nodes.py\", line 1542, in sample\n    return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/nodes.py\", line 1509, in common_ksampler\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/sample.py\", line 45, in sample\n    samples = sampler.sample(noise, positive, negative, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 1133, in sample\n    return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 1023, in sample\n    return cfg_guider.sample(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 1008, in sample\n    output = executor.execute(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/patcher_extension.py\", line 110, in execute\n    return self.original(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 976, in outer_sample\n    output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 959, in inner_sample\n    samples = executor.execute(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/patcher_extension.py\", line 110, in execute\n    return self.original(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 738, in sample\n    samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/k_diffusion/sampling.py\", line 161, in sample_euler\n    denoised = model(x, sigma_hat * s_in, **extra_args)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 390, in __call__\n    out = self.inner_model(x, sigma, model_options=model_options, seed=seed)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 939, in __call__\n    return self.predict_noise(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 942, in predict_noise\n    return sampling_function(self.inner_model, x, timestep, self.conds.get(\"negative\", None), self.conds.get(\"positive\", None), self.cfg, model_options=model_options, seed=seed)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 370, in sampling_function\n    out = calc_cond_batch(model, conds, x, timestep, model_options)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 206, in calc_cond_batch\n    return executor.execute(model, conds, x_in, timestep, model_options)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/patcher_extension.py\", line 110, in execute\n    return self.original(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/samplers.py\", line 319, in _calc_cond_batch\n    output = model.apply_model(input_x, timestep_, **c).chunk(batch_chunks)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/model_base.py\", line 133, in apply_model\n    return comfy.patcher_extension.WrapperExecutor.new_class_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/patcher_extension.py\", line 110, in execute\n    return self.original(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/comfy/model_base.py\", line 164, in _apply_model\n    model_output = self.diffusion_model(xc, t, context=context, control=control, transformer_options=transformer_options, **extra_conds).float()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/ComfyUI/custom_nodes/svdquant/nodes/models/flux.py\", line 53, in forward\n    out = self.model(\n          ^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/diffusers/models/transformers/transformer_flux.py\", line 522, in forward\n    encoder_hidden_states, hidden_states = block(\n                                           ^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/lib/python3.12/site-packages/nunchaku/models/transformer_flux.py\", line 49, in forward\n    assert image_rotary_emb.shape[2] == batch_size * (txt_tokens + img_tokens)\n```", "choices": "(A) 1. Update to Nunchaku v0.1.4 by downloading the corresponding version whl file. 2. Ensure all dependencies are correctly installed, including CUDA 12.6 if necessary. 3. Downgrade CUDA to version 11.0. 4. Verify the installation by running the workflow again. (B) 1. Verify the installation by running the workflow again. 2. Update to Nunchaku v0.1.4 by downloading the corresponding version whl file. 3. Ensure all dependencies are correctly installed, including CUDA 12.6 if necessary. (C) 1. Update to Nunchaku v0.1.4 by downloading the corresponding version whl file. 2. Ensure all dependencies are correctly installed, including CUDA 12.6 if necessary. 3. Verify the installation by running the workflow again. (D) 1. Update to Nunchaku v0.1.4 by downloading the corresponding version whl file. 2. Verify the installation by running the workflow again. (E) 1. Update to Nunchaku v0.1.4 by downloading the corresponding version whl file. 2. Delete the downloaded whl file. 3. Ensure all dependencies are correctly installed, including CUDA 12.6 if necessary. 4. Verify the installation by running the workflow again. (F) 1. Update to Nunchaku v0.1.4 by downloading the corresponding version whl file. 2. Ensure all dependencies are correctly installed, including CUDA 12.6 if necessary. (G) 1. Ensure all dependencies are correctly installed, including CUDA 12.6 if necessary. 2. Update to Nunchaku v0.1.4 by downloading the corresponding version whl file. 3. Verify the installation by running the workflow again.", "answer": "C"}
{"uuid": "931f04a0-e7b7-43e6-9817-1f0cdfade814", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face Hub](https://huggingface.co/nunchaku-tech)  \n   Example for FLUX.1-Kontext:  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/FLUX.1-Kontext-4bit\n   ```\n\n## 4. Basic Inference\n1. **Run Example Script**  \n   ```bash\n   python examples/flux.1-kontext-dev.py \\\n     --model-path ./FLUX.1-Kontext-4bit \\\n     --prompt \"A futuristic cityscape\"\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py \\\n     --model-path ./FLUX.1-Kontext-4bit \\\n     --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py \\\n     --model-path ./FLUX.1-Kontext-4bit \\\n     --controlnet canny\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git \\\n     /path/to/ComfyUI/custom_nodes/\n   ```\n\n## 7. Custom Quantization\n1. **Quantize Custom Models**  \n   Follow [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) guide.\n\n## 8. Monitoring\n- Check GPU utilization:  \n  ```bash\n  nvidia-smi\n  ```", "issue_title": "Diffusion_models directory problem in comfyui", "issue_body": "I found that when using nunchaku's comfyui, there must be a diffusion_models directory under the ComfyUI\\models\\ directory; in fact, I used comfyui's extra_model_paths.yaml to customize the directory, but nunchaku thinks this is a missing directory, which makes it impossible to enable the nunchaku plugin normally.", "choices": "(A) 1. Identify the rank of your LoRA. If it's higher than 64, you'll need to modify the `LoraRanks` sequence in the specified CUDA file.\n2. If you encounter errors during the build process, ensure all dependencies are correctly installed and compatible with the versions required by nunchaku and svdquant.\n3. For ranks up to 64, update to nunchaku v0.1.4 and svdquant node to v0.1.5.\n4. For higher ranks (e.g., 800), modify the `LoraRanks` sequence in the CUDA file to include your specific rank values, then rebuild the package. Note that compilation may be slow and performance may be affected.\n5. Rebuild the package using `python -m build --no-isolation -v` after making changes to the CUDA file. (B) 1. Identify the rank of your LoRA. If it's higher than 64, you'll need to modify the `LoraRanks` sequence in the specified CUDA file.\n2. For ranks up to 64, update to nunchaku v0.1.4 and svdquant node to v0.1.5.\n3. For higher ranks (e.g., 800), modify the `LoraRanks` sequence in the CUDA file to include your specific rank values, then rebuild the package. Note that compilation may be slow and performance may be affected.\n4. Rebuild the package using `python -m build --no-isolation -v` after making changes to the CUDA file. (C) 1. Identify the rank of your LoRA. If it's higher than 64, you'll need to modify the `LoraRanks` sequence in the specified CUDA file.\n2. For ranks up to 64, update to nunchaku v0.1.4 and svdquant node to v0.1.5.\n3. For higher ranks (e.g., 800), modify the `LoraRanks` sequence in the CUDA file to include your specific rank values, then rebuild the package. Note that compilation may be slow and performance may be affected.\n4. Rebuild the package using `python -m build --no-isolation -v` after making changes to the CUDA file.\n5. If you encounter errors during the build process, ensure all dependencies are correctly installed and compatible with the versions required by nunchaku and svdquant. (D) 1. Identify the rank of your LoRA. If it's higher than 64, you'll need to modify the `LoraRanks` sequence in the specified CUDA file.\n2. For ranks up to 64, update to nunchaku v0.1.4 and svdquant node to v0.1.5.\n3. Downgrade nunchaku to v0.1.0 and svdquant node to v0.1.0.\n4. For higher ranks (e.g., 800), modify the `LoraRanks` sequence in the CUDA file to include your specific rank values, then rebuild the package. Note that compilation may be slow and performance may be affected.\n5. Rebuild the package using `python -m build --no-isolation -v` after making changes to the CUDA file.\n6. If you encounter errors during the build process, ensure all dependencies are correctly installed and compatible with the versions required by nunchaku and svdquant. (E) 1. Identify the rank of your LoRA. If it's higher than 64, you'll need to modify the `LoraRanks` sequence in the specified CUDA file.\n2. For ranks up to 64, update to nunchaku v0.1.4 and svdquant node to v0.1.5.\n3. For higher ranks (e.g., 800), modify the `LoraRanks` sequence in the CUDA file to include your specific rank values, then rebuild the package. Note that compilation may be slow and performance may be affected.\n4. If you encounter errors during the build process, ensure all dependencies are correctly installed and compatible with the versions required by nunchaku and svdquant. (F) 1. Identify the rank of your LoRA. If it's higher than 64, you'll need to modify the `LoraRanks` sequence in the specified CUDA file.\n2. For ranks up to 64, update to nunchaku v0.1.4 and svdquant node to v0.1.5.\n3. For higher ranks (e.g., 800), modify the `LoraRanks` sequence in the CUDA file to include your specific rank values, then delete the CUDA file.\n4. Rebuild the package using `python -m build --no-isolation -v` after making changes to the CUDA file.\n5. If you encounter errors during the build process, ensure all dependencies are correctly installed and compatible with the versions required by nunchaku and svdquant. (G) 1. Identify the rank of your LoRA. If it's higher than 64, you'll need to modify the `LoraRanks` sequence in the specified CUDA file.\n2. For ranks up to 64, update to nunchaku v0.1.4 and svdquant node to v0.1.5.\n3. Rebuild the package using `python -m build --no-isolation -v` after making changes to the CUDA file.\n4. For higher ranks (e.g., 800), modify the `LoraRanks` sequence in the CUDA file to include your specific rank values, then rebuild the package. Note that compilation may be slow and performance may be affected.\n5. If you encounter errors during the build process, ensure all dependencies are correctly installed and compatible with the versions required by nunchaku and svdquant.", "answer": "C"}
{"uuid": "7315d9d0-a487-4bdb-a9c0-3c35b1a53ca9", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_compatibility())\"\n   ```\n\n## 4. Basic Usage\n1. **Run a Pre-Quantized Model**  \n   ```bash\n   python examples/flux.1-dev.py\n   ```\n\n2. **Enable FP16 Attention (Optional)**  \n   Add `--use_fp16_attention` flag to the command above.\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n3. **Low-Memory Inference (4 GiB)**  \n   ```bash\n   python examples/flux.1-dev-low-memory.py\n   ```\n\n## 6. Custom Model Quantization\n1. **Quantize Your Model**  \n   ```bash\n   python -m deepcompressor.quantize --model_path YOUR_MODEL --output_path quantized_model\n   ```\n\n## 7. ComfyUI Integration\n1. **Install ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku\n   pip install -r requirements.txt\n   ```\n\n## 8. Monitoring and Optimization\n1. **Profile Performance**  \n   ```bash\n   nvprof python examples/flux.1-dev.py\n   ```\n\n## 9. Troubleshooting\n- **Common Issues**: Refer to [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html)\n- **Community Support**: Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm)\n\n## 10. Updates\n- **Check for Updates**  \n  ```bash\n  git pull origin main\n  pip install --upgrade -r requirements.txt\n  ```", "issue_title": "RuntimeError: CUDA error: out of memory", "issue_body": "self._target(*self._args, **self._kwargs)\n  File \"D:\\ComfyUI-aki-v1.2\\main.py\", line 175, in prompt_worker\n    e.execute(item[2], prompt_id, item[3], item[4])\n  File \"D:\\ComfyUI-aki-v1.2\\execution.py\", line 531, in execute\n    comfy.model_management.unload_all_models()\n  File \"D:\\ComfyUI-aki-v1.2\\comfy\\model_management.py\", line 1214, in unload_all_models\n    free_memory(1e30, get_torch_device())\n  File \"D:\\ComfyUI-aki-v1.2\\comfy\\model_management.py\", line 503, in free_memory\n    if current_loaded_models[i].model_unload(memory_to_free):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ComfyUI-aki-v1.2\\comfy\\model_management.py\", line 431, in model_unload\n    self.model.detach(unpatch_weights)\n  File \"D:\\ComfyUI-aki-v1.2\\comfy\\model_patcher.py\", line 835, in detach\n    self.unpatch_model(self.offload_device, unpatch_weights=unpatch_all)\n  File \"D:\\ComfyUI-aki-v1.2\\comfy\\model_patcher.py\", line 734, in unpatch_model\n    self.model.to(device_to)\n  File \"D:\\ComfyUI-aki-v1.2\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\ComfyUI-aki-v1.2\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"D:\\ComfyUI-aki-v1.2\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"D:\\ComfyUI-aki-v1.2\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"D:\\ComfyUI-aki-v1.2\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.", "choices": "(A) 1. Upgrade ComfyUI-nunchaku to version 0.2.0.\n2. Verify that the issue is resolved by running the code again. (B) 1. Upgrade nunchaku to version 0.2.0 by downloading the prebuilt wheels from Hugging Face, ModelScope, or GitHub release page.\n2. Upgrade ComfyUI-nunchaku to version 0.2.0. (C) 1. Upgrade ComfyUI-nunchaku to version 0.2.0.\n2. Upgrade nunchaku to version 0.2.0 by downloading the prebuilt wheels from Hugging Face, ModelScope, or GitHub release page.\n3. Verify that the issue is resolved by running the code again. (D) 1. Delete the ComfyUI-nunchaku folder.\n2. Upgrade nunchaku to version 0.2.0 by downloading the prebuilt wheels from Hugging Face, ModelScope, or GitHub release page.\n3. Upgrade ComfyUI-nunchaku to version 0.2.0.\n4. Verify that the issue is resolved by running the code again. (E) 1. Verify that the issue is resolved by running the code again.\n2. Upgrade nunchaku to version 0.2.0 by downloading the prebuilt wheels from Hugging Face, ModelScope, or GitHub release page.\n3. Upgrade ComfyUI-nunchaku to version 0.2.0. (F) 1. Upgrade nunchaku to version 0.2.0 by downloading the prebuilt wheels from Hugging Face, ModelScope, or GitHub release page.\n2. Upgrade ComfyUI-nunchaku to version 0.2.0.\n3. Verify that the issue is resolved by running the code again. (G) 1. Downgrade nunchaku to version 0.1.0.\n2. Upgrade nunchaku to version 0.2.0 by downloading the prebuilt wheels from Hugging Face, ModelScope, or GitHub release page.\n3. Upgrade ComfyUI-nunchaku to version 0.2.0.\n4. Verify that the issue is resolved by running the code again.", "answer": "F"}
{"uuid": "ebfa749a-000f-419e-a3e1-5cc4056f3ddc", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- Ensure you have an NVIDIA GPU with CUDA support (RTX 4090/5090 recommended)\n- Python 3.8 or later installed\n- [Optional] Join community channels (Slack/Discord/WeChat) for support\n\n## 2. Installation\n```bash\n# Clone the repository\ngit clone https://github.com/nunchaku-tech/nunchaku.git\ncd nunchaku\n\n# Create and activate virtual environment (recommended)\npython -m venv nunchaku-env\nsource nunchaku-env/bin/activate  # Linux/Mac\n.\\nunchaku-env\\Scripts\\activate   # Windows\n\n# Install core dependencies\npip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118\npip install -r requirements.txt\n```\n\n## 3. Model Setup\n```bash\n# Download pre-quantized models (example: FLUX.1)\nwget https://huggingface.co/nunchaku-tech/nunchaku-flux1/resolve/main/flux1-4bit.safetensors\n\n# Or quantize your own model using DeepCompressor\ngit clone https://github.com/nunchaku-tech/deepcompressor\ncd deepcompressor\npython quantize.py --model your_model --output quantized_model.safetensors\n```\n\n## 4. Basic Inference\n```bash\n# Run text-to-image generation with FLUX.1\npython examples/flux1-t2i.py \\\n  --model flux1-4bit.safetensors \\\n  --prompt \"A scenic landscape\" \\\n  --output result.png\n```\n\n## 5. Advanced Features\n```bash\n# Multi-LoRA inference\npython examples/flux.1-dev-multiple-lora.py \\\n  --model flux1-4bit.safetensors \\\n  --lora lora1.safetensors lora2.safetensors \\\n  --prompt \"Portrait with artistic style\"\n\n# ControlNet integration\npython examples/flux.1-dev-controlnet-union-pro.py \\\n  --model flux1-4bit.safetensors \\\n  --controlnet canny \\\n  --input_image sketch.png\n```\n\n## 6. ComfyUI Integration\n```bash\n# Install ComfyUI plugin\ngit clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\ncd ComfyUI-nunchaku\npip install -r requirements.txt\n\n# Launch ComfyUI\npython main.py\n```\n\n## 7. Verification\n- Check output images in specified output directory\n- Monitor GPU utilization with `nvidia-smi`\n- Refer to [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html) for troubleshooting\n\n## 8. Optimization\n```bash\n# Enable FP16 attention for faster inference\nexport NUNCHAKU_USE_FP16_ATTN=1\n\n# Use First-Block Cache for repeated prompts\npython examples/flux.1-dev-double_cache.py \\\n  --model flux1-4bit.safetensors \\\n  --prompt \"The same prompt used repeatedly\" \\\n  --use_cache\n```\n\n## 9. Deployment Options\n```bash\n# Launch Gradio demo\npython app/flux1/t2i/app.py\n\n# Build Docker image\ndocker build -t nunchaku .\ndocker run --gpus all -p 7860:7860 nunchaku\n```\n\n## 10. Maintenance\n- Check for updates: `git pull origin main`\n- Reinstall dependencies after updates: `pip install -r requirements.txt`\n- Monitor [roadmap](https://github.com/nunchaku-tech/nunchaku/issues/431) for future features", "issue_title": "comfyui crack with error :Unable to pin memory: operation not supported", "issue_body": "got prompt\nUsing xformers attention in VAE\nUsing xformers attention in VAE\nVAE load device: cuda:0, offload device: cpu, dtype: torch.bfloat16\n[2025-03-04 18:52:26.009] [info] Initializing QuantizedFluxModel\n[2025-03-04 18:52:26.050] [info] Loading weights from D:\\ai\\ComfyUI\\.cache\\huggingface\\hub\\models--mit-han-lab--svdq-int4-flux.1-dev\\snapshots\\38507ee5e39e174ab7f7015a6ea5b42216cd421d\\transformer_blocks.safetensors\n[2025-03-04 18:52:26.056] [warning] Unable to pin memory: operation not supported\n[2025-03-04 18:52:31.724] [info] Done.\n\n--------\n[程序异常退出，退出代码为 3 (0x00000003)]\n\n\nwin10 ,python 3.10, 4090,", "choices": "(A) 1. Download the model `mit-han-lab/svdq-fp4-flux.1-dev` and place it in the appropriate directory.  \n2. Update the node to `svdquant` comfy node v0.1.4.  \n3. Ensure `lora_name` is set to None as LoRA for NVFP4 is not supported currently.  \n4. Set the model name in SVDQuant Flux DiT Loader to `svdq-fp4-flux.1-dev`.  \n5. If using a GPU with limited VRAM (e.g., 12GB), avoid using int4 for the text encoder and switch to bf16.  \n6. For systems with RTX5090, set the environment variable `NUNCHAKU_LOAD_METHOD` to `READNOPIN` (export NUNCHAKU_LOAD_METHOD=\"READNOPIN\") to prevent system reboots.  \n7. Ensure PyTorch is compatible with CUDA 12.8 by reinstalling if necessary: `pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128`. (B) 1. Download the model `mit-han-lab/svdq-fp4-flux.1-dev` and place it in the appropriate directory.  \n2. Update the node to `svdquant` comfy node v0.1.4.  \n3. Set the model name in SVDQuant Flux DiT Loader to `svdq-fp4-flux.1-dev`.  \n4. Enable LoRA for NVFP4 by setting `lora_name` to `experimental_lora`.  \n5. If using a GPU with limited VRAM (e.g., 12GB), force int4 for the text encoder for better performance.  \n6. For systems with RTX5090, set the environment variable `NUNCHAKU_LOAD_METHOD` to `READNOPIN` (export NUNCHAKU_LOAD_METHOD=\"READNOPIN\") to prevent system reboots.  \n7. Ensure PyTorch is compatible with CUDA 12.8 by reinstalling if necessary: `pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128`. (C) 1. Download the model `mit-han-lab/svdq-fp4-flux.1-dev` and place it in the appropriate directory.  \n2. Update the node to `svdquant` comfy node v0.1.4.  \n3. Ensure `lora_name` is set to None as LoRA for NVFP4 is not supported currently.  \n4. For systems with RTX5090, set the environment variable `NUNCHAKU_LOAD_METHOD` to `READNOPIN` (export NUNCHAKU_LOAD_METHOD=\"READNOPIN\") to prevent system reboots.  \n5. Ensure PyTorch is compatible with CUDA 12.8 by reinstalling if necessary: `pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128`. (D) 1. Download the model `mit-han-lab/svdq-fp4-flux.1-dev` and place it in the appropriate directory.\n2. Update the node to `svdquant` comfy node v0.1.4.\n3. Set the model name in SVDQuant Flux DiT Loader to `svdq-fp4-flux.1-dev`.\n4. Ensure `lora_name` is set to None as LoRA for NVFP4 is not supported currently.\n5. If using a GPU with limited VRAM (e.g., 12GB), avoid using int4 for the text encoder and switch to bf16.\n6. For systems with RTX5090, set the environment variable `NUNCHAKU_LOAD_METHOD` to `READNOPIN` (export NUNCHAKU_LOAD_METHOD=\"READNOPIN\") to prevent system reboots.\n7. Ensure PyTorch is compatible with CUDA 12.8 by reinstalling if necessary: `pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128`. (E) 1. Ensure PyTorch is compatible with CUDA 12.8 by reinstalling if necessary: `pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128`.  \n2. Download the model `mit-han-lab/svdq-fp4-flux.1-dev` and place it in the appropriate directory.  \n3. Update the node to `svdquant` comfy node v0.1.4.  \n4. Set the model name in SVDQuant Flux DiT Loader to `svdq-fp4-flux.1-dev`.  \n5. Ensure `lora_name` is set to None as LoRA for NVFP4 is not supported currently.  \n6. If using a GPU with limited VRAM (e.g., 12GB), avoid using int4 for the text encoder and switch to bf16.  \n7. For systems with RTX5090, set the environment variable `NUNCHAKU_LOAD_METHOD` to `READNOPIN` (export NUNCHAKU_LOAD_METHOD=\"READNOPIN\") to prevent system reboots. (F) 1. Update the node to `svdquant` comfy node v0.1.4.  \n2. Set the model name in SVDQuant Flux DiT Loader to `svdq-fp4-flux.1-dev`.  \n3. Ensure `lora_name` is set to None as LoRA for NVFP4 is not supported currently.  \n4. If using a GPU with limited VRAM (e.g., 12GB), avoid using int4 for the text encoder and switch to bf16.  \n5. For systems with RTX5090, set the environment variable `NUNCHAKU_LOAD_METHOD` to `READNOPIN` (export NUNCHAKU_LOAD_METHOD=\"READNOPIN\") to prevent system reboots. (G) 1. Download the model `mit-han-lab/svdq-fp4-flux.1-dev` and place it in the appropriate directory.  \n2. Update the node to `svdquant` comfy node v0.1.4.  \n3. Set the model name in SVDQuant Flux DiT Loader to `svdq-fp4-flux.1-dev`.  \n4. Ensure `lora_name` is set to None as LoRA for NVFP4 is not supported currently.  \n5. If using a GPU with limited VRAM (e.g., 12GB), avoid using int4 for the text encoder and switch to bf16.  \n6. Downgrade PyTorch to CUDA 11.8 for compatibility: `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`.  \n7. Skip setting `NUNCHAKU_LOAD_METHOD` for RTX5090 to test system stability.", "answer": "D"}
{"uuid": "6973f845-a8dc-44c0-a3cf-e8fbe6d2e07a", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: \n  - Python 3.8+\n  - CUDA 12.1+ (for GPU acceleration)\n  - PyTorch 2.2+\n\n## 2. Installation\n### Option A: Pip Install\n```bash\npip install nunchaku\n```\n\n### Option B: Build from Source\n```bash\ngit clone https://github.com/nunchaku-tech/nunchaku.git\ncd nunchaku\npip install -e .\n```\n\n## 3. Verify Installation\n```bash\npython -c \"import nunchaku; print(nunchaku.__version__)\"\n```\n\n## 4. Download Pre-Quantized Models\n```bash\n# Example: Download FLUX.1 4-bit model\nwget https://huggingface.co/nunchaku-tech/flux.1-4bit/resolve/main/model.nk\n```\n\n## 5. Basic Inference\nCreate a Python script (`inference.py`):\n```python\nimport nunchaku as nk\nmodel = nk.load(\"model.nk\")\noutput = model.generate(prompt=\"A cat wearing sunglasses\")\noutput.save(\"output.png\")\n```\nRun with:\n```bash\npython inference.py\n```\n\n## 6. Advanced Features\n### Multi-LoRA Support\n```python\nmodel = nk.load(\"model.nk\", lora=[\"lora1.safetensors\", \"lora2.safetensors\"])\n```\n\n### ControlNet Integration\n```python\nmodel = nk.load(\"model.nk\", controlnet=\"controlnet-union-pro-2.0.nk\")\n```\n\n## 7. Performance Optimization\nEnable FP16 attention and First-Block Cache:\n```python\nmodel = nk.load(\"model.nk\", fp16_attn=True, fb_cache=True)\n```\n\n## 8. ComfyUI Integration (Optional)\n```bash\ngit clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\ncd ComfyUI-nunchaku\npip install -r requirements.txt\n```\n\n## 9. Monitoring and Debugging\nCheck GPU utilization:\n```bash\nnvidia-smi\n```\n\n## 10. Community Resources\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for support\n- Report issues on [GitHub](https://github.com/nunchaku-tech/nunchaku/issues)", "issue_title": "There is a serious problem.", "issue_body": "There is a serious problem. The installation is successful now. To be precise, it was successfully installed last night. Why? Because after executing the simplest workflow in comfyui last night, when loading svdq-int4-flux.1-schnell, the computer was stuck directly, twice! So, I thought it was because I installed something incomplete. Today, I reinstalled a new environment again, and this time it was stuck when saving the image node, the browser was stuck, and the image could not be displayed, also twice! Now, I wrote this question after restarting the computer. So, I am not sure if it is an installation problem. \n\nMy environment is as follows:\nTotal VRAM 11938 MB, total RAM 32040 MB\npytorch version: 2.6.0+cu124\nSet vram state to: NORMAL_VRAM\nDevice: cuda:0 NVIDIA GeForce RTX 3060 : cudaMallocAsync\nUsing pytorch attention\nComfyUI version: 0.3.18\nCUDA Version: 12.6\nnvcc: 12.4.11\npython: 3.12.6\n\n![Image](https://github.com/user-attachments/assets/f6325822-9d45-4c6f-8cf0-51172726814d)\n![Image](https://github.com/user-attachments/assets/50c5af9c-2ba4-4e65-942a-a30e87b15f13)\n![Image](https://github.com/user-attachments/assets/5903c9d1-bec9-4591-89af-de4a9a300a82)\n![Image](https://github.com/user-attachments/assets/08590f0c-25ba-4150-994e-885a3fe6595e)\n\n\nThe following is the inference picture (only half):\n![Image](https://github.com/user-attachments/assets/b995f14a-ab53-4891-a266-7d8368823eb6)\n\nOne thing I need to point out is that after I shut down the comfyui service, there are still programs in the background reading the disk crazily, and the file window cannot be used (I think the problem is here, which method must have entered an infinite loop when reading the file). Now I dare not execute this workflow again. I hope this problem can be fixed. If anyone has the same problem, please leave a message below. Thank you very much.", "choices": "(A) 1. Verify that the model files are correctly placed in the custom directory (e.g., 'ComfyUI/models/diffusion_models/svdq-int4-flux.1-dev').\n2. Ensure you select the correct model folder option in the node interface (e.g., 'svdqfluxdev' for local models) instead of the default Hugging Face option.\n3. If using a LoRa node, ensure the 'Base models path' in the LoRa loader matches the path specified in the DiT loader node to avoid conflicts. (B) 1. If using a LoRa node, ensure the 'Base models path' in the LoRa loader matches the path specified in the DiT loader node to avoid conflicts.\n2. Verify that the model files are correctly placed in the custom directory (e.g., 'ComfyUI/models/diffusion_models/svdq-int4-flux.1-dev'). (C) 1. Ensure you select the correct model folder option in the node interface (e.g., 'svdqfluxdev' for local models) instead of the default Hugging Face option.\n2. If using a LoRa node, ensure the 'Base models path' in the LoRa loader matches the path specified in the DiT loader node to avoid conflicts.\n3. Verify that the model files are correctly placed in the custom directory (e.g., 'ComfyUI/models/diffusion_models/svdq-int4-flux.1-dev').\n4. Delete the model files from the custom directory to free up space. (D) 1. Ensure you select the correct model folder option in the node interface (e.g., 'svdqfluxdev' for local models) instead of the default Hugging Face option.\n2. If using a LoRa node, ensure the 'Base models path' in the LoRa loader matches the path specified in the DiT loader node to avoid conflicts.\n3. Verify that the model files are correctly placed in the custom directory (e.g., 'ComfyUI/models/diffusion_models/svdq-int4-flux.1-dev').\n4. Switch back to the default Hugging Face option for faster loading. (E) 1. Ensure you select the correct model folder option in the node interface (e.g., 'svdqfluxdev' for local models) instead of the default Hugging Face option.\n2. If using a LoRa node, ensure the 'Base models path' in the LoRa loader matches the path specified in the DiT loader node to avoid conflicts.\n3. Verify that the model files are correctly placed in the custom directory (e.g., 'ComfyUI/models/diffusion_models/svdq-int4-flux.1-dev'). (F) 1. If using a LoRa node, ensure the 'Base models path' in the LoRa loader matches the path specified in the DiT loader node to avoid conflicts.\n2. Ensure you select the correct model folder option in the node interface (e.g., 'svdqfluxdev' for local models) instead of the default Hugging Face option.\n3. Verify that the model files are correctly placed in the custom directory (e.g., 'ComfyUI/models/diffusion_models/svdq-int4-flux.1-dev'). (G) 1. Ensure you select the correct model folder option in the node interface (e.g., 'svdqfluxdev' for local models) instead of the default Hugging Face option.\n2. If using a LoRa node, ensure the 'Base models path' in the LoRa loader matches the path specified in the DiT loader node to avoid conflicts.", "answer": "E"}
{"uuid": "3e48fe57-3dfa-4171-b6bc-6444b51d9101", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: \n  - Python 3.8+\n  - CUDA 11.8+\n  - PyTorch 2.0+\n\n## 2. Installation\n1. **Clone Repository**:  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n2. **Install Dependencies**:  \n   ```bash\n   pip install -r requirements.txt\n   ```\n3. **Install Nunchaku Engine**:  \n   ```bash\n   pip install nunchaku --upgrade\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models** (e.g., FLUX.1):  \n   ```bash\n   huggingface-cli download nunchaku-tech/nunchaku-flux1 --local-dir ./models/flux1\n   ```\n2. **Verify Model Files**:  \n   Ensure `*.nkc` (Nunchaku format) files are present in the `./models/flux1` directory.\n\n## 4. Basic Inference\n1. **Run Text-to-Image Example**:  \n   ```bash\n   python examples/flux.1-dev.py --prompt \"A cat wearing sunglasses\" --output ./outputs/cat.png\n   ```\n   *Flags*:  \n   - `--prompt`: Input text prompt.  \n   - `--output`: Path to save generated image.  \n\n## 5. Advanced Features\n1. **Multi-LoRA Inference**:  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --prompt \"A dog in a spacesuit\" --lora_weights ./loras/space_style.nkc\n   ```\n2. **ControlNet Integration**:  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --sketch ./inputs/sketch.jpg --output ./outputs/controlled.png\n   ```\n\n## 6. Performance Optimization\n1. **Enable FP16 Attention**:  \n   Add `--use_fp16_attention` flag to inference scripts.  \n2. **First-Block Cache**:  \n   Use `--enable_fb_cache` for faster sequential inferences.  \n\n## 7. Monitoring\n- Check GPU utilization:  \n  ```bash\n  nvidia-smi --query-gpu=utilization.gpu --format=csv -l 1\n  ```\n\n## 8. Troubleshooting\n- **CUDA Errors**: Verify CUDA version matches PyTorch installation.  \n- **Model Loading Issues**: Ensure `*.nkc` files are uncorrupted (compare SHA checksums).  \n\n## 9. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for help.  \n- Report bugs via [GitHub Issues](https://github.com/nunchaku-tech/nunchaku/issues).  \n\n## 10. Citation\n```bibtex\n@inproceedings{li2024svdquant,\n  title={SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models},\n  author={Li*, Muyang and Lin*, Yujun and Zhang*, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},\n  booktitle={ICLR},\n  year={2025}\n}\n```", "issue_title": "When generating with a custom LoRA, the output resulted in a black image.", "issue_body": "Name: nunchaku\nVersion: 0.1.3+torch2.4\nSystem: ubuntu\n\n\nWhen generating with a custom LoRA, the output resulted in a black image.\nI'm trying convert a lora with this svdq-int4-flux.1-dev/transformer_blocks.safetensors\nAfter following the Readme.md，output is success.\n\n![Image](https://github.com/user-attachments/assets/c020703a-e45b-4d01-a05b-da706c45fbc3)\n\nHere is [convert.log:](https://github.com/user-attachments/files/19055947/convert.log)\nHere is my converted [lora](https://huggingface.co/BigSEAnight/test/blob/main/svdq-int4-haunted_linework_flux.safetensors)   \nHere is source [lora](https://huggingface.co/alvdansen/haunted_linework_flux)\n\n\n\n==================================================================\nIn the other hand, after following the instructions in DeepCompressor's examples/diffusion/README.md, I encountered an error during the conversion process.\nError:\n  File \"D:\\B_ComfyUI\\test\\deepcompressor\\deepcompressor\\backend\\nunchaku\\convert_lora.py\", line 168, in convert_to_nunchaku_flux_single_transformer_block_lowrank_dict\n    return convert_to_nunchaku_transformer_block_lowrank_dict(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\B_ComfyUI\\test\\deepcompressor\\deepcompressor\\backend\\nunchaku\\convert_lora.py\", line 104, in convert_to_nunchaku_transformer_block_lowrank_dict\n    lora = (extra_lora[0].to(default_dtype), extra_lora[1].to(default_dtype))\n            ^^^^^^^^^^^^^^^^\nAttributeError: 'tuple' object has no attribute 'to'\n\ndeepcompressor 0.0.1 requires diffusers>=0.32.0, but you have diffusers 0.30.0 which is incompatible.\nnunchaku 0.1.3+torch2.5 requires diffusers>=0.32.2, but you have diffusers 0.30.0 which is incompatible.\nSuccessfully installed diffusers-0.30.0\n\ndiffusers==0.32.0,diffusers==0.30.0,diffusers==0.32.2, there are all tested.\ndeepcompressor cannot convert lora.", "choices": "(A) 1. Modify the LoRA node code to handle temporary file paths correctly by replacing the temporary file path with a fixed path in the LoRA directory.\n2. Use the updated `nunchaku node v0.1.6` from the new repository to avoid further issues with temporary file handling. (B) 1. Modify the LoRA node code to handle temporary file paths correctly by replacing the temporary file path with a fixed path in the LoRA directory.\n2. Replace the code in `flux.py` with the provided fix to ensure the temporary file is saved in the correct location.\n3. Use the updated `nunchaku node v0.1.6` from the new repository to avoid further issues with temporary file handling. (C) 1. Replace the code in `flux.py` with the provided fix to ensure the temporary file is saved in the correct location.\n2. Use the updated `nunchaku node v0.1.6` from the new repository to avoid further issues with temporary file handling. (D) 1. Use the updated `nunchaku node v0.1.6` from the new repository to avoid further issues with temporary file handling.\n2. Modify the LoRA node code to handle temporary file paths correctly by replacing the temporary file path with a fixed path in the LoRA directory.\n3. Replace the code in `flux.py` with the provided fix to ensure the temporary file is saved in the correct location. (E) 1. Delete all temporary files manually to clear any existing issues.\n2. Modify the LoRA node code to handle temporary file paths correctly by replacing the temporary file path with a fixed path in the LoRA directory.\n3. Replace the code in `flux.py` with the provided fix to ensure the temporary file is saved in the correct location.\n4. Use the updated `nunchaku node v0.1.6` from the new repository to avoid further issues with temporary file handling. (F) 1. Modify the LoRA node code to handle temporary file paths correctly by replacing the temporary file path with a fixed path in the LoRA directory.\n2. Use the updated `nunchaku node v0.1.6` from the new repository to avoid further issues with temporary file handling.\n3. Replace the code in `flux.py` with the provided fix to ensure the temporary file is saved in the correct location. (G) 1. Modify the LoRA node code to handle temporary file paths correctly by replacing the temporary file path with a fixed path in the LoRA directory.\n2. Replace the code in `flux.py` with the provided fix to ensure the temporary file is saved in the correct location.\n3. Revert the changes made to `flux.py` to test if the issue persists.\n4. Use the updated `nunchaku node v0.1.6` from the new repository to avoid further issues with temporary file handling.", "answer": "B"}
{"uuid": "a0f587f6-cfaa-4da5-9cd2-b25a7e7197b0", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone the repository**:\n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install dependencies**:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku** (choose one method):\n   - **From source**:\n     ```bash\n     pip install -e .\n     ```\n   - **From PyPI**:\n     ```bash\n     pip install nunchaku\n     ```\n\n## 3. Basic Usage\n1. **Load a pre-quantized model**:\n   ```python\n   from nunchaku import NunchakuModel\n   model = NunchakuModel.from_pretrained(\"nunchaku-tech/flux.1-dev-4bit\")\n   ```\n\n2. **Run inference**:\n   ```python\n   output = model.generate(prompt=\"A scenic landscape\")\n   ```\n\n## 4. Advanced Features\n1. **Multi-LoRA support**:\n   ```python\n   model.load_lora([\"lora1.safetensors\", \"lora2.safetensors\"])\n   ```\n\n2. **ControlNet integration**:\n   ```python\n   from nunchaku import ControlNet\n   cnet = ControlNet.from_pretrained(\"nunchaku-tech/controlnet-union-pro-2.0\")\n   output = model.generate(prompt=\"A cat\", controlnet=cnet, control_image=control_img)\n   ```\n\n## 5. ComfyUI Integration\n1. **Install ComfyUI plugin**:\n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku\n   pip install -r requirements.txt\n   ```\n\n2. **Launch ComfyUI**:\n   ```bash\n   python main.py\n   ```\n\n## 6. Custom Model Quantization\n1. **Install DeepCompressor**:\n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install -e .\n   ```\n\n2. **Quantize your model**:\n   ```python\n   from deepcompressor import quantize\n   quantize(input_model=\"your_model.pth\", output_path=\"quantized_model\")\n   ```\n\n## 7. Running Demos\n1. **Gradio demo**:\n   ```bash\n   python app/gradio/app.py\n   ```\n\n2. **FLUX.1-Kontext demo**:\n   ```bash\n   python examples/flux.1-kontext-dev.py\n   ```\n\n## 8. Troubleshooting\n- For GPU compatibility issues, refer to `examples/flux.1-dev-turing.py`\n- For low-memory systems, use per-layer CPU offloading:\n  ```python\n  model = NunchakuModel.from_pretrained(\"nunchaku-tech/flux.1-dev-4bit\", cpu_offload=True)\n  ```\n\n## 9. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for help\n- Check [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html) for common issues", "issue_title": "how to use NVFP4 model in ComfyUI?", "issue_body": "![Image](https://github.com/user-attachments/assets/65e31c13-acdd-432f-ba53-da3b3bca2607)\n\n![Image](https://github.com/user-attachments/assets/ac43e71c-486b-461d-8363-68e90555f8a6)\n\nWhich model should I download ? and which node should I use to load it?\n\nI have RTX 50 cards.", "choices": "(A) 1. Ensure the LoRA file follows the required format for compatibility.\n2. Use the provided conversion command with the correct LoRA strength setting (0.0625).\n3. Execute the following command in your terminal:\n```shell\npython -m nunchaku.lora.flux.convert --lora-format comfyui --lora-path loras/comfyui-lyt.safetensors --lora-name svdq-int-flux.1-dev-lyt\n``` (B) 1. Use the provided conversion command with the correct LoRA strength setting (0.0625).\n2. Execute the following command in your terminal:\n```shell\npython -m nunchaku.lora.flux.convert --lora-format comfyui --lora-path loras/comfyui-lyt.safetensors --lora-name svdq-int-flux.1-dev-lyt\n```\n3. Ensure the LoRA file follows the required format for compatibility. (C) 1. Execute the following command in your terminal:\n```shell\npython -m nunchaku.lora.flux.convert --lora-format comfyui --lora-path loras/comfyui-lyt.safetensors --lora-name svdq-int-flux.1-dev-lyt\n```\n2. Ensure the LoRA file follows the required format for compatibility. (D) 1. Use the provided conversion command with the correct LoRA strength setting (0.0625).\n2. Delete the LoRA file to free up space:\n```shell\nrm loras/comfyui-lyt.safetensors\n```\n3. Execute the following command in your terminal:\n```shell\npython -m nunchaku.lora.flux.convert --lora-format comfyui --lora-path loras/comfyui-lyt.safetensors --lora-name svdq-int-flux.1-dev-lyt\n```\n4. Ensure the LoRA file follows the required format for compatibility.", "answer": "B"}
{"uuid": "e18e42df-66d1-45be-bff6-be2de6566202", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_compatibility())\"\n   ```\n\n2. **Run Basic Test**  \n   ```bash\n   python examples/flux.1-dev-basic.py\n   ```\n\n## 4. Model Setup\n1. **Download Pre-Quantized Models**  \n   ```bash\n   huggingface-cli download nunchaku-tech/nunchaku-flux --local-dir ./models/flux\n   ```\n\n2. **Custom Quantization (Optional)**  \n   Follow [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 5. Inference\n1. **Basic Text-to-Image**  \n   ```bash\n   python examples/flux.1-dev-basic.py --model-path ./models/flux --prompt \"A scenic landscape\"\n   ```\n\n2. **Advanced Features**  \n   - **Multi-LoRA**:  \n     ```bash\n     python examples/flux.1-dev-multiple-lora.py\n     ```\n   - **ControlNet**:  \n     ```bash\n     python examples/flux.1-dev-controlnet-union-pro.py\n     ```\n\n## 6. ComfyUI Integration (Optional)\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI**  \n   Follow ComfyUI’s standard restart procedure.\n\n## 7. Monitoring & Optimization\n1. **Enable First-Block Cache**  \n   Add `--enable-fb-cache` flag to inference scripts.\n\n2. **Low-Memory Mode**  \n   ```bash\n   python examples/flux.1-dev-low-memory.py --cpu-offload\n   ```\n\n## 8. Community & Support\n- **Slack**: Join [here](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q)  \n- **Discord**: Join [here](https://discord.gg/Wk6PnwX9Sm)  \n- **WeChat**: Scan QR code [here](https://huggingface.co/datasets/nunchaku-tech/cdn/resolve/main/nunchaku/assets/wechat.jpg)", "issue_title": "The custom folder directory doesn't work; it still downloads the model from Hugging Face.", "issue_body": "The custom folder directory doesn't work; it still downloads the model from Hugging Face.\nmy custom folder directory , uploading models to this path:\n\n```\nComfyUI/models/diffusion_models/svdq-int4-flux.1-dev\n```\n![Image](https://github.com/user-attachments/assets/27c7dc8c-d066-4c6a-8f86-f78668f906e7)\n\n\n\n---\n### SVDQuantFluxDiTLoader Node not recognized, it still downloads the model from Hugging Face.\n![Image](https://github.com/user-attachments/assets/d3ef80eb-6181-4d03-b861-cb1f77c945e4)\n\n---\n### Looking at the fluy.py code, it supports custom paths, so I'm not sure what the error could be.\n![Image](https://github.com/user-attachments/assets/64b37936-79dd-4a3c-a0fa-99b5c2ad21ce)\n\n---\nsvdq-flux.1-dev.json is works, just model loading issues.\n![Image](https://github.com/user-attachments/assets/267a4331-b103-464c-9e2a-f883f2278b03)", "choices": "(A) 1. Download the prebuilt wheels for Nunchaku v0.2.0 from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n2. Uninstall PyTorch using the command: `python.exe -m pip uninstall torch -y`.\n3. Ensure the downloaded wheel matches your Python (3.10–3.13) and PyTorch (2.5–2.8) versions.\n4. Install the wheel in the same Python environment used by ComfyUI using the command: `python.exe -m pip install [wheel_filename].whl`.\n5. Upgrade ComfyUI-nunchaku to v0.2.0 for full compatibility by following the instructions on its GitHub repository.\n6. Verify the installation by running Nunchaku and checking for any import or runtime errors. (B) 1. Download the prebuilt wheels for Nunchaku v0.2.0 from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n2. Install the wheel in the same Python environment used by ComfyUI using the command: `python.exe -m pip install [wheel_filename].whl`.\n3. Upgrade ComfyUI-nunchaku to v0.2.0 for full compatibility by following the instructions on its GitHub repository.\n4. Verify the installation by running Nunchaku and checking for any import or runtime errors. (C) 1. Download the prebuilt wheels for Nunchaku v0.2.0 from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n2. Install the wheel in the same Python environment used by ComfyUI using the command: `python.exe -m pip install [wheel_filename].whl`.\n3. Ensure the downloaded wheel matches your Python (3.10–3.13) and PyTorch (2.5–2.8) versions.\n4. Upgrade ComfyUI-nunchaku to v0.2.0 for full compatibility by following the instructions on its GitHub repository.\n5. Verify the installation by running Nunchaku and checking for any import or runtime errors. (D) 1. Download the prebuilt wheels for Nunchaku v0.2.0 from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n2. Ensure the downloaded wheel matches your Python (3.10–3.13) and PyTorch (2.5–2.8) versions.\n3. Install an older version of Nunchaku using the command: `python.exe -m pip install nunchaku==0.1.0`.\n4. Install the wheel in the same Python environment used by ComfyUI using the command: `python.exe -m pip install [wheel_filename].whl`.\n5. Upgrade ComfyUI-nunchaku to v0.2.0 for full compatibility by following the instructions on its GitHub repository.\n6. Verify the installation by running Nunchaku and checking for any import or runtime errors. (E) 1. Download the prebuilt wheels for Nunchaku v0.2.0 from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n2. Ensure the downloaded wheel matches your Python (3.10–3.13) and PyTorch (2.5–2.8) versions.\n3. Install the wheel in the same Python environment used by ComfyUI using the command: `python.exe -m pip install [wheel_filename].whl`.\n4. Verify the installation by running Nunchaku and checking for any import or runtime errors.\n5. Upgrade ComfyUI-nunchaku to v0.2.0 for full compatibility by following the instructions on its GitHub repository. (F) 1. Download the prebuilt wheels for Nunchaku v0.2.0 from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n2. Ensure the downloaded wheel matches your Python (3.10–3.13) and PyTorch (2.5–2.8) versions.\n3. Install the wheel in the same Python environment used by ComfyUI using the command: `python.exe -m pip install [wheel_filename].whl`.\n4. Verify the installation by running Nunchaku and checking for any import or runtime errors. (G) 1. Download the prebuilt wheels for Nunchaku v0.2.0 from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n2. Ensure the downloaded wheel matches your Python (3.10–3.13) and PyTorch (2.5–2.8) versions.\n3. Install the wheel in the same Python environment used by ComfyUI using the command: `python.exe -m pip install [wheel_filename].whl`.\n4. Upgrade ComfyUI-nunchaku to v0.2.0 for full compatibility by following the instructions on its GitHub repository.\n5. Verify the installation by running Nunchaku and checking for any import or runtime errors.", "answer": "G"}
{"uuid": "5517d934-5989-4d75-8fbe-20b0a9bddd49", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install nunchaku --upgrade\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - FLUX.1 models: [Hugging Face Hub](https://huggingface.co/nunchaku-tech)\n   - Example command for FLUX.1-dev:  \n     ```bash\n     git lfs install\n     git clone https://huggingface.co/nunchaku-tech/flux.1-dev-4bit\n     ```\n\n2. **Custom Quantization (Optional)**  \n   Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 4. Basic Inference\n1. **Run Text-to-Image Example**  \n   ```bash\n   python examples/flux.1-dev.py --model-path ./flux.1-dev-4bit --prompt \"A cat wearing sunglasses\"\n   ```\n\n2. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model-path ./flux.1-dev-4bit --lora-paths lora1.safetensors lora2.safetensors\n   ```\n\n## 5. Advanced Features\n1. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --controlnet-path ./controlnet-union-pro-2.0\n   ```\n\n2. **Low-Memory Inference (4GB VRAM)**  \n   ```bash\n   python examples/flux.1-dev-low-memory.py --cpu-offload\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Load Models in ComfyUI**  \n   - Use `NunchakuLoader` node to load 4-bit models.\n\n## 7. Monitoring & Optimization\n1. **Enable FP16 Attention**  \n   Add `--fp16-attn` flag to inference scripts for faster performance.\n\n2. **First-Block Cache**  \n   Use `--fbcache` flag to reduce latency in multi-batch inference.\n\n## 8. Troubleshooting\n- **Common Issues**:  \n  - CUDA errors: Verify CUDA version matches PyTorch build.\n  - Model loading failures: Check `git lfs` installation for large files.\n- **Community Support**:  \n  Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for help.", "issue_title": "'utf-8' codec can't decode byte 0xc1 in position 0: invalid start byte", "issue_body": "There are a few issues when using LoRA: First, if I load a regular LoRA and select the ComfyUI (or XLab) type, the following error occurs:\nmodel_type FLUX\n!!! Exception during processing !!!\nTraceback (most recent call last):\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\custom_nodes\\svdquant\\nodes\\lora\\flux.py\", line 94, in load_lora\n    input_lora = comfyui2diffusers(lora_path)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\python312\\Lib\\site-packages\\nunchaku\\lora\\flux\\comfyui_converter.py\", line 53, in comfyui2diffusers\n    assert \"lora_unet_single_blocks\" in k\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n\nIf I switch to the Diffusers type:\n\n[2025-02-25 01:38:22.066] [info] Loading weights from E:\\comfyui\\ComfyUI-aki-v1.3\\models\\diffusion_models\\svdqfluxdev\\transformer_blocks.safetensors\n[2025-02-25 01:38:23.854] [info] Done.\nmodel_type FLUX\nConverting 57 transformer blocks...\nConverting LoRA branch for block single_transformer_blocks.0...\n - Found single_transformer_blocks.0 LoRA of qkv_proj (rank: 32)\n    - Using original LoRA\n - Found single_transformer_blocks.0 LoRA of out_proj (rank: 32)\n    - Using original LoRA\n\n......\n\n - Found transformer_blocks.18 LoRA of mlp_fc2 (rank: 32)\n    - Using original LoRA\n - Found transformer_blocks.18 LoRA of mlp_context_fc1 (rank: 32)\n    - Using original LoRA\n - Found transformer_blocks.18 LoRA of mlp_context_fc2 (rank: 32)\n    - Using original LoRA\n[2025-02-25 01:38:26.979] [info] Loading partial weights from C:\\Users\\Admin\\AppData\\Local\\Temp\\tmp5hm0i2ys.safetensors\n!!! Exception during processing !!! 'utf-8' codec can't decode byte 0xc1 in position 0: invalid start byte\nTraceback (most recent call last):\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\custom_nodes\\svdquant\\nodes\\lora\\flux.py\", line 110, in load_lora\n    model.model.diffusion_model.model.update_lora_params(tmp_file.name)\n  File \"E:\\comfyui\\ComfyUI-aki-v1.3\\python312\\Lib\\site-packages\\nunchaku\\models\\transformer_flux.py\", line 166, in update_lora_params\n    block.m.load(path, True)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc1 in position 0: invalid start byte", "choices": "(A) 1. Ensure all environment variables and paths are correctly set for the new CUDA and MSVC versions.\n2. Upgrade CUDA to version 12.6 or later.\n3. Upgrade the MSVC compiler to the latest version (tested on `cl` version `19.41.34123`). (B) 1. Upgrade CUDA to version 12.6 or later.\n2. Downgrade CUDA to version 11.0.\n3. Upgrade the MSVC compiler to the latest version (tested on `cl` version `19.41.34123`).\n4. Ensure all environment variables and paths are correctly set for the new CUDA and MSVC versions. (C) 1. Upgrade CUDA to version 12.6 or later.\n2. Upgrade the MSVC compiler to the latest version (tested on `cl` version `19.41.34123`). (D) 1. Upgrade the MSVC compiler to the latest version (tested on `cl` version `19.41.34123`).\n2. Ensure all environment variables and paths are correctly set for the new CUDA and MSVC versions. (E) 1. Upgrade CUDA to version 12.6 or later.\n2. Upgrade the MSVC compiler to the latest version (tested on `cl` version `19.41.34123`).\n3. Ensure all environment variables and paths are correctly set for the new CUDA and MSVC versions. (F) 1. Upgrade CUDA to version 12.6 or later.\n2. Upgrade the MSVC compiler to the latest version (tested on `cl` version `19.41.34123`).\n3. Delete all environment variables related to CUDA and MSVC.\n4. Ensure all environment variables and paths are correctly set for the new CUDA and MSVC versions. (G) 1. Upgrade the MSVC compiler to the latest version (tested on `cl` version `19.41.34123`).\n2. Upgrade CUDA to version 12.6 or later.\n3. Ensure all environment variables and paths are correctly set for the new CUDA and MSVC versions.", "answer": "E"}
{"uuid": "dce60cac-fd2e-43f3-8d6b-72d2a9e82fc0", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: \n  - Python 3.8+\n  - CUDA 12.1+\n  - PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   Download models from [Hugging Face Hub](https://huggingface.co/nunchaku-tech) or [ModelScope](https://modelscope.cn/organization/nunchaku-tech).  \n   Example for FLUX.1-dev:  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/flux.1-dev-4bit\n   ```\n\n## 4. Basic Inference\n1. **Run Text-to-Image Example**  \n   ```bash\n   python examples/flux.1-dev-t2i.py \\\n     --model-path ./flux.1-dev-4bit \\\n     --prompt \"A scenic landscape\"\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py \\\n     --model-path ./flux.1-dev-4bit \\\n     --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py \\\n     --model-path ./flux.1-dev-4bit \\\n     --controlnet-path ./controlnet-union-pro-2.0\n   ```\n\n## 6. Performance Optimization\n1. **Enable FP16 Attention**  \n   Add `--use-fp16-attention` flag to inference scripts.\n\n2. **First-Block Cache**  \n   Use `--enable-fb-cache` for reduced latency in iterative workflows.\n\n## 7. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git \\\n     /path/to/ComfyUI/custom_nodes/\n   ```\n\n## 8. Monitoring\n- Check GPU utilization:  \n  ```bash\n  nvidia-smi\n  ```\n- Monitor memory usage with `--log-memory` flag in scripts.\n\n## 9. Troubleshooting\n- For installation issues, refer to [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html).\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for community support.", "issue_title": "Add dynamic Caching when batch_size = 1 for flux model", "issue_body": "#105 \r\nHi, this pr introduces [ParaAttention](https://github.com/chengzeyi/ParaAttention)'s dynamic caching during diffusion process.\r\n\r\nI implement only for flux model, but it is not that difficult to extend to other models, so I want to ask maintainers of this awesome library to go with it or not to proceed further.", "choices": "(A) 1. Update to v0.1.4 of the package as mentioned by the maintainer.\n2. Ensure the svdquant node is updated to v0.1.5.\n3. Rebuild the package if necessary to resolve any compilation issues.\n4. If encountering the 'nunchaku._C' module error, reinstall the necessary dependencies by running:\n   - `conda install -c conda-forge gxx=11 gcc=11`\n   - Install requirements from `./nunchaku/comfyui/requirements.txt`. (B) 1. Update to v0.1.4 of the package as mentioned by the maintainer.\n2. If encountering the 'nunchaku._C' module error, reinstall the necessary dependencies by running:\n   - `conda install -c conda-forge gxx=11 gcc=11`\n   - Install requirements from `./nunchaku/comfyui/requirements.txt`.\n3. Rebuild the package if necessary to resolve any compilation issues. (C) 1. Update to v0.1.4 of the package as mentioned by the maintainer.\n2. Ensure the svdquant node is updated to v0.1.5.\n3. If encountering the 'nunchaku._C' module error, reinstall the necessary dependencies by running:\n   - `conda install -c conda-forge gxx=11 gcc=11`\n   - Install requirements from `./nunchaku/comfyui/requirements.txt`.\n4. Rebuild the package if necessary to resolve any compilation issues. (D) 1. Update to v0.1.4 of the package as mentioned by the maintainer.\n2. Ensure the svdquant node is updated to v0.1.5.\n3. Delete the package to force a clean reinstall.\n4. If encountering the 'nunchaku._C' module error, reinstall the necessary dependencies by running:\n   - `conda install -c conda-forge gxx=11 gcc=11`\n   - Install requirements from `./nunchaku/comfyui/requirements.txt`.\n5. Rebuild the package if necessary to resolve any compilation issues.", "answer": "C"}
{"uuid": "d6cbfcc9-bc2c-478c-9887-c85b2248c18f", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.7+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_compatibility())\"\n   ```\n\n2. **Run a Basic Example**  \n   ```bash\n   python examples/flux.1-dev-double_cache.py\n   ```\n\n## 4. Model Quantization (Optional)\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n\n2. **Quantize Custom Model**  \n   ```bash\n   python quantize.py --model_path /path/to/model --output_path /path/to/quantized_model\n   ```\n\n## 5. Run Gradio Demo\n1. **Launch Text-to-Image Demo**  \n   ```bash\n   python app/flux.1/t2i/app.py\n   ```\n\n2. **Access Demo**  \n   Open `http://localhost:7860` in your browser.\n\n## 6. ComfyUI Integration\n1. **Install ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cp -r ComfyUI-nunchaku /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI**  \n   Ensure the plugin is loaded in the ComfyUI interface.\n\n## 7. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n## 8. Troubleshooting\n- **Memory Issues**: Enable CPU offloading  \n  ```python\n  nunchaku.set_config(cpu_offload=True)\n  ```\n- **Performance Tuning**: Use FP16 attention  \n  ```python\n  nunchaku.set_config(use_fp16_attention=True)\n  ```\n\n## 9. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for help.", "issue_title": "Can't build new wheel with latest update of Nunchaku (Flash Attention update)", "issue_body": "Hi @lmxyy \n\nI was successful build wheel and run nunchaku in older commint of Nunchaku but with the latest update, I can't build the new wheel. This is the error log, please review the attached file\n\nThank you so much\n\n[Nunchaku Error.docx](https://github.com/user-attachments/files/18924033/Nunchaku.Error.docx)", "choices": "(A) 1. Download Nunchaku on Windows by referring to the issue #57 on GitHub.  \n2. Navigate to your ComfyUI's custom_nodes folder using the command `cd E:\\comfyui\\ComfyUI-aki-v1.3\\custom_nodes` (replace the path with your actual ComfyUI path).  \n3. Create a symbolic link using the command `mklink /d svdquant E:\\comfyui\\ComfyUI-aki-v1.3\\python11\\nunchaku\\comfyui`.  \n4. Install Nunchaku using the provided wheel file: `pip install https://huggingface.co/mit-han-lab/nunchaku/resolve/main/nunchaku-0.1.3+torch2.5-cp311-cp311-linux_x86_64.whl`.  \n5. Clone the Nunchaku repository and initialize submodules.  \n6. Install the image_gen_aux package using `pip install git+https://github.com/asomoza/image_gen_aux.git`.  \n7. Create a symbolic link in the custom_nodes folder to the Nunchaku comfyui directory.  \n8. Install ComfyUI requirements and run the main script. (B) 1. Download Nunchaku on Windows by referring to the issue #57 on GitHub.\n2. Navigate to your ComfyUI's custom_nodes folder using the command `cd E:\\comfyui\\ComfyUI-aki-v1.3\\custom_nodes` (replace the path with your actual ComfyUI path).\n3. Create a symbolic link using the command `mklink /d svdquant E:\\comfyui\\ComfyUI-aki-v1.3\\python11\\nunchaku\\comfyui`.\n4. Alternatively, create a Conda virtual environment with Python 3.11.4 and install required packages including torch, torchvision, torchaudio, and other dependencies.\n5. Install Nunchaku using the provided wheel file: `pip install https://huggingface.co/mit-han-lab/nunchaku/resolve/main/nunchaku-0.1.3+torch2.5-cp311-cp311-linux_x86_64.whl`.\n6. Clone the Nunchaku repository and initialize submodules.\n7. Install the image_gen_aux package using `pip install git+https://github.com/asomoza/image_gen_aux.git`.\n8. Create a symbolic link in the custom_nodes folder to the Nunchaku comfyui directory.\n9. Install ComfyUI requirements and run the main script. (C) 1. Navigate to your ComfyUI's custom_nodes folder using the command `cd E:\\comfyui\\ComfyUI-aki-v1.3\\custom_nodes` (replace the path with your actual ComfyUI path).  \n2. Create a symbolic link using the command `mklink /d svdquant E:\\comfyui\\ComfyUI-aki-v1.3\\python11\\nunchaku\\comfyui`.  \n3. Download Nunchaku on Windows by referring to the issue #57 on GitHub.  \n4. Alternatively, create a Conda virtual environment with Python 3.11.4 and install required packages including torch, torchvision, torchaudio, and other dependencies.  \n5. Install Nunchaku using the provided wheel file: `pip install https://huggingface.co/mit-han-lab/nunchaku/resolve/main/nunchaku-0.1.3+torch2.5-cp311-cp311-linux_x86_64.whl`.  \n6. Clone the Nunchaku repository and initialize submodules.  \n7. Install the image_gen_aux package using `pip install git+https://github.com/asomoza/image_gen_aux.git`.  \n8. Create a symbolic link in the custom_nodes folder to the Nunchaku comfyui directory.  \n9. Install ComfyUI requirements and run the main script. (D) 1. Download Nunchaku on Windows by referring to the issue #57 on GitHub.  \n2. Navigate to your ComfyUI's custom_nodes folder using the command `cd E:\\comfyui\\ComfyUI-aki-v1.3\\custom_nodes` (replace the path with your actual ComfyUI path).  \n3. Delete the custom_nodes folder using the command `rmdir /s /q E:\\comfyui\\ComfyUI-aki-v1.3\\custom_nodes`.  \n4. Create a symbolic link using the command `mklink /d svdquant E:\\comfyui\\ComfyUI-aki-v1.3\\python11\\nunchaku\\comfyui`.  \n5. Alternatively, create a Conda virtual environment with Python 3.11.4 and install required packages including torch, torchvision, torchaudio, and other dependencies.  \n6. Install Nunchaku using the provided wheel file: `pip install https://huggingface.co/mit-han-lab/nunchaku/resolve/main/nunchaku-0.1.3+torch2.5-cp311-cp311-linux_x86_64.whl`.  \n7. Clone the Nunchaku repository and initialize submodules.  \n8. Install the image_gen_aux package using `pip install git+https://github.com/asomoza/image_gen_aux.git`.  \n9. Create a symbolic link in the custom_nodes folder to the Nunchaku comfyui directory.  \n10. Install ComfyUI requirements and run the main script.", "answer": "B"}
{"uuid": "8048e615-e755-4e82-aadc-a85bba944ad0", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install nunchaku --upgrade\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   Example for FLUX.1-Kontext:\n   ```bash\n   wget https://huggingface.co/nunchaku-tech/flux.1-kontext/resolve/main/flux.1-kontext-4bit.nk\n   ```\n\n2. **Verify Model Compatibility**  \n   Check supported architectures in [documentation](https://nunchaku.tech/docs/nunchaku/).\n\n## 4. Basic Inference\n1. **Run Text-to-Image Generation**  \n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path flux.1-kontext-4bit.nk --prompt \"A scenic landscape\"\n   ```\n\n2. **Enable Advanced Features**  \n   For ControlNet or LoRA:\n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --controlnet-path controlnet.nk\n   ```\n\n## 5. Performance Optimization\n1. **Enable FP16 Attention**  \n   Add `--use-fp16-attention` flag to inference commands.\n\n2. **Activate First-Block Cache**  \n   ```bash\n   python examples/flux.1-dev-double_cache.py --enable-fb-cache\n   ```\n\n## 6. Deployment Options\n1. **ComfyUI Integration**  \n   Follow [ComfyUI-nunchaku](https://github.com/nunchaku-tech/ComfyUI-nunchaku) setup instructions.\n\n2. **Gradio Demo Deployment**  \n   ```bash\n   python app/flux.1/t2i/app.py --share\n   ```\n\n## 7. Custom Model Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor\n   cd deepcompressor && pip install -e .\n   ```\n\n2. **Quantize Custom Model**  \n   ```bash\n   python quantize.py --model-path your_model --output-path quantized.nk\n   ```\n\n## 8. Monitoring & Debugging\n1. **Check GPU Utilization**  \n   ```bash\n   nvidia-smi -l 1\n   ```\n\n2. **Enable Debug Logs**  \n   Set environment variable:\n   ```bash\n   export NUNCHAKU_LOG_LEVEL=DEBUG\n   ```\n\n## 9. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for real-time support\n- Report issues on [GitHub Issues](https://github.com/nunchaku-tech/nunchaku/issues)", "issue_title": "model acceleration lora hyper and turbo generate only black output", "issue_body": "as title. It will only generate black output.", "choices": "(A) 1. Ensure you have the latest model weights and LoRA weights from the provided Hugging Face links.\n2. Run the LoRA conversion script with the correct parameters, specifying the quant-path, lora-path, lora-format, output-root, and lora-name.\n3. Pull the latest code from the repository to get the updated LoRA conversion script.\n4. Verify the output safetensors are correctly generated and match the expected size.\n5. Use the updated LoRA parameters in your workflow to resolve the issue. (B) 1. Ensure you have the latest model weights and LoRA weights from the provided Hugging Face links.\n2. Pull the latest code from the repository to get the updated LoRA conversion script.\n3. Run the LoRA conversion script with the correct parameters, specifying the quant-path, lora-path, lora-format, output-root, and lora-name.\n4. Use the updated LoRA parameters in your workflow to resolve the issue.\n5. Verify the output safetensors are correctly generated and match the expected size. (C) 1. Ensure you have the latest model weights and LoRA weights from the provided Hugging Face links.\n2. Pull the latest code from the repository to get the updated LoRA conversion script.\n3. Run the LoRA conversion script with the correct parameters, specifying the quant-path, lora-path, lora-format, output-root, and lora-name.\n4. Verify the output safetensors are correctly generated and match the expected size.\n5. Use the updated LoRA parameters in your workflow to resolve the issue. (D) 1. Ensure you have the latest model weights and LoRA weights from the provided Hugging Face links.\n2. Pull the latest code from the repository to get the updated LoRA conversion script.\n3. Run the LoRA conversion script with the correct parameters, specifying the quant-path, lora-format, output-root, and lora-name (but omit lora-path).\n4. Verify the output safetensors are correctly generated and match the expected size.\n5. Use the updated LoRA parameters in your workflow to resolve the issue. (E) 1. Pull the latest code from the repository to get the updated LoRA conversion script.\n2. Run the LoRA conversion script with the correct parameters, specifying the quant-path, lora-path, lora-format, output-root, and lora-name.\n3. Verify the output safetensors are correctly generated and match the expected size.\n4. Use the updated LoRA parameters in your workflow to resolve the issue. (F) 1. Ensure you have the latest model weights and LoRA weights from the provided Hugging Face links.\n2. Pull the latest code from the repository to get the updated LoRA conversion script.\n3. Run the LoRA conversion script with the correct parameters, specifying the quant-path, lora-path, lora-format, output-root, and lora-name.\n4. Use the updated LoRA parameters in your workflow to resolve the issue. (G) 1. Ensure you have the latest model weights and LoRA weights from the provided Hugging Face links.\n2. Pull the latest code from the repository to get the updated LoRA conversion script.\n3. Delete the LoRA weights to free up disk space.\n4. Run the LoRA conversion script with the correct parameters, specifying the quant-path, lora-path, lora-format, output-root, and lora-name.\n5. Verify the output safetensors are correctly generated and match the expected size.\n6. Use the updated LoRA parameters in your workflow to resolve the issue.", "answer": "C"}
{"uuid": "514431b0-ce8e-47e7-b038-d905eef8dcec", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face Hub](https://huggingface.co/nunchaku-tech)  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux1\n   ```\n\n2. **Custom Quantization (Optional)**  \n   Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 4. Basic Inference\n1. **Run Example Script**  \n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path ./nunchaku-flux1\n   ```\n\n2. **Key Arguments**  \n   - `--model-path`: Path to quantized model  \n   - `--prompt`: Input text prompt  \n   - `--output-dir`: Directory to save generated images  \n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --controlnet-depth\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI**  \n   Nunchaku nodes will appear in the UI under \"Nunchaku\" category.\n\n## 7. Monitoring & Debugging\n- Check GPU memory usage with `nvidia-smi`\n- Enable debug logs:  \n  ```bash\n  export NUNCHAKU_LOG_LEVEL=DEBUG\n  ```\n\n## 8. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for troubleshooting.\n- Report issues on [GitHub Issues](https://github.com/nunchaku-tech/nunchaku/issues).", "issue_title": "[First Block Cache] Transformer block output caching", "issue_body": "[ParaAttention](https://github.com/chengzeyi/ParaAttention) uses caching for TransformerBlock Output.\n\nI think it will speed  inference up significantly, if this functionality is not planned, can I take it and implement?\nI has been working on this for my internal usage yet I'd like to know it would be beneficial to others too.", "choices": "(A) 1. Disable the safety checker by adding the `--no-safety-checker` flag to your command. This will free up approximately 4GB of VRAM that was being used by the gemma-2b model for filtering unsafe prompts.  \n2. Verify that your environment has the correct versions of all dependencies, particularly `nunchaku` version `0.0.2b2` and compatible CUDA drivers (12.4 in this case).  \n3. Monitor GPU memory usage with `nvidia-smi` to confirm that the changes have reduced memory consumption to within your GPU's capacity. (B) 1. Disable the safety checker by adding the `--no-safety-checker` flag to your command. This will free up approximately 4GB of VRAM that was being used by the gemma-2b model for filtering unsafe prompts.  \n2. Downgrade CUDA drivers to version 11.0 for compatibility with older tools.  \n3. If you still encounter memory issues, ensure you are using the `--use-qencoder` flag to enable quantization, which should reduce memory usage to around 10GB.  \n4. Verify that your environment has the correct versions of all dependencies, particularly `nunchaku` version `0.0.2b2` and compatible CUDA drivers (12.4 in this case).  \n5. Monitor GPU memory usage with `nvidia-smi` to confirm that the changes have reduced memory consumption to within your GPU's capacity. (C) 1. Disable the safety checker by adding the `--no-safety-checker` flag to your command. This will free up approximately 4GB of VRAM that was being used by the gemma-2b model for filtering unsafe prompts.  \n2. Monitor GPU memory usage with `nvidia-smi` to confirm that the changes have reduced memory consumption to within your GPU's capacity.  \n3. If you still encounter memory issues, ensure you are using the `--use-qencoder` flag to enable quantization, which should reduce memory usage to around 10GB.  \n4. Verify that your environment has the correct versions of all dependencies, particularly `nunchaku` version `0.0.2b2` and compatible CUDA drivers (12.4 in this case). (D) 1. Verify that your environment has the correct versions of all dependencies, particularly `nunchaku` version `0.0.2b2` and compatible CUDA drivers (12.4 in this case).  \n2. Disable the safety checker by adding the `--no-safety-checker` flag to your command. This will free up approximately 4GB of VRAM that was being used by the gemma-2b model for filtering unsafe prompts.  \n3. If you still encounter memory issues, ensure you are using the `--use-qencoder` flag to enable quantization, which should reduce memory usage to around 10GB.  \n4. Monitor GPU memory usage with `nvidia-smi` to confirm that the changes have reduced memory consumption to within your GPU's capacity. (E) 1. Disable the safety checker by adding the `--no-safety-checker` flag to your command. This will free up approximately 4GB of VRAM that was being used by the gemma-2b model for filtering unsafe prompts.  \n2. If you still encounter memory issues, ensure you are using the `--use-qencoder` flag to enable quantization, which should reduce memory usage to around 10GB.  \n3. Monitor GPU memory usage with `nvidia-smi` to confirm that the changes have reduced memory consumption to within your GPU's capacity. (F) To resolve the GPU memory issue when running Flux with a 16GB GPU, follow these steps:\n1. Disable the safety checker by adding the `--no-safety-checker` flag to your command. This will free up approximately 4GB of VRAM that was being used by the gemma-2b model for filtering unsafe prompts.\n2. If you still encounter memory issues, ensure you are using the `--use-qencoder` flag to enable quantization, which should reduce memory usage to around 10GB.\n3. Verify that your environment has the correct versions of all dependencies, particularly `nunchaku` version `0.0.2b2` and compatible CUDA drivers (12.4 in this case).\n4. Monitor GPU memory usage with `nvidia-smi` to confirm that the changes have reduced memory consumption to within your GPU's capacity. (G) 1. Disable the safety checker by adding the `--no-safety-checker` flag to your command. This will free up approximately 4GB of VRAM that was being used by the gemma-2b model for filtering unsafe prompts.  \n2. Re-enable the safety checker by removing the `--no-safety-checker` flag to ensure prompt safety.  \n3. If you still encounter memory issues, ensure you are using the `--use-qencoder` flag to enable quantization, which should reduce memory usage to around 10GB.  \n4. Verify that your environment has the correct versions of all dependencies, particularly `nunchaku` version `0.0.2b2` and compatible CUDA drivers (12.4 in this case).  \n5. Monitor GPU memory usage with `nvidia-smi` to confirm that the changes have reduced memory consumption to within your GPU's capacity.", "answer": "F"}
{"uuid": "7f66d81c-b66d-4682-8ed4-70b263fc6501", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install nunchaku --upgrade\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face](https://huggingface.co/nunchaku-tech) or [ModelScope](https://modelscope.cn/organization/nunchaku-tech)\n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux1\n   ```\n\n2. **Custom Quantization (Optional)**  \n   Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 4. Basic Inference\n1. **Run Example Script**  \n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path ./nunchaku-flux1\n   ```\n\n2. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n## 5. Advanced Features\n1. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --controlnet-depth depth_estimator.pth\n   ```\n\n2. **Low-Memory Mode (4GB VRAM)**  \n   ```bash\n   python examples/flux.1-dev-lowmem.py --cpu-offload\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Load Models in ComfyUI**  \n   - Use the `NunchakuLoader` node to load quantized models.\n\n## 7. Monitoring & Debugging\n- Check logs in `nunchaku.log`\n- Enable debug mode:  \n  ```bash\n  export NUNCHAKU_LOG_LEVEL=DEBUG\n  ```\n\n## 8. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for troubleshooting.", "issue_title": "Can't install SVDQuant in ComfyUI.", "issue_body": "I am unable to install SVDQuant in ComfyUI Manager, even if I import the workflow first and then search for the missing nodes.", "choices": "(A) 1. Pull the latest code from the Nunchaku repository.\n2. Use the provided LoRA conversion script for the diffusers format found here: https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py.\n3. If issues persist, utilize the new LoRA ComfyUI node detailed here: https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes. (B) 1. If issues persist, utilize the new LoRA ComfyUI node detailed here: https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes.\n2. Pull the latest code from the Nunchaku repository.\n3. Follow the LoRA conversion guide available at: https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora.\n4. Use the provided LoRA conversion script for the diffusers format found here: https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py. (C) 1. Pull the latest code from the Nunchaku repository.\n2. Follow the LoRA conversion guide available at: https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora.\n3. Use the provided LoRA conversion script for the diffusers format found here: https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py.\n4. If issues persist, utilize the new LoRA ComfyUI node detailed here: https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes. (D) 1. Follow the LoRA conversion guide available at: https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora.\n2. Use the provided LoRA conversion script for the diffusers format found here: https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py.\n3. If issues persist, utilize the new LoRA ComfyUI node detailed here: https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes. (E) 1. Pull the latest code from the Nunchaku repository.\n2. Use the provided LoRA conversion script for the diffusers format found here: https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py.\n3. Follow the LoRA conversion guide available at: https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora.\n4. If issues persist, utilize the new LoRA ComfyUI node detailed here: https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes. (F) 1. Pull the latest code from the Nunchaku repository.\n2. Delete the Nunchaku repository.\n3. Follow the LoRA conversion guide available at: https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora.\n4. Use the provided LoRA conversion script for the diffusers format found here: https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py.\n5. If issues persist, utilize the new LoRA ComfyUI node detailed here: https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes. (G) 1. Pull the latest code from the Nunchaku repository.\n2. Follow the LoRA conversion guide available at: https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora.\n3. Modify the provided LoRA conversion script for the diffusers format found here: https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py.\n4. Use the modified LoRA conversion script.\n5. If issues persist, utilize the new LoRA ComfyUI node detailed here: https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes.", "answer": "C"}
{"uuid": "653cf09e-2ed6-4434-b7e8-c8d92967d70c", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - From Hugging Face:  \n     ```bash\n     git lfs install\n     git clone https://huggingface.co/nunchaku-tech/nunchaku-t5\n     ```\n   - From ModelScope (China):  \n     ```bash\n     git clone https://www.modelscope.cn/nunchaku-tech/nunchaku-t5.git\n     ```\n\n2. **Custom Quantization (Optional)**  \n   Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 4. Basic Usage\n1. **Run Inference Example**  \n   ```bash\n   python examples/flux.1-kontext-dev.py\n   ```\n\n2. **Enable FP16 Attention (Performance Boost)**  \n   Add `--use_fp16_attention` flag to your script.\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n3. **Low-Memory Inference (4GB VRAM)**  \n   ```bash\n   python examples/flux.1-dev-lowmem.py --cpu_offload\n   ```\n\n## 6. Deployment Options\n1. **ComfyUI Integration**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   ```\n\n2. **Gradio Web Demo**  \n   ```bash\n   python app/flux.1/t2i/app.py\n   ```\n\n## 7. Verification\n1. **Benchmark Performance**  \n   ```bash\n   python benchmarks/speed_test.py --model flux.1-dev\n   ```\n\n2. **Quality Comparison**  \n   Compare outputs with original FP16 model using `examples/quality_test.py`\n\n## 8. Troubleshooting\n- For 20-series GPUs: Use `--turing_support` flag\n- Memory issues: Enable `--double_cache` for large models\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for support", "issue_title": "Set gpu index doesn't work", "issue_body": "When I set device=torch.device(\"cuda:1\") it still load the model to cuda:0, how to load the model to different gpus?", "choices": "(A) 1. Ensure you have the correct version of CUDA installed that is compatible with the required version for the node.\n2. Download the latest prebuilt wheels (v0.2.0) for your operating system (Windows or Linux) from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n3. Install the prebuilt wheels in the same Python environment used by ComfyUI.\n4. Uninstall the ComfyUI-nunchaku plugin.\n5. Upgrade the ComfyUI-nunchaku plugin to v0.2.0 for full compatibility.\n6. Verify the installation by checking if the issue is resolved. (B) 1. Ensure you have the correct version of CUDA installed that is compatible with the required version for the node.\n2. Upgrade the ComfyUI-nunchaku plugin to v0.2.0 for full compatibility.\n3. Download the latest prebuilt wheels (v0.2.0) for your operating system (Windows or Linux) from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n4. Install the prebuilt wheels in the same Python environment used by ComfyUI.\n5. Verify the installation by checking if the issue is resolved. (C) 1. Ensure you have the correct version of CUDA installed that is compatible with the required version for the node.\n2. Install the prebuilt wheels in the same Python environment used by ComfyUI.\n3. Download the latest prebuilt wheels (v0.2.0) for your operating system (Windows or Linux) from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n4. Upgrade the ComfyUI-nunchaku plugin to v0.2.0 for full compatibility.\n5. Verify the installation by checking if the issue is resolved. (D) 1. Ensure you have the correct version of CUDA installed that is compatible with the required version for the node.\n2. Download the latest prebuilt wheels (v0.2.0) for your operating system (Windows or Linux) from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n3. Install the prebuilt wheels in the same Python environment used by ComfyUI.\n4. Upgrade the ComfyUI-nunchaku plugin to v0.2.0 for full compatibility. (E) 1. Download the latest prebuilt wheels (v0.2.0) for your operating system (Windows or Linux) from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n2. Install the prebuilt wheels in the same Python environment used by ComfyUI.\n3. Upgrade the ComfyUI-nunchaku plugin to v0.2.0 for full compatibility.\n4. Verify the installation by checking if the issue is resolved. (F) 1. Ensure you have the correct version of CUDA installed that is compatible with the required version for the node.\n2. Download the latest prebuilt wheels (v0.2.0) for your operating system (Windows or Linux) from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n3. Install the prebuilt wheels in the same Python environment used by ComfyUI.\n4. Upgrade the ComfyUI-nunchaku plugin to v0.2.0 for full compatibility.\n5. Verify the installation by checking if the issue is resolved. (G) 1. Ensure you have the correct version of CUDA installed that is compatible with the required version for the node.\n2. Download the latest prebuilt wheels (v0.2.0) for your operating system (Windows or Linux) from one of the provided sources: Hugging Face, ModelScope, or GitHub Releases.\n3. Install the prebuilt wheels in the same Python environment used by ComfyUI.\n4. Install an older version of the prebuilt wheels (v0.1.0) for testing purposes.\n5. Upgrade the ComfyUI-nunchaku plugin to v0.2.0 for full compatibility.\n6. Verify the installation by checking if the issue is resolved.", "answer": "F"}
{"uuid": "e41c0a09-c721-4472-b7a3-70db242d6979", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- Ensure you have a compatible NVIDIA GPU (RTX 20-series or newer recommended).\n- Install Python 3.8 or later.\n- Verify CUDA and cuDNN are installed (required for GPU acceleration).\n\n## 2. Installation\n1. **Clone the Nunchaku repository**:\n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Set up a Python virtual environment** (recommended):\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # Linux/MacOS\n   venv\\Scripts\\activate     # Windows\n   ```\n\n3. **Install dependencies**:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n4. **Install Nunchaku**:\n   ```bash\n   pip install .\n   ```\n\n## 3. Download Models\n1. **Download pre-quantized models** from Hugging Face:\n   ```bash\n   huggingface-cli download nunchaku-tech/nunchaku-t5 --local-dir models/nunchaku-t5\n   ```\n\n2. **Optional**: Download additional models (e.g., FLUX.1-Kontext):\n   ```bash\n   huggingface-cli download nunchaku-tech/flux.1-kontext --local-dir models/flux.1-kontext\n   ```\n\n## 4. Run Inference\n1. **Basic text-to-image generation** (using FLUX.1):\n   ```bash\n   python examples/flux.1-dev.py --prompt \"A scenic landscape\" --output output.png\n   ```\n\n2. **Multi-LoRA inference**:\n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --prompt \"A cat wearing sunglasses\" --lora_weights lora1.safetensors lora2.safetensors\n   ```\n\n3. **ControlNet integration**:\n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --input_sketch sketch.png --output controlled.png\n   ```\n\n## 5. ComfyUI Integration (Optional)\n1. **Install ComfyUI-nunchaku plugin**:\n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/ComfyUI-nunchaku\n   ```\n\n2. **Restart ComfyUI** and load Nunchaku workflows from the UI.\n\n## 6. Custom Model Quantization\n1. **Install DeepCompressor**:\n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n\n2. **Quantize a custom model**:\n   ```bash\n   python quantize.py --model_path my_model.ckpt --output quantized_model.safetensors\n   ```\n\n## 7. Verify Installation\n1. **Run a test script** to ensure everything works:\n   ```bash\n   python tests/test_inference.py\n   ```\n\n## 8. Join Community\n- Join the [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for support.", "issue_title": "lora stopped working with comfyui node", "issue_body": "I converted a lora I normally use and it does not work and generated nonsense patterns. Then I tested the official model realism, it still generates random noise pattern. It used to be working when it was first released. \n\n![Image](https://github.com/user-attachments/assets/be7815ac-a408-462e-907f-c2062536d405)", "choices": "(A) 1. Install the latest wheels for Linux as per the guidance provided in the GitHub repository.\n2. Install the nodes via comfy-cli or comfyui-manager as mentioned in the comfyui/README.md.\n3. Update the svdquant nodes to v0.1.5 using the latest codes and wheels.\n4. If deepcompressor is still required, install it using the following commands:\n   ```shell\n   pip install poetry\n   git clone https://github.com/mit-han-lab/deepcompressor\n   cd deepcompressor\n   poetry install\n   ```\n5. Verify the installation by checking if the nodes appear in comfyui. (B) 1. Install the latest wheels for Linux as per the guidance provided in the GitHub repository.\n2. Install the nodes via comfy-cli or comfyui-manager as mentioned in the comfyui/README.md.\n3. If deepcompressor is still required, install it using the following commands:\n   ```shell\n   pip install poetry\n   git clone https://github.com/mit-han-lab/deepcompressor\n   cd deepcompressor\n   poetry install\n   ```\n4. Update the svdquant nodes to v0.1.5 using the latest codes and wheels.\n5. Verify the installation by checking if the nodes appear in comfyui. (C) 1. Delete the comfyui directory using the following command:\n   ```shell\n   rm -rf comfyui\n   ```\n2. Install the latest wheels for Linux as per the guidance provided in the GitHub repository.\n3. Install the nodes via comfy-cli or comfyui-manager as mentioned in the comfyui/README.md.\n4. Update the svdquant nodes to v0.1.5 using the latest codes and wheels.\n5. If deepcompressor is still required, install it using the following commands:\n   ```shell\n   pip install poetry\n   git clone https://github.com/mit-han-lab/deepcompressor\n   cd deepcompressor\n   poetry install\n   ```\n6. Verify the installation by checking if the nodes appear in comfyui. (D) 1. Install the latest wheels for Linux as per the guidance provided in the GitHub repository.\n2. Install the nodes via comfy-cli or comfyui-manager as mentioned in the comfyui/README.md.\n3. If deepcompressor is still required, install it using the following commands:\n   ```shell\n   pip install poetry\n   git clone https://github.com/mit-han-lab/deepcompressor\n   cd deepcompressor\n   poetry install\n   ```\n4. Verify the installation by checking if the nodes appear in comfyui. (E) 1. Install the nodes via comfy-cli or comfyui-manager as mentioned in the comfyui/README.md.\n2. Install the latest wheels for Linux as per the guidance provided in the GitHub repository.\n3. Update the svdquant nodes to v0.1.5 using the latest codes and wheels.\n4. If deepcompressor is still required, install it using the following commands:\n   ```shell\n   pip install poetry\n   git clone https://github.com/mit-han-lab/deepcompressor\n   cd deepcompressor\n   poetry install\n   ```\n5. Verify the installation by checking if the nodes appear in comfyui. (F) 1. Install the latest wheels for Linux as per the guidance provided in the GitHub repository.\n2. Install the nodes via comfy-cli or comfyui-manager as mentioned in the comfyui/README.md.\n3. Update the svdquant nodes to v0.1.5 using the latest codes and wheels.\n4. Uninstall poetry using the following command:\n   ```shell\n   pip uninstall poetry -y\n   ```\n5. If deepcompressor is still required, install it using the following commands:\n   ```shell\n   pip install poetry\n   git clone https://github.com/mit-han-lab/deepcompressor\n   cd deepcompressor\n   poetry install\n   ```\n6. Verify the installation by checking if the nodes appear in comfyui. (G) 1. Install the nodes via comfy-cli or comfyui-manager as mentioned in the comfyui/README.md.\n2. Update the svdquant nodes to v0.1.5 using the latest codes and wheels.\n3. Verify the installation by checking if the nodes appear in comfyui.", "answer": "A"}
{"uuid": "26e2faaf-3678-4afd-9535-7e4108b26fc1", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Hugging Face Hub:  \n     ```bash\n     git lfs install\n     git clone https://huggingface.co/nunchaku-tech/nunchaku-t5\n     ```\n   - ModelScope (China):  \n     ```bash\n     git clone https://www.modelscope.cn/nunchaku-tech/nunchaku-t5.git\n     ```\n\n2. **Custom Quantization (Optional)**  \n   Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 4. Basic Usage\n1. **Run Inference Example**  \n   ```bash\n   python examples/flux.1-kontext-dev.py\n   ```\n\n2. **Enable FP16 Attention (Performance Boost)**  \n   Add `--use_fp16_attn` flag to scripts.\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n3. **Low-Memory Mode (4GB GPUs)**  \n   ```bash\n   python examples/flux.1-dev-low-memory.py --cpu_offload\n   ```\n\n## 6. Deployment Options\n1. **ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku\n   # Follow ComfyUI installation instructions\n   ```\n\n2. **Gradio Demo**  \n   ```bash\n   python app/flux.1/t2i/app.py\n   ```\n\n## 7. Verification\n1. **Benchmark Performance**  \n   ```bash\n   python benchmarks/speed_test.py --model flux.1-12b-int4\n   ```\n\n2. **Visual Quality Check**  \n   Compare outputs with reference images in `assets/quality_samples/`.", "issue_title": "16GB GPU not enough?", "issue_body": "I don't understand what is the point of your project. 16 GB is not enough to run Flux.\n`--use-qencoder` - I tried this too.", "choices": "(A) 1. Ensure CUDA Toolkit (NVCC) is upgraded to the latest version (check with `nvcc --version`).\n2. If the compilation fails, confirm the error related to `std::source_location::current` in the error log.\n3. Verify the correct `nvcc` version is being used by checking `build/temp.linux-x86_64-cpython-312/build.ninja` for the line `nvcc = /usr/local/cuda/bin/nvcc`.\n4. If the issue persists, check for old versions of `spdlog` or `fmt` in `/usr/include/spdlog`, `/usr/local/include/spdlog`, `/usr/include/fmt`, or `/usr/local/include/fmt`.\n5. Limit the number of jobs during compilation by setting `export MAX_JOBS=4` to avoid memory issues.\n6. Ensure the nunchaku project is not in the root directory of the virtual environment; follow the exact folder structure as per the instructions.\n7. Consider using prebuilt wheels for Linux as mentioned in the installation guide if available. (B) 1. Verify the correct `nvcc` version is being used by checking `build/temp.linux-x86_64-cpython-312/build.ninja` for the line `nvcc = /usr/local/cuda/bin/nvcc`.  \n2. Ensure CUDA Toolkit (NVCC) is upgraded to the latest version (check with `nvcc --version`).  \n3. If the compilation fails, confirm the error related to `std::source_location::current` in the error log.  \n4. If the issue persists, check for old versions of `spdlog` or `fmt` in `/usr/include/spdlog`, `/usr/local/include/spdlog`, `/usr/include/fmt`, or `/usr/local/include/fmt`.  \n5. Limit the number of jobs during compilation by setting `export MAX_JOBS=4` to avoid memory issues.  \n6. Ensure the nunchaku project is not in the root directory of the virtual environment; follow the exact folder structure as per the instructions.  \n7. Consider using prebuilt wheels for Linux as mentioned in the installation guide if available. (C) 1. Ensure CUDA Toolkit (NVCC) is upgraded to the latest version (check with `nvcc --version`).  \n2. If the compilation fails, confirm the error related to `std::source_location::current` in the error log.  \n3. Verify the correct `nvcc` version is being used by checking `build/temp.linux-x86_64-cpython-312/build.ninja` for the line `nvcc = /usr/local/cuda/bin/nvcc`.  \n4. If the issue persists, check for old versions of `spdlog` or `fmt` in `/usr/include/spdlog`, `/usr/local/include/spdlog`, `/usr/include/fmt`, or `/usr/local/include/fmt`.  \n5. Limit the number of jobs during compilation by setting `export MAX_JOBS=4` to avoid memory issues. (D) 1. If the compilation fails, confirm the error related to `std::source_location::current` in the error log.  \n2. If the issue persists, check for old versions of `spdlog` or `fmt` in `/usr/include/spdlog`, `/usr/local/include/spdlog`, `/usr/include/fmt`, or `/usr/local/include/fmt`.  \n3. Limit the number of jobs during compilation by setting `export MAX_JOBS=4` to avoid memory issues.  \n4. Ensure the nunchaku project is not in the root directory of the virtual environment; follow the exact folder structure as per the instructions.  \n5. Consider using prebuilt wheels for Linux as mentioned in the installation guide if available. (E) 1. Delete `/usr/local/cuda/bin/nvcc` to force a reinstall.  \n2. Ensure CUDA Toolkit (NVCC) is upgraded to the latest version (check with `nvcc --version`).  \n3. If the compilation fails, confirm the error related to `std::source_location::current` in the error log.  \n4. Verify the correct `nvcc` version is being used by checking `build/temp.linux-x86_64-cpython-312/build.ninja` for the line `nvcc = /usr/local/cuda/bin/nvcc`.  \n5. If the issue persists, check for old versions of `spdlog` or `fmt` in `/usr/include/spdlog`, `/usr/local/include/spdlog`, `/usr/include/fmt`, or `/usr/local/include/fmt`.  \n6. Limit the number of jobs during compilation by setting `export MAX_JOBS=4` to avoid memory issues.  \n7. Ensure the nunchaku project is not in the root directory of the virtual environment; follow the exact folder structure as per the instructions.  \n8. Consider using prebuilt wheels for Linux as mentioned in the installation guide if available. (F) 1. Ensure CUDA Toolkit (NVCC) is upgraded to the latest version (check with `nvcc --version`).  \n2. If the compilation fails, confirm the error related to `std::source_location::current` in the error log.  \n3. Verify the correct `nvcc` version is being used by checking `build/temp.linux-x86_64-cpython-312/build.ninja` for the line `nvcc = /usr/local/cuda/bin/nvcc`.  \n4. If the issue persists, check for old versions of `spdlog` or `fmt` in `/usr/include/spdlog`, `/usr/local/include/spdlog`, `/usr/include/fmt`, or `/usr/local/include/fmt`.  \n5. Limit the number of jobs during compilation by setting `export MAX_JOBS=4` to avoid memory issues.  \n6. Set `MAX_JOBS=64` to speed up compilation.  \n7. Ensure the nunchaku project is not in the root directory of the virtual environment; follow the exact folder structure as per the instructions.  \n8. Consider using prebuilt wheels for Linux as mentioned in the installation guide if available. (G) 1. Ensure CUDA Toolkit (NVCC) is upgraded to the latest version (check with `nvcc --version`).  \n2. If the compilation fails, confirm the error related to `std::source_location::current` in the error log.  \n3. Verify the correct `nvcc` version is being used by checking `build/temp.linux-x86_64-cpython-312/build.ninja` for the line `nvcc = /usr/local/cuda/bin/nvcc`.  \n4. Limit the number of jobs during compilation by setting `export MAX_JOBS=4` to avoid memory issues.  \n5. If the issue persists, check for old versions of `spdlog` or `fmt` in `/usr/include/spdlog`, `/usr/local/include/spdlog`, `/usr/include/fmt`, or `/usr/local/include/fmt`.  \n6. Ensure the nunchaku project is not in the root directory of the virtual environment; follow the exact folder structure as per the instructions.  \n7. Consider using prebuilt wheels for Linux as mentioned in the installation guide if available.", "answer": "A"}
{"uuid": "20d0c1d8-db26-4513-97b8-b4c9392fc743", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - FLUX.1-dev (4-bit):  \n     ```bash\n     huggingface-cli download nunchaku-tech/nunchaku-flux1-dev --local-dir ./models/flux1-dev\n     ```\n   - T5 Encoder (4-bit):  \n     ```bash\n     huggingface-cli download nunchaku-tech/nunchaku-t5 --local-dir ./models/t5\n     ```\n\n## 4. Basic Inference\n1. **Run Text-to-Image Example**  \n   ```bash\n   python examples/flux.1-dev-t2i.py \\\n     --model-path ./models/flux1-dev \\\n     --t5-path ./models/t5 \\\n     --prompt \"A futuristic cityscape\"\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py \\\n     --model-path ./models/flux1-dev \\\n     --lora-paths ./lora1.safetensors ./lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py \\\n     --model-path ./models/flux1-dev \\\n     --controlnet-path ./controlnet.safetensors \\\n     --input-image ./input.png\n   ```\n\n## 6. ComfyUI Integration\n1. **Install ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git \\\n     /path/to/ComfyUI/custom_nodes/ComfyUI-nunchaku\n   ```\n\n2. **Restart ComfyUI** to load Nunchaku nodes.\n\n## 7. Custom Quantization (Optional)\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n\n2. **Quantize Custom Model**  \n   ```bash\n   python quantize.py \\\n     --model ./original_model \\\n     --output ./quantized_model \\\n     --wbits 4 --abits 4\n   ```\n\n## 8. Verification\n1. **Check Installation**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.__version__)\"\n   ```\n\n2. **Test GPU Compatibility**  \n   ```bash\n   nunchaku-check\n   ```\n\n## 9. Troubleshooting\n- For 20-series GPUs: Add `--use-turing` flag to examples\n- Low VRAM systems: Use `--cpu-offload` in examples\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for support", "issue_title": "UNABLE TO INSTALL ON ALREADY DOWNLOADED AND COMPLETLY WORKING COMFYUI", "issue_body": "I have completely working comfyui, which I use daily.\nI have asked solution on ChatGPT, but nothing is working.\n\nI am trying to install it manually, but nothing is working:\nI tried \"pip install -e .\", but is giving me error.\nI also tried to run through anaconda, but still same error.\nTried reinstalling numpy, also.\n\nThese are chatgpt instruciton I have followed::: \nhttps://chatgpt.com/share/678b5f81-9ff8-8011-98d1-f4ec9a5799ef\nhttps://chatgpt.com/share/678b5fa2-7ad4-8011-9aa3-b7dc51fd6890\nhttps://chatgpt.com/share/678b5faf-ebfc-8011-9f45-1b18cf3a3a03\n\n\nPlease provide me solution. \nThank you.\n\n![Image](https://github.com/user-attachments/assets/6e3f3108-2da1-493e-8d45-9874159556a2)\n\n![Image](https://github.com/user-attachments/assets/fe632943-5c4f-4c15-8cdc-46ec53915285)\n\n![Image](https://github.com/user-attachments/assets/b9d7071c-2753-4b4f-b527-f6e5c2344f78)\n\n![Image](https://github.com/user-attachments/assets/5643f5b6-8ce4-4670-95d6-2ad5a9cd9fed)\n\n![Image](https://github.com/user-attachments/assets/0b2c2714-3c76-41ed-b9ef-e263a43658a0)\n\nNo missing node found on comfyui manager::\n\n![Image](https://github.com/user-attachments/assets/b8f7b48f-4dea-4c46-b57b-345f00bed103)", "choices": "(A) 1. Upgrade ComfyUI-nunchaku to version v0.2.0. (B) 1. Delete the ComfyUI-nunchaku directory.  \n2. Upgrade nunchaku to version v0.2.0.  \n3. Upgrade ComfyUI-nunchaku to version v0.2.0. (C) Upgrade both nunchaku and ComfyUI-nunchaku to version v0.2.0 to resolve the shutdown issue. This version includes fixes for the problem encountered on various GPUs including RTX 3090, A100, 4090, and L20. (D) 1. Upgrade ComfyUI-nunchaku to version v0.2.0.  \n2. Upgrade nunchaku to version v0.2.0. (E) 1. Upgrade nunchaku to version v0.1.0.  \n2. Upgrade ComfyUI-nunchaku to version v0.2.0. (F) 1. Upgrade nunchaku to version v0.2.0. (G) 1. Upgrade ComfyUI-nunchaku to version v0.2.0.", "answer": "C"}
{"uuid": "af0a6cde-0f59-457f-a649-ca0af240da27", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_compatibility())\"\n   ```\n\n2. **Run a Test Script**  \n   ```bash\n   python examples/flux.1-kontext-dev.py\n   ```\n\n## 4. Basic Usage\n1. **Load a Pre-Quantized Model**  \n   ```python\n   import nunchaku\n   model = nunchaku.load_model(\"nunchaku-tech/flux.1-dev-4bit\")\n   ```\n\n2. **Run Inference**  \n   ```python\n   output = model.generate(prompt=\"A scenic landscape\")\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```python\n   model.load_lora([\"lora1.safetensors\", \"lora2.safetensors\"])\n   ```\n\n2. **ControlNet Integration**  \n   ```python\n   controlnet = nunchaku.load_controlnet(\"nunchaku-tech/controlnet-union-pro-2.0\")\n   output = model.generate(prompt=\"A cat\", controlnet=controlnet)\n   ```\n\n## 6. Custom Model Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n\n2. **Quantize Your Model**  \n   ```python\n   from deepcompressor import quantize\n   quantize(\"your_model.ckpt\", output_path=\"quantized_model.nk\", bits=4)\n   ```\n\n## 7. ComfyUI Integration\n1. **Install ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cp -r ComfyUI-nunchaku /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI** and select Nunchaku nodes in the UI.\n\n## 8. Troubleshooting\n- **Memory Issues**: Enable CPU offloading  \n  ```python\n  model.enable_cpu_offload()\n  ```\n- **Performance Tuning**: Use FP16 attention  \n  ```python\n  model.set_attention_precision(\"fp16\")\n  ```\n\n## 9. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for help.\n- Check [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html) for common issues.", "issue_title": "comfyui / svdquant custom nodes import failed", "issue_body": "hello , i followed your guide to install nunchaku then svdquant custom nodes for comfyui , Nunchaku is well installed while i can't get the nodes inside comfyui \r\ni'm using linux with a rtx3090", "choices": "(A) 1. Upgrade Microsoft Visual Studio to the latest version.\n2. Set up the ComfyUI environment with Python 3.11 and install specific versions of torch (2.4.1+cu124 or 2.5.1+cu124) using pip.\n3. Install additional required packages: diffusers, ninja, wheel, transformers, accelerate, sentencepiece, protobuf, huggingface_hub, peft, opencv-python, einops, gradio, spaces, and GPUtil.\n4. Clone the nunchaku repository and initialize/update submodules.\n5. Build and install nunchaku using the command: `python.exe -m pip install --no-build-isolation -e .` in the nunchaku directory.\n6. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub Releases for easier installation, ensuring compatibility with ComfyUI and upgrading ComfyUI-nunchaku to v0.2.0 if necessary. (B) 1. Download and install CUDA 12.6 from the official NVIDIA website.\n2. Upgrade Microsoft Visual Studio to the latest version.\n3. Set up the ComfyUI environment with Python 3.11 and install specific versions of torch (2.4.1+cu124 or 2.5.1+cu124) using pip.\n4. Install additional required packages: diffusers, ninja, wheel, transformers, accelerate, sentencepiece, protobuf, huggingface_hub, peft, opencv-python, einops, gradio, spaces, and GPUtil.\n5. Clone the nunchaku repository and initialize/update submodules.\n6. Delete the nunchaku repository.\n7. Build and install nunchaku using the command: `python.exe -m pip install --no-build-isolation -e .` in the nunchaku directory.\n8. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub Releases for easier installation, ensuring compatibility with ComfyUI and upgrading ComfyUI-nunchaku to v0.2.0 if necessary. (C) 1. Download and install CUDA 12.6 from the official NVIDIA website.\n2. Upgrade Microsoft Visual Studio to the latest version.\n3. Set up the ComfyUI environment with Python 3.11 and install specific versions of torch (2.4.1+cu124 or 2.5.1+cu124) using pip.\n4. Install additional required packages: diffusers, ninja, wheel, transformers, accelerate, sentencepiece, protobuf, huggingface_hub, peft, opencv-python, einops, gradio, spaces, and GPUtil.\n5. Clone the nunchaku repository and initialize/update submodules.\n6. Build and install nunchaku using the command: `python.exe -m pip install --no-build-isolation -e .` in the nunchaku directory.\n7. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub Releases for easier installation, ensuring compatibility with ComfyUI and upgrading ComfyUI-nunchaku to v0.2.0 if necessary. (D) 1. Download and install CUDA 12.6 from the official NVIDIA website.\n2. Set up the ComfyUI environment with Python 3.11 and install specific versions of torch (2.4.1+cu124 or 2.5.1+cu124) using pip.\n3. Upgrade Microsoft Visual Studio to the latest version.\n4. Install additional required packages: diffusers, ninja, wheel, transformers, accelerate, sentencepiece, protobuf, huggingface_hub, peft, opencv-python, einops, gradio, spaces, and GPUtil.\n5. Clone the nunchaku repository and initialize/update submodules.\n6. Build and install nunchaku using the command: `python.exe -m pip install --no-build-isolation -e .` in the nunchaku directory.\n7. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub Releases for easier installation, ensuring compatibility with ComfyUI and upgrading ComfyUI-nunchaku to v0.2.0 if necessary. (E) 1. Download and install CUDA 12.6 from the official NVIDIA website.\n2. Upgrade Microsoft Visual Studio to the latest version.\n3. Install additional required packages: diffusers, ninja, wheel, transformers, accelerate, sentencepiece, protobuf, huggingface_hub, peft, opencv-python, einops, gradio, spaces, and GPUtil.\n4. Clone the nunchaku repository and initialize/update submodules.\n5. Build and install nunchaku using the command: `python.exe -m pip install --no-build-isolation -e .` in the nunchaku directory.\n6. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub Releases for easier installation, ensuring compatibility with ComfyUI and upgrading ComfyUI-nunchaku to v0.2.0 if necessary. (F) 1. Download and install CUDA 12.6 from the official NVIDIA website.\n2. Upgrade Microsoft Visual Studio to the latest version.\n3. Set up the ComfyUI environment with Python 3.11 and install specific versions of torch (2.4.1+cu124 or 2.5.1+cu124) using pip.\n4. Clone the nunchaku repository and initialize/update submodules.\n5. Install additional required packages: diffusers, ninja, wheel, transformers, accelerate, sentencepiece, protobuf, huggingface_hub, peft, opencv-python, einops, gradio, spaces, and GPUtil.\n6. Build and install nunchaku using the command: `python.exe -m pip install --no-build-isolation -e .` in the nunchaku directory.\n7. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub Releases for easier installation, ensuring compatibility with ComfyUI and upgrading ComfyUI-nunchaku to v0.2.0 if necessary. (G) 1. Download and install CUDA 12.6 from the official NVIDIA website.\n2. Upgrade Microsoft Visual Studio to the latest version.\n3. Set up the ComfyUI environment with Python 3.11 and install specific versions of torch (2.4.1+cu124 or 2.5.1+cu124) using pip.\n4. Uninstall Python 3.11.\n5. Install additional required packages: diffusers, ninja, wheel, transformers, accelerate, sentencepiece, protobuf, huggingface_hub, peft, opencv-python, einops, gradio, spaces, and GPUtil.\n6. Clone the nunchaku repository and initialize/update submodules.\n7. Build and install nunchaku using the command: `python.exe -m pip install --no-build-isolation -e .` in the nunchaku directory.\n8. Alternatively, use prebuilt wheels from Hugging Face, ModelScope, or GitHub Releases for easier installation, ensuring compatibility with ComfyUI and upgrading ComfyUI-nunchaku to v0.2.0 if necessary.", "answer": "C"}
{"uuid": "eb5c4d23-3ac7-4518-af17-12b138cbec31", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.7+, PyTorch 2.1+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   Visit [Hugging Face Hub](https://huggingface.co/nunchaku-tech) or [ModelScope](https://modelscope.cn/organization/nunchaku-tech) to download models. Example:\n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux1-dev\n   ```\n\n2. **Verify Model Compatibility**  \n   Ensure the model version matches Nunchaku's supported versions (e.g., FLUX.1-Kontext).\n\n## 4. Basic Usage\n1. **Run Inference Script**  \n   Example for FLUX.1-Kontext:\n   ```bash\n   python examples/flux.1-kontext-dev.py --model-path ./nunchaku-flux1-dev\n   ```\n\n2. **Enable Advanced Features**  \n   - **Multi-LoRA**: Use `--lora-weights lora1.safetensors,lora2.safetensors`  \n   - **ControlNet**: Add `--controlnet controlnet-union-pro-2.0`  \n\n## 5. Performance Optimization\n1. **Enable FP16 Attention**  \n   Add `--use-fp16-attention` to the inference command.\n\n2. **Activate First-Block Cache**  \n   ```bash\n   python examples/flux.1-dev-double_cache.py --enable-fb-cache\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI**  \n   Ensure the plugin appears in the node list.\n\n## 7. Custom Model Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n\n2. **Quantize Your Model**  \n   Follow the [DeepCompressor guide](https://github.com/nunchaku-tech/deepcompressor#usage).\n\n## 8. Troubleshooting\n- **CUDA Errors**: Verify CUDA version with `nvcc --version`  \n- **Memory Issues**: Enable per-layer CPU offloading with `--cpu-offload`  \n- **Community Support**: Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for help.\n\n## 9. Documentation\n- Explore full docs at [nunchaku.tech](https://nunchaku.tech/docs/nunchaku/)  \n- Check [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html) for common issues.", "issue_title": "tried 2 install 3 times on ubuntu always end with errors", "issue_body": "`here is the message, looking for using it with comfyui \r\n\r\n\r\nBuilding wheels for collected packages: nunchaku\r\n  Building editable for nunchaku (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  × Building editable for nunchaku (pyproject.toml) did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [325 lines of output]\r\n      /tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\r\n        cpu = _conversion_method_template(device=torch.device(\"cpu\"))\r\n      /tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/dist.py:330: InformationOnly: Normalizing '0.0.2beta0' to '0.0.2b0'\r\n        self.metadata.version = self._normalize_version(self.metadata.version)\r\n      running editable_wheel\r\n      creating /tmp/pip-wheel-hu865_m8/.tmp-b16zse7t/nunchaku.egg-info\r\n      writing /tmp/pip-wheel-hu865_m8/.tmp-b16zse7t/nunchaku.egg-info/PKG-INFO\r\n      writing dependency_links to /tmp/pip-wheel-hu865_m8/.tmp-b16zse7t/nunchaku.egg-info/dependency_links.txt\r\n      writing requirements to /tmp/pip-wheel-hu865_m8/.tmp-b16zse7t/nunchaku.egg-info/requires.txt\r\n      writing top-level names to /tmp/pip-wheel-hu865_m8/.tmp-b16zse7t/nunchaku.egg-info/top_level.txt\r\n      writing manifest file '/tmp/pip-wheel-hu865_m8/.tmp-b16zse7t/nunchaku.egg-info/SOURCES.txt'\r\n      reading manifest file '/tmp/pip-wheel-hu865_m8/.tmp-b16zse7t/nunchaku.egg-info/SOURCES.txt'\r\n      reading manifest template 'MANIFEST.in'\r\n      warning: no files found matching '*.hpp' under directory 'src'\r\n      warning: no files found matching '*.ipp' under directory 'src'\r\n      warning: no files found matching '*.hpp' under directory 'nunchaku/csrc'\r\n      warning: no files found matching '*.ipp' under directory 'nunchaku/csrc'\r\n      warning: no files found matching '*.cu' under directory 'nunchaku/csrc'\r\n      warning: no files found matching '*.cuh' under directory 'nunchaku/csrc'\r\n      warning: no files found matching '*.hpp' under directory 'third_party/Block-Sparse-Attention/csrc/block_sparse_attn'\r\n      warning: no files found matching '*.ipp' under directory 'third_party/Block-Sparse-Attention/csrc/block_sparse_attn'\r\n      warning: no files found matching '*.cpp' under directory 'third_party/cutlass/include'\r\n      warning: no files found matching '*.ipp' under directory 'third_party/cutlass/include'\r\n      warning: no files found matching '*.cu' under directory 'third_party/cutlass/include'\r\n      warning: no files found matching '*.cuh' under directory 'third_party/cutlass/include'\r\n      warning: no files found matching '*.cpp' under directory 'third_party/json/include'\r\n      warning: no files found matching '*.h' under directory 'third_party/json/include'\r\n      warning: no files found matching '*.ipp' under directory 'third_party/json/include'\r\n      warning: no files found matching '*.cpp' under directory 'third_party/mio/include'\r\n      warning: no files found matching '*.h' under directory 'third_party/mio/include'\r\n      warning: no files found matching '*.cpp' under directory 'third_party/spdlog/include'\r\n      warning: no files found matching '*.hpp' under directory 'third_party/spdlog/include'\r\n      warning: no files found matching '*.ipp' under directory 'third_party/spdlog/include'\r\n      adding license file 'LICENCE.txt'\r\n      writing manifest file '/tmp/pip-wheel-hu865_m8/.tmp-b16zse7t/nunchaku.egg-info/SOURCES.txt'\r\n      creating '/tmp/pip-wheel-hu865_m8/.tmp-b16zse7t/nunchaku-0.0.2b0.dist-info'\r\n      creating /tmp/pip-wheel-hu865_m8/.tmp-b16zse7t/nunchaku-0.0.2b0.dist-info/WHEEL\r\n      running build_py\r\n      running build_ext\r\n      /tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.0) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\r\n        warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\r\n      /tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no g++ version bounds defined for CUDA version 12.0\r\n        warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\r\n      building 'nunchaku._C' extension\r\n      creating /tmp/tmprtqst5pb.build-temp/nunchaku/csrc\r\n      creating /tmp/tmprtqst5pb.build-temp/src\r\n      creating /tmp/tmprtqst5pb.build-temp/src/interop\r\n      creating /tmp/tmprtqst5pb.build-temp/src/kernels\r\n      creating /tmp/tmprtqst5pb.build-temp/src/kernels/awq\r\n      creating /tmp/tmprtqst5pb.build-temp/third_party/Block-Sparse-Attention/csrc/block_sparse_attn\r\n      creating /tmp/tmprtqst5pb.build-temp/third_party/Block-Sparse-Attention/csrc/block_sparse_attn/src\r\n      Emitting ninja build file /tmp/tmprtqst5pb.build-temp/build.ninja...\r\n      Compiling objects...\r\n      Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n      [1/24] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/activation_kernels.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/activation_kernels.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/activation_kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      FAILED: /tmp/tmprtqst5pb.build-temp/src/kernels/activation_kernels.o\r\n      /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/activation_kernels.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/activation_kernels.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/activation_kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      /home/theduc/nunchaku/third_party/spdlog/include/spdlog/fmt/bundled/format.h(4449): error: a literal operator template must have a template parameter list equivalent to \"<char ...>\"\r\n      \r\n      /home/theduc/nunchaku/src/common.h(41): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      /home/theduc/nunchaku/src/common.h(48): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      3 errors detected in the compilation of \"/home/theduc/nunchaku/src/kernels/activation_kernels.cu\".\r\n      [2/24] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/misc_kernels.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/misc_kernels.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/misc_kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      FAILED: /tmp/tmprtqst5pb.build-temp/src/kernels/misc_kernels.o\r\n      /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/misc_kernels.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/misc_kernels.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/misc_kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      /home/theduc/nunchaku/third_party/spdlog/include/spdlog/fmt/bundled/format.h(4449): error: a literal operator template must have a template parameter list equivalent to \"<char ...>\"\r\n      \r\n      /home/theduc/nunchaku/src/common.h(41): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      /home/theduc/nunchaku/src/common.h(48): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      3 errors detected in the compilation of \"/home/theduc/nunchaku/src/kernels/misc_kernels.cu\".\r\n      [3/24] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/layernorm_kernels.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/layernorm_kernels.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/layernorm_kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      FAILED: /tmp/tmprtqst5pb.build-temp/src/kernels/layernorm_kernels.o\r\n      /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/layernorm_kernels.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/layernorm_kernels.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/layernorm_kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      /home/theduc/nunchaku/third_party/spdlog/include/spdlog/fmt/bundled/format.h(4449): error: a literal operator template must have a template parameter list equivalent to \"<char ...>\"\r\n      \r\n      /home/theduc/nunchaku/src/common.h(41): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      /home/theduc/nunchaku/src/common.h(48): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      3 errors detected in the compilation of \"/home/theduc/nunchaku/src/kernels/layernorm_kernels.cu\".\r\n      [4/24] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/awq/gemv_awq.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/awq/gemv_awq.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/awq/gemv_awq.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      FAILED: /tmp/tmprtqst5pb.build-temp/src/kernels/awq/gemv_awq.o\r\n      /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/awq/gemv_awq.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/awq/gemv_awq.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/awq/gemv_awq.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      /home/theduc/nunchaku/third_party/spdlog/include/spdlog/fmt/bundled/format.h(4449): error: a literal operator template must have a template parameter list equivalent to \"<char ...>\"\r\n      \r\n      /home/theduc/nunchaku/src/common.h(41): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      /home/theduc/nunchaku/src/common.h(48): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      3 errors detected in the compilation of \"/home/theduc/nunchaku/src/kernels/awq/gemv_awq.cu\".\r\n      [5/24] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_f16.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/gemm_f16.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_f16.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      FAILED: /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_f16.o\r\n      /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_f16.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/gemm_f16.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_f16.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      /home/theduc/nunchaku/third_party/spdlog/include/spdlog/fmt/bundled/format.h(4449): error: a literal operator template must have a template parameter list equivalent to \"<char ...>\"\r\n      \r\n      /home/theduc/nunchaku/src/common.h(41): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      /home/theduc/nunchaku/src/common.h(48): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      3 errors detected in the compilation of \"/home/theduc/nunchaku/src/kernels/gemm_f16.cu\".\r\n      [6/24] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_batched.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/gemm_batched.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_batched.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      FAILED: /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_batched.o\r\n      /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_batched.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/gemm_batched.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_batched.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      /home/theduc/nunchaku/third_party/spdlog/include/spdlog/fmt/bundled/format.h(4449): error: a literal operator template must have a template parameter list equivalent to \"<char ...>\"\r\n      \r\n      /home/theduc/nunchaku/src/common.h(41): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      /home/theduc/nunchaku/src/common.h(48): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      3 errors detected in the compilation of \"/home/theduc/nunchaku/src/kernels/gemm_batched.cu\".\r\n      [7/24] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_w4a4.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/gemm_w4a4.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_w4a4.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      FAILED: /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_w4a4.o\r\n      /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_w4a4.o.d -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/kernels/gemm_w4a4.cu -o /tmp/tmprtqst5pb.build-temp/src/kernels/gemm_w4a4.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -g -std=c++20 -UNDEBUG -Xcudafe --diag_suppress=20208 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_HALF2_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --threads=2 --expt-relaxed-constexpr --expt-extended-lambda --generate-line-info --ptxas-options=--allow-expensive-optimizations=true -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      /home/theduc/nunchaku/third_party/spdlog/include/spdlog/fmt/bundled/format.h(4449): error: a literal operator template must have a template parameter list equivalent to \"<char ...>\"\r\n      \r\n      /home/theduc/nunchaku/src/common.h(41): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      /home/theduc/nunchaku/src/common.h(48): error: call to consteval function \"std::source_location::current\" did not produce a valid constant expression\r\n      /usr/include/c++/12/source_location(59): note #2703-D: cannot call non-constexpr function \"__builtin_source_location\" (declared implicitly)\r\n      \r\n      /home/theduc/nunchaku/src/kernels/gemm_w4a4.cu(3063): warning #549-D: variable \"epilogueArgs\" is used before its value is set\r\n      \r\n      Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\r\n      \r\n      3 errors detected in the compilation of \"/home/theduc/nunchaku/src/kernels/gemm_w4a4.cu\".\r\n      [8/24] c++ -MMD -MF /tmp/tmprtqst5pb.build-temp/src/layernorm.o.d -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O3 -Wall -fPIC -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/layernorm.cpp -o /tmp/tmprtqst5pb.build-temp/src/layernorm.o -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -fvisibility=hidden -g -std=c++20 -UNDEBUG -Og -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      [9/24] c++ -MMD -MF /tmp/tmprtqst5pb.build-temp/src/activation.o.d -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O3 -Wall -fPIC -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/activation.cpp -o /tmp/tmprtqst5pb.build-temp/src/activation.o -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -fvisibility=hidden -g -std=c++20 -UNDEBUG -Og -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      [10/24] c++ -MMD -MF /tmp/tmprtqst5pb.build-temp/src/Linear.o.d -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O3 -Wall -fPIC -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/Linear.cpp -o /tmp/tmprtqst5pb.build-temp/src/Linear.o -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -fvisibility=hidden -g -std=c++20 -UNDEBUG -Og -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      [11/24] c++ -MMD -MF /tmp/tmprtqst5pb.build-temp/src/FluxModel.o.d -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O3 -Wall -fPIC -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/FluxModel.cpp -o /tmp/tmprtqst5pb.build-temp/src/FluxModel.o -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -fvisibility=hidden -g -std=c++20 -UNDEBUG -Og -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      [12/24] c++ -MMD -MF /tmp/tmprtqst5pb.build-temp/src/Serialization.o.d -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O3 -Wall -fPIC -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/Serialization.cpp -o /tmp/tmprtqst5pb.build-temp/src/Serialization.o -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -fvisibility=hidden -g -std=c++20 -UNDEBUG -Og -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      [13/24] c++ -MMD -MF /tmp/tmprtqst5pb.build-temp/src/interop/torch.o.d -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O3 -Wall -fPIC -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/src/interop/torch.cpp -o /tmp/tmprtqst5pb.build-temp/src/interop/torch.o -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -fvisibility=hidden -g -std=c++20 -UNDEBUG -Og -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      [14/24] c++ -MMD -MF /tmp/tmprtqst5pb.build-temp/nunchaku/csrc/pybind.o.d -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O3 -Wall -fPIC -I/home/theduc/nunchaku/src -I/home/theduc/nunchaku/third_party/cutlass/include -I/home/theduc/nunchaku/third_party/json/include -I/home/theduc/nunchaku/third_party/mio/include -I/home/theduc/nunchaku/third_party/spdlog/include -I/home/theduc/nunchaku/third_party/Block-Sparse-Attention/csrc/block_sparse_attn -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/TH -I/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/include/THC -I/home/theduc/.pyenv/versions/3.12.8/include/python3.12 -c -c /home/theduc/nunchaku/nunchaku/csrc/pybind.cpp -o /tmp/tmprtqst5pb.build-temp/nunchaku/csrc/pybind.o -DENABLE_BF16=1 -DBUILD_NUNCHAKU=1 -fvisibility=hidden -g -std=c++20 -UNDEBUG -Og -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n      In file included from /usr/include/c++/13/cassert:44,\r\n                       from /home/theduc/nunchaku/third_party/spdlog/include/spdlog/details/circular_q.h:7,\r\n                       from /home/theduc/nunchaku/third_party/spdlog/include/spdlog/details/backtracer.h:6,\r\n                       from /home/theduc/nunchaku/third_party/spdlog/include/spdlog/logger.h:18,\r\n                       from /home/theduc/nunchaku/third_party/spdlog/include/spdlog/details/registry-inl.h:12,\r\n                       from /home/theduc/nunchaku/third_party/spdlog/include/spdlog/details/registry.h:128,\r\n                       from /home/theduc/nunchaku/third_party/spdlog/include/spdlog/spdlog.h:13,\r\n                       from /home/theduc/nunchaku/src/common.h:23,\r\n                       from /home/theduc/nunchaku/src/interop/torch.h:5,\r\n                       from /home/theduc/nunchaku/nunchaku/csrc/gemm.h:3,\r\n                       from /home/theduc/nunchaku/nunchaku/csrc/pybind.cpp:1:\r\n      /home/theduc/nunchaku/nunchaku/csrc/gemm.h: In member function ‘std::string QuantizedGEMM::dumpTensorINT4(Tensor)’:\r\n      /home/theduc/nunchaku/nunchaku/csrc/gemm.h:89:43: warning: comparison of integer expressions of different signedness: ‘int’ and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]\r\n         89 |                         assert(offset + i < x.numel() / 4);\r\n            |                                ~~~~~~~~~~~^~~~~~~~~~~~~~~\r\n      In file included from /home/theduc/nunchaku/nunchaku/csrc/pybind.cpp:2:\r\n      /home/theduc/nunchaku/nunchaku/csrc/flux.h: In lambda function:\r\n      /home/theduc/nunchaku/nunchaku/csrc/flux.h:181:48: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<float, std::allocator<float> >::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\r\n        181 |                 for (int i = skipRanks / 16; i < m->lora_scales.size(); i++) {\r\n            |                                              ~~^~~~~~~~~~~~~~~~~~~~~~~\r\n      ninja: build stopped: subcommand failed.\r\n      Traceback (most recent call last):\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2104, in _run_ninja_build\r\n          subprocess.run(\r\n        File \"/home/theduc/.pyenv/versions/3.12.8/lib/python3.12/subprocess.py\", line 571, in run\r\n          raise CalledProcessError(retcode, process.args,\r\n      subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\r\n      \r\n      The above exception was the direct cause of the following exception:\r\n      \r\n      Traceback (most recent call last):\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 139, in run\r\n          self._create_wheel_file(bdist_wheel)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 340, in _create_wheel_file\r\n          files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 263, in _run_build_commands\r\n          self._run_build_subcommands()\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 290, in _run_build_subcommands\r\n          self.run_command(name)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 995, in run_command\r\n          super().run_command(command)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\r\n          cmd_obj.run()\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 99, in run\r\n          _build_ext.run(self)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\r\n          self.build_extensions()\r\n        File \"<string>\", line 18, in build_extensions\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 868, in build_extensions\r\n          build_ext.build_extensions(self)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 476, in build_extensions\r\n          self._build_extensions_serial()\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 502, in _build_extensions_serial\r\n          self.build_extension(ext)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 264, in build_extension\r\n          _build_ext.build_extension(self, ext)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 557, in build_extension\r\n          objects = self.compiler.compile(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 681, in unix_wrap_ninja_compile\r\n          _write_ninja_file_and_compile_objects(\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1784, in _write_ninja_file_and_compile_objects\r\n          _run_ninja_build(\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2120, in _run_ninja_build\r\n          raise RuntimeError(message) from e\r\n      RuntimeError: Error compiling objects for extension\r\n      /tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py:973: _DebuggingTips: Problem in editable installation.\r\n      !!\r\n      \r\n              ********************************************************************************\r\n              An error happened while installing `nunchaku` in editable mode.\r\n      \r\n              The following steps are recommended to help debug this problem:\r\n      \r\n              - Try to install the project normally, without using the editable mode.\r\n                Does the error still persist?\r\n                (If it does, try fixing the problem before attempting the editable mode).\r\n              - If you are using binary extensions, make sure you have all OS-level\r\n                dependencies installed (e.g. compilers, toolchains, binary libraries, ...).\r\n              - Try the latest version of setuptools (maybe the error was already fixed).\r\n              - If you (or your project dependencies) are using any setuptools extension\r\n                or customization, make sure they support the editable mode.\r\n      \r\n              After following the steps above, if the problem still persists and\r\n              you think this is related to how setuptools handles editable installations,\r\n              please submit a reproducible example\r\n              (see https://stackoverflow.com/help/minimal-reproducible-example) to:\r\n      \r\n                  https://github.com/pypa/setuptools/issues\r\n      \r\n              See https://setuptools.pypa.io/en/latest/userguide/development_mode.html for details.\r\n              ********************************************************************************\r\n      \r\n      !!\r\n        cmd_obj.run()\r\n      Traceback (most recent call last):\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2104, in _run_ninja_build\r\n          subprocess.run(\r\n        File \"/home/theduc/.pyenv/versions/3.12.8/lib/python3.12/subprocess.py\", line 571, in run\r\n          raise CalledProcessError(retcode, process.args,\r\n      subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\r\n      \r\n      The above exception was the direct cause of the following exception:\r\n      \r\n      Traceback (most recent call last):\r\n        File \"/home/theduc/.pyenv/versions/3.12.8/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"/home/theduc/.pyenv/versions/3.12.8/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/home/theduc/.pyenv/versions/3.12.8/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 273, in build_editable\r\n          return hook(wheel_directory, config_settings, metadata_directory)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 476, in build_editable\r\n          return self._build_with_temp_dir(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 407, in _build_with_temp_dir\r\n          self.run_setup()\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 320, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 115, in <module>\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/__init__.py\", line 117, in setup\r\n          return distutils.core.setup(**attrs)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 183, in setup\r\n          return run_commands(dist)\r\n                 ^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 199, in run_commands\r\n          dist.run_commands()\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\r\n          self.run_command(cmd)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 995, in run_command\r\n          super().run_command(command)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\r\n          cmd_obj.run()\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 139, in run\r\n          self._create_wheel_file(bdist_wheel)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 340, in _create_wheel_file\r\n          files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 263, in _run_build_commands\r\n          self._run_build_subcommands()\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/editable_wheel.py\", line 290, in _run_build_subcommands\r\n          self.run_command(name)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 995, in run_command\r\n          super().run_command(command)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\r\n          cmd_obj.run()\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 99, in run\r\n          _build_ext.run(self)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\r\n          self.build_extensions()\r\n        File \"<string>\", line 18, in build_extensions\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 868, in build_extensions\r\n          build_ext.build_extensions(self)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 476, in build_extensions\r\n          self._build_extensions_serial()\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 502, in _build_extensions_serial\r\n          self.build_extension(ext)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 264, in build_extension\r\n          _build_ext.build_extension(self, ext)\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 557, in build_extension\r\n          objects = self.compiler.compile(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 681, in unix_wrap_ninja_compile\r\n          _write_ninja_file_and_compile_objects(\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1784, in _write_ninja_file_and_compile_objects\r\n          _run_ninja_build(\r\n        File \"/tmp/pip-build-env-o540kg68/overlay/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2120, in _run_ninja_build\r\n          raise RuntimeError(message) from e\r\n      RuntimeError: Error compiling objects for extension\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building editable for nunchaku\r\nFailed to build nunchaku\r\nERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (nunchaku)`", "choices": "(A) 1. Download the required model files (`config.json`, `unquantized_layers.safetensors`, and `svdq-int4-flux.1-dev.safetensors`) from the Huggingface repository (https://huggingface.co/mit-han-lab/svdq-int4-flux.1-dev).  \n2. Rename the main model file (`svdq-int4-flux.1-dev.safetensors`) to `transformer_blocks.safetensors`.  \n3. Place all three files in the same folder within your ComfyUI models directory.  \n4. Update the `nodes.py` file in the `custom_nodes/ComfyUI-SVDQuant/` directory to support local model loading by modifying the `model_path` input type to a custom string (Option a) or hardcoding the local model path (Option b).  \n5. Restart ComfyUI to apply the changes and test the local model loading functionality. (B) 1. Download the required model files (`config.json`, `unquantized_layers.safetensors`, and `svdq-int4-flux.1-dev.safetensors`) from the Huggingface repository (https://huggingface.co/mit-han-lab/svdq-int4-flux.1-dev).  \n2. Place all three files in the same folder within your ComfyUI models directory.  \n3. Update the `nodes.py` file in the `custom_nodes/ComfyUI-SVDQuant/` directory to support local model loading by modifying the `model_path` input type to a custom string (Option a) or hardcoding the local model path (Option b).  \n4. Ensure the model folder name includes the model type (`schnell` or `dev`) for proper recognition.  \n5. Restart ComfyUI to apply the changes and test the local model loading functionality. (C) 1. Download the required model files (`config.json`, `unquantized_layers.safetensors`, and `svdq-int4-flux.1-dev.safetensors`) from the Huggingface repository (https://huggingface.co/mit-han-lab/svdq-int4-flux.1-dev).  \n2. Rename the main model file (`svdq-int4-flux.1-dev.safetensors`) to `transformer_blocks.safetensors`.  \n3. Convert the `unquantized_layers.safetensors` file to `.bin` format.  \n4. Place all three files in the same folder within your ComfyUI models directory.  \n5. Update the `nodes.py` file in the `custom_nodes/ComfyUI-SVDQuant/` directory to support local model loading by modifying the `model_path` input type to a custom string (Option a) or hardcoding the local model path (Option b).  \n6. Ensure the model folder name includes the model type (`schnell` or `dev`) for proper recognition.  \n7. Restart ComfyUI to apply the changes and test the local model loading functionality. (D) 1. Download the required model files (`config.json`, `unquantized_layers.safetensors`, and `svdq-int4-flux.1-dev.safetensors`) from the Huggingface repository (https://huggingface.co/mit-han-lab/svdq-int4-flux.1-dev).  \n2. Place all three files in the same folder within your ComfyUI models directory.  \n3. Rename the main model file (`svdq-int4-flux.1-dev.safetensors`) to `transformer_blocks.safetensors`.  \n4. Update the `nodes.py` file in the `custom_nodes/ComfyUI-SVDQuant/` directory to support local model loading by modifying the `model_path` input type to a custom string (Option a) or hardcoding the local model path (Option b).  \n5. Ensure the model folder name includes the model type (`schnell` or `dev`) for proper recognition.  \n6. Restart ComfyUI to apply the changes and test the local model loading functionality. (E) 1. Update the `nodes.py` file in the `custom_nodes/ComfyUI-SVDQuant/` directory to support local model loading by modifying the `model_path` input type to a custom string (Option a) or hardcoding the local model path (Option b).  \n2. Download the required model files (`config.json`, `unquantized_layers.safetensors`, and `svdq-int4-flux.1-dev.safetensors`) from the Huggingface repository (https://huggingface.co/mit-han-lab/svdq-int4-flux.1-dev).  \n3. Rename the main model file (`svdq-int4-flux.1-dev.safetensors`) to `transformer_blocks.safetensors`.  \n4. Place all three files in the same folder within your ComfyUI models directory.  \n5. Ensure the model folder name includes the model type (`schnell` or `dev`) for proper recognition.  \n6. Restart ComfyUI to apply the changes and test the local model loading functionality. (F) 1. Download the required model files (`config.json`, `unquantized_layers.safetensors`, and `svdq-int4-flux.1-dev.safetensors`) from the Huggingface repository (https://huggingface.co/mit-han-lab/svdq-int4-flux.1-dev).  \n2. Rename the main model file (`svdq-int4-flux.1-dev.safetensors`) to `transformer_blocks.safetensors`.  \n3. Place all three files in the same folder within your ComfyUI models directory.  \n4. Delete the `config.json` file from the folder.  \n5. Update the `nodes.py` file in the `custom_nodes/ComfyUI-SVDQuant/` directory to support local model loading by modifying the `model_path` input type to a custom string (Option a) or hardcoding the local model path (Option b).  \n6. Ensure the model folder name includes the model type (`schnell` or `dev`) for proper recognition.  \n7. Restart ComfyUI to apply the changes and test the local model loading functionality. (G) 1. Download the required model files (`config.json`, `unquantized_layers.safetensors`, and `svdq-int4-flux.1-dev.safetensors`) from the Huggingface repository (https://huggingface.co/mit-han-lab/svdq-int4-flux.1-dev).\n2. Rename the main model file (`svdq-int4-flux.1-dev.safetensors`) to `transformer_blocks.safetensors`.\n3. Place all three files in the same folder within your ComfyUI models directory.\n4. Update the `nodes.py` file in the `custom_nodes/ComfyUI-SVDQuant/` directory to support local model loading by modifying the `model_path` input type to a custom string (Option a) or hardcoding the local model path (Option b).\n5. Ensure the model folder name includes the model type (`schnell` or `dev`) for proper recognition.\n6. Restart ComfyUI to apply the changes and test the local model loading functionality.", "answer": "G"}
{"uuid": "aefa2b89-c85a-4601-ab04-1298effa3a78", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- Ensure NVIDIA GPU with CUDA support (RTX 4090/5090 recommended)\n- Python 3.8+ environment\n- Linux OS (Ubuntu 22.04 recommended)\n\n## 2. Installation\n```bash\n# Create conda environment (recommended)\nconda create -n nunchaku python=3.10\nconda activate nunchaku\n\n# Install PyTorch with CUDA support\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Install Nunchaku core package\npip install nunchaku-engine\n```\n\n## 3. Model Setup\n```bash\n# Download example models from Hugging Face\ngit lfs install\ngit clone https://huggingface.co/nunchaku-tech/nunchaku-t5\ngit clone https://huggingface.co/nunchaku-tech/FLUX.1-dev-4bit\n```\n\n## 4. Basic Inference Test\n```bash\n# Run example script with 4-bit FLUX.1 model\npython examples/flux.1-dev-basic.py \\\n    --model-path ./FLUX.1-dev-4bit \\\n    --t5-path ./nunchaku-t5 \\\n    --prompt \"A beautiful sunset over mountains\"\n```\n\n## 5. Advanced Features\n```bash\n# Multi-LoRA example\npython examples/flux.1-dev-multiple-lora.py \\\n    --model-path ./FLUX.1-dev-4bit \\\n    --lora-weights ./path/to/lora1.safetensors ./path/to/lora2.safetensors\n\n# ControlNet example\npython examples/flux.1-dev-controlnet-union-pro.py \\\n    --model-path ./FLUX.1-dev-4bit \\\n    --controlnet-path ./path/to/controlnet \\\n    --input-image ./input.png\n```\n\n## 6. ComfyUI Integration (Optional)\n```bash\n# Install ComfyUI plugin\ngit clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\ncd ComfyUI-nunchaku\npip install -r requirements.txt\n\n# Follow ComfyUI setup instructions at:\n# https://github.com/nunchaku-tech/ComfyUI-nunchaku#installation\n```\n\n## 7. Verification\n- Check output images in `./outputs` directory\n- Monitor GPU utilization with `nvidia-smi`\n- Refer to [troubleshooting guide](https://nunchaku.tech/docs/nunchaku/faq/faq.html) for issues\n\n## 8. Next Steps\n- Explore [Gradio demos](https://github.com/nunchaku-tech/nunchaku/tree/main/app)\n- Learn [custom quantization](https://github.com/nunchaku-tech/deepcompressor)\n- Join [community channels](https://github.com/nunchaku-tech/nunchaku/issues/149) for support", "issue_title": "run the script the pc will shut down", "issue_body": "1. run the program and when string \"Done\" apperance  the pc shutdown\r\n2. who can help to solve the problem.", "choices": "(A) 1. Download the pre-built .whl file from the provided Hugging Face link: https://huggingface.co/hdfhssg/torch-2.6.0-cu128/tree/main.\n2. Delete the downloaded .whl file.\n3. Install the downloaded .whl file using pip.\n4. Verify the installation by running the example script again. The issue should be resolved. (B) 1. Download the pre-built .whl file from the provided Hugging Face link: https://huggingface.co/hdfhssg/torch-2.6.0-cu128/tree/main.\n2. Install the downloaded .whl file using pip. (C) 1. Download the pre-built .whl file from the provided Hugging Face link: https://huggingface.co/hdfhssg/torch-2.6.0-cu128/tree/main.\n2. Install the downloaded .whl file using pip.\n3. Verify the installation by running the example script again. The issue should be resolved. (D) 1. Install the downloaded .whl file using pip.\n2. Download the pre-built .whl file from the provided Hugging Face link: https://huggingface.co/hdfhssg/torch-2.6.0-cu128/tree/main.\n3. Verify the installation by running the example script again. The issue should be resolved. (E) 1. Download the pre-built .whl file from the provided Hugging Face link: https://huggingface.co/hdfhssg/torch-2.6.0-cu128/tree/main.\n2. Install the downloaded .whl file using pip.\n3. Uninstall the package using pip.\n4. Verify the installation by running the example script again. The issue should be resolved. (F) 1. Download the pre-built .whl file from the provided Hugging Face link: https://huggingface.co/hdfhssg/torch-2.6.0-cu128/tree/main.\n2. Verify the installation by running the example script again. The issue should be resolved.\n3. Install the downloaded .whl file using pip. (G) 1. Download the pre-built .whl file from the provided Hugging Face link: https://huggingface.co/hdfhssg/torch-2.6.0-cu128/tree/main.\n2. Verify the installation by running the example script again. The issue should be resolved.", "answer": "C"}
{"uuid": "8759ab80-3ba8-4165-b293-5a18d44d9125", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: \n  - Python 3.8+\n  - CUDA 12.1+ (for GPU acceleration)\n  - PyTorch 2.3+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_compatibility())\"\n   ```\n\n2. **Run Basic Test**  \n   ```bash\n   python examples/flux.1-dev-basic.py\n   ```\n\n## 4. Model Quantization (Optional)\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n\n2. **Quantize Custom Model**  \n   ```bash\n   python quantize.py --model_path /path/to/model --output_dir quantized_model\n   ```\n\n## 5. Run Inference\n1. **FLUX.1 Example**  \n   ```bash\n   python examples/flux.1-kontext-dev.py\n   ```\n\n2. **ControlNet Example**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI**  \n   Ensure the plugin appears in the ComfyUI interface.\n\n## 7. Advanced Features\n1. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   ```\n\n2. **Low-Memory Mode**  \n   ```bash\n   python examples/flux.1-dev-low-memory.py --cpu_offload\n   ```\n\n## 8. Monitoring & Optimization\n1. **Profile Performance**  \n   ```bash\n   nvprof python examples/flux.1-dev-basic.py\n   ```\n\n2. **Enable FP16 Attention**  \n   Add `--use_fp16_attention` flag to inference scripts.\n\n## 9. Troubleshooting\n- **Common Issues**:  \n  - CUDA version mismatch: Reinstall PyTorch with correct CUDA version.  \n  - Out-of-memory: Use `--cpu_offload` or reduce batch size.  \n- **Community Support**:  \n  - [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q)  \n  - [Discord](https://discord.gg/Wk6PnwX9Sm)\n\n## 10. Updates\n- Regularly check for new releases:  \n  ```bash\n   git pull origin main\n   pip install --upgrade .\n   ```", "issue_title": "Support loading local svdquant model in ComfyUI", "issue_body": "Hi @lmxyy \r\n\r\nI finally manage to install your nunchanku node on windows however when I try to run your example workflow, it need to download the whole diffuser into huggingface cache in my pc which, however I get error after downloading finished. I already download your full svdq-int4-flux.1-dev.safetensors and put it into model location in comfyui but don't know how to make your node loading it. Can you support us. Thank you\r\n\r\n![image](https://github.com/user-attachments/assets/4362ff8f-1ac7-4aec-a629-880882f096cb)\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/a001caa0-1243-4894-a5dd-28069dc3415a)", "choices": "(A) 1. Check the updated installation guide for prebuilt wheels for Linux at [this link](https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#wheels-linux-only-for-now).\n2. Windows users should wait for the upcoming Windows wheels as mentioned by the maintainer. (B) 1. Check the updated installation guide for prebuilt wheels for Linux at [this link](https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#wheels-linux-only-for-now).\n2. Download and install outdated wheels from an unofficial source to speed up the process.\n3. For ComfyUI installation, follow the updated guide which now supports the ComfyUI manager at [this link](https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#installation).\n4. Windows users should wait for the upcoming Windows wheels as mentioned by the maintainer. (C) 1. Windows users should wait for the upcoming Windows wheels as mentioned by the maintainer.\n2. Check the updated installation guide for prebuilt wheels for Linux at [this link](https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#wheels-linux-only-for-now).\n3. For ComfyUI installation, follow the updated guide which now supports the ComfyUI manager at [this link](https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#installation). (D) 1. Check the updated installation guide for prebuilt wheels for Linux at [this link](https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#wheels-linux-only-for-now).\n2. For ComfyUI installation, follow the updated guide which now supports the ComfyUI manager at [this link](https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#installation).\n3. Windows users should wait for the upcoming Windows wheels as mentioned by the maintainer.", "answer": "D"}
{"uuid": "f8539ee1-78d2-46e3-94c2-6e1b6fe834ca", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Basic Usage\n1. **Download Pre-Quantized Models**  \n   Example for FLUX.1:\n   ```bash\n   wget https://huggingface.co/nunchaku-tech/nunchaku-flux1/resolve/main/flux1-4bit.safetensors\n   ```\n\n2. **Run Inference**  \n   Use the example script:\n   ```bash\n   python examples/flux.1-dev.py --model-path flux1-4bit.safetensors --prompt \"A cat wearing sunglasses\"\n   ```\n\n## 4. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model-path flux1-4bit.safetensors --lora-paths lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --model-path flux1-4bit.safetensors --controlnet canny\n   ```\n\n## 5. Custom Model Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor\n   pip install .\n   ```\n\n2. **Quantize Your Model**  \n   ```bash\n   python quantize.py --model-path your_model.safetensors --output-path quantized_model.safetensors\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/ComfyUI-nunchaku\n   ```\n\n2. **Restart ComfyUI**  \n   Nunchaku nodes will appear in the UI.\n\n## 7. Troubleshooting\n- For GPU compatibility issues, refer to `examples/flux.1-dev-turing.py` for 20-series support.\n- Memory issues? Use per-layer CPU offloading:  \n  ```bash\n  python examples/flux.1-dev.py --cpu-offload\n  ```\n\n## 8. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for help.\n- Check [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html) for common issues.", "issue_title": "HUNYUAN video model Quants? when? 😁", "issue_body": "🍷 cheers guys and thanks for your amazing work.\r\nJust wanted to know which are your plans about Hunyuan. ty", "choices": "(A) 1. Use the prebuilt wheels provided for Linux as mentioned in the installation guide to avoid compilation issues. (B) 1. Downgrade nvcc to version 11.0 or lower. 2. Update nvcc to version 12.6 or higher to resolve the compilation errors. 3. Alternatively, use the prebuilt wheels provided for Linux as mentioned in the installation guide to avoid compilation issues. (C) 1. Use the prebuilt wheels provided for Linux as mentioned in the installation guide to avoid compilation issues. 2. Update nvcc to version 12.6 or higher to resolve the compilation errors. (D) 1. Update nvcc to version 12.6 or higher to resolve the compilation errors. 2. Alternatively, use the prebuilt wheels provided for Linux as mentioned in the installation guide to avoid compilation issues.", "answer": "D"}
{"uuid": "a4c17e16-66b0-4def-86e5-f335e0c4423d", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face Hub](https://huggingface.co/nunchaku-tech)  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux.1-dev\n   ```\n\n2. **Custom Quantization (Optional)**  \n   Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 4. Basic Inference\n1. **Run Example Script**  \n   ```bash\n   python examples/flux.1-dev-basic.py --model-path ./nunchaku-flux.1-dev\n   ```\n\n2. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model-path ./nunchaku-flux.1-dev\n   ```\n\n## 5. Advanced Features\n1. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --model-path ./nunchaku-flux.1-dev\n   ```\n\n2. **Low-Memory Inference (4GB VRAM)**  \n   ```bash\n   python examples/flux.1-dev-low-memory.py --model-path ./nunchaku-flux.1-dev --cpu-offload\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI**  \n   Load Nunchaku nodes via ComfyUI interface.\n\n## 7. Monitoring & Optimization\n1. **Enable FP16 Attention**  \n   Add `--fp16-attn` flag to inference scripts.\n\n2. **First-Block Cache**  \n   Use `--fbcache` flag for faster multi-prompt inference.\n\n## 8. Troubleshooting\n- **Common Issues**: Refer to [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html)\n- **Community Support**: Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm)", "issue_title": "Unable to pin memory: operation not supported", "issue_body": "(venv) ubuntu@hp:~/nunchaku$ ls\r\napp  assets  build  comfyui  example.py  LICENCE.txt  MANIFEST.in  nunchaku  nunchaku.egg-info  pyproject.toml  README.md  schnell.model  setup.py  src  third_party\r\n(venv) ubuntu@hp:~/nunchaku$ python example.py\r\n[2024-12-10 10:42:02.110] [info] Initializing QuantizedFluxModel\r\n[2024-12-10 10:42:02.243] [info] Loading weights from /home/ubuntu/.cache/huggingface/hub/models--mit-han-lab--svdq-int4-flux.1-schnell/snapshots/3957d6ece9852f09ccadc064f1fd25964394a490/transformer_blocks.safetensors\r\n[2024-12-10 10:42:02.243] [warning] Unable to pin memory: operation not supported\r\n[2024-12-10 10:42:02.243] [info] Try MAP_PRIVATE\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/nunchaku/example.py\", line 5, in <module>\r\n    transformer = NunchakuFluxTransformer2dModel.from_pretrained(\"mit-han-lab/svdq-int4-flux.1-schnell\")\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/ComfyUI/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/nunchaku/nunchaku/models/transformer_flux.py\", line 206, in from_pretrained\r\n    m = load_quantized_module(transformer_block_path, device=device)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/nunchaku/nunchaku/models/transformer_flux.py\", line 114, in load_quantized_module\r\n    m.load(path)\r\nRuntimeError: CUDA error: invalid argument (at /home/ubuntu/nunchaku/src/Serialization.cpp:101)\r\n(venv) ubuntu@hp:~/nunchaku$ nvidia-smi\r\nTue Dec 10 10:42:14 2024\r\n+-----------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\r\n|-----------------------------------------+------------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                        |               MIG M. |\r\n|=========================================+========================+======================|\r\n|   0  NVIDIA GeForce RTX 4060        Off |   00000000:01:00.0 Off |                  N/A |\r\n| 33%   32C    P8             N/A /  115W |       2MiB /   8188MiB |      0%      Default |\r\n|                                         |                        |                  N/A |\r\n+-----------------------------------------+------------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------------------+\r\n| Processes:                                                                              |\r\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n|        ID   ID                                                               Usage      |\r\n|=========================================================================================|\r\n|  No running processes found                                                             |\r\n+-----------------------------------------------------------------------------------------+\r\n(venv) ubuntu@hp:~/nunchaku$", "choices": "(A) 1. Refer to the LoRA conversion guide available at [https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora](https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora).\n2. Delete the LoRA conversion script file at [https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py](https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py).\n3. Use the LoRA conversion script for the diffusers format found at [https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py](https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py).\n4. For integration with ComfyUI, refer to the new LoRA ComfyUI node documentation at [https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes](https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes). (B) 1. Refer to the LoRA conversion guide available at [https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora](https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora).\n2. For integration with ComfyUI, refer to the new LoRA ComfyUI node documentation at [https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes](https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes). (C) 1. Refer to the LoRA conversion guide available at [https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora](https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora).\n2. For integration with ComfyUI, refer to the new LoRA ComfyUI node documentation at [https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes](https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes).\n3. Use the LoRA conversion script for the diffusers format found at [https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py](https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py). (D) 1. Refer to the LoRA conversion guide available at [https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora](https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file#customized-lora).\n2. Use the LoRA conversion script for the diffusers format found at [https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py](https://github.com/mit-han-lab/nunchaku/blob/main/nunchaku/convert_lora.py).\n3. For integration with ComfyUI, refer to the new LoRA ComfyUI node documentation at [https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes](https://github.com/mit-han-lab/nunchaku/tree/main/comfyui#svdquant-nodes).", "answer": "D"}
{"uuid": "8145246c-a19d-4c36-8665-97eb4f4d244b", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face Hub](https://huggingface.co/nunchaku-tech)  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux.1-dev\n   ```\n\n2. **Custom Quantization (Optional)**  \n   - Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 4. Basic Inference\n1. **Run Text-to-Image Example**  \n   ```bash\n   python examples/flux.1-dev-t2i.py --model-path ./nunchaku-flux.1-dev --prompt \"A scenic landscape\"\n   ```\n\n2. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model-path ./nunchaku-flux.1-dev --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n## 5. Advanced Features\n1. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --model-path ./nunchaku-flux.1-dev --controlnet canny\n   ```\n\n2. **Low-Memory Inference (4GB VRAM)**  \n   ```bash\n   python examples/flux.1-dev-lowmem.py --model-path ./nunchaku-flux.1-dev --cpu-offload\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/ComfyUI-nunchaku\n   ```\n\n2. **Restart ComfyUI**  \n   - Models will appear in the UI under `NunchakuLoader`.\n\n## 7. Monitoring & Debugging\n- Check logs in `~/.cache/nunchaku/logs/`\n- Enable debug mode:  \n  ```bash\n  export NUNCHAKU_LOG_LEVEL=DEBUG\n  ```\n\n## 8. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for assistance.", "issue_title": "Please write a more detailed install guide on how to install in ComfyUI", "issue_body": "Hi @lmxyy \r\n\r\nThank you so much for your effort to make a ComfyUI node to work with your SVDQuant\r\n\r\nHowever your install guide is a bit complicated for normal people like us.\r\n\r\nNormally we install something like this: Find a custom node on ComfyUI manager and hit install then everything automatic install\r\nIf we dont find it on ComfyUI manager, we just git clone your github link and paste it inside custom node folder and run pip install requirement.txt inside the git clone folder\r\n\r\nI understand that your nunchaku need to build a wheel. I often find a single wheel package and install it because build wheel very complicated depend on add on and extension we install sometimes it is failed to build but we don't know why\r\n\r\nSo can you do a little more to support us to use your amazing node\r\n\r\nThank you in advance", "choices": "(A) 1. Ensure you have the latest codebase installed.\n2. Use the quantized encoder by modifying the pipeline initialization as follows:\n```python\npipeline = nunchaku_flux.from_pretrained(\n    \"black-forest-labs/FLUX.1-schnell\",\n    torch_dtype=torch.bfloat16,\n    qmodel_path=\"mit-han-lab/svdq-int4-flux.1-schnell\",\n    qencoder_path=\"mit-han-lab/svdquant-models/svdq-w4a16-t5.pt\",\n).to(\"cuda\")\n```\n3. Verify the issue is resolved after applying these changes.\n4. Install deepcompressor by running the following commands:\n```shell\ngit clone https://github.com/mit-han-lab/deepcompressor\ncd deepcompressor\npip install poetry\npoetry install\n``` (B) 1. Ensure you have the latest codebase installed.\n2. Uninstall torch by running:\n```shell\npip uninstall torch -y\n```\n3. Use the quantized encoder by modifying the pipeline initialization as follows:\n```python\npipeline = nunchaku_flux.from_pretrained(\n    \"black-forest-labs/FLUX.1-schnell\",\n    torch_dtype=torch.bfloat16,\n    qmodel_path=\"mit-han-lab/svdq-int4-flux.1-schnell\",\n    qencoder_path=\"mit-han-lab/svdquant-models/svdq-w4a16-t5.pt\",\n).to(\"cuda\")\n```\n4. Install deepcompressor by running the following commands:\n```shell\ngit clone https://github.com/mit-han-lab/deepcompressor\ncd deepcompressor\npip install poetry\npoetry install\n```\n5. Verify the issue is resolved after applying these changes. (C) 1. Ensure you have the latest codebase installed.\n2. Use the quantized encoder by modifying the pipeline initialization as follows:\n```python\npipeline = nunchaku_flux.from_pretrained(\n    \"black-forest-labs/FLUX.1-schnell\",\n    torch_dtype=torch.bfloat16,\n    qmodel_path=\"mit-han-lab/svdq-int4-flux.1-schnell\",\n    qencoder_path=\"mit-han-lab/svdquant-models/svdq-w4a16-t5.pt\",\n).to(\"cuda\")\n```\n3. Verify the issue is resolved after applying these changes. (D) 1. Ensure you have the latest codebase installed.\n2. Install deepcompressor by running the following commands:\n```shell\ngit clone https://github.com/mit-han-lab/deepcompressor\ncd deepcompressor\npip install poetry\npoetry install\n```\n3. Use the quantized encoder by modifying the pipeline initialization as follows:\n```python\npipeline = nunchaku_flux.from_pretrained(\n    \"black-forest-labs/FLUX.1-schnell\",\n    torch_dtype=torch.bfloat16,\n    qmodel_path=\"mit-han-lab/svdq-int4-flux.1-schnell\",\n    qencoder_path=\"mit-han-lab/svdquant-models/svdq-w4a16-t5.pt\",\n).to(\"cuda\")\n```\n4. Verify the issue is resolved after applying these changes. (E) 1. Ensure you have the latest codebase installed.\n2. Install deepcompressor by running the following commands:\n```shell\ngit clone https://github.com/mit-han-lab/deepcompressor\ncd deepcompressor\npip install poetry\npoetry install\n```\n3. Verify the issue is resolved after applying these changes. (F) 1. Ensure you have the latest codebase installed.\n2. Use the quantized encoder by modifying the pipeline initialization as follows:\n```python\npipeline = nunchaku_flux.from_pretrained(\n    \"black-forest-labs/FLUX.1-schnell\",\n    torch_dtype=torch.bfloat16,\n    qmodel_path=\"mit-han-lab/svdq-int4-flux.1-schnell\",\n    qencoder_path=\"mit-han-lab/svdquant-models/svdq-w4a16-t5.pt\",\n).to(\"cuda\")\n```\n3. Install deepcompressor by running the following commands:\n```shell\ngit clone https://github.com/mit-han-lab/deepcompressor\ncd deepcompressor\npip install poetry\npoetry install\n```\n4. Verify the issue is resolved after applying these changes. (G) 1. Ensure you have the latest codebase installed.\n2. Use the quantized encoder by modifying the pipeline initialization as follows:\n```python\npipeline = nunchaku_flux.from_pretrained(\n    \"black-forest-labs/FLUX.1-schnell\",\n    torch_dtype=torch.bfloat16,\n    qmodel_path=\"mit-han-lab/svdq-int4-flux.1-schnell\",\n    qencoder_path=\"mit-han-lab/svdquant-models/svdq-w4a16-t5.pt\",\n).to(\"cuda\")\n```\n3. Install deepcompressor by running the following commands:\n```shell\ngit clone https://github.com/mit-han-lab/deepcompressor\ncd deepcompressor\npip install poetry\npoetry install\n```\n4. Delete the deepcompressor directory:\n```shell\nrm -rf deepcompressor\n```\n5. Verify the issue is resolved after applying these changes.", "answer": "F"}
{"uuid": "ff329cff-41db-42a6-8e78-3a30b6d908fa", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install nunchaku --upgrade\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face Hub](https://huggingface.co/nunchaku-tech)  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-t5\n   ```\n\n2. **Custom Quantization (Optional)**  \n   Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 4. Basic Inference\n1. **Run Example Script**  \n   ```bash\n   python examples/flux.1-kontext-dev.py\n   ```\n\n2. **API Usage**  \n   ```python\n   from nunchaku import Nunchaku\n   model = Nunchaku.from_pretrained(\"nunchaku-tech/flux.1-dev-4bit\")\n   outputs = model.generate(prompts=[\"your prompt here\"])\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n## 6. Deployment Options\n1. **ComfyUI Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   ```\n\n2. **Gradio Demo**  \n   ```bash\n   python app/flux.1/t2i/app.py\n   ```\n\n## 7. Troubleshooting\n- **Memory Issues**: Enable per-layer CPU offloading  \n  ```python\n  model = Nunchaku.from_pretrained(\"model_name\", offload_cpu=True)\n  ```\n- **Performance Tuning**: Use `FP16 attention` and `First-Block Cache` flags.\n\n## 8. Community Support\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for help.", "issue_title": "Even if it does not download the model, initialization takes 48gb ram.", "issue_body": "As for @lmxyy 's request, I am opening a separate issue. I have a pc with a 4060ti with 16gb vram and 32gb Ram. \r\nWhen I start the example.py, it fills the entire 32gb of ram, goes up with 8gb of swap and then it kills the process. I have downloaded the latest codebase.\r\n\r\nI have tried with cpu offload, but it still gives problems. Can you please give me an advise?\r\n\r\nHere is the modified example I use: \r\n\r\nimport torch\r\n\r\nfrom nunchaku.pipelines import flux as nunchaku_flux\r\n\r\npipeline = nunchaku_flux.from_pretrained(\r\n\"black-forest-labs/FLUX.1-schnell\",\r\ntorch_dtype=torch.bfloat16,\r\nqmodel_path=\"mit-han-lab/svdq-int4-flux.1-schnell\",\r\n#qmodel_path=\"mit-han-lab/svdquant-models/svdq-int4-flux.1-schnell.safetensors\", # download from Huggingface\r\n).to(\"cuda\")\r\n\r\npipeline.enable_sequential_cpu_offload()\r\n\r\nimage = pipeline(\"A cat holding a sign that says hello world\", num_inference_steps=4, guidance_scale=0).images[0]\r\nimage.save(\"example.png\")\r\n\r\nThank you for your patience and your amazing work.", "choices": "(A) 1. Pull the Docker image:\n   ```\n   docker pull nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04\n   ```\n2. Download Miniconda installer:\n   ```\n   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n   ```\n3. Install Miniconda in Docker:\n   ```\n   bash Miniconda3-latest-Linux-x86_64.sh\n   source ~/.bashrc\n   ```\n4. Add your user to the docker group and apply changes in the current session:\n   ```\n   sudo usermod -aG docker $USER\n   newgrp docker\n   ```\n5. Follow the nunchaku instructions in the README, using CUDA 12.4 instead of 12.1, and modify setup.py to target the A100 architecture (sm_80):\n   ```\n   pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n   ```\n   Edit setup.py to include only:\n   ```\n   \"arch=compute_80,code=sm_80\",\n   ```\n6. Set up the conda environment and install dependencies:\n   ```\n   conda create -n nunchaku python=3.11\n   conda activate nunchaku\n   pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n   pip install diffusers ninja wheel transformers accelerate sentencepiece protobuf\n   pip install huggingface_hub peft opencv-python einops gradio spaces GPUtil\n   ```\n7. Clone the nunchaku repository and install it:\n   ```\n   git clone https://github.com/mit-han-lab/nunchaku.git\n   cd nunchaku\n   git submodule init\n   git submodule update\n   pip install -e .\n   ``` (B) 1. Pull the Docker image:\n   ```\n   docker pull nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04\n   ```\n2. Download Miniconda installer:\n   ```\n   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n   ```\n3. Install Miniconda in Docker:\n   ```\n   bash Miniconda3-latest-Linux-x86_64.sh\n   source ~/.bashrc\n   ```\n4. Follow the nunchaku instructions in the README, using CUDA 12.4 instead of 12.1, and modify setup.py to target the A100 architecture (sm_80):\n   ```\n   pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n   ```\n   Edit setup.py to include only:\n   ```\n   \"arch=compute_80,code=sm_80\",\n   ```\n5. Set up the conda environment and install dependencies:\n   ```\n   conda create -n nunchaku python=3.11\n   conda activate nunchaku\n   pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n   pip install diffusers ninja wheel transformers accelerate sentencepiece protobuf\n   pip install huggingface_hub peft opencv-python einops gradio spaces GPUtil\n   ```\n6. Clone the nunchaku repository and install it:\n   ```\n   git clone https://github.com/mit-han-lab/nunchaku.git\n   cd nunchaku\n   git submodule init\n   git submodule update\n   pip install -e .\n   ``` (C) 1. Add your user to the docker group and apply changes in the current session:\n   ```\n   sudo usermod -aG docker $USER\n   newgrp docker\n   ```\n2. Pull the Docker image:\n   ```\n   docker pull nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04\n   ```\n3. Download Miniconda installer:\n   ```\n   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n   ```\n4. Install Miniconda in Docker:\n   ```\n   bash Miniconda3-latest-Linux-x86_64.sh\n   source ~/.bashrc\n   ```\n5. Follow the nunchaku instructions in the README, using CUDA 12.4 instead of 12.1, and modify setup.py to target the A100 architecture (sm_80):\n   ```\n   pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n   ```\n   Edit setup.py to include only:\n   ```\n   \"arch=compute_80,code=sm_80\",\n   ```\n6. Set up the conda environment and install dependencies:\n   ```\n   conda create -n nunchaku python=3.11\n   conda activate nunchaku\n   pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n   pip install diffusers ninja wheel transformers accelerate sentencepiece protobuf\n   pip install huggingface_hub peft opencv-python einops gradio spaces GPUtil\n   ```\n7. Clone the nunchaku repository and install it:\n   ```\n   git clone https://github.com/mit-han-lab/nunchaku.git\n   cd nunchaku\n   git submodule init\n   git submodule update\n   pip install -e .\n   ``` (D) 1. Add your user to the docker group and apply changes in the current session:\n   ```\n   sudo usermod -aG docker $USER\n   newgrp docker\n   ```\n2. Pull the Docker image:\n   ```\n   docker pull nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04\n   ```\n3. Remove the Docker image:\n   ```\n   docker rmi nvcr.io/nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04\n   ```\n4. Download Miniconda installer:\n   ```\n   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n   ```\n5. Install Miniconda in Docker:\n   ```\n   bash Miniconda3-latest-Linux-x86_64.sh\n   source ~/.bashrc\n   ```\n6. Follow the nunchaku instructions in the README, using CUDA 12.4 instead of 12.1, and modify setup.py to target the A100 architecture (sm_80):\n   ```\n   pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n   ```\n   Edit setup.py to include only:\n   ```\n   \"arch=compute_80,code=sm_80\",\n   ```\n7. Set up the conda environment and install dependencies:\n   ```\n   conda create -n nunchaku python=3.11\n   conda activate nunchaku\n   pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n   pip install diffusers ninja wheel transformers accelerate sentencepiece protobuf\n   pip install huggingface_hub peft opencv-python einops gradio spaces GPUtil\n   ```\n8. Clone the nunchaku repository and install it:\n   ```\n   git clone https://github.com/mit-han-lab/nunchaku.git\n   cd nunchaku\n   git submodule init\n   git submodule update\n   pip install -e .\n   ```", "answer": "C"}
{"uuid": "43fb2d0d-3fa6-46db-a1f3-f5a5a3b3c005", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- Ensure system meets hardware/software requirements (check [Installation Guide](https://nunchaku.tech/docs/nunchaku/installation/installation.html))\n- NVIDIA GPU with CUDA support (RTX 4090/5090 recommended)\n- Python 3.8+ environment\n\n## 2. Installation\n```bash\n# Clone repository\ngit clone https://github.com/nunchaku-tech/nunchaku.git\ncd nunchaku\n\n# Create and activate virtual environment\npython -m venv nunchaku-env\nsource nunchaku-env/bin/activate  # Linux/Mac\n# OR\nnunchaku-env\\Scripts\\activate     # Windows\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n## 3. Model Setup\n```bash\n# Download pre-quantized models from Hugging Face\npython scripts/download_models.py --model flux.1-dev\n```\n\n## 4. Basic Inference Test\n```bash\n# Run sample inference (adjust model path as needed)\npython examples/flux.1-dev.py \\\n  --model-path ./models/flux.1-dev-4bit \\\n  --prompt \"A scenic landscape\"\n```\n\n## 5. Advanced Features\n```bash\n# Multi-LoRA example\npython examples/flux.1-dev-multiple-lora.py \\\n  --model-path ./models/flux.1-dev-4bit \\\n  --lora-paths ./lora/style1.safetensors ./lora/style2.safetensors\n\n# ControlNet example\npython examples/flux.1-dev-controlnet-union-pro.py \\\n  --model-path ./models/flux.1-dev-4bit \\\n  --control-image ./inputs/sketch.png\n```\n\n## 6. ComfyUI Integration (Optional)\n```bash\n# Install ComfyUI plugin\ngit clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git \\\n  /path/to/ComfyUI/custom_nodes/\n```\n\n## 7. Verification\n- Check output images in `./outputs/` directory\n- Monitor GPU utilization with `nvidia-smi`\n- Refer to [API Docs](https://nunchaku.tech/docs/nunchaku/python_api/nunchaku.html) for advanced usage\n\n## 8. Troubleshooting\n- Consult [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html)\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for support\n- Open GitHub issues for bugs", "issue_title": "Quality comparison: SVDQuant vs GGUF Q4 ?", "issue_body": "SVDQuant quality is great! the paper compares the quality with NF4, however **GGUF Q4 (Q4_0, Q4_K_S)** is another popular quantization in community and broadly used in ComfyUI with Image Gen model like FLUX.1 and Video Generation Model like Mochi 1. GGUF Q4 FLUX.1-schenell is also ~6.5GB with good quality, and it does not need big inference engine change.\r\n\r\nCan we have any quality comparison between SVDQuant vs GGUF Q4 too, on FLUX.1 (and future Mochi 1)?\r\n\r\nToday new image/video gen models like FLUX.1 or Mochi 1, community will first try ComfyUI + GGUF Q4 DiT to enable it on consumer GPU, especially low GPU RAM. If we can see SVDQuant is better than GGUF Q4, on FLUX.1 and/or Mochi 1, it will give community much bigger motivation to adopt or prioritize this very novel SVDQuant.", "choices": "(A) 1. Pull the latest code from the repository to benefit from the optimizations made to reduce RAM usage from ~24G to ~10G.\n2. Use the new pre-built wheels provided by the maintainers to resolve the issue.\n3. Monitor system logs (`/var/log/kern.log` and `/var/log/messages`) for any kernel-level issues that might still occur, although the main configuration issue should be resolved with the updated code and pre-built wheels.\n4. Ensure your Docker container and host system have sufficient resources (RAM and CPU) to handle the reduced memory requirements. (B) 1. Pull the latest code from the repository to benefit from the optimizations made to reduce RAM usage from ~24G to ~10G.\n2. Use the new pre-built wheels provided by the maintainers to resolve the issue.\n3. Ensure your Docker container and host system have sufficient resources (RAM and CPU) to handle the reduced memory requirements.\n4. Monitor system logs (`/var/log/kern.log` and `/var/log/messages`) for any kernel-level issues that might still occur, although the main configuration issue should be resolved with the updated code and pre-built wheels. (C) 1. Pull the latest code from the repository to benefit from the optimizations made to reduce RAM usage from ~24G to ~10G.\n2. Use the new pre-built wheels provided by the maintainers to resolve the issue.\n3. Manually increase RAM allocation to 24G in the Docker container configuration.\n4. Ensure your Docker container and host system have sufficient resources (RAM and CPU) to handle the reduced memory requirements.\n5. Monitor system logs (`/var/log/kern.log` and `/var/log/messages`) for any kernel-level issues that might still occur, although the main configuration issue should be resolved with the updated code and pre-built wheels. (D) 1. Pull the latest code from the repository to benefit from the optimizations made to reduce RAM usage from ~24G to ~10G.\n2. Use the new pre-built wheels provided by the maintainers to resolve the issue.\n3. Monitor system logs (`/var/log/kern.log` and `/var/log/messages`) for any kernel-level issues that might still occur, although the main configuration issue should be resolved with the updated code and pre-built wheels. (E) 1. Pull the latest code from the repository to benefit from the optimizations made to reduce RAM usage from ~24G to ~10G.\n2. Ensure your Docker container and host system have sufficient resources (RAM and CPU) to handle the reduced memory requirements.\n3. Monitor system logs (`/var/log/kern.log` and `/var/log/messages`) for any kernel-level issues that might still occur, although the main configuration issue should be resolved with the updated code and pre-built wheels. (F) 1. Use the new pre-built wheels provided by the maintainers to resolve the issue.\n2. Pull the latest code from the repository to benefit from the optimizations made to reduce RAM usage from ~24G to ~10G.\n3. Ensure your Docker container and host system have sufficient resources (RAM and CPU) to handle the reduced memory requirements.\n4. Monitor system logs (`/var/log/kern.log` and `/var/log/messages`) for any kernel-level issues that might still occur, although the main configuration issue should be resolved with the updated code and pre-built wheels. (G) 1. Pull the latest code from the repository to benefit from the optimizations made to reduce RAM usage from ~24G to ~10G.\n2. Use the new pre-built wheels provided by the maintainers to resolve the issue.\n3. Ensure your Docker container and host system have sufficient resources (RAM and CPU) to handle the reduced memory requirements.\n4. Delete all contents of `/var/log/kern.log` and `/var/log/messages`.\n5. Monitor system logs (`/var/log/kern.log` and `/var/log/messages`) for any kernel-level issues that might still occur, although the main configuration issue should be resolved with the updated code and pre-built wheels.", "answer": "B"}
{"uuid": "2e5a11d9-51ee-45ee-a34b-28554ca9c402", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install nunchaku --upgrade\n   ```\n\n## 3. Basic Usage\n1. **Load a Quantized Model**  \n   ```python\n   from nunchaku import NunchakuModel\n   model = NunchakuModel.from_pretrained(\"nunchaku-tech/FLUX.1-dev-4bit\")\n   ```\n\n2. **Run Inference**  \n   ```python\n   output = model.generate(prompt=\"A cat wearing sunglasses\")\n   ```\n\n## 4. Advanced Features\n1. **Multi-LoRA Support**  \n   Example script: [`examples/flux.1-dev-multiple-lora.py`](./examples/flux.1-dev-multiple-lora.py)\n\n2. **ControlNet Integration**  \n   Example script: [`examples/flux.1-dev-controlnet-union-pro.py`](./examples/flux.1-dev-controlnet-union-pro.py)\n\n3. **Low-Memory Inference**  \n   Enable per-layer CPU offloading:  \n   ```python\n   model.enable_cpu_offload(min_memory=4)  # 4 GiB minimum\n   ```\n\n## 5. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku && pip install -e .\n   ```\n\n2. **Launch ComfyUI**  \n   ```bash\n   python main.py\n   ```\n\n## 6. Custom Model Quantization\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor && pip install -e .\n   ```\n\n2. **Quantize Your Model**  \n   Follow instructions in [DeepCompressor README](https://github.com/nunchaku-tech/deepcompressor#custom-model-quantization).\n\n## 7. Demo Deployment\n1. **Launch Gradio Demo**  \n   ```bash\n   python app/flux.1/t2i/app.py\n   ```\n\n2. **Access Web UI**  \n   Open `http://localhost:7860` in your browser.\n\n## 8. Troubleshooting\n- **GPU Compatibility**: For 20-series GPUs, use Turing-specific scripts (`examples/flux.1-dev-turing.py`).\n- **Performance Issues**: Enable FP16 attention with `model.enable_fp16_attention()`.\n- **Community Support**: Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm) for help.", "issue_title": "A100 RuntimeError: CUDA error: no kernel image is available for execution on the device", "issue_body": "When run example.py, hit \r\nRuntimeError: CUDA error: no kernel image is available for execution on the device (at /home/ubuntu/nunchaku/src/kernels/awq/gemv_awq.cu:311)\r\n\r\nI'm on Lambda A100 GPU instance, unbuntu env. Since env CUDA is 12.4, I changed pip install to use cu124 instead.\r\n\r\npip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\r\n-->\r\npip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\r\n\r\n**A100 Unbuntu Env:**\r\nPyTorch version: 2.3.1\r\nCUDA available: True\r\nCUDA version: 12.4\r\nDevice: NVIDIA A100-SXM4-40GB\r\nnvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2024 NVIDIA Corporation\r\nBuilt on Thu_Mar_28_02:18:24_PDT_2024\r\nCuda compilation tools, release 12.4, V12.4.131\r\nBuild cuda_12.4.r12.4/compiler.34097967_0\r\npython --version\r\nPython 3.10.12\r\n\r\n\r\n(nunchaku) ubuntu@129-146-61-54:~/nunchaku$ python example.py \r\n\r\nRuntimeError: CUDA error: no kernel image is available for execution on the device (at /home/ubuntu/nunchaku/src/kernels/awq/gemv_awq.cu:311)", "choices": "(A) 1. Downgrade CUDA to version 11.0.\n2. Upgrade CUDA to version 12.4 (as confirmed by user 'olokojoh' that it resolved the issue).\n3. If the issue persists, try using the command `python setup.py develop` instead of `pip install -e .`.\n4. If the system crashes during installation, it might be due to memory constraints. Set `MAX_JOBS` to a lower value (e.g., `MAX_JOBS=2 pip install -e .`) to reduce memory usage during the build process. (B) 1. Upgrade CUDA to version 12.4 (as confirmed by user 'olokojoh' that it resolved the issue).\n2. If the issue persists, try using the command `python setup.py develop` instead of `pip install -e .`.\n3. If the system crashes during installation, it might be due to memory constraints. Set `MAX_JOBS` to a lower value (e.g., `MAX_JOBS=2 pip install -e .`) to reduce memory usage during the build process. (C) 1. Upgrade CUDA to version 12.4 (as confirmed by user 'olokojoh' that it resolved the issue).\n2. Delete the `setup.py` file.\n3. If the issue persists, try using the command `python setup.py develop` instead of `pip install -e .`.\n4. If the system crashes during installation, it might be due to memory constraints. Set `MAX_JOBS` to a lower value (e.g., `MAX_JOBS=2 pip install -e .`) to reduce memory usage during the build process. (D) 1. If the issue persists, try using the command `python setup.py develop` instead of `pip install -e .`.\n2. Upgrade CUDA to version 12.4 (as confirmed by user 'olokojoh' that it resolved the issue).\n3. If the system crashes during installation, it might be due to memory constraints. Set `MAX_JOBS` to a lower value (e.g., `MAX_JOBS=2 pip install -e .`) to reduce memory usage during the build process. (E) 1. If the issue persists, try using the command `python setup.py develop` instead of `pip install -e .`.\n2. If the system crashes during installation, it might be due to memory constraints. Set `MAX_JOBS` to a lower value (e.g., `MAX_JOBS=2 pip install -e .`) to reduce memory usage during the build process. (F) 1. Upgrade CUDA to version 12.4 (as confirmed by user 'olokojoh' that it resolved the issue).\n2. If the issue persists, try using the command `python setup.py develop` instead of `pip install -e .`. (G) 1. If the system crashes during installation, it might be due to memory constraints. Set `MAX_JOBS` to a lower value (e.g., `MAX_JOBS=2 pip install -e .`) to reduce memory usage during the build process.\n2. Upgrade CUDA to version 12.4 (as confirmed by user 'olokojoh' that it resolved the issue).\n3. If the issue persists, try using the command `python setup.py develop` instead of `pip install -e .`.", "answer": "B"}
{"uuid": "10104273-4b4d-4d7c-97a1-5977c2da36c7", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Basic Usage\n1. **Load a Quantized Model**  \n   ```python\n   from nunchaku import NunchakuModel\n   model = NunchakuModel.from_pretrained(\"nunchaku-tech/flux.1-dev-4bit\")\n   ```\n\n2. **Run Inference**  \n   ```python\n   output = model.generate(prompt=\"A cat wearing sunglasses\")\n   ```\n\n## 4. Advanced Features\n1. **Multi-LoRA Support**  \n   See example script:  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n3. **Low-Memory Inference (4GB VRAM)**  \n   ```bash\n   python examples/flux.1-dev-low-memory.py\n   ```\n\n## 5. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku && pip install -e .\n   ```\n\n## 6. Quantizing Custom Models\n1. **Install DeepCompressor**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/deepcompressor.git\n   cd deepcompressor && pip install -e .\n   ```\n\n2. **Run Quantization**  \n   ```python\n   from deepcompressor import quantize\n   quantize(input_model=\"your_model\", output=\"quantized_model\")\n   ```\n\n## 7. Running Demos\n1. **Gradio Web UI**  \n   ```bash\n   python app/flux.1/t2i/app.py\n   ```\n\n## 8. Troubleshooting\n- For GPU compatibility issues, refer to `examples/flux.1-dev-turing.py`\n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for support\n\n## 9. Documentation\n- Full API reference:  \n  ```bash\n   python -m pydoc nunchaku\n   ```", "issue_title": "Can nunchaku support more Video-generator model？", "issue_body": "In addition to flux, does nunchaku support prompt-to-video models, such as OpenSoraPlan, Cogvideo，etc.\r\nFlux is also a transformer-based model", "choices": "(A) 1. Ensure all third-party libraries are updated to their required versions as specified by the code.  \n2. Check the GPU architecture compatibility with the code requirements. The issue may arise if the GPU does not support specific features (e.g., INT4 CUDA core).  \n3. If the GPU is incompatible, try using a different GPU that meets the code's architecture requirements (e.g., Tesla V100-PCIE-16GB worked for one user). (B) 1. Check the GPU architecture compatibility with the code requirements. The issue may arise if the GPU does not support specific features (e.g., INT4 CUDA core).  \n2. Force the code to run on the current GPU even if it is incompatible by disabling compatibility checks.  \n3. Ensure all third-party libraries are updated to their required versions as specified by the code.  \n4. If the GPU is incompatible, try using a different GPU that meets the code's architecture requirements (e.g., Tesla V100-PCIE-16GB worked for one user). (C) 1. If the GPU is incompatible, try using a different GPU that meets the code's architecture requirements (e.g., Tesla V100-PCIE-16GB worked for one user).  \n2. Check the GPU architecture compatibility with the code requirements. The issue may arise if the GPU does not support specific features (e.g., INT4 CUDA core).  \n3. Ensure all third-party libraries are updated to their required versions as specified by the code. (D) 1. Check the GPU architecture compatibility with the code requirements. The issue may arise if the GPU does not support specific features (e.g., INT4 CUDA core).  \n2. Downgrade all third-party libraries to their oldest stable versions to avoid compatibility issues.  \n3. Ensure all third-party libraries are updated to their required versions as specified by the code.  \n4. If the GPU is incompatible, try using a different GPU that meets the code's architecture requirements (e.g., Tesla V100-PCIE-16GB worked for one user). (E) 1. Check the GPU architecture compatibility with the code requirements. The issue may arise if the GPU does not support specific features (e.g., INT4 CUDA core).  \n2. If the GPU is incompatible, try using a different GPU that meets the code's architecture requirements (e.g., Tesla V100-PCIE-16GB worked for one user). (F) 1. Check the GPU architecture compatibility with the code requirements. The issue may arise if the GPU does not support specific features (e.g., INT4 CUDA core).\n2. Ensure all third-party libraries are updated to their required versions as specified by the code.\n3. If the GPU is incompatible, try using a different GPU that meets the code's architecture requirements (e.g., Tesla V100-PCIE-16GB worked for one user). (G) 1. Ensure all third-party libraries are updated to their required versions as specified by the code.  \n2. If the GPU is incompatible, try using a different GPU that meets the code's architecture requirements (e.g., Tesla V100-PCIE-16GB worked for one user).", "answer": "F"}
{"uuid": "4104de7f-d369-404e-8e6d-81aba8ded36c", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.0+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   - Available on [Hugging Face Hub](https://huggingface.co/nunchaku-tech)  \n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux1\n   ```\n\n2. **Custom Quantization (Optional)**  \n   Use [DeepCompressor](https://github.com/nunchaku-tech/deepcompressor) for custom model quantization.\n\n## 4. Basic Inference\n1. **Run Example Script**  \n   ```bash\n   python examples/flux.1-dev.py --model-path ./nunchaku-flux1\n   ```\n\n2. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py\n   ```\n\n## 5. Advanced Features\n1. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py\n   ```\n\n2. **Low-Memory Mode**  \n   ```bash\n   python examples/flux.1-dev-low-memory.py --cpu-offload\n   ```\n\n## 6. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git\n   cd ComfyUI-nunchaku && pip install -r requirements.txt\n   ```\n\n2. **Launch ComfyUI**  \n   ```bash\n   python main.py\n   ```\n\n## 7. Monitoring & Optimization\n- Enable `--fp16-attention` for faster attention layers  \n- Use `--double-cache` for improved throughput in multi-batch scenarios  \n\n## 8. Troubleshooting\n- Check [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html)  \n- Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) for support", "issue_title": "Running 'example. py' in a Docker container can cause the system to freeze or be forced to shut down", "issue_body": "I am running it in a Docker container and nunkaku has been successfully compiled.\r\n\r\nI replaced the schnell model in example. py with the dev model and ran it.\r\nWhen the program reaches the prompt to load svdq-int4-flux.1-dev.safedetectors and outputs' Done '.\r\nThe system is stuck or forced to shut down\r\n\r\nWarning of very high system memory usage detected when stuck or forced to shut down\r\n\r\nMay I ask if the current version of the code requires extremely high memory usage and approximately how much memory is needed\r\nOr whether there are memory management or leakage issues", "choices": "(A) 1. Install `deepcompressor` if needed:\n   ```shell\n   git clone https://github.com/mit-han-lab/deepcompressor\n   cd deepcompressor\n   pip install poetry\n   poetry install\n   ```\n2. Ensure the image dimensions are multiples of 65,536. For example, use width 960 and height 1024.\n3. To address OOM issues, use the 4-bit text encoder by adding the option `--use-qencoder`.\n4. The dependency on `deepcompressor` will be removed in the next release. (B) 1. Ensure the image dimensions are multiples of 65,536. For example, use width 960 and height 1024.\n2. To address OOM issues, use the 4-bit text encoder by adding the option `--use-qencoder`.\n3. Install `deepcompressor` if needed:\n   ```shell\n   git clone https://github.com/mit-han-lab/deepcompressor\n   cd deepcompressor\n   pip install poetry\n   poetry install\n   ```\n4. Uninstall `deepcompressor`:\n   ```shell\n   pip uninstall deepcompressor -y\n   ```\n5. The dependency on `deepcompressor` will be removed in the next release. (C) 1. Ensure the image dimensions are multiples of 65,536. For example, use width 960 and height 1024.\n2. To address OOM issues, use the 4-bit text encoder by adding the option `--use-qencoder`.\n3. Install `deepcompressor` if needed:\n   ```shell\n   git clone https://github.com/mit-han-lab/deepcompressor\n   cd deepcompressor\n   pip install poetry\n   poetry install\n   ```\n4. The dependency on `deepcompressor` will be removed in the next release. (D) 1. To address OOM issues, use the 4-bit text encoder by adding the option `--use-qencoder`.\n2. Install `deepcompressor` if needed:\n   ```shell\n   git clone https://github.com/mit-han-lab/deepcompressor\n   cd deepcompressor\n   pip install poetry\n   poetry install\n   ```\n3. The dependency on `deepcompressor` will be removed in the next release.", "answer": "C"}
{"uuid": "d08b49cf-be7e-4cbd-ada2-35efda17f598", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 11.8+, PyTorch 2.1+\n\n## 2. Installation\n1. **Clone Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Model Setup\n1. **Download Pre-Quantized Models**  \n   Visit [Hugging Face Hub](https://huggingface.co/nunchaku-tech) or [ModelScope](https://modelscope.cn/organization/nunchaku-tech) to download models. Example:\n   ```bash\n   git lfs install\n   git clone https://huggingface.co/nunchaku-tech/nunchaku-flux1-dev\n   ```\n\n2. **Verify Model Integrity**  \n   Check SHA256 hashes if provided in the model repository.\n\n## 4. Basic Inference\n1. **Run Example Script**  \n   ```bash\n   python examples/flux.1-dev.py --model-path ./nunchaku-flux1-dev --prompt \"A photo of a cat\"\n   ```\n\n2. **Enable FP16 Attention (Optional)**  \n   Add `--use-fp16-attn` flag for faster performance on supported GPUs.\n\n## 5. Advanced Features\n1. **Multi-LoRA Inference**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model-path ./nunchaku-flux1-dev --lora-paths lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --model-path ./nunchaku-flux1-dev --controlnet-path ./controlnet-union-pro\n   ```\n\n## 6. Performance Optimization\n1. **Enable First-Block Cache**  \n   Add `--use-fb-cache` flag to reduce memory usage for iterative generation.\n\n2. **Low-Memory Mode (4GB GPUs)**  \n   ```bash\n   python examples/flux.1-dev.py --model-path ./nunchaku-flux1-dev --cpu-offload --use-4bit-encoder\n   ```\n\n## 7. ComfyUI Integration\n1. **Install Plugin**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/ComfyUI-nunchaku.git /path/to/ComfyUI/custom_nodes/\n   ```\n\n2. **Restart ComfyUI** and load workflows from `examples/comfyui/`.\n\n## 8. Monitoring\n- Check GPU utilization: `nvidia-smi -l 1`\n- Monitor memory usage: `watch -n 1 free -h`\n\n## 9. Troubleshooting\n- **Common Issues**: Refer to [FAQ](https://nunchaku.tech/docs/nunchaku/faq/faq.html)\n- **Community Support**: Join [Slack](https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q) or [Discord](https://discord.gg/Wk6PnwX9Sm)\n\n## 10. Updates\n- Regularly check for new releases:  \n  ```bash\n  git pull origin main\n  pip install --upgrade -r requirements.txt\n  ```", "issue_title": "Question about why getting blank image when using flux ControlNet.", "issue_body": "<details>\r\n<summary> generate image code detail </summary>\r\n\r\n```python\r\n\r\nfrom diffusers import FluxTransformer2DModel\r\nimport torch\r\n\r\n\r\ndef load_flux_model(\r\n    model_path: str,\r\n    load_from_file: bool = True,\r\n    dtype: torch.dtype = torch.bfloat16,\r\n) -> FluxTransformer2DModel:\r\n    \"\"\"\r\n    加载FLUX模型，支持从单文件或预训练目录加载\r\n\r\n    参数:\r\n        model_path: 模型路径，可以是safetensors文件路径或预训练模型目录\r\n        load_from_file: 是否从单个文件加载\r\n        dtype: 模型计算精度\r\n    \"\"\"\r\n    quantization_config = None\r\n\r\n    if load_from_file:\r\n        model = FluxTransformer2DModel.from_single_file(\r\n            model_path, quantization_config=quantization_config, torch_dtype=dtype\r\n        )\r\n    else:\r\n        model = FluxTransformer2DModel.from_pretrained(\r\n            model_path, quantization_config=quantization_config, torch_dtype=dtype\r\n        )\r\n\r\n    return model\r\n\r\nfrom huggingface_hub import hf_hub_download\r\n\r\nckpt_repo = \"Kijai/flux-fp8\"\r\nckpt_filename = \"flux1-dev-fp8-e4m3fn.safetensors\"\r\n\r\nckpt_path = hf_hub_download(ckpt_repo, filename=ckpt_filename)\r\n\r\nmodel = load_flux_model(ckpt_path, )\r\n\r\nfrom nunchaku.models.flux  import  load_quantized_model\r\n\r\nqmodel_path = \"mit-han-lab/svdquant-models/svdq-int4-flux.1-dev.safetensors\"\r\n\r\nif not os.path.exists(qmodel_path):\r\n    hf_repo_id = os.path.dirname(qmodel_path)\r\n    filename = os.path.basename(qmodel_path)\r\n    qmodel_path = hf_hub_download(repo_id=hf_repo_id, filename=filename)\r\n\r\n\r\nm = load_quantized_model(\r\n        qmodel_path, \"cuda\"\r\n    )\r\n\r\nfrom nunchaku.models.flux  import  NunchakuFluxModel, EmbedND, QuantizedFluxModel, SVD_RANK\r\nimport types\r\n\r\ndef inject_transformer(\r\n    transformer_model: FluxTransformer2DModel, m: QuantizedFluxModel\r\n) -> None:\r\n    \"\"\"注入自定义transformer模型\r\n\r\n    Args:\r\n        transformer_model: 原始transformer模型\r\n        custom_model: 要注入的自定义模型\r\n    \"\"\"\r\n    # 注入位置编码\r\n    transformer_model.pos_embed = EmbedND(\r\n        dim=transformer_model.inner_dim, theta=10000, axes_dim=[16, 56, 56]\r\n    )\r\n\r\n    # 替换transformer块\r\n    transformer_model.transformer_blocks = torch.nn.ModuleList([NunchakuFluxModel(m)])\r\n    transformer_model.single_transformer_blocks = torch.nn.ModuleList([])\r\n\r\n    def update_params(self: FluxTransformer2DModel, path: str):\r\n        if not os.path.exists(path):\r\n            hf_repo_id = os.path.dirname(path)\r\n            filename = os.path.basename(path)\r\n            path = hf_hub_download(repo_id=hf_repo_id, filename=filename)\r\n        block = self.transformer_blocks[0]\r\n        assert isinstance(block, NunchakuFluxModel)\r\n        block.m.load(path, True)\r\n\r\n    def set_lora_scale(self: FluxTransformer2DModel, scale: float):\r\n        block = self.transformer_blocks[0]\r\n        assert isinstance(block, NunchakuFluxModel)\r\n        block.m.setLoraScale(SVD_RANK, scale)\r\n\r\n    transformer_model.nunchaku_update_params = types.MethodType(\r\n        update_params, transformer_model\r\n    )\r\n    transformer_model.nunchaku_set_lora_scale = types.MethodType(\r\n        set_lora_scale, transformer_model\r\n    )\r\n\r\n    return transformer_model\r\n\r\n\r\nimport torch\r\n\r\n\r\nmodel = inject_transformer(model, m)\r\n\r\nmodel = model.to(\"cuda\")\r\n\r\nfrom diffusers.pipelines import FluxControlNetPipeline\r\ndtype = torch.bfloat16\r\nfrom diffusers import FluxPipeline, FluxTransformer2DModel\r\n \r\ndtype = torch.bfloat16\r\n\r\nflux_id = \"black-forest-labs/FLUX.1-dev\"\r\n\r\npipeline = FluxPipeline.from_pretrained(\r\n                flux_id,\r\n                transformer=model,\r\n                torch_dtype=dtype,\r\n            )\r\n\r\npipeline.vae.to(\"cuda\")\r\npipeline.text_encoder.to(\"cuda\")\r\npipeline.text_encoder_2.to(\"cuda\")\r\n\r\nprint(11)\r\n\r\nimage = pipeline(\r\n    \"A cat holding a sign that says hello world\",\r\n    num_inference_steps=28,\r\n    guidance_scale=3.5,\r\n).images[0]\r\n\r\nimage\r\n\r\n```\r\n\r\n</details>\r\n\r\nWhen I don't use flux ControlNet, everything is just fine. And the generation is pretty fast.\r\n\r\nHowever, when I use flux ControlNet, I only get a blank image. Nothing throws errors.\r\n\r\n<details>\r\n<summary> generate image using controlnet code detail </summary>\r\n\r\n```python\r\n\r\nfrom diffusers import FluxControlNetModel\r\n\r\ndel pipeline\r\ntorch.cuda.empty_cache()\r\n\r\n# Load pipeline\r\ncontrolnet = FluxControlNetModel.from_pretrained(\r\n  \"jasperai/Flux.1-dev-Controlnet-Depth\",\r\n  torch_dtype=torch.bfloat16\r\n)\r\n\r\nflux_id = \"black-forest-labs/FLUX.1-dev\"\r\n\r\npipeline = FluxControlNetPipeline.from_pretrained(\r\n                flux_id,\r\n                transformer=model,\r\n                torch_dtype=dtype,\r\n      controlnet=controlnet,\r\n\r\n            )\r\npipeline.controlnet.to(\"cuda\")\r\nprint(1)\r\n\r\npipeline.vae.to(\"cuda\")\r\npipeline.text_encoder.to(\"cuda\")\r\n\r\nprint(11)\r\n\r\nfrom diffusers.utils import load_image\r\n\r\n\r\ncontrol_image = load_image(\r\n  \"https://hf-mirror.com/jasperai/Flux.1-dev-Controlnet-Depth/resolve/main/examples/depth.jpg\"\r\n)\r\n\r\n\r\n\r\nprompt = \"a statue of a gnome in a field of purple tulips\"\r\n\r\nimage = pipeline(\r\n    prompt, \r\n    control_image=control_image,\r\n    controlnet_conditioning_scale=0.6,\r\n    num_inference_steps=28, \r\n    guidance_scale=3.5,\r\n    height=control_image.size[1],\r\n    width=control_image.size[0]\r\n).images[0]\r\nimage\r\n```\r\n</details>\r\n\r\nAny help ?", "choices": "(A) 1. Update MSVC to the latest version (Visual Studio 17.11).\n2. Ensure all submodules are updated by running `git submodule update`.\n3. Use the command `python -m build --no-isolation -v` to build the project.\n4. If building from source fails, try using the provided unofficial wheel file for installation.\n5. After installation, ensure to run the example script from a different directory to avoid module import issues. (B) 1. Upgrade CUDA to version 12.6 or later to support the c++20 standard used in the project.\n2. Update MSVC to the latest version (Visual Studio 17.11).\n3. Delete the submodules directory to save disk space.\n4. Ensure all submodules are updated by running `git submodule update`.\n5. Use the command `python -m build --no-isolation -v` to build the project.\n6. If building from source fails, try using the provided unofficial wheel file for installation.\n7. After installation, ensure to run the example script from a different directory to avoid module import issues. (C) 1. Upgrade CUDA to version 12.6 or later to support the c++20 standard used in the project.\n2. Update MSVC to the latest version (Visual Studio 17.11).\n3. Ensure all submodules are updated by running `git submodule update`.\n4. Downgrade CUDA to version 11.0 to avoid compatibility issues.\n5. Use the command `python -m build --no-isolation -v` to build the project.\n6. If building from source fails, try using the provided unofficial wheel file for installation.\n7. After installation, ensure to run the example script from a different directory to avoid module import issues. (D) 1. Upgrade CUDA to version 12.6 or later to support the c++20 standard used in the project.\n2. Ensure all submodules are updated by running `git submodule update`.\n3. Use the command `python -m build --no-isolation -v` to build the project.\n4. Update MSVC to the latest version (Visual Studio 17.11).\n5. If building from source fails, try using the provided unofficial wheel file for installation.\n6. After installation, ensure to run the example script from a different directory to avoid module import issues. (E) 1. Upgrade CUDA to version 12.6 or later to support the c++20 standard used in the project.\n2. Update MSVC to the latest version (Visual Studio 17.11).\n3. Use the command `python -m build --no-isolation -v` to build the project.\n4. If building from source fails, try using the provided unofficial wheel file for installation.\n5. After installation, ensure to run the example script from a different directory to avoid module import issues. (F) 1. Upgrade CUDA to version 12.6 or later to support the c++20 standard used in the project.\n2. Update MSVC to the latest version (Visual Studio 17.11).\n3. Use the command `python -m build --no-isolation -v` to build the project.\n4. Ensure all submodules are updated by running `git submodule update`.\n5. If building from source fails, try using the provided unofficial wheel file for installation.\n6. After installation, ensure to run the example script from a different directory to avoid module import issues. (G) 1. Upgrade CUDA to version 12.6 or later to support the c++20 standard used in the project.\n2. Update MSVC to the latest version (Visual Studio 17.11).\n3. Ensure all submodules are updated by running `git submodule update`.\n4. Use the command `python -m build --no-isolation -v` to build the project.\n5. If building from source fails, try using the provided unofficial wheel file for installation.\n6. After installation, ensure to run the example script from a different directory to avoid module import issues.", "answer": "G"}
{"uuid": "e11d466b-e747-497e-8040-5b990b52f1c5", "setup_instruct": "# Nunchaku Deployment Plan\n\n## 1. Prerequisites\n- **Hardware**: NVIDIA GPU (RTX 20-series or newer recommended)\n- **Software**: Python 3.8+, CUDA 12.1+, PyTorch 2.2+\n\n## 2. Installation\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/nunchaku-tech/nunchaku.git\n   cd nunchaku\n   ```\n\n2. **Install Dependencies**  \n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Install Nunchaku Engine**  \n   ```bash\n   pip install .\n   ```\n\n## 3. Verify Installation\n1. **Check GPU Compatibility**  \n   ```bash\n   python -c \"import nunchaku; print(nunchaku.check_gpu_compatibility())\"\n   ```\n\n2. **Run Basic Test**  \n   ```bash\n   python examples/flux.1-dev-basic.py\n   ```\n\n## 4. Model Deployment\n1. **Download Pre-Quantized Models**  \n   ```bash\n   huggingface-cli download nunchaku-tech/nunchaku-flux.1 --local-dir ./models\n   ```\n\n2. **Run Inference**  \n   ```bash\n   python examples/flux.1-dev-inference.py --model-path ./models/flux.1-4bit.safetensors\n   ```\n\n## 5. Advanced Features\n1. **Multi-LoRA Support**  \n   ```bash\n   python examples/flux.1-dev-multiple-lora.py --model-path ./models/flux.1-4bit.safetensors --lora-weights lora1.safetensors lora2.safetensors\n   ```\n\n2. **ControlNet Integration**  \n   ```bash\n   python examples/flux.1-dev-controlnet-union-pro.py --model-path ./models/flux.1-4bit.safetensors --controlnet-path ./controlnet/union-pro.safetensors\n   ```\n\n## 6. Performance Optimization\n1. **Enable FP16 Attention**  \n   Add `--use-fp16-attention` flag to inference scripts.\n\n2. **Activate First-Block Cache**  \n   Add `--enable-fb-cache` flag to inference scripts.\n\n## 7. Monitoring\n1. **Check Memory Usage**  \n   ```bash\n   nvidia-smi --query-gpu=memory.used --format=csv -l 1\n   ```\n\n2. **Profile Latency**  \n   ```bash\n   python -m cProfile -o profile_stats examples/flux.1-dev-basic.py\n   ```\n\n## 8. Troubleshooting\n- **CUDA Errors**: Verify CUDA version matches PyTorch requirements.\n- **Model Loading Issues**: Check file integrity with `safetensors` utility.\n- **Performance Drops**: Ensure no thermal throttling via `nvidia-smi -q -d PERFORMANCE`.", "issue_title": "Kernel error for running example.py", "issue_body": "Hi, thanks for this amazing work! When running the given example, I got the following error: `RuntimeError: CUDA error: no kernel image is available for execution on the device (at /data/nunchaku/src/kernels/awq/gemv_awq.cu:312)`. I have followed the instructions for installation using torch 2.4.1.", "choices": "(A) 1. Run `lshw -numeric -class display &> gpu-info` to get GPU information.\n2. Run `lstopo-no-graphics --of txt > lstopo.txt` to check PCIe topology.\n3. Install dependencies: `sudo apt install graphviz hwloc -y`.\n4. Run `hwloc-ls hwloc.png` for a graphical representation.\n5. Run `nvidia-smi -q > nvidia_smi_q.txt` for detailed GPU info.\n6. If PCIe gen3 is detected (as in this case), consider upgrading to a motherboard that supports PCIe gen4 for better bandwidth (24-26 GB/s).\n7. Ensure the GPU is plugged into a PCIe slot with 16 lanes for optimal performance.\n8. Re-run the bandwidth test (`bandwidthTest`) to verify improvements after making hardware changes. (B) 1. Check the PCIe generation and lanes using the following commands:\n   - Install dependencies: `sudo apt install graphviz hwloc -y`.\n   - Run `lshw -numeric -class display &> gpu-info` to get GPU information.\n   - Run `lstopo-no-graphics --of txt > lstopo.txt` to check PCIe topology.\n   - Run `hwloc-ls hwloc.png` for a graphical representation.\n   - Run `nvidia-smi -q > nvidia_smi_q.txt` for detailed GPU info.\n2. If PCIe gen3 is detected (as in this case), consider upgrading to a motherboard that supports PCIe gen4 for better bandwidth (24-26 GB/s).\n3. Manually set the PCIe slot to 8 lanes using `sudo setpci -s 01:00.0 CAP_EXP+0x10.W=0x8`.\n4. Ensure the GPU is plugged into a PCIe slot with 16 lanes for optimal performance.\n5. Re-run the bandwidth test (`bandwidthTest`) to verify improvements after making hardware changes. (C) 1. Check the PCIe generation and lanes using the following commands:\n   - Install dependencies: `sudo apt install graphviz hwloc -y`\n   - Run `lshw -numeric -class display &> gpu-info` to get GPU information.\n   - Run `lstopo-no-graphics --of txt > lstopo.txt` to check PCIe topology.\n   - Run `hwloc-ls hwloc.png` for a graphical representation.\n   - Run `nvidia-smi -q > nvidia_smi_q.txt` for detailed GPU info.\n2. If PCIe gen3 is detected (as in this case), consider upgrading to a motherboard that supports PCIe gen4 for better bandwidth (24-26 GB/s).\n3. Ensure the GPU is plugged into a PCIe slot with 16 lanes for optimal performance.\n4. Re-run the bandwidth test (`bandwidthTest`) to verify improvements after making hardware changes. (D) 1. Check the PCIe generation and lanes using the following commands:\n   - Install dependencies: `sudo apt install graphviz hwloc -y`.\n   - Run `lshw -numeric -class display &> gpu-info` to get GPU information.\n   - Run `lstopo-no-graphics --of txt > lstopo.txt` to check PCIe topology.\n   - Run `hwloc-ls hwloc.png` for a graphical representation.\n   - Run `nvidia-smi -q > nvidia_smi_q.txt` for detailed GPU info.\n2. If PCIe gen3 is detected (as in this case), consider upgrading to a motherboard that supports PCIe gen4 for better bandwidth (24-26 GB/s).\n3. Re-run the bandwidth test (`bandwidthTest`) to verify improvements after making hardware changes.", "answer": "C"}
{"uuid": "84f30f5a-da8c-46a3-9297-ccff13e52423", "setup_instruct": "# ShadowKV Deployment and Execution Plan\n\n## 1. Environment Setup\n### 1.1 Create Conda Environment\n```bash\nconda create -n ShadowKV python=3.10 -y\nconda activate ShadowKV\n```\n\n### 1.2 Install Core Packages\n```bash\npip install -r requirements.txt\npip install flash-attn --no-build-isolation\n```\n\n### 1.3 Install NeMo Toolkit Dependencies\n```bash\npip install wheel\npip install Cython\npip install youtokentome\npip install nemo_toolkit[all]==1.23\n```\n\n### 1.4 Install FlashInfer\n```bash\npip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.3/\n```\n\n### 1.5 Install CUTLASS\n```bash\nmkdir 3rdparty\ngit clone https://github.com/NVIDIA/cutlass.git 3rdparty/cutlass\n```\n\n### 1.6 Build ShadowKV Kernels\n```bash\npython setup.py build_ext --inplace\n```\n\n## 2. Dataset Preparation (RULER Benchmark)\n### 2.1 Download NLTK Data\n```bash\npython -c \"import nltk; nltk.download('punkt')\"\n```\n\n### 2.2 Build RULER Dataset\n```bash\ncd data/ruler\nbash create_dataset.sh \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\" \"llama-3\"\n```\n\n## 3. Accuracy Evaluation\n### 3.1 Full Attention Baseline (8xA100 GPUs)\n```bash\nOMP_NUM_THREADS=48 torchrun --standalone --nnodes=1 --nproc_per_node 8 test/eval_acc.py --datalen 131072 --method full --dataset_name \"ruler/niah_single_1,ruler/niah_single_2,ruler/niah_single_3,ruler/niah_multikey_1,ruler/niah_multikey_2,ruler/niah_multiquery,ruler/niah_multivalue,ruler/vt,ruler/fwe,ruler/qa_1,ruler/qa_2\" --model_name \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\"\n```\n\n### 3.2 ShadowKV Evaluation (8xA100 GPUs)\n```bash\nOMP_NUM_THREADS=48 torchrun --standalone --nnodes=1 --nproc_per_node 8 test/eval_acc.py --datalen 131072 --method shadowkv --dataset_name \"ruler/niah_single_1,ruler/niah_single_2,ruler/niah_single_3,ruler/niah_multikey_1,ruler/niah_multikey_2,ruler/niah_multiquery,ruler/niah_multivalue,ruler/vt,ruler/fwe,ruler/qa_1,ruler/qa_2\" --sparse_budget 2048 --rank 160 --chunk_size 8\n```\n\n### 3.3 MInference Integration (Optional)\n#### Full Attention with MInference\n```bash\nOMP_NUM_THREADS=48 torchrun --standalone --nnodes=1 --nproc_per_node 8 test/eval_acc.py --datalen 131072 --method full --dataset_name \"ruler/niah_single_1,ruler/niah_single_2,ruler/niah_single_3,ruler/niah_multikey_1,ruler/niah_multikey_2,ruler/niah_multiquery,ruler/niah_multivalue,ruler/vt,ruler/fwe,ruler/qa_1,ruler/qa_2\" --minference\n```\n\n#### ShadowKV with MInference\n```bash\nOMP_NUM_THREADS=48 torchrun --standalone --nnodes=1 --nproc_per_node 8 test/eval_acc.py --datalen 131072 --method shadowkv --dataset_name \"ruler/niah_single_1,ruler/niah_single_2,ruler/niah_single_3,ruler/niah_multikey_1,ruler/niah_multikey_2,ruler/niah_multiquery,ruler/niah_multivalue,ruler/vt,ruler/fwe,ruler/qa_1,ruler/qa_2\" --sparse_budget 2048 --rank 160 --chunk_size 8 --minference\n```\n\n## 4. Efficiency Evaluation (Single A100 GPU)\n```bash\npython test/e2e.py --model_name \"meta-llama/Meta-Llama-3.1-8B-Instruct\" --datalen \"122k\"\n```", "issue_title": "[QUESTION] Reproduction of Long bench Dataset", "issue_body": "Hi,\r\n\r\nThank you for your great work. I am trying to reproduce the results of the LongBench datasets reported in your paper. I used the meta-llama/Llama-3.1-8B-Instruct model and tested the hotpotqa and multifieldqa_en datasets. I used full attention without any additional strategies.\r\n\r\nWhen I evaluated the results using the Longbench dataset's evaluation method, I noticed the following discrepancies:\r\n\r\nFor hotpotqa and multifieldqa_en, the results using qa_f1_score were 16.1827 and 28.6904, respectively.\r\nHowever, when using your string_match_part metric for evaluation, the results were 57.4359 and 16.3636, respectively.\r\nThe results for hotpotqa seem to align with the ones reported in the paper, but for multifieldqa_en, they do not match. Could you clarify what might be causing this discrepancy? Specifically, could you provide detailed information on the evaluation methods you used for each dataset in your paper?\r\n\r\nBest regards,\r\nLucas\r\n\r\n![Screenshot from 2025-01-14 11-14-53](https://github.com/user-attachments/assets/58fd9fd0-b6e5-48fd-be43-57950ffbc364)\r\n![Screenshot from 2025-01-14 11-10-25](https://github.com/user-attachments/assets/8ee8f1df-6744-4647-953b-c86d105ee5ad)\r\n![Screenshot from 2025-01-14 11-05-54](https://github.com/user-attachments/assets/b2fb5232-b2ef-4f00-aa08-9812a99337cf)\r\n![Screenshot from 2025-01-14 11-01-38](https://github.com/user-attachments/assets/838930e3-e0f4-44a6-be6e-7f790cf53bdb)", "choices": "(A) 1. Upgrade the CUDA version to ensure compatibility with selective_scan_cuda_oflex. 2. Reconfigure the environment by following the author's requirements.txt strictly. 3. Verify the installation of selective_scan and ensure it is optimized for the current CUDA version. (B) 1. Delete the requirements.txt file to avoid conflicts. 2. Reconfigure the environment by following the author's requirements.txt strictly. 3. Upgrade the CUDA version to ensure compatibility with selective_scan_cuda_oflex. 4. Verify the installation of selective_scan and ensure it is optimized for the current CUDA version. (C) 1. Reconfigure the environment by following the author's requirements.txt strictly. 2. Verify the installation of selective_scan and ensure it is optimized for the current CUDA version. 3. Upgrade the CUDA version to ensure compatibility with selective_scan_cuda_oflex. (D) 1. Reconfigure the environment by following the author's requirements.txt strictly. 2. Downgrade the CUDA version to an older, incompatible version. 3. Upgrade the CUDA version to ensure compatibility with selective_scan_cuda_oflex. 4. Verify the installation of selective_scan and ensure it is optimized for the current CUDA version. (E) 1. Reconfigure the environment by following the author's requirements.txt strictly. 2. Verify the installation of selective_scan and ensure it is optimized for the current CUDA version. (F) 1. Reconfigure the environment by following the author's requirements.txt strictly. 2. Upgrade the CUDA version to ensure compatibility with selective_scan_cuda_oflex. (G) 1. Reconfigure the environment by following the author's requirements.txt strictly. 2. Upgrade the CUDA version to ensure compatibility with selective_scan_cuda_oflex. 3. Verify the installation of selective_scan and ensure it is optimized for the current CUDA version.", "answer": "G"}
{"uuid": "2d5a45df-9c39-44fa-8ab5-24116068f616", "setup_instruct": "# ShadowKV Deployment and Execution Plan\n\n## 1. Environment Setup\n### 1.1 Create Conda Environment\n```bash\nconda create -n ShadowKV python=3.10 -y\nconda activate ShadowKV\n```\n\n### 1.2 Install Core Dependencies\n```bash\npip install -r requirements.txt\npip install flash-attn --no-build-isolation\n```\n\n### 1.3 Install NeMo Toolkit Dependencies\n```bash\npip install wheel\npip install Cython\npip install youtokentome\npip install nemo_toolkit[all]==1.23\n```\n\n### 1.4 Install FlashInfer\n```bash\npip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.3/\n```\n\n### 1.5 Clone CUTLASS\n```bash\nmkdir 3rdparty\ngit clone https://github.com/NVIDIA/cutlass.git 3rdparty/cutlass\n```\n\n### 1.6 Build ShadowKV Kernels\n```bash\npython setup.py build_ext --inplace\n```\n\n## 2. Dataset Preparation (RULER Benchmark)\n### 2.1 Download NLTK Data\n```bash\npython -c \"import nltk; nltk.download('punkt')\"\n```\n\n### 2.2 Build RULER Dataset\n```bash\ncd data/ruler\nbash create_dataset.sh \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\" \"llama-3\"\n```\n\n## 3. Accuracy Evaluation\n### 3.1 Full Attention Baseline (8 GPUs)\n```bash\nOMP_NUM_THREADS=48 torchrun --standalone --nnodes=1 --nproc_per_node 8 test/eval_acc.py --datalen 131072 --method full --dataset_name \"ruler/niah_single_1,ruler/niah_single_2,ruler/niah_single_3,ruler/niah_multikey_1,ruler/niah_multikey_2,ruler/niah_multiquery,ruler/niah_multivalue,ruler/vt,ruler/fwe,ruler/qa_1,ruler/qa_2\" --model_name \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\"\n```\n\n### 3.2 ShadowKV Evaluation (8 GPUs)\n```bash\nOMP_NUM_THREADS=48 torchrun --standalone --nnodes=1 --nproc_per_node 8 test/eval_acc.py --datalen 131072 --method shadowkv --dataset_name \"ruler/niah_single_1,ruler/niah_single_2,ruler/niah_single_3,ruler/niah_multikey_1,ruler/niah_multikey_2,ruler/niah_multiquery,ruler/niah_multivalue,ruler/vt,ruler/fwe,ruler/qa_1,ruler/qa_2\" --sparse_budget 2048 --rank 160 --chunk_size 8\n```\n\n### 3.3 MInference Integration (Optional)\n#### Full Attention with MInference:\n```bash\nOMP_NUM_THREADS=48 torchrun --standalone --nnodes=1 --nproc_per_node 8 test/eval_acc.py --datalen 131072 --method full --dataset_name \"ruler/niah_single_1,ruler/niah_single_2,ruler/niah_single_3,ruler/niah_multikey_1,ruler/niah_multikey_2,ruler/niah_multiquery,ruler/niah_multivalue,ruler/vt,ruler/fwe,ruler/qa_1,ruler/qa_2\" --minference\n```\n\n#### ShadowKV with MInference:\n```bash\nOMP_NUM_THREADS=48 torchrun --standalone --nnodes=1 --nproc_per_node 8 test/eval_acc.py --datalen 131072 --method shadowkv --dataset_name \"ruler/niah_single_1,ruler/niah_single_2,ruler/niah_single_3,ruler/niah_multikey_1,ruler/niah_multikey_2,ruler/niah_multiquery,ruler/niah_multivalue,ruler/vt,ruler/fwe,ruler/qa_1,ruler/qa_2\" --sparse_budget 2048 --rank 160 --chunk_size 8 --minference\n```\n\n## 4. Efficiency Evaluation (Single GPU)\n```bash\npython test/e2e.py --model_name \"meta-llama/Meta-Llama-3.1-8B-Instruct\" --datalen \"122k\"\n```\n\n## 5. Citation\nAdd the following BibTeX entry to your publications:\n```bibtex\n@article{sun2024shadowkv,\n  title={ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference},\n  author={Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi},\n  journal={arXiv preprint arXiv:2410.21465},\n  year={2024}\n}\n```", "issue_title": "[QUESTION] Request for Baseline Code for Experiment Reproduction", "issue_body": "Hi, thank you very much for your excellent work and open sourcing the code. \r\nI noticed that in the experimental section of ShadowKV paper, you provided results for several baselines, such as Quest and Loki. However, I wasn't able to find the corresponding code for these baselines in the current repository. I want to reproduce some results of these baseline, would you be able to provide the code used for the baselines mentioned in the paper, or perhaps guide me on how to implement them?\r\nThank you!", "choices": "(A) 1. Install the 'mamba-ssm' package if not already installed.\n2. Locate the 'vmamba.py' file in your project.\n3. Replace all instances of the string \"core\" with \"oflex\" in the 'vmamba.py' file.\n4. Ensure that the 'selective_scan_backend' parameter is set to \"core\" in any relevant function calls or configurations. (B) 1. Install the 'mamba-ssm' package if not already installed.\n2. Locate the 'vmamba.py' file in your project.\n3. Ensure that the 'selective_scan_backend' parameter is set to \"oflex\" in any relevant function calls or configurations. (C) 1. Install the 'mamba-ssm' package if not already installed.\n2. Locate the 'vmamba.py' file in your project.\n3. Replace all instances of the string \"core\" with \"oflex\" in the 'vmamba.py' file. (D) 1. Install the 'mamba-ssm' package if not already installed.\n2. Locate the 'vmamba.py' file in your project.\n3. Replace all instances of the string \"core\" with \"oflex\" in the 'vmamba.py' file.\n4. Ensure that the 'selective_scan_backend' parameter is set to \"oflex\" in any relevant function calls or configurations. (E) 1. Install the 'mamba-ssm' package if not already installed.\n2. Locate the 'vmamba.py' file in your project.\n3. Replace all instances of the string \"core\" with \"oflex\" in the 'vmamba.py' file.\n4. Revert all changes made to the 'vmamba.py' file.\n5. Ensure that the 'selective_scan_backend' parameter is set to \"oflex\" in any relevant function calls or configurations. (F) 1. Install the 'mamba-ssm' package if not already installed.\n2. Locate the 'vmamba.py' file in your project.\n3. Ensure that the 'selective_scan_backend' parameter is set to \"oflex\" in any relevant function calls or configurations.\n4. Replace all instances of the string \"core\" with \"oflex\" in the 'vmamba.py' file. (G) 1. Install the 'mamba-ssm' package if not already installed.\n2. Replace all instances of the string \"core\" with \"oflex\" in the 'vmamba.py' file.\n3. Locate the 'vmamba.py' file in your project.\n4. Ensure that the 'selective_scan_backend' parameter is set to \"oflex\" in any relevant function calls or configurations.", "answer": "D"}
{"uuid": "08b4640c-3dba-43fb-bae1-784595211df7", "setup_instruct": "# VMamba Execution Plan\n\n## 1. Environment Setup\n**Objective**: Create a conda environment and install required dependencies.\n\n```bash\nconda create -n vmamba python=3.10\nconda activate vmamba\npip install torch==2.2 torchvision torchaudio triton pytest chardet yacs termcolor fvcore seaborn packaging ninja einops numpy==1.24.4 timm==0.4.12\npip install https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.2cxx11abiTRUE-cp310-cp310-linux_x86_64.whl\n```\n\n## 2. Clone Repository\n**Objective**: Get the VMamba source code.\n\n```bash\ngit clone https://github.com/MzeroMiko/VMamba.git\ncd VMamba\n```\n\n## 3. Install Additional Dependencies\n**Objective**: Install project-specific requirements and selective scan kernel.\n\n```bash\npip install -r requirements.txt\ncd kernels/selective_scan && pip install .\n```\n\n## 4. (Optional) Verify Selective Scan Implementation\n**Objective**: Validate selective scan module against reference implementations.\n\n```bash\n# Edit selective_scan/test_selective_scan.py to set MODE = \"mamba_ssm_sscore\" or \"sscore\"\npytest selective_scan/test_selective_scan.py\n```\n\n## 5. Run VMamba\n**Objective**: Execute the main VMamba script.\n\n```bash\npython vmamba.py\n```\n\n## 6. (Optional) Install Detection/Segmentation Dependencies\n**Objective**: Set up environment for downstream tasks.\n\n```bash\npip install mmengine==0.10.1 mmcv==2.1.0 opencv-python-headless ftfy regex\npip install mmdet==3.3.0 mmsegmentation==1.2.2 mmpretrain==1.2.0\n```\n\n## 7. Training/Inference Commands\n**Objective**: Run model training or evaluation.\n\n### Classification\n```bash\n# Training\npython -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=8 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg <config_path> --batch-size 128 --data-path <dataset_path> --output /tmp\n\n# Inference\npython -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=1 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg <config_path> --batch-size 128 --data-path <dataset_path> --output /tmp --pretrained <checkpoint_path>\n```\n\n### Detection/Segmentation\n```bash\n# Evaluation\nbash ./tools/dist_test.sh <config_path> <checkpoint_path> 1\n\n# Training\nbash ./tools/dist_train.sh <config_path> 8\n```\n\n## 8. Analysis Tools\n**Objective**: Run visualization and performance analysis.\n\n```bash\n# Attention visualization\nCUDA_VISIBLE_DEVICES=0 python analyze/attnmap.py\n\n# Effective receptive field analysis\nCUDA_VISIBLE_DEVICES=0 python analyze/erf.py\n\n# Throughput analysis\nCUDA_VISIBLE_DEVICES=0 python analyze/tp.py\n```", "issue_title": "Clarification on Segmentation Results ?", "issue_body": "Can you share more details about your segmentation experiments ? \r\n\r\nI am interested in VMamba-T[s1l8] for example. The config [here](https://github.com/MzeroMiko/VMamba/blob/main/segmentation/configs/vssm1/upernet_vssm_4xb4-160k_ade20k-512x512_tiny.py) says nothing about the learning rate, batch size, etc. as one may assume it is the same as Swin-T [config](https://github.com/MzeroMiko/VMamba/blob/main/segmentation/configs/swin/swin-tiny-patch4-window7-in1k-pre_upernet_8xb2-160k_ade20k-512x512.py). \r\n\r\nBut this just makes no sense as VMamba-T significantly outperforms Swin even from the first 16k iterations. \r\n\r\nSee below for example:\r\n\r\nHere's the performance of Swin-T:\r\nmIoU: 0.3591\r\n{\"mode\": \"val\", \"epoch\": 13, \"iter\": 16000, \"lr\": 5e-05, \"mIoU\": 0.3591, \"mAcc\": 0.4795, \"aAcc\": 0.778}\r\n\r\nHere's the performance of VMamba-T at the same 16000 iterations:\r\nmIoU: 42.0500  mAcc: 54.8000 \r\n\r\nHow can it be almost +6.14 mIoU better in the first 16000 ?\r\n\r\nIf you use the same exact training procedure, then performance should be comparable ?", "choices": "(A) 1. Check the batch size used during validation in your configuration.\n2. Randomize the batch size to a value between 1 and 100.\n3. Adjust the batch size to match the one used in the original setup (as indicated by the maintainer's logs).\n4. Re-run the validation to ensure consistent results with the expected performance metrics. (B) 1. Check the batch size used during validation in your configuration.\n2. Re-run the validation to ensure consistent results with the expected performance metrics. (C) 1. Check the batch size used during validation in your configuration.\n2. Adjust the batch size to match the one used in the original setup (as indicated by the maintainer's logs).\n3. Re-run the validation to ensure consistent results with the expected performance metrics. (D) 1. Adjust the batch size to match the one used in the original setup (as indicated by the maintainer's logs).\n2. Re-run the validation to ensure consistent results with the expected performance metrics. (E) 1. Check the batch size used during validation in your configuration.\n2. Adjust the batch size to match the one used in the original setup (as indicated by the maintainer's logs).\n3. Skip re-running the validation to save time. (F) 1. Adjust the batch size to match the one used in the original setup (as indicated by the maintainer's logs).\n2. Check the batch size used during validation in your configuration.\n3. Re-run the validation to ensure consistent results with the expected performance metrics. (G) 1. Check the batch size used during validation in your configuration.\n2. Re-run the validation to ensure consistent results with the expected performance metrics.\n3. Adjust the batch size to match the one used in the original setup (as indicated by the maintainer's logs).", "answer": "C"}
{"uuid": "881ca7b0-7c95-4cf6-8179-637ee72b804f", "setup_instruct": "# VMamba Deployment Plan\n\n## 1. Environment Setup\n- **Description**: Create a conda environment and install required dependencies\n- **Commands**:\n  ```bash\n  conda create -n vmamba python=3.10\n  conda activate vmamba\n  pip install torch==2.2 torchvision torchaudio triton pytest chardet yacs termcolor fvcore seaborn packaging ninja einops numpy==1.24.4 timm==0.4.12\n  pip install https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.2cxx11abiTRUE-cp310-cp310-linux_x86_64.whl\n  ```\n\n## 2. Repository Setup\n- **Description**: Clone the VMamba repository and navigate to project directory\n- **Commands**:\n  ```bash\n  git clone https://github.com/MzeroMiko/VMamba.git\n  cd VMamba\n  ```\n\n## 3. Install Additional Dependencies\n- **Description**: Install project-specific requirements and selective scan kernel\n- **Commands**:\n  ```bash\n  pip install -r requirements.txt\n  cd kernels/selective_scan && pip install .\n  ```\n\n## 4. Optional Verification\n- **Description**: Verify selective scan implementation (optional)\n- **Commands**:\n  ```bash\n  # Edit selective_scan/test_selective_scan.py to set MODE as needed\n  pytest selective_scan/test_selective_scan.py\n  ```\n\n## 5. Task-Specific Setup\n- **Description**: Install additional dependencies for detection/segmentation tasks\n- **Commands**:\n  ```bash\n  pip install mmengine==0.10.1 mmcv==2.1.0 opencv-python-headless ftfy regex\n  pip install mmdet==3.3.0 mmsegmentation==1.2.2 mmpretrain==1.2.0\n  ```\n\n## 6. Model Execution\n- **Description**: Run the VMamba model\n- **Commands**:\n  ```bash\n  python vmamba.py\n  ```\n\n## 7. Training (Classification)\n- **Description**: Train VMamba models on ImageNet\n- **Commands**:\n  ```bash\n  python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=8 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg <config_path> --batch-size 128 --data-path <dataset_path> --output /tmp\n  ```\n\n## 8. Evaluation (Detection/Segmentation)\n- **Description**: Evaluate models using mmdetection/mmsegmentation\n- **Commands**:\n  ```bash\n  bash ./tools/dist_test.sh <config_path> <checkpoint_path> 1\n  # For training:\n  bash ./tools/dist_train.sh <config_path> 8\n  ```\n\n## 9. Analysis Tools\n- **Description**: Run various analysis tools\n- **Commands**:\n  ```bash\n  # Attention visualization\n  CUDA_VISIBLE_DEVICES=0 python analyze/attnmap.py\n  \n  # Effective receptive field analysis\n  CUDA_VISIBLE_DEVICES=0 python analyze/erf.py\n  \n  # Throughput analysis\n  CUDA_VISIBLE_DEVICES=0 python analyze/tp.py\n  ```", "issue_title": "Can not import selective_scan_cuda_oflex. This affects speed.", "issue_body": "Hello, this is a great job! I am now using vmamba as backbone on other models, but I get the following warning message, Can not import selective_scan_cuda_oflex. This affects speed. So what should I do and expect your reply", "choices": "(A) 1. Enable `force_fp32=True` in the AMP (Automatic Mixed Precision) configuration to ensure certain parts of the data remain in float32 during training and inference.\n2. If `inf` values persist, consider adjusting the loss scale in AMP to prevent the gradient from becoming `nan`.\n3. Monitor the training logs for any occurrences of `inf` or `nan` values in the gradient or loss.\n4. Verify that the training does not collapse and that the inference results are consistent with `force_fp32` set to both `True` and `False`. (B) 1. Enable `force_fp32=True` in the AMP (Automatic Mixed Precision) configuration to ensure certain parts of the data remain in float32 during training and inference.\n2. Disable AMP entirely to avoid mixed precision issues.\n3. Monitor the training logs for any occurrences of `inf` or `nan` values in the gradient or loss.\n4. If `inf` values persist, consider adjusting the loss scale in AMP to prevent the gradient from becoming `nan`.\n5. Verify that the training does not collapse and that the inference results are consistent with `force_fp32` set to both `True` and `False`. (C) 1. Verify that the training does not collapse and that the inference results are consistent with `force_fp32` set to both `True` and `False`.\n2. Enable `force_fp32=True` in the AMP (Automatic Mixed Precision) configuration to ensure certain parts of the data remain in float32 during training and inference.\n3. Monitor the training logs for any occurrences of `inf` or `nan` values in the gradient or loss.\n4. If `inf` values persist, consider adjusting the loss scale in AMP to prevent the gradient from becoming `nan`. (D) 1. Enable `force_fp32=True` in the AMP (Automatic Mixed Precision) configuration to ensure certain parts of the data remain in float32 during training and inference.\n2. If `inf` values persist, consider adjusting the loss scale in AMP to prevent the gradient from becoming `nan`.\n3. Verify that the training does not collapse and that the inference results are consistent with `force_fp32` set to both `True` and `False`. (E) 1. Enable `force_fp32=True` in the AMP (Automatic Mixed Precision) configuration to ensure certain parts of the data remain in float32 during training and inference.\n2. Monitor the training logs for any occurrences of `inf` or `nan` values in the gradient or loss.\n3. If `inf` values persist, consider adjusting the loss scale in AMP to prevent the gradient from becoming `nan`.\n4. Verify that the training does not collapse and that the inference results are consistent with `force_fp32` set to both `True` and `False`. (F) 1. Enable `force_fp32=True` in the AMP (Automatic Mixed Precision) configuration to ensure certain parts of the data remain in float32 during training and inference.\n2. Force all data to float16 to maximize performance.\n3. Monitor the training logs for any occurrences of `inf` or `nan` values in the gradient or loss.\n4. If `inf` values persist, consider adjusting the loss scale in AMP to prevent the gradient from becoming `nan`.\n5. Verify that the training does not collapse and that the inference results are consistent with `force_fp32` set to both `True` and `False`. (G) 1. Enable `force_fp32=True` in the AMP (Automatic Mixed Precision) configuration to ensure certain parts of the data remain in float32 during training and inference.\n2. Monitor the training logs for any occurrences of `inf` or `nan` values in the gradient or loss.\n3. If `inf` values persist, consider adjusting the loss scale in AMP to prevent the gradient from becoming `nan`.", "answer": "E"}
{"uuid": "6489e434-ed3e-4c11-a491-383735c99e90", "setup_instruct": "# VMamba Deployment Plan\n\n## 1. Environment Setup\n**Objective**: Create a conda environment and install required dependencies.\n\n```bash\nconda create -n vmamba python=3.10\nconda activate vmamba\npip install torch==2.2 torchvision torchaudio triton pytest chardet yacs termcolor fvcore seaborn packaging ninja einops numpy==1.24.4 timm==0.4.12\npip install https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.2cxx11abiTRUE-cp310-cp310-linux_x86_64.whl\n```\n\n## 2. Clone Repository\n**Objective**: Download the VMamba source code.\n\n```bash\ngit clone https://github.com/MzeroMiko/VMamba.git\ncd VMamba\n```\n\n## 3. Install Additional Dependencies\n**Objective**: Install project-specific requirements and selective scan kernel.\n\n```bash\npip install -r requirements.txt\ncd kernels/selective_scan && pip install .\n```\n\n## 4. (Optional) Verify Selective Scan Implementation\n**Objective**: Validate the selective scan module against reference implementations.\n\n```bash\n# For mamba_ssm comparison\npytest selective_scan/test_selective_scan.py -v -k \"MODE = 'mamba_ssm_sscore'\"\n\n# For reference implementation comparison\npytest selective_scan/test_selective_scan.py -v -k \"MODE = 'sscore'\"\n```\n\n## 5. Install Task-Specific Dependencies (Optional)\n**Objective**: Add packages for detection/segmentation tasks.\n\n```bash\npip install mmengine==0.10.1 mmcv==2.1.0 opencv-python-headless ftfy regex\npip install mmdet==3.3.0 mmsegmentation==1.2.2 mmpretrain==1.2.0\n```\n\n## 6. Run Basic Test\n**Objective**: Execute the minimal test script.\n\n```bash\npython vmamba.py\n```\n\n## 7. Model Training (Classification Example)\n**Objective**: Train a VMamba model on ImageNet.\n\n```bash\npython -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=8 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg classification/configs/vssm/vmambav2_small_224.yaml --batch-size 128 --data-path /path/to/imagenet --output /tmp\n```\n\n## 8. Model Inference (Classification Example)\n**Objective**: Evaluate a pretrained model.\n\n```bash\npython -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=1 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg classification/configs/vssm/vmambav2_small_224.yaml --batch-size 128 --data-path /path/to/imagenet --output /tmp --pretrained /path/to/checkpoint.pth\n```\n\n## 9. Analysis Tools\n**Objective**: Run diagnostic utilities.\n\n```bash\n# Visualize attention maps\nCUDA_VISIBLE_DEVICES=0 python analyze/attnmap.py\n\n# Analyze effective receptive field\nCUDA_VISIBLE_DEVICES=0 python analyze/erf.py\n\n# Benchmark throughput\nCUDA_VISIBLE_DEVICES=0 python analyze/tp.py\n```", "issue_title": "Can you help me solve this error?", "issue_body": "I'm really confused here, where exactly did it go wrong?\r\nrank0]:   File \"/root/VMamba/classification/models/vmamba.py\", line 509, in forward_corev2\r\n[rank0]:     assert selective_scan_backend in [None, \"oflex\", \"mamba\", \"torch\"]\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: AssertionError\r\nE0720 18:17:46.647000 139799760049984 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 7751) of binary: /root/miniconda3/bin/python3", "choices": "(A) 1. Ignore the unsupported operators (aten::mul, aten::mul_, aten::add, aten::gelu) as they are considered negligible in FLOPs counting by `fvcore`.\n2. Directly apply weights (e.g., self.dt_proj.weight) instead of creating and deleting modules to avoid tracing issues.\n3. Note that drop_path is set to 0 during testing, so it does not contribute to FLOPs in this phase. (B) 1. Ignore the unsupported operators (aten::mul, aten::mul_, aten::add, aten::gelu) as they are considered negligible in FLOPs counting by `fvcore`.\n2. For modules not being called during the trace (e.g., ssm.SSM_b.dt_proj, ssm.SSM_b.x_proj, ssm.SSM_f.dt_proj, ssm.SSM_f.x_proj), ensure they are properly initialized and used if they are intended to contribute to the model's FLOPs.\n3. Directly apply weights (e.g., self.dt_proj.weight) instead of creating and deleting modules to avoid tracing issues.\n4. Delete the applied weights to free memory.\n5. Note that drop_path is set to 0 during testing, so it does not contribute to FLOPs in this phase. (C) 1. Ignore the unsupported operators (aten::mul, aten::mul_, aten::add, aten::gelu) as they are considered negligible in FLOPs counting by `fvcore`.\n2. Note that drop_path is set to 0 during testing, so it does not contribute to FLOPs in this phase. (D) 1. Ignore the unsupported operators (aten::mul, aten::mul_, aten::add, aten::gelu) as they are considered negligible in FLOPs counting by `fvcore`.\n2. For modules not being called during the trace (e.g., ssm.SSM_b.dt_proj, ssm.SSM_b.x_proj, ssm.SSM_f.dt_proj, ssm.SSM_f.x_proj), ensure they are properly initialized and used if they are intended to contribute to the model's FLOPs.\n3. Directly apply weights (e.g., self.dt_proj.weight) instead of creating and deleting modules to avoid tracing issues.\n4. Note that drop_path is set to 0 during testing, so it does not contribute to FLOPs in this phase. (E) 1. Ignore the unsupported operators (aten::mul, aten::mul_, aten::add, aten::gelu) as they are considered negligible in FLOPs counting by `fvcore`.\n2. Directly apply weights (e.g., self.dt_proj.weight) instead of creating and deleting modules to avoid tracing issues.\n3. For modules not being called during the trace (e.g., ssm.SSM_b.dt_proj, ssm.SSM_b.x_proj, ssm.SSM_f.dt_proj, ssm.SSM_f.x_proj), ensure they are properly initialized and used if they are intended to contribute to the model's FLOPs.\n4. Note that drop_path is set to 0 during testing, so it does not contribute to FLOPs in this phase. (F) 1. Ignore the unsupported operators (aten::mul, aten::mul_, aten::add, aten::gelu) as they are considered negligible in FLOPs counting by `fvcore`.\n2. For modules not being called during the trace (e.g., ssm.SSM_b.dt_proj, ssm.SSM_b.x_proj, ssm.SSM_f.dt_proj, ssm.SSM_f.x_proj), ensure they are properly initialized and used if they are intended to contribute to the model's FLOPs.\n3. Directly apply weights (e.g., self.dt_proj.weight) instead of creating and deleting modules to avoid tracing issues.\n4. Enable drop_path during testing to measure its impact on FLOPs.\n5. Note that drop_path is set to 0 during testing, so it does not contribute to FLOPs in this phase. (G) 1. Note that drop_path is set to 0 during testing, so it does not contribute to FLOPs in this phase.\n2. Ignore the unsupported operators (aten::mul, aten::mul_, aten::add, aten::gelu) as they are considered negligible in FLOPs counting by `fvcore`.\n3. For modules not being called during the trace (e.g., ssm.SSM_b.dt_proj, ssm.SSM_b.x_proj, ssm.SSM_f.dt_proj, ssm.SSM_f.x_proj), ensure they are properly initialized and used if they are intended to contribute to the model's FLOPs.\n4. Directly apply weights (e.g., self.dt_proj.weight) instead of creating and deleting modules to avoid tracing issues.", "answer": "D"}
{"uuid": "31456c82-86b6-462f-9cfb-c64d9bfd367b", "setup_instruct": "# VMamba Deployment Plan\n\n## 🔥 Quick Start (Minimal Setup)\n1. **Create Conda Environment**  \n   Create and activate a new Conda environment with Python 3.10.  \n   ```bash\n   conda create -n vmamba python=3.10\n   conda activate vmamba\n   ```\n\n2. **Install Core Dependencies**  \n   Install PyTorch, Mamba-SSM, and other required packages.  \n   ```bash\n   pip install torch==2.2 torchvision torchaudio triton pytest chardet yacs termcolor fvcore seaborn packaging ninja einops numpy==1.24.4 timm==0.4.12\n   pip install https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.2cxx11abiTRUE-cp310-cp310-linux_x86_64.whl\n   ```\n\n3. **Run VMamba**  \n   Execute the main script to verify the setup.  \n   ```bash\n   python vmamba.py\n   ```\n\n---\n\n## Full Installation & Setup\n\n### **1. Clone Repository**  \nClone the VMamba repository and navigate to the project directory.  \n```bash\ngit clone https://github.com/MzeroMiko/VMamba.git\ncd VMamba\n```\n\n### **2. Environment Setup**  \n- **Create Conda Environment**  \n  ```bash\n  conda create -n vmamba\n  conda activate vmamba\n  ```\n\n- **Install Dependencies**  \n  Install packages listed in `requirements.txt` and compile the selective scan kernel.  \n  ```bash\n  pip install -r requirements.txt\n  cd kernels/selective_scan && pip install .\n  ```\n\n- **(Optional) Validate Selective Scan**  \n  Run tests to verify the selective scan implementation.  \n  ```bash\n  pytest selective_scan/test_selective_scan.py\n  ```\n\n### **3. Task-Specific Setup**  \n#### **Detection & Segmentation**  \nInstall additional dependencies for MMDetection/MMSegmentation.  \n```bash\npip install mmengine==0.10.1 mmcv==2.1.0 opencv-python-headless ftfy regex\npip install mmdet==3.3.0 mmsegmentation==1.2.2 mmpretrain==1.2.0\n```\n\n---\n\n## **Execution Commands**\n\n### **1. Classification (ImageNet)**  \n- **Training**  \n  ```bash\n  python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=8 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg <config_path> --batch-size 128 --data-path <dataset_path> --output /tmp\n  ```\n\n- **Inference**  \n  ```bash\n  python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=1 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg <config_path> --batch-size 128 --data-path <dataset_path> --output /tmp --pretrained <checkpoint_path>\n  ```\n\n### **2. Detection & Segmentation**  \n- **Evaluation**  \n  ```bash\n  bash ./tools/dist_test.sh <config_path> <checkpoint_path> 1\n  ```\n\n- **Training**  \n  ```bash\n  bash ./tools/dist_train.sh <config_path> 8\n  ```\n\n### **3. Analysis Tools**  \n- **Visualize Attention Maps**  \n  ```bash\n  CUDA_VISIBLE_DEVICES=0 python analyze/attnmap.py\n  ```\n\n- **Effective Receptive Field**  \n  ```bash\n  CUDA_VISIBLE_DEVICES=0 python analyze/erf.py\n  ```\n\n- **Throughput Analysis**  \n  ```bash\n  CUDA_VISIBLE_DEVICES=0 python analyze/tp.py\n  ```\n\n---\n\n## **Notes**  \n- For CUDA-related slowness, disable `torch.backends.cudnn.enabled` in `vmamba.py`.  \n- Model checkpoints and configs are linked in the [performance tables](#main-results).  \n- Refer to [`modelcard.sh`](./modelcard.sh) for advanced training scripts.", "issue_title": "pretrained weights reload error.", "issue_body": "when I load pretraind weights of vmamba/vssm1_tiny_0230s_ckpt_epoch_264.pth in mmdetction, it shows:\r\n\r\n```\r\nSuccessfully load ckpt /checkpoints/vmamba/vssm1_tiny_0230s_ckpt_epoch_264.pth\r\nFailed loading checkpoint form /checkpoints/vmamba/vssm1_tiny_0230s_ckpt_epoch_264.pth: 'layers.2.blocks.8.op.in_proj.weight'\r\n```\r\n\r\nwhen I check in file of 'vmamba.py':\r\n```\r\n        try:\r\n            _ckpt = torch.load(open(ckpt, \"rb\"), map_location=torch.device(\"cpu\"))\r\n            print(f\"Successfully load ckpt {ckpt}\")\r\n            incompatibleKeys = self.load_state_dict(_ckpt[key], strict=False)\r\n            print(incompatibleKeys)\r\n        except Exception as e:\r\n            print(f\"Failed loading checkpoint form {ckpt}: {e}\")\r\n```\r\n\r\ndose this mean that loading pretraind weights fails, that is to say, there is nothing to load to the backbone Backbone_VSSM?\r\n\r\n因为打印出了Failed loading checkpoint form /checkpoints/vmamba/vssm1_tiny_0230s_ckpt_epoch_264.pth: 'layers.2.blocks.8.op.in_proj.weight'这句话，也就意味着加载失败了，所以相当于作为backbone的时候，并没有加载任何的vssm1_tiny_0230s_ckpt_epoch_264.pth里面的预训练参数，对吗？这是不是哪里有问题？", "choices": "(A) 1. Ensure that the CUDA toolkit version matches the version required by PyTorch (e.g., torch.version = 2.0.0+cu118 requires CUDA 11.8).\n2. Use `conda install` to install PyTorch instead of `pip install` to automatically handle CUDA toolkit dependencies.\n3. Verify the CUDA toolkit version with `nvcc -V` and ensure it matches the PyTorch CUDA version.\n4. If using `pip install`, manually adjust the CUDA toolkit to the version corresponding to the installed PyTorch.\n5. Force Docker installation even if it causes issues, as it is the recommended approach. (B) 1. Ensure that the CUDA toolkit version matches the version required by PyTorch (e.g., torch.version = 2.0.0+cu118 requires CUDA 11.8).\n2. Use `conda install` to install PyTorch instead of `pip install` to automatically handle CUDA toolkit dependencies.\n3. Verify the CUDA toolkit version with `nvcc -V` and ensure it matches the PyTorch CUDA version.\n4. If using `pip install`, manually adjust the CUDA toolkit to the version corresponding to the installed PyTorch.\n5. Avoid using Docker if it causes installation issues; instead, use a fresh conda environment on a new machine for installation. (C) 1. Ensure that the CUDA toolkit version matches the version required by PyTorch (e.g., torch.version = 2.0.0+cu118 requires CUDA 11.8).\n2. Manually uninstall the CUDA toolkit to avoid conflicts.\n3. Use `conda install` to install PyTorch instead of `pip install` to automatically handle CUDA toolkit dependencies.\n4. Verify the CUDA toolkit version with `nvcc -V` and ensure it matches the PyTorch CUDA version.\n5. If using `pip install`, manually adjust the CUDA toolkit to the version corresponding to the installed PyTorch.\n6. Avoid using Docker if it causes installation issues; instead, use a fresh conda environment on a new machine for installation. (D) 1. Ensure that the CUDA toolkit version matches the version required by PyTorch (e.g., torch.version = 2.0.0+cu118 requires CUDA 11.8).\n2. Use `conda install` to install PyTorch instead of `pip install` to automatically handle CUDA toolkit dependencies.\n3. If using `pip install`, manually adjust the CUDA toolkit to the version corresponding to the installed PyTorch.\n4. Avoid using Docker if it causes installation issues; instead, use a fresh conda environment on a new machine for installation. (E) 1. Use `conda install` to install PyTorch instead of `pip install` to automatically handle CUDA toolkit dependencies.\n2. Verify the CUDA toolkit version with `nvcc -V` and ensure it matches the PyTorch CUDA version.\n3. If using `pip install`, manually adjust the CUDA toolkit to the version corresponding to the installed PyTorch.\n4. Avoid using Docker if it causes installation issues; instead, use a fresh conda environment on a new machine for installation. (F) 1. Verify the CUDA toolkit version with `nvcc -V` and ensure it matches the PyTorch CUDA version.\n2. Ensure that the CUDA toolkit version matches the version required by PyTorch (e.g., torch.version = 2.0.0+cu118 requires CUDA 11.8).\n3. Use `conda install` to install PyTorch instead of `pip install` to automatically handle CUDA toolkit dependencies.\n4. If using `pip install`, manually adjust the CUDA toolkit to the version corresponding to the installed PyTorch.\n5. Avoid using Docker if it causes installation issues; instead, use a fresh conda environment on a new machine for installation. (G) 1. Use `conda install` to install PyTorch instead of `pip install` to automatically handle CUDA toolkit dependencies.\n2. Verify the CUDA toolkit version with `nvcc -V` and ensure it matches the PyTorch CUDA version.\n3. Ensure that the CUDA toolkit version matches the version required by PyTorch (e.g., torch.version = 2.0.0+cu118 requires CUDA 11.8).\n4. If using `pip install`, manually adjust the CUDA toolkit to the version corresponding to the installed PyTorch.\n5. Avoid using Docker if it causes installation issues; instead, use a fresh conda environment on a new machine for installation.", "answer": "B"}
{"uuid": "e0bf9e64-ed50-4ab0-8aaa-9ecad4ebd507", "setup_instruct": "# VMamba Execution Plan\n\n## 1. Environment Setup\n**Objective**: Create a conda environment and install required dependencies.\n\n```bash\nconda create -n vmamba python=3.10\nconda activate vmamba\npip install torch==2.2 torchvision torchaudio triton pytest chardet yacs termcolor fvcore seaborn packaging ninja einops numpy==1.24.4 timm==0.4.12\npip install https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.2cxx11abiTRUE-cp310-cp310-linux_x86_64.whl\n```\n\n## 2. Clone Repository\n**Objective**: Download the VMamba source code.\n\n```bash\ngit clone https://github.com/MzeroMiko/VMamba.git\ncd VMamba\n```\n\n## 3. Install Selective Scan Kernel\n**Objective**: Build and install the selective scan module.\n\n```bash\ncd kernels/selective_scan && pip install .\ncd ../..\n```\n\n## 4. (Optional) Verify Selective Scan Implementation\n**Objective**: Test if selective scan matches reference implementation.\n\n```bash\npytest selective_scan/test_selective_scan.py\n```\n\n## 5. Install Additional Dependencies\n**Objective**: Install packages for detection/segmentation tasks.\n\n```bash\npip install mmengine==0.10.1 mmcv==2.1.0 opencv-python-headless ftfy regex\npip install mmdet==3.3.0 mmsegmentation==1.2.2 mmpretrain==1.2.0\n```\n\n## 6. Run Basic Test\n**Objective**: Verify installation by running the main script.\n\n```bash\npython vmamba.py\n```\n\n## 7. Model Training (Classification)\n**Objective**: Train a VMamba model on ImageNet.\n\n```bash\npython -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=8 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg <config_path> --batch-size 128 --data-path <dataset_path> --output /tmp\n```\n\n## 8. Model Inference (Classification)\n**Objective**: Evaluate a pre-trained model.\n\n```bash\npython -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=1 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg <config_path> --batch-size 128 --data-path <dataset_path> --output /tmp --pretrained <checkpoint_path>\n```\n\n## 9. Detection/Segmentation Evaluation\n**Objective**: Run inference on detection/segmentation tasks.\n\n```bash\nbash ./tools/dist_test.sh <config_path> <checkpoint_path> 1\n```\n\n## 10. Analysis Tools\n**Objective**: Generate visualizations and performance metrics.\n\n```bash\n# Attention maps\nCUDA_VISIBLE_DEVICES=0 python analyze/attnmap.py\n\n# Effective receptive field\nCUDA_VISIBLE_DEVICES=0 python analyze/erf.py\n\n# Throughput analysis\nCUDA_VISIBLE_DEVICES=0 python analyze/tp.py\n```", "issue_title": "cannot reproduce segmentation results", "issue_body": "Would you kindly report the variance of segmentation results for tinys1l8 on mIoU(SS)? It seems the fine tuning result might have a certain variance, with the same pth, I only got 47.1 mIoU(SS).\r\n\r\n06/03 21:58:45 - mmengine - INFO - Saving checkpoint at 160000 iterations\r\n06/03 21:58:48 - mmengine - INFO - Iter(val) [ 50/500]    eta: 0:00:15  time: 0.0337  data_time: 0.0011  memory: 1789  \r\n06/03 21:58:50 - mmengine - INFO - Iter(val) [100/500]    eta: 0:00:13  time: 0.0331  data_time: 0.0010  memory: 1868  \r\n06/03 21:58:52 - mmengine - INFO - Iter(val) [150/500]    eta: 0:00:11  time: 0.0336  data_time: 0.0011  memory: 1594  \r\n06/03 21:58:53 - mmengine - INFO - Iter(val) [200/500]    eta: 0:00:10  time: 0.0331  data_time: 0.0010  memory: 1668  \r\n06/03 21:58:55 - mmengine - INFO - Iter(val) [250/500]    eta: 0:00:08  time: 0.0342  data_time: 0.0012  memory: 1636  \r\n06/03 21:58:57 - mmengine - INFO - Iter(val) [300/500]    eta: 0:00:06  time: 0.0340  data_time: 0.0013  memory: 2610  \r\n06/03 21:58:59 - mmengine - INFO - Iter(val) [350/500]    eta: 0:00:05  time: 0.0329  data_time: 0.0011  memory: 1594  \r\n06/03 21:59:00 - mmengine - INFO - Iter(val) [400/500]    eta: 0:00:03  time: 0.0334  data_time: 0.0011  memory: 1627  \r\n06/03 21:59:02 - mmengine - INFO - Iter(val) [450/500]    eta: 0:00:01  time: 0.0337  data_time: 0.0011  memory: 1643  \r\n06/03 21:59:04 - mmengine - INFO - Iter(val) [500/500]    eta: 0:00:00  time: 0.0333  data_time: 0.0010  memory: 1857  \r\n06/03 21:59:06 - mmengine - INFO - per class results:\r\n06/03 21:59:06 - mmengine - INFO - \r\n+---------------------+-------+-------+\r\n|        Class        |  IoU  |  Acc  |\r\n+---------------------+-------+-------+\r\n|         wall        | 76.56 | 88.21 |\r\n|       building      | 82.17 | 91.57 |\r\n|         sky         | 94.07 | 97.39 |\r\n|        floor        | 80.72 | 90.83 |\r\n|         tree        | 74.37 | 87.45 |\r\n|       ceiling       |  84.1 |  91.8 |\r\n|         road        | 83.44 | 89.96 |\r\n|         bed         | 87.58 | 95.44 |\r\n|      windowpane     | 60.53 | 76.88 |\r\n|        grass        | 66.84 | 81.72 |\r\n|       cabinet       | 59.24 | 70.35 |\r\n|       sidewalk      | 65.52 | 82.74 |\r\n|        person       | 79.28 | 90.91 |\r\n|        earth        | 35.16 | 50.57 |\r\n|         door        | 46.14 | 59.65 |\r\n|        table        | 57.42 | 73.76 |\r\n|       mountain      | 58.33 | 72.43 |\r\n|        plant        | 51.81 |  63.8 |\r\n|       curtain       | 73.27 | 84.31 |\r\n|        chair        | 53.73 | 66.52 |\r\n|         car         | 83.54 | 91.12 |\r\n|        water        | 51.89 | 68.62 |\r\n|       painting      | 71.89 | 86.33 |\r\n|         sofa        | 63.81 | 79.67 |\r\n|        shelf        | 37.43 | 55.24 |\r\n|        house        | 43.91 | 64.91 |\r\n|         sea         | 49.77 | 71.57 |\r\n|        mirror       | 67.69 | 77.68 |\r\n|         rug         | 60.14 | 68.75 |\r\n|        field        | 32.75 | 50.55 |\r\n|       armchair      | 38.95 | 56.56 |\r\n|         seat        | 62.39 |  80.7 |\r\n|        fence        | 40.75 | 56.26 |\r\n|         desk        | 45.26 | 68.17 |\r\n|         rock        | 41.67 | 66.83 |\r\n|       wardrobe      | 46.45 | 66.27 |\r\n|         lamp        | 59.79 | 71.94 |\r\n|       bathtub       | 73.95 | 81.16 |\r\n|       railing       | 35.65 | 47.87 |\r\n|       cushion       | 56.29 | 70.18 |\r\n|         base        | 26.91 | 40.25 |\r\n|         box         | 24.37 | 32.06 |\r\n|        column       | 46.33 | 57.02 |\r\n|      signboard      | 35.78 | 48.07 |\r\n|   chest of drawers  |  38.5 | 60.27 |\r\n|       counter       | 37.34 | 44.37 |\r\n|         sand        | 42.15 | 58.57 |\r\n|         sink        | 69.05 | 78.22 |\r\n|      skyscraper     | 60.29 | 76.34 |\r\n|      fireplace      | 75.61 | 90.32 |\r\n|     refrigerator    |  73.0 | 82.24 |\r\n|      grandstand     | 42.13 | 68.22 |\r\n|         path        | 29.02 | 41.26 |\r\n|        stairs       | 28.68 | 36.57 |\r\n|        runway       | 68.59 | 94.96 |\r\n|         case        | 42.58 |  62.0 |\r\n|      pool table     | 91.64 | 96.29 |\r\n|        pillow       | 54.62 | 62.69 |\r\n|     screen door     | 69.26 | 78.51 |\r\n|       stairway      | 27.27 | 34.16 |\r\n|        river        | 11.99 | 22.12 |\r\n|        bridge       | 28.49 | 34.33 |\r\n|       bookcase      | 40.23 |  64.0 |\r\n|        blind        | 41.81 | 49.08 |\r\n|     coffee table    | 52.27 |  77.9 |\r\n|        toilet       |  82.0 | 89.15 |\r\n|        flower       | 44.19 | 55.59 |\r\n|         book        | 45.39 |  67.3 |\r\n|         hill        | 11.65 | 21.12 |\r\n|        bench        | 43.48 | 53.27 |\r\n|      countertop     | 56.31 |  72.1 |\r\n|        stove        |  75.3 | 81.04 |\r\n|         palm        | 49.34 | 70.37 |\r\n|    kitchen island   | 35.54 | 68.26 |\r\n|       computer      | 60.59 | 73.25 |\r\n|     swivel chair    | 43.51 | 57.38 |\r\n|         boat        | 46.18 | 53.38 |\r\n|         bar         | 35.94 | 50.48 |\r\n|    arcade machine   | 47.77 | 52.44 |\r\n|        hovel        | 12.21 | 14.83 |\r\n|         bus         | 86.25 | 96.04 |\r\n|        towel        | 62.59 | 74.43 |\r\n|        light        | 52.68 | 60.26 |\r\n|        truck        | 23.62 | 28.02 |\r\n|        tower        | 34.57 | 48.77 |\r\n|      chandelier     | 65.68 | 79.97 |\r\n|        awning       | 23.85 | 28.48 |\r\n|     streetlight     | 24.89 | 32.14 |\r\n|        booth        | 46.41 | 48.75 |\r\n| television receiver | 69.37 | 80.44 |\r\n|       airplane      | 55.54 | 64.37 |\r\n|      dirt track     |  7.68 | 13.88 |\r\n|       apparel       | 38.39 | 52.77 |\r\n|         pole        | 22.21 | 30.71 |\r\n|         land        |  2.42 |  3.18 |\r\n|      bannister      |  9.67 | 12.92 |\r\n|      escalator      | 23.15 | 27.28 |\r\n|       ottoman       | 40.71 | 57.33 |\r\n|        bottle       | 34.82 | 58.07 |\r\n|        buffet       |  45.6 | 58.89 |\r\n|        poster       | 25.71 | 35.74 |\r\n|        stage        | 19.22 | 28.42 |\r\n|         van         | 44.19 | 63.16 |\r\n|         ship        |  49.3 | 72.98 |\r\n|       fountain      | 19.62 | 20.41 |\r\n|    conveyer belt    | 81.77 |  91.8 |\r\n|        canopy       | 15.42 | 18.41 |\r\n|        washer       | 67.93 | 70.48 |\r\n|      plaything      | 17.91 | 26.68 |\r\n|    swimming pool    | 51.12 | 63.27 |\r\n|        stool        | 37.42 | 56.01 |\r\n|        barrel       | 44.47 | 64.77 |\r\n|        basket       | 27.85 | 41.94 |\r\n|      waterfall      | 49.95 | 59.44 |\r\n|         tent        | 80.56 | 98.22 |\r\n|         bag         | 11.34 | 13.47 |\r\n|       minibike      | 67.39 | 82.21 |\r\n|        cradle       | 76.26 | 92.09 |\r\n|         oven        | 30.67 |  65.1 |\r\n|         ball        | 25.98 | 26.59 |\r\n|         food        | 33.19 | 40.36 |\r\n|         step        | 11.48 | 14.51 |\r\n|         tank        | 50.44 | 57.61 |\r\n|      trade name     | 28.77 |  33.4 |\r\n|      microwave      | 56.41 | 62.86 |\r\n|         pot         | 38.42 | 45.16 |\r\n|        animal       | 53.43 | 55.88 |\r\n|       bicycle       | 55.99 | 75.31 |\r\n|         lake        | 60.09 | 63.02 |\r\n|      dishwasher     | 70.64 | 81.01 |\r\n|        screen       | 69.63 |  79.2 |\r\n|       blanket       |  9.19 | 10.63 |\r\n|      sculpture      | 49.26 | 76.69 |\r\n|         hood        | 60.53 | 68.21 |\r\n|        sconce       | 43.28 | 52.17 |\r\n|         vase        | 33.14 | 45.48 |\r\n|    traffic light    | 25.63 | 44.32 |\r\n|         tray        |  3.16 |  7.58 |\r\n|        ashcan       | 43.88 | 52.82 |\r\n|         fan         | 56.37 | 69.33 |\r\n|         pier        | 63.08 | 77.07 |\r\n|      crt screen     |  4.71 | 11.84 |\r\n|        plate        | 52.65 | 67.62 |\r\n|       monitor       |  6.42 |  8.13 |\r\n|    bulletin board   | 46.63 |  58.2 |\r\n|        shower       |  0.05 |  0.18 |\r\n|       radiator      | 59.11 | 64.59 |\r\n|        glass        | 11.54 | 12.28 |\r\n|        clock        |  28.1 | 31.69 |\r\n|         flag        | 43.86 | 47.88 |\r\n+---------------------+-------+-------+\r\n06/03 21:59:06 - mmengine - INFO - Iter(val) [500/500]    aAcc: 82.3300  mIoU: 47.1000  mAcc: 58.8000  data_time: 0.0011  time: 0.0335", "choices": "(A) 1. Ensure you have installed the correct version of CUDA (preferably 12.1 or compatible versions).\n2. Install the required dependencies: `conda create -n myenv python=3.10.14`\n3. Install `causal-conv1d` and `mamba-ssm` using the provided wheel files or from PyPI.\n4. Activate the environment: `source activate myenv`\n5. Install PyTorch with CUDA support: `conda install pytorch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 pytorch-cuda=12.1 -c pytorch -c nvidia`\n6. Modify the import statements in your code to use `selective_scan_cuda_oflex` instead of `selective_scan_cuda_core` or `selective_scan_cuda` as the latter are deprecated.\n7. Ensure all other dependencies like `einops`, `ninja`, and `packaging` are installed and up to date. (B) 1. Ensure you have installed the correct version of CUDA (preferably 12.1 or compatible versions).\n2. Install the required dependencies: `conda create -n myenv python=3.10.14`\n3. Activate the environment: `source activate myenv`\n4. Install PyTorch with CUDA support: `conda install pytorch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 pytorch-cuda=12.1 -c pytorch -c nvidia`\n5. Uninstall CUDA: `sudo apt remove --purge cuda-12-1`\n6. Install `causal-conv1d` and `mamba-ssm` using the provided wheel files or from PyPI.\n7. Modify the import statements in your code to use `selective_scan_cuda_oflex` instead of `selective_scan_cuda_core` or `selective_scan_cuda` as the latter are deprecated.\n8. Ensure all other dependencies like `einops`, `ninja`, and `packaging` are installed and up to date. (C) 1. Ensure you have installed the correct version of CUDA (preferably 12.1 or compatible versions).\n2. Install the required dependencies: `conda create -n myenv python=3.10.14`\n3. Activate the environment: `source activate myenv`\n4. Install `causal-conv1d` and `mamba-ssm` using the provided wheel files or from PyPI.\n5. Modify the import statements in your code to use `selective_scan_cuda_oflex` instead of `selective_scan_cuda_core` or `selective_scan_cuda` as the latter are deprecated.\n6. Ensure all other dependencies like `einops`, `ninja`, and `packaging` are installed and up to date. (D) 1. Ensure you have installed the correct version of CUDA (preferably 12.1 or compatible versions).\n2. Install the required dependencies: `conda create -n myenv python=3.10.14`\n3. Downgrade Python to 3.8: `conda install python=3.8`\n4. Activate the environment: `source activate myenv`\n5. Install PyTorch with CUDA support: `conda install pytorch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 pytorch-cuda=12.1 -c pytorch -c nvidia`\n6. Install `causal-conv1d` and `mamba-ssm` using the provided wheel files or from PyPI.\n7. Modify the import statements in your code to use `selective_scan_cuda_oflex` instead of `selective_scan_cuda_core` or `selective_scan_cuda` as the latter are deprecated.\n8. Ensure all other dependencies like `einops`, `ninja`, and `packaging` are installed and up to date. (E) 1. Install PyTorch with CUDA support: `conda install pytorch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 pytorch-cuda=12.1 -c pytorch -c nvidia`\n2. Ensure you have installed the correct version of CUDA (preferably 12.1 or compatible versions).\n3. Install the required dependencies: `conda create -n myenv python=3.10.14`\n4. Activate the environment: `source activate myenv`\n5. Install `causal-conv1d` and `mamba-ssm` using the provided wheel files or from PyPI.\n6. Modify the import statements in your code to use `selective_scan_cuda_oflex` instead of `selective_scan_cuda_core` or `selective_scan_cuda` as the latter are deprecated.\n7. Ensure all other dependencies like `einops`, `ninja`, and `packaging` are installed and up to date. (F) 1. Ensure you have installed the correct version of CUDA (preferably 12.1 or compatible versions).\n2. Install the required dependencies: `conda create -n myenv python=3.10.14`\n3. Activate the environment: `source activate myenv`\n4. Install PyTorch with CUDA support: `conda install pytorch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 pytorch-cuda=12.1 -c pytorch -c nvidia`\n5. Install `causal-conv1d` and `mamba-ssm` using the provided wheel files or from PyPI.\n6. Ensure all other dependencies like `einops`, `ninja`, and `packaging` are installed and up to date. (G) 1. Ensure you have installed the correct version of CUDA (preferably 12.1 or compatible versions).\n2. Install the required dependencies: `conda create -n myenv python=3.10.14`\n3. Activate the environment: `source activate myenv`\n4. Install PyTorch with CUDA support: `conda install pytorch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 pytorch-cuda=12.1 -c pytorch -c nvidia`\n5. Install `causal-conv1d` and `mamba-ssm` using the provided wheel files or from PyPI.\n6. Modify the import statements in your code to use `selective_scan_cuda_oflex` instead of `selective_scan_cuda_core` or `selective_scan_cuda` as the latter are deprecated.\n7. Ensure all other dependencies like `einops`, `ninja`, and `packaging` are installed and up to date.", "answer": "G"}
{"uuid": "339989a8-8e2a-45db-a819-c9d408b3761f", "setup_instruct": "# VMamba Deployment Plan\n\n## 1. Environment Setup\n- **Description**: Create a conda environment and install required dependencies\n- **Commands**:\n  ```bash\n  conda create -n vmamba python=3.10\n  conda activate vmamba\n  pip install torch==2.2 torchvision torchaudio triton pytest chardet yacs termcolor fvcore seaborn packaging ninja einops numpy==1.24.4 timm==0.4.12\n  pip install https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.2cxx11abiTRUE-cp310-cp310-linux_x86_64.whl\n  ```\n\n## 2. Quick Start\n- **Description**: Run the minimal VMamba implementation\n- **Command**:\n  ```bash\n  python vmamba.py\n  ```\n\n## 3. Full Installation (Optional)\n- **Description**: Clone repository and complete setup\n- **Commands**:\n  ```bash\n  git clone https://github.com/MzeroMiko/VMamba.git\n  cd VMamba\n  pip install -r requirements.txt\n  cd kernels/selective_scan && pip install .\n  ```\n\n## 4. Verification (Optional)\n- **Description**: Test selective scan implementation\n- **Command**:\n  ```bash\n  pytest selective_scan/test_selective_scan.py\n  ```\n\n## 5. Additional Dependencies (Optional)\n- **Description**: Install detection/segmentation dependencies\n- **Commands**:\n  ```bash\n  pip install mmengine==0.10.1 mmcv==2.1.0 opencv-python-headless ftfy regex\n  pip install mmdet==3.3.0 mmsegmentation==1.2.2 mmpretrain==1.2.0\n  ```\n\n## 6. Model Training\n- **Description**: Train classification model\n- **Command**:\n  ```bash\n  python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=8 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg </path/to/config> --batch-size 128 --data-path </path/of/dataset> --output /tmp\n  ```\n\n## 7. Model Inference\n- **Description**: Test model performance\n- **Command**:\n  ```bash\n  python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=1 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg </path/to/config> --batch-size 128 --data-path </path/of/dataset> --output /tmp --pretrained </path/of/checkpoint>\n  ```\n\n## 8. Analysis Tools\n- **Description**: Run various analysis scripts\n- **Commands**:\n  ```bash\n  # Attention visualization\n  CUDA_VISIBLE_DEVICES=0 python analyze/attnmap.py\n  \n  # Effective receptive field\n  CUDA_VISIBLE_DEVICES=0 python analyze/erf.py\n  \n  # Throughput analysis\n  CUDA_VISIBLE_DEVICES=0 python analyze/tp.py\n  ```", "issue_title": "About training on ImageNet", "issue_body": "Hi，\r\n\r\nI noticed in your training log (imagenet) that the training loss is normal, but the loss in the test phase is nan in many epochs, I also encountered this problem, could you please tell me how to solve it?", "choices": "(A) 1. Locate the 'model' file in your project directory.\n2. Overwrite the 'model' file with a blank file to reset it.\n3. Use the 'sys' module to add the path of the 'model' file to the Python path before importing it in 'train.py'.\n4. Insert the following code before the 'import model' line in 'train.py':\n   ```python\n   import sys\n   sys.path.append('/path/to/model/directory')\n   ```\n5. Replace '/path/to/model/directory' with the actual path where the 'model' file is located.\n6. Save the changes and run 'train.py' again. (B) 1. Locate the 'model' file in your project directory.\n2. Delete the 'model' file to avoid conflicts.\n3. Use the 'sys' module to add the path of the 'model' file to the Python path before importing it in 'train.py'.\n4. Insert the following code before the 'import model' line in 'train.py':\n   ```python\n   import sys\n   sys.path.append('/path/to/model/directory')\n   ```\n5. Replace '/path/to/model/directory' with the actual path where the 'model' file is located.\n6. Save the changes and run 'train.py' again. (C) 1. Locate the 'model' file in your project directory.\n2. Use the 'sys' module to add the path of the 'model' file to the Python path before importing it in 'train.py'.\n3. Insert the following code before the 'import model' line in 'train.py':\n   ```python\n   import sys\n   sys.path.append('/path/to/model/directory')\n   ```\n4. Save the changes and run 'train.py' again. (D) 1. Locate the 'model' file in your project directory.\n2. Insert the following code before the 'import model' line in 'train.py':\n   ```python\n   import sys\n   sys.path.append('/path/to/model/directory')\n   ```\n3. Use the 'sys' module to add the path of the 'model' file to the Python path before importing it in 'train.py'.\n4. Replace '/path/to/model/directory' with the actual path where the 'model' file is located.\n5. Save the changes and run 'train.py' again. (E) 1. Locate the 'model' file in your project directory.\n2. Insert the following code before the 'import model' line in 'train.py':\n   ```python\n   import sys\n   sys.path.append('/path/to/model/directory')\n   ```\n3. Replace '/path/to/model/directory' with the actual path where the 'model' file is located.\n4. Save the changes and run 'train.py' again. (F) 1. Locate the 'model' file in your project directory.\n2. Use the 'sys' module to add the path of the 'model' file to the Python path before importing it in 'train.py'.\n3. Replace '/path/to/model/directory' with the actual path where the 'model' file is located.\n4. Insert the following code before the 'import model' line in 'train.py':\n   ```python\n   import sys\n   sys.path.append('/path/to/model/directory')\n   ```\n5. Save the changes and run 'train.py' again. (G) The error occurs due to a relative path issue. To resolve it, you need to ensure the 'model' file is accessible by the 'train.py' script. Follow these steps:\n1. Locate the 'model' file in your project directory.\n2. Use the 'sys' module to add the path of the 'model' file to the Python path before importing it in 'train.py'.\n3. Insert the following code before the 'import model' line in 'train.py':\n   ```python\n   import sys\n   sys.path.append('/path/to/model/directory')\n   ```\n4. Replace '/path/to/model/directory' with the actual path where the 'model' file is located.\n5. Save the changes and run 'train.py' again.", "answer": "G"}
{"uuid": "6f2d3a7e-1518-433a-b4ea-c0d2e043def1", "setup_instruct": "# VMamba Deployment Plan\n\n## Step 1: Environment Setup\n- **Description**: Create a conda environment and install required dependencies.\n- **Commands**:\n  ```bash\n  conda create -n vmamba python=3.10\n  conda activate vmamba\n  pip install torch==2.2 torchvision torchaudio triton pytest chardet yacs termcolor fvcore seaborn packaging ninja einops numpy==1.24.4 timm==0.4.12\n  pip install https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.2cxx11abiTRUE-cp310-cp310-linux_x86_64.whl\n  ```\n\n## Step 2: Clone Repository\n- **Description**: Clone the VMamba repository and navigate to the project directory.\n- **Commands**:\n  ```bash\n  git clone https://github.com/MzeroMiko/VMamba.git\n  cd VMamba\n  ```\n\n## Step 3: Install Additional Dependencies\n- **Description**: Install dependencies listed in `requirements.txt` and selective scan kernel.\n- **Commands**:\n  ```bash\n  pip install -r requirements.txt\n  cd kernels/selective_scan && pip install .\n  ```\n\n## Step 4: Verify Installation (Optional)\n- **Description**: Verify selective scan implementation by running tests.\n- **Commands**:\n  ```bash\n  pytest selective_scan/test_selective_scan.py\n  ```\n\n## Step 5: Install Detection/Segmentation Dependencies (Optional)\n- **Description**: Install additional dependencies for detection and segmentation tasks.\n- **Commands**:\n  ```bash\n  pip install mmengine==0.10.1 mmcv==2.1.0 opencv-python-headless ftfy regex\n  pip install mmdet==3.3.0 mmsegmentation==1.2.2 mmpretrain==1.2.0\n  ```\n\n## Step 6: Run VMamba\n- **Description**: Execute the VMamba script.\n- **Commands**:\n  ```bash\n  python vmamba.py\n  ```\n\n## Step 7: Training/Inference (Optional)\n- **Description**: Train or evaluate VMamba models for classification, detection, or segmentation.\n- **Commands**:\n  ```bash\n  # Classification Training\n  python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=8 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg </path/to/config> --batch-size 128 --data-path </path/of/dataset> --output /tmp\n\n  # Classification Evaluation\n  python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=1 --master_addr=\"127.0.0.1\" --master_port=29501 main.py --cfg </path/to/config> --batch-size 128 --data-path </path/of/dataset> --output /tmp --pretrained </path/of/checkpoint>\n\n  # Detection/Segmentation Evaluation\n  bash ./tools/dist_test.sh </path/to/config> </path/to/checkpoint> 1\n\n  # Detection/Segmentation Training\n  bash ./tools/dist_train.sh </path/to/config> 8\n  ```\n\n## Step 8: Analysis Tools (Optional)\n- **Description**: Use analysis tools for visualizing attention, effective receptive field, and throughput.\n- **Commands**:\n  ```bash\n  # Visualize Mamba \"Attention\"\n  CUDA_VISIBLE_DEVICES=0 python analyze/attnmap.py\n\n  # Analyze Effective Receptive Field\n  CUDA_VISIBLE_DEVICES=0 python analyze/erf.py\n\n  # Analyze Throughput\n  CUDA_VISIBLE_DEVICES=0 python analyze/tp.py\n  ```", "issue_title": "selective scan  install failed", "issue_body": "Error1 :   ` subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1. `\r\nSolution1:   ` modify def _run_ninja_build(build_directory: str, verbose: bool, error_prefix: str) -> None:\r\n    command = ['ninja', '-v'] ` to `def _run_ninja_build(build_directory: str, verbose: bool, error_prefix: str) -> None:\r\n    command = ['ninja', '--version']`\r\n**ninja -v not supported.**\r\n\r\nError2:  ` g++ -pthread -B /root/miniconda3/envs/mamba/compiler_compat -shared -Wl,--allow-shlib-undefined -Wl,-rpath,/root/miniconda3/envs/mamba/lib -Wl,-rpath-link,/root/miniconda3/envs/mamba/lib -L/root/miniconda3/envs/mamba/lib -Wl,--allow-shlib-undefined -Wl,-rpath,/root/miniconda3/envs/mamba/lib -Wl,-rpath-link,/root/miniconda3/envs/mamba/lib -L/root/miniconda3/envs/mamba/lib /root/workspace/code/mamba_all/VMamba/kernels/selective_scan/build/temp.linux-x86_64-3.8/csrc/selective_scan/cus/selective_scan.o /root/workspace/code/mamba_all/VMamba/kernels/selective_scan/build/temp.linux-x86_64-3.8/csrc/selective_scan/cus/selective_scan_core_bwd.o /root/workspace/code/mamba_all/VMamba/kernels/selective_scan/build/temp.linux-x86_64-3.8/csrc/selective_scan/cus/selective_scan_core_fwd.o -L/root/miniconda3/envs/mamba/lib/python3.8/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.8/selective_scan_cuda_core.cpython-38-x86_64-linux-gnu.so\r\n      g++: error: /root/workspace/code/mamba_all/VMamba/kernels/selective_scan/build/temp.linux-x86_64-3.8/csrc/selective_scan/cus/selective_scan.o: No such file or directory\r\n      g++: error: /root/workspace/code/mamba_all/VMamba/kernels/selective_scan/build/temp.linux-x86_64-3.8/csrc/selective_scan/cus/selective_scan_core_bwd.o: No such file or directory\r\n      g++: error: /root/workspace/code/mamba_all/VMamba/kernels/selective_scan/build/temp.linux-x86_64-3.8/csrc/selective_scan/cus/selective_scan_core_fwd.o: No such file or directory\r\n      error: command '/usr/bin/g++' failed with exit code 1\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for selective-scan\r\n  Running setup.py clean for selective-scan\r\nFailed to build selective-scan\r\nERROR: Could not build wheels for selective-scan, which is required to install pyproject.toml-based projects`\r\n\r\nSolution: ????  \r\n\r\nmay anyone help?", "choices": "(A) 1. Use the provided configuration file with the following settings for ImageNet-R dataset:\n   - Optimizer: 'adam'\n   - Learning rate: 1e-2\n   - Epochs: 20\n   - Model name: 'sdlora'\n   - Backbone type: 'vit_base_patch16_224'\n   - Device: ['0']\n   - Scheduler: 'constant'\n   - Filepath: './ImageNetR3/'\n   - Seed: [1995]\n   - Batch size: 128\n   - Weight decay: 2e-4\n2. Ensure the timm library is used to load the pretrained ViT-B/16 model, which by default is 'vit_base_patch16_224.augreg2_in21k_ft_in1k'.\n3. Do not modify the code to fix the optimizer issue; the provided configuration works as is.\n4. Verify the results against the paper's reported performance metrics. (B) 1. Use the provided configuration file with the following settings for ImageNet-R dataset:\n   - Optimizer: 'adam'\n   - Learning rate: 1e-2\n   - Epochs: 20\n   - Model name: 'sdlora'\n   - Backbone type: 'vit_base_patch16_224'\n   - Device: ['0']\n   - Scheduler: 'constant'\n   - Filepath: './ImageNetR3/'\n   - Seed: [1995]\n   - Batch size: 128\n   - Weight decay: 2e-4\n2. Ensure the timm library is used to load the pretrained ViT-B/16 model, which by default is 'vit_base_patch16_224.augreg2_in21k_ft_in1k'.\n3. Do not modify the code to fix the optimizer issue; the provided configuration works as is. (C) 1. Use the provided configuration file with the following settings for ImageNet-R dataset:\n   - Optimizer: 'adam'\n   - Learning rate: 1e-2\n   - Epochs: 20\n   - Model name: 'sdlora'\n   - Backbone type: 'vit_base_patch16_224'\n   - Device: ['0']\n   - Scheduler: 'constant'\n   - Filepath: './ImageNetR3/'\n   - Seed: [1995]\n   - Batch size: 128\n   - Weight decay: 2e-4\n2. Ensure the timm library is used to load the pretrained ViT-B/16 model, which by default is 'vit_base_patch16_224.augreg2_in21k_ft_in1k'.\n3. Modify the optimizer to 'sgd' with a learning rate of 0.1.\n4. Do not modify the code to fix the optimizer issue; the provided configuration works as is.\n5. Verify the results against the paper's reported performance metrics. (D) 1. Ensure the timm library is used to load the pretrained ViT-B/16 model, which by default is 'vit_base_patch16_224.augreg2_in21k_ft_in1k'.\n2. Use the provided configuration file with the following settings for ImageNet-R dataset:\n   - Optimizer: 'adam'\n   - Learning rate: 1e-2\n   - Epochs: 20\n   - Model name: 'sdlora'\n   - Backbone type: 'vit_base_patch16_224'\n   - Device: ['0']\n   - Scheduler: 'constant'\n   - Filepath: './ImageNetR3/'\n   - Seed: [1995]\n   - Batch size: 128\n   - Weight decay: 2e-4\n3. Do not modify the code to fix the optimizer issue; the provided configuration works as is.\n4. Verify the results against the paper's reported performance metrics.", "answer": "A"}
