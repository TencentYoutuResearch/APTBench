Request: Choose the correct execution plan to setup the environment for running the repository according to the repository information.
Repo information: File Analysis:\n\nFile: configs/ESAM-E+FF/ESAM-E+FF_online_scannet.py\nImports:\n- (None)\n\nFile: configs/ESAM-E+FF/ESAM-E+FF_online_scenenn_test.py\nImports:\n- (None)\n\nFile: configs/ESAM-E+FF/ESAM-E+FF_sv_scannet.py\nImports:\n- (None)\n\nFile: configs/ESAM-E/ESAM-E_online_scannet.py\nImports:\n- (None)\n\nFile: configs/ESAM-E/ESAM-E_online_scenenn_test.py\nImports:\n- (None)\n\nFile: configs/ESAM-E/ESAM-E_sv_scannet.py\nImports:\n- (None)\n\nFile: configs/ESAM-E_CA/ESAM-E_online_3rscan_CA_test.py\nImports:\n- (None)\n\nFile: configs/ESAM-E_CA/ESAM-E_online_scannet200_CA.py\nImports:\n- (None)\n\nFile: configs/ESAM-E_CA/ESAM-E_online_scenenn_CA_test.py\nImports:\n- (None)\n\nFile: configs/ESAM-E_CA/ESAM-E_online_stream.py\nImports:\n- (None)\n\nFile: configs/ESAM-E_CA/ESAM-E_sv_scannet200_CA.py\nImports:\n- (None)\n\nFile: configs/ESAM/ESAM_online_scannet.py\nImports:\n- (None)\n\nFile: configs/ESAM/ESAM_online_scenenn_test.py\nImports:\n- (None)\n\nFile: configs/ESAM/ESAM_sv_scannet.py\nImports:\n- (None)\n\nFile: configs/ESAM_CA/ESAM_online_3rscan_CA_test.py\nImports:\n- (None)\n\nFile: configs/ESAM_CA/ESAM_online_scannet200_CA.py\nImports:\n- (None)\n\nFile: configs/ESAM_CA/ESAM_online_scenenn_CA_test.py\nImports:\n- (None)\n\nFile: configs/ESAM_CA/ESAM_sv_scannet200_CA.py\nImports:\n- (None)\n\nFile: data/3RScan-mv/download-3RScan.py\nImports:\n- import sys\n- import argparse\n- import os\n- import tempfile\n- import re\n\nFile: data/3RScan-mv/load_3rscan_mv_data.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from plyfile import PlyData, PlyElement\n- from scipy import stats\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import pdb\n- import torch\n- import pointops\n- from load_scannet_data import export\n- from tqdm import tqdm\n\nFile: data/3RScan-mv/load_3rscan_mv_data_fast.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from plyfile import PlyData, PlyElement\n- from scipy import stats\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import pdb\n- import torch\n- import pointops\n- from load_scannet_data import export\n- from fastsam import FastSAM\n- from tqdm import tqdm\n\nFile: data/3RScan-mv/load_scannet_data.py\nImports:\n- import argparse\n- import inspect\n- import json\n- import os\n- import numpy\n- import scannet_utils\n\nFile: data/3RScan-mv/scannet_utils.py\nImports:\n- import csv\n- import os\n- import numpy\n- from plyfile import PlyData\n\nFile: data/3RScan/batch_load_3rscan_data.py\nImports:\n- import argparse\n- import datetime\n- import os\n- from os import path\n- import torch\n- import segmentator\n- import open3d\n- import numpy\n- from load_scannet_data import export\n\nFile: data/3RScan/load_scannet_data.py\nImports:\n- import argparse\n- import inspect\n- import json\n- import os\n- import numpy\n- import scannet_utils\n\nFile: data/3RScan/scannet_utils.py\nImports:\n- import csv\n- import os\n- import numpy\n- from plyfile import PlyData\n\nFile: data/scannet-mv/load_scannet_data.py\nImports:\n- import argparse\n- import inspect\n- import json\n- import os\n- import numpy\n- import scannet_utils\n\nFile: data/scannet-mv/load_scannet_mv_data.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from plyfile import PlyData, PlyElement\n- from scipy import stats\n- import open3d\n- from sklearn.decomposition import PCA\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import trimesh\n- import multiprocessing\n- import pdb\n- import imageio\n- import skimage.transform\n- import torch\n- import pointops\n- from load_scannet_data import export\n- from tqdm import tqdm\n\nFile: data/scannet-mv/load_scannet_mv_data_fast.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from plyfile import PlyData, PlyElement\n- from scipy import stats\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import pdb\n- import torch\n- import pointops\n- from load_scannet_data import export\n- from fastsam import FastSAM\n- from tqdm import tqdm\n\nFile: data/scannet-mv/meta_data/delete_more_points.py\nImports:\n- import os\n- import sys\n\nFile: data/scannet-mv/meta_data/generate_sv_txt.py\nImports:\n- import os\n- import pdb\n\nFile: data/scannet-mv/scannet_utils.py\nImports:\n- import csv\n- import os\n- import numpy\n- from plyfile import PlyData\n\nFile: data/scannet-sv/load_scannet_data.py\nImports:\n- import argparse\n- import inspect\n- import json\n- import os\n- import numpy\n- import scannet_utils\n\nFile: data/scannet-sv/load_scannet_sv_data_v2.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from sklearn.decomposition import PCA\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import pdb\n- import torch\n- import pointops\n- from load_scannet_data import export\n- from tqdm import tqdm\n\nFile: data/scannet-sv/load_scannet_sv_data_v2_fast.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from sklearn.decomposition import PCA\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import pdb\n- import torch\n- import pointops\n- from load_scannet_data import export\n- from fastsam import FastSAM\n- from tqdm import tqdm\n\nFile: data/scannet-sv/meta_data/generate_sv_txt.py\nImports:\n- import os\n- import pdb\n\nFile: data/scannet-sv/scannet_utils.py\nImports:\n- import csv\n- import os\n- import numpy\n- from plyfile import PlyData\n\nFile: data/scannet/batch_load_scannet_data.py\nImports:\n- import argparse\n- import datetime\n- import os\n- from os import path\n- import torch\n- import segmentator\n- import open3d\n- import numpy\n- from load_scannet_data import export\n\nFile: data/scannet/load_scannet_data.py\nImports:\n- import argparse\n- import inspect\n- import json\n- import os\n- import numpy\n- import scannet_utils\n\nFile: data/scannet/scannet_utils.py\nImports:\n- import csv\n- import os\n- import numpy\n- from plyfile import PlyData\n\nFile: data/scannet200-mv/load_scannet_data.py\nImports:\n- import argparse\n- import inspect\n- import json\n- import os\n- import numpy\n- import scannet_utils\n\nFile: data/scannet200-mv/load_scannet_mv_data.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from plyfile import PlyData, PlyElement\n- from scipy import stats\n- import open3d\n- from sklearn.decomposition import PCA\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import trimesh\n- import multiprocessing\n- import pdb\n- import imageio\n- import skimage.transform\n- import torch\n- import pointops\n- from load_scannet_data import export\n- from tqdm import tqdm\n\nFile: data/scannet200-mv/load_scannet_mv_data_fast.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from scipy import stats\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import pdb\n- import torch\n- import pointops\n- from load_scannet_data import export\n- from fastsam import FastSAM\n- from tqdm import tqdm\n\nFile: data/scannet200-mv/meta_data/delete_more_points.py\nImports:\n- import os\n- import sys\n\nFile: data/scannet200-mv/meta_data/generate_sv_txt.py\nImports:\n- import os\n- import pdb\n\nFile: data/scannet200-mv/meta_data/sort.py\nImports:\n- import os\n- import os.path\n- import numpy\n- import natsort\n\nFile: data/scannet200-mv/scannet_utils.py\nImports:\n- import csv\n- import os\n- import numpy\n- from plyfile import PlyData\n\nFile: data/scannet200-sv/load_scannet_data.py\nImports:\n- import argparse\n- import inspect\n- import json\n- import os\n- import numpy\n- import scannet_utils\n\nFile: data/scannet200-sv/load_scannet_sv_data_v2.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from sklearn.decomposition import PCA\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import pdb\n- import torch\n- import pointops\n- from load_scannet_data import export\n- from tqdm import tqdm\n\nFile: data/scannet200-sv/load_scannet_sv_data_v2_fast.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from sklearn.decomposition import PCA\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import pdb\n- import torch\n- import pointops\n- from load_scannet_data import export\n- from fastsam import FastSAM\n- from tqdm import tqdm\n\nFile: data/scannet200-sv/meta_data/generate_sv_txt.py\nImports:\n- import os\n- import pdb\n\nFile: data/scannet200-sv/scannet_utils.py\nImports:\n- import csv\n- import os\n- import numpy\n- from plyfile import PlyData\n\nFile: data/scannet200/batch_load_scannet_data.py\nImports:\n- import argparse\n- import datetime\n- import os\n- from os import path\n- import torch\n- import segmentator\n- import open3d\n- import numpy\n- from load_scannet_data import export\n\nFile: data/scannet200/load_scannet_data.py\nImports:\n- import argparse\n- import inspect\n- import json\n- import os\n- import numpy\n- import scannet_utils\n\nFile: data/scannet200/scannet_utils.py\nImports:\n- import csv\n- import os\n- import numpy\n- from plyfile import PlyData\n\nFile: data/scenenn-mv/load_scannet_data.py\nImports:\n- import argparse\n- import inspect\n- import json\n- import os\n- import numpy\n- import scannet_utils\n\nFile: data/scenenn-mv/load_scenenn_mv_data.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from plyfile import PlyData, PlyElement\n- from scipy import stats\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import pdb\n- import torch\n- import pointops\n- import scannet_utils\n- from tqdm import tqdm\n\nFile: data/scenenn-mv/load_scenenn_mv_data_fast.py\nImports:\n- import enum\n- import cv2\n- import shutil\n- import numpy\n- import math\n- from scipy import stats\n- import os\n- from plyfile import PlyData, PlyElement\n- from scipy import stats\n- from sklearn.cluster import KMeans\n- from segment_anything import build_sam, SamAutomaticMaskGenerator\n- import pdb\n- import torch\n- import pointops\n- import scannet_utils\n- from fastsam import FastSAM\n- from tqdm import tqdm\n\nFile: data/scenenn-mv/scannet_utils.py\nImports:\n- import os\n- import sys\n- import json\n- import csv\n- import pdb\n- import xml.dom.minidom\n- import xml.etree.ElementTree\n- import pdb\n\nFile: data/scenenn/batch_load_scenenn_data.py\nImports:\n- import argparse\n- import datetime\n- import os\n- from os import path\n- import torch\n- import segmentator\n- import open3d\n- import numpy\n- import scannet_utils\n- import json\n\nFile: data/scenenn/load_scannet_data.py\nImports:\n- import argparse\n- import inspect\n- import json\n- import os\n- import numpy\n- import scannet_utils\n\nFile: data/scenenn/scannet_utils.py\nImports:\n- import os\n- import sys\n- import json\n- import csv\n- import pdb\n- import xml.dom.minidom\n- import xml.etree.ElementTree\n- import pdb\n\nFile: demo/open3d_vis.py\nImports:\n- import copy\n- import numpy\n- import torch\n\nFile: demo/scannet200_mv_demo.py\nImports:\n- from argparse import ArgumentParser\n- import pdb\n- import os\n- from os import path\n- from copy import deepcopy\n- import random\n- import numpy\n- import torch\n- import mmengine\n- from mmdet3d.apis import init_model\n- from mmengine.dataset import Compose, pseudo_collate\n- from concurrent import futures\n- from pathlib import Path\n- from oneformer3d.scannet_dataset import ScanNet200SegMVDataset_\n- from show_result import show_seg_result, write_oriented_bbox, show_seg_result_ply\n\nFile: demo/scannet200_sv_demo.py\nImports:\n- from argparse import ArgumentParser\n- import pdb\n- import os\n- from os import path\n- from copy import deepcopy\n- import random\n- import numpy\n- import torch\n- import mmengine\n- from mmdet3d.apis import init_model\n- from mmengine.dataset import Compose, pseudo_collate\n- from concurrent import futures\n- from pathlib import Path\n- from tools.update_infos_to_v2 import get_empty_standard_data_info, clear_data_info_unused_keys\n- from oneformer3d.scannet_dataset import ScanNet200SegDataset_\n- from show_result import show_seg_result\n\nFile: demo/show_result.py\nImports:\n- from os import path\n- import mmengine\n- import numpy\n- import trimesh\n- from plyfile import PlyData\n- from tqdm import tqdm, trange\n\nFile: oneformer3d/__init__.py\nImports:\n- from oneformer3d import ScanNetOneFormer3D, ScanNet200OneFormer3D, S3DISOneFormer3D, ScanNet200OneFormer3D_Online\n- from mixformer3d import ScanNet200MixFormer3D, ScanNet200MixFormer3D_Online\n- from geo_aware_pool import GeoAwarePooling\n- from instance_merge import ins_merge_mat, ins_cat, ins_merge, OnlineMerge, GTMerge\n- from merge_head import MergeHead\n- from merge_criterion import ScanNetMergeCriterion_Seal\n- from multilevel_memory import MultilevelMemory\n- from mink_unet import Res16UNet34C\n- from query_decoder import ScanNetQueryDecoder, S3DISQueryDecoder\n- from unified_criterion import ScanNetUnifiedCriterion, ScanNetMixedCriterion, S3DISUnifiedCriterion\n- from semantic_criterion import ScanNetSemanticCriterion, S3DISSemanticCriterion\n- from instance_criterion import InstanceCriterion, MixedInstanceCriterion, QueryClassificationCost, MaskBCECost, MaskDiceCost, HungarianMatcher, SparseMatcher\n- from loading import LoadAnnotations3D_, NormalizePointsColor_\n- from formatting import Pack3DDetInputs_\n- from transforms_3d import ElasticTransfrom, AddSuperPointAnnotations, SwapChairAndFloor\n- from data_preprocessor import Det3DDataPreprocessor_\n- from unified_metric import UnifiedSegMetric\n- from scannet_dataset import ScanNetSegDataset_, ScanNet200SegDataset_, ScanNet200SegMVDataset_\n\nFile: oneformer3d/data_preprocessor.py\nImports:\n- from mmdet3d.models.data_preprocessors.data_preprocessor import Det3DDataPreprocessor\n- from mmdet3d.registry import MODELS\n\nFile: oneformer3d/formatting.py\nImports:\n- import numpy\n- from mmengine.structures import InstanceData\n- from mmdet3d.datasets.transforms import Pack3DDetInputs\n- from mmdet3d.datasets.transforms.formating import to_tensor\n- from mmdet3d.registry import TRANSFORMS\n- from mmdet3d.structures import BaseInstance3DBoxes, Det3DDataSample, PointData\n- from mmdet3d.structures.points import BasePoints\n- import math\n- import PIL.Image\n\nFile: oneformer3d/geo_aware_pool.py\nImports:\n- from turtle import forward\n- import torch\n- import torch.nn\n- import pdb\n- import time\n- from mmengine.model import BaseModule\n- from mmdet3d.registry import MODELS\n- from torch_scatter import scatter_mean, scatter\n\nFile: oneformer3d/img_backbone.py\nImports:\n- from ultralytics import YOLO\n- from PIL import Image\n- import torch\n- from torch import nn\n- from torch.nn import functional\n- import torchvision\n- import pdb\n- from ultralytics.yolo.utils import is_git_dir\n- from ultralytics.yolo.utils import ops\n- from ultralytics.yolo.utils.torch_utils import select_device, smart_inference_mode\n- from ultralytics.yolo.v8.segment import SegmentationPredictor\n- from ultralytics.yolo.cfg import get_cfg\n- import sys\n- from mmengine.model import BaseModule\n- from mmdet3d.registry import MODELS\n- from mmdet3d.structures.bbox_3d import points_cam2img, rotation_3d_in_axis\n- from functools import partial\n- from mmdet3d.structures.points import CameraPoints, LiDARPoints, DepthPoints\n- from abc import abstractmethod\n- import warnings\n- import numpy\n- import os\n- import csv\n\nFile: oneformer3d/instance_criterion.py\nImports:\n- import torch\n- import torch.nn.functional\n- from scipy.optimize import linear_sum_assignment\n- from mmengine.structures import InstanceData\n- from mmdet3d.registry import MODELS, TASK_UTILS\n\nFile: oneformer3d/instance_merge.py\nImports:\n- import torch\n- import numpy\n- from scipy.optimize import linear_sum_assignment\n- import torch.nn.functional\n- from mmdet3d.structures import AxisAlignedBboxOverlaps3D\n- import pdb\n- from sklearn.cluster import AgglomerativeClustering\n- import networkx\n\nFile: oneformer3d/instance_seg_eval.py\nImports:\n- import numpy\n- from mmengine.logging import print_log\n- from terminaltables import AsciiTable\n- import pdb\n- from mmdet3d.evaluation.functional.instance_seg_eval import scannet_eval\n\nFile: oneformer3d/instance_seg_metric.py\nImports:\n- from mmengine.logging import MMLogger\n- from mmdet3d.evaluation import InstanceSegMetric\n- from mmdet3d.registry import METRICS\n- from instance_seg_eval import instance_seg_eval\n\nFile: oneformer3d/loading.py\nImports:\n- import mmengine\n- import numpy\n- from typing import List, Optional, Union\n- import os\n- import pdb\n- import json\n- from mmdet3d.datasets.transforms import LoadAnnotations3D\n- from mmdet3d.datasets.transforms.loading import get\n- from mmdet3d.datasets.transforms.loading import NormalizePointsColor\n- from mmcv.transforms.base import BaseTransform\n- from mmcv.transforms import Compose, LoadImageFromFile\n- from mmdet3d.registry import TRANSFORMS\n- from mmdet3d.structures.bbox_3d import get_box_type\n- from mmdet3d.structures.points import BasePoints, get_points_type\n\nFile: oneformer3d/mask_matrix_nms.py\nImports:\n- import torch\n\nFile: oneformer3d/merge_criterion.py\nImports:\n- from multiprocessing import reduction\n- import torch\n- import torch.nn.functional\n- import torch.nn\n- import numpy\n- import pdb\n- import time\n- from mmdet3d.registry import MODELS\n\nFile: oneformer3d/merge_head.py\nImports:\n- import torch\n- import torch.nn\n- import torch.nn.functional\n- import pdb\n- from mmengine.model import BaseModule\n- from mmdet3d.registry import MODELS\n\nFile: oneformer3d/mink_unet.py\nImports:\n- from enum import Enum\n- from collections.abc import Sequence\n- import torch.nn\n- import MinkowskiEngine\n- import MinkowskiEngine.MinkowskiOps\n- from MinkowskiEngine import MinkowskiReLU\n- from mmengine.model import BaseModule\n- from mmdet3d.registry import MODELS\n\nFile: oneformer3d/mixformer3d.py\nImports:\n- import torch\n- import torch.nn\n- import torch.nn.functional\n- from torch_scatter import scatter_mean, scatter\n- import MinkowskiEngine\n- import pointops\n- import pdb\n- import time\n- from functools import partial\n- from mmdet3d.registry import MODELS\n- from mmdet3d.structures import PointData\n- from mmdet3d.models import Base3DDetector\n- from mmdet3d.structures.bbox_3d import get_proj_mat_by_coord_type\n- from mmengine.structures import InstanceData\n- from mask_matrix_nms import mask_matrix_nms\n- from oneformer3d import ScanNetOneFormer3DMixin\n- from instance_merge import ins_merge_mat, ins_cat, ins_merge, OnlineMerge, GTMerge\n- import numpy\n- from img_backbone import point_sample\n- import os\n\nFile: oneformer3d/multilevel_memory.py\nImports:\n- import torch\n- import torch.nn\n- from mmdet3d.registry import MODELS\n- from mmengine.model import BaseModule, constant_init\n- import pdb\n\nFile: oneformer3d/oneformer3d.py\nImports:\n- import torch\n- import torch.nn.functional\n- import spconv.pytorch\n- from torch_scatter import scatter_mean\n- import MinkowskiEngine\n- import pointops\n- import copy\n- from mmdet3d.registry import MODELS\n- from mmdet3d.structures import PointData\n- from mmengine.structures import InstanceData\n- from mmdet3d.models import Base3DDetector\n- from mask_matrix_nms import mask_matrix_nms\n- import numpy\n- import os\n\nFile: oneformer3d/query_decoder.py\nImports:\n- import torch\n- import torch.nn\n- import pdb\n- import time\n- from mmengine.model import BaseModule\n- from mmdet3d.registry import MODELS\n- from torch_scatter import scatter_mean, scatter_add\n\nFile: oneformer3d/scannet_dataset.py\nImports:\n- from os import path\n- import numpy\n- import random\n- from mmdet3d.datasets.scannet_dataset import ScanNetSegDataset\n- from mmdet3d.registry import DATASETS\n\nFile: oneformer3d/semantic_criterion.py\nImports:\n- import torch\n- import torch.nn.functional\n- from mmdet3d.registry import MODELS\n\nFile: oneformer3d/transforms_3d.py\nImports:\n- import numpy\n- import scipy\n- import torch\n- from torch_scatter import scatter_mean\n- from mmcv.transforms import BaseTransform\n- import pdb\n- from mmdet3d.registry import TRANSFORMS\n\nFile: oneformer3d/unified_criterion.py\nImports:\n- from mmdet3d.registry import MODELS\n- from mmengine.structures import InstanceData\n- import torch\n\nFile: oneformer3d/unified_metric.py\nImports:\n- import torch\n- import numpy\n- from typing import Sequence\n- from mmengine.logging import MMLogger\n- from mmdet3d.evaluation.metrics import SegMetric\n- from mmdet3d.registry import METRICS\n- from mmdet3d.evaluation import panoptic_seg_eval, seg_eval\n- from instance_seg_eval import instance_seg_eval, instance_cat_agnostic_eval\n\nFile: thirdparty/pointops/__init__.py\nImports:\n- from functions import *\n\nFile: thirdparty/pointops/functions/__init__.py\nImports:\n- from query import knn_query, ball_query, random_ball_query\n- from sampling import farthest_point_sampling\n- from grouping import grouping, grouping2\n- from interpolation import interpolation, interpolation2\n- from subtraction import subtraction\n- from aggregation import aggregation\n- from attention import attention_relation_step, attention_fusion_step\n- from utils import query_and_group, knn_query_and_group, ball_query_and_group, batch2offset, offset2batch\n\nFile: thirdparty/pointops/functions/aggregation.py\nImports:\n- import torch\n- from torch.autograd import Function\n- from pointops._C import aggregation_forward_cuda, aggregation_backward_cuda\n\nFile: thirdparty/pointops/functions/attention.py\nImports:\n- import torch\n- from torch.autograd import Function\n- from pointops._C import attention_relation_step_forward_cuda, attention_relation_step_backward_cuda, attention_fusion_step_forward_cuda, attention_fusion_step_backward_cuda\n\nFile: thirdparty/pointops/functions/grouping.py\nImports:\n- import torch\n- from torch.autograd import Function\n- from pointops._C import grouping_forward_cuda, grouping_backward_cuda\n\nFile: thirdparty/pointops/functions/interpolation.py\nImports:\n- import torch\n- from torch.autograd import Function\n- from pointops._C import interpolation_forward_cuda, interpolation_backward_cuda\n- from query import knn_query\n\nFile: thirdparty/pointops/functions/query.py\nImports:\n- import torch\n- from torch.autograd import Function\n- from pointops._C import knn_query_cuda, random_ball_query_cuda, ball_query_cuda\n\nFile: thirdparty/pointops/functions/sampling.py\nImports:\n- import torch\n- from torch.autograd import Function\n- from pointops._C import farthest_point_sampling_cuda\n\nFile: thirdparty/pointops/functions/subtraction.py\nImports:\n- import torch\n- from torch.autograd import Function\n- from pointops._C import subtraction_forward_cuda, subtraction_backward_cuda\n\nFile: thirdparty/pointops/functions/utils.py\nImports:\n- import torch\n- from pointops import knn_query, ball_query, grouping\n\nFile: thirdparty/pointops/setup.py\nImports:\n- import os\n- from setuptools import setup\n- from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n- from distutils.sysconfig import get_config_vars\n\nFile: thirdparty/pointops/src/__init__.py\nImports:\n- (None)\n\nFile: tools/__init__.py\nImports:\n- (None)\n\nFile: tools/create_data.py\nImports:\n- import argparse\n- from os import path\n- from indoor_converter import create_indoor_info_file\n- from update_infos_to_v2 import update_pkl_infos\n\nFile: tools/fix_spconv_checkpoint.py\nImports:\n- import argparse\n- import torch\n\nFile: tools/indoor_converter.py\nImports:\n- import os\n- import mmengine\n- from scannet_data_utils import ScanNetData, ScanNetSVData, ScanNetMVData\n- from scenenn_data_utils import SceneNNData, SceneNNMVData\n- from trscan_data_utils import TRScanData, TRScanMVData\n\nFile: tools/scannet_data_utils.py\nImports:\n- import os\n- from concurrent import futures\n- from os import path\n- import mmengine\n- import numpy\n- import pdb\n\nFile: tools/scenenn_data_utils.py\nImports:\n- import os\n- from concurrent import futures\n- from os import path\n- import mmengine\n- import numpy\n- import pdb\n- import math\n\nFile: tools/test.py\nImports:\n- import argparse\n- import os\n- import os.path\n- from mmengine.config import Config, ConfigDict, DictAction\n- from mmengine.registry import RUNNERS\n- from mmengine.runner import Runner\n- from mmdet3d.utils import replace_ceph_backend\n\nFile: tools/train.py\nImports:\n- import argparse\n- import logging\n- import os\n- import os.path\n- from mmengine.config import Config, DictAction\n- from mmengine.logging import print_log\n- from mmengine.registry import RUNNERS\n- from mmengine.runner import Runner\n- from mmdet3d.utils import replace_ceph_backend\n\nFile: tools/trscan_data_utils.py\nImports:\n- import os\n- from concurrent import futures\n- from os import path\n- import mmengine\n- import numpy\n- import pdb\n- import math\n\nFile: tools/update_infos_to_v2.py\nImports:\n- import argparse\n- import time\n- from os import path\n- from pathlib import Path\n- import pdb\n- import mmengine\n\nFile: vis_demo/online_demo.py\nImports:\n- from argparse import ArgumentParser\n- import os\n- from os import path\n- import random\n- import numpy\n- import torch\n- import mmengine\n- from mmdet3d.apis import init_model\n- from mmdet3d.registry import DATASETS\n- from mmengine.dataset import Compose, pseudo_collate\n- import open3d\n- from PIL import Image\n- from utils.vis_utils import vis_pointcloud, Vis_color\n- import sys\n\nFile: vis_demo/stream_demo.py\nImports:\n- from argparse import ArgumentParser\n- import os\n- import time\n- import numpy\n- import torch\n- import warnings\n- from mmdet3d.registry import MODELS\n- from mmengine.registry import init_default_scope\n- from mmengine.dataset import pseudo_collate\n- from mmdet3d.structures import Det3DDataSample, PointData\n- from mmengine.config import Config\n- from mmengine.runner import load_checkpoint\n- import sys\n- from vis_demo.utils.vis_utils import vis_pointcloud, Vis_color\n- from vis_demo.utils.stream_data_utils import DataPreprocessor, StreamDataloader, StreamBotDataloader\n\nFile: vis_demo/utils/__init__.py\nImports:\n- from vis_utils import vis_pointcloud, Vis_color\n- from stream_data_utils import DataPreprocessor, StreamDataloader, StreamBotDataloader\n\nFile: vis_demo/utils/ply_utils.py\nImports:\n- import numpy\n- import struct\n- import math\n\nFile: vis_demo/utils/stream_data_utils.py\nImports:\n- import cv2\n- import numpy\n- import os\n- from sklearn.cluster import KMeans\n- from fastsam import FastSAM\n- from tqdm import tqdm\n\nFile: vis_demo/utils/vis_utils.py\nImports:\n- import open3d\n- import os
Execution plans: (A) *step1: Download and install Miniconda from the official website;*step2: Create a new conda environment named ESAM with Python 3.8 and activate it;*step3: Install PyTorch following official instructions;*step4: Uninstall PyTorch;*step5: Install mmcv, mmdet3d, and mmdet using mmdetection3d installation guide;*step6: Install MinkowskiEngine by building from source;*step7: Install SAM and FastSAM, then download their respective checkpoints and place them in the 'data' folder;*step8: Download the backbone checkpoint from Mask3D and place it in 'work_dirs/tmp';*step9: Install pointops by running its setup script;*step10: Clone the segmentator repository into the 'data' folder for mesh segmentation;*step11: Install other dependencies listed in requirements.txt; (B) *step1: Download and install Miniconda from the official website;*step2: Create a new conda environment named ESAM with Python 3.8 and activate it;*step3: Install PyTorch following official instructions;*step4: Install mmcv, mmdet3d, and mmdet using mmdetection3d installation guide;*step5: Install MinkowskiEngine by building from source;*step6: Install other dependencies listed in requirements.txt;*step7: Install SAM and FastSAM, then download their respective checkpoints and place them in the 'data' folder;*step8: Download the backbone checkpoint from Mask3D and place it in 'work_dirs/tmp';*step9: Install pointops by running its setup script;*step10: Clone the segmentator repository into the 'data' folder for mesh segmentation; (C) *step1: Download and install Miniconda from the official website;*step2: Create a new conda environment named ESAM with Python 3.8 and activate it;*step3: Install PyTorch following official instructions;*step4: Install mmcv, mmdet3d, and mmdet using mmdetection3d installation guide;*step5: Install MinkowskiEngine by building from source;*step6: Install SAM and FastSAM, then download their respective checkpoints and place them in the 'data' folder;*step7: Download the backbone checkpoint from Mask3D and place it in 'work_dirs/tmp';*step8: Install pointops by running its setup script;*step9: Clone the segmentator repository into the 'data' folder for mesh segmentation;*step10: Install other dependencies listed in requirements.txt; (D) *step1: Download and install Miniconda from the official website;*step2: Create a new conda environment named ESAM with Python 3.8 and activate it;*step3: Install PyTorch following official instructions;*step4: Install mmcv, mmdet3d, and mmdet using mmdetection3d installation guide;*step5: Install SAM and FastSAM, then download their respective checkpoints and place them in the 'data' folder;*step6: Download the backbone checkpoint from Mask3D and place it in 'work_dirs/tmp';*step7: Install pointops by running its setup script;*step8: Clone the segmentator repository into the 'data' folder for mesh segmentation;*step9: Install other dependencies listed in requirements.txt; (E) *step1: Download and install Miniconda from the official website;*step2: Create a new conda environment named ESAM with Python 3.8 and activate it;*step3: Install PyTorch following official instructions;*step4: Install mmcv, mmdet3d, and mmdet using mmdetection3d installation guide;*step5: Install MinkowskiEngine by building from source;*step6: Install SAM and FastSAM, then download their respective checkpoints and place them in the 'data' folder;*step7: Download the backbone checkpoint from Mask3D and place it in 'work_dirs/tmp';*step8: Clone the segmentator repository into the 'data' folder for mesh segmentation;*step9: Install other dependencies listed in requirements.txt; (F) *step1: Download and install Miniconda from the official website;*step2: Create a new conda environment named ESAM with Python 3.8 and activate it;*step3: Install MinkowskiEngine by building from source;*step4: Install PyTorch following official instructions;*step5: Install mmcv, mmdet3d, and mmdet using mmdetection3d installation guide;*step6: Install SAM and FastSAM, then download their respective checkpoints and place them in the 'data' folder;*step7: Download the backbone checkpoint from Mask3D and place it in 'work_dirs/tmp';*step8: Install pointops by running its setup script;*step9: Clone the segmentator repository into the 'data' folder for mesh segmentation;*step10: Install other dependencies listed in requirements.txt; (G) *step1: Download and install Miniconda from the official website;*step2: Create a new conda environment named ESAM with Python 3.8 and activate it;*step3: Install PyTorch following official instructions;*step4: Install mmcv, mmdet3d, and mmdet using mmdetection3d installation guide;*step5: Install MinkowskiEngine by building from source;*step6: Install SAM and FastSAM, then download their respective checkpoints and place them in the 'data' folder;*step7: Delete the 'data' folder;*step8: Download the backbone checkpoint from Mask3D and place it in 'work_dirs/tmp';*step9: Install pointops by running its setup script;*step10: Clone the segmentator repository into the 'data' folder for mesh segmentation;*step11: Install other dependencies listed in requirements.txt
The correct execution plan is (C)

Request: Choose the correct execution plan to setup the environment for running the repository according to the repository information.
Repo information: File Analysis:\n\nFile: src/config.py\nImports:\n- from dataclasses import dataclass\n- from pathlib import Path\n- from typing import Literal, Optional, Type, TypeVar\n- from dacite import Config, from_dict\n- from omegaconf import DictConfig, OmegaConf\n- from dataset import DatasetCfgWrapper\n- from dataset.data_module import DataLoaderCfg\n- from loss import LossCfgWrapper\n- from model.decoder import DecoderCfg\n- from model.encoder import EncoderCfg\n- from model.model_wrapper import OptimizerCfg, TestCfg, TrainCfg\n\nFile: src/dataset/__init__.py\nImports:\n- from dataclasses import fields\n- from torch.utils.data import Dataset\n- from dataset_scannet_pose import DatasetScannetPose, DatasetScannetPoseCfgWrapper\n- from misc.step_tracker import StepTracker\n- from dataset_re10k import DatasetRE10k, DatasetRE10kCfg, DatasetRE10kCfgWrapper, DatasetDL3DVCfgWrapper, DatasetScannetppCfgWrapper\n- from types import Stage\n- from view_sampler import get_view_sampler\n\nFile: src/dataset/data_module.py\nImports:\n- import random\n- from dataclasses import dataclass\n- from typing import Callable\n- import numpy\n- import torch\n- from lightning.pytorch import LightningDataModule\n- from torch import Generator, nn\n- from torch.utils.data import DataLoader, Dataset, IterableDataset\n- from misc.step_tracker import StepTracker\n- from  import DatasetCfgWrapper, get_dataset\n- from types import DataShim, Stage\n- from validation_wrapper import ValidationWrapper\n\nFile: src/dataset/dataset.py\nImports:\n- from dataclasses import dataclass\n- from view_sampler import ViewSamplerCfg\n\nFile: src/dataset/dataset_re10k.py\nImports:\n- import json\n- from dataclasses import dataclass\n- from functools import cached_property\n- from io import BytesIO\n- from pathlib import Path\n- from typing import Literal\n- import torch\n- import torchvision.transforms\n- from einops import rearrange, repeat\n- from jaxtyping import Float, UInt8\n- from PIL import Image\n- from torch import Tensor\n- from torch.utils.data import IterableDataset\n- from geometry.projection import get_fov\n- from dataset import DatasetCfgCommon\n- from shims.augmentation_shim import apply_augmentation_shim\n- from shims.crop_shim import apply_crop_shim\n- from types import Stage\n- from view_sampler import ViewSampler\n- from misc.cam_utils import camera_normalization\n\nFile: src/dataset/dataset_scannet_pose.py\nImports:\n- import json\n- import os\n- import os.path\n- from dataclasses import dataclass\n- from functools import cached_property\n- from io import BytesIO\n- from pathlib import Path\n- from typing import Literal\n- import torch\n- import torchvision.transforms\n- import numpy\n- from einops import rearrange, repeat\n- from jaxtyping import Float, UInt8\n- from PIL import Image\n- from torch import Tensor\n- from torch.utils.data import IterableDataset\n- from torch.distributed import get_rank, get_world_size\n- from geometry.projection import get_fov\n- from dataset import DatasetCfgCommon\n- from shims.augmentation_shim import apply_augmentation_shim\n- from shims.crop_shim import apply_crop_shim\n- from types import Stage\n- from view_sampler import ViewSampler\n\nFile: src/dataset/shims/augmentation_shim.py\nImports:\n- import torch\n- from jaxtyping import Float\n- from torch import Tensor\n- from types import AnyExample, AnyViews\n\nFile: src/dataset/shims/bounds_shim.py\nImports:\n- import torch\n- from einops import einsum, reduce, repeat\n- from jaxtyping import Float\n- from torch import Tensor\n- from types import BatchedExample\n\nFile: src/dataset/shims/crop_shim.py\nImports:\n- import numpy\n- import torch\n- from einops import rearrange\n- from jaxtyping import Float\n- from PIL import Image\n- from torch import Tensor\n- from types import AnyExample, AnyViews\n\nFile: src/dataset/shims/normalize_shim.py\nImports:\n- import torch\n- from einops import einsum, reduce, repeat\n- from jaxtyping import Float\n- from torch import Tensor\n- from types import BatchedExample\n\nFile: src/dataset/shims/patch_shim.py\nImports:\n- from types import BatchedExample, BatchedViews\n\nFile: src/dataset/types.py\nImports:\n- from typing import Callable, Literal, TypedDict\n- from jaxtyping import Float, Int64\n- from torch import Tensor\n\nFile: src/dataset/validation_wrapper.py\nImports:\n- from typing import Iterator, Optional\n- import torch\n- from torch.utils.data import Dataset, IterableDataset\n\nFile: src/dataset/view_sampler/__init__.py\nImports:\n- from typing import Any\n- from misc.step_tracker import StepTracker\n- from types import Stage\n- from view_sampler import ViewSampler\n- from view_sampler_all import ViewSamplerAll, ViewSamplerAllCfg\n- from view_sampler_arbitrary import ViewSamplerArbitrary, ViewSamplerArbitraryCfg\n- from view_sampler_bounded import ViewSamplerBounded, ViewSamplerBoundedCfg\n- from view_sampler_evaluation import ViewSamplerEvaluation, ViewSamplerEvaluationCfg\n\nFile: src/dataset/view_sampler/additional_view_hack.py\nImports:\n- import torch\n- from jaxtyping import Int\n- from torch import Tensor\n\nFile: src/dataset/view_sampler/view_sampler.py\nImports:\n- from abc import ABC, abstractmethod\n- from typing import Generic, TypeVar\n- import torch\n- from jaxtyping import Float, Int64\n- from torch import Tensor\n- from misc.step_tracker import StepTracker\n- from types import Stage\n\nFile: src/dataset/view_sampler/view_sampler_all.py\nImports:\n- from dataclasses import dataclass\n- from typing import Literal\n- import torch\n- from jaxtyping import Float, Int64\n- from torch import Tensor\n- from view_sampler import ViewSampler\n\nFile: src/dataset/view_sampler/view_sampler_arbitrary.py\nImports:\n- from dataclasses import dataclass\n- from typing import Literal\n- import torch\n- from jaxtyping import Float, Int64\n- from torch import Tensor\n- from additional_view_hack import add_addtional_context_index\n- from view_sampler import ViewSampler\n\nFile: src/dataset/view_sampler/view_sampler_bounded.py\nImports:\n- from dataclasses import dataclass\n- from typing import Literal\n- import torch\n- from jaxtyping import Float, Int64\n- from torch import Tensor\n- from view_sampler import ViewSampler\n\nFile: src/dataset/view_sampler/view_sampler_evaluation.py\nImports:\n- import json\n- from dataclasses import dataclass\n- from pathlib import Path\n- from typing import Literal\n- import torch\n- from dacite import Config, from_dict\n- from jaxtyping import Float, Int64\n- from torch import Tensor\n- from evaluation.evaluation_index_generator import IndexEntry\n- from global_cfg import get_cfg\n- from misc.step_tracker import StepTracker\n- from types import Stage\n- from additional_view_hack import add_addtional_context_index\n- from view_sampler import ViewSampler\n\nFile: src/eval_pose.py\nImports:\n- import json\n- from dataclasses import dataclass\n- from pathlib import Path\n- import hydra\n- import torch\n- from jaxtyping import install_import_hook\n- from omegaconf import DictConfig\n- from lightning import Trainer\n- from src.evaluation.pose_evaluator import PoseEvaluator\n- from src.loss import get_losses, LossCfgWrapper\n- from src.misc.wandb_tools import update_checkpoint_path\n- from src.model.decoder import get_decoder\n- from src.model.encoder import get_encoder\n\nFile: src/evaluation/evaluation_cfg.py\nImports:\n- from dataclasses import dataclass\n- from pathlib import Path\n\nFile: src/evaluation/evaluation_index_generator.py\nImports:\n- import json\n- from dataclasses import asdict, dataclass\n- from pathlib import Path\n- from typing import Optional\n- import torch\n- from einops import rearrange\n- from lightning.pytorch import LightningModule\n- from tqdm import tqdm\n- from geometry.epipolar_lines import project_rays\n- from geometry.projection import get_world_rays, sample_image_grid\n- from misc.image_io import save_image\n- from visualization.annotation import add_label\n- from visualization.layout import add_border, hcat\n\nFile: src/evaluation/metric_computer.py\nImports:\n- import csv\n- import os\n- from pathlib import Path\n- import torch\n- from lightning.pytorch import LightningModule\n- from tabulate import tabulate\n- from loss.loss_ssim import ssim\n- from misc.image_io import load_image, save_image\n- from misc.utils import inverse_normalize, get_overlap_tag\n- from visualization.annotation import add_label\n- from visualization.color_map import apply_color_map_to_image\n- from visualization.layout import add_border, hcat, vcat\n- from evaluation_cfg import EvaluationCfg\n- from metrics import compute_lpips, compute_psnr, compute_ssim\n\nFile: src/evaluation/metrics.py\nImports:\n- from functools import cache\n- import torch\n- from einops import reduce\n- from jaxtyping import Float\n- from lpips import LPIPS\n- from skimage.metrics import structural_similarity\n- from torch import Tensor\n\nFile: src/evaluation/pose_evaluator.py\nImports:\n- import json\n- import os\n- import sys\n- from typing import Any\n- import math\n- from pytorch_lightning.utilities.types import STEP_OUTPUT\n- from dataset.data_module import get_data_shim\n- from dataset.types import BatchedExample\n- from misc.cam_utils import camera_normalization, pose_auc, update_pose, get_pnp_pose\n- import csv\n- from pathlib import Path\n- import cv2\n- import numpy\n- import torch\n- import torch.nn\n- import torch.nn.functional\n- from einops import rearrange\n- from lightning import LightningModule\n- from tabulate import tabulate\n- from loss.loss_ssim import ssim\n- from misc.image_io import load_image, save_image\n- from misc.utils import inverse_normalize, get_overlap_tag\n- from visualization.annotation import add_label\n- from visualization.color_map import apply_color_map_to_image\n- from visualization.layout import add_border, hcat, vcat\n- from evaluation_cfg import EvaluationCfg\n- from metrics import compute_lpips, compute_psnr, compute_ssim, compute_pose_error\n\nFile: src/geometry/camera_emb.py\nImports:\n- from einops import rearrange\n- from projection import sample_image_grid, get_local_rays\n- from misc.sht import rsh_cart_2, rsh_cart_4, rsh_cart_6, rsh_cart_8\n\nFile: src/geometry/epipolar_lines.py\nImports:\n- import itertools\n- from typing import Iterable, Literal, Optional, TypedDict\n- import torch\n- from einops import einsum, repeat\n- from jaxtyping import Bool, Float\n- from torch import Tensor\n- from torch.utils.data.dataloader import default_collate\n- from projection import get_world_rays, homogenize_points, homogenize_vectors, intersect_rays, project_camera_space\n\nFile: src/geometry/projection.py\nImports:\n- from math import prod\n- import torch\n- from einops import einsum, rearrange, reduce, repeat\n- from jaxtyping import Bool, Float, Int64\n- from torch import Tensor\n\nFile: src/geometry/ptc_geometry.py\nImports:\n- import torch\n- import numpy\n- from scipy.spatial import cKDTree\n- from model.encoder.backbone.croco.misc import invalid_to_zeros, invalid_to_nans\n\nFile: src/global_cfg.py\nImports:\n- from typing import Optional\n- from omegaconf import DictConfig\n\nFile: src/loss/__init__.py\nImports:\n- from loss import Loss\n- from loss_depth import LossDepth, LossDepthCfgWrapper\n- from loss_lpips import LossLpips, LossLpipsCfgWrapper\n- from loss_mse import LossMse, LossMseCfgWrapper\n\nFile: src/loss/loss.py\nImports:\n- from abc import ABC, abstractmethod\n- from dataclasses import fields\n- from typing import Generic, TypeVar\n- from jaxtyping import Float\n- from torch import Tensor, nn\n- from dataset.types import BatchedExample\n- from model.decoder.decoder import DecoderOutput\n- from model.types import Gaussians\n\nFile: src/loss/loss_depth.py\nImports:\n- from dataclasses import dataclass\n- import torch\n- from einops import reduce\n- from jaxtyping import Float\n- from torch import Tensor\n- from dataset.types import BatchedExample\n- from model.decoder.decoder import DecoderOutput\n- from model.types import Gaussians\n- from loss import Loss\n\nFile: src/loss/loss_lpips.py\nImports:\n- from dataclasses import dataclass\n- import torch\n- from einops import rearrange\n- from jaxtyping import Float\n- from lpips import LPIPS\n- from torch import Tensor\n- from dataset.types import BatchedExample\n- from misc.nn_module_tools import convert_to_buffer\n- from model.decoder.decoder import DecoderOutput\n- from model.types import Gaussians\n- from loss import Loss\n\nFile: src/loss/loss_mse.py\nImports:\n- from dataclasses import dataclass\n- from jaxtyping import Float\n- from torch import Tensor\n- from dataset.types import BatchedExample\n- from model.decoder.decoder import DecoderOutput\n- from model.types import Gaussians\n- from loss import Loss\n\nFile: src/loss/loss_point.py\nImports:\n- import torch\n- import torch.nn\n- from copy import copy, deepcopy\n- from geometry.ptc_geometry import geotrf, inv, normalize_pointcloud, depthmap_to_pts3d\n\nFile: src/loss/loss_ssim.py\nImports:\n- import warnings\n- from typing import List, Optional, Tuple, Union\n- import torch\n- import torch.nn.functional\n- from torch import Tensor\n\nFile: src/main.py\nImports:\n- import os\n- from pathlib import Path\n- import hydra\n- import torch\n- import wandb\n- import signal\n- from colorama import Fore\n- from jaxtyping import install_import_hook\n- from lightning.pytorch import Trainer\n- from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n- from lightning.pytorch.loggers.wandb import WandbLogger\n- from lightning.pytorch.plugins.environments import SLURMEnvironment\n- from omegaconf import DictConfig, OmegaConf\n- from src.misc.weight_modify import checkpoint_filter_fn\n- from src.model.distiller import get_distiller\n\nFile: src/misc/LocalLogger.py\nImports:\n- import os\n- from pathlib import Path\n- from typing import Any, Optional\n- from lightning.pytorch.loggers.logger import Logger\n- from lightning.pytorch.utilities import rank_zero_only\n- from PIL import Image\n\nFile: src/misc/benchmarker.py\nImports:\n- import json\n- from collections import defaultdict\n- from contextlib import contextmanager\n- from pathlib import Path\n- from time import time\n- import numpy\n- import torch\n\nFile: src/misc/cam_utils.py\nImports:\n- import cv2\n- import numpy\n- import torch\n- from jaxtyping import Float\n- from torch import Tensor\n\nFile: src/misc/collation.py\nImports:\n- from typing import Callable, Dict, Union\n- from torch import Tensor\n\nFile: src/misc/discrete_probability_distribution.py\nImports:\n- import torch\n- from einops import reduce\n- from jaxtyping import Float, Int64\n- from torch import Tensor\n\nFile: src/misc/heterogeneous_pairings.py\nImports:\n- import torch\n- from einops import repeat\n- from jaxtyping import Int\n- from torch import Tensor\n\nFile: src/misc/image_io.py\nImports:\n- import io\n- from pathlib import Path\n- from typing import Union\n- import numpy\n- import skvideo.io\n- import torch\n- import torchvision.transforms\n- from einops import rearrange, repeat\n- from jaxtyping import Float, UInt8\n- from matplotlib.figure import Figure\n- from PIL import Image\n- from torch import Tensor\n\nFile: src/misc/nn_module_tools.py\nImports:\n- from torch import nn\n\nFile: src/misc/sh_rotation.py\nImports:\n- from math import isqrt\n- import torch\n- from e3nn.o3 import matrix_to_angles, wigner_D\n- from einops import einsum\n- from jaxtyping import Float\n- from torch import Tensor\n\nFile: src/misc/sht.py\nImports:\n- import torch\n- from typing import Optional\n- import torch\n\nFile: src/misc/step_tracker.py\nImports:\n- from multiprocessing import RLock\n- import torch\n- from jaxtyping import Int64\n- from torch import Tensor\n- from torch.multiprocessing import Manager\n\nFile: src/misc/utils.py\nImports:\n- import torch\n- from src.visualization.color_map import apply_color_map_to_image\n\nFile: src/misc/wandb_tools.py\nImports:\n- from pathlib import Path\n- import wandb\n\nFile: src/misc/weight_modify.py\nImports:\n- import logging\n- from typing import List, Dict\n- import math\n- import torch\n- from torch import nn\n- import torch.nn.functional\n\nFile: src/model/decoder/__init__.py\nImports:\n- from decoder import Decoder\n- from decoder_splatting_cuda import DecoderSplattingCUDA, DecoderSplattingCUDACfg\n\nFile: src/model/decoder/cuda_splatting.py\nImports:\n- from math import isqrt\n- from typing import Literal\n- import torch\n- from diff_gaussian_rasterization import GaussianRasterizationSettings, GaussianRasterizer\n- from einops import einsum, rearrange, repeat\n- from jaxtyping import Float\n- from torch import Tensor\n- from geometry.projection import get_fov, homogenize_points\n\nFile: src/model/decoder/decoder.py\nImports:\n- from abc import ABC, abstractmethod\n- from dataclasses import dataclass\n- from typing import Generic, Literal, TypeVar\n- from jaxtyping import Float\n- from torch import Tensor, nn\n- from types import Gaussians\n\nFile: src/model/decoder/decoder_splatting_cuda.py\nImports:\n- from dataclasses import dataclass\n- from typing import Literal\n- import torch\n- from einops import rearrange, repeat\n- from jaxtyping import Float\n- from torch import Tensor\n- from dataset import DatasetCfg\n- from types import Gaussians\n- from cuda_splatting import DepthRenderingMode, render_cuda\n- from decoder import Decoder, DecoderOutput\n\nFile: src/model/distiller/__init__.py\nImports:\n- import torch\n- from dust3d_backbone import Dust3R\n\nFile: src/model/distiller/dust3d_backbone.py\nImports:\n- from copy import deepcopy\n- import torch\n- from encoder.backbone.croco.croco import CroCoNet\n- from encoder.backbone.croco.misc import fill_default_args, freeze_all_params, is_symmetrized, interleave, transpose_to_landscape, make_batch_symmetric\n- from encoder.backbone.croco.patch_embed import get_patch_embed\n- from encoder.heads import head_factory\n\nFile: src/model/encoder/__init__.py\nImports:\n- from typing import Optional\n- from encoder import Encoder\n- from encoder_noposplat import EncoderNoPoSplatCfg, EncoderNoPoSplat\n- from encoder_noposplat_multi import EncoderNoPoSplatMulti\n- from visualization.encoder_visualizer import EncoderVisualizer\n\nFile: src/model/encoder/backbone/__init__.py\nImports:\n- from typing import Any\n- import torch.nn\n- from backbone import Backbone\n- from backbone_croco_multiview import AsymmetricCroCoMulti\n- from backbone_dino import BackboneDino, BackboneDinoCfg\n- from backbone_resnet import BackboneResnet, BackboneResnetCfg\n- from backbone_croco import AsymmetricCroCo, BackboneCrocoCfg\n\nFile: src/model/encoder/backbone/backbone.py\nImports:\n- from abc import ABC, abstractmethod\n- from typing import Generic, TypeVar\n- from jaxtyping import Float\n- from torch import Tensor, nn\n- from dataset.types import BatchedViews\n\nFile: src/model/encoder/backbone/backbone_croco.py\nImports:\n- from copy import deepcopy\n- from dataclasses import dataclass\n- from typing import Literal\n- import torch\n- from torch import nn\n- from croco.blocks import DecoderBlock\n- from croco.croco import CroCoNet\n- from croco.misc import fill_default_args, freeze_all_params, transpose_to_landscape, is_symmetrized, interleave, make_batch_symmetric\n- from croco.patch_embed import get_patch_embed\n- from backbone import Backbone\n- from geometry.camera_emb import get_intrinsic_embedding\n\nFile: src/model/encoder/backbone/backbone_croco_multiview.py\nImports:\n- from copy import deepcopy\n- from dataclasses import dataclass\n- from typing import Literal\n- import torch\n- from einops import rearrange\n- from torch import nn\n- from backbone_croco import BackboneCrocoCfg\n- from croco.blocks import DecoderBlock\n- from croco.croco import CroCoNet\n- from croco.misc import fill_default_args, freeze_all_params, transpose_to_landscape, is_symmetrized, interleave, make_batch_symmetric\n- from croco.patch_embed import get_patch_embed\n- from backbone import Backbone\n- from geometry.camera_emb import get_intrinsic_embedding\n\nFile: src/model/encoder/backbone/backbone_dino.py\nImports:\n- from dataclasses import dataclass\n- from typing import Literal\n- import torch\n- from einops import rearrange, repeat\n- from jaxtyping import Float\n- from torch import Tensor, nn\n- from dataset.types import BatchedViews\n- from backbone import Backbone\n- from backbone_resnet import BackboneResnet, BackboneResnetCfg\n\nFile: src/model/encoder/backbone/backbone_resnet.py\nImports:\n- import functools\n- from dataclasses import dataclass\n- from typing import Literal\n- import torch\n- import torch.nn.functional\n- import torchvision\n- from einops import rearrange\n- from jaxtyping import Float\n- from torch import Tensor, nn\n- from torchvision.models import ResNet\n- from dataset.types import BatchedViews\n- from backbone import Backbone\n\nFile: src/model/encoder/backbone/croco/__init__.py\nImports:\n- (None)\n\nFile: src/model/encoder/backbone/croco/blocks.py\nImports:\n- import torch\n- import torch.nn\n- from itertools import repeat\n- import collections.abc\n\nFile: src/model/encoder/backbone/croco/croco.py\nImports:\n- import torch\n- import torch.nn\n- from functools import partial\n- from blocks import Block, DecoderBlock, PatchEmbed\n- from pos_embed import get_2d_sincos_pos_embed, RoPE2D\n- from masking import RandomMask\n\nFile: src/model/encoder/backbone/croco/curope/__init__.py\nImports:\n- from curope2d import cuRoPE2D\n\nFile: src/model/encoder/backbone/croco/curope/curope2d.py\nImports:\n- import torch\n\nFile: src/model/encoder/backbone/croco/curope/setup.py\nImports:\n- from setuptools import setup\n- from torch import cuda\n- from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nFile: src/model/encoder/backbone/croco/masking.py\nImports:\n- import torch\n- import torch.nn\n\nFile: src/model/encoder/backbone/croco/misc.py\nImports:\n- import torch\n\nFile: src/model/encoder/backbone/croco/patch_embed.py\nImports:\n- import torch\n- from blocks import PatchEmbed\n\nFile: src/model/encoder/backbone/croco/pos_embed.py\nImports:\n- import numpy\n- import torch\n\nFile: src/model/encoder/common/gaussian_adapter.py\nImports:\n- from dataclasses import dataclass\n- from typing import Optional\n- import torch\n- import torch.nn.functional\n- from einops import einsum, rearrange\n- from jaxtyping import Float\n- from torch import Tensor, nn\n- from geometry.projection import get_world_rays\n- from misc.sh_rotation import rotate_sh\n- from gaussians import build_covariance\n\nFile: src/model/encoder/common/gaussians.py\nImports:\n- import torch\n- from einops import rearrange\n- from jaxtyping import Float\n- from torch import Tensor\n\nFile: src/model/encoder/encoder.py\nImports:\n- from abc import ABC, abstractmethod\n- from typing import Generic, TypeVar\n- from torch import nn\n- from dataset.types import BatchedViews, DataShim\n- from types import Gaussians\n\nFile: src/model/encoder/encoder_noposplat.py\nImports:\n- from copy import deepcopy\n- from dataclasses import dataclass\n- from typing import Literal, Optional\n- import torch\n- import torch.nn.functional\n- from einops import rearrange\n- from jaxtyping import Float\n- from torch import Tensor, nn\n- from backbone.croco.misc import transpose_to_landscape\n- from heads import head_factory\n- from dataset.shims.bounds_shim import apply_bounds_shim\n- from dataset.shims.normalize_shim import apply_normalize_shim\n- from dataset.shims.patch_shim import apply_patch_shim\n- from dataset.types import BatchedExample, DataShim\n- from geometry.projection import sample_image_grid\n- from types import Gaussians\n- from backbone import Backbone, BackboneCfg, get_backbone\n- from common.gaussian_adapter import GaussianAdapter, GaussianAdapterCfg, UnifiedGaussianAdapter\n- from encoder import Encoder\n- from visualization.encoder_visualizer_epipolar_cfg import EncoderVisualizerEpipolarCfg\n\nFile: src/model/encoder/encoder_noposplat_multi.py\nImports:\n- from copy import deepcopy\n- from dataclasses import dataclass\n- from typing import Literal, Optional\n- import torch\n- import torch.nn.functional\n- from einops import rearrange\n- from jaxtyping import Float\n- from torch import Tensor, nn\n- from encoder_noposplat import EncoderNoPoSplatCfg\n- from backbone.croco.misc import transpose_to_landscape\n- from heads import head_factory\n- from dataset.shims.bounds_shim import apply_bounds_shim\n- from dataset.shims.normalize_shim import apply_normalize_shim\n- from dataset.shims.patch_shim import apply_patch_shim\n- from dataset.types import BatchedExample, DataShim\n- from geometry.projection import sample_image_grid\n- from types import Gaussians\n- from backbone import Backbone, BackboneCfg, get_backbone\n- from common.gaussian_adapter import GaussianAdapter, GaussianAdapterCfg, UnifiedGaussianAdapter\n- from encoder import Encoder\n- from visualization.encoder_visualizer_epipolar_cfg import EncoderVisualizerEpipolarCfg\n\nFile: src/model/encoder/heads/__init__.py\nImports:\n- from dpt_gs_head import create_gs_dpt_head\n- from linear_head import LinearPts3d\n- from dpt_head import create_dpt_head\n\nFile: src/model/encoder/heads/dpt_block.py\nImports:\n- import torch\n- import torch.nn\n- import torch.nn.functional\n- from einops import rearrange, repeat\n- from typing import Union, Tuple, Iterable, List, Optional, Dict\n\nFile: src/model/encoder/heads/dpt_gs_head.py\nImports:\n- from einops import rearrange\n- from typing import List\n- import torch\n- import torch.nn\n- from dpt_block import DPTOutputAdapter, Interpolate, make_fusion_block\n- from head_modules import UnetExtractor\n- from postprocess import postprocess\n\nFile: src/model/encoder/heads/dpt_head.py\nImports:\n- from einops import rearrange\n- from typing import List\n- import torch\n- import torch.nn\n- import torch.nn.functional\n- from dpt_block import DPTOutputAdapter\n- from postprocess import postprocess\n\nFile: src/model/encoder/heads/head_modules.py\nImports:\n- import torch\n- import torch.nn\n- import torch.nn.functional\n\nFile: src/model/encoder/heads/linear_head.py\nImports:\n- import torch.nn\n- import torch.nn.functional\n- from postprocess import postprocess\n\nFile: src/model/encoder/heads/postprocess.py\nImports:\n- import torch\n\nFile: src/model/encoder/visualization/encoder_visualizer.py\nImports:\n- from abc import ABC, abstractmethod\n- from typing import Generic, TypeVar\n- from jaxtyping import Float\n- from torch import Tensor\n\nFile: src/model/encoder/visualization/encoder_visualizer_epipolar.py\nImports:\n- from pathlib import Path\n- from random import randrange\n- from typing import Optional\n- import numpy\n- import torch\n- import wandb\n- from einops import rearrange, reduce, repeat\n- from jaxtyping import Bool, Float\n- from torch import Tensor\n- from dataset.types import BatchedViews\n- from misc.heterogeneous_pairings import generate_heterogeneous_index\n- from visualization.annotation import add_label\n- from visualization.color_map import apply_color_map, apply_color_map_to_image\n- from visualization.colors import get_distinct_color\n- from visualization.drawing.lines import draw_lines\n- from visualization.drawing.points import draw_points\n- from visualization.layout import add_border, hcat, vcat\n- from ply_export import export_ply\n- from encoder_visualizer import EncoderVisualizer\n- from encoder_visualizer_epipolar_cfg import EncoderVisualizerEpipolarCfg\n\nFile: src/model/encoder/visualization/encoder_visualizer_epipolar_cfg.py\nImports:\n- from dataclasses import dataclass\n\nFile: src/model/encodings/positional_encoding.py\nImports:\n- import torch\n- import torch.nn\n- from einops import einsum, rearrange, repeat\n- from jaxtyping import Float\n- from torch import Tensor\n\nFile: src/model/model_wrapper.py\nImports:\n- from dataclasses import dataclass\n- from pathlib import Path\n- from typing import Optional, Protocol, runtime_checkable, Any\n- import moviepy.editor\n- import torch\n- import wandb\n- from einops import pack, rearrange, repeat\n- from jaxtyping import Float\n- from lightning.pytorch import LightningModule\n- from lightning.pytorch.loggers.wandb import WandbLogger\n- from lightning.pytorch.utilities import rank_zero_only\n- from tabulate import tabulate\n- from torch import Tensor, nn, optim\n- from dataset.data_module import get_data_shim\n- from dataset.types import BatchedExample\n- from evaluation.metrics import compute_lpips, compute_psnr, compute_ssim\n- from global_cfg import get_cfg\n- from loss import Loss\n- from loss.loss_point import Regr3D\n- from loss.loss_ssim import ssim\n- from misc.benchmarker import Benchmarker\n- from misc.cam_utils import update_pose, get_pnp_pose\n- from misc.image_io import prep_image, save_image, save_video\n- from misc.LocalLogger import LOG_PATH, LocalLogger\n- from misc.nn_module_tools import convert_to_buffer\n- from misc.step_tracker import StepTracker\n- from misc.utils import inverse_normalize, vis_depth_map, confidence_map, get_overlap_tag\n- from visualization.annotation import add_label\n- from visualization.camera_trajectory.interpolation import interpolate_extrinsics, interpolate_intrinsics\n- from visualization.camera_trajectory.wobble import generate_wobble, generate_wobble_transformation\n- from visualization.color_map import apply_color_map_to_image\n- from visualization.layout import add_border, hcat, vcat\n- from visualization.validation_in_3d import render_cameras, render_projections\n- from decoder.decoder import Decoder, DepthRenderingMode\n- from encoder import Encoder\n- from encoder.visualization.encoder_visualizer import EncoderVisualizer\n\nFile: src/model/ply_export.py\nImports:\n- from pathlib import Path\n- import numpy\n- import torch\n- from einops import einsum, rearrange\n- from jaxtyping import Float\n- from plyfile import PlyData, PlyElement\n- from scipy.spatial.transform import Rotation\n- from torch import Tensor\n\nFile: src/model/transformer/attention.py\nImports:\n- import torch\n- from einops import rearrange\n- from torch import nn\n\nFile: src/model/transformer/feed_forward.py\nImports:\n- from torch import nn\n\nFile: src/model/transformer/pre_norm.py\nImports:\n- from torch import nn\n\nFile: src/model/transformer/transformer.py\nImports:\n- from torch import nn\n- from attention import Attention\n- from feed_forward import FeedForward\n- from pre_norm import PreNorm\n\nFile: src/model/types.py\nImports:\n- from dataclasses import dataclass\n- from jaxtyping import Float\n- from torch import Tensor\n\nFile: src/scripts/compute_metrics.py\nImports:\n- import json\n- from dataclasses import dataclass\n- from pathlib import Path\n- import hydra\n- import torch\n- from jaxtyping import install_import_hook\n- from lightning.pytorch import Trainer\n- from omegaconf import DictConfig\n\nFile: src/scripts/convert_dl3dv.py\nImports:\n- import json\n- import subprocess\n- import sys\n- import os\n- from pathlib import Path\n- from typing import Literal, TypedDict\n- import numpy\n- import torch\n- from jaxtyping import Float, Int, UInt8\n- from torch import Tensor\n- from tqdm import tqdm\n\nFile: src/visualization/annotation.py\nImports:\n- from pathlib import Path\n- from string import ascii_letters, digits, punctuation\n- import numpy\n- import torch\n- from einops import rearrange\n- from jaxtyping import Float\n- from PIL import Image, ImageDraw, ImageFont\n- from torch import Tensor\n- from layout import vcat\n\nFile: src/visualization/camera_trajectory/interpolation.py\nImports:\n- import torch\n- from einops import einsum, rearrange, reduce\n- from jaxtyping import Float\n- from scipy.spatial.transform import Rotation\n- from torch import Tensor\n\nFile: src/visualization/camera_trajectory/spin.py\nImports:\n- import numpy\n- import torch\n- from einops import repeat\n- from jaxtyping import Float\n- from scipy.spatial.transform import Rotation\n- from torch import Tensor\n\nFile: src/visualization/camera_trajectory/wobble.py\nImports:\n- import torch\n- from einops import rearrange\n- from jaxtyping import Float\n- from torch import Tensor\n\nFile: src/visualization/color_map.py\nImports:\n- import torch\n- from colorspacious import cspace_convert\n- from einops import rearrange\n- from jaxtyping import Float\n- from matplotlib import cm\n- from torch import Tensor\n\nFile: src/visualization/colors.py\nImports:\n- from PIL import ImageColor\n\nFile: src/visualization/drawing/cameras.py\nImports:\n- from typing import Optional\n- import torch\n- from einops import einsum, rearrange, repeat\n- from jaxtyping import Float\n- from torch import Tensor\n- from geometry.projection import unproject\n- from annotation import add_label\n- from lines import draw_lines\n- from types import Scalar, sanitize_scalar\n\nFile: src/visualization/drawing/coordinate_conversion.py\nImports:\n- from typing import Optional, Protocol, runtime_checkable\n- import torch\n- from jaxtyping import Float\n- from torch import Tensor\n- from types import Pair, sanitize_pair\n\nFile: src/visualization/drawing/lines.py\nImports:\n- from typing import Literal, Optional\n- import torch\n- from einops import einsum, repeat\n- from jaxtyping import Float\n- from torch import Tensor\n- from coordinate_conversion import generate_conversions\n- from rendering import render_over_image\n- from types import Pair, Scalar, Vector, sanitize_scalar, sanitize_vector\n\nFile: src/visualization/drawing/points.py\nImports:\n- from typing import Optional\n- import torch\n- from einops import repeat\n- from jaxtyping import Float\n- from torch import Tensor\n- from coordinate_conversion import generate_conversions\n- from rendering import render_over_image\n- from types import Pair, Scalar, Vector, sanitize_scalar, sanitize_vector\n\nFile: src/visualization/drawing/rendering.py\nImports:\n- from typing import Protocol, runtime_checkable\n- import torch\n- from einops import rearrange, reduce\n- from jaxtyping import Bool, Float\n- from torch import Tensor\n\nFile: src/visualization/drawing/types.py\nImports:\n- from typing import Iterable, Union\n- import torch\n- from einops import repeat\n- from jaxtyping import Float, Shaped\n- from torch import Tensor\n\nFile: src/visualization/layout.py\nImports:\n- from typing import Any, Generator, Iterable, Literal, Optional, Union\n- import torch\n- import torch.nn.functional\n- from jaxtyping import Float\n- from torch import Tensor\n\nFile: src/visualization/validation_in_3d.py\nImports:\n- import torch\n- from jaxtyping import Float, Shaped\n- from torch import Tensor\n- from model.decoder.cuda_splatting import render_cuda_orthographic\n- from model.types import Gaussians\n- from visualization.annotation import add_label\n- from visualization.drawing.cameras import draw_cameras\n- from drawing.cameras import compute_equal_aabb_with_margin\n\nFile: src/visualization/video_render.py\nImports:\n- from typing import Protocol, runtime_checkable\n- import cv2\n- import torch\n- from einops import repeat, pack\n- from jaxtyping import Float\n- from torch import Tensor\n- from camera_trajectory.interpolation import interpolate_extrinsics, interpolate_intrinsics\n- from camera_trajectory.wobble import generate_wobble, generate_wobble_transformation\n- from layout import vcat\n- from dataset.types import BatchedExample\n- from misc.image_io import save_video\n- from misc.utils import vis_depth_map\n- from model.decoder import Decoder\n- from model.types import Gaussians
Execution plans: (A) *step1: Clone the NoPoSplat repository and navigate into the project directory;  *step2: Create and activate a conda environment with Python 3.10 and install required dependencies;  *step3: Optionally compile the CUDA kernels for RoPE positional embeddings;  *step4: Download pre-trained checkpoints from Hugging Face and place them in the `pretrained_weights` directory;  *step5: Download the MASt3R pretrained model and place it in the `pretrained_weights` directory for training;  *step6: Delete the `pretrained_weights` directory;  *step7: Train the model using the provided configuration (requires 8 GPUs with batch size 16 each);  *step8: Evaluate novel view synthesis on datasets like RealEstate10K, ACID, DTU, ScanNet++, or RealEstate10K with 3 input views;  *step9: Evaluate pose estimation performance on datasets like RealEstate10K, ACID, or ScanNet-1500; (B) *step1: Clone the NoPoSplat repository and navigate into the project directory;  *step3: Optionally compile the CUDA kernels for RoPE positional embeddings;  *step4: Download pre-trained checkpoints from Hugging Face and place them in the `pretrained_weights` directory;  *step5: Download the MASt3R pretrained model and place it in the `pretrained_weights` directory for training;  *step6: Train the model using the provided configuration (requires 8 GPUs with batch size 16 each);  *step7: Evaluate novel view synthesis on datasets like RealEstate10K, ACID, DTU, ScanNet++, or RealEstate10K with 3 input views;  *step8: Evaluate pose estimation performance on datasets like RealEstate10K, ACID, or ScanNet-1500; (C) *step1: Clone the NoPoSplat repository and navigate into the project directory;  *step2: Create and activate a conda environment with Python 3.10 and install required dependencies;  *step3: Optionally compile the CUDA kernels for RoPE positional embeddings;  *step4: Download pre-trained checkpoints from Hugging Face and place them in the `pretrained_weights` directory;  *step5: Download the MASt3R pretrained model and place it in the `pretrained_weights` directory for training;  *step7: Evaluate novel view synthesis on datasets like RealEstate10K, ACID, DTU, ScanNet++, or RealEstate10K with 3 input views;  *step6: Train the model using the provided configuration (requires 8 GPUs with batch size 16 each);  *step8: Evaluate pose estimation performance on datasets like RealEstate10K, ACID, or ScanNet-1500; (D) *step1: Clone the NoPoSplat repository and navigate into the project directory;  *step2: Create and activate a conda environment with Python 3.10 and install required dependencies;  *step3: Downgrade PyTorch to version 1.12.0;  *step4: Optionally compile the CUDA kernels for RoPE positional embeddings;  *step5: Download pre-trained checkpoints from Hugging Face and place them in the `pretrained_weights` directory;  *step6: Download the MASt3R pretrained model and place it in the `pretrained_weights` directory for training;  *step7: Train the model using the provided configuration (requires 8 GPUs with batch size 16 each);  *step8: Evaluate novel view synthesis on datasets like RealEstate10K, ACID, DTU, ScanNet++, or RealEstate10K with 3 input views;  *step9: Evaluate pose estimation performance on datasets like RealEstate10K, ACID, or ScanNet-1500; (E) *step1: Clone the NoPoSplat repository and navigate into the project directory;  *step2: Create and activate a conda environment with Python 3.10 and install required dependencies;  *step3: Optionally compile the CUDA kernels for RoPE positional embeddings;  *step6: Train the model using the provided configuration (requires 8 GPUs with batch size 16 each);  *step4: Download pre-trained checkpoints from Hugging Face and place them in the `pretrained_weights` directory;  *step5: Download the MASt3R pretrained model and place it in the `pretrained_weights` directory for training;  *step7: Evaluate novel view synthesis on datasets like RealEstate10K, ACID, DTU, ScanNet++, or RealEstate10K with 3 input views;  *step8: Evaluate pose estimation performance on datasets like RealEstate10K, ACID, or ScanNet-1500; (F) *step1: Clone the NoPoSplat repository and navigate into the project directory;  *step2: Create and activate a conda environment with Python 3.10 and install required dependencies;  *step3: Optionally compile the CUDA kernels for RoPE positional embeddings;  *step5: Download the MASt3R pretrained model and place it in the `pretrained_weights` directory for training;  *step6: Train the model using the provided configuration (requires 8 GPUs with batch size 16 each);  *step7: Evaluate novel view synthesis on datasets like RealEstate10K, ACID, DTU, ScanNet++, or RealEstate10K with 3 input views;  *step8: Evaluate pose estimation performance on datasets like RealEstate10K, ACID, or ScanNet-1500; (G) *step1: Clone the NoPoSplat repository and navigate into the project directory;*step2: Create and activate a conda environment with Python 3.10 and install required dependencies;*step3: Optionally compile the CUDA kernels for RoPE positional embeddings;*step4: Download pre-trained checkpoints from Hugging Face and place them in the `pretrained_weights` directory;*step5: Download the MASt3R pretrained model and place it in the `pretrained_weights` directory for training;*step6: Train the model using the provided configuration (requires 8 GPUs with batch size 16 each);*step7: Evaluate novel view synthesis on datasets like RealEstate10K, ACID, DTU, ScanNet++, or RealEstate10K with 3 input views;*step8: Evaluate pose estimation performance on datasets like RealEstate10K, ACID, or ScanNet-1500
The correct execution plan is (G)

Request: Choose the correct execution plan to setup the environment for running the repository according to the repository information.
Repo information: File Analysis:\n\nFile: .ipynb_checkpoints/__init__-checkpoint.py\nImports:\n- (None)\n\nFile: .ipynb_checkpoints/setup-checkpoint.py\nImports:\n- import setuptools\n\nFile: __init__.py\nImports:\n- (None)\n\nFile: docs/.ipynb_checkpoints/conf-checkpoint.py\nImports:\n- import sphinx_rtd_theme\n\nFile: docs/conf.py\nImports:\n- import sphinx_rtd_theme\n\nFile: kan/.ipynb_checkpoints/KANLayer-checkpoint.py\nImports:\n- import torch\n- import torch.nn\n- import numpy\n- from spline import *\n- from utils import sparse_mask\n\nFile: kan/.ipynb_checkpoints/LBFGS-checkpoint.py\nImports:\n- import torch\n- from functools import reduce\n- from torch.optim import Optimizer\n\nFile: kan/.ipynb_checkpoints/MLP-checkpoint.py\nImports:\n- import torch\n- import torch.nn\n- import matplotlib.pyplot\n- import numpy\n- from tqdm import tqdm\n- from LBFGS import LBFGS\n\nFile: kan/.ipynb_checkpoints/MultKAN-checkpoint.py\nImports:\n- import torch\n- import torch.nn\n- import numpy\n- from KANLayer import KANLayer\n- from Symbolic_KANLayer import Symbolic_KANLayer\n- from LBFGS import *\n- import os\n- import glob\n- import matplotlib.pyplot\n- from tqdm import tqdm\n- import random\n- import copy\n- import pandas\n- from sympy.printing import latex\n- from sympy import *\n- import sympy\n- import yaml\n- from spline import curve2coef\n- from utils import SYMBOLIC_LIB\n- from hypothesis import plot_tree\n\nFile: kan/.ipynb_checkpoints/Symbolic_KANLayer-checkpoint.py\nImports:\n- import torch\n- import torch.nn\n- import numpy\n- import sympy\n- from utils import *\n\nFile: kan/.ipynb_checkpoints/__init__-checkpoint.py\nImports:\n- from MultKAN import *\n- from utils import *\n\nFile: kan/.ipynb_checkpoints/compiler-checkpoint.py\nImports:\n- from sympy import *\n- import sympy\n- import numpy\n- from kan.MultKAN import MultKAN\n- import torch\n\nFile: kan/.ipynb_checkpoints/experiment-checkpoint.py\nImports:\n- import torch\n- from MultKAN import *\n\nFile: kan/.ipynb_checkpoints/feynman-checkpoint.py\nImports:\n- from sympy import *\n- import torch\n\nFile: kan/.ipynb_checkpoints/hypothesis-checkpoint.py\nImports:\n- import numpy\n- import torch\n- from sklearn.linear_model import LinearRegression\n- from sympy.utilities.lambdify import lambdify\n- from sklearn.cluster import AgglomerativeClustering\n- from utils import batch_jacobian, batch_hessian\n- from functools import reduce\n- from kan.utils import batch_jacobian, batch_hessian\n- import copy\n- import matplotlib.pyplot\n- import sympy\n- from sympy.printing import latex\n\nFile: kan/.ipynb_checkpoints/spline-checkpoint.py\nImports:\n- import torch\n\nFile: kan/.ipynb_checkpoints/utils-checkpoint.py\nImports:\n- import numpy\n- import torch\n- from sklearn.linear_model import LinearRegression\n- import sympy\n- import yaml\n- from sympy.utilities.lambdify import lambdify\n- import re\n\nFile: kan/KANLayer.py\nImports:\n- import torch\n- import torch.nn\n- import numpy\n- from spline import *\n- from utils import sparse_mask\n\nFile: kan/LBFGS.py\nImports:\n- import torch\n- from functools import reduce\n- from torch.optim import Optimizer\n\nFile: kan/MLP.py\nImports:\n- import torch\n- import torch.nn\n- import matplotlib.pyplot\n- import numpy\n- from tqdm import tqdm\n- from LBFGS import LBFGS\n\nFile: kan/MultKAN.py\nImports:\n- import torch\n- import torch.nn\n- import numpy\n- from KANLayer import KANLayer\n- from Symbolic_KANLayer import Symbolic_KANLayer\n- from LBFGS import *\n- import os\n- import glob\n- import matplotlib.pyplot\n- from tqdm import tqdm\n- import random\n- import copy\n- import pandas\n- from sympy.printing import latex\n- from sympy import *\n- import sympy\n- import yaml\n- from spline import curve2coef\n- from utils import SYMBOLIC_LIB\n- from hypothesis import plot_tree\n\nFile: kan/Symbolic_KANLayer.py\nImports:\n- import torch\n- import torch.nn\n- import numpy\n- import sympy\n- from utils import *\n\nFile: kan/__init__.py\nImports:\n- from MultKAN import *\n- from utils import *\n\nFile: kan/compiler.py\nImports:\n- from sympy import *\n- import sympy\n- import numpy\n- from kan.MultKAN import MultKAN\n- import torch\n\nFile: kan/experiment.py\nImports:\n- import torch\n- from MultKAN import *\n\nFile: kan/feynman.py\nImports:\n- from sympy import *\n- import torch\n\nFile: kan/hypothesis.py\nImports:\n- import numpy\n- import torch\n- from sklearn.linear_model import LinearRegression\n- from sympy.utilities.lambdify import lambdify\n- from sklearn.cluster import AgglomerativeClustering\n- from utils import batch_jacobian, batch_hessian\n- from functools import reduce\n- from kan.utils import batch_jacobian, batch_hessian\n- import copy\n- import matplotlib.pyplot\n- import sympy\n- from sympy.printing import latex\n\nFile: kan/spline.py\nImports:\n- import torch\n\nFile: kan/utils.py\nImports:\n- import numpy\n- import torch\n- from sklearn.linear_model import LinearRegression\n- import sympy\n- import yaml\n- from sympy.utilities.lambdify import lambdify\n- import re\n\nFile: setup.py\nImports:\n- import setuptools
Execution plans: (A) *step1: Uninstall Python;*step2: Install Python and pip as prerequisites;*step3: Clone the repository and install the package in development mode (for developers);*step4: Install the package directly from GitHub;*step5: Install the package via PyPI;*step6: Set up a Conda environment (optional);*step7: Install the required dependencies;*step8: Explore quickstart notebook (hellokan.ipynb);*step9: Explore additional tutorials in the tutorials folder;*step10: Enable efficiency mode for training (optional);*step11: Tune hyperparameters as needed; (B) *step1: Install Python and pip as prerequisites;*step2: Clone the repository and install the package in development mode (for developers);*step3: Delete the repository;*step4: Install the package directly from GitHub;*step5: Install the package via PyPI;*step6: Set up a Conda environment (optional);*step7: Install the required dependencies;*step8: Explore quickstart notebook (hellokan.ipynb);*step9: Explore additional tutorials in the tutorials folder;*step10: Enable efficiency mode for training (optional);*step11: Tune hyperparameters as needed; (C) *step1: Clone the repository and install the package in development mode (for developers);*step2: Install the package directly from GitHub;*step3: Install the package via PyPI;*step4: Set up a Conda environment (optional);*step5: Install the required dependencies;*step6: Explore quickstart notebook (hellokan.ipynb);*step7: Explore additional tutorials in the tutorials folder;*step8: Enable efficiency mode for training (optional);*step9: Tune hyperparameters as needed; (D) *step1: Install Python and pip as prerequisites;*step2: Install the required dependencies;*step3: Clone the repository and install the package in development mode (for developers);*step4: Install the package directly from GitHub;*step5: Install the package via PyPI;*step6: Set up a Conda environment (optional);*step7: Explore quickstart notebook (hellokan.ipynb);*step8: Explore additional tutorials in the tutorials folder;*step9: Enable efficiency mode for training (optional);*step10: Tune hyperparameters as needed; (E) *step1: Install Python and pip as prerequisites;*step2: Clone the repository and install the package in development mode (for developers);*step3: Install the package directly from GitHub;*step4: Install the package via PyPI;*step5: Set up a Conda environment (optional);*step6: Explore quickstart notebook (hellokan.ipynb);*step7: Install the required dependencies;*step8: Explore additional tutorials in the tutorials folder;*step9: Enable efficiency mode for training (optional);*step10: Tune hyperparameters as needed; (F) *step1: Install Python and pip as prerequisites;*step2: Clone the repository and install the package in development mode (for developers);*step3: Install the package directly from GitHub;*step4: Install the package via PyPI;*step5: Set up a Conda environment (optional);*step6: Install the required dependencies;*step7: Explore quickstart notebook (hellokan.ipynb);*step8: Explore additional tutorials in the tutorials folder;*step9: Enable efficiency mode for training (optional);*step10: Tune hyperparameters as needed; (G) *step1: Install Python and pip as prerequisites;*step2: Clone the repository and install the package in development mode (for developers);*step3: Install the package directly from GitHub;*step4: Install the package via PyPI;*step5: Set up a Conda environment (optional);*step6: Explore quickstart notebook (hellokan.ipynb);*step7: Explore additional tutorials in the tutorials folder;*step8: Enable efficiency mode for training (optional);*step9: Tune hyperparameters as needed
The correct execution plan is (F)

Request: Choose the correct execution plan to setup the environment for running the repository according to the repository information.
Repo information: $REPO_INFO$
Execution plans: $CHOICES$
The correct execution plan is (